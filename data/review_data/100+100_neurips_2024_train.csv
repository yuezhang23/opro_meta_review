id;text;label
tEEpVPDaRf;"REVIEW 
Summary:
MuDI is a novel framework designed for multi-subject personalization in text-to-image diffusion models. It effectively decouples identities of multiple subjects, using segmented subjects from a foundation model for data augmentation and generation initialization. A new metric is introduced to evaluate multi-subject personalization. Experimental results show MuDI produces high-quality personalized images without identity mixing, outperforming existing methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper is well-written and easy to follow.
- The experimental results are comprehensive and sufficiently support the claims made in the paper.
- MuDI successfully prevents identity mixing when generating multiple subjects, maintaining clear individual identities.
- Introduction of a new metric provides a better assessment of multi-subject personalization performance.

Weaknesses:
- Inheriting from DreamBooth, MuDI requires test-time fine-tuning for each set of concepts, which might hinder its practical application. In contrast, some approaches like IP-Adapter can achieve customization in a tuning-free manner.
- There is a spelling error on line 796. Additionally, there is a miscitation on line 690, where FastComposer[51] is incorrectly referred to as a ""single-subject personalization"" method; it is actually a zero-shot multi-subject customization method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a training and inference pipeline and an evaluation benchmark for multi-concept customization for text-to-image diffusion models. Specifically, the pipeline comprises a Seg-Mix training stage, which can be viewed as a data augmentation trick to prevent the fine-tuned model from learning mixed attributes, and a mean-shifted noise initialization for the very first step of the denoising process, aiming to inject appearance and layout priors. The evaluation benchmark takes into consideration the dysfunctionality of previous methods when evaluating the disentanglement extent between customized subjects. The benchmark utilizes a Detect-and-Compare workflow considering the similarity for the same subject and dissimilarity between two distinct subjects.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is well-written and easy to follow. I didn’t encounter many confusing expressions during my first reading.
2.	The introduced methods are intuitively effective, and the design of the evaluation benchmark is convincing for evaluating the extent of disentanglement.
3.	This work introduces two interesting additional uses: size control and modular customization, which are helpful for improving the application scenarios for customization.

Weaknesses:
1.	Though the method is intuitively effective, as I stated in Strength #2, the novelty is limited since some designs have been experimentally verified by previous methods, such as the utilization of descriptive class [1] and a data-augmentation pipeline for multi-concept customization [2].
2.	Though the proposed method seems reasonable for combining distinct subjects with different region locations, like a cat and a dog sitting beside each other, it may be challenged in scenarios where the subjects have rich semantic interactions, like a person wearing glasses. This limitation is induced by the region-control designs, where both augmented training and the initialization during inference pose strong regularization for decoupling two instances. This limitation makes this method difficult for general multi-concept customization.

[1] InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetunin

[2] SVDiff: Compact Parameter Space for Diffusion Fine-Tuning

Limitations:
See Weakness#2

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces MuDI, a novel framework designed to improve multi-subject personalization in text-to-image diffusion models. Unlike current methods that often mix identities and attributes from different subjects when generating multiple subjects simultaneously, MuDI effectively decouples these identities. The framework employs segmented subjects generated by a foundation model for segmentation, known as Segment Anything, which is used for both training and inference. This approach serves as data augmentation for training and as initialization for the generation process. Additionally, the authors introduce a new metric to better evaluate the performance of their method in multi-subject personalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The topic is interesting and it has a good novelty.
2.	The presentation is good and the results look promising.

Weaknesses:
The dataset is small, and more analysis should be made. Please see the detailed Questions.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes MuDI, a novel method for generating images with multiple personalized subjects. By leveraging segmented subjects from reference images for both training and inference, MuDI effectively addresses the challenge of identity mixing in multi-subject image generation. Key contributions include a new data augmentation technique (Seg-Mix) and a new evaluation metric for multi-subject fidelity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a novel approach to multi-subject image generation by leveraging segmented subjects for both training and inference, effectively decoupling subject identities. This represents a creative combination of existing techniques in image segmentation and text-to-image generation.
- The paper is well-structured and clearly presented, with a solid experimental evaluation demonstrating the effectiveness of the proposed method.
- The authors effectively communicate the problem, the proposed solution, and the experimental results. The paper is well-organized and easy to follow.
- By addressing the critical challenge of identity mixing in multi-subject image generation, the paper offers a valuable contribution to the field. The proposed method has the potential to significantly impact applications requiring the generation of multiple distinct subjects within a single image.

Weaknesses:
- While the paper presents a comparative analysis with existing methods, a more comprehensive evaluation against a wider range of baselines, including recent advancements in image generation and personalization, would strengthen the paper's claims. It would be essential to compare with methods like PortraitBooth (CVPR 2024) and FastComposer. 
- Additionally, exploring different evaluation metrics beyond the proposed D&C metric could provide a more holistic assessment of the method's performance.
- The paper lacks sufficient details about the dataset used for training and evaluation. A more in-depth description of the dataset, including its size and diversity, would enhance the reproducibility of the work.
- Although the paper includes some ablation studies, a more comprehensive analysis of the impact of different components of the proposed method (e.g., Seg-Mix, initialization, descriptive class) on the overall performance would provide deeper insights into the method's effectiveness.
- While the paper acknowledges the limitations of existing methods, a more thorough discussion of the potential limitations of the proposed MuDI method, such as its sensitivity to image complexity or its performance on highly similar subjects, would strengthen the paper's overall contribution. Some studies on how the size of the objects composed using SegMix during training affects the model, i.e, does it lead to any size biases in the model?

Limitations:
- Could the authors elaborate on the limitations of MuDI, such as its performance on highly similar subjects or complex scenes?

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
nMFVdphOc9;"REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3mzFmBPFIX;"REVIEW 
Summary:
This work presents a method for learning metriplectic systems from data. Metriplectic systems are a model which conserve energy and produce entropy, two desirable features. Their method, termed “neural metriplectic systems” (NMS), is based on a more efficient system parametrization. The authors also prove universal approximation results on non-degenerate systems, and generalization error bounds. They verify that their method outperforms other metriplectic-learning baselines, GNODE and GFINN, on two physical experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: Although I am not at all an expert in this field and therefore cannot properly judge, it seems that the main theorem (Theorem 3.4) is novel and non-trivial.

Quality: The proposed method outperforms baselines in two experimental settings, verifying the expected gain from having a more efficient parametrization. The corresponding theoretical results, on universality and generalization, provide a fairly thorough picture of the method. 

Significance: Within the field of learning metriplectic systems, this paper seems to make a valuable contribution and improve on prior work.

Clarity: The paper is very well-written, and mathematically rigorous. Although the details are not accessible to someone without a background that matches the subject material rather closely, the high-level ideas about the benefits of metriplectic systems, what past work has done, and the advantages of their new method, are conveyed well.

Weaknesses:
Clarity:
1. Although well-written, the paper is not accessible to most machine learning audiences, and seemingly requires the reader to already have a physics background in phenomenological modeling, or exterior algebra.

2. Mathematical terms such as algebraic brackets, Poisson brackets, degenerate metric brackets, etc. should be defined in the beginning of the paper, or with a reference to a textbook or other paper defining them. The “exterior algebra” background is suitable for only those with a strong mathematical background already, using terms like “wedge product” and “graded algebra” without definition. (Admittedly, it would be impossible to fully explain all of these concepts in only 9 pages — perhaps a citation to a textbook would be helpful here, but in practice if the reader needs to understand the decomposition result properly to grasp the contribution, then this work may be more suitable for a venue other than a machine learning conference.)

Quality: The baselines in experiments, as well as the methods discussed in the exposition, are all metriplectic. However, it seems like other methods (e.g. which preserve energy but do not increase entropy, or even those which are not physics-informed at all), should be included too.

Significance: I am not sure how widely applicable metriplectic learning systems are, or what alternative (non-metriplectic) methods can be used for the same problems. The paper would be improved by providing more of this background/motivation.

Overall, as a non-expert, my main concern is with the suitability of this work for a machine learning conference - I defer to the AC on this point. It seems that the machine learning techniques used within NMS are fairly straightforward, while it is the parametrization in Theorem 3.4 that seemingly constitutes the crux of the method. However, the statement and proof of Theorem 3.4 would be more accessible to a physics or math audience, than an ML audience.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new parameterization for neural metriplectic systems which explicitly incorporates structural information about the degeneracy conditions $\{ S, \cdot \} = 0, [E,\cdot ] = 0$ into the model. The model requires $\sim O(n^2)$ learnable parameters for a problem with $n$ state variables instead of some prior methods which need $O(n^3)$. Further, it also encodes this degeneracy condition in a hard constraint, leading to models which will by construction respect these desired physical conditions. The authors provide a deep learning implementation scheme for their method which involves learning $E(x), S(x)$ and using $\nabla E, \nabla S$ to construct the matrices $L, M$ needed for the bracket from observed trajectories of the physical system. The gradients $\nabla E, \nabla S$ needed for the brackets are computed with autodifferentiation. The authors show that this system is trained end-to-end on simple physical systems including a two-gas system and a thermo-elastic pendulum and can outperform existing methods on these benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper motivates the need for metriplectic systems which can be efficiently implemented and that incorporate the dynamical constraints required for these systems (energy conservation and entropy production). This captures a potentially interesting class of physical systems that could be modeled by machine learning methods like those employed in the present work. The method that they present as Algorithm 1 is straightforward and improves upon the cubic time complexity of GNODE or GFINN. The authors also provide an approximation result for their algorithm and support their claims with some experiments.

Weaknesses:
While the authors improve the scaling from cubic to quadratic in the number of state variables, the total complexity (quadratic) still scales poorly with size of the problem (number of state variables / dimensions). Further, the current experiments and comparisons were performed on small benchmarks. However, since this paper is the first to point out that the cubic scaling can be improved by reflecting constraints due to degeneracy, I think the experimental component of the contribution is not the most important.

Limitations:
The authors do mention the primary limitations of this present work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a parameter efficient parameterization for neural networks simulating metripletic systems, which are differential equations that have both an energy dissipative piece and an energy conservative piece. The method works by learning several of the required quantities (L and M, which trade off dissipation and conservation, I believe), while also using a small neural network to estimate the dissipation and conservation pieces (E(x) and S(x) ). As not all quantities in the state, x, can be observed, they use a time based diffusion model to emulate the hidden states (e.g. entropy) to develop initial conditions for these. Experiments are performed on two systems of this class, where it seems like the method performs better (probably due to having better inductive biases).


Unfortunately, due to not having a strong physics background, I feel somewhat unqualified to judge many of the technical strengths although things seem reasonable from a skim. I don’t know if I can properly assess novelty and significance as a result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Significance:
-	Building better emulators of physical systems that are complicated is a good first step in what the authors term “phenomenonological” understanding of these systems.

-	Even quicker training time (and demonstrated both practically and theoretically) is quite helpful. I remember one of the original issues with NODE was that it took a very long time to converge.

Clarity: 
-	Overall, the paper is pretty well written, even if quite dense, and okay to follow for a non expert physicist. I was able to follow at least the ML pieces and the experiments section quite well.

-	The relevant literature is reasonably well signposted; I learned a fair bit about the state of this field by checking the references.

Novelty:

-	The approach seems to have a clear inductive bias win over the prior works GFINN and GNODE due to better parameterization of the system.

Weaknesses:
Unfortunately, the writing ends up being quite dense and technical with minimal outside applications. 

Sure, emulating these physical systems in the experiments is quite nice, but what types of applications does this lead to? This is more of a writing based thing and the paper could be refactored around one of these applications if possible.

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
rcch4UsMBi;"REVIEW 
Summary:
## Overall summary
- This paper introduces GLAN, a method for enhancing LLMs by generating synthetic instruction data using a taxonomy of human knowledge and capabilities. GLAN constructs this taxonomy by decomposing knowledge into fields and disciplines, leveraging LLMs for generating a comprehensive syllabus for each subject. 
- GLAN’s scalable and customizable framework allows for easy integration of new fields of skills, highlighting its potential for ongoing improvement and adaptation.

## My opinion of the paper
- I think this is a really interesting approach to generate data that can allow LLMs to be potentially smarter. However, I am wondering if there are newer topics, for example (within the medical area, we have the new topic called ""Covid-19"".) Since GLAN is very dependent on LLMs, the main area of concern would be ensuring that the LLMs that GLAN depends on remains updated.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
## Originality
- The approach is quite interesting. The authors made use of real life scenarios, which is to use the structure of human education systems to build the taxonomy. This approach mimics the systematic acquisition of knowledge and skills in education, providing a framework for generating instruction data.
## Clarity
- Pseudo Algorithm provided and figures are easy to understand.
## Significance
- By creating a general and scalable method for instruction tuning, GLAN has the potential to improve the performance of LLMs across a wide range of tasks and domains.

Weaknesses:
## Quality
- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.

Limitations:
Indicated in the appendix (do consider placing it in main paper), but did not mention about computation cost like what was mentioned in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces generalized isntruction tuning (GLAN), an approach for synthesizing instruction tuning data using a taxonomy-based approach. GLAN generates synthetic instruction data from pre-curated taxonomy of human knowledge and capabilities and aims to create diverse and broad-ranging instruction dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Comprehensive Coverage of Evaluation: The paper presents extensive experiments demonstrating that GLAN outperforms various popular instruction-tuned LLMs across multiple dimensions, including mathematical reasoning, coding, logical reasoning, and general instruction following.
2. Minimization of Human Involvement: The generation process significantly reduces human involvement, requiring human verification only at the taxonomy construction stage. This makes the approach scalable and less labor-intensive.
3. Customizability and Extensibility: The taxonomy-based approach allows for easy customization and extension. New fields or skills can be incorporated by simply adding new nodes to the taxonomy.

Weaknesses:
1. While the paper addresses generalization, there is a risk that the generated synthetic data might overfit to the taxonomy's structure, potentially missing out on more nuanced, real-world instructions.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GLAN, a general and scalable method for instruction tuning of Large Language Models (LLMs). GLAN employs a top-down approach to generate high-quality instruction tuning datasets. Experiments across various benchmarks demonstrate that GLAN performs comparably to other existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper focuses on the alignment of Large Language Models, which is a trendy and important topic. If the dataset is released, it will be beneficial for the community.
2. This method is easy to follow. The process is highly scalable, leveraging LLMs like GPT-4 for generating instructions on a massive scale.
GLAN allows for easy customization. New fields can be added by incorporating new nodes into the taxonomy.

Weaknesses:
The novelty is limited as similar top-down designs have been utilized in many previous works. Besides, the main experimental results in Table 1 appear mediocre compared to other methods.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generalized way of creating instruction data. The high-level motivation is to take inspiration from how curriculum is designed for human learning into a taxonomy of subjects and use the same to prompt an off-the-shelf LLM to create data. GLAN does not need seed examples, or pre built-taxonomy like prior work. Human verification is also performed post the building of taxonomy to weed out unimportant or inaccurate divisions. The overall process is High level taxonomy -> subjects -> syllabus -> instructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Overall strong Performance: Extensive experiments show GLAN's effectiveness in various tasks, outperforming or matching state-of-the-art models in several benchmarks (Table 1)
2. Figure 2 on scaling properties of GLAN: I found this figure quite interesting. It suggests a log linear scaling trend in performance as GLAN data is scaled up. This is quite promising.
3. Section 3.5 on Task-specific overfitting: Another great analysis section that discusses how GLAN does not particularly overfit to the training data. This ensures that the synthetic data remains generalizable across different domains.
4. Modularity of the pipeline: The modular nature of the GLAN pipeline allows for easy customization and extension by incorporating new nodes into the taxonomy without re-generating the entire dataset.

Weaknesses:
1. No use of actual human curriculum: The paper set the expectation right in the abstract of using/getting strongly inspired from human curriculum. I was disappointed that the method does not utilize existing human curriculum structures, potentially missing out on years of insights in developing the same. Generating synthetic data, and in this case entire taxonomies from pre-existsing models can lead to extremely large amounts of bias. I would have much rather seen the authors delegate only lower level questions to LLMs than high level abstractions, which would lead to a trickle down effect on every single node in the taxonomy. This study, in my opinion, is incomplete without using either human generated taxonomies, and/or a comparison between how different the taxonomies are.
2. Computation cost not compared: The paper does not provide a comparison of computational costs with similar methods, such as WizardLM. For instance, GLAN training required approximately 8 days using 32 A100 GPUs to generate 10 million instructions, but no direct comparisons are made to illustrate the efficiency or cost-effectiveness relative to other approaches.
3. The method is limited by the performance of GPT-3.5/4: The quality of the generated taxonomy and syllabus heavily depends on the capabilities of the underlying LLMs used in the process, namely GPT-3.5 and GPT-4. In general, GLAN does not inform how we can improve capabilities of models beyond GPT4. But also, does not consider the cost of generating 10 million instructions.
4. High variability in results (Table 2): There is significant variability in GLAN's performance across different categories, with particularly weaker results in humanities and social sciences compared to STEM fields. The authors should address this, also discuss the document proportion of each taxonomy, and potentially see if there is a correlation between the data size and performance.

Limitations:
Please see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RMdnTnffou;"REVIEW 
Summary:
This paper presents a type of label free concept bottleneck model for ante-hoc interpretability that incorporates a hierarchichal concept representation. The concepts are represented in a two-level hierarchy with high-level concepts denoting scenes/objects and lower level concepts denoting more specific attributes at a patch-level. Experimentally, the authors show better accuracy and better concept prediction (on datasets with GT annotations) while proposing a metric based on Jaccard index to judge quality of concept prediciton.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. I really like the idea of hierarchichal representation of concepts. It is natural and intuitive and novel in the context of concept representations for interpretability. It's specific instantiation here is also reasonable.
2. Experiments are comprehensive in in terms of multiple, diverse large-scale datasets and baselines generally.
3. The presentation in general is strong and motivations are clear.

Weaknesses:
1. I have some important concerns about interpretability and soundness of label-free CBMs in general that rely on CLIP embedding similarity for concept prediction. Please see Q.1, 2 in Questions tab.

2. The only baseline I would suggest adding would be standard CBMs, specially for concept prediction accuracy (Tab. 2). 
Although I expect standard CBM to perform better given its supervised training, but it'd be interesting to see how much is the performance gap if there is any.

3. It'd be interesting to see (even if qualitatively) how the system behaves with concept intervention like the original CBMs.

Limitations:
The authors do discuss them separately and clearly (partly in main paper and partly in appendix).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduced a novel Concept Bottleneck Model (CBM) that facilitates hierarchical concept learning. Specifically, the proposed Concept Discovery Block (CDB) plays a pivotal role in uncovering concepts from preprocessed image-text similarity embeddings by employing a variational Bayesian framework to learn a binary mask. Additionally, by applying the CDB module to each patch-level image to detect low-level concepts and the entire image for high-level concept discovery, information propagation between the two levels leads to robust classification performance through sparse concept learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- S1: As an ante-hoc interpretable CBM, the proposed method is intuitive and requires lightweight computation, which is desirable. 
- S2: The performance of the proposed method is superior to other multimodal CBM baselines.

Weaknesses:
- W1: The class/label designated at the high level and its attributes at the low level are strong hierarchical constraints. So, the proposed method was limited to showing its applicability only in cases with a transparent hierarchical relationship between attributes and classes by specifying the pool of low-level concepts corresponding to each class. This may require burdensome human inspection to configure.
- W2: Another concern is the fixation of the interpretable threshold to all CDB modules as 0.05. The author described it as the probability value used to determine whether the specific concept is active. However, even if some image patches have the same concept, it is evident that the concept may contribute to each patch to a different extent. Therefore, dynamically adjusting or learning the threshold may perform better than a fixed threshold.

Limitations:
Please check out the Weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces a novel framework that leverages recent advances in vision-language models and a Bayesian approach for coarse-to-fine concept selection. It introduces the notion of concept hierarchy, allowing high-level concepts to be characterized by lower-level attributes and exploiting granular information in image patches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is fluent and easily comprehensible.
- Propose a novel way of assessing the interpretation capacity of CF-CBMs based on the Jaccard index between ground truth concepts and learned data-driven binary indicators. 
- Extensive experiments were conducted to demonstrate that the proposed CF-CBMs outperform other state-of-the-art methods in terms of classification accuracy and interpretability.

Weaknesses:
- Over-reliance on the vision-language backbone's capability might result in poor performance for images from uncommon datasets.
- There is a lack of experiments on test-time concept interventions.

Limitations:
The author mention in the Limitations of the dependence on the vision-language backbone.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose coarse-to-fine concept selection in Concept Bottleneck Models (CBMs). They introduce a concept hierarchy that identifies low-level concepts in local patches of input images, as well as high-level concepts in the overall images.  Additionally, the authors enhance interpretability by considering sparsity in concept predictions. Their proposed model, CF-CBM, achieves high classification performance while maintaining interpretability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a novel evaluation metric based on Jaccard similarity to evaluate concept predictions.

2. The proposed method enhances both classification accuracy and concept prediction accuracy by making predictions from local patches in a sparse manner.

Weaknesses:
1. While the Jaccard similarity metric effectively assesses the alignment between the model's predicted concepts and the actual concepts, it serves only as one aspect of interpretability evaluation. Notably, Table 2 indicates that the Jaccard index is quite low, raising questions about whether such a score sufficiently demonstrates the model's interpretability. Additionally, it would be helpful to clarify what factors contribute to the low Jaccard index.

2. Moreover, the concepts described in the paper appear somewhat ambiguous. I encourage the authors to refer to the Questions section for further clarification.

3. A clearer explanation is needed regarding how the authors' proposed approach enhances classification accuracy and interpretability. Specifically, it would be helpful to understand whether predicting concepts from local patches is effective, if learning class predictions aids in identifying low-level concepts, and how the application of sparsity contributes to these improvements. Additionally, an ablation study is necessary to support these claims. This also includes an explanation of why this approach can be referred to as coarse-to-fine.

Limitations:
The authors have adequately addressed limitations in the Limitations & Conclusions section. Since the proposed method uses a frozen pretrained CLIP as its backbone, the ability to discover concepts may be constrained by the limitations inherent in CLIP's training.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ctXYOoAgRy;"REVIEW 
Summary:
This paper delves deeper into how LLMs handle multilingualism. The authors hypothesized a three-stage multilingual workflow called MWork: understanding, task-solving, and generating and which language(s) become essential in each stage. To verify their proposed workflow, they experimentally identify language-specific parameters and selectively deactivate them within different structures so this allows to assess the functionality of corresponding structures and enables the hypothesis. To do so, the authors develop a novel approach called Parallel Language-specific Neuron Detection (PLND). Without requiring labeled data, PLND can identify the language specific neurons. Following the PLND, the authors successfully identify the language-specific neurons which account for only 0.13% of all neurons. Their extensive results report that, by deactivating those neurons, the multilingual task performance could significantly dropped.

This paper tackles an important research question of LLMs: how do large language models handle multilingualism? To address this, they deliberately design a new approach of PLND and successfully identifies a language-specific neurons in the LLM models. Along with their MWork hypothesis, they carefully conducted experiments with multiple languages with different scrips and verified their hypothesized three-stage multilingual workflow.

This paper develops a useful model analysis tool of PLND. Their hypothesis is well explored in multiple languages. Technically well sound.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- well-organized paper. clear presentation.
- Technically sound by extensive experiments with different natural language understanding tasks with diverse language sets

Weaknesses:
- no major weakness though

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents two key contributions. One is Parallel Language-specific Neuron Detection, a method of identifying elements of multilingual LLMs that are responsible for handling particular languages; the method only requires unlabeled text data in each language in order to detect these neurons, which makes it cheap and efficient. The second contribution is an insight into the workflow of multilingual LLMs (referred to in the paper as MWork), claiming that LLMs process input in three steps: understanding, task solving and output generation; task solving further splits into retrieval and thinking. Also, the thinking step is shown to be happening in English, while understanding, knowledge retrieval and generation are handled multilingually / specifically to the language of the input. Authors use PLND to verify that the models chosen for the experiments actually follow this workflow and conduct thorough experiments on two models about a dozen languages other than English, including high-resource and low-resource examples.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Both key contributions (PLND and MWork) are of fundamental relevance to the field of LLM interpretability and analysis

- Presented experiments are rigorous and thorough

- The paper is written clearly and although the subject is complex and multi-faceted, the authors do a good job of introducing the necessary concepts and definitions and explaining the rationale behind their work as well as the conducted experiments and analysis of their results.

Weaknesses:
W1. Most importantly, the claims are very broad (""LLMs are this / LLMs do that""), however this is verified on two very similar models: the included Mistral and Vicuna models both have 7 billion parameters, which puts them on the modest end of modern LLMs. How can we be sure that models with more parameters (13 / 30 / 70 / ...) behave in a similar manner without testing?, what about the influence of the context length on the performance? This is even mentioned in the limitations section of the paper: I am not suggesting that the authors run an additional set of expensive experiments -- I am suggesting that the paper text should be adjusted to reflect the actual findings of the presented experiments and would avoid bold claims about ""all LLMs"".

W2. Another weakly supported claim is that 400 documents was enough for language-specific neuron tuning, simply because tuning on 800 documents did not yield better results. I find that this is a single instance and without running more comparisons and more thorough comparisons this claim does not hold and should be adjusted in the text. This is especially important since improvements of 2.3% (low-resource) and 3.6% (high-resource) are small-scaled improvements and bigger amounts of language-specific
 data could be expected to yield bigger improvements. Again, rather than running tons of more experiments for a single paper I instead suggest that the paper text should be adjusted (e.g. calling it a ""pilot study"" or ""preliminary indications/results"" -- PLND and MWork themselves are already worthy contributions).

W3. Not a strong weakness, but minor typos:
- row 101: ""With a set of n corpus"" --> ""With a set of n corpora"", same on row 103 ('corpus' in plural is 'corpora')
- row 319: ""one brunch of work"" --> ""one branch of work""

Limitations:
All good.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes MWork, a workflow to study how large language models handle multilingual inputs. The main idea is to detect English and non-English neurons by probing their value differences and performance gaps. Based on the results, they argue that there are three steps in the workflow: understanding, task solving, and generating. They also use experiments to support their hypothesis.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- They propose an interesting way to understand the mechanism of large language models.

Weaknesses:
- The authors classify neurons based on English and non-English, which is not very convincing to me. I believe that, beyond these two categories, there should be some neurons that are helpful for **all** languages. These neurons can be used for general understanding and logical reasoning. However, this paper ignores this aspect. See more explanations below.
  -  For neuron detection, when computing the impact, I am curious why you do not remove the overlapping neurons. In this way, you can detect the real language-specific ones. Specifically, in Appendix C, although the authors argue that the intersection with English from other languages is relatively limited based on rows, if we look at the columns, it seems that all the language-specific neurons are covered by English ones, suggesting that they are actually not language-specific. It is very likely that these neurons are essential for general understanding and are language-agnostic.
  - It is not clear how English and non-English tokens are detected in Figure 1. Do you determine this by applying a decoder to decode from the hidden representations?
  - The authors argue that large language models will use English tokens for task solving with multilingual inputs based on the interpretation of Figure 1. However, Figure 1 also shows that models will use non-English tokens for task solving with English. I don’t think this is reasonable. From my perspective, the way to classify tokens or neurons is not accurate, and the ignorance of language-agnostic neurons makes this figure unreasonable.
  - From the experimental results, deactivating language-specific neurons sometimes causes a performance drop for both English and non-English, suggesting the existence of general neurons.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines how LLMs handle multilingualism. The author proposes a hypothetic workflow (MWork), which suggests that LLMs understand the multilingual query, think in English, and than generate results in the input language. A neuron detection method is proposed to detect language specific neurons. By deactivation some of the neurons, the authors validates the above workflow, and show that the  ability for a specific language could be improved by finetuning only language specific neurons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents an interesting  hypothesized workflow and validate it with neuron analysis.

The analysis with neuron deactivation is sound enough.

Weaknesses:
The analysis of this paper is based on the understanding of  layers. However, it might be possible that different tasks or even different instances may have different splitting of layers for different stages in the workflow. How would this affect the analysis?

It is still strange to know that 400 documents could improve the language ability of a low-resource languages, especially when 800 documents will not be more helpful.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zm1LcgRpHm;"REVIEW 
Summary:
This paper introduces a new method for time-series representation learning that enhances the modeling of non-adjacent segment dependencies. Specifically, the proposed method segments, shuffles in a learned manner and stitches the shuffled segments to combine with original time series. The proposed method is model-agnostic without adding significant parameter overhead and shows performance improvement across multiple classification and forecasting base models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method permutes the original segments to better capture inter-relations between distant segments. It is model-agnostic and introduces minimal parameter overhead to the original model.

2. Extensive experiments on various base models for both classification and forecasting tasks demonstrate the effectiveness of the proposed method.

Weaknesses:
1. It it not clear how the sorting process, specifically the calculation of permutation $\sigma$ from $P$, is made differentiable.

2. The compared forecasting baselines such as Informer are no longer state-of-the-art methods. Adding more recent baselines such as Time-LLM, GPT4TS, DLinear, PatchTST would provide a clearer understanding of the proposed method's comparative benefits.

3. The basic assumption for S3 is that modeling non-adjacent dependencies is important. However, the paper lacks detailed case studies that demonstrate the specific types of non-adjacent dependencies effectively captured by S3, which are not addressed by existing models. Additionally, there is no case study to validate that the learned shuffling weights accurately represent these segment dependencies.

Limitations:
The paper mentions potential expansions into tasks like imputation and anomaly detection. Further details on limitations from the reviewer are discussed in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to enhance time-series representation learning in existing models. S3 operates by dividing the original sequence into non-overlapping segments and shuffling them in a learned manner that is optimal for the given task. It then reattaches the shuffled segments and performs a learned weighted sum with the original input to capture both the newly shuffled sequence and the original sequence. This proposed model can enhance the performance of specific models in classification and prediction tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is easily comprehensible and straightforward.

Sufficient experiments are conducted to confirm the effectiveness of the method.

Weaknesses:
Lack of comparative methods:
In fact, the proposed method seems to share the same spirit as data augmentation methods in the time series field[1-4]. Why hasn't any data augmentation method been compared?


Selection of baseline models:
The selected baseline model, Informer, seems somewhat outdated. Why not choose a more recent model, e.g., iTransformer[5] or PatchTST[6]?


Dataset for prediction task:
The author conducted experiments on three ETT datasets, but for prediction tasks, more datasets should be considered, e.g., traffic, electricity, and weather.


Time-Series Representation Claim:
 As the author pointed out, more tasks should be considered for time series representation learning.


[1]FRAUG: FREQUENCY DOMAIN AUGMENTATION FOR TIME SERIES FORECASTING  [2]Time Series Data Augmentation for Deep Learning: A Survey  [3]SimPSI: A Simple Strategy to Preserve Spectral Information in Time Series Data Augmentation [4]TOWARDS DIVERSE AND COHERENT AUGMENTATION FOR TIME-SERIES FORECASTING [5]ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE FOR TIME SERIES FORECASTING [6]A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new neural network design element which segments, shuffles, and stitches time series for improved representation learning. They evaluate their methods on forecasting and classification tasks, and show that S3 benefits some widely used baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. To the best of my knowledge, the idea is novel, and fundamentally challenges and changes how to learn representations for time series data
2. The paper is well written and easy to follow
3. Experiments are well-designed, and results are promising

Weaknesses:
I have not found any major weaknesses in the methodology or experimental design. However,  I think that the paper might benefit from showing what the S3 module is actually learning. For example, the authors can include the segmented, shuffled, and stitched time series on a particular dataset as an example, along with the weighted time series (used as input to the model), and the original time series. This might provide some intuition as to how this design element improves predictive performance. 

I think there's always scope to improve experimental design. TS2Vec is a excellent choice for classification, but not for forecasting. I would recommend that the authors use methods such as PatchTST (transformer-based) or iTransformer, TimesNet (CNN-based), N-BEATs or N-HITS (MLP-based) etc. for time series forecasting. For classification, it would also be good to compare with fully supervised methods such as ResNet1D (see [1]). 

### References
[1] Ismail Fawaz, Hassan, et al. ""Deep learning for time series classification: a review."" Data mining and knowledge discovery 33.4 (2019): 917-963.

Limitations:
The authors have a very brief description of limitations of their study.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper paper introduces a new approach called Segment, Shuffle, and Stitch (S3) to enhance time-series representation learning. The method involves segmenting the time-series into non-overlapping parts, shuffling them optimally, and stitching them back together along with the original sequence.

Key contributions include:

- Proposing the S3 mechanism to improve time-series representation learning by dynamically reordering segments.
- Demonstrating that S3 can be integrated with existing neural architectures like CNNs and Transformers, resulting in significant performance improvements.
- Showing through extensive experiments that S3 enhances performance in time-series classification and forecasting tasks, with improvements up to 68%.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Code is available, making reproducing this paper easier.
- Paper is clear.
- Results appear good, when considered on the set of baselines and dataset picked by the authors.

Weaknesses:
- Tables 1 and 2 focus on the ETT datasets, which are only a (highly intra-correlated) subset of the common forecasting datasets: Electricity, Traffic, Weather, Illness...
- I see no mention of CoST in the results tables, despite being cited in the paper. This is usually a very strong baseline for contrastive approaches. Including it would certainly paint a more complete picture of the results landscape. On a related note this also applies to e.g. more recent transformer baselines. Informer is relevant, but also very far from state of the art.
- Error bars would help one better contextualize the results.
- The lack of an ablation study makes understanding the reason this works more complicated.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a simple but effective differentiable module that performs pre-processing to input multivariate time-series before being fed into any differentiable model for arbitrary task. The pre-processing involves segmenting, shuffling the segments and stiching them together. The novelty include making this seemingly discrete operations into a differentiable module. This simple idea yields significant improvement in performance of different kinds of models over variety of datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The method is simple and easy to add to most deep learning models
2. The technical details are well-motivated and explained
3. The method also improves training efficiency and convergence time along with performance with very little increate in model complexity
4. Experimental results across different tasks are strong

Weaknesses:
1. Visualization and any qualitative study on the shuffling and segments generalted by S3 would greatly benefit the readers.
2. How well does it optimize transformer based models, especially those that already do segmentation like PatchTST since the attention module captures the relations all pairs of segments already?
3. Does the representations due to S3 generalize to multiple tasks at a time or do we need to retrain for each task?

Limitations:
1. Lack of understanding on the segment permutations generated and why they are better for the model performance atleast qualitatively

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
J8m0DEjxEO;"REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TWfNFCOPaK;"REVIEW 
Summary:
The paper proposes the use of the Wireless Geometric Algebra Transformer (Wi-GATr) to model signal propagation. Based on the Wi-GATr network, it introduces a differentiable prediction model and a diffusion model. Compared to traditional statistical and ray-tracing methods, the proposed approach not only addresses conventional signal prediction problems but also tackles inverse problems such as receiver localization and 3D environment reconstruction. Experimental results demonstrate the effectiveness of this method. Furthermore, the authors present two large-scale wireless signal propagation datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The perspective is interesting. This paper models wireless propagation as a probability model based on diffusion, thus offering a unified approach to address signal prediction and inverse problems such as receiver localization or 3D scene reconstruction.
2) The paper is logically structured, with a comprehensive background introduction and high readability.

Weaknesses:
1) There is a gap between the challenges and the solutions. The author asserts that wireless surrogate modelling faces challenges like data scarcity and diverse data types. However, the lack of analysis on these issues makes the proposed solutions appear abrupt. It is recommended to provide insights that lead to the proposed solutions of this paper.
2) The innovation is somewhat limited. Wi-GATr primarily extends the GATr method into a wireless setting, with equivariance being a pre-existing property of the original framework. Apart from tokenizing input data, did the paper introduce any additional advancements? It would be beneficial for the authors to highlight these aspects.
3) The experimental evaluation is not sufficiently convincing. For more details, please refer to the ""Questions*"" part.
4) There are some typographical errors in the paper. For example, lines 56 and 57 do not correspond to Figure 1. Additionally, the abstract mentions transmitter localization, but the main text describes receiver localization, among other discrepancies.

Limitations:
Sufficiently discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the use of a transformer architecture to model electromagnetic propagation of physical systems. The approach is claimed to outperform existing methods by (i) computational efficiency (compared to raytracers) and (ii) enabling solving inverse problems. The method is evaluated on a number of benchmark tasks and the paper is accompanied by two new datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of modeling electromagnetic wave propagation using transformer architectures is novel and interesting. 

The generality of the approach enables a large number of tasks in wireless communication systems that would otherwise the use of raytracers or other electromagnetic modeling software.

Large parts of the main body of the paper are well-written and easy to follow.

Weaknesses:
The main body of the paper lacks the details of the proposed pipeline and transformer architecture. In fact, all of the interesting and technical details are relegated to the appendices. 

One of the main motivations of the paper seems to be that most raytracers are too slow. However, the authors seem to ignore recent projects, such as Instant RM (https://github.com/NVlabs/instant-rm) which can compute coverage maps in a few milliseconds, depending on the desired accuracy.

It is unclear why [29] is cited as a non-differentiable raytracer although it is, to my knowledge, the only raytracer that actually is. Instant RM is also differentiable and calibration results for both tools were already demonstrated. To be honest, I have the impression that the authors tried to cover up the fact that [29] is a powerful *differentiable* raytracer that enables solving inverse problems. 

Although the authors claim that channel impulse responses can be generated, this is not demonstrated in the paper. I think that this claim should be removed unless the authors demonstrate that it is actually feasible.

The description of scene geometry recovery is incomprehensible to me.

In Figure 7, the Wi-GATr is around 20ms for inference for a tiny indoor scene. The authors should compare this against Instant RM which can probably run even faster and is differentiable.

It would be good to get confidence information (e.g., standard deviation) in Fig. 3 and Fig. 4.

Limitations:
The paper lacks a detailed comparison to the capabilities of the differential raytracer from [29]. In fact, I feel that for the individual tasks, more baselines should be included. 

Scalability to very large datasets and extremely complex scenes is unclear. 

It is unclear whether the method generalizes to electromagnetic environments that are nonreciprocal (e.g., containing certain nonreciprocal metasurfaces). 

It is unclear whether the method generalizes to scenarios in which ray-tracing is inaccurate (e.g., scenarios at low carrier frequencies).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Authors proposed transformer based ideas on very well studied area: Wireless environment simulation.
The key idea here is to capitalize Geometric Transformers to simulate radio environments. It true that wireless (directional) signal propagation is a ray tracing approach, meaning a highly directional wireless signal (Ray) may bounce off ambient surfaces or directly reach a receiver. The proposed method inserts geometric shapes in the environment as tokens in a transformer networks. The trained transformer predicts the received power at a given point in 3D space. Transformer is trained and evaluated using two datasets: Wi3R and WiPTR that simulate indoor signal propagation environments. In comparison to the baselines, transformer architecture requires 20 times less data.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. It is interesting idea to see if transformer architectures are suitable to model radio propagation environments
2.  Propagation models consider the material of ambient surface, antenna parameters, location of transmitter and receiver

Weaknesses:
1. It is not clear if datasets are of any relevance to real world environment, since the primary challenge of any modelling problem is simulation to reality gap. Since wireless channel modelling is very well studied area, a novel contribution must take in to account such differences rather than results that show the ability of transformer architecture in modelling a wireless environment
2. There are several high fidelity radio propagation modelling software, perhaps it is important to consider datasets generated from such model, current evaluation is very limited and primitive. Fig 2 and 5 are no comparison to robust channel models that are available to wireless researcher and practitioners.
3. The modelling of radio environment is no clear, reviewer is of impression that several affects like diffraction, refraction are not considered in the datasets
4. The paper also has weakness: WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations which is both papers have not considered user mobility: coherence time, coherence bandwidth
5. Current evaluation is only limited to indoor environments
6. Since the prior work has already established neural network architecture are useful modelling, to push the state of the art, it is important to show accurate modelling than yet another architecture to model wireless channel.
7. Upon inspecting table 2, the reviewer is afraid that there might be issue with results here. There is 80dBm difference is accuracy with transformers based modelling, usually such a difference is unacceptable, can author please explain the training of transformer model and why it produced such a large error. The reviewer is concerned that whether such sample point is a fair to benchmark againt

Limitations:
In reviewer's opinion authors have not sufficiently addressed all the limitations of the current work. I encourage authors to look at weakness section and update the limitations of the work in the current draft

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The motivation for this work is that modeling the propagation of electromagnetic signals is critical for designing modern communication systems. Ray tracing simulators are not suitable for inverse problems or integration as channel models in designing communication systems.

In this context, the goal of this paper is to model the interplay between the 3D environment F, transmitting and receiving antennas (each characterized by a 3D position, orientation, and specific antenna characteristics) represented by t and r, respectively, and the signal h between each transmitter and receiver. The 3D geometry F is represented by a triangular mesh, where each triangle is assigned a material type from predefined classes, modeling both the shape and materials of the environment. Once the model is learned, three tasks can be performed:
1. Prediction of the received signal p(h∣F,t,r): The model is trained for this task. At test time, the network can predict signals in unseen, novel scenes. This approach is faster, fine-tunable on real measurements. The model obtained is also differentiable. This is referred to as the forward problem.
1. Localization of the receiver p(r∣F,t,h).
1. Sensing the environment p(F∣t,r,h).

The last two tasks are referred to as inverse problems. The model introduced is an adaptation of the Geometric Algebra Transformer called Wireless (Wi-GATr), used for simulating wireless propagation in a 3D environment. The authors also cast this problem as a generative modeling task of the joint distribution p(F,t,r,h) (from which the above three tasks can be accomplished) using Denoising Diffusion Probabilistic Models and Wi-GATr.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The main contributions of this work are as follows:
1. Introducing a new tokenization method for geometric wireless communication environments and transmitter and receiver characteristics.
1. Integrating diffusion-based models with Wi-GATr to model the wireless environment as a generative model, thereby determining the joint distribution of F, t, r, and h.
1. Providing new, larger datasets for the wireless environment modeling to the research community.

Weaknesses:
As per my understanding, the main weaknesses of the work are: 
1. The novelty of the work lies in tokenizing various geometric objects encountered in the wireless communication scene. However, the same tokenization is used in the vanilla transformer, making it unclear if the new tokenization provides any benefit.
1. As the authors point out, the channel is modeled only in terms of time-averaged non-coherent received power, missing crucial information such as time and direction of arrival, which are essential for modeling wireless environments.
1. While the proposed solutions seem general, most results are presented for the single antenna case. Additionally, the dataset includes only transmitting sinusoidal waveforms, which is limiting as it does not cover larger bandwidths. The wave propagation depends on frequency, and non-linearities can occur with wider bandwidths.

Limitations:
The authors mention limitations in the Discussion section (Section 6).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents the Wireless Geometric Algebra Transformer (Wi-GATr), a new architecture for simulating wireless signal propagation in 3D environments. This model utilizes geometric algebra to handle the geometric complexities of wireless scenes and ensures E(3) equivariance to respect the symmetries of the physical problem. The authors introduce two datasets, Wi3R and WiPTR, to benchmark their model. Wi-GATr outperforms existing baselines in terms of prediction fidelity and data efficiency, and it can solve both forward (signal prediction) and inverse (receiver localization and geometry reconstruction) problems in wireless communication.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of geometric algebra for handling complex 3D geometric data and ensuring E(3) equivariance is a novel and effective approach. This addresses the core challenge of accurately modeling wireless signal propagation in diverse environments.

2. The paper provides a thorough evaluation of Wi-GATr against multiple baselines across various tasks, demonstrating superior performance in signal prediction, receiver localization, and geometry reconstruction.

3. Wi-GATr shows remarkable data efficiency, achieving high-fidelity predictions with significantly less training data compared to other models. This is particularly beneficial for scenarios where obtaining large amounts of training data is challenging.

4. The model's ability to handle both forward (predictive modeling) and inverse (localization and reconstruction) problems showcases its versatility and potential for a wide range of applications in wireless communication.

Weaknesses:
1. Limited Real-World Testing: While the model performs well on the introduced datasets, its application in real-world, dynamic environments remains underexplored. Additional experiments in more varied and complex real-world scenarios, such as urban or industrial settings, would strengthen the paper.

2. Scalability and Computational Load: The paper could provide more detailed insights into the computational requirements and scalability of Wi-GATr. Understanding the model's performance with larger datasets and more complex environments would be valuable for practical deployment.

3. Generalizability Across Frequencies: The model is tested at a specific frequency (3.5 GHz). Evaluating its performance across different frequencies and under various signal conditions would provide a more comprehensive understanding of its robustness and generalizability.


4. Detailed Case Studies: While the paper presents strong experimental results, including more detailed case studies or examples of practical applications, such as network design or optimization in real-world environments, would illustrate the model's impact and practical benefits.

Limitations:
The authors briefly discussed the limitation in Sec. 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
kOp0kiXZ3a;"REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Wc0vlQuoLb;"REVIEW 
Summary:
This paper proposes a training-based confidence calibration method, named IDK-tuning, for improving LLM factuality. Specifically, a special `[IDK]` (""I Don't Know"") token is added to the model's vocabulary and an objective is introduced which shifts some probability mass of wrong predictions to the `[IDK]` token during continued pretraining, thereby encouraging the model to express uncertainty explicitly. Results of the IDK-tuned models are reported on commonly used factual benchmarks, showing the potential of this method for reducing hallucination while only suffering a slight sacrifice in terms of knowledge recall.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* Confidence calibration and LM hallucination are both important topics and this paper connects them in an interesting way.
* The proposed IDK-tuning method is intuitive and well-motivated. While there have been papers on supervised finetuning for calibration, most of them focus on aligning the model with human demonstration (which requires annotation) or synthetic data (which incur extra cost). The direct adaptation of the training objective that incorporates uncertainty seems quite novel to my best knowledge, and results seem promising with much higher precision and only slightly lower recall, which is suited for the current generally over-confident LLMs. 
* The authors perform extensive experiments on the scaling behavior and ablation for different components of the objective function.
* The authors do not overclaim their contribution. Singularities of the experiment results (e.g. `NaN`s in loss and collapsed recall from the `pythia` models) are mentioned and analyzed, and interesting insights are drawn from them.
* The writing is clear and the paper is easy to follow.

Weaknesses:
* For multiple-choice QA tasks, the precision improvement as well as the absolute values appear lower than those for the factual sentence completion tasks (comparing Table 1 and 2).  Given this, and that the multiple-choice QA tasks might be more applicable, it would be helpful if the authors could report more detailed results on each of the `lm-eval-harness` tasks, and investigate QA or other downstream tasks more carefully.
* Code or tuned model checkpoints are not provided, although some details about the settings and resources are mentioned in the paper.

Limitations:
The authors highlight several limitations, including the need for full pretraining on large corpus, which is costly, and that the method can slightly sacrifice recall or the overall performance on some downstream tasks such as text generation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes calibrating LLMs during a continued pertaining phase via an added [IDK] token to model uncertainty.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is structured well and written coherently.
- The introduction of the $\texttt{[IDK]}$ token to explicitly model uncertainty in LLMs is a novel approach 
- Through ablation studies, the behavior of the proposed method is investigated extensively.

Weaknesses:
- **Theory**: 
  - The approach is mathematically not very well grounded. Also, mathematical expressions such as $prob(\texttt{[tok]}, \hat{y})$ and $prob(argmax(\hat{y}), \hat{y})$ do not cohere with common practices [1,2,3,4]. The authors could consider something like $p(y_{t}=\texttt{[tok]} | y_{<t}, x)$ and $max_{i} \ p(y_{t}=i | y_{<t}, x)$.
  - The uncertainty factor is only bigger than $0$ if any other token gets assigned a higher probability than the $\texttt{[gold]}$ token. It does not account for the case where a model is uncertain about *any* token and thus predicts a (low) probability for all tokens. For instance, consider the tokens relating to ($\text{""Paris""}$, $\text{""Berlin""}$, $\text{""London""}$, $\text{""Rome""}$, $\text{""Vienna""}$). If the model predicts any of $p(y_{t} | \text{""The capital of France is""}) \in [(0.2, 0.2, 0.2, 0.2, 0.2), (0.3, 0.1, 0.2, 0.2, 0.2), ...]$, the uncertainty factor is $0$ no matter the hyperparameter $\Pi$, while it is clear that in all those cases the model is uncertain about the correct next token. The probability of the $\texttt{[IDK]}$ gets even decreased via the uncertainty regularization.

- **Evaluation**: The authors do not compare against other uncertainty quantification methods, such as (length-normalized) predictive entropy [1], p(true) [2], or semantic entropy [3,4]. These methods do not require additional pertaining, and thus do not suffer from training instabilities, mode collapse, or high computational costs, but can directly be applied to ""off-the-shelf"" models. Additionally, there exist methods that consider fine-tuning models to express their lack of knowledge that have not been considered.
  

---
[1] A. Malinin and M. Gales. Uncertainty estimation in autoregressive structured prediction.

[2] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Hatfield-Dodds, N. DasSarma, E. Tran-Johnson, S. Johnston, S. El-Showk, A. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bowman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson, J. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson, S. Ringer, D. Amodei, T. Brown, J. Clark, N. Joseph, B. Mann, S. McCandlish, C. Olah, J. Kaplan. Language Models (Mostly) Know What They Know.

[3] L. Kuhn, Y. Gal, and S. Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.

[4] L. Aichberger, K. Schweighofer, M. Ielanskyi, and S. Hochreiter. Semantically Diverse Language Generation for Uncertainty Estimation in Language Models.

Limitations:
The authors adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To allow LMs to express their uncertainty for generative tasks, the authors introduce a new special IDK token. The authors modify the cross-entropy training objective to assign part of the probability mass to the IDK token in cases where the model gets the prediction wrong. The token embedding is randomly initialized and then refined through additional fine-tuning of pretrained LLMs of various sizes and types (Pythia, Mistralv1, BERT).
Through experiments on a range of completion, QA & MCQA datasets the authors show that IDK tuning positively affects precision at a slight cost of recall. The authors perform further ablation experiments solidifying their choices for the loss weight hyperparameter as well as the regularization term. The authors further show that IDK tuning does not significantly adversely affect other capabilities of the underlying LMs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Well written and easy to follow
- To the best of my knowledge, presents a novel approach to quantifying uncertainty in LMs
- Exhaustive experimental evaluation & ablation over different hyperparameter choices demonstrating robustness of the proposed approach

Weaknesses:
- The IDK-tuning setup requires sometimes prohibitive additonal fine-tuning of the base LM
- It is unclear how the method would be applied when a model would be pretrained from scratch with the IDK token included - it is natural that LMs will be worse in predicting tokens as initial stages of training, so the pretrain - add IDK - tune paradigm seems as the only current option. The two previous points slightly limit the applicability of the metod.
- While the reported F scores are generally higher compared to baselines and alternatives, the IDK-tuned models still suffer from tangibly lower recall.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
It introduces a novel method to address the issue of hallucinations in Large Language Models (LLMs). These models, despite their proficiency in capturing and generating human knowledge, can sometimes produce factually incorrect text. To combat this, the authors propose a calibration method that incorporates an [IDK] (""I don’t know"") token into the model's vocabulary.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of the [IDK] token is a creative solution to an existing problem in LLMs. It represents a novel way to handle uncertainty, which is not just a new definition or problem formulation but also a practical application within language models.

The paper proposes a new pretraining objective that modifies the conventional cross-entropy loss to incorporate the [IDK] token. This is an original contribution to the field of natural language processing.

Weaknesses:
The paper primarily uses The Pile for training, which may not be representative of all possible language use cases or domains.

While the paper provides a good overview of the performance metrics, an in-depth error analysis could offer more insights into the types of errors the models are making and how the [IDK] token impacts these.

As the model is trained on web-crawled data, there is a risk of learning and perpetuating societal biases present in that data.

Limitations:
While the paper notes the potential for bias in the training data, it could provide more details on how this might affect the model's predictions and decision-making.

The paper could more explicitly discuss the potential for the model to contribute to the spread of misinformation, especially if it fails to correctly identify uncertain or incorrect information.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
mOK4yD8JFd;"REVIEW 
Summary:
This paper proposes a method to fuse a pair of short-exposure (noisy) and long-exposure (blurry) captures to produce clean and clear polarized snapshots. The proposed method consists of three phases to reconstruct the irradiance, texture, and polarization.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written, the proposed method is well-described, and experiments using both synthetic and real-world data are conducted to evaluate the effectiveness of the proposed method.

Weaknesses:
It is difficult to fully understand and evaluate the real-world experiments. Please refer to the corresponding questions for further details.

Limitations:
Limitations of this paper are well discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones. It adopts a neural network-based three-phase fusing scheme with specially designed modules tailored to each phase, which can not only improve the image quality but also preserve the polarization properties.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using complementarily fusing to achieve quality-improved and property-preserved polarimetric imaging is novel and reasonable. As obtaining high-quality polarized images is significant in polarization-based vision applications, while previous methods based on single modality (either noisy or blurry) tend to suffer from various artifacts, the proposed method could be a practical way to increase the performance of polarization-based vision applications.

The network module designs are also reasonable. All modules (Irradiance restoration, Polarization reconstruction, and Artifact suppression) are carefully and specially designed to solve the problems in the fusing process, which means the authors do spend efforts in observing and analyzing the properties in both the noisy and blurry polarized snapshots. 

The idea is clearly presented, and the experiments are sufficient. The performance improvement shows that the proposed method is effective.

In addition to the experiments on synthetic and real data, the authors also show the results of reflection removal, which makes the paper convincing.

Weaknesses:
The authors say that they adopt the PLIE dataset [32] as the source data to generate their own dataset. However, they do not tell the reasons why to choose the PILE dataset [32]. For example, [25] also provides a dataset (LLCP dataset) similar to the PLIE dataset [32], so why not choose the LLCP dataset as the source data? Any reasons?

Limitations:
The authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the first method for polarimetric image enhancement by fusing noisy and blurry pairs. While a short exposure polarimetric image produces sharp but noisy DoP and AoP, a long exposure makes them smooth but blurred. To effectively exploit the complementary advantages of these two images and satisfy the physics constraints of the polarimetric image, this paper proposes a three-phase fusing scheme. Experimental results show that the proposed method outperforms existing polarimetric image enhancement methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
+ The first method for polarimetric image enhancement by fusing noisy and blurry pairs.
+ Propose a novel fusing scheme to effectively use complementary polarimetric information of noisy and blurry pairs and retain polarimetric cues by directly processing DoP and AoP.
+ Experimentally validate the effectiveness of the fusion of noisy and blurry polarimetric image pairs and the proposed network. The accurate restoration of polarimetric cues is critical for downstream tasks.

Weaknesses:
- While the proposed method improves the pSNR of DoP and AoP, their SSIMs are almost the same as PLIE [32].
- Requiring two shots is undesirable for some downstream tasks.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
oe5ZEqTOaz;"REVIEW 
Summary:
This paper proposes a balanced multimodal learning method. Compared to existing methods that only consider the gradient size, it also considers the direction of the gradient.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The experiment includes multiple data sets and multiple tasks.

Weaknesses:
There is less visualization analysis of the experiment.

Limitations:
Yes, the authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a balanced multi-modal learning method with Classifier-Guided Gradient Modulation (CGGM), considering both the magnitude and directions of the gradients, with no limitations on the type of tasks, optimizers, the number of modalities.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Balanced multi-modal learning considering both the magnitude and directions of the gradients is a reasonable idea.
3. The proposed method is easy to follow.

Weaknesses:
1. Balanced multi-modal learning considering the directions of the gradients is not novel. A previous work [1] have already analyze the issue of modality dominance caused by gradient conflicts. The difference and the comparison with this method should be considered in detail. Besides, the approach to controlling gradient magnitude is similar to the ideas of OGM[2] and PMR[3].
2. This framework still does not explore the imbalance issue of multi-modal learning in more flexible task formats, such as the potential imbalance in tasks like AVQA and multi-modal generation. Expanding task formats to regression and segmentation tasks is only a minor improvement. Existing work can also be extended to these tasks with minor adjustments.

[1] Wang, H., Luo, S., Hu, G. and Zhang, J., 2024, March. Gradient-Guided Modality Decoupling for Missing-Modality Robustness. In *Proceedings of the AAAI Conference on Artificial Intelligence* (Vol. 38, No. 14, pp. 15483-15491).
[2] Peng, X., Wei, Y., Deng, A., Wang, D. and Hu, D., 2022. Balanced multimodal learning via on-the-fly gradient modulation. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition* (pp. 8238-8247).
[3] Fan, Y., Xu, W., Wang, H., Wang, J. and Guo, S., 2023. Pmr: Prototypical modal rebalance for multimodal learning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 20029-20038).

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the notorious modal imbalance problem in multi-modal learning. To alleviate the modality imbalance, the proposed method modulates gradient magnitude and the directions of the gradient simultaneously. Experiments on various multi-modal datasets demonstrate the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This paper explores an interesting problem. In the current joint learning paradigm, the dominant modality overpowers the learning process and the resulting gradient prohibits further exploration of the features in the weak modality.
- This paper is well-written and easy to follow.

Weaknesses:
- **The motivation is not stated clearly.** From Eq(3) and Eq(4), we can observe that the gradient magnitude can affect the update of a specific modality. However, the explanation of how the gradient direction between the specific modality and their fusion influences the modality update is unconvincing.
- **The experiments are not convincing.** The authors ignore recent state-of-the-art methods, such as UMT/UME [1], QMF [2], and ReconBoost [3]. It is recommended that the authors compare these methods. Additionally, the authors should plot the gradient direction, accuracy curve, and gradient profile after using their method and compare these to Fig. 2 to better highlight the effectiveness of their approach.
- **The related work section lacks discussion on recent research.** UMT [1] distills well-trained uni-modal features to assist multi-modal learning. QMF [2] provides a quality-aware multimodal fusion framework to mitigate the influence of low-quality multimodal data. ReconBoost [3] finds that the major issue arises from the current joint learning paradigm. They propose an alternating learning paradigm to fully harness the power of multi-modal learning.
- Some notations are confusing. Please see the questions below.

For now, I recommend a borderline for this paper, leaning to reject. If the concerns in weakness can be addressed in the rebuttal phase, I am willing to raise my concern and accept this paper.

[1]  On Uni-Modal Feature Learning in Supervised Multi-Modal Learning. ICML 2023.

[2] Provable Dynamic Fusion for Low-Quality Multimodal Data. ICML2023

[3] ReconBoost: Boosting Can Achieve Modality Reconcilement. ICML 2024.

Limitations:
None.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes CGGM, a novel strategy to balance the multimodal training process. Compared with existing methods, it can deal with the unbalanced multimodal learning problem with different optimizers, takes,  and more than two modalities.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The motivation is sufficient and the experiments on different tasks and datasets prove that the proposed method solves the problem well.

CGGM stands out by considering both the magnitude and direction of gradients for balancing multimodal learning. This combined approach effectively addresses the modality competition problem and ensures that all modalities contribute equally to the model’s performance.

Weaknesses:
1. The reviewer is curious about the computational complexity of the additional classifier or decoder. Is there any experimental result? 
2. As present in Line 149, the classifier fi consists of 1-2 multi-head self-attention (MSA) layers and a fully connected layer for classification and regression tasks. Does this apply to all models or classification tasks? Why is it set up like this? Why not just set it to the same classifier structure as the multimodal head?

3. What's the light decoder used for segmentation tasks?

4. The introduction of unimodal classification may limit the learning of multimodal tasks, such as gradient conflicts. How to deal with this problem?

5. PMR has also discussed the problem of gradient direction and introduced unimodal loss to assist multimodal learning. What's the difference between CGGM and PMR?

6. The reviewer is concerned about the accuracy of using the difference between the two consecutive ε to denote the modality-specific improvement for each iteration. According to my experience, the loss of the dominant modality will quickly drop to the magnitude of 1e-2 to 1e-3, while the magnitude of the weak modality is around 1e-1. At this time, the loss change of the weak modality will be larger, and according to the author, it will be regarded as the dominant modality. I 

7. What's the performance of the proposed method on CRAME-D and AVE datasets?  They are also widely used in previous studies.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
iNS3SC949v;"REVIEW 
Summary:
The paper is well-written and proposes a new modular mechanism that can (selectively) combine local and global interactions in a model. The authors discuss the theory behind their proposed smoothness operator and pooling mechanism and provide detailed proof for their claims. They have also evaluated their model against other methods on three different datasets (1 CT and 2 H&E) with reporting AUC and F1 score as their evaluation metrics. Their method is superior to other methods and the proper ablation study supports their claims.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The writing is concise and the contributions are clear. The author proposes a new smoothness mechanism, generally an interesting idea, to integrate local and global information. Besides the quantitative experiments, there are some qualitative experiments such as attention maps that show the model's relative importance on different regions in a slide.

Weaknesses:
There are three major concerns with this work:
1. The choice of encoder. The authors have used Resnet18 for RSNA and Camelyon16 and Resnet50 for PANDA. I could not find any justification for why different encoders have been used! Also, with the trend toward foundation models, transformer-based backbones are now of interest to the community. Therefore, for the sake of consistency, the author should use the same encoder for different datasets or report both resent50 and resnet18. And, for the sake of the generality of their work, they should add one encoder such as ViT or Swin as well (w/ ImageNet weights) to support the generality of their claims.

2. The body of research has been founded on local-to-global interactions. There are quite a few standard graph-based methodologies in the literature that the authors need to compare their work against. Currently, there are no representatives from those families of the MIL method in the benchmark. Two examples of such methods are (1) and (2).

3. There is a family of MIL methods in the literature that try to pseudo-label the instances during training, which is essentially equivalent to localization in this work. For instance, two of the most recent such methods are (3) and (4). The author should compare their method against these as they are essentially from the same family.

(1) Chen, Richard J., et al. ""Whole slide images are 2d point clouds: Context-aware survival prediction using patch-based graph convolutional networks."" Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part VIII 24. Springer International Publishing, 2021.

(2) Li, R., Yao, J., Zhu, X., Li, Y., Huang, J. (2018). Graph CNN for Survival Analysis on Whole Slide Pathological Images. In: Frangi, A., Schnabel, J., Davatzikos, C., Alberola-López, C., Fichtinger, G. (eds) Medical Image Computing and Computer Assisted Intervention – MICCAI 2018. MICCAI 2018. Lecture Notes in Computer Science(), vol 11071. Springer, Cham. https://doi.org/10.1007/978-3-030-00934-2_20

(3) Z. Shao et al., ""LNPL-MIL: Learning from Noisy Pseudo Labels for Promoting Multiple Instance Learning in Whole Slide Image,"" 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Paris, France, 2023, pp. 21438-21438, doi: 10.1109/ICCV51070.2023.01965.

(4) Ren, Q. et al. (2023). IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis. In: Greenspan, H., et al. Medical Image Computing and Computer Assisted Intervention – MICCAI 2023. MICCAI 2023. Lecture Notes in Computer Science, vol 14225. Springer, Cham. https://doi.org/10.1007/978-3-031-43987-2_54

Limitations:
It has been justified :)

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a multi-instance learning approach. The basic idea is to make use of the spatial dependency between training samples. A smoothing operator was proposed to regularize the attention matrix with respect to inter-sample similarity (to my understanding), which the authors claimed to improved both quality of localization and classification tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea seems unique and based, but more justifications are merited.

Weaknesses:
1. Although the smoothness property do exist in multi-instance learning as the authors claimed and may introduce useful information. It is still unclear why imposing it in the neural network helps with classification. It doesn't seem to serve as an induction bias from its looks.
2. Actually only 3 dataset are tested (though each with different variants), and the results are not so persuasive. Especially for the classification tasks that the authors want to credit, they failed to get 1st place in 2/3 of the AUC or F1 scores.

Limitations:
The method required strict ordering (or spatial information) or 3d-to-2d samples, which is quite a strong demand but it is not discussed in the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a technique to improve the localization capabilities of the current MIL, especially for CAD models that perform CT and WSI analysis. The method is based on seeing the attention attributed to each patch as a graph and minimizing its Dirichlet energy, promoting smooth transitions on the attention values of neighbor patches, which is in accordance with the assumption that neighboring patches share the same labels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The work is well-developed and well written, with a methodology explained through concise equations and theorems that guide the reader through the development of the idea. The results validate what is proposed, but minor adjustments and additional information will make the work even more valuable.

Weaknesses:
The method introduces the smooth operator on top of the existing ABMIL model. The proposed approach improves the performance of ABMIL on the CAMELYON dataset, both on localization and classification. Nevertheless, when analyzing the attention maps provided in the appendix, the differences between ABMIL and smAP are minimal. Although we have a numerical evaluation of the Dirichlet energy, we cannot tell if the drop in energy is significant only by these numbers, so this visual empiric evaluation is important for a better insight regarding the developed work. 

Observing the appendix figures, ABMIL already performs well in matching its attention to the tiles annotations. I suggest the authors apply the smooth operator on top of other models that did not perform well on CAMELYON for this task, such as CLAM, CAMIL, and SETMIL, to evaluate how minimizing the Dirichlet energy of the attention improves the localization task quantitatively and qualitatively.

I suggest the authors perform an ablation study to evaluate how minimizing the Dirichlet energy compares to other smoothing strategies. For instance, summing the variance of the attentions to the model’s loss may provide similar results as minimizing the Dirichlet energy. 

The analysis of the alpha parameter is great, but again, a qualitative approach is valuable for this case. Seeing how the attention map changes as the parameter alpha increases is highly valuable for the reader.

Lines 127 to 131: trying to debate equations not present in the paper is not good for the reader. I suggest writing the debated equations in the article or explaining the arguments without mentioning them.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
YZoGmJrOS9;"REVIEW 
Summary:
This work presents an analysis of in-context learning (ICL) for a variety of hybrid architectures (composed of different blocks from preexisting large language model architectures) on different regression tasks. The experiments are built on top of a couple of prior works [1, 2] that also explored ICL in similar contexts. This paper highlights that several prior results can be reproduced, and for novel hybrid architectures which are the main focus of this work – most of them converge to optimal solutions while some others can escape suboptimal solutions or even fail to converge in the first place. The authors also propose a new metric “ICL regression score” to evaluate ICL performance in comparison to a known baseline. The modularized code for this work is publicly available for the broader scientific community.


[1] Garg S, Tsipras D, Liang PS, Valiant G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems. 2022.
[2] Park J, Park J, Xiong Z, Lee N, Cho J, Oymak S, Lee K, Papailiopoulos D. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248. 2024.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The open source codebase with simple abstractions and interfaces to facilitate reproducibility, extensions, and modifications are a welcome contribution.
- The intuitive explanation behind the ICL regression score values in Figure 2(a) are well-appreciated and helpful to follow along the results.
- The authors evaluate multiple architectures and tasks and clearly outline what components they are using from prior works to build on top of.

Weaknesses:
- The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did? 
- Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature. 
- The technical novelty of the work is limited.
- The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of. 
- The authors mention and describe a 6th task Vector MQAR but do not report or discuss any of its results in detail in the main text of the paper. One figure is present in the Appendix but it is not explained and it is too hard to read the text in the figure.
- Some typos:
     - Line 161: Mention the word “**Figure**” before 2a.
     - Line 164: “**Figure 2b**” instead of “Table 2b”
     - Line 185: Park et al. [14] **show** that ..
     - Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..”
- References to result tables (Table 3 for lines 213, 230)  and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding.

Limitations:
In terms of negative societal impacts, could the potential misuse of natural language tasks via the hybrid architectures presented be a legitimate concern? It is certainly out of scope of this work for evaluation but could be listed as part of the broader impacts section of the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors explore a number of different attention-based architectures along with Mamba on a series of in-context regression tasks. The architectures vary in their choice of normalization, positional encodings, activations, and hybridization with Mamba. They discover some varying capacities for the different ICL tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper appears to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), and I'm uncertain what additional scientific value it adds. As such, it's originality and significance appear to be limited.

Weaknesses:
As mentioned above, the present work seems to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them.

The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?

A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. 

Additional minor formatting comments:
- Line 25: consider compressing citations (e.g. with sort&compress)
- Table 1: third hline from the top intersects with text
- Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall
- For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style
- Figure 15: text is unreadable
- Figure text overall is small and hard to read

Limitations:
The authors adequately state the limitations of their approach.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors build on the ICL work of [1] and [2], wherein networks are ""trained to in-context learn"" several tasks of varying complexity (e.g., Linear Regression, Sparse Linear Regression, Vector MQAR, etc.).  Carrying on from [2], the main contribution of the presented work is the combination of various permutations of the attention/SSM blocks from GPT-2, LLama-2, and Mamba models.  Such permutations include swapping the Layer Norm in GPT-2 attention blocks with RMSNorm, the LLama-2 SwigGLU with a Mamba Mixer, and so on; in total, 9 different hybrid combinations are considered.  For each task and specific hybrid model, the model is trained over task samples for 500k steps, then evaluated on ICL performance on that task.  In addition to reporting squared error per task, the authors also propose a new metric, called the ""ICL regression score.""  The results of specific model-task pairs are plotted and several trends are discussed.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.
[2] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
**Originality** - while the authors largely adapt the ""trained to in-context learn"" framework from other works, they consider new model configurations.  This can be useful to determine what permutations of model components leads to failure modes in ICL.

**Quality** - In considering 9 different model configurations for 8 different tasks, the author thoroughly consider a large number (72) of different LLM building blocks and their ICL capabilities across tasks of varying complexities.

**Significance** - The framework of [1] has grown as an interesting alternative to standard ICL, as a quick means to assess the ICL success and failure modes of different models.  Thus, the presented work can aid in flushing out this information over new LLMs and previously proposed tasks.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.

Weaknesses:
### Clarity
There is significant room to improve the clarity of the presented work.  Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read).  It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted; the question of ""why"" remains for all the presented results.  E.g.,
> Specific hybrid architectures can hesitate to learn/converge for certain function classes.

Do the authors have intuition or an explanation which can be explored to explain this?  Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way.

Wrt important missing information:
- ""We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of ""difficulty"" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)."" <- - How training instances are produced per task? How many test samples are produced per task?  If this follows Garg et al., then each model is trained *from scratch* on 40 samples per task. Can you please clarify and state these in the main text?
- It is not clear	what the author's mean by ""zero estimator.""  Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents.
- ""To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14].""  <- It would help to clearly state the paper trains the models ""from scratch"" to in-context learn, as in previous works.

Limitations:
The did well to state limitations of the presented evaluation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a codebase for benchmarking the in-context learning ability of language models, especially for hybrid models. In addition, several empirical results are presented to show that some model architectures fail entirely or have suboptimal performance on specific in-context learning tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The codebase for studying in-context learning ability could be useful to understand capabilities and limitations of hybrid models, which will accelerate research in this area.
* It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that.

Weaknesses:
* I feel that it is hard to assess the contribution of this work. It seems that this work's main contribution is the implementation of the in-context learning ability benchmark codebase. While such a codebase is important and useful, I did not find what technical challenges the codebase is trying to address and the effectiveness of the codebase.
* Another contribution is the empirical findings of the relationship between architectures and per-task performance on in-context learning. However, I found the empirical results are not systematic and hard to interpret. I am unsure how these findings motivate future architecture design.

Limitations:
see above

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
RzlCqnncQv;"REVIEW 
Summary:
The paper presents an approach that leverages LLMs to generate PDDL domain and problem files from natural language descriptions, and refine them iteratively based on environment interactions. In particular, it proposes an Exploration Walk (EW) metric that provides feedback signals to guide the iterative refinement process. In experiments, the proposed method successfully recovers PDDL files in 7 of 10 domains, outperforming LLM-based planners.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper studies the important problem of learning PDDL domains from embodied interaction for classical planning. This is a promising direction to enable long-horizon planning with formal guarantee. You may also find this recent paper [1] highly relevant.
- In contrast to existing work that require human intervention, the paper boldly attempts to generate PDDL files automatically using feedabck from environment interaction. The proposed EW metric does make some sense to me.
- In experiments, the method seems to be fairly capable in recovering valid PDDL files.

[1] Han, Muzhi, Yifeng Zhu, Song-Chun Zhu, Ying Nian Wu, and Yuke Zhu. ""InterPreT: Interactive Predicate Learning from Language Feedback for Generalizable Task Planning."" RSS 2024.

Weaknesses:
- My major concern is on the major contribution of the paper - the automatic mechanism that iteratively refines the generated PDDL files.
  - **EW does not provide a sufficient objective.** The paper presents an Exploration Walk (EW) metric to provide feedback for the refinement process. The metric measures the difference of generated PDDL domain and ground truth environment by the feasible action sequences within. While EW=1 is the necessary condition for the generated domain to be valid, it is not the sufficient condition. I agree that EW can provide guidance at the initial stage, but in the end the objective in Equation (1) should be the one to optimize to produce a valid domain.
  - **The exact feedback not explained enough.** The paper seems not to elaborate on the form of feedback provided to LLM for refinement - given the EW score which is a number. While the authors mention this briefly in the Appedix, I'm still don't fully understand how it works exactly. As this is the key part that makes the proposed approach possible, I would suggest the authors to provide more details in the main paper
  - **The effectiveness of scalar-based feedback is doubtable.** Also, given the feedback is a number that provides little information on what the exact issue is (whether it's on problem file or domain file, whether it's on a precondition / effect term or on the predicate design, and which line), I doubt whether the LLM can perform reasonable refinement. I think it's highly possible that the iterative refinement process will goto nowhere.
- Another important doubt is on the problem setup - where the natual language descriptions are translated line-by-line from the ground-truth PDDL files. 
  - **This setup is foundamentally different from what the problem of ""generating PDDL"" should be.** Under this setup, the challenge is no long generating PDDL files that requires exploiting environment interactions, but **translating** natural language into PDDL precisely without losing any information. More specifically, I believe the difficulty is to identify important predicates. Once the predicates are ready, the precondition & effect terms and initial & goal states should be relatively simple to be translated with GPT-4 with some prompt engineering.
  - **The proposed approach seems to be unaligned with the challenge**. While I think utilizing something similar to EW metric is the way to go, it doesn't aligns well with the challenge posed by this problem setup.

Limitations:
See Weakness and Questions

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work talks about generating PDDL domain and problem files with LLMs. Specifically, it improves existing frameworks, particularly Guan et al. [8], in terms of increasing the degree of automation & eliminating the need for human corrective feedback. The core contribution of this work is the EW score. To compute the score, it only requires access to a set of executable plans and an executability checker (which can be either a simulator or the actual environment). The EW score is used to select sampled domain models given by the LLMs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written, with precise and rigorous wording and formalism.
- The attempt to reduce the need for human feedback in domain generation is a meaningful and useful step forward. 
- The introduction of the EW score not only forms the foundation of this work but also holds potential for future applications/research (e.g., for evaluation or as a heuristic)

Weaknesses:
1. The domain model sampling/generation is done in a relatively simple way. The feedback message could be more informative than just indicating the inexecutable step or action in a plan.

2. Regarding the structure of the related work section, the distinction between ""intrinsic reasoning"" and  ""external reasoning"" seems unclear to me, especially given that ""with the assistance of basic external tools [14]"" is mentioned under the ""intrinsic reasoning"" subsection. Also, even for the task of PDDL generation,  certain degree of ""intrinsic reasoning"" is needed. Rather than saying ""intrinsic reasoning"", I guess the authors likely meant ""direct plan generation.""

3. It doesn’t make it clear how much knowledge is provided in the domain NL description. Clarifying on this helps readers understand whether this work leverages LLMs as a knowledge source or a ""translator."" -- from the examples in appendix, it seems that LLMs are used as the latter in this work (note that I am not saying translation is trivial)

4. While the EM score may be effective for selecting candidate domain models and guiding their generation, its suitability as an evaluation metric is questionable. We know there exists a ""ground-truth"" PDDL and our goal is to fully recover its functionality. This is a binary 0/1 problem. This is not like a generated model with 0.8 avg solve rate is more usable than a model with 0.2 avg solve rate.

5. Also, I think it's important to mention that the avg solve rate can only serve as an approximate measure of the equivalency between two domain models. A 100% avg. solve rate doesn't guarantee model equivalency (but this seems to be an easier-to-compute measure)

6. Assumption 2 is stated in a loose way. An NL description of a domain can be given at different degrees of detail (which correspond to different levels of difficulty in domain PDDL generation).

7. Line 211: I think it's better to say ""as long as PDDL is expressive enough to capture the dynamic / working mechanisms of the environment"" rather than ""the env supports PDDL action interface.""

8. It's unclear what the takeaway of Sec. 4.2 should be. Firstly, ""plan-not-found"" only accounts for a certain fraction of consequences caused by removing a term or predicate. Other consequences, such as producing invalid plans, can also occur. Secondly, it is well known that obtaining a valid domain model is challenging, even for humans. The authors should better explain the connection between Sec. 4.2 and the other parts of the paper.

9. The authors should give more information on the computational complexity/cost (e.g., time consumption) associated with the calculation of EW score per candidate model.

------------------------
Overall, I find this manuscript well-written, and the idea can be valuable to the community. Therefore, I am leaning towards recommending acceptance.

Limitations:
See the weakness section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an approach to leverage LLMs and environment feedback to automatically generate PDDL domain and problem description files without human intervention.  They do so by an iterative refinement approach that generates multiple PDDL problem and domain candidates based on feedback obtained from the environment.  The authors show their approach works experimentally in 66% of 10 PDDL domains that they have tried.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem addressed in the paper is an important and interesting problem. The proposed approach with regard to the EW metric is novel and promising.

Weaknesses:
Assumptions: Assumption 2 may not be realistic. Often times, people may not know exactly what is the right way to capture the domain knowledge, that is what kind of things they should have said to ensure the pre/effects/initial state all are captured. What about the case that the domain description is missing a constraint or precondition?

Environment requirement: in regard to the applicability of the work, there is a dependency on the environment to do the refinement, and that also may limit the impact of the proposed solution as the environment may not always be available for all domains. 


Novelty: the authors claim to be the first to enable PDDL generation using LLM without human intervention. However, there exists at least two related work that does also generate PDDL domain and problem without human intervention: 

1. Large Language Models as Planning Domain Generators ICAPS 2024 (https://github.com/IBM/NL2PDDL)

2. AUTOPLANBENCH: Automatically generating benchmarks for LLM planners from PDDL PRL 2024 (https://github.
com/minecraft-saar/autoplanbench/tree/main.)



The paper presentation can be improved. See the question section.

Limitations:
Assumption 2 may be limiting the scope and the level of impact of the work.  Also what about cases that the ground truth domain/problem is not known which is in most cases when it comes to real applications. Also having to rely on the PDDL environment is also limiting the scope and impact of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents an approach for modeling planning environments via PDDL generation using LLMs and environment feedback, without relying on human intervention. This is achieved by an Exploration Walk (EW) metric to measure domain similarity and guide domain refinement, and an iterative rectifying method that leverages LLMs to generate and refine PDDL domain and problem files. The evaluation of this method is performed alongside baselines on ten different standard planning domains from IPC.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The presentation of the work is quite clear and concise. 
2. The exploration walk method included in the PDDL file correction loop is a rather simple and unsophisticated way to obtain approximate scores for the domain generation process.
3. A decent amount of experimentation and analysis have been performed and stated in the work.

Weaknesses:
1. Section 4.2 - demonstration of the brittleness of PDDL generation can be made more realistic such as additionally including hallucinated object identifiers or actions or symbols, which are highly probable with LLM-based code format generators.
2. The PDDL generation with LLMs approach is not as novel and the exploration walk/ environment feedback approach may not be as useful in generating completely new domains from descriptions or making custom modifications to existing domain files. The method does not really differ for rectifying problem files.
3. There are intrinsic problems with generating domain files from descriptions for domains such as Barman - where the levels of shaker and actions such as clean shot, empty shot do not translate well for LLM-based generation. More analysis and description in this line of argument are necessary.

Limitations:
Limitations have been addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
5atraF1tbg;"REVIEW 
Summary:
The paper proposes a method to estimate a lower bound on the (pure) Differential Privacy parameter $\epsilon$ for already trained machine learning models using a post-hoc empirical evaluation based on conducting Membership Inference Attacks. Building on the analysis by Steinke et al., 2023, for auditing DP with $O(1)$ training runs, this paper tweaks the setting by using synthetically generated canary data points that closely resemble the training data population, rather than inserting or omitting pathologically crafted canaries that degrade the ML model's performance. This approach is useful because if the canary distribution and the training distribution are (nearly) identical, an already trained model can be considered as the output of the training algorithm on a random partition of the combined training and canary datasets. This means any post-hoc auditing analysis based on membership inference on a randomly and independently selected training dataset yields a valid lower bound on $\epsilon$. To get such a canary distribution, the authors suggest training a synthetic data generation model to create a canary dataset to get a distribution similar to the population. Additionally, the authors extend Steinke et al.'s analysis to situations where the synthetic data distribution is $c$-close to the true population in a DP-like divergence, although the estimator does not technically yield a lower bound in this case. Through extensive empirical evaluations, the paper demonstrates that the proposed estimation technique is useful as it provides reasonable values that approximate the DP lower bounds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper focuses on an important problem and has the following merits.

- The paper proposes an auditing method that seeks to eliminate the need of altering anything about the training process of ML models, thus allowing post-hoc auditing. 

- The paper proposes using synthetic data that resemble the training data instead of adversarially created canaries can reduce the drop in model performance while offering useful DP estimations.

- Empirical evaluations suggest that the estimator correlates with the DP upper bounds and can help identify situations of high privacy leakage.

- The appendix provides an attempt to extend the budget estimator for $(\epsilon, \delta)$-DP.

Weaknesses:
- I'm not entirely certain that the analysis presented in Proposition 2 is correct. In the proof of Proposition 2 (lines 433 and 434), the authors use the fact that the model $f$ is $\epsilon$-DP with respect to the Bernoulli random variables $S$. But in Algorithm 1, we see that $f$ was trained on $D_{f}$ in phase 1, which is independent of $S$ sampled in phase 2. So, the model $f$ is independent of the selection $S$ (i.e., $f \bot S$). In other words, the equation after line 434 should evaluate to 1, as given $x_i$, the prediction $f(x_i)$ remains the same whether $S_i = 0$ or $S_i = 1$. Perhaps Algorithm 1 requires one training run like Steinke et al. where the model $f$ is trained on $X$ as described in line 2 of Algorithm 1? If that is the case, then the claim that the auditing does not require retraining becomes invalid. If I'm mistaken, could you explain Proposition 2 further?

- In line 196, the authors mention that Algorithm 1 (lines 4-7) does a sweep over all the recall values of the attack, and they adjust the overall significance-level or the $p$-value by taking a union bound over all equation (1) with the significance level discounted to $\beta \leftarrow \beta / m$. The value of $m$ in the experiments ranges from 500 to 30,000. For such values,  assuming $\beta \leq 0.05$, the sweeping over all recall values requires $\sqrt{\log(m/\beta)} \approx 2.4$ whereas when the level of recall is predetermined, the factor in equation (1) is only $\sqrt{\log(1/\beta)} \approx 1.15$. So, I'm not sure if doing a sweep over recall values will give larger DP estimate values.

- Use of synthetic data from a distribution that is $c$-close (with a small $c$) can make it difficult for the membership inference attacks to work well. If the goal is auditing DP, perhaps the higher MIA precision for a given recall outweighs the drop in ML model's performance. 

- The figures haven't been explained very well. In particular, the theoretical maximum precision in Figure 3 and Figure 12 and the empirical maximum value in Figure 4 aren't clear.

- The paper only studies the problem of estimating pure DP and does not present an operationalizable algorithm for $(\epsilon, \delta)$-DP, although some results along these lines are motivated in the appendix. Additionally, the obvious weakness (which the authors acknowledge) is that their method does not technically provide a lower bound for $\epsilon$-DP.

### Minor Points

- Intuitively, when $\mathcal{G} = \mathcal{D}$ or in the Real-Member;Real-Nonmember (RM;RN) case where the non-members follow the same distribution, I think the no-retraining-needed argument could work. This would involve modifying Algorithm 1 and reworking Proposition 2 by (A) assuming entries in $D_f$ and $D_G$ are i.i.d. from the same distribution and (B) setting $S$ according to the original train-test split instead. On the other hand, when $\mathcal{G} \neq \mathcal{D}$, I'm not sure if such an argument can be made to work, at least not trivially.

- It's not clear how the mechanism $B(S, X) = \\{b(x_1), \cdots, b(x_m)\\}$ in Proposition 1 incorporates the helper model mentioned in Section 5.1 used in the experiments.

- Algorithm 1 (lines 4-7) does not seem to reflect the $p$-value adjustment discussed in lines 198-200. Perhaps the authors might have overlooked this $p$-value adjustment in the experiments as well?

Limitations:
The authors have acknowledged some limitations in the paper. There do not appear to be any negative societal impacts associated with this work. I encourage the authors to address the issues and questions raised in this review.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel privacy auditing procedure called PANORAMIA. The method works with a single model that uses all available training data, with no modifications to the training procedure, and with only access to a subset of the training data. This is achieved by synthetically generating non-member samples for auditing. The method produces an estimate of the $\epsilon$ differential privacy parameter, which, however, is not necessarily a lower bound.

Summary of the method given a model and its training data:
1. Train a generative model on a subset of the training data.
2. Use the remaining training samples as ""members"" for auditing.
3. Generate an equal amount of ""non-members"".
4. Split both into training and test sets.
5. Fit a baseline classifier that distinguishes synthetic and real data on the training set (only input are samples).
6. Fit a membership inference classifier on the training set (inputs are samples and model statistics).
7. Use predictions on the test set and hypothesis testing to obtain a confidence interval for quantities of interest.

The hypothesis is of the form ""The generative model is $c$-close to the true data distribution and the training procedure is $\epsilon$-DP"", where $c$ is a distance measure defined in this paper. The baseline predictions are used to reject the first claim, and the membership predictions to reject the second one. Hence, the returned $\epsilon$ is a lower bound on the true $\epsilon$ only if the generative model is indeed at least $c$-close to the data distribution. The theoretical analysis relies on results from the (1) procedure by [Steinke et al., 2023](https://openreview.net/forum?id=f38EY21lBw) but adapts them to the relaxed setting in this paper.

For evaluation, the paper applies the new auditing procedure to various CNNs with and without DP guarantees on image data, small GPT2 models on text, and on tabular data. As baselines, the evaluation uses the (1) method by [Steinke et al., 2023](https://openreview.net/forum?id=f38EY21lBw) and real instead of generated members. PANORAMIA does not outperform the baselines, but achieves reasonably close results on image and text data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper considers a practical and relevant setting for privacy auditing. In particular, training the model to audit using all training data is a big benefit: otherwise, the audit either uses a slightly different setting that uses fewer training samples, or one has to forfeit a potentially large amount of training data (which hurts utility). Additionally, the procedure works even when the auditor knows only part of the training data, which can be relevant (e.g., federated learning).
- The experiments are broad (considering both DP and non-DP training, and different degrees of overfitting), sound, and the conclusions are convincing. Although the results are weaker than for existing methods, those existing methods use a less appealing setting. A particularly convincing result is that PANORAMIA with synthetic non-members often comes close to the same method but using real non-member samples.
- The paper is overall structured well and the auditing procedure is laid-out clearly. The authors manage to explain the auditing results well, despite their complex interpretation. I also appreciated that Algorithm 1 collects all parts of the method into one place, including the calculations to obtain confidence intervals.

Weaknesses:
- The considered privacy semantics are partially misleading and could be made more clear.
	- The presented auditing game is incomplete; Definition 2 only describes how samples are selected but should also include the goal and actions of the adversary.
	- The paper states (as a benefit) that PANORAMIA audits a target model, not the algorithm (L46--47). However, the results of the procedure are DP parameters, and DP is always a property of an algorithm (not model). If the considered semantics are meant to be different from DP, they should be made more clear and discussed.
	- Similarly, the paper claims that PANORAMIA does not require worst-case canaries because it audits the target dataset (via a model). However, this ignores that certain samples in the training data might be more vulnerable; hence, auditing should still consider the worst-case sample *in the training data* (especially since privacy is averaged over the dataset). This might be an explanation why the reported ""lower bounds on $\epsilon$"" are still far from tight in Figure 6c (as is the prior (1) procedure in a white-box setting).
- Relatedly, the definition of c-closeness (Definition 3) might not require a generator to capture the tail of the data distribution. However, this tail contains outliers that are often particularly vulnerable to privacy leakage. Hence, ignoring such outliers might significantly underestimate the $\epsilon$ lower bound. I would have appreciated a more thorough discussion of this limitation (or a clarification).
- The paper could be more explicit about whether PANORAMIA is intended to be a procedure to be used in practice, or an important stepping stone towards more practical procedures. Right now, PANORAMIA achieves worse results than the existing (1) method in a white-box setting, and requires training of a strong synthetic data generator (which might be non-trivial). Nevertheless, I believe this paper is still relevant from a conceptual perspective as a path for future work.
- There are minor notation issues in Proposition 1 and 2: the statement mentions only $T$ and $t$, but the inequality uses $T^b$ and $t^b$ (or $T^a$ and $t^a$). Additionally, the left-hand sides both take probabilities over $T^b$/$T^a$ but simultaneously condition on the respective random variable. Fixing those points makes the propositions clearer. Also, the last sentence on L148 seems misplaced.

Limitations:
The authors are very transparent about all limitations of their work and discuss them transparently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel way for privacy auditing of machine learning models through MIAs. The framework they propose, panoramia, aims to audit the privacy of a ML model post-hoc (so with no control over the training procedure), and with access to a known member subdataset. The method first consists of training a generative model on the known subset of members, which is used to then generate synthetic data samples from the same distribution. These synthetic points are then used as non-members, which combined with the known member dataset allows to fit and apply an MIA on the target model. Importantly, authors recognize that there might be a distribution shift between members and synthetic non-members, so they fit a classifier without leveraging the target model as a baseline. 

Next, they use the difference between the MIA performance (thus using the target model) and this baseline to estimate the privacy loss. Authors provide the formula (and add proof in appendix) to compute a value of epsilon approximating a lower bound on epsilon-DP. 

They further apply the privacy auditing to three kinds of ML models (image classification, language modeling, and tabular data classification). Authors consider models with varying degree of overfitting, DP training, and increasing the amount of member data available to the auditor.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Originality: The paper introduces a way to audit the privacy of ML models post-hoc, without any control of the training data, which is novel. They cite and position themselves correctly to relevant prior work such as Steinke et al. 
- Quality: The proposed method is technically interesting, formally supported and evaluated extensively across data/model modalities. 
- Clarity: NA
- Significance: The paper proposes a way to compute an approximation for the lower bound on epsilon, to audit ML models post-hoc, which is technically interesting.

Weaknesses:
- Originality: Authors should include an appropriate related work section, touching on other privacy auditing techniques and potentially other (post-hoc) membership inference attacks. 
- Quality: The proposed method strongly depends on the quality of a generator and the baseline MIA, the impact and limitations of which can be further explored (see questions/limitations). 
- Clarity: I find that the paper's clarity can be improved significantly. Especially the results section (tables, text) is quite notation heavy and hard to follow what everything refers to. 
- Significance: While technically interesting, the relevance of a technique to compute a proxy lower bound privacy loss in practice needs more compelling motivation (see questions).

Limitations:
Authors are very clear that their method only provides a proxy for the lower bound of epsilon and thus carefully caveat their method. However, can authors elaborate on the limitations associated with the development of both a generator and a baseline, both of which seem fundamental for panoramia. Currently, their implementation seems rather ad-hoc than adequately discussed for wider applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a privacy auditing technique that utilizes partial access to member data to generate synthetic non-member data, which in-turn is used to train a meta-classifier that can be used to empirically measure privacy leakage relating to record membership. The authors evaluate their technique on models as large as GPT2 and find correlation between audit scores and the expected leakage from models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- L226-228: Good to include negative results!
- Having access to non-members is often taken for granted (especially when the auditor is also the model trainer), but in most third-party cases getting good-quality non-member data that is not significantly different from member distribution is hard. The methods proposed in this work can be useful there, with a generator and discriminator that can help ensure any differences in members and non-members (and subsequent MIA performance) do not arise from distributional differences. 
- The paper is well written and structured, and experiments are thorough, spanning quite a lot of models and modalities.

Weaknesses:
- As an auditor, access to validation/test data used in model training would not be a far stretch- why not use the non-member set as used in standard MIA setup (validation/test data). What is the added benefit from this extra step of generating synthetic non-members?

- Regarding the contributions, (1) and (2) has already been explored by [1, 2]. While the method in [1] does not currently support large models/datasets like CIFAR10, it would be nice to highlight differences here. If you indeed have knowledge of a decent-chunk size of members and non-members, I'd imagine you could do something better like [2], and other related methods.

- I have some concerns over the dependency on how well the baseline in/out distribution detection system works. As a concrete example consider Figure 7(a, b) - even as a human I see a very clear difference in resolution of the generated images and find it hard to believe that the distinguisher does not work well here. Even a non-ML technique (that can work around with blurring) would work pretty well here. 

- Table 1: Accuracy of models here is not good enough, especially when using entire data, with very clear signs of overfitting. If using the entire data (and not half split, as in most MIA evaluations [3, 4] where even with half the dataset test accuracy is ~92%), should be able to train well-performing models that do not overfit so heavily. Please see [this resource](https://paperswithcode.com/sota/image-classification-on-cifar-10)

- Figure 1: While the trends suggest that the proposed audit correlates with actual leakage, one could also argue that (100 - test accuracy) is also a useful privacy metric in this comparison given the correlation. The utility of the audit would be more apparent, then, when studying models that have **comparable** test performance, but have (by design) different leakage, perhaps focusing on moderate/large values of $\epsilon$ with Differential Privacy-training.

- L288-290: (1) and (2) are ambiguously enforced- if you have a large portion of train data, you can control process in most cases, and also obtain non-member data. As far as (3) is concerned, that is a compute-related constraint, not a threat model difference. The auditor could, for instance, use available data knowledge to train ""in"" models.

- L331-334: I do not find the justification behind having access to partial member data convincing. While I am okay with just stating that ""this is a possible limitation, but okay to assume for an auditor"", but relating it to how it could be useful in situations like FL is not practical. For instance, here empirical experiments use more than 1/5th of the data; no FL training will have just 5 participants. This is closer to a specific case of distributed learning, or pure data aggregation. Even in such cases, the data distributions per client will be considerably different, as opposed to the experiments here which have uniform samples. 

## Minor comments

- Please consider a more descriptive abstract. ""scheme"" here is not very descriptive
- L121: ""We formalize a privacy game"" - this is standard membership inference evaluation and not a contribution of this work. Please rephrase to make this clear.
- L493-494: Please provide direct distinguishing accuracies of these baseline classifiers for reference and easier understanding.

### References
- [1] Suri, Anshuman, Xiao Zhang, and David Evans. ""Do Parameters Reveal More than Loss for Membership Inference?."" High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning.
- [2] Nasr, Milad, Reza Shokri, and Amir Houmansadr. ""Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning."" 2019 IEEE symposium on security and privacy (SP). IEEE, 2019.
- [3] Carlini, Nicholas, et al. ""Membership inference attacks from first principles."" 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 2022.
- [4] Zarifzadeh, Sajjad, Philippe Cheng-Jie Marc Liu, and Reza Shokri. ""Low-cost high-power membership inference by boosting relativity."" (2023).

Limitations:
There are a few statements that currently serve as justification for limitations (see above) but should just be posed directly as base assumptions. Apart from that (and some potential shortcomings in models used for evaluation), most limitations are already stated in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
7t6aq0Fa9D;"REVIEW 
Summary:
The paper proposes FASTopic, a topic model using pre-trained document embeddings and embedding transport between documents and topics as well as words and topics. The minimized objective function is then a combination of the DSR and ETP which is optimized via finding topic, and word embeddings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Interesting idea which is well documented
- Initial experimental results show potential 
- potential benefit of getting interpretable word-embeddings while training topic model

Weaknesses:
- Overall, I quite like the paper and the idea. However, I have some reservations, primarily due to the aggressive tone of the presentation, starting from the abstract. The paper repeatedly criticizes comparable methods, while presenting its own method as the ultimate and superior topic model. Such claims need robust support through experimental evidence. Although the experiments provide some validation, the tone of the paper creates a discrepancy, making the experimental results appear insufficiently convincing.

  Overall, more results are needed to back up the strong claims made in the paper.

  Below are some suggestions that, in my opinion, would support the claims more effectively. If these suggestions are unjustified, I would appreciate an explanation.


  **Minor:** I think the high-resolution plots are causing some trouble. I cannot really scroll through the paper smoothly.


1. **Include further models** -> ETM [1], ProdLDA [2], CEDC [3], CTMneg [4]. While I wouldn't suspect ETM/ProdLDA to be better than any other of the models, the simple models CTMneg and CEDC have shown to outperform BERTopic.

2. **Incorporate other evaluation metrics**: since evaluation is very difficult [5]. E.g. use some presented in [3] or [6].

3. **How do you measure training time for all of the models?**
   - 3.1) Are all steps, including the document encodings, part of the taken training time?
   - 3.2) Do you use the same encoding model for all comparison models (where applicable)?
   - 3.3) Given that FASTopic outperforms BERTopic in terms of speed, I wonder for how many epochs you are training your model, and what it is that takes so long in BERTopic? The document encoding steps are the same for both models and while dimensionality reduction takes some time using UMAP, I have a hard time believing that it is slower than training FASTopic for a reasonable amount of epochs.

4. **How did you choose the number of topics for BERTopic?** Did you use Kmeans instead of HDBSCAN or hierarchicaly reduced the number of topics?

5. **How many parameters does FASTopic have compared to the other neural models?**


[1] Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in embedding spaces. Transactions of the Association for Computational Linguistics, 8, 439-453.

[2] Srivastava, A., & Sutton, C. (2017). Autoencoding variational inference for topic models. arXiv preprint arXiv:1703.01488.

[3] Adhya, S., Lahiri, A., Sanyal, D. K., & Das, P. P. (2023). Improving contextualized topic models with negative sampling. arXiv preprint arXiv:2303.14951.

[4] Thielmann, A., Reuter, A., Seifert, Q., Bergherr, E., & Säfken, B. (2024). Topics in the haystack: Enhancing topic quality through corpus expansion. Computational Linguistics, 1-37.

[5] Hoyle, A., Goel, P., Hian-Cheong, A., Peskov, D., Boyd-Graber, J., & Resnik, P. (2021). Is automated topic model evaluation broken? the incoherence of coherence. Advances in neural information processing systems, 34, 2018-2033.

[6] Stammbach, D., Zouhar, V., Hoyle, A., Sachan, M., & Ash, E. (2023). Revisiting automated topic model evaluation with large language models. arXiv preprint arXiv:2305.12152.

Limitations:
Not needed

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a fast, adaptive, stable, and transferable (FAST) topic modeling paradigm by using dual semantic-relation reconstruction (DSR) to model topic-document and topic-word relations. It enhances topic modeling by incorporating a embedding transport plan (ETP) method to address relation biases. Experimental results show FASTopic outperforms existing baselines.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This work introduces FASTopic by integrating the DSR paradigm, which is straightforward and provides a fresh aspect on handling semantic relations in topic modeling. Authors further propose ETP method to avoid relation bias issue.
2. Comprehensive experiments show FASTopic's superiority in effectiveness, efficiency, and adaptability, stability, and transferability.
3. The paper is well-written, and the code is available.

Weaknesses:
1. The method's performance may heavily rely on the document embedding model, which makes it harder to interpret the model results. It need to futher discuss that how specific changes in embeddings affect topic modeling. 
2. The misplacement of tables and figures, such as Table~1, could potentially distract readers but do not detract significantly from the content's quality. But it does not matter.

Limitations:
No potential negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a fast, adaptive, stable, and transferable topic model, FASTopic. Instead of using the VAE or clustering method, it incorporates a new model structure named Dual Semantic-relation Reconstruction (DSR). DSR learns topics by directly optimizing the semantic relations among topics, documents, and words. The semantic relations are further regularized by an Embedding Transport Plan (ETP) method as an optimal transport problem. Experiments demonstrate the effectiveness of the proposed model.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and easy to follow
2. The proposed DSR framework is simple and neat.
3. The experiments and ablation studies demonstrate the effectiveness and efficiency of the proposed FASTopic method.

Weaknesses:
1. [76,64] should be compared as baselines in the experiments since they both incorporate optimal transport objectives in their model as FASTopic does.
2.  As far as I know, the time complexity of solving the optimal transmission problem is very high. Is there any technique used in FASTopic to efficiently derive a solution? Furthermore, it would be great to theoretically analyze the time complexity of the current FASTopic framework.
3. Is it fair to compare FASTopic, which takes document embeddings as input extracted by sentence-BERT, with other baselines in the experiments?
4. As LLM has shown great performances in many NLP tasks, I believe it is necessary to include a discussion about the reasons and advantages of developing a topic model that is not LLM-based.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author found that existing methods (VAE-based or clustering) suffer from low efficiency, poor quality of topic words, and instability. To address these issues, this paper proposes a novel topic modeling paradigm called Dual Semantic-Relation Reconstruction (DSR) for efficient modeling of semantic relations among three types of embeddings: document, topic, and word embeddings. The author attributes the low quality and instability of previous methods to relation bias issues, leading to repetitive topics and inaccurate document-topic distributions. To tackle this, the paper introduces the Embedding Transport Plan (ETP) to regularize relations among the three embeddings. Combined, DSR and ETP form the proposed topic model, FASTopic, which is evaluated on six benchmark datasets, showing encouraging performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is clearly presented and easy to follow, with an explanatory and easy-to-understand mathematical formulation.
2. The proposed DSR objective is effective, significantly reducing training time compared to VAE-based methods such as ECRTM and CombinedTM.
3. The experimental evaluation is extensive, showing that FASTopic consistently outperforms multiple strong baselines on topic coherence and topic diversity, while also demonstrating advantages in terms of running speed and transferability.
4. The paper demonstrates the model's robustness under multiple numbers of topics (K=75, 100, ..., 200).

Weaknesses:
1. Tables 6, 8, and 9 present the ablation study of FASTopic using ETP and parameterized softmax. ETP, compared to the ECR used in the previous VAE-based method ECRTM, adds semantic relations between topics and documents. The paper lacks proof the dual transmission effectiveness of ETP by not comparing it with ECR (which only includes semantic relations between topics and words) and another case that only includes semantic relations between documents and topics.
2. I am concerned that while the DSR training method improves efficiency compared to VAE-based methods, it may make it difficult to build meaningful relations between topic embeddings and document embeddings for some long-text corpora due to its singular objective.
3. It would be nice to showcase a comparison of topic words or transferability between FASTopic and ECRTM to demonstrate that FASTopic offers improvements in multiple dimensions, not just speed.

Limitations:
1. The proposed method relies on embedding transport semantic relations and may be limited by the max input length of pre-trained document embedding models.
2. See Weakness 3.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
iRHxp1ibFj;"REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Rllg9nOasE;"REVIEW 
Summary:
This paper introduces a novel approach using cross-modal and multi-modal models to align brain activity with naturalistic stimuli, evaluates several unimodal Transformer models, and examines the effects of removing unimodal features from multi-modal representations on brain alignment.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- (S1) The paper presents a thorough imaging data analysis, including a detailed description of the dataset, a comprehensive comparison of methods, and a clear summarization of results, which enhances the robustness and transparency of the study.

- (S2) The paper is well-written and clearly presented, making the complex methodologies and findings accessible and easy to understand for readers.

- (S3) The study provides a rigorous comparison of methods, including various method variations, which highlights the strengths and weaknesses of each approach and demonstrates the thoroughness of the analysis.

Weaknesses:
- (W1) Figure caption should be more demonstrative and detailed. For example for Figure 1's captions, ""Residual analysis"" is too vague for the reader to understand the figure. For Figure 5, if the colorbar range from -0.5 to 0.5, then it does not represetn the percentage, but the proportion instead.

Limitations:
The Limitation section is properly included in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a framework for applying brain encoding models with multimodal stimuli. They apply this to a series of video, audio, and mutlimodal models (cross-modal and jointly embedded models). They introduce a residual analysis to analyze the impact each particular feature had on the corresponding fit in the encoding model. They find that multimodal models significantly out-perform their unimodal counterparts on certain language- and vision-related regions in their fMRI dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Incorporating a new collection of models used for fits to the brain for comparison.
* Expanding to multimodal models, a relatively new space.
* Incorporation of video/speech models, allowing to capture input stimuli over time and removing problems with parsing the stimuli into individual modalities such as ImageBind or VideoMAE.
* Interesting results as seen in Figure 3 showing improvement across several brain regions with multimodal networks. Figure 2 also shows really interesting results with language and visual regions separated.

Weaknesses:
* Clarification on feature removal: I think I found the feature removal description in this paper and prior papers a bit confusing and want to ask for some clarification. I wish more space was spent on that in this paper to provide more intuition. I think some extra descriptions would be useful here. See questions.
* In general, I am quite skeptical of how well the feature removal works. For example, there is no guarantee that the features are completely removed in the residual analysis. I would actually like to see a probing analysis to actually establish that the feature is removed. 
    * Furthermore, the method of projection is rather confusing. The authors use a regression to “project” unimodal video features (referring to figure 1) into the same space as the multimodal feature space. I don’t think this is necessarily wrong but potentially unreliable without any extra metric to establish how well this works. Having some MSE score or pearson correlation (with the averaged embedding) could help understand how well the projection worked. 
    * In my opinion, I wonder why the opposite direction wasn’t taken: instead, project the video features out of the cross-modal/jointly pretrained multimodal representation. You could train a projection matrix to do so using your current vision-language data. To me, this is cleaner and easier to interpret primarily because you aren’t dependent on the quality of your visual representations to capture visual information. 
* The paper compares multimodal and unimodal models to demonstrate improvement in brain alignment. One explanation for this improvement could be an improvement in unimodal processing. For example, one interpretation of the current results is that a multimodal model such as TVLT has better visual processing than ViT-B (as an example).  Is this addressed by feature removal? I’m not sure it is. Some extra text to discuss this would be useful. Some extra discussion on model performance would also be useful. 
* Baselines
    * The paper doesn’t consider the baseline comparison with randomly initialized models. Why? I think this is a very important baseline for characterizing architectural bias. This was also done in prior works.

Limitations:
* I believe these are addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript investigated the process of multi-modal information in human brains through predicting neural responses based on semantic features extracted by existing models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is interesting.
2. The results show insights into brain region's roles in processing multi-modal information.

Weaknesses:
## Major
1. The method builds ridge regression based on features extracted by pretrained models. However, I am worried that the findings will be affected by choice of pretrained models. It is important to demonstrate the replication of different pretrained models.
2. For some observations in Section 6, the author only presents the observations and does not give insights based on the observations. For example,
    - What does observation i) in lines 311-312 indicate?
    - What does observation ii) in lines 313-314 indicate?
    - Why is AC an exception for observation (1) in lines 316-317?
    - For observation (2) in lines 320 -322, why is TVLT different from IB-concat, given that both of them contains multi-modal information?
3. Why does the author choose ridge regression instead of more complex machine learning models? Is it possible that more intricate interactions of features extracted by pre-trained models are not captured by a ridge regression model, potentially affecting the results? And if you choose a more complex model, the rank of alignment scores of different models could be altered.
4. I do not know if it is too hard or even impossible, but it would be better to check if the results consistent with some existing neuroscientific findings.
5. In section 6.3, why do IB-concat and TVLT act differently given that they are both multi-modal representations.

## Minor
1. There seems to be a trailing 3 in Fig.3's caption.
2. The author moves the results of some brain regions in Figure 3 to the appendix due to the page limit. Since the author refers to those regions from the main text, it would be better to still include those regions in the main text in my opinion.

Limitations:
The authors have adequately addressed most limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses an important question of how accurately multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. The key challenge is how to integrate or separate the information from different sensory modalities. This work explored two types of models, ie cross-modal and joint pretrained models. Through extensive experiments, this paper found some things that are important to unveil the brain encoding principles, which are important to the AI community.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper writing good, and research problems are well explained.
The encoding pripline is clearly illustrated.  
Experimental designs are insightful.

Weaknesses:
- My major concern is about the train-test settings. There exist `clock’ (temporal) relationship which might lead to information leakage during inference. This paper did not mention how to advoid such an issue. 

- The data collection process should be blocked to aviod inter-data correlation, espeically for joint-modal training. The three settings mentioned in the paper do not really account for the speciality of brain signals.

Limitations:
See above comments

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
cS63YtJ49A;"REVIEW 
Summary:
The paper presents a novel approach titled ""AdaptiveDiffusion"" for accelerating diffusion models used in high-quality image and video synthesis. The core issue addressed is the high computational cost and latency associated with existing denoising techniques in diffusion models, which are typically based on step-by-step noise predictions.

**Contributions of the Paper:**
1. **Adaptive Diffusion Process**: The paper introduces AdaptiveDiffusion, which adaptively reduces the number of noise prediction steps during the denoising process. This is achieved by skipping steps where the potential redundancy is high, guided by the third-order latent difference that indicates stability between timesteps.

2. **Plug-and-Play Criterion**: A new criterion is proposed to decide whether to infer new noise predictions or reuse previous results based on the third-order difference distribution. This allows for an adaptive acceleration paradigm that is prompt-dependent.

3. **Extensive Experiments**: The method's effectiveness is demonstrated through extensive experiments on both image and video diffusion models. The results show significant speedups of 2 to 5 times on average in the denoising process without quality degradation.

4. **Error Analysis**: The paper provides a theoretical analysis of the upper bound of the error induced by the step-skipping strategy, ensuring that the quality of the final output is maintained.

5. **Adaptive Acceleration**: The approach is designed to be adaptive to different input prompts, offering a practical solution to the high computational costs of sequential denoising techniques.

6. **Generalization Capability**: AdaptiveDiffusion shows a strong generalization capability, being able to adapt to different models and tasks, including text-to-image, image-to-video, and text-to-video generation.

In summary, the paper offers a substantial advancement in efficient diffusion model acceleration, with the potential to enable real-time and interactive applications of diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
### Strengths Assessment of the Paper

#### Originality
The paper demonstrates a high degree of originality through the introduction of the AdaptiveDiffusion method, which offers a novel perspective on accelerating diffusion models. The approach创造性地 addresses the computational inefficiency inherent in traditional denoising techniques by adaptively reducing noise prediction steps. This innovation is not just a technical tweak but a strategic rethinking of the denoising process itself. The use of the third-order latent difference as a criterion for deciding when to skip steps is an ingenious way to balance efficiency and quality, which has not been explored in prior works.

#### Quality
The quality of the paper is reflected in its rigorous theoretical foundation and comprehensive empirical validation. The authors have provided a detailed error analysis to support their method's robustness, ensuring that the acceleration does not compromise the output quality. The experiments are thorough, covering a range of models and tasks, which substantiates the method's effectiveness and generalizability. The paper also discusses the relationship between different orders of latent differences and the optimal skipping path, which adds depth to the understanding of the proposed technique.

#### Clarity
The paper is well-structured, with a clear progression from the introduction of the problem to the explanation of the proposed solution, followed by a detailed methodology and extensive experimental results. The figures and tables are used effectively to illustrate the method and results, enhancing the readability and comprehension of the paper. The theoretical proofs and algorithm descriptions are presented in a manner that is accessible to readers with a background in the field.

#### Significance
The significance of this paper lies in its potential to transform the applicability of diffusion models. By significantly reducing the computational cost and latency, AdaptiveDiffusion opens up new possibilities for real-time and interactive applications of diffusion models, which are currently limited by their resource-intensive nature. The ability to tailor the denoising process to different prompts is also significant, as it allows for more flexible and responsive generative models that can cater to diverse content creation needs.

In summary, the paper is a substantial contribution to the field of generative modeling, offering a creative, high-quality, and clearly articulated solution to a pressing problem. Its significance extends beyond technical advancement, promising to enable new applications and use cases for diffusion models.

Weaknesses:
While the paper presents a compelling approach to accelerating diffusion models, there are areas where it could be further strengthened:

### Theoretical Depth
- **Assumption Limitations**: The paper relies on certain assumptions for its theoretical analysis, such as the Lipschitz continuity of the noise prediction model. It would be beneficial to discuss how violations of these assumptions might impact the method's effectiveness and under what conditions these assumptions hold true.

### Experimental Scope
- **Diversity of Models**: Although the method is tested on various tasks, the paper could benefit from testing on a broader range of diffusion models to further establish the generalizability of AdaptiveDiffusion.
- **Real-World Applications**: Demonstrating the method's effectiveness in real-world applications or use cases would provide additional context and significance to the work.

### Hyperparameter Sensitivity
- **Threshold δ and Cmax**: The paper discusses the impact of these hyperparameters on performance but could provide more guidance on how to select these values in practice, especially given their critical role in balancing speed and quality.

### Computational Complexity
- **Memory Usage**: While the method aims to reduce computational cost, it would be insightful to have a discussion on memory usage, especially since diffusion models can be memory-intensive.

### Long-Term Viability
- **Adaptability to Model Updates**: The paper could address how well AdaptiveDiffusion might adapt to future updates in diffusion model architectures or training regimes.

### Societal Impact Consideration
- **Ethical Considerations**: Although the paper does not explicitly discuss societal impacts, it would be beneficial to include a brief discussion on potential ethical considerations, especially given the generative capabilities of the models involved.

### Reproducibility
- **Code and Data Availability**: Ensuring that the code and data used for experiments are publicly available would greatly enhance the reproducibility of the results.

### Documentation
- **Algorithm Pseudocode**: Providing pseudocode or flowcharts for the algorithms could help readers better understand the step-skipping strategy and its integration into the overall process.

### Future Work
- **Extensions and Limitations**: While the paper outlines future directions, a more detailed discussion on the limitations and potential extensions of the current work would be valuable.

By addressing these points, the paper could provide a more comprehensive understanding of AdaptiveDiffusion's capabilities and limitations, setting the stage for further research and development in this area.

Limitations:
Based on the information provided and the typical guidelines of the NeurIPS Paper Checklist, it appears that the authors have made an effort to address limitations and societal impacts. However,  I offer general advice on how authors can improve their discussion of these topics:

1. **Clear Acknowledgment**: Authors should explicitly acknowledge the limitations of their work in the main text of the paper. This includes potential constraints on the generalizability of their findings, assumptions made, and any conditions under which the method may not perform as expected.

2. **Depth of Discussion**: While acknowledging limitations, authors should provide a thorough explanation of how these limitations might affect the results and the applicability of their method. This could include a discussion of how the method behaves under different conditions or with different types of data.

3. **Societal Impact Analysis**: Authors should consider the broader societal impacts of their work, including both positive and negative outcomes. This discussion should be grounded in the context of the work and consider potential misuse, ethical concerns, privacy issues, and fairness.

4. **Mitigation Strategies**: If there are potential negative societal impacts, authors should discuss possible mitigation strategies. This could involve suggesting guidelines for the responsible use of the technology, potential regulatory frameworks, or technical safeguards.

5. **Ethical Considerations**: The paper should include a section on ethical considerations, especially if the work involves generative models that could be used to create misleading or harmful content.

6. **Transparency**: Authors should be transparent about any potential conflicts of interest, funding sources, or affiliations that might influence the research or its interpretation.

7. **Openness to Feedback**: Authors should demonstrate a willingness to engage with the community for feedback on the societal impacts of their work and be open to adjusting their approach based on this feedback.

8. **Long-Term Vision**: While discussing limitations, authors could also provide a long-term vision for how they anticipate overcoming these limitations in future work.

Rating:
3: reject, not good enough

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
## Summary
* This paper propose a greedy approach that accelerate the Probability Flow ODE solvers for txt to image diffusion models. Empirical results on SD 1.5 and SD XL with mulitple solvers (DDIM, DPM, Euler) demonstrate the advantage of their approach over previous acceleration techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
## Strength
* The idea of prompt adaptative acceleration is interesting and promising. As previous solvers designed for more general PF-ODE are not adaptative to different prompts. It is quite natural for a prompt adaptative approach to obtain better result.
* The empirical advantage over previous accelerations is obvious, especially in terms of image quality such as PSNR and LPIPS.

Weaknesses:
## Weakness 
* A new trend in image generation diffusion model is the adoptation of Flow matching optimal transport (FM-OT) / Rectified flow (RF) (See Learning to Generate and Transfer Data with Rectified Flow). Those lines of works adopt a forward SDE that is neither VP nor VE. Their special SDE has a constant velocity in the corresponding PF ODE. Rectified flow can even achieve single step sampling with this PF ODE. Their PF-ODE has a path that is very close to straight line and be solved with less steps compared with VP / VE SDE. The latest Stable Diffusion 3 already adopt this type of diffusion. Those efforts in diffusion community also speed up sampling, not by solvers but by different models. Those line of works should be discussed as they have the same goal with this paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a strategy to speed up image and video diffusion generative models. The speed-up is achieved by skipping denoising steps. The authors suggest implementing an adaptive skipping schedule, where the decision of which steps to skip depends on the processed image or video. Specifically, the proposed algorithm calculates the norm of the third-order derivative in the latent space. It then skips denoising steps when the norm of the third-order derivative is below a predefined threshold while limiting the number of consecutive skips to a set maximum.

The authors evaluate the proposed skipping strategy through various experiments with image and video diffusion models, comparing the speed-up and quality of the generated content with DeepCache [1] and Adaptive DPM-Solver [2]. In most experiments, the proposed skipping strategy achieves higher speed-up and better image and video quality than competitors.

[1] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024
[2] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed skipping strategy leverages the generated content to speed up image and video diffusion generative models through an adaptive skipping scheme. In experiments with image and video generation, the authors demonstrate that this adaptive skipping scheme can achieve higher speed and better quality than competitors. In my opinion, the concept of using an adaptive skipping scheme to speed up diffusion generative models is potentially valuable and could be of interest to the research community.

Weaknesses:
The choice of employing the norm of the third-order derivative as a criterion for skipping denoising steps was made empirically. Theorem 1 (Equation 3) explains why it makes sense to consider the first-order derivative as a criterion for skipping. However, the authors empirically demonstrate that the first-order and second-order derivatives barely correlate with an optimal skipping schedule found by a greedy search. There is a lack of mathematical justification for choosing the third-order derivative.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To enhance the sampling speed in diffusion models, this paper introduces the AdaptiveDiffusion framework, which utilizes a skipping strategy. Specifically, this strategy is guided by the third-order latent difference, assessing the stability between timesteps throughout the denoising process. Experiments results on image and video diffusion models demonstrate the superiority of the proposed adaptively sampling framework.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The motivation is reasonable: to accelerate the sampling speed by reducing redundant time steps.

2. The contribution is helpful in diffusion community.

3. The figures are pretty and the presentation is readable.

4. The proposed AdaptiveDiffusion is effective on multitask.

Weaknesses:
1. In my humble opinion, some improvements are marginal to me, especially on ImageNet 256×256.

2. I am not sure the Theorem one is guaranteed even in larger sampling step size.

3. Since many methods investigate reducing sampling steps by employing higher-order solutions, this approach lacks novelty.

Limitations:
Please see in Weaknesses and Questions. If all of my concerns are addressed, I will definitely improve my score.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yaYJlpidX1;"REVIEW 
Summary:
The paper proposes a novel technique to automatically discover in-context continual learning dynamics for image classification task sequences through meta-learning. In order to achieve this purpose, the approach relies on 2 main novelties: 
* Using self referential weight matrices on top of an image encoder - SRWM, as self-modifying that adapts itself to the stream of inputs, is an natural model for continual learning. 
* Encoding continual learning desiderata in the meta-objective, i.e. backward and forward transfer. 

The authors first apply the approach in a classic two-task setting (Split-MNIST) that allows them to showcase and analyse the emergence of in-context catastrophic forgetting phenomena, and to show that using their ACL loss can help reduce it. They further evaluate their method and compare them to replay-free baselines from the CL and meta-CL literature, showing an advantage of their approach in scenarios with up to 3 tasks. 

The authors further test the limits of their approach by comparing it to more recent learning to prompt techniques for continual learning, leveraging the power of pretrained large models. This scenario show a limitation of the technique in more complex scenarios with more tasks, more diverse and complex data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper takes an interesting perspective on continual learning, leveraging the interesting properties of SRWM and the capability of meta-learning to encode the desired behavior in the meta-learning objective. The combination of these two contributions is novel to the best of my knowledge, and lead to interesting insights. 

* The approach leads to interesting performance in relatively simple scenarios, outperforming some of the existing continual learning techniques. 

* I also particularly appreciated the authors discussion of the method limitations. Both the experiments with learning to prompts and the discussion provide very valuable insights that can help building on the work in the future.

Weaknesses:
* In my opinion, the main limitation of the approach is its practicality. From the experiments reported in Table 4, it seems that the approach requires to met-train on a sequence of similar length and/or complexity to provide its potential. This is not possible to know in advance in practice. Moreover, one limitation that the authors have not mentioned is that the meta-objective seems to require keeping in memory a number of copies of the model that is equal to the number of tasks. This can quickly become cumbersome for real applications that can require more complex models and very long sequences of tasks.  

* While the authors focus on classic benchmarks for continual and meta-learning, these benchmarks are artificial, relatively simple and lack of diversity. Different works highlight the limits of these benchmarks, I invite the authors to look at ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification"" Ullah et al. 2023, and ""NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research"" Bornschein et al. 2023 for examples of more realistic benchmarks. 

* It would be interesting to add a discussion of the cost of the approach (computation, memory, ...). Even is it gives a substantial boost in many cases, it would be interesting for practitioners to compare what they gain to what they pay.

Limitations:
The authors provide a detailed discussion of the work limitations, both in the experiments and the discussion sections. Some other limitations are highlighted in the Weaknesses paragraph above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on Automated Continual Learning which is different than handcrafted continual learning. It uses self referential neural networks to meta learn their own in-context continual learning algorithm. First, the paper shows the emergence of in-context catastrophic forgetting. Second, the paper analyze the performance of proposed method (ACL) and finally the paper discuss the limitation of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written and easy to follow
- The paper introduces original idea of Automated Continual Learning
- The paper identifies ""in-context"" catastrophic forgetting

Weaknesses:
- The paper claims to do in-context continual learning but the concept of in-context learning is not clearly explained.
- The paper mainly focus on two task and five task settings but it would be more helpful to see the more different settings such as three task or four task
- How is the size of SRWM affects the maximum sequence length that can be train?

Limitations:
Authors have addresses the limitation of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes a method for in-context continual learning (CL) by using a type of meta-learning neural architecture based on ‘self-referential weight matrices’ (SRWM). Proposed in prior work, these models learn to modify weight matrices iteratively as they process more and more inputs. In this work, they are given few-shot examples from different tasks and iteratively update the weight matrices as the examples are processed. This update process is referred to as “in-context” learning in this work. The key innovation is to define the loss function of SRWM training to optimise for both forward (improving performance of subsequent CL tasks) and backward (improving performance of previous CL tasks) transfer while achieving good performance on the current task. Experiments are conducted on commonimage classification meta-learning benchmarks such as Split-MNIST and Mini-ImageNet. Results show the proposed method prevents catastrophic forgetting (without using replay), outperforming existing meta-learning baselines on the evaluated benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Studies the problem of in-context catastrophic forgetting via a two-task toy setting and reveals the issue when training with no backward transfer loss term. This is shown to be mitigated by including the backward transfer loss term.

Proposes an in-context CL method using models based on SRWM and a novel loss to mitigate catastrophic forgetting as more tasks are learned. The method does not use a replay buffer.

Studies and covers standard image classification meta-learning tasks such as Split-MNIST, FashionMNIST, and CIFAR-10. On Split-MNIST, shows improvements over existing CL and meta-baselines in both domain and class incremental evaluation settings. The improvements, when additional 5-task fine-tuning is used, is significantly above baselines. 

The paper is clearly written, with thorough literature review.

Weaknesses:
One weakness of the proposed method is that the number of loss function terms increases with the number of CL tasks, as pointed out by the authors in Appendix A.5. This prevents this method from being scaled to more practically relevant settings where a large number (much more than 2 or 3 that this paper has mostly focused the experiments on) of tasks are considered in a CL setting. Method of reducing the loss terms would strengthen the paper.

Another weakness, which is also noted by the authors in Table 4 and Section 4.3, is that the performance of the proposed model and method is poor compared with those based on pre-trained transformer models, even on an easier evaluation task. The authors in Section 5 also discuss a potential connection between LLM transformer training as an implicit version of the proposed model and method. Given these existing strong and more widely adopted methods, it is unclear how much value the proposed method adds. SRWMs are not widely used and LLMs training can scale to a massive number of tasks with a single loss [1] (albeit not CL). A more detailed explanation of the application of the findings of this paper beyond those interested in SRWMs would be helpful.

Another weakness of this paper is its focus on image classification meta-learning tasks only. It is helpful to know the generality of this method, for example on language modelling tasks or multimodal tasks. An experiment demonstrating the method in CL language tasks would be helpful.

[1] Finetuned language models are zero-shot learners. Wei et al. ICLR 2022.

Limitations:
Limitations have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of catastrophic forgetting (CF) by formulating continual learning (CL) as learning from a sequence of demonstrations of tasks. The paper proposes a meta-learning objective function that includes backward transfer terms. These terms compute the error of the predictor on previous tasks after receiving demonstrations of the current task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The approach of formulating (continual learning) CL as learning from a sequence of demonstrations of tasks is interesting.
- The experiment shows positive results when compared to non-meta-learning approaches

Weaknesses:
- The paper is difficult to follow. Many definitions and the algorithm are not very well explained.
    - The motivation of formulating (continual learning) CL as meta-learning is not well presented.
    - Some details of the architecture are mentioned in the background section only (e.g. replacing self-attention with SRWN and the multi-head version.)
    - The details of the training and inference process are not well presented.
- The training process can be very costly and poorly scaled with the number of tasks and the number of examples per task. In each step over a sequence of demonstrations, the method needs to compute and store a new weight matrix in order to perform back-propagation. It might require more memory during training and at inference.
- Even being a meta-learning approach, the model still needs fine-tuning when given a new task to adapt to a new number of tasks.

Limitations:
There are no negative social impacts. My suggestions have been listed in the previous sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cO1llRY2Br;"REVIEW 
Summary:
This paper focuses on model editing at a low cost. Evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks. Therefore, the authors propose a method, namely iReVa, to initialize and retrofit key-value pairs into MLP blocks in a Transformer for explicitly inserting new knowledge. Specifically, they insert new neurons in the MLP blocks for each piece of knowledge. Each neuron is initialized with the embedded key and value derived from the input-output pair, respectively. To prevent dramatic change to the irrelevant knowledge, iReVa further retrofits the key and value by fine-tuning with multiple objectives. Compared to the existing methods such as MEND, ROME, MEMIT, and MELO, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. The experiments on zsRE-10K and PARAREL-10K datasets reveal that iReVa has superior performance regarding edit success, generalization, and specificity. Further edit withdrawal test indicates that iReVa can explicitly manipulate the activation of neurons and easily withdraw the edits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper focuses on modeling editing, which has significant applications in the era of LLMs. It can be applied to alleviate the hallucination issue of LMs and resolve the out-of-date as well as missing knowledge in an LM.
2.	This paper introduces a novel editing method with key-value adaptors for traceable model editing. The proposed method makes sense to me. The initialization with embedded key and value derived from the input-output pair can easily make precise edits to the model. Further retrofitting refines the adaptors to satisfy the task.
3.	For experiments, the author has comprehensively shown the superiority of their method in the perspectives of edit success, generalization, and specificity. And more analyses reveal the generalization of iReVa. Particularly, the edit withdrawal test in Section 6.2 is well-designed, which shows the effect of traceable edits and could provide a potential solution for dynamic knowledge maintenance for LMs.
4.	Overall, this paper is well-written and easy to follow.

Weaknesses:
1.	The discussions on the limitations and broader societal impacts of iReVa are not included in the paper. I have some questions about the application scope of the proposed method. Please see the questions below.

Questions
1.	Could iReVa lead to a dramatically increasing number of parameters? Let’s see if there are millions of knowledge for editing, how can you potentially insert all the knowledge into LMs with iReVa? 
2.	After you change a piece of knowledge, can the reasoning still be conducted for the edited knowledge? For example, if we have edited the president of America, could some reasoning questions like ``Who is the wife of the president of America” also be resolved with the new knowledge?
3.	Typo: ``evident’’ in line 6 should be ``evidence’’. Please check.

Limitations:
No, the author should discuss the limitations of the proposed method such as the application scope, the potential risks, and future improvement to indicate how robust the results are to violate the
assumption. I would like the author to add such information during the rebuttal.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the tracable sequential model editing challenge by plugging in additional model components to a transformer MLP blocks. The proposed approach adds additional model components for each edit, allowing for traceability for each edit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	The results indicate that it is a strong approach compared to relevant literature and its performance is relatively stable when scaling it to thousands of edits.
•	The approach allows for ""separability of each edit"" which in turn allows for additional operations such as edit updation or deletion as showcased in the Edit withdrawal experiment. 
•	The edit withdrawal experiment is both unique and intriguing, as the concept of removing edits appears to be a novel area of exploration.

Weaknesses:
•	The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.

•	Experiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.

•	Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a ""stronger capacity for carrying traceable edits"" seems unfounded.

•	The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.

•	Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces iReVa, a novel method for model editing that explicitly initializes and retrofits key-value pairs into MLP blocks of transformer models to perform CRUD (Create, Read, Update, Delete) operations on LMs. iReVa aims to update knowledge in LMs without damaging irrelevant knowledge, offering better interpretability and traceability compared to existing methods. The method is validated through experiments on GPT series models, showing significant improvements in edit success and generalization without affecting specificity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Provision of the first attempt at conducting knowledge withdrawal tests for model editing methods.

The paper includes a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests and generalization tests.

iReVa's approach to model editing is innovative, focusing on retrofitting key-value adaptors into MLP blocks for traceable model editing

Weaknesses:
This paper could benefit from a more detailed comparison with other model editing methods, especially those focusing on lifelong learning and continual editing [1][2].

It does not discuss the computational efficiency of iReVa in terms of inference time or memory, which is crucial for real-world applications.

The reliance on the hypothesis that factual knowledge is stored in MLP blocks may be limiting [3], and the authors could explore the broader implications of this assumption.

The method's applicability to other types of tasks, such as erasing hallucinations, is not validated.

There is a noticeable absence of experimental validation on other recent and updated models such as GPT-J (used by ROME etc.), LLaMA.

The technical novelty of iReVa is somewhat limited, as it builds upon existing concepts like MEMIT [4] and key-value memory structures in MLPs [2].

The absence of a strategy for selecting the adaptor layer may hinder the method's rapid migration and application to various language models。

Equation 3 requires clarification, why 'i' and 'o' in Equation 3 are both passed through SELF_ATTEN again?

References

[1] Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, Hartvigsen et al,
Neurips 2023.

[2] Transformer-Patcher: One Mistake worth One Neuron, Huang et al, ICLR 2023.

[3] What does the Knowledge Neuron Thesis Have to do with Knowledge? Niu et al, ICLR 2024

[4] Mass-Editing Memory in a Transformer, Meng et al, ICLR 2023.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called iReVa for knowledge editing. iReVa initializes and retrofits key-value pairs into MLP blocks to create a new mapping of knowledge without affecting related information. Compared to existing methods, iReVa offers better interpretability and a stronger ability to make traceable edits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed methods demonstrate great performance compared to other baselines under the batch editing scenarios.

Weaknesses:
1. The color in the figure is not obvious to discriminate between the original knowledge neurons and new knowledge neurons.
2. The computation of the proposed method is similar to T-Patcher, I'm curious about the difference between them. The proposed methods are designed to tackle the batch edit, but it seems it still needs to add one neuron for each example.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TI7Vy90B9j;"REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
0LfgE6kvKZ;"REVIEW 
Summary:
In this paper Local Superior Soups (LSS) is proposed to minimize communication rounds in federated learning (FL) using pre-trained models, specifically tackling data heterogeneity challenges. LSS achieves this by employing sequential model interpolation, maintaining connectivity, and integrating diversity and affinity regularization terms. These innovations enable more local training steps and fewer communication rounds, effectively preventing client drift. Designed for adapting pre-trained models in FL, LSS enhances training efficiency, making it well-suited for deployment in edge computing applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method can effectively reduce the communication rounds in federated learning (FL) using pre-trained models.

2. The proposed method seems sound.

3. This paper is well written.

Weaknesses:
1. Only two small-scale image datasets are used in experiments. More large-scale datasets, especially those in other modalities, should be used.

2. More pretrained models should be explored.

3. More tasks besides image classification should be incorporated into experiments.

Limitations:
See weakness part

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method called Local Superior Soups (LSS), a novel technique for model merging in cross-silo federated learning aimed at reducing communication rounds while enhancing model performance. This paper introduces random interpolation, diversity term, and affinity term to alleviate the need for time-consuming model selection and redundant model training. Rigorous experiments on 4 datasets with 11baselines demonstrate the effectiveness of LSS.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper discusses the importance of bridging two low-loss valley to reduce communication rounds.
2. This paper introduces two quantifiable metrics, diversity and affinity, which serve as indicators of model quality..
3. This paper conducts extensive experiments to illustrate the effectiveness of LSS.

Weaknesses:
1. The distinction between LSS and similar federated learning methods such as FedProx, which also incorporates weights from global models to regulate client loss, is not clearly discussed.
2. The subsection 3.3.1 titled ""Random interpolation conserving connected low-loss region."" lacks mathematical detail to fully understand the interpolation process.
3. The requirement for clients to receive the interpolated model pool ($M$) could potentially lead to significant communication overheads, which may not present a clear advantage over simpler methods like FedAvg or FedProx.
4. The connection between Theorem 3.1 and the core methodology of LSS, specifically the diversity and affinity terms, appears tenuous. These terms do not seem to be directly derived from the theorem, which may weaken the theoretical foundation of the proposed method.
5. This paper should consider referencing relevant literatures or conducting preliminary experiments to support its statments on the part called ""Limitation of previous model soups methods"".

Limitations:
Please see the weaknesses and the questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LSS, a model interpolation-based local training technique to reduce the number of communication rounds required. The intuition is to regularize local models to connected low-loss valleys, so the aggregated model may have lower loss. LSS is empirically evaluated on a variety of datasets and types of distribution shifts.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Figure 1 and 2, though may not be very rigorous, are clear and provide intuition to the readers. 
- The proposed algorithm is tested on a variety of datasets and types of distribution shifts.

Weaknesses:
- Readability: the notation in section 3 is not clear enough. For example, $n$ is used for both number of data (section 3.1) and number of averaged model (Alg 1), which is confusing. 
- (minor) It might be a little abuse of notation when using $\mathcal{D}_i$ for both distribution and dataset. I suggest using different notations.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
FSgwgQXTxo;"REVIEW 
Summary:
The paper addresses the challenges of autonomous driving by integrating behavior among interactive agents, specifically focusing on issues caused by multi-agent scene uncertainty and heterogeneous interactions.  To tackle this, the paper introduces a topological formation called Behavioral Topology (BeTop), derived from braid theory, to represent consensual behavioral patterns among multiple agents. This formulation guides downstream trajectory generations and enhances the consistency of behavior prediction and planning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The experimental results seem to support the authors' claims.
2.	It is developed based on exiting braids topology. Novel method for an existing problem setup. 
3.	The use of braid theory to distill compliant interactive topology from multi-agent future trajectories seems a good and intuitive idea to me.

Weaknesses:
The paper is generally not well-written, with extensive use of ChatGPT leading to paragraphs that are hard to follow.

- As far as I understand, the paper uses braid topology for just one step in planning and prediction. In long-horizon planning and prediction, this may not be sufficient as the motion of vehicles is  no longer independent. Could you provide some basis to the idea why one step topology will be enough? 

-  There has been prior work using braid topology for planning (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812118). Benchmarking your method against this prior work would provide a clearer picture of this method. 

- The method appears very similar to existing methods like Wayformer, with the main difference being the use of braid topology. I would like to see a detailed comparison showing how much the encoding of braid topology improves performance compared to Wayformer, especially given that only one-step braid topology is used instead of long-horizon topology.

Overall, the paper has potential, and I would be happy to discuss more with you. I would be willing to increase my score if my questions and concerns are addressed satisfactorily.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a approach to address the challenges of autonomous driving in multi-agent scenarios and heterogeneous interaction. It introduces the concept of Behavioral Topology (BeTop) and its corresponding network BeTopNet. BeTop is based on braid theory and aims to provide a topological representation of multi-agent interactions to enhance the prediction and planning of autonomous vehicles (AVs). The proposed method focuses on creating a compliant behavioral pattern among agents, which guides the trajectory generation for AVs. Extensive experiments on large-scale datasets such as nuPlan and WOMD demonstrate the good performance of BeTop in both prediction and planning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well-written and polished.
* The idea of using braid theory to explicit formulate the interactions among agents is interesting and lays a solid mathematical foundation for BeTop.
* The proposed method effectively integrates prediction and planning in a unified framework.
* The proposed method and baselines are extensively evaluated on two large-scale real-world datasets to demonstrate the performance on both motion prediction and planning.
* Ablation studies show that each component of the proposed method contribute to its performance on the planning task.

Weaknesses:
* The proposed method infer one type of agent behavior topology from one mode of future trajectories (e.g., 8s), while the topology of multi-agent interaction for real-world autonomous driving is usually multi-modal and dynamic. A discussion of modeling multi-step and dynamic topologies of vehicles for a long horizon would be beneficial.

Limitations:
* The current implementation of BeTop considers only one-step future topology. Extending this to multi-step reasoning could provide more robust predictions and planning.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach to enhance the safety and social consistency of autonomous driving systems through improved multi-agent behavioral integration. To address inefficiencies and inconsistencies in current behavioral representations, the authors propose Behavioral Topology (BeTop), a topological framework derived from braid theory that captures consensual behavioral patterns among multiple agents. This framework guides downstream trajectory generations and ensures stable collective behavior when integrating prediction and planning. The paper also presents BeTopNet, a synergistic learning framework supervised by BeTop that manages behavioral uncertainty and enhances prediction and planning consistency. Extensive experiments on large-scale real-world datasets, including nuPlan and WOMD, demonstrate that BeTop achieves state-of-the-art performance in prediction and planning tasks, showcasing its effectiveness in interactive scenarios

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper introduces a novel approach to enhance the safety and social consistency of autonomous driving systems through improved multi-agent behavioral integration. To address inefficiencies and inconsistencies in current behavioral representations, the authors propose Behavioral Topology (BeTop), a topological framework derived from braid theory that captures consensual behavioral patterns among multiple agents. This framework guides downstream trajectory generations and ensures stable collective behavior when integrating prediction and planning. The paper also presents BeTopNet, a synergistic learning framework supervised by BeTop that manages behavioral uncertainty and enhances prediction and planning consistency. Extensive experiments on large-scale real-world datasets, including nuPlan and WOMD, demonstrate that BeTop achieves state-of-the-art performance in prediction and planning tasks, showcasing its effectiveness in interactive scenarios

Weaknesses:
In general, the paper is well written and there are no major weakness, however, there are some aspects that can be further discussed in the paper: 1.While the paper demonstrates effectiveness on specific datasets, it remains uncertain how well the method generalizes to diverse driving environments and conditions not covered in the training data. 2. The computational overhead associated with the topological framework and synergistic learning might be higher compared to simpler models, possibly affecting real-time performance.

Limitations:
Please refer to the weakness section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new approach, called Behavioral Topology (BeTop), to address the challenges in modeling multi-agent behaviors in autonomous driving. By utilizing braid theory, BeTop explicitly represents the consensual behavioral patterns among multiple agents, facilitating better prediction and planning. The framework, BeTopNet, incorporates this topological reasoning into a synergistic learning model that guides both behavioral prediction and planning. Good experiments on large-scale datasets, such as nuPlan and WOMD, demonstrate the superior performance of BeTopNet in both prediction and planning tasks, showcasing significant improvements over existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Good Presentation: The paper is well-organized and clearly presents the motivation, methodology, and results. The introduction of BeTop is logically structured, and the figures help in understanding the complex concepts.

Reasonable Formulation: The use of braid theory to represent multi-agent interactions is innovative and provides a solid theoretical foundation. This formulation helps in capturing the interactive behaviors more effectively compared to traditional dense or sparse representations.

Extensive Experiments: The authors have conducted comprehensive experiments on large-scale real-world datasets. These experiments cover both prediction and planning tasks, providing a thorough evaluation of the proposed method.

Performance Improvement: The experimental results demonstrate that BeTopNet achieves improved performance in prediction and planning tasks, especially in planning scores and prediction accuracy, with detailed metrics provided to back these claims.

Weaknesses:
Lack of Discussion on Multi-Agent Settings: While the paper introduces a topological approach for multi-agent behavior modeling, it lacks an in-depth discussion on how this method scales and handles various multi-agent settings. More insights into the limitations and potential scalability issues would strengthen the paper.

Formulation for Multi-Agent Settings: The paper could benefit from a more detailed formulation of the multi-agent setting. While the braid theory is used to model interactions, a clearer and more comprehensive explanation of how this integrates with different numbers and types of agents would be helpful.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
5iUxMVJVEV;"REVIEW 
Summary:
This paper introduces Peri-midFormer to capture multi-periodicity in time series data. Specifically, it designs a pyramid structure and attention mechanisms to effectively model complex temporal variations. The proposed methods demonstrates great performance in several time series analysis tasks in the author's experiments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper compares a variety of cutting-edge methods.
2. Overall, this paper is solid and the authors conducted thorough experiments.

Weaknesses:
None.

Limitations:
As mentioned above, the authors have already discussed the limitations, though I have some concerns for it.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors proposed a new method Peri-midFormer, which uses the multi-periodicity of time series and modeling the periodic part of a time series in a pyramid way. They further proposed an attention mechanism to use the neighborhood relation in the pyramid. Extensive experiments on different tasks show the effectiveness of the proposed method.
After reading the rebuttal, I raise my rating from 5 to 6.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The idea of using attention in the pyramid structure seems to be novel.
- Extensive experiments are conducted for different tasks on benchmark datasets.
- The proposed model is light-weight.

Weaknesses:
- Some important related works are missing. 
  - There are other related works utilizing the idea of modeling time series in multi-scale, e.g. [1] [2] [3].
- Experiments could be improved. 
  - The authors should clearly state whether they reproduce the results for comparison methods or they copy the numbers from the paper.
  - In the original paper of PatchTST, the authors use a longer context window than 96. The authors are suggested to tune this parameter for all comparison methods (as many papers did) and provide the best results, rather than fix the context window. 
  - The authors should provide the script for hyper-parameter tuning to improve the reproducibility of the paper.
  - The ablation study (Table 5) is only shown on one dataset (ETTh2). The authors are suggested to include results in more datasets.
  - The training and inference complexity and actual time could be analyzed, as well as memory efficiency. 
[1] TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting. ICLR 2024.
[2] PERIODICITY DECOUPLING FRAMEWORK FOR LONGTERM SERIES FORECASTING. ICLR 2024.
[3] Disentangling Structured Components: Towards Adaptive, Interpretable and Scalable Time Series Forecasting. TKDE 2024.

Limitations:
Focused only on periodic signal

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Peri-midFormer, a novel transformer-based architecture designed for time series analysis. 
By leveraging the multi-periodicity inherent in time series data, the model constructs a Periodic Pyramid structure that decouples complex periodic variations into inclusion and overlap relationships among different periodic components. 
The proposed method incorporates self-attention mechanisms to capture dependencies between these periodic components, achieving state-of-the-art performance across five mainstream time series tasks: short- and long-term forecasting, imputation, classification, and anomaly detection.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1.  The concept of decoupling time series data into a Periodic Pyramid is a valid point. 
This new representation seems to capture the multi-periodicity of time series effectively.

S2. The effectiveness of the proposed method is extensively verified on five tasks.

Weaknesses:
W1. While the Periodic Pyramid and self-attention mechanisms are well-explained, the model's complexity might pose challenges for practical implementation and scalability, especially for users with limited computational resources.
It would be useful if the authors could report in the main body of the paper, at least the main conclusion of the training and inference time for the proposed method, compared with existing baselines, across the five tasks.

W2. The improvements provided by Peri-midFormer seem to be insignificant when compared with Time-LLM and GPT4TS on forecasting, imputation, and anomaly detection tasks.
It would be better if the author could further discuss this in the main body of the paper (e.g., discuss the time-accuracy tradeoff as shown in Appendix E.4 Complexity Analysis).

Limitations:
L1. It is not clear how the proposed periodic pyramid can be effectively and efficiently integrated into multi-dimensional time series.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The abstract succinctly introduces the Peri-midFormer, a novel approach designed for time series analysis, acknowledging the challenges posed by the discrete nature of time series data and the complexity of capturing periodic variations directly. It proposes a method to address these challenges by decomposing complex periodic variations into hierarchical periodic components, termed the periodic pyramid. This approach leverages inclusion and overlap relationships among these components, mimicking the natural pyramid structure observed in time series data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Innovative Approach: The concept of a periodic pyramid to model time series data is innovative and promises to address the limitations of traditional methods that struggle with capturing complex periodic patterns.

Hierarchical Representation: By representing time series as a pyramid with progressively shorter periodic components, the model potentially enhances the understanding of multi-scale temporal relationships.

Self-Attention Mechanism: Incorporating self-attention into the periodic pyramid allows capturing intricate relationships among periodic components, which is crucial for tasks like anomaly detection and forecasting.

Weaknesses:
Complexity and Scalability: The introduction of a hierarchical pyramid structure combined with self-attention could potentially introduce computational complexities and scalability issues, especially with larger datasets or real-time applications. Addressing these concerns in the paper would strengthen its practical utility.

State-of-the-Art Comparison: It is not explicitly stated whether Peri-midFormer achieves state-of-the-art (SOTA) performance when compared to strong baseline models. Without clear comparative results, it is difficult to ascertain if the proposed method truly represents an advancement over current leading approaches in time series analysis.

Interpretability: The abstract lacks interpretability regarding why the periodic pyramid structure exists in the applications and why it plays a critical role in improving forecasting. Providing a rationale or theoretical justification for the efficacy of the pyramid structure in capturing temporal patterns would enhance the understanding and acceptance of the proposed method.

Limitations:
see weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
eYUbBKxABP;"REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
azkuhJBZXi;"REVIEW 
Summary:
This paper develops a structured and generalized reasoning framework, CreDes, for long-range reasoning in LLMs. In the framework, the Causal Relationship Enhancement (CRE) is used to guarantee the solid causal rightness between each step of reasoning and state transition, and the Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree, to improve the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper is well-structured and clearly states the problem they studied. It considers the long-range reasoning of LLMs from two aspects: the correctness from one-step reasoning (OSR) to state transition, and the efficiency of the solving process.
2. This paper transits the long-range reasoning problem of LLMs into the construction of causal probability trees from the initial and goal states and uses Dual-End Searching to improve efficiency. This is a reasonable and interesting thought.
3. The experimental results are SOTA in long-range reasoning tasks in terms of both accuracy and time efficiency.

Weaknesses:
1. The main concern is the understanding of ATE. This paper frequently uses ATE as part of the loss function and thinks the lower ATE can guarantee the solid causal rightness between each step of reasoning and state transition. However, ATE is used to measure the causal influence level between variables from the observational data, and causality does not mean rightness.
2. The DES section is not clear enough. It is suggested that more explanation be provided for the reason for the ATE as part of the loss. For example, if “B is the number of unfolded layers where the current leaf is located Ni”, what does E(A|do(B)) and E(A) mean in Formula (5)?
3. This paper needs to supplement the usage scenarios of methods, specifically in which scenario to use CreDes, in which scenario to use Cre alone, and whether Des is used separately.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CreDes, a framework to improve the long-range reasoning capabilities of LLMs, consisting of two main components: Causal Relationship Enhancement (CRE) and Dual-End Searching (DES). CRE is developed to reduce causal hallucinations in LLMs by strengthening the causal relationships between reasoning steps and state transitions; it uses structural causal modeling and optimizes the Average Treatment Effect (ATE) during training. DES breaks down long-range reasoning tasks into shorter segments by simultaneously searching from both the initial and goal states on a causal probability tree. The authors evaluate CreDes on Blocksworld, GSM8K, and Hanoi Tower puzzles, showing improvements in both accuracy and efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- CreDes demonstrates significant improvements over existing methods, especially for complex tasks requiring many reasoning steps.
- The use of causal modeling concepts like ATE provides a solid theoretical foundation for the proposed approach.
- The method shows effectiveness across different types of reasoning tasks (e.g., spatial reasoning, math problems).
- CreDes enables simultaneous multi-step reasoning, potentially reducing computation time compared to sequential methods.

Weaknesses:
Major concerns: 

- The generalizability and scalability need better justification. The paper primarily tests the CreDes framework on Blocksworld, Hanoi Tower, and some mathematical reasoning tasks (GSM8K). These are relatively structured, rule-based problems that may not represent the full spectrum of reasoning challenges. In addition, the proposed method cannot be well scaled to long-range reasoning; for example, in Table 1, performance drops significantly for Blocksworld tasks beyond 8 steps, with success rates falling from 0.68 to 0.34 for 12-step problems using Llama-2-7B + CreDes. Table 3 shows even steeper declines for Hanoi Tower, with success rates dropping from 0.27 at 9 steps to just 0.07 at 13 steps for Llama-2-7B + CreDes. Notably, the authors explicitly acknowledge this limitation in Section 4.6, stating: ""The DES approach, while effective for moderate-length tasks, struggles with very long reasoning steps, leading to a decline in performance.""

- The presentation of this paper could be improved.

  -- In the problem definition, there is no explanation of the difference between training without common instructions and with common instructions.

  -- There is no detailed discussion of the differences between correlation and causation in Sec 3.2. I am confused about whether the correlation of two variables has anything to do with their distributions.

  -- While efficiency gains are mentioned, the added complexity of CRE and DES likely introduces some computational overhead, which could be further discussed.

  -- There is no analysis of the impact of the choices of hyperparameters on the methods, particularly in the CRE component.

- The proposed method lacks comparison to more recent state-of-the-art methods. The paper compares CreDes mainly to older baselines: Reasoning via Planning (RAP), Chain of Thought (CoT), and Reflexion of Thoughts (RoT). However, it doesn't evaluate against more recent advances in LLM reasoning, such as Tree of Thoughts (ToT) extensions in line 42, and the paper doesn't mention or compare to other recent works such as [a] and [b], which also address multi-step reasoning challenges. As a result, the technical contribution is not entirely clear.

[a] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, & Xinyun Chen (2024). Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations.

[b] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, & Zhiting Hu (2023). Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Minor concerns: 

- Experiments are primarily conducted with 7B parameter models, leaving questions about scalability to larger models. How does the performance of CreDes scale with increasing model size (e.g., to 10B+ parameters)? The computational overhead may limit the framework’s scalability and applicability in real-world scenarios with limited resources.

- The approach achieves significantly lower accuracy in tasks with very strict ordering constraints, such as the Hanoi Tower problem.

- Since Blocksworld involves random steps, an analysis of the robustness of the performance may be needed.

 - More analysis/discussion on the sequential ordering of steps may be helpful. Notably, the ATE cannot recognize casual logic.

 - Some editorial issues, e.g., Line 110

Limitations:
The authors have discussed the limitations, and it is adequate to me. I do not see any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The integration of Causal Relationship Enhancement (CRE) and Dual-End Searching (DES) mechanisms presents a novel solution to addressing causal hallucinations and large search spaces in long-range reasoning tasks. The CRE mechanism’s use of Structural Causal Modeling (SCM) and Average Treatment Effect (ATE) is  ensure causality between reasoning steps. Extensive testing on datasets such as Blocksworld, GSM8K, and Hanoi Tower demonstrates the effectiveness of the CreDes framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea seems novel and it test on well-known reasoning datasets.

Weaknesses:
The method presented in this paper evaluates ATE on LLMs, but this approach's validity hinges on the assumption that LLMs can perfectly represent the real-world environment. The very reason we criticize LLMs for their reasoning issues is because their inferences are not accurate. Estimating ATE might only bring the prediction results closer to Y while maximizing the influence of the intervention factor on Y. However, it does not necessarily mean that the intervention factor is the true cause of Y. In other words, since there is no alignment with the causal relationships in real-world scenarios, the implementation of this method does not prove that the reasoning is causally sound.

The method lacks deeper thinking. The authors just apply the concept of ATE to the Chain-of-Thought (CoT) without thorough analysis. This oversight leads to a misalignment between the experimental results and the motivation of the paper. Suppose LLMs are not a good s simulations of the real world. In that case,  performing interventions on LLMs (whether they align with the real world or their identifiability) requires sound theoretical analysis and experimental validation. The current paper lacks a deep discussion on this matter.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve LLMs in dealing with long-reason reasoning problems, especially the challenges of causal hallucination (inconsistency between one-step reasoning and corresponding state transition) and large search space. To tackle the first challenge, average causal effect of the one-step reasoning (treatment) on the state transition (outcome) is added to the loss function of the LLM; and for the second challenge, a dual-end (i.e. bi-directional) search approach is taken to improve efficiency. Experiments are conducted to demonstrate the effectiveness of the proposed method and its superiority over the compared existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. An interesting idea of formalizing the problem from the perspective of causal effect and incorporating causal effect into the loss function.
2. The adoption of a dual-end search approach for improving efficiency.
3. The motivation of the paper is well presented in general.

Weaknesses:
1. The soundness of the proposed CRE method (for dealing with the challenge of causal hallucination) is in doubt.

(a) It's not clear why the method aims to $\textbf{minimize} the absolute value of the average treatment effect (ATE) of the one-step reasoning on state transition. Assuming that the ATE can be accurately estimated, what we want here would be to maximize the ATE that can be achieved by the LLM, i.e. when the one-step reasoning is correct done will likely lead to a correct state transition.

(b) It's not clear how an unbiased estimation of the ATE can be obtained, and what assumptions are made in terms of ATE estimation.

(c) The definition or understanding of ATE is incorrect. In particular, formula (2) is wrong, and formula (5) is incorrect too. 

2. The presentation/technical quality requires improvement, including the presentation of related work. Please find below some examples:

(a) In Lines 42 to 44, it is said that the existing methods such as CoT are limited in task decomposition, but Lines 78-80 state that they can breakdown queries into manageable steps.

(b) Section 3.1 is titled as ""Problem Definition"", but it rather looks like a section on experiment setting.

(c) Lines 145-146 state that Fig. 1 shows ""we leave the reasoning path selection to be controlled by the cross-entropy loss"", but I cannot see this indicated in Fig. 2.

(d) Line 159: do(.) is an operator, specifically the do operator, rather do-calculus, although do-calculus uses this operator.

(e) Lines 159-160: the statement on the do(.) operator or do-calculus is incorrect, since an do operation on the treatment X would lead to the change of the outcome Y, especially if X is a cause of Y.

Limitations:
The authors have presented some discussions on the limitations of the proposed method. It would be better if the assumptions made could be presented more clearly and what the practical implications would be if the assumptions are violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new framework, CreDes, designed to enhance causal reasoning in large language models (LLMs) and solve complex, long-range reasoning problems. The framework integrates two main innovations: the Causal Relationship Enhancement (CRE) mechanism, which applies cause-effect interventions to maintain causal accuracy across reasoning steps, and the Dual-End Searching (DES) method, which approaches problem-solving by initiating searches from both the initial and goal states to efficiently navigate large search spaces. The efficacy of CreDes is demonstrated through rigorous testing on challenging datasets like Blocksworld and Hanoi Tower, where it outperforms existing state-of-the-art models in both accuracy and efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach: The paper addresses essential limitations in LLMs' reasoning capabilities for long-range tasks in a causal perspective.
2. Comprehensive evaluation: The authors test their method on multiple datasets and compare against several baselines and shows improvements in both accuracy and time efficiency.

Weaknesses:
1. Limited model sizes: The experiments are primarily conducted on 7B parameter models, which may not reflect performance on larger state-of-the-art LLMs.
2. Lack of error analysis: The paper doesn't provide a detailed analysis of the types of errors made by the model or how they differ from baseline methods.
3. Dataset validity and construction: More details is needed for the use of a custom-made Hanoi Tower dataset which potentially limiting the reproducibility and generalizability of the results.
4. Computational efficiency and scalability: As mentioned in the Limitation, the paper lacks a detailed discussion of the computational requirements and scalability of the CreDes framework.
5. Generalization to less structured tasks: The framework's effectiveness is primarily demonstrated on highly structured tasks but it's unclear about its applicability to more dynamic or open-ended reasoning scenarios.
6. Lack of statistical significance: The paper doesn't report error bars or statistical significance for its experimental results.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
vMMzjCr5Zj;"REVIEW 
Summary:
Training large time series (TS) models is often limited by the scarce data available for a specific application. Existing pretraining methods use a simplistic tokenization scheme where the TS is cut up into equally sized parts, independent of its content. The newly proposed method *Large Pre-trained Time-series Models*, therefore, adaptively segments the input time series into (potentially) overlapping tokens depending on the TS itself. It shows very good forecasting performance in zero-shot and finetuning settings. It can also be used for classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The relevance of the problem and motivation for adaptive segmentation is convincing.
- The method of adaptive segmentation is an interesting solution to the issue.
- LPTM is compared against a plethora of appropriate and challenging baselines.
- It shows promising empirical results.

Weaknesses:
- I am under the impression that this paper may have found a strong method yet does not sufficiently investigate *why* it works. The interplay between learning the scoring function and training the encoder is not very clear. See below.
- A lot of experimental claims are not adequately substantiated. It is claimed in question 6 of the checklist that error bars are provided and that statistical significance tests are performed, yet I did not find them. See below.
- The overall presentation (language and formatting) should be improved.
- The provided implementation is not accessible. (Error: ""The repository is expired"") In the current state, results are not reproducible since key hyperparameters are missing. The authors claim in question 6 of the checklist that they state how hyperparameters were chose, yet I could not find it in the paper.

Limitations:
Depending on the answers to the questions above, possible limitations mentioned could be made more transparent. For example, the insights into the newly induced biases are currently limited.

The answer to section 2. in the checklist is neither sufficient nor truthful. For example, ""multivariate"" is never mentioned in Section 7.

The discussion of the societal impact could also consider a possible data leakage from one private application (e.g., in the medical domain) to another one. Even a rather mundane problem, like a feasible membership inference attack, could be problematic in privacy-sensitive scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new approach for creating pre-trained models for time-series data, similar to those used in language and vision tasks. The authors propose a model called Large Pre-trained Time-series Models (LPTM), which includes an innovative adaptive segmentation module to handle diverse time-series data from multiple domains.

Key contributions include:

- Developing a framework for pre-training time-series models on multi-domain datasets, using a novel adaptive segmentation module to tokenize inputs effectively. This is achieved via a self-supervised learning objective.
- Demonstrating that LPTM performs as well or better than state-of-the-art domain-specific models when fine-tuned for various time-series tasks, such as forecasting and classification, with less training data and compute time.
- Proving that LPTM achieves superior results in both zero-shot and fine-tuned settings across diverse domains like epidemiology, energy, and economics, requiring up to 40% less data and 50% less training time compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper has the following strengths:

- Well-written, clear, easy to follow. Algorithm is a nice plus.
- Baseline choice reasonable: most recent methods are considered.
- Experimental results good, when considered on the set of datasets chosen (more points on that in the weaknesses section).

Weaknesses:
- It's a bit hard to get a good feel for the relative advantage of the proposed method. In table 2, the approach is clearly better, but we are left to infer that from that fact that it is commonly second or first in the rankings. Could the authors maybe add some for of aggregate metric, e.g. the average rank across datasets of a given method?
- Despite mentioning code is available, the link does not work (subscript 3 on page 7, time of access 2024-07-12, and previously): ""The repository is expired"".
- For a paper dealing in large part with forecasting, I was surprised by the absence of almost all of the classical long-term forecasting datasets used by other papers: traffic, electricity, weather, illness... Given that these are by far the most heavily studied ones in the literature, including them (as proposed in the questions section). While I don't find it a critical (but still important) concern, I strongly advise the authors to consider adding them as it will help avoid concerns other readers might have about cherry-picking of results.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Large Pre-trained Time-series Models (LPTM), a novel method designed to improve the efficiency and performance of time-series analysis across multiple domains. 
The key contribution is an adaptive segmentation module that automatically identifies optimal segmentation strategies for diverse datasets during pre-training. 
This approach aims to overcome the limitations of fixed-length segmentation, which may not adequately capture the temporal patterns of heterogeneous time-series data. 
LPTM demonstrates superior forecasting and classification performance, requiring up to 40% less data and 50% less training time compared to state-of-the-art models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1. This paper focuses on the time series segmentation problem.
As the basic semantic unit in time series is not as clear as in text, a proper segmentation is a promising direction towards better series modeling.

S2. The proposed segmentation method is adaptively calculated over each specific input series.

S3. The experiments are extensive.

Weaknesses:
W1. Although time series has a weaker semantic structure than natural language, it is closer connection to images.
In both time series and images, a semantic unit, e.g., a small item or a texture in an image, can have different lengths and scales.
This raises a challenge against the main motivation: why a full self-attention-based architecture works for images (e.g., ViT), why for time series the segmentation needs to be explicitly done?
It would be interesting if the authors can further discuss this problem and provide their intuitions.

W2. The introduction of the adaptive segmentation module seems to bring instability in the initial model training, as well as requiring longer training time (although the authors propose to backpropagate the gradients every 10 batches).
Specifically, the loss function for segmentation is a hard loss based on the selected subset of best segments.
However, the parameters seem to be randomly initialized, which could provide highly random ""best"" segments.
Hence, the convergence stability and the training time with and without the dynamic segmentation modules should be discussed.

W3. The dynamic segmentation modules seem not to be fine-tuned with specific attention.
However, as the author(s) mentioned, different datasets could have very different best segmentation.
Hence, it would be interesting to discuss why this is sufficient and provide theoretical or empirical evidences.

Limitations:
L1. There is a lack of explanability and interpretability in the adaptive segmentation results.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel contribution to pretrained time series models for forecasting and classification by paying attention to the fact that currently several transformer models take time series segmentations of the same size, regardless of the particular characteristics of the time series in consideration. For instance, time series that have yearly frequency or minute frequency might require different segmentation lengths, or it might be that dynamics are more complex in certain time intervals requiring a more detailed segmentation. Based on this observation the authors proposed a model that can find a suitable segmentation schema that later on allows to observe where are the time intervals where more complex dynamics are shown.

The authors perform several experiments and claim empirically that the proposed approach is at least competitive to the state of the art.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors study a clearly interesting problem: how to provide a suitable segmentation scheme for time series so that different time regions are segmented in different ways, depending on their complexity and amount of information. The motivation for this is well stated by the authors, leading to a novel approach to achieve this. 

The authors further set up this in an Self-supervised learning setting, and consider multiple datasets to pretrain their model and further provide several evaluations. This is interesting because depending on the field/topic/area of time series a different segmentation scheme might be more suitable.

Weaknesses:
Some of the main limitations are as follows:
- The proposed framework is not differentiable. The authors have acknowledged this in the paper and propose a workaround for this, basically to update the segmentation scores every 10 batches. Yet, this poses challenges like the interpretation of the training loss, and discontinuities in the test loss.
- It is unclear if the proposed approach is able to handle missing values. If not, is there anyway to overcome this? Missing values are very often present in practice and having a sound way to handle them is relevant.
- It is unclear if the current evaluation is fair. The authors present a corpus of datasets for which they pretrained the proposed model, but it is unclear which datasets where hold-out from pretraining. This is relevant as several of the pretrained models considered might have not been exposed to these datasets, which gives an unfair advantage to the proposed model. Further, since the amount of pretraining datasets is rather limited, there is the possibility that the proposed model is overly focused on these datasets, whereas other models, like (Ansari 2024) and (Woo 2024) were trained in a larger corpus of datasets.

Limitations:
The authors have acknowledged limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
D80tRH9CXF;"REVIEW 
Summary:
The paper examines prediction and estimation risk of ridgeless least squares estimator in the setting of a general error structure. The iid assumption on the error structure is often not valid in settings such as time series data , panel data, grouped data etc. The current paper introduces a theoretical framework which investigates the variance component estimation of both prediction and estimation risks in the above mentioned data settings. The benefits of overparametrization which has been seen in iid context has been shown to exist in the dependent error structure context as well.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Following are the strengths of the paper-

- Investigation of prediction and estimation risk under non i.i.d. regressor errors with specific focus on time series and cluster data

- Explicit quantification of the variance component of both the risks (as mentioned above) which depends on the trace of the error covariance matrix and the trace of a function of design matrix as a separable product.

- Explicit analysis of the variance and bias term of both the risks (as mentioned above) in the high-dimensional asymptotics

- Well constructed numerical experiments to support the theory

Weaknesses:
Following are the weaknesses of the paper

- The theoretical results particularly the bias component analysis section could have been more rigorous and better written. There are some notational discrepancies and theoretical inconsistencies.

- Some remarks following theorem 3.4 and 3.5 where the design matrix $X$ has a known distribution say Gaussian would have been useful  examples to get insight on the results proved in the theorems

- Some notations such as $a(X)$ and $b$ used in theorem 3.4 have been clarified later in the appendix. It would be better to introduce them in the sketch of the proof if you are using them anyway there.

Limitations:
The authors have adequately addressed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the prediction risk and estimation risk of the ridgeless least squares estimator under more general assumptions on regression errors. It highlights the benefits of overparameterization in a realistic setting that allows for clustered or serial dependence. The paper establishes that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. The findings suggest that the benefits of overparameterization can extend to time series, panel, and grouped data. The paper is a theoretical work that discusses various aspects of linear regression models, providing details on the assumptions and proofs for the theoretical results presented. It also includes information on the experimental setting and provides code and instructions for reproducing the main results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study addresses an important research gap by considering more realistic assumptions on regression errors. It provides exact finite-sample characterizations of the variance components of prediction and estimation risks, includes numerical experiments that validate the theoretical results, and demonstrates the relationship between the expected variance and the covariance of the regression errors. Additionally, it analyzes the bias components of prediction and estimation risks, offers a comprehensive overview of linear regression models covering various theoretical aspects, and provides detailed proofs for the theoretical results, ensuring the validity of their claims.

Weaknesses:
Is it possible to provide validation on large-scale data?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the properties of minimum norm (ridgeless) interpolation least squares estimators, analyzing prediction risk and estimation risk under broader regression error assumptions, including clustered or serial dependence. This diverges from the typical assumption of i.i.d. errors with zero mean and common variance. The paper shows that the challenges in estimating the variance components of prediction and estimation risks can be captured by the trace of the variance-covariance matrix of the regression errors.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a more general theoretical analysis of minimum norm interpolation least squares estimators, going beyond the restrictive i.i.d. error assumption.

2. The paper suggests that the benefits of overparameterization can extend to a wider range of regression settings, including time series, panel, and grouped data.

Weaknesses:
While the paper examines broader error structures, it might not fully grasp the complexity of real-world regression challenges, which could involve even more intricate patterns of error dependence.

Limitations:
The authors have addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the ridgeless least-squares estimator, and derives its prediction and estimation risk. One of the assumptions used is that the expectation of the noise variance matrix is finite and positive-definite. This is more general than the assumption that this expectation is some positive multiple of the identity matrix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper has an easy-to-follow introduction that motivates the need to derive theoretical results under general assumptions on regression errors.
- Related works are sufficiently discussed. The most relevant papers are those of Chinot et al. [9] and Chinot and Lerasle [8], which are based on different noise assumptions that this paper makes.
- The technical presentation is clear with examples and figures to help the reader understand the notations and results.

Weaknesses:
The major concern I have is whether the paper makes sufficient technical contributions. Even with the more general assumption on noise (Assumption 2.1), the technical change in the proofs seems very small compared to prior work. For example, the proof of Theorem 3.4 is short and relatively straightforward (and this might further simplify if we make Gaussian assumptions on data rather than left-spherical assumptions. Gaussian assumptions are what I like to make personally). It is always nice to have short and concise proofs whenever possible, but this might also indicate that the paper is not very technically solid.

Limitations:
N.A.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
O6YRAOfHGt;"REVIEW 
Summary:
After rebuttal

While I think there are still some problems with this paper, e.g. the short training duration, and the slight exaggeration of claims (that SHED outperforms UED). I think, however, that the idea is nice, and getting RL environment design to work better is a good goal.


-----


This paper aims to improve Unsupervised Environment Design in two ways.
First, it introduces a hierarchical MDP formulation, where the top level corresponds to the teacher, and the lower level corresponds to the learning agent. Each transition in the top-level MDP involves training the lower level agent on generated levels. Related to this, they develop a state representation for the adversary, which is the performance of the agent on a fixed set of diverse levels.

Separately to this, they use a diffusion model to upsample the number of experiences for the teacher, effectively training on synthetic data, to improve sample efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- I think the H-MDP formulation itself is very valuable; it moves away from treating the generation of environments as a black-box, sparse reward, multi-step generation process (as in PAIRED), and towards a more informed process, where the teacher gets feedback in terms of the state (i.e., the performance vector).
- The analysis in the appendix investigating the ability of the model to generate good synthetic trajectories is useful.

Weaknesses:
- Major
	- The results do not very convincingly demonstrate that SHED is better than current SoTA. Looking at figure 3 particularly, I would say that ACCEL has about the same performance as SHED. However, comparing against the RL baseline, SHED does do better.
	- The method is limited in the types of environments it can generate. For instance, mazes are generated using an LLM instead of directly placing blocks. This method therefore is not quite as broad in scope as PAIRED or ACCEL, which can generate arbitrary environments.
	- Relatedly, in the minigrid experiments, do all methods generate the levels in the same way using an LLM, providing the difficulty numbers? It would be good to compare this against the standard version of ACCEL that directly places blocks in the maze level, as it does not have the same restriction as SHED.
- Minor
	- The figures can be improved:
		- Make the alpha value on the error bars a bit less
		- Keep colours consistent across figures, so the same method has the same colour
		- Keep capitalisation consistent across the figure labels.
	- line 80, the period ends on the line after the maths, it should end on the same line.
	- Footnote 1: Jiang et al. (2021) use (amongst others) the positive value loss, which is not quite the GAE, as it clips it at zero before summing.
	- equation one, you use $\beta_t$ but $t$ does not seem to be defined? Should this be $\beta_k$?
	- Line 159, PARIED should be PAIRED
	- There is no reward scale in figure 4
	- Figure 9's caption can be made clearer. I understand it to be the performance of each method in different testing environments. 
	- Line 718 does not link to a figure.
	- Figure 11's caption: zero-shot and not zeros-shot
	- Capitalise the first word in the title of appendix C.2
	- In terms of notation, in line 96, $\pi^*$ usually has a dependence on $\theta$ (e.g. $\pi^*_\theta$) to indicate it is optimal w.r.t. that particular level.
	- Line 217, maybe add a citation to the first sentence, as I thought that is what you do, which confused me for a second.
	- line 237 space after period.
	- Line 241 ""given"" instead of giving?
	- Lines 296 - 297 are a bit confusing, as the word environment is used three times.
	- The assumption in theorem 1 is pretty strong.

Limitations:
I think the authors can list a few more limitations. 
Primarily, the restriction on the type of environment that can be generated, i.e., it needs numerical parameters, and generating a maze outright is challenging. This is quite a large difference to prior settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to Unsupervised Environment Design (UED) that addresses the challenges of efficiency by introducing a hierarchical MDP framework and using synthetic data. This framework involves an upper-level RL teacher agent that generates training environments tailored to a lower-level student agent's capabilities. The paper proposes the Synthetically-enhanced Hierarchical Environment Design (SHED) method, which uses generative modeling to create synthetic trajectory datasets, thereby reducing the resource-intensive interactions between agents and environments. The effectiveness of SHED is demonstrated through empirical experiments across various domains, showing superior performance compared to existing UED methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of diffusion models to generate synthetic trajectories is a novel approach that effectively reduces the computational burden of training the teacher agent.
- The paper provides comprehensive experiments across different domains, demonstrating the effectiveness and robustness of the proposed method compared to state-of-the-art UED approaches.

Weaknesses:
- The proposed method introduces significant complexity, particularly in the implementation of the hierarchical MDP and the generative modeling components. This might limit the accessibility and reproducibility of the approach.
- While the empirical results are promising, the evaluation is limited to a few specific domains. It would be beneficial to see broader applicability across more diverse and complex environments.
- Figure 4 is not properly formatted (no values on the axes).

Limitations:
The limitations are discussed in Appendix F.1 but I think the authors should discuss the limitations in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Unsupervised Environment Design problem, where a teacher agent seeks to design environments to train a student. Methods such as PLR, PAIRED and ACCEL have recently shown promising performance for random, RL and evolutionary generators. This paper proposes a handful of modifications, using RL with a different objective vs. PAIRED (performance on held out set vs. regret) and also proposes to add synthetic data to accelerate the RL process.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This is an interesting method in a relevant area of research. UED seems to be one of the most active areas of research with plenty of opportunities for impact.
* The use of evaluation environments is sensible and novel.
* The idea of combining this with Genie is incredibly exciting. It would be interesting to hear how this could be possible or could work. Is there any way to show a simple proof of concept?

Weaknesses:
* There appear to be two confounding features of the method, the new objective for PAIRED and then the synthetic data. Why do they make sense to combine in this way? It just feels like the authors tried to do ""enough for a paper"" rather than contribute something meaningful that people can build on. I say this because its unclear how these two independent features interact with other existing algorithms. Maybe we should just do ACCEL with synthetic data for instance? Did the authors try that? If it is in the Appendix already and I missed it then I will increase my score.
* The performance gains are fairly minor, and presented in an unclear fashion with just a bunch of curves on a single plot. Can we get some more rigorous analysis for example using the recommendations from Agarwal et al, ""Deep Reinforcement Learning at the Edge of the Statistical Precipice""?
* The Maze experiment seems to have many inductive biases and seems distinct from the diffusion based approach for BipedalWalker and LunarLander. What happens if ACCEL has access to ChatGPT as an editor and then uses replay? This seems like a simpler extension that alone could be a strong paper - although it would resemble ELM (Lehman et al 2022) so it wouldn't be particularly novel.
* The related work is very light. This is disappointing since the paper builds on so many related areas, such as synthetic data, diffusion models, UED, language models for evolution, procedural content generation etc.

Limitations:
Covered in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors of this paper use hierarchical MDP formulation and a teacher agent trained by RL to perform curriculum learning. To address the sparse data available for the teacher agent, this paper uses diffusion models to synthesize datasets for training. This paper performs experiments on lunar lander and bipedal walker environments to validate their claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Data sparsity is one of the main limitations of using a teacher agent in curricular RL. This paper uses diffusion models to synthesize a dataset for the teacher agent.

Weaknesses:
- This paper designs the teacher agent via hierarchical MDP to model the learning process of the student agent to perform curricular RL. However, Fingerprint Policy Optimization (Paul et al, 2019) also has a similar idea of modeling the learning process of the student agent. It would be interesting to explain more about how this paper's idea is related and contributes to this line of thought.

- A fully trained algorithm on the BipedalWalker should approach a cumulative reward of 300. Even the modified version used in the ACCEL paper is measured on a scale of 0 out of 300. However, from Figure 3, it appears all baselines perform less than 50 on the BipedalWalker benchmark. It is questionable whether all baselines were fully trained with the right settings. Also, the performance of the proposed algorithm and those of the baselines are statistically too similar to see whether SHED improves over the baselines in Lunar Lander and BipedalWalker benchmarks. Finally, other than the version of ACCEL in this paper not performing as well as the ACCEL in the original paper, I am curious whether ACCEL can be considered state-of-the-art in the benchmarks as written in line 324. Genetic Curriculum (Song et al, 2022) reports higher cumulative reward on the BipedalWalkerHardcore environment.

- Figure 4 has no scale on timestep and reward.

Limitations:
The authors has addressed the limitations of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
hGhLd2ByoR;"REVIEW 
Summary:
This paper reveals novel pathologies in existing unsupervised methods aimed at discovering latent knowledge from large language model (LLM) activations. Instead of extracting knowledge, these methods tend to identify the most prominent features of the activations.

The paper theoretically demonstrates that arbitrary features (not just knowledge) can satisfy the consistency structure of a popular unsupervised knowledge-elicitation method, namely contrast-consistent search. Additionally, the authors conducted a series of experiments showing that current unsupervised methods for discovering latent knowledge are insufficient. While the paper proposes potential future solutions, it does not provide a definitive solution to the problem with existing unsupervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is well-written, and its theoretical and analytical contributions may be useful. I am impressed about the extensive experiments.

Weaknesses:
More experiments on other LLMs are needed to further validate the claim.

It would be better to offer possible solutions to address the problems in existing unsupervised methods.

Limitations:
No solutions to address the problems in existing unsupervised methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a careful study on existing methods for discovering the latent knowledge from large language models (LLMs), especially Contrastive-Consistent Search (CCS). The authors prove that CCS might not actually discover the knowledge of LLMs, instead, it could fit any features that satisfy certain conditions. Through a series of experiments, the authors further demonstrate that CCS could be distracted by random words, irrelavant texts like the character's opinion, and remain sensitive to the choice of prompt. Finally, the authors propose some general principles for the future works about unsupervised LLM task discovery.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, the paper is well written and eazy to follow. The authors made interesting obervations about existing methods on knowledge discovery of LLMs. The theoretical analysis is well supported by the experiments. Sevaral guiding principles are also proposed for the future works. I think this paper would provide good information to the research community about unsupervised knowledge discovery of LLMs.

Weaknesses:
From my experience on unsupervised learning, I'd argue that the content of this paper *would not be sufficient to refute existing methods about unsupervised knowledge discovery (CCS)*. First of all, CCS is a method built on top of features from pretrained models. It'd definitely be sensitive to the features and thus also sensitive to the prompts, because features changes from different prompts (this could also be seen from the PCA visialization). Furthermore, as an unsupervised method, it'd be expected that the method might find multiple valid solutions, where only one of the solutions corresponds to the knowledge we are looking for. Taking the experiments from Section 4.2 as an example. The constructed dataset actually has two valid labels: the sentiment of the text and the sentiment of Alice. Depending on the optimization and the implicit bias of the algorithm, it could totally happen that an unsupervised method could found both valid labeling, or could only find one of them. I believe this is a common phenomenon shared by exsiting off-the-shelf unsupervised methods (like K-Means) cause they're searching for labels without supervision. From this perspective, I'd regard that this paper provides a method to construct ""adversarial datasets"" for CCS. However, it would not be a problem for CCS in practice.

Furthermore, the authors don’t provide solutions to this issue.

Also, I believe the mathematical notation in Section 3 could be simplified.

Minor issues: typo $c(x_i^+=1), c(x_i^+)=0$ in line 102

Limitations:
The authors have mentioned that this paper is focused on current methods and might not be directly applied to future works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the failure modes of the method called ""constraint-consistent search (CCS)"" in knowledge discovery for language models. In particular, they showed: there is no unique identification on the minimizer of CCS, as there are a class of features achieves the optimal loss; demonstrated experimentally classic unsupervised methods detect features other than knowledge; discovered features are sensitive to prompt formats.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper points out a popular method's overlooked short-comings and presents both theoretical and experimental results to support that CCS may not be able to discover the true knowledge feature: 1. the observation on CCS loss is driven by xor operator rather than the feature is clever; 2. given the vast space of feasible features, CCS method is very sensitive to prompts and thus deserves more careful examination if to use CCS in practice.

Weaknesses:
The main weakness of the paper is its lack of novelty and potential impact to the field. The paper is more an analysis work on the application of a single method [1] proposed in 2023, which given the speed of ML innovation, it is hard to see long-term benefits of this criticism.  The general principles proposed in the discussion section (Section 6) are interesting and fit more into the line of proposing desiderata for the field - though in their current status, require more rigorous work. 

[1] C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 357 2023.

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
eM3Wzs6Unt;"REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RTaSjoxmw9;"REVIEW 
Summary:
This paper addresses the challenge of achieving outlier robustness in phase retrieval, specifically focusing on the recovery of real-valued signals from intensity measurements that have been corrupted by adversarial outliers. The contribution of this work is the development of a nearly-linear time algorithm that is nearly sample-optimal and can accurately recover the true vector despite the presence of outliers. This is done through a two-step process that involves robust spectral initialization and robust gradient descent, utilizing recent results in high-dimensional robust statistics.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+A two-stage algorithm achieves nearly-linear time complexity consisting of an initial spectral initialization phase and a gradient descent refinement phase.

+Theoretical analysis showing the algorithm can recover the ground truth signal despite the presence of outliers, while maintaining near-optimal sample efficiency.

Weaknesses:
- The paper's theoretical results assume the corruption level $\epsilon$ as a constant in Theorem 3.1, despite it being initially introduced as a variable in Definition 1.2 to denote the extent of sample corruption. This constant treatment affects the robustness of the results by neglecting the influence of $\epsilon$ on the sample complexity. A thorough analysis that explicitly considers the variability of $\epsilon$ would significantly strengthen the theoretical foundation.
- Lack of numerical validations for the theoretical claims
- The idea and design of the two-stage robust phase retrieval algorithm is not novel, by adapting existing reweighting phase retrieval algorithms (throught different design of the reweights) to handle outliers through robust statistics. The contamination model is also from existing works, which has been recent studied in a number of different contexts; e.g., robust linear/nonlinear estimation by e.g., Diakonikolas and coauthors.
- Lack of numerical validation and comparison of the proposed algorithm with respect to existing (robust) Gaussian phase retrieval algorithms. It is difficult to judge if the proposed algorithm is of practical interest (or only of theoretical interest).
- Quite a lot statements in the paper are rather confusing, e.g., ""we propose the problem of outlier robust phase retrieval""; robust phase retrieval has been long studied in the literature; as far as I understand, the paper studies a new robust phase retrieval problem by considering also adversarial a_i's on top of existing formualtions. ii) ""It is well-known that natural nonconvex formulations of phase retrieval do not have spurious local optima."" which is not precise enough and is true under very stringent assumptions. iii) ""This is first achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Cand`es et al. (2015c))."" I guess the first Gaussian phase retrieval algorithm was the AltMin algorithm (Netrapalli, et al 2013. Phase retrieval using alternating minimization. Advances in Neural Information Processing Systems, 26.) which was interestingly not cited in the submission. iv) ""Similar landscape results are known for other natural nonconvex formulations of phase retrieval as well (e.g., min f(z) = P i( √ yi − |⟨ai, z⟩|)2 (Soltanolkotabi, 2019))."" The first work to study the Gaussian phase retrieval based on the magnitude-based least-squares nonconvex formulation and achieve provable guarantees is (Wang et al (2017). Solving systems of random quadratic equations via truncated amplitude flow. IEEE Transactions on Information Theory. 64(2):773-94.) Please be careful and fair in stating related results.

Limitations:
NA

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focuses on the problem of outlier robust phase retrieval, whose goal is to recover a vector $x \in \mathbb{R}^d$ from $n$ intensity measurements $y_i = (a_i^\top x)^2$ when a small fraction of the samples are adversarially corrupted. The authors propose and study this problem, providing a nearly sample-optimal and nearly linear-time algorithm to recover the ground-truth vector $x$ in the presence of outliers.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper provides an analysis of the practically interesting problem of outlier robust phase retrieval. The algorithm and framework might have implications for various applications that are relevant to phase retrieval.

Weaknesses:
1. The analysis appears to be more incremental in nature compared to those in previous theoretical works related to phase retrieval. More precisely, the key novelty of the spectral initialization step resides in the assignment of a nonnegative weight to each sample. Regarding the gradient descent step, the problem seems to be simplified to the analysis of robust mean estimation algorithms.

2. Important references are missing. For instance, the authors ought to cite the works related to robust compressed sensing (and it would be better to discuss in more detail the disparities between the analysis for the robust gradient descent step in this work and the analysis in these relevant works), such as

- Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. ""High dimensional robust sparse regression."" In International Conference on Artificial Intelligence and Statistics, pp. 411-421. PMLR, 2020.
- Liu, Liu, Tianyang Li, and Constantine Caramanis. ""High Dimensional Robust $ M $-Estimation: Arbitrary Corruption and Heavy Tails."" arXiv preprint arXiv:1901.08237 (2019).

3. The authors solely consider the scenario of noiseless intensity measurements and fail to take into account the noisy case.

4. The paper does not include experimental results, which could limit the confidence in the practical effectiveness of the proposed approach.

Limitations:
No experimental validations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies a classical problem called phase retrieval. The goal is to obtain unknown $d$-dimensional vector $x$ from $n$ datapoints $(a_i, \langle a_i, x \rangle^2)$. This work assumes that $a_i$ are iid Gaussian vectors, but also that a small $\varepsilon$ fraction of the data is corrupted. The authors suggest a two-stage process to identify the vector up to a small error. First, a spectral based algorithm is used to have a small constant error. Further, a robust gradient descent is used to approximate the initial guess up to a small error.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Paper is well-written and provides a good overview of the problem and of the techniques.
2. Phase retrieval is a traditional non-convex problem, which was largely studied before, and understanding how robust algorithms perform on it is important.
3. Paper uses prior technique in a simple way, and it is possible that this two-stage approach can be applied to other problems.

Weaknesses:
1. Results are limited to the Gaussian setting.
2. The method for RME that is used assumes that variance $\sigma$ is known? But in the way it is used here, $\sigma$ depends on the distance between current solution and the true vector. Authors do not comment on this issue.
3. $\tilde O, \tilde \Omega$ notation is not defined.
4.  Intuition in line 98 in my interpretation contradicts more exact version in line 214 (In the end, if I understand correctly, the crucial reason why the spectral initialization algorithm works is that the adversary cannot change the top eigendirection, but can only add new directions).

Limitations:
There are no ethical limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the phase retrieval problem for retrieving a real signal under the influence of arbitrary corruption. The corruption is allowed to be present in labels or features. They propose a two-step solution. First, they ensure that the initialization is robust to the corruption and second, they show that the gradient descent updates can be made resilient. Authors claim that their method can recover the true signal (with possibly sign mismatch) to an arbitrary precision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The ideas presented in the paper are certainly interesting. If resilience to corruption can be achieved in individual steps, then it makes sense that it might lead to a good overall recovery.

Weaknesses:
1. The authors claim that corruption level up to some universal constant $\epsilon'$ can be handled through their method. Although, to the best of my understanding, this quantity is not characterized in the main paper. What is the maximum value for $\epsilon'$? 
2. There is no discussion on the dependency of $\epsilon'$ on $n$ or $d$.   
3. The claim of signal recovery to an arbitrary precision puzzles me. It is known that for $\epsilon$-corrupted vectors, the robust mean estimation can only be done up to $\Omega(\sqrt{\epsilon})$ error. Despite that, the authors claim signal recovery (with possibly a flipped sign) to an arbitrary precision. Can the authors comment on how this is achieved?   
4. The claim in line 213 says that $y_i$ is always greater than $0$. Why is that true when the adversary can corrupt $y_i$ arbitrarily? As far as I can tell, the algorithm does not discard negative $y_i$s.

Limitations:
Please see above.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
xtpY1kQmW9;"REVIEW 
Summary:
This paper appears to suggest that any decision is composed of two Bayesian decisions and it trys to evaluate the implications of this idea. 

I am very confused by this paper and really don't know what to make out of it. For example, the conclusion seems to be only a brainstorming session of random ideas and the rest of the paper does not appear to be much better.

At the very least, it is not well written, at worst the proposed approach does not make any sense.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Given that I don't properly understand what exactly the authors want to achieve, I am unable to formulate the strengths of this paper.

Weaknesses:
The presentation is very messy. The paper jumps from topic to topic without me understanding their relations to each other.

Limitations:
see above

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the implications of Bayes' theorem, making assumptions inspired by a thought experiment of communicating a message. Prior (and model) elicitation by solving a fixed point equation is discussed.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The paper takes a fresh look at decision marking under uncertainty, which is at the center of machine learning.
* The generality of the setting makes the discussion applicable to virtually all of ML.

Weaknesses:
While I am sensible to the topic of prior and model elicitation from coherence arguments, I believe the paper needs a thorough revision focussing on clarity. While I have some intuition now, it is still not crystal clear to me what the exact goal or claims of the paper are. See bullets below for constructive comments.

## Major
1. Section 4: what is the probability $P$? What is the underlying space and sigma algebra? What are they supposed to represent?  
2. Section 4 introduces several very strong assumptions, like $1-P(A\vert B) = P(B\vert A)$ (is it for all $A,B$ in some sigma-algebra or for a specific pair of events?), that are motivated by an analogy about communicating a message. It is not clear why I should be prepared to make these strong assumptions. The fact that I don't know what $P$ is supposed to model or serve as does not help. Is it a joint probability over the variables describing a decision problem, as in decision theory? In that case, will it be used in conjunction to a loss function to make decisions? Will it be judged by some measure of decision accuracy? Or are we in a de Finetti framework, coming up with a personal probability $P$ which we will use to make predictions about unobserved variables? My intuition is that we are dealing with the latter kind, but this should be explained. And the strong assumptions need to be motivated by more than an analogy about communication.
3. The information analogy which motivates imposing the fixed point equation (9) is unclear, as well to what probability and what events it should apply.
4. p5 L179: the sentence about the parameter being a dynamic parameter for a learning system is unclear. We haven't discussed any learning algorithm yet.
5. I am not sure I see where Eqn (11) comes from. $\lambda$ has been chosen to derive (10) from Bayes' theorem, but it doesn't have to be the right base to write (11), right? Same remark for (18).

## Minor
1. p7 L248: Although neural networks have been a popular class of models and algorithms, supervised learning is not synonymous with neural network training.
2. p7 L252: the meaning of ""the $\lambda$ expression"" is unclear.

Limitations:
This is fundamental work that does not have any immediate negative societal impact.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The purpose of this paper is to investigate the optimality of a classifier. It is known that the Bayes classifier is optimal, and it is likewise known that an explicit computation of the Bayes classifier is often very challenging if not impossible. This paper offers an analysis of the Bayes classifier as a sequential solution of two problems. An analysis and interpretation of a vase / faces example is presented and some theory is developed to further understand it. The paper concludes with an application.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors are exploring an idea which is novel, and the whole thinking about Bayes classifiers as comprising two sub-problems seems novel and worth pursuing.

Weaknesses:
I did not really understand the discussion with the vase, the sender and receiver. Perhaps the authors should somehow connect the Bayesian ideas to the description of the problem earlier? I think the paper would really benefit from rewriting Section 4 with the vase as a running example, because it is hard to connect the various decisions with the probabilities. Maybe it's worth to add more illustrations / diagrams for this? The authors are presenting novel ideas and it's hard to understand them as they are currently presented.

For the theoretical implications, I think it would be better to illustrate the approach on a simpler model like a linear one. 

The paper started by mentioning the Bayes classifier but does not come back to it as an example. 

The paper states that the Bayes classifier is broken up into two decisions, but those are just briefly mentioned in the vase / faces example. The authors should carry this thread of reasoning through the whole paper.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ftqjwZQz10;"REVIEW 
Summary:
The paper proposes a novel method to address the accuracy degradation caused by downsampling on small AI processors. The authors observed that the input layer often has a small number of channels, leading to underutilization of the processors. To mitigate this issue, they introduce a technique involving patch-wise even sampling and channel-wise stacking. This method incorporates additional spatial information, thereby improving accuracy while efficiently using processing resources that would otherwise be wasted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The motivation for the proposed method is clear and well-justified.
* The idea of sampling and channel-wise stacking is both simple and effective, demonstrating a practical solution to a common problem in small AI processors.
* The paper provides well-defined baselines, including normal downsampling and CoordConv, and offers comparisons with various data channel extension approaches. 
* The authors conduct a thorough sensitivity analysis, evaluating accuracy, model size, and latency across different channel sizes.
* The method shows low latency and minimal model size overhead, making it a promising solution for improving accuracy without significant performance trade-offs.

Weaknesses:
* Further comparisons with a broader range of more complex models could strengthen the evaluation.
* There could be more discussion of potential limitations and scenarios where the method might be less effective.

Limitations:
The authors discussed the limitations concerning small models and acknowledged the potential negative societal impact due to the increased use of computational resources to improve accuracy. In my opinion, their discussion is sufficient and adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces DEX, a novel technique designed to enhance the efficiency of DNN inference on resource-constrained tiny AI accelerators by extending the data channels. This approach aims to improve both resource utilization and inference accuracy by incorporating additional spatial information from the original image through patch-wise even sampling and channel-wise stacking. The authors identify that the limited memory budget on resource-constrained tiny AI accelerators requires downscaling the input image, which can degrade model quality and underutilize resources. By extending the data channel, the proposed method retains more information from the input image, thereby improving inference accuracy and maximizing resource utilization without increasing inference latency. Evaluations on real tiny AI accelerator devices demonstrate a 3.1% accuracy improvement with no additional inference latency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper identifies the accuracy degradation and resource under-utilization issues for DNN inference on tiny AI accelerators and proposes a simple yet effective solution that can improve inference accuracy without additional inference latency.
- The evaluations are conducted on real hardware devices.
- The paper is well-written with the motivation, methodology and results being presented in a clear and logical manner.

Weaknesses:
While the paper has notable strengths, several areas could be improved:
- End-to-end Performance: The impact of proposed DEX on the end-to-end latency and throughput is not evaluated. Although the authors claim no increase in inference latency, the overhead of the channel expansion from input RGB images (including several preprocessing steps) needs to be studied.
- Power/Energy Measurement: The paper does not include the analysis of power consumption and energy efficiency with DEX, which are crucial considerations for tiny AI accelerators.
- Scope of Models: The study is limited to classification DNN models. The impact on other tasks, such as object detection, face recognition, and more complex applications, should be explored.

Limitations:
The authors have acknowledged the limitations regarding the exploration of larger models. However, they have not addressed the broader applicability of DEX to other tasks beyond classification. It would be beneficial to include evaluations on other applications such as object detection, segmentation, and more to demonstrate the generalizability of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Recent advancements in tiny ML accelerators, such as MAX 78000 and MAX 78002, have significantly boosted hardware processing power. On one hand, these accelerators feature 64 parallel processors with per-processor memory instances, enhancing CNN inference speed compared to traditional MCUs. On the other hand, downsampling of input images due to limited data memory can lead to accuracy degradation. To address this, this paper proposes DEX, which integrates patch-wise even sampling and channel-wise stacking to incorporate spatial information from original images into input images. Evaluation results demonstrate that DEX improves accuracy without introducing additional latency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper presents a simple yet compelling idea to tackle CNN inference on a specific tiny ML accelerator.
+ Figures, such as Figure 5, clearly illustrate the DEX procedure.
+ The analysis in the paper provides a clear understanding of how DEX operates.

Weaknesses:
- This approach appears suitable only for specific small devices.
- Some procedures are unclear and require further clarification. Detailed questions are listed below.
- Limiting the approach to processing only the first layer for simplicity may be a limitation of this work.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper indicates that current AI accelerators with limited data memory often require downsampling input images, which leads to reduced accuracy. Therefore, the proposed Data channel EXtension (DEX) includes additional spatial information from original images as informative input through two procedures: patch-wise even sampling and channel-wise stacking. This effectively extends data across input channels. As a result, DEX enables parallel execution without increasing inference latency. The numerical experiments consistently show improved model performance on four datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
•	The proposed method is easy to understand, with clearly written paragraphs and well-organized sections.

•	The experiments conducted demonstrate the effectiveness of the proposed method.

Weaknesses:
-	The proposed data channel extension requires the assumption that only a limited number of processors tied to memory instances are utilized while the remaining processors remain idle. However, it is not ensured that such an assumption is always true to trigger the proposed method.
-	The proposed method is as simple as an implementation trick; hence, the technical contribution is limited. 
 * The compared channel extension methods are all proposed by the authors and hence failed to show a fair comparison.
 * It is curious what the performance of patch-wise random sampling could achieve.

Limitations:
Included

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
34dHGTri2w;"REVIEW 
Summary:
This paper presents a novel parallel sampling method named ""Follow Hamiltonian Leader"" (FHL) designed to address sampling challenges by leveraging zeroth-order information, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism to enhance the efficiency and effectiveness of the sampling process. Experimental results indicate that FHL significantly improves the exploration of target distributions and outperforms traditional sampling techniques, especially in scenarios involving corrupted gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative combination of zeroth and first-order information.
2. The effectiveness of the method is demonstrated in multiple task scenarios.
3. Theoretical analysis and prove are sufficient.

Weaknesses:
1. Is there any quantitative experiments like evaluating FID and IS on cifar10 datasets and I think it's more compelling whether a novel sampling methods combined with generative models can be used on image datasets with more complex distributions.
2. Lack of experiment of OOD in combination with EBMs or score-based models to valid the stability during sampling with proposed method.

Limitations:
1. Limited exploration of integration with other advanced MCMC methods.
2. Lack of quantitative experiments to demonstrate the advantage of proposed sampling method compared with other methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an interesting parallel sampling method that leverages zeroth-order information to address challenges in sampling from probability distributions, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism, enhancing efficiency and effectiveness by connecting multiple sampling instances through a selected leader. The proposed method, named Follow Hamiltonian Leader (FHL), extends the Hamiltonian Monte Carlo (HMC) framework by concurrently running multiple replicas at different energy levels and combining both zeroth and first-order information from various chains. Experimental results demonstrate that FHL significantly improves the exploration of target distributions and produces higher-quality outcomes compared to traditional sampling techniques, showing resilience against corrupted gradients and excelling in scenarios characterized by instability, metastability, and pseudo-stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed Follow Hamiltonian Leader (FHL) method markedly improves the efficiency and effectiveness of sampling processes, significantly expediting the exploration of target distributions and producing superior quality outcomes compared to traditional sampling techniques.
- FHL demonstrates greater resilience against the detrimental impacts of corrupted gradients by incorporating zeroth-order information. This robustness makes the method particularly valuable in scenarios where first-order information is compromised, ensuring more reliable and accurate sampling.

Weaknesses:
- The proposed FHL method involves intricate modifications to the traditional Hamiltonian Monte Carlo framework, such as the leader-guiding mechanism and elastic leapfrog technique, which may increase the complexity of implementation and require significant computational resources.
- The effectiveness of the FHL method heavily relies on the appropriate selection of the leader particle. If the leader is not accurately chosen, it could lead to suboptimal sampling performance, potentially compromising the overall efficiency and accuracy of the method.
- While the paper presents experimental results to demonstrate the efficacy of the FHL method, there is a lack of in-depth theoretical analysis to rigorously establish the convergence properties and performance guarantees of the proposed approach.
- The method’s scalability to high-dimensional problems or extremely large datasets is not thoroughly addressed. The parallel sampling approach may encounter challenges in maintaining efficiency and effectiveness as the dimensionality and size of the data increase.

Limitations:
The authors have not adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to incorporate the energy $U$ into the gradient-based sampling techniques. In particular, it proposes to choose the lowest energy particle as the leader and then add an extra elastic tension between the leader and followers in the Hamiltonian Monte Carlo method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea is simple and clear, the toy examples are easy to understand and demonstrate the benefit of the proposed method well. In addition, the authors conduct experiments for each of the three challenging sampling scenarios identified by the authors.

Weaknesses:
It might worth including the overhead of the proposed method, how much slower the algorithm is per iteration compared to HMC for instance.

The tension coefficient $\lambda$ is critical, setting it to 0 recovers the baseline. But I did not find an ablation over the $\lambda$, is it hard to choose? From my understanding, if you set $\lambda$ pretty large it might recover something like gradient descent and the sampling will collapse.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an interesting new approach for improving sampling methods for energy-based generative models and score-matching models. The key idea is to incorporate zeroth-order information (energy values) in addition to the typical first-order gradient information used by most sampling algorithms like Hamiltonian Monte Carlo (HMC).

The authors identify several challenging scenarios where relying solely on gradients can be problematic - cases of instability, metastability, and pseudo-stability. They argue that incorporating energy values can help mitigate issues in these situations and improve sampling efficiency and quality.

Overall, the core idea of leveraging zeroth-order information in addition to gradients is quite novel and the FHL algorithm is an elegant way to implement this for improving sampling efficiency and quality. The paper is well-motivated, the method is clearly explained, and the empirical results are compelling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Novel idea of incorporating zeroth-order energy information into sampling algorithms like HMC, which typically only use gradients. This can help address issues like instability and metastability.

2. The Follow Hamiltonian Leader (FHL) algorithm is an elegant way to exchange both energy and gradient information across parallel sampling chains in a principled manner.

3. Thorough experimental evaluation across synthetic examples illustrating the identified challenging scenarios of instability, metastability, and pseudo-stability.

4. Promising results showing improved sampling quality over baselines for energy-based generative models on real datasets like CLEVR.

5. Clear motivation and well-explained methodology.

Weaknesses:
1. It would be better to show exploration of the sensitivity to key hyperparameters like the number of parallel sampling chains.

2. Discussion of computational cost/overhead compared to baseline sampling methods are missing in the manuscript.

Limitations:
Based on the provided paper, the authors do not appear to have explicitly discussed the limitations or potential negative societal impacts of their work. The paper is primarily focused on presenting the technical details of the proposed Follow Hamiltonian Leader (FHL) sampling algorithm and its empirical evaluation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
egGFHCFPiU;"REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
PnSTlFUfcd;"REVIEW 
Summary:
The paper proposes an online shielding approach for safe RL that does not assume prior knowledge of environment dynamics and utilizes finite-horizon model checking with learned approximations of the environment dynamics. It specifically focuses on RL with regular safety properties provided as a PCTL formula. The authors present a framework that dynamically identifies unsafe actions and deploys a safe backup policy when necessary. The main technical contributions of the paper are:

- Definition of a constrained RL problem based on regular safety properties.
- Presentation of model checking algorithms to verify finite-horizon satisfaction probability.
- Development of sample complexity results for statistical model checking procedures.

The novelty of the paper lies in its approach to reinforcement learning with regular safety properties without requiring prior knowledge of environment dynamics. Unlike traditional shielding approaches that need full environment models or simulators, this framework uses learned approximations and finite-horizon model checking. The authors represent the synthesis problem as finding an optimal policy under a constraint that the resultant product Markov chain (from the policy) and the DFA (from the safety property) has probability $\leq p_1$ of violating the safety property in finite horizon $H$. The authors make use of the CMDP formulation to separate the reward and safety considerations by treating the safety property as a cumulative constraint in the CMDP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this is a very strong submission and very clearly written. The authors present their problem statement precisely and also compare against many other related settings. This is challenging to do in safe RL since it is such a wide field with many parallel approaches, but I feel the authors did a commendable job here, especially in demonstrating how related and alternative formulations can be represented in their setting. 

This is a very interesting combination of shielding from temporal logic specifications with a CMDP formulation. Especially, as the framework allows for regular safety properties instead of just invariant properties, it allows for quite general specifications. 

The authors also do a good job in showing the generality of the work as it pertains to levels of model knowledge.

Weaknesses:
The main weakness I can see in this paper is perhaps a lack of experiments in settings with more complex models and safety specifications. Especially settings in which safety and optimality of the reward are in conflict, it would be interesting to see how the agent is able to achieve a trade off. 

Another consideration is perhaps motivation of using a CMDP as a framework for safe RL. I don't see this as well motivated in the paper. There are several approaches to safe RL that use MDPs and are able to use probabilistic model checking type techniques to give guarantees for cost thresholds.

Limitations:
There is some discussion of limitations in the section before the conclusion. I agree with the authors on the downsides of separating reward and safety.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new safe RL approach, building on safety shields.
The idea is to leverage model-checking techniques during the RL training to block actions that are identified as unsafe in the shield and use a learned backup policy if this is the case. In contrast to previous approaches, the ""meta-algorithm"" presented does not require an a-priori known model of the safety aspects of the environment. The approach comes with theoretical guarantees on the satisfaction of finite-horizon PCTL specifications and the evaluation assesses the potential of the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I'd first like to pinpoint that the paper is quite clear and very well written.

The proposed approach has several key advantages. First, it doesn't necessarily require providing a shield beforehand, in contrast to previous work. Second, the properties the agent needs to enforce during training are specified via PCTL, a well-established specification formalism that is not prone to exponential blow-up in the size of the automaton for translating the formula (as is the case for LTL). Third, the approach comes with guarantees on the shield: (i) when the hyper-parameters of the optimization procedure are well-chose, one can bound the probability of failure of the system from the initial state, (ii) the optimal policy found under the PCTL constraint is ensured to be a feasible policy for standard constrained MDP objectives. Finally, the authors discuss and provide guarantees under different assumptions, namely, the access to a model of the safety-relevant aspect of the environment (as in previous work), under a black-box model, and when one has access to an approximate model such that the total variation between the true and approximate transition probabilities is bounded. Additional statistical guarantees are provided for these last two assumptions. 

Experiments successfully highlight the potential of the approach.

Weaknesses:
**Beyond tabular settings.**
The main concern I have with this paper is the fact that the guarantees seem to solely hold in the tabular setting, i.e., when the state-action space is finite and tractable. Notably, in the second round of experiments, the authors use Dreamer-v3 to learn an approximate model of the environment. Although the resulting method seems to outperform constrained RL methods, the theoretical guarantees do not hold in practice. 

**On the assumptions.**
Assumption 5.2 is confusing. Indeed, when reading it for the first time, I thought one needs to have access to a *generative model*, i.e., a black box model of $\mathcal{P}$ that can be requested at any time, under any state and action, akin to the setting of [1, 2]. Specifically, this would mean that one doesn't necessarily need to sequentially execute the environment to obtain samples (as this is the case in RL), but, at any time, for any given state-action pair $(s, a)$, one could request the model to obtain a finite number of samples from $\mathcal{P}(\cdot \mid s, a)$. Note that this is not compliant with RL. When reading further, it seems that Monte-Carlo model checking only needs to produce episodes, which is fully compliant with RL. Thus, the distinction is really important here.

On another point, Assumption 5.3 looks rather restrictive. How to ensure that the approximate model learned through Algorithm 1 (line 300) yields a bounded total variation? This should be discussed in the main text. Moreover, the linked guarantees (Proposition 5.5) are not evaluated in the experiments. It could be interesting to have an example of how the statistical guarantees can be applied in practice.

[1] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Mach. Learn. 49(2-3): 193-208 (2002)\
[2] Yujia Jin, Aaron Sidford: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs. ICML 2021: 5055-5064

Limitations:
Apart from the points raised above, the limitations of the work have been successfully addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies RL with 'regular' safety properties. The constraint of safe RL is based on the satisfaction of a logic formula in probability. The action from the 'backup' policy will proactively override the potentially unsafe action from RL to ensure/optimize safety, a typical shielding mechanism in formal safe control methods. The authors demonstrated the effectiveness of their approach against CMDP and regular RL (Q-learning) in two examples.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The approach is sound, as a typical shielding method, should work well in a safe RL setting, in 2 examples shown in the paper. 
The problem studied in this paper is important.

Weaknesses:
1. Novelty. Novelty is my biggest concern for this paper. First, Problem 4.1 has already been discussed and solved in [1][2]. Second, the shielding approach is nothing new, a very standard way in formal methods. You can even trace back to simplex architecture with an advanced controller with safety back control. Third, the model-checking approach of this paper is not novel. 
2. Significance. The experiments are weak with only 2 baselines (1 as original RL, 1 as CMDP) on 2 simple examples.

[1] Wang, Yixuan, et al. ""Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments."" International Conference on Machine Learning. PMLR, 2023.
[2] Wachi, Akifumi, et al. ""Safe exploration in reinforcement learning: A generalized formulation and algorithms."" Advances in Neural Information Processing Systems 36 (2024).

You may want to extend the related works part including two papers above and more.

Limitations:
The reviewer would like to know the limitations discussion by the authors in the rebuttal phase.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an approach to online shielding for reinforcement learning agents. Namely, safety is formulated in probabilistic temporal logic with a parametric threshold as an indicator for reachability of the goal state. The proposed algorithm checks the reachability probability threshold in each state of the environment and raises a warning when the threshold is violated. A pre-trained backup policy is then proposed to be deployed which overrides the action of the agent. The approach is evaluated on tabular and visual RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper addresses an interesting and valuable problem. 

Evaluation includes visual RL benchmarks, which are interesting to provide online safety for.

Weaknesses:
The paper gives a lot of choice to the reader to compose a problem setting of their interest. It does not, however, provide precise enough approach description for each of them. This makes the contributions blurred. Described problem settings have been extensively studied before and the proposed approach does not significantly improve on them. It is also claimed that the approach can be used both during training and deployment, it is, however, not clear if the authors formulate these as two distinct settings and evaluate separately or not. In the latter case, it would be a dangerous simplification. Figures in the evaluation section are not readable.

Presentation: The presentation suffers from imprecise narrative leaving multiple questions until the evaluation section. Assumptions are introduced twice and it is not clear what exact problem the authors propose to address, or to what exact problem setting it generalizes. The use of ""etc."" and ""some other"" give the impression that more problems can be addressed than presented in the evaluation.

Minor:
- p.7: ""if need be we""
- ""don't"" --> do not

Limitations:
The approach is of limited novelty and employs existing components. Technical details in terms of the exact problem setting and guarantees are insufficient to judge the contribution. The approach is not yet placed in the larger context of online safety for RL agents, which would require more precise problem formulation and discussion of limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses safety and constraint compliance in deploying reinforcement learning (RL) systems. Such issues have triggered a vast body of research in the area of safe RL over the last few years. The paper introduces a safe RL framework for so-called regular safety properties, focusing on satisfying these properties with high probability. The paper compares and places this setup to common constrained Markov decision processes (CMDP) settings and presents a meta-algorithm with provable safety guarantees to prevent violations of regular safety properties during training and deployment. The approach is evaluated in both tabular and deep RL settings.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Safe RL is of utmost importance in order to enable RL agents to be successfully deployed in the real world. This paper tackles an important problem: How to ensure safety if the safety criteria go beyond standard reward or simple reachability features. The motivation of the paper is well done, and the placement in the literature is mostly complete and extensive.

Weaknesses:
While this paper addresses an important research question, I feel it is not ready for publication at a major ML conference. I will first summarize the issues that I see and then elaborate in more detail.

1. The claims made in the intro are not properly met. This concerns particularly the claim that the approaches are most permissive in terms of prior knowledge. 

2. The evaluation is not sufficient. The experiments are very sparse and miss important information to assess the quality of the research.

3. The paper is packed with formal definitions, most of them being standard in the formal methods community. Conversely, little space is spent on describing the methods in detail and providing a proper evaluation.

I overall feel that the paper aims too high and wants to solve all kinds of aspects in model-based safe RL with shields. It would have been better to pick certain key aspects more clearly. As an example, the paper claims to 
- not need a model
- to learn a model
- to use most of the available model checking paradigms (and even introduce them)
- evaluate with tabular and deep RL

Many of these aspects are then not properly discussed. 

I will now comment on the previously listed weaknesses.

1. In the introduction, the authors claim to operate in a most permissive setting where environment dynamics are not known. Yet, then, they introduce three model-checking paradigms, which are essentially standard numerical model checking, Monte Carlo-based statistical model checking, or model checking with approximate models. In particular, the last version is shady. What is a guarantee for a learned model? How can a claim be made for a most permissive setting but still obtain hard guarantees when, as a consequence, all guarantees rely effectively on statistics or a learned model? Other approaches are simply very clear about their assumptions, such as knowing a model. The model learning procedure is standard, and if the whole 'permissiveness' of the setting depends on the fact that a model can learned, I do not see a novel approach here. 

Very importantly: While a model is learned, no safety guarantees can be given, defying the notion of shielded RL. This aspect is not discussed in the paper. A step further, even the safe fallback policy that is explained in Section 6 relies on learning. 

2. The evaluation only considers a few environments and compares a simple tabular and deel RL agent. What I would have liked to see is how the different assumptions and model checking paradigms affect the learning, the safety, the performance. I believe, with a thorough evaluation in this direction, the paper would be much stronger. If we can believe that it is feasible to learn a model, then I would like to see a comparison between the strong assumption of knowing the model, and having learned a model, and what the effects on safety guarantees are. 

3. The contributions only start at page 5, and are then still interleaved with standard definitions. It is important to have a paper safe-contained, but, as an example, why is it necessary to introduce both LTL, PCTL, DFA in detail? And then, the evaluation even uses PCTL^*. To me this seems as if the space had been filled up with long definitions, while it should have been used more on the contributions of the paper. 

Generally, I feel the paper follows a great direction, especially investigating Shielded RL with a learned model. With a stronger evaluation and emphasis of this part, the research and contribution could be much improved in my opinion. 

Minor comments:

- proposition 4.2 is obvious and seems like an overformal statement of a simple fact. 

- Def 4.3. Make clear that this is reward engineering

- Non-Markovian cost: Compare to reward machines

- The tradeoff between safety and exploration has (for instance) been investigated in 

Carr et al.: Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2023.

- Learning the model, proposition 5.5: I think the topology (graph) of the model needs to be known to estimate the probabilities.

Limitations:
The limitations of the work in terms of assumptions and their real-world relation are not properly explained in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
rC0OM4Jm4v;"REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
dkpmfIydrF;"REVIEW 
Summary:
The authors propose a method for robust (against adversarial attacks) concept-erasing for latent diffusion models. Specifically text-to-image diffusion models.

The main contributions are:
1. Integrating adversarial training into machine unlearning by modifying it as a bi-level optimization problem.
2. Contrary to the conventional techniques where only the parameters of the UNeT is updated, the authors show that updating the text encoder can effectively maintains a robustness-utility trade-off.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of robust-concept erasing is relatively new and important to handle.
2. The authors have formulated the task clearly and proposed a solution to address this important problem.
3. The initial results on text-based attacks (UnlearnDiffAttack) demonstrate an interesting direction.

Weaknesses:
Lack of Sufficient Evaluation:
1. The authors majorly evaluate the robustness of the method against only the UnlearnDiff attack, which is a text-based attack.
2. They have not extended their evaluations to other recent peer-reviewed SOTA attacks such as the CCE (ICLR-24) [1] and RAB (ICLR-24) [2] attacks. 
3. Specifically, CCE is a strong inversion-based attack that introduces a new token into the vocabulary. CCE relatively takes much less time than the UnlearnDiffAttack. 
4. GIVEN CCE ATTACK INVERTS THE ERASED CONCEPT INTO A NEW TOKEN, THE AUTHOR'S CLAIMS OF UPDATING ONLY THE TEXT-ENCODER FOR A BETTER ROBUSTNESS-UTILITY TRADEOFF CAN BE CALLED INTO QUESTION.
5. In addition to the UnlearnDiffAttack, the performance of AdvUnlearn on CCE would be the right benchmark to assess the model's robustness.

Lack of Completeness:
1. The authors have not presented the quantitative erasing results of AdvUnlearn for any of the unlearning scenarios. In addition to the robustness and utility performance, studying the erasing performance of the method is important to understand whether optimizing for the former two properties has any compromise on the latter's performance.

References:
[1] https://arxiv.org/pdf/2308.01508 (Circumventing Concept Erasure Methods For
Text-to-Image Generative Models)
[2] https://arxiv.org/pdf/2310.10012 (RING-A-BELL! HOW RELIABLE ARE CONCEPT REMOVAL METHODS FOR DIFFUSION MODELS?)

Limitations:
1. The authors have discussed the limitations of the proposed method.
2. However, as highlighted in the ""weakness"" section, the author's claim of robustification through updating only the text-encoder cannot be verified unless evaluated against other adversarial attacks from the literature such as CCE and RAB.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces AdvUnlearn, a robust unlearning framework that integrates adversarial training into diffusion models to enhance concept erasure. This method aims to prevent DMs from generating harmful or inappropriate content, such as nudity, even under adversarial prompt attacks. AdvUnlearn focuses on optimizing the text encoder rather than the UNet, achieving a balance between effective concept erasure and high image generation quality. Extensive experiments demonstrate AdvUnlearn's robustness and utility across various unlearning scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Enhanced Robustness:
AdvUnlearn improves the robustness of diffusion models against adversarial prompt attacks, effectively preventing the generation of undesired content.

2. Maintains Image Quality:
By focusing on the text encoder and utilizing utility-retaining regularization, AdvUnlearn preserves high image generation quality post-unlearning.

3. Plug-and-Play Usability:
The method allows the robust text encoder to be shared across different DMs, making it easy to implement and enhancing its practical usability.

Weaknesses:
1. ASR needs to be measured based on more baseline attacks. Since the proposed method is based on the CLIP text encoder, incorporating CLIP-based adversarial attack methods [1] and [2] would provide greater understanding.


[1] Wen et al., Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery\
[2] Mahajan et al., Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models

Limitations:
1. More discussion comparing the proposed method with CLIP-based attacks [1] and [2] would be welcomed.

[1] Wen et al., ""Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery""\
[2] Mahajan et al., ""Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models""

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes AdvUnlearn, a method aimed at enhancing the robustness of concept erasing. The approach integrates the principles of adversarial training into the unlearning process. Specifically, the text encoder is identified as the most suitable module for robustification. Experiments are conducted on eight DM unlearning methods, including nudity unlearning, style unlearning, and object unlearning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1, The problem addressed in this paper is significant. Enhancing the robustness of concept erasing could substantially reduce safety risks and copyright violations.

2, The idea is intuitive and reasonable. Adversarial training can be used to enhance robustness, and the paper identifies a straightforward method to preserve image quality while focusing on the text encoder as the most suitable module for robustification.

3, The experiments are relatively thorough, including eight unlearning baselines and various concept-erasing tasks.

Weaknesses:
The experimental section lacks a sufficient number of attack methods.

The introduction to C_{retrain} is inadequate.

Limitations:
The limitations have been discussed in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
lxuXvJSOcP;"REVIEW 
Summary:
The paper proposes an Unified Domain Generalization and Adaptation(UDGA) scheme for multi-view 3D object detection. The main components of the proposed method are (1) depth inconsistency-based constraints from multi-view and (2) an efficient domain adaptation scheme (LEDA). The multi-view depth inconsistency constraints improve the domain gap caused by geometric gaps when it comes to perspective view changes. LEDA offers adaptation with fewer amounts of labels while preserving the knowledge from the source domain.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper clearly addresses which problem it focuses on, and each proposed module has solid goals to address existing limitations. The experimental results from multiple datasets also seem convincing.

Weaknesses:
I have a few concerns about the demonstration of the proposed method as well as addressing existing works.

(1) The method part needs to be improved. For instance, specifying every input and output tensor dimension (i.e., depth estimation network, etc.) would significantly help readers understand what each module is doing more quickly. 

(2) Using the depth inconsistency seems similar to DETR3D's[1] inconsistency constraint on RGB features. In DETR3D, it is even mentioned that using the RGB feature is better than having explicit depth estimation.
Based on this, the depth estimation network can introduce additional parameters for optimization while not being very helpful. I cannot see experiments or comparisons with DETR3D. 

(3) More related works need to be addressed. Currently, the paper focuses on multi-view-based domain generalization for 3D detection. However, there is also another active line of research with LiDAR-based domain adaptation for 3D detection, pioneered by ST3D[2]. Clearly addressing the difference between those researches will make the reader recognize the line of research that the paper focuses on. 

(4) The effectiveness of LEDA is not well demonstrated, as claimed in the paper, compared to existing approaches. 

[1] Detr3d: 3d object detection from multi-view images via 3d-to-2d queries, Wang, Yue, et al. CORL, 2022

[2] St3d: Self-training for unsupervised domain adaptation on 3d object detection, Yang, Jihan. et al., CVPR 2021.

Limitations:
The limitations are addressed

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper is about Unified Domain Generalization and Adaptation for outdoor 3D object detection. To address the geometric misalignment between the source and target domains, Multi-view Overlap Depth Constraint that leveraging the strong association between multi-views and Label-Efficient Domain Adaptation are proposed. Experiments are done with several cross-dataset settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The domain generalization and adaption is important for 3D object detection.
2. The writing is easy to follow.

Weaknesses:
1.	In the past works, the comparative methods, such as DG-BEV, and PD-BEV, reported the performance on both source and target domains. Yet it seems that the reported metrics in Table 1 do not align with the reported results in the two papers. What is the reason for such changes?
2.	It is noteworthy that the NDS is specific for nuScenes and different for other datasets. It is important to discriminate such differences.
3.	The authors claim that this work is toward 3D universal detection, yet there are other works toward 3D universal detection, such as [1][2]. It is better to illustrate the differences over these methods, both in discussion and experimental comparison.  Besides, there are other works devoted to multi-dataset training for 3D detection for generalization, also missed in the paper. Can the proposed method be applied in multi-source training and multi-target testing?
4.	The Domain Adapter design seems simple, yet the motivation and potential of such design is missed. It is important since it  is  one of the two components of this work.
[1] Towards universal LiDAR-based 3D object detection by multi-domain knowledge transfer. ICCV 2023.
[2] Cross-Dataset Sensor Alignment: Making Visual 3D Object Detector Generalizable. PMLR 2023.

Limitations:
The limitation part has been addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents an adaptation of 3D object detectors to varying target environments using two major strategies. The proposed multi-view overlap depth constraint leverages associations across views to learn view-invariant features. Additionally, a LORA-like structure is designed for parameter-efficient adaptation, accommodating scenarios with limited target data. Experiments on three benchmark datasets demonstrate the effectiveness of the proposed approach, with minimal modification of parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The paper addresses a significant and practical issue in 3D object detection, aiding in the development of robust models for dynamically changing testing environments.
+ The flowchart clearly illustrates the core components, making it easy for readers to grasp the main idea.
+ Extensive experiments have been conducted, and the proposed strategy significantly outperforms the pre-trained source model and even surpasses full fine-tuning strategies.

Weaknesses:
- The two proposed strategies appear somewhat disconnected. While the multi-view idea is interesting, the technical details are confusing. Equation (5) includes three different objectives, but their importance or sensitivity is not discussed. The latter adaptation strategy resembles existing works and lacks specific design for the 3D detection task. This is also not correlated with the view-transform. The isolated adaptation modules seem more like a stacking of existing techniques rather than a cohesive addition.
- More discussion and comparisons with previous multi-view augmentation strategies would help clarify the merits and innovations of the proposed approach. The current claim that existing strategies are poorly generalizable is somewhat unconvincing.

Limitations:
The authors have not provided a discussion on the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on multi-view 3D object detection. The authors proposed a unified domain generalization and adaptation-based detection method. To enhance the detection model for unseen datasets and address the geometric misalignment problem, the authors proposed a multi-view overlap depth constraint module and a label-efficient domain adaptation technique. The comprehensive experiments on lar-scale datasets, including nuScenes, Lyfy and Waymo have demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The storyline is clear, the authors provided detailed motivation analysis in the introduction and present their innovation clearly.
- The performance is strong. The method shows SOTA performance in variance benchmarks with fast speed.
- Most of the figures are clear. This paper is also well-written.

Weaknesses:
- There is no source code for review.

Limitations:
Please refer to the above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Hc2ZwCYgmB;"REVIEW 
Summary:
This work presents a zero-shot face-generation method based on diffusion models. The proposed method first extracts face features using Face2Vec and trains a network to map these features into the textual space (i.e., the prompt embedding space for diffusion’s text condition). The main difference from existing zero-shot face generation approaches is that, instead of adding conditions in the denoising UNet, the presented method incorporates the condition into the textual embedding. To prevent the subject information from overwhelming the generation (e.g., preserving only the subject ID while ignoring other descriptions), the authors propose a Composition Distillation Loss. This contrastive loss encourages the model to generate descriptions other than the subject information.

The main shortcoming of the paper lies in the experimental validation. There are multiple components proposed, but their effectiveness is not adequately demonstrated. Additionally, both qualitative and quantitative evidence fail to show that the proposed method outperforms SoTA methods.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The Compositional Distillation Loss is an interesting and intuitive approach to reducing the problem of subject information overwhelming the generation

Weaknesses:
* There is a lack of intuition behind using multi-timestep distillation. This approach can cause accumulated errors, and there is no evidence provided to support the benefits of adopting such a strategy.
* The section on dynamic model expansion is very unclear. It is not specified which part of the model is expanded or if the tokens are simply replicated. While adding Gaussian noise to replicated tokens is mentioned, there is no empirical validation of its effectiveness.
* The qualitative comparison, especially in Figure 7, does not support the claim that the proposed method has advantages over baselines like PuLID. Additionally, the benchmarking results in Table 1 show limited improvement in facial identity preservation, and the text alignment can be inferior to PuLID. Thus, the experiments are not comprehensive enough to convincingly demonstrate that the proposed approach is superior to existing methods like PuLID.

Limitations:
The authors have discussed the limitations.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AdaFace, a face encoder that maps facial features from the image space to the text space through the AdaFace Prompt Inverter, utilizing the structure and pre-trained weights of the CLIP text encoder for initialization. During the face distillation phase, AdaFace employs random Gaussian face embeddings and multi-timestep distillation, enhancing the model's ability to capture subtle facial details through dynamic model expansion. In the composition distillation phase, AdaFace uses a comparative learning loss, aligning feature increments with orthogonal subtraction, while introducing an elastic face preservation loss to address the misalignment of facial features caused by different prompts.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written and easy to follow
- The proposed method requires fewer training resources
- The approach to constructing contrastive pairs during the composition distillation stage sounds reasonable

Weaknesses:
- I tried the demo provided by the authors, and the ID similarity on a few test images was relatively low; it should be far from the state-of-the-art (SOTA) level of ID similarity claimed in the paper.
- The test dataset consists of celebrities, which does not guarantee whether these IDs have appeared in the training set. Furthermore, the number of test samples is too small to be convincing.
- The upper bound of ID fidelity is constrained by the frozen Face2Image model, in this paper, Arc2Face.
- The proposed improvements like Random Gaussian Face Embeddings, Orthogonal Subtraction, etc. are not effectively validated through ablation study.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a method for personalizing text-to-image diffusion models for human faces. At its core, it learns a prompt inverter that maps face embeddings from a pretrained face encoder to the text embedding space of diffusion prompts. It leverages various components including face distillation, composition distillation and elastic face preserving loss to preserve subject identity while attaining good compositionality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper designs targeted training losses and regularizations for the task at hand. The explanation of the methods is detailed. The video qualitative results show improvement over ID-Animator.

Weaknesses:
*  The quantitative metrics do not show a clear advantage of AdaFace over other existing personalization methods like PuLID. The number of qualitative examples for comparing with those methods is also limited -- just the 5 images per method in Figure 7, and not sufficient to clearly demonstrate that AdaFace outperforms existing methods. It would be helpful to show a larger number of uncurated examples comparing AdaFace and baselines to get a better comparison of their performance.
* The training of the prompt inverter involves a number of components -- such as model expansion, the inclusion of different feature types in composition distillation, orthogonal subtraction and elastic face preserving loss -- but there are no ablation studies on most of them to demonstrate their effects on the performance.

Limitations:
The authors have discussed limitations and societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a test-time-tuning-free method for personalized text-to-face-image generation. Previous methods involving face features in the feature space of a face encoder, which is not flexibly composable with natural language for personalized generation. Thus, this paper proposes to map the face features into the features in the text conditioning space. Several techniques are proposed to enhance the performance, like Random Gaussian Face Embeddings, Multi-Timestep Distillation, Dynamic Model Expansion, Composition Distillation, and Elastic Face Preserving Loss. Some experiments demonstrate that the proposed method achieves good visual results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The visual results are satisfactory in general.
2. The proposed method can also be applied for personalized text-to-video generation.

Weaknesses:
1. The overall motivation is not novel enough. Finding ways to convert input images into the textual space is a fundamental goal in text-to-image personalization, which has been emphasized in the very first TextualInversion work. Even in the context of tuning-free based methods, the proposed framework is not so novel compared with ELITE [a], which also involves training a mapper from the image space into the textual space. Similar compositional distillation technique has also been explored in SuTI [b].
2. Lack of detailed studies of the proposed components, either qualitatively or quantitively. The authors propose a bag of techniques to improve the performance. Although their motivation is mentioned in the texts, there is no supportive results to illustrate how these techniques work. 
    * There is only one quantitive study in Tab. 1 regarding the compositional distillation. However, there are actually a lot of technical details in the proposed compositional distillation techniques, like the orthogonal subtraction and compositional delta loss, which lack careful experimental analysis against their alternatives. 
    * The proposed face distillation is not well supported. Can we simply train the face encoder with the simple noise prediction loss of diffusion models?
    * The analysis of the proposed Elastic Face Preserving Loss is also missing.
3. ELITE [a] mentions that using multiple token to represent an image may hurt the textual compatibility. It is necessary for the authors to provide a rationale for doing so.
4. How about the method comparing with the popular IP-Adapter (face version) [c]?
5. The overall training pipeline requires multiple stages of training, which is not so elegant.
6. The authors would like to consider merging multiple figures with similar functionalities and structures to one, like Figs. 2, 3, 4 and 5, to leave enough space for necessary experimental results.

[a] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation, Wei et al., ICCV 2023.

[b] Subject-driven Text-to-Image Generation via Apprenticeship Learning, Chen et al., NeurIPS 2023.

[c] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models, Ye et al..

Limitations:
The authors have discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8ihVBYpMV4;"REVIEW 
Summary:
This paper designs two self-consistency methods for formalizing mathematical statements in natural language into Isabelle --- symbolic equivalence and semantic consistency. In particular, symbolic equivalence measures whether the formalized statements are equivalent (judged by the Isabelle’s built-in automated theorem provers), and semantic consistency measures the similarity of the natural language translation of the formalized statement and its original statement (In other words, semantic consistency measures whether the formalized statement keeps the semantic meaning.) Tested with various base models, this paper shows that the combination of the two proposed methods improves the success rate of the autoformalization process.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
-	This paper studies an interesting topic, self-consistency for autoformalization, which is non-trivial because the target is not unique. To the best of my knowledge, this is the first paper to study the self-consistency methods for autoformalization.
-	Figure 2 shows that self-consistency methods for autoformalization have great potential because the pass@k accuracy keeps increasing with more trails.

Weaknesses:
-	The major weakness of this paper is the scope of the autoformalization --- the symbolic equivalence method only applies to the formalization of theorem statements instead of their proofs. Autoformalization of theorem statements is an arguably less significant problem since it cannot be used to verify any informal proofs.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper targets the task of autoformalization for mathematics, which inputs an informal theorem statement (in English) and outputs a formal theorem statement (inside an interactive theorem prover). Prior work has used LLMs in-context to produce autoformalizations. The work is motivated by a disparity between top-1 accuracy and top-k accuracy when sampling formalizations from LLMs. The proposed method works atop any generative model of formalized statements. Their approach is to refine the k samples to a final choice via a voting mechanism. The approach is twofold - first a symbolic equivalence method uses automated theorem provers (ATPs) to check for equivalence between two formalized statements. The k-samples are then clustered by equivalence, and then they perform semantic consistency: measuring the embedding similarity between the original informal statement and the informalized formal statement. They indicate improvements over the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to compare formalization candidates via symbolic equivalence is interesting. It is a nice strategy to impose additional structure on the k-autoformalization samples. The use of the semantic consistency is a natural extension of the distilled backpropogation idea presented in (https://arxiv.org/abs/2302.12433). While measuring alignment with the informal statement is not automatable, the symbolic equivalence is a nice intermediate which I believe will be further explored in the future.

2. The results indicate that their approach is effective. As the approach can be placed atop any autoformalization strategy sampling > 1 candidates from a generative model, the idea is generally applicable and should be useful for all such future work.

Weaknesses:
1. The semantic consistency alone does not appear to perform much better, I wonder if this is due to the choice of BERT for the embeddings. Perhaps you can use a newer model to judge its efficacy?

2. The formulae for combining the semantic consistency score do not seem to have any motivation for the particular choice of $f(x) = x, x^2, \log x$. Can the authors explain the rationale for these choices?

3. The definition of $\tau$-semantic consistency is not used elsewhere. In particular, it is unclear the purpose of the incorporation of $\tau$ as it is not mentioned again.

Limitations:
I believe the authors have adequately addressed the potential limitations of their work in the limitations section in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces two methods metrics, symbolic equivalence and semantic consistency, to determine the equivalence relation between formal statements. They design novel technical solutions to measure each method and show that this improves autoformalization by allowing self-consistency-based clustering.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper addresses an important and novel problem of improving autoformalization with more test-time compute.
2. The methods used are very intuitive and easy to understand.
3. The evaluation is sound and shows good performance gain.
4. The writing is clear and easy to follow.

Weaknesses:
Since the authors investigated 5 models of different sizes, I wish they investigated the models' performances with the same amount of test-time compute. I.e., given the same amount of FLOP (as measured by cost, since Codex and GPT-4 did not reveal their parameter count), which model is preferred. This helps the reader understand which model they should choose for their workload.

Limitations:
In the conclusions, the authors did mention the things they wish to leave for future work, but it feels somewhat vague. It would be good if the authors can be more concrete and lay out future ideas they'd like to try or would like the community to carry out.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel framework aimed at enhancing the accuracy of autoformalization in mathematics using LLMs. It addresses the challenge of translating natural language mathematical statements into formal language by introducing a two-pronged approach based on symbolic equivalence and semantic consistency. Symbolic equivalence leverages automated theorem provers to ensure logical homogeneity among autoformalization candidates, while semantic consistency evaluates the preservation of the original statement's meaning by comparing embeddings of the re-informalized text and the original. The framework scores and selects the best result from multiple autoformalization candidates, significantly improving accuracy across various LLMs and datasets like MATH and miniF2F. The extensive experiments demonstrate the synergistic effect of the two consistency methods, achieving relative improvements in autoformalization accuracy and reducing the need for manual verification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Clear Presentation and Structure: The paper is well-organized, with a nice logical flow. I really like the figures and tables in the paper.
2. Open Source: The authors submit the implementation of the proposed methods, which is beneficial for the research community.
3. Innovative Framework: Introduces a new approach that combines symbolic equivalence and semantic consistency to improve the accuracy of autoformalization. Symbolic equivalence and semantic consistency are mutually complementary, with their combination further enhancing performance.
4. Comprehensive Evaluation: The authors conduct extensive experiments on two mathematical datasets, MATH and miniF2F, demonstrating the efficacy of the proposed methods.
5. Wide Applicability: The proposed framework is effective across various model sizes and is not limited to a specific LLM, indicating broad applicability.

Weaknesses:
This paper makes a valuable contribution to the field of formal mathematics. While I'm new to autoformalization and not an expert in this area, I believe the paper is well-written and presents a strong argument. However, due to my limited experience, my confidence level is relatively low as I might have missed some key points.

Limitations:
The authors adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
1URMG6B3WW;"REVIEW 
Summary:
This paper introduces the KrwEmd algorithm, a novel approach to hand abstraction in Texas Hold’em-style games. The main contribution is the integration of historical information using K-recall winrate features and earth mover’s distance, addressing the limitations of previous imperfect recall abstraction methods. The algorithm demonstrates significant performance improvements over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of historical information via K-recall win rate features enhances the accuracy and reliability of hand abstraction.
2. KrwEmd significantly outperforms existing methods, showcasing its practical value and potential for real-world applications.
3. The algorithm is technically sound, with strong experimental validation supporting its claims.

Weaknesses:
Scalability: While the paper demonstrates KrwEmd's effectiveness in Texas Hold’em-style games, its scalability to more complex and diverse game scenarios remains to be explored.

Comparative Analysis: The paper could benefit from a more detailed comparison with existing hand abstraction methods to better highlight KrwEmd's unique advantages and limitations.

Limitations:
The paper primarily focuses on Texas Hold’em-style games. While this is a significant achievement, a discussion on the model's scalability and generalization to more complex and varied game scenarios would be beneficial. Including potential strategies to address these challenges would strengthen the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to hand abstraction in Texas Hold'em-style poker games, addressing the limitations of current methods that often disregard historical information. The authors make two primary contributions: First, they develop KRWI (K-Recall Winrate Isomorphism), a new abstraction method that incorporates historical information from previous game phases. Second, they present KrwEmd, the first hand abstraction algorithm to effectively combine K-recall win rate features with earth mover's distance for hand classification. Through experiments conducted in the Numeral211 Hold'em environment, the authors demonstrate that KrwEmd significantly outperforms state-of-the-art algorithms such as Ehs and PaEmd in terms of exploitability, while maintaining the same number of abstracted information sets. This work shows that incorporating historical information can substantially enhance the performance of hand abstraction algorithms, potentially leading to more advanced strategic computation in large-scale adversarial games and stronger poker AI systems.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* Overall, the paper provides a new technique that is promising for an important area of research
* The results indicate strong improvements over alternative methods

Weaknesses:
For me, the paper's primary weakness is the presentation method. I had trouble understanding the significance and nature of the contribution from the current submission. In general, a clearer description of this area of research for people who, e.g., work on games but don't focus on poker would be quite helpful. Some specific suggestions/areas for improvement are:

* Clearer introduction of key concepts: The paper jumps into technical terms like 'imperfect recall abstraction' and 'hand abstraction' without adequately explaining them for a broader audience. A brief explanation of why these concepts are important in poker AI would be beneficial.
* More intuitive explanations of the algorithms: The descriptions of PWI, KRWI, and KrwEmd are highly technical. Including some simple examples or diagrams to illustrate how these algorithms work could greatly improve understanding.
* Better contextualization of the contribution: While the paper claims to outperform existing methods, it's not clear how significant this improvement is in the broader context of poker AI. A discussion of the practical implications of this improvement would be valuable.
* Clarification of experimental setup: The Numeral211 Hold'em environment is not well-known. A clearer explanation of how this relates to standard poker variants would help readers understand the relevance of the results.
* More accessible presentation of results: The graphs and tables are dense with information but lack clear explanations. Simplifying these visualizations or providing more guidance on how to interpret them would be helpful.
* Glossary of terms: Given the many technical terms used (e.g., 'earth mover's distance', 'K-recall winrate feature'), a glossary could be a valuable addition to help readers keep track of these concepts.

Limitations:
There's limited discussion of limitations. It would be good to include an explicit limitations section. In particular, it would be good to discuss potential computational challenges in more detail, any limitations on the scope of the evaluation, and future work that might be planned.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of hand abstraction for Texas Hold-Em style poker games. Hand abstraction is the process of partitioning game histories into infosets which still contain enough information to make strategically advantageous decisions. Previous approaches have focused on abstractions that primarily focus on the future outcomes from each hand, but the authors suggest that it may instead be beneficial to also include past information. They design a hand abstraction algorithm called KwrEmd that outperforms previous work by incorporating historical information.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The results seem to show that KrwEmd outperforms other imperfect recall hand abstraction algorithms in terms of exploitability.

Weaknesses:
As somebody who is unfamiliar with the subfield of imperfect recall abstraction, I found the paper to be quite confusing throughout. The authors do not often provide intuition or examples for their method, and I found it difficult to tell exactly which contributions were novel compared to Fu et al. While it's reasonable for a paper to use technical language at times, well-written papers are usually understandable by a broader set of readers than just those in the specific subfield.

Limitations:
Not sure.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes KrwEmd, a novel hand abstraction algorithm for imperfect recall settings in Texas Hold'em poker. The algorithm leverages K-recall winrate features, incorporating historical information in addition to future information for constructing hand abstractions. The authors introduce two new isomorphism frameworks: Potential Winrate Isomorphism (PWI) and K-recall Winrate Isomorphism (KRWI). They demonstrate that KRWI outperforms existing methods like POI in identifying distinct infosets. KrwEmd, which combines KRWI with Earth Mover's Distance (EMD) for hand classification, shows superior performance compared to POI, Ehs, and PaEmd in the Numeral211 Hold'em environment.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
*Originality*: The paper presents a novel combination of K-recall winrate features and EMD for hand abstraction in imperfect recall settings, addressing a critical limitation of current approaches that solely rely on future information.
The introduction of KRWI and PWI provides valuable new tools for understanding and constructing hand abstractions in poker AI.

*Quality*: The experimental results in the Numeral211 environment demonstrate a clear improvement over existing methods, supporting the claims of the paper. The paper includes an appendix with algorithm details and supplementary experimental data.

*Significance*: The proposed KrwEmd algorithm advances the state-of-the-art in hand abstraction for imperfect recall settings, offering a potentially significant improvement for developing stronger poker AI agents.
The incorporation of historical information is a valuable contribution that benefit positively future research in poker and other imperfect information games.

Weaknesses:
I found the paper  challenging to understand, though this may be due to my limited background knowledge in poker AI and game theory.  While the authors provide a background section, the density of the technical content and the numerous specialized terms make comprehension difficult.

The description of the accelerated algorithm in Appendix A.3 could be expanded for better understanding. Additionally, a clear discussion of the limitations of the accelerated algorithm would be beneficial.

The paper provides limited information about the proposed algorithms, particularly KrwEmd. While the core concepts are presented, the details regarding implementation and specific design choices are limited. More in-depth explanation and pseudocode would enhance the paper's quality.

Limitations:
The authors acknowledge the computational complexity of clustering with EMD and introduce an accelerated algorithm. However, the paper lacks a dedicated section addressing the limitations of the proposed methods and the accelerated algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops new hand abstraction techniques for Texas Hold'em-style games (in general: games with ordered signals), which fare better than previous methods in both the number of hands identified, and performance (exploitability) in a simplified version of the game. 

Hand abstraction is a technique aiding the strategy construction in a Texas Hold'em, where concrete hands, or rather concrete signal infosets (i.e. ""possible words"" according to the information revealed so far), are replaced by abstract infosets, represented in an abstract feature space (here $\mathbb{R}^n$).

The core idea of the paper is to use the features of hands from previous rounds in the construction of the current round feature. More precisely, the paper investigates a simple method (KRWI = *k-recall winrate isomorphism*) of maintaining, at a given round, the collection of all potential-winrate isomorphims (PWI) features from previous $k$ rounds by concatenating them all together. PWI for an $n$-player game is a categorical probability distribution over $n+1$ events of a form *""this player outperformed exactly $l-1$ other players while losing to none""* for $l = 1, 2, \ldots n$ and *""this player lost to at least one player""*. Those distributions can be computed by a dynamic programming method. To reduce the cardinality of the space, the paper later clusters KRWI features with k-means using the Wasserstein distance, naming it the KrwEmd method.

All of the methods are benchmarked against currently used techniques that do not use historical information. Experiments find that KRWI identifies similar proportion of signal infosets as the previously used KROI. Using the metric of exploitability of the equilibrium (how it deviates from Nash equilibrium) of the strategy found by an imperfect-information game solver, authors find that 2-RWI-based approach performs almost the same as 2-ROI, and that KrwEmd outpefrorms previously used Ehs and PaEmd by a relatively large margin.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes a reasonable extension of the currently used techniques for hand abstraction in Texas Hold'e, and shows that the new idea beats SOTA. It does not shy away from introducing the reader to the relevant background in a rigorous way (which is what made it possible for me to even start reading it). Experiments show a meaningful improvement, and provide additional insights (such as the decreasing worth of historical information).

Weaknesses:
From the perspective of someone not at all acquainted with the field of imperfect information games/games with ordered signals, the paper was quite hard to read and understand - even though (assuming that the authors agree with my summary), the contribution is a relatively straightforward idea.

The introduction was uninformative and confusing (I would recommend rewriting the whole second paragraph); the preliminaries, although presented in-depth and trying to be formal, also posed quite a few questions; sections 4 and 5 describing the main contribution lacked detail and justification (i.e. ideally I would like to see definition/theorem/proof style - otherwise the text is impossible to read for someone unfamiliar with the field), and the experimental setup is assuming a lot of background knowledge that was not explained neither in the main paper nor in the appendix (it was also difficult to gauge if the comparison between SOTA and the new approach was fair from the resources pov - the paper reports some numbers, but never an aggregated ""memory/time used"" for all methods).

Please see Questions below for a detailed explanation of what I found lacking or hard to understand.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Ib2iHIJRTh;"REVIEW 
Summary:
In the paper the authors develop a generative model for global climate simulations. By combining the DYffusion framework for generative modeling with Spherical FNOs that respect the spherical earth geometry the resulting model has stable rollouts for 10-year climate simulations. Experiments show that the resulting model achieves more accurate climate simulations than existing machine learning baselines and the sampled ensemble reproduces meaningful climate variability. Due to the DYffusion framework the generative modeling is achieved without a massive increase in the number of forward passes required to sample a state trajectory.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. ML has a high potential for impact in the development of climate models. As outlines in the paper, ML has seen great success in medium-range weather forecasting and we are starting to see this also translate to climate modeling. This paper significantly pushes this development forward in bringing important advances in terms of both model accuracy and ensemble modeling.
2. The paper is well written and relatively easy to follow, despite containing combinations of multiple complex components (DYfussion, SFNOs, climate modeling). The authors do a good job at making climate modeling concepts accessible to a wider ML audience.
3. The proposed method of combining SFNOs and DYfussion is well-motivated by requirements on stable rollouts + sampling speed + ensemble modeling.
4. The evaluation includes relevant analysis, baselines and metrics.

Weaknesses:
1. The stochasticity in the ensemble comes from MC dropout + Stochastic depth in the inference network. There is no clear motivation for this choice. 
    1. While MC dropout is an easy way to introduce stochasticity, it is generally known to not represent the true uncertainty very accurately. It is unclear to me if this is a suitable choice for representing the full distribution of possible interpolated states. On the other hand, the SFNO network with MC dropout here sits inside the DYfussion framework. It is now obvious to me how crucial it is for interpolation network to accurately represent the distribution of the atmospheric state for the final ensemble to match the distribution. Perhaps the authors can shed some light on this.
    2. While MC dropout is used in the original DYffusion framework, it is unclear why Stochastic depth was added here. It would be valuable with an ablation to see the importance of MC dropout vs Stochastic depth.
2. The ability of the ensemble to accurately capture climate variability is mainly evaluated in Table 2. In this table I am missing the same spread computed for the (generative) baselines. It is somewhat hard to put the spread of the proposed method into context without comparing to the baselines.

Limitations:
The authors extensively and accurately discuss the limitations of the work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a method for approximating a physics-based climate model with a faster data-driven model. To make the model probabilistic the method uses diffusion models (specifically DYffusion). Predictions over longer timescales are generated using autoregressive rollouts. Spherical Fourier Neural Operators (SFNOs) are used to handle the non-euclidean geometry of the earth. Training is performed on a large climate simulation (FV3GFS). 

In experiments, the method is compared to a set of alternative approaches. Each method is used to generate an ensemble of climate simulations over 10 years, and the statistics of these runs can be compared to a similar ensemble from the physics-based model. Comparison metrics include the bias and variability of time-averages of atmospheric variables.  In addition to the standard physics-based climate models, experimental comparisons are made to (1) the deterministic ACE model (which uses the SFNO), (2) a probabilistic version of ACE (using MC-dropout), and (3) a DYffusion model with a U-Net architecture instead of SFNO.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is very well written. It is well-organized and the methods used are clearly explained. 
- The relation to previous work is clearly explained.
- The experiments compare against strong baselines and support the hypotheses.
- The topic is of significant interest. Physics-based climate models are key to our understanding of climate change, but they are fundamentally limited by the computational costs. There is keen interest in using machine learning to address these challenges by building fast emulators that can be used to generate larger ensembles and help quantify variability and uncertainty. The authors address the key problem by proposing a method that attempts to balance performance with computational complexity.

Weaknesses:
- The proposed model is never evaluated using probabilistic performance metrics (such as data likelihood). Diffusion models are attractive for probabilistic modeling because they have been shown to be good at maximizing the training data likelihood (e.g. Song, et al. Maximum Likelihood Training of Score-Based Diffusion Models).  The authors use DYffusion for its speed and say it matches or outperforms the accuracy of standard diffusion models, but is accuracy the right metric for emulating a physical system like this? The experimental results suggest that the model is working well for estimating climate variables, but it would be nice to have a stronger argument that the DYffusion model is a good probabilistic model of this data.

Limitations:
The paper clearly addresses the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this manuscript, the authors demonstrate a domain-specific, generative approach for a climate model emulator (ACE) trained to reproduce a climate model (FV3GFS) by training on FV3GFS simulation. Their approach is stable and reproduces the climate of the reference climate model with minimal biases. The addition of diffusion-based methodology significantly reduces the biases compared to the original ACE model. The spherical DYffusion approach addresses several important issues: 1.) the long inference and training time for diffusion-based models, 2.) helps introduce correct inductive biases as the atmosphere has spherical geometry, and 3.) allows for climate ensembles.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The spherical DYffusion is an important contribution to both the weather/climate domain as it incorporates the spherical nature of the atmosphere and demonstrates the success of the DYffusion approach on a complex, large system. One of the biggest problems with diffusion-based modeling is the long training and inference times (see GenCast where a single forecast takes 8 minutes compared to GraphCast which takes a few seconds). The spherical DYffusion is a good compromise between the benefits of a generative modeling approach and computational time during inference (something important for climate modeling). This approach only increases inference time by a factor of 3 and opens up the possibility for scalable, fast diffusion-based models for weather and climate.

The DYffusion methodology presented in this paper has a large number of applications well outside the weather and climate field. 
 
For climate research, the ability to rapidly run an ensemble of climate simulations is a large selling point for data-driven emulators compared to traditional physics-based, numerical methods.

Weaknesses:
Some of the claims in the conclusion are misleading and I suggest toning down the climate change-related conclusions (e.g. ""Our method’s climate biases nearly reach a gold standard for successful climate model emulation and thus represent a significant leap towards efficient, data-driven climate simulations that can help society to better understand the Earth and adapt to a changing climate""). Right now this model makes a stationary climate assumption (e.g. boundary conditions and forcings are consistent in both the training data set and during model inference). As mentioned in the limitation section, there is still significant research and evaluation needed before these emulators can start tackling climate change. 

The emulator is also trained on climate simulation not reanalysis (e.g. ERA5) which is typically done for data-driven weather models. The FV3GFS itself has biases with respect to the past and present climate which should be noted.

Limitations:
The authors are very upfront with the limitations of the work. My only suggestion in the limitation section is to mention this is climate simulation and trained on observation-based products (ERA5) and thus has some biases with respect to the real atmosphere.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a new method for probabilistic emulator of climate models, based on a type of diffusion model with spherical geometry. The results for climate model emulation are generally quite impressive.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The paper establishes what appears to be a new state of the art for climate model emulation, significantly improving upon a previous effort (ACE) by a very experienced team of AI/climate scientists.
2. Very clear results, showing large improvements both in reduced bias, and reaosnably variability
3. The method is quite computationally efficient compared to standard diffusion models.

Overall, this problem makes a very significant contribution to the field of AI-based climate modeling, and I strongly support publishing it at NeurIPS.

Weaknesses:
1. ACE is stable for up to 100 year time integration. Is this model similarly stable? My default assumption is that this model is not, which indicates it is not necessarily an improvement in practice. (If I'm mistaken, please correct me and the manuscript!)

2. This is not necessarily a weakness of this paper, per se, but I think the ""noise floor"" approach of ACE for estimating the significance of improvements in terms of reduced bias could be significantly improved upon. You are attempting to estimate a difference in means, so errors and statistical signifiance can be estimated using the central limit theorem, e.g., by taking the average over each year as a (mostly) statistically independent sample.

3. Using CRPS to measure distance between probability distributions, as is done in Figure 8, is a weird choice. It would be better to use a distance measure appropriate to distributions, like the Earth mover's distance.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
TXsRGrzICz;"REVIEW 
Summary:
This paper studies 3 types of factor graph inference for planning and in particular their relationship to optimal planning. The well known $\exp(\lambda R(x, a, x'))$ factor is used. MAP inference computes the maximum energy configuration over states and actions which corresponds to zero posterior entropy. Marginal inference computes the joint over states and actions. Marginal MAP inference set the action entropy in marginal inference to zero. An interesting connection was made with the dual LP formulation of MDP. The method was further applied to factored state MDP. The relationship between MDP stochasticity and inference approximation to optimal planning was studied in the experiments.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
**Originality & significance**: The paper is original and the three types of inference have not been studied in prior ""planning as inference"" literature to my knowledge. The connection between the variational formulation and the dual LP formulation is also interesting. In the discussion on stochastic dynamics, the notion of ""reactivity"" is also interesting and formalizes intuition from prior work to some extent. 

**Quality & clarity**: The paper is well written and the notation and exposition are clear.

Weaknesses:
There isn't any salient weaknesses of the paper as far as I can tell. My impression is the results are somewhat ""expected"" given this type of ""planning as inference"" has been widely studied in prior work, at least empirically. 

One thing that would be of interest is how ""reactivity"" interacts with online re-planning.

Limitations:
This paper does not contain a limitations section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the concept of ""planning as inference"", which frames planning problems (i.e. coming up with actions to reach a goal or attain reward) using the vocabulary and mathematics of probabilistic inference. The paper surveys existing formalizations of this broad concept, finding that none of them exactly correspond to the standard planning notion of computing a policy that maximizes expected cumulative reward. In response, the authors introduce a new variational objective, $F_\text{planning}(q)$ the optimization of which is a generalization of the objective of maximizing expected cumulative reward using some (potentially reactive) policy $\pi$ (instead of just finding a single action sequence $\mathbf{a}$ that performs this maximization, as in MMAP). They also show that this objective differs from existing inference objectives (MAP, MMAP, and Marginal inference) in terms of the entropy terms associated with their variational formulations. The authors then derive several practical methods for optimizing $F_\text{planning}(q)$ based on linear programming (VI LP) or loopy belief propagation (VBP) to compute the optimal $q$ (which corresponds to the action policy $\pi$). In both synthetic experiments and benchmark IPC problems, they show that these methods are competitive with other planning-as-inference algorithms based on other inference formulations, and confirm that optimizing $F_\text{planning}(q)$ leads to (desirably) different properties than MAP or MMAP inference, e.g. in terms of policy reactivity.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
This was a detailed, interesting, and well-substantiated paper that addresses an important conceptual question for the field of planning-as-inference: What inference objective corresponds to the standard decision-theoretic notion of maximizing expected cumulative reward? To my knowledge, this question hasn't been adequately addressed, and this paper does a very thorough job of providing an answer --- one that I believe will be valuable for future practitioners in the field.

Overall, the paper was well-presented for researchers already with an understanding of planning-as-inference, and the theoretical results were mostly well-explained (see later comments for how the exposition could be improved for a broader audience, as well as several theoretical claims that need to be better justified / clarified). I especially appreciated the explanations as to how the optimization of  $F_\text{planning}(q)$ differs from other inference objectives (vis a vis policy reactivity), and the derivation (mostly in the Appendix) of how a variant of their objective can capture determinization in planning (which makes clear the variational gap this introduces).

Regard soundness, the proofs that I checked (in Appendices A, B, ad E) appear to be correct, and the experiments provide sufficient validation for both the new objective and the approximate methods they use to optimize it. Even though these approximate methods mostly match existing methods for planning-as-inference like SOGBOFA-LC, the experiments show that in at least some cases (e.g. higher entropy cases or cases requiring reactivity), optimizing the right objective (i.e.  $F_\text{planning}(q)$) leads to higher cumulative reward.

Weaknesses:
In my opinion, the main way this paper could be improved is in exposition, especially for a broader audience than people already familiar with planning-as-inference, factor graphs, and variational inference. The introduction right now is very short, and then paper goes straight into mathematical background. IMO, some of the discussion in the Related Work should be moved into the Introduction itself, and I think the Related Work probably is best placed after the Introduction. I also think both the Abstract and the Introduction could foreground the intuitive / standard notion of planning as ""maximizing expected cumulative reward"", and communicate that their variational formulation of planning-as-inference both reproduces this objective and generalizes it, whereas other inference objectives do not. This, to me, is one of the main upshots of the paper, and it should be communicated clearly from to get-go as a motivation. Right now, the idea that maximizing $F_\text{planning}(q)$ generalizes ""maximize expected cumulative reward"" only starts appearing in Table 1, and  Section 3.1.

As an additional organizational suggestion, after introducing the Background, I think it is probably better to cover the content in Section 4 first, by introducing $F_\text{planning}(q)$ as a variational objective, then comparing it to the other existing inference objectives. After you have established that $F_\text{planning}(q)$ is the ""right"" objective for planning, then you can go into the content of Section 3.2 as a separate section that is focused on practical methods for optimizing the planning objective. By organizing things in this way, you first clearly motivate the desirability of $F_\text{planning}(q)$ as an objective, which then motivates practical methods for optimizing $F_\text{planning}(q)$.

Aside from exposition, I think a number of theoretical claims that the paper makes could be better explained and clarified. In particular, it was not obvious to me that the ranking of inference types in Section 4.1 could be read off from the entropy terms in Table 1 -- there should be an associated proof in either the main paper or Appendix. There are also a few notational choices that I think need to be clearly defined before use, and several sentence discussing the relationship of  $F_\text{planning}(q)$ to other inference objectives that could be clarified. I will detail these more in the Questions section.

I think the experiments could be made even stronger by comparing against e.g. Monte Carlo methods for planning-as-inference (e.g. Piche et al. (2018)) and model-based planning algorithms for MDPs (standard value iteration, RTDP, etc.), but this is not strictly necessary.

Limitations:
While there is no explicit limitations section, the authors adequately discuss the approximate nature of their proposed optimization methods, and also discuss cases where their proposed objective is equivalent (and hence does not improve on) existing planning-as-inference formulations (namely, cases without deterministic dynamics).

There is some discussion of this already, but I think it would be good for the paper to better recognize when Marginal$^\text{U}$ might be desirable as an objective, as this corresponds to a notion of ""soft planning"" that is similar to Boltzmann-rational models of action selection and max-entropy inverse RL (Ziebart et al, 2010). Sometimes, our goal is not to derive a reward-optimal policy, but to get a policy that is stochastic (because we desire diversity, or because we want to model human planning).

The paper is largely theoretical in nature, and discussions of societal impact are not immediately relevant.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a variational inference (VI) framework to characterise different types of inference for planning, specifically for finite-horizon MDPs represented as probabilistic graphical models. They also develop an inference algorithm (VBP) that takes inspiration from the loopy belief propagation and adapts it to planning. An experimental evaluation seems to confirm the theoretical analysis.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is very interesting and original. They also performed an extensive experimental analysis.

Weaknesses:
The paper has two major flaws. First, the text is poorly written, and it definitely doesn’t meet the standards of a high profile conference as NeurIPS. In particular,
 
1) The introduction and the first subsection of the backgrounds have zero references.

2) The paper is not self-contained. I understand the limit of space, but the background section doesn’t provide all the necessary background. Moreover, the experiment on “Reactivity avoidance” is interesting, but most of the explanation is in the appendix, making it hard to grasp with the sole information in the paper.
3) The notation is not always properly introduced, for example:

 - At the beginning of 2.2 I would explicitly write the equations for the different types of inference. Considering how central they are to the paper, and the fact that the work if theoretical, I would expect a more formal definition.
 
- The risk parameter \lambda is introduced in section 3 without any context on what that means in terms of planning. It is not in the standard formulation of MDPs, nor introduced in your background section.

- The “discussion” is not really a discussion, maybe we can call it a conclusion, but anyway it looks rushed and not very “conclusive”
See detailed comments below for more examples.

Second, the soundness of the contribution is undermined by missing details and not very convincing results. In particular:

4) Your definition of “planning inference”, that is central to the paper, is too fuzzy. On line 96 you state that your definition of “planning inference” corresponds to the entropy in Eq.4, however later on line 181, you state that your definition of inference is exact. This I think is a bit confusing. VI is by nature an approximation. I presume I understand what you want to say, nevertheless I think it should be clarified.

5) The discussion on the different types of inference lacks rigour. The notation is not always clear, and most importantly, it is not clear what is the impact of this finding on the planning community. What does it mean in practice that “planning inference” is different from all the others in stochastic settings?

6) The experiments, despite being extensive, don’t show a significant impact coming from the new “planning inference” introduced by the authors. See my questions to the authors for more details.

Limitations:
Okay.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper shows that planning in an MDP can be posed as a specific form of
inference in a graphical model; in this context of inference, different forms
of inference can be applied, with different results. Additionally, alternate
forms of inference such as variational techniques can be used, which allow
better plans to be inferred than existing baselines. 

This is not a perfect paper, but overall it is a good idea, well executed and
I recommend it for acceptance.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The primary strength of this paper is the technical contribution, which is
how the paper places the planning problem in the context of inference in
graphical models. The relationship between planning and inference is not
novel, but the generalisation of this relationship to different forms of
inference is novel.

- The paper (mostly) shows how the policy for a standard, flat MDP can be
found by solving for the variational posterior over states, and also shows how
a variant of belief propagation can be used to find this posterior.

- The paper generalises this result to factored MDPs, and shows how belief
propagation is even more useful.

- The paper evaluates the VBP technique against gradient descent baselines and
  exact solution techniques on both 5000 synthetic MDPs and also 6 domains
  from the internation planning competition. The paper shows that VBP is very
  close to the exact solution and outperforms the other baselines on the
  synthetic domains. On the IPC domains, the VBP seems to match the
  performance of the best baseline across all IPC domains (different baselines
  have different performance across different domains -- VBP seems to match
  the best one on each domain).

Weaknesses:
The paper has two primary weaknesses:

- There is no discussions of the weaknesses of the inference-based
  technique. It is not clear if the VBP is computationally more costly than,
  for instance, the gradient-based SOGBOFA techniques. Timing information
  would have been very helpful.

  It is also the case that while VBP consistently matches the best performing
  baseline, it does not seem to outperform any of the baselines. It would have
  been helpful to know why VBP isn't able to match the exact planning
  technique -- where is the loss in performance coming from?

- Some of the writing is less than clear. For instance, the exponential
  utility is introduced in table 1, and is justified on page 3 with a short
  reference to being ""more suitable for a factor graph representation"" but
  more explanation would have been helpful. Please note in the author response
  that the concern is not that the exponential utility is problematic, but the
  text needs to spend more time addressing and justifying this.

  Similarly, the introduction of the $\lambda$ risk setting is not the common
  case, and needs more justification. It's not clear that the $\lambda$ term
  adds much to the primary point of the paper, and seems to be set to 0 for
  most (if not all) of the experimental results. Furthermore, appendix F.3.7
  indicates that $\lambda$ has no meaning if ""if the reward can have arbitrary
  scaling"". This is a crucial point that should be in the main body of the
  paper, and in fact the paper might be clearer without incorporating
  $\lambda$ at all since it does not add much to the explanation of planning
  as inference.

Limitations:
The authors did not include a discussion of limitations, and this is one of the
weaknesses of the paper.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a framework to understand, as the title suggests, what type of inference is (probabilistic) planning. The authors make use of variational inference which encompasses marginal, MAP, and marginal MAP (MMAP) inference to theoretically compare the power of such types of inference in bounding the optimal expected reward for probabilistic planning. Furthermore, the authors propose an approximation of planning inference for factored MDPs as they search space is exponential in the input problem. Experiments are run to support the theoretical results and claims.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper poses an interesting theoretical study from a statistical viewpoint on optimisation functions for planning.
2. The main story and results are well presented and easy to understand.
3. The experiments are diverse and well motivated, with results supporting the theoretical claims.
4. The paper provides a good coverage of related work.

Weaknesses:
1. The main weakness of the paper is its clarity. The paper can be better self-contained if some of the other sections can be better motivated and/or explained by an additional sentence or two, rather than references to the appendix. Furthermore, some of the equations and technical details seem to be ambiguous and not well defined. See suggestions/comments below.

Limitations:
The authors correctly fill in the checklist and justify their answers.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
9m87e9Keq1;"REVIEW 
Summary:
There have been previous attempts at doing finetuning on positive synthetic examples to improve LM reasoning, but the performance gain from these attempts are generally quite limited (and possibly even negative). In this paper, the authors take a different approach which also accounts for negative examples and “critical” intermediate steps, improving upon previous approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
I was really impressed by this paper – at least for me, it contains quite a lot of insights, and is substantially better than most papers on synthetic data that I’ve seen in the past. 
- I thought the idea of using negative data was fairly natural but also good, and I’m glad to see it implemented in practice
- I found that the proposed link to model collapse seemed really interesting, which allegedly can occur if a model generating synthetic data has memorized the training data using a spurious feature. 
- I really liked the idea of looking at “critical steps”, and I thought the approach of relating this to credit assignment was impressive.

More generally, I liked that the authors really made clear attempts to both quantify and explain their empirical observations. For example, I appreciated the section comparing the sample efficiency of self-generated and GPT-4/Gemini 1.5 Pro synthetic data, which does both.

Weaknesses:
The biggest concrete thing that I felt was missing was discussion about computational costs. I think this is really important because I’d be interested in the scalability of approaches like the ones outlined in the paper. I think having some comparison of the overall FLOP required for finetuning (including the FLOP for synthetic data generation) would be very helpful. 

Another weakness is that in my understanding, the main approaches in the paper apply most directly to mathematics, since identifying critical steps involves checking results based on MC rollouts. There’s still a question about whether or not similar approaches can be applied to LMs in domains where outputs are harder to “verify” – though to be clear, I think it’s entirely reasonable that the authors focus on math in the context of this paper.

Lines 197-199: This compares the scaling exponent for data for the MATH and GSM8K datasets with the exponent from the Chinchilla paper (on MassiveText). How do we know that this implies improving coverage over samples? Other possible reasons for the different exponent come to mind – e.g. different functional forms for the scaling law in finetuning and pretraining, use of different datasets, etc.

As a minor comment, I think “It is predicted that we will run out of high-quality internet data by 2026” should be modified to be about human-generated public text data in particular, and it looks like the median year from source [53] is 2028 rather than 2026.

Limitations:
I think the authors did a fair job discussing the limitations of their work. One minor weakness is that the paper is framed around when synthetic data improves LM reasoning, but the investigation itself is more narrowly focused on mathematics.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to effectively leverage synthetic data to improve reasoning capability in the GSM8K and MATH datasets. The authors identify that sampling correct synthetic data from a fine-tuned model is more sample-efficient but comes with the risk of overfitting artifacts in the synthetic data. Instead, they propose a per-step DPO to leverage negative synthetic data to enhance sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Synthetic data for reasoning is a highly important topic.
2. The idea is straightforward and effective.
3. The experiments are extensive, with comprehensive ablation studies.

Weaknesses:
1. Missing Relevant References: The paper lacks references to relevant literature. Scaling laws on how synthetic data plateaus or performs slower than real data during pretraining have been observed and analyzed in several studies [1][2]. Additionally, the observation that self-generated data is more sample-efficient has been noted in research on images and machine translation [3][4][5].
2. Unclear Per-Step DPO Definition: The per-step DPO is not fully defined. In Line 180, how is the “first pit” $\hat{y_c}$ determined? Does it involve looping over all intermediate steps from the beginning to find the smallest $c$? It would be helpful for the authors to provide an algorithmic outline in the appendix. In Theorem 6.1, the per-step DPO is trained with pairs (x, $[y_{1:i}, +y_{i+1}]$, $[y_{1:i}, -y_{i+1}]$), whereas Line 180 defines it as using $+y$ instead of $[y_{1:i}, +y_{i+1}]$. Which version is implemented in the experiments, and why?
3. Claims on Sample Efficiency: The improved performance on filtered $\mathcal{D}_{syn}$ from the fine-tuned model requires sampling 100 samples per question, and the per-step DPO samples require Monte Carlo rollouts for intermediate steps. This adds significant computational overhead during inference, much more than during the fine-tuning stage. A proper comparison of sample efficiency should include the inference compute spent generating these examples. Given the same computational resources, would DPO perform better as it does not require identifying the “first pit” while utilizing more synthetic data?
4. Explanation of Figure 7: The explanation of Figure 7 is unclear. How are the average Q-values calculated? What if a problem requires fewer than 8 steps in GSM8K? Will the average for step 8 be biased toward harder questions?

### Reference
[1] Dohmatob, Elvis, et al. ""A tale of tails: Model collapse as a change of scaling laws."" arXiv preprint arXiv:2402.07043 (2024).

[2] Fan, Lijie, et al. ""Scaling laws of synthetic images for model training... for now."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

[3] Mobahi, Hossein, Mehrdad Farajtabar, and Peter Bartlett. ""Self-distillation amplifies regularization in hilbert space."" Advances in Neural Information Processing Systems 33 (2020): 3351-3361.

[4] Gu, Jiatao, et al. ""Non-autoregressive neural machine translation."" arXiv preprint arXiv:1711.02281 (2017).

[5] Zhou, Chunting, Graham Neubig, and Jiatao Gu. ""Understanding knowledge distillation in non-autoregressive machine translation."" arXiv preprint arXiv:1911.02727 (2019).

Limitations:
Limitations are discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the impact of synthetic data on improving the reasoning capabilities of large language models (LLMs). The authors conduct an empirical study followed by a theoretical formalization to understand when and how synthetic data helps or hurts model performance on reasoning tasks.

Key findings include:-
(1) Finetuning on synthetic correct/positive problem-solution pairs offers modest gains, but sampling more correct solutions from the finetuned learner doubles sample efficiency.
(2) Training solely on model-generated positives can amplify spurious correlations, leading to flat or inverse scaling trends as data increases.
(3) Utilizing negative responses (incorrect model-generated responses) alongside positives, with appropriate credit assignment for intermediate steps, yields consistent gains over positive-only data.
(4) The proposed per-step negative data approach is equivalent to advantage-weighted reinforcement learning (RL) and helps unlearn spurious correlations.

The authors demonstrate that their per-step negative data approach improves sample efficiency by 8x compared to standard finetuning on positives. They also provide theoretical and empirical analyses to explain why credit assignment improves model generalization.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The work reveals important nuances in using synthetic data, particularly the value of leveraging negative examples and the importance of credit assignment for intermediate steps.
- The findings offer concrete guidance for improving LLM training with synthetic data, potentially addressing data scarcity issues in language model development.
- The authors conduct extensive experiments, including scaling studies and comparisons with various baselines, strengthening the validity of their claims.

Weaknesses:
- While the focus on reasoning tasks is well-justified, it's unclear how well the findings generalize to other types of language tasks or domains.
- The study primarily uses 7B parameter models (DeepSeek-Math-7B and LLama2-7B). It would be valuable to see if the results hold for larger or smaller model sizes.
- The paper doesn't discuss the computational costs of their approach, which could be a significant factor in its practical application, especially for larger models or datasets.
- While the authors compare their method to several baselines, it would be beneficial to see comparisons with more recent techniques in synthetic data generation or model finetuning.
- The study doesn't address potential long-term consequences of training on synthetic data, such as potential drift or accumulation of errors over multiple generations of synthetic data.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the effect of synthetic data on improving the reasoning abilities of LLMs. The authors have compared with multiple approaches including SFT, RFT, DPO, per-step DPO, etc., and characterized the model performance w.r.t. different scales of synthetic data under each training regime. Several practical findings about improving sample efficiency are presented, including leveraging the finetuned model to sample for positive samples, utilizing per-step negative examples to alleviate the spurious correlations that might appear in the synthetic positive data, etc.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper studies an important problem.
- The conceptual model of negative sample construction covered in Section 6.1 is an interesting read, as the authors formally express the synthetic data problem in the context of RL.

Weaknesses:
- It seems that the authors wanted to cram lots of conclusions into a single paper. While I appreciate the incentive, the paper would make a much more academic read if more experimental results are clearly presented in tables. Currently, it seems that there are few quantitative results in the paper. This also led many specific concerns (see below).

- In Lines 242-245, it reads `But, we find that even when RFT data is restricted to one solution per question, LLM trained on it outperforms SFT consistently by > 1%. Since verification is cheap, we can sample more solutions and also benefit from coverage.` Where is the evidence to this claim? In this claim, does the size of D_{syn} equal that of D_{SFT}^{+}? 

- It makes me confused when I compare Lines 224-244 and Lines 245-263. 

    In Lines 224-244, the authors suggest that RFT outperforms SFT in terms of sample efficiency. But in Lines 245-263, the authors suggest that RFT data contain spurious correlations, and when manually raising the percentage of spurious correlations in RFT data, SFT would outperform RFT. Especially, in Lines 248-249, `This is unlike scaling of problems and solutions in SFT data (in Figure 2(a,b)).` Since RFT might even suffer from negative scaling because of spurious correlations, I don't think that RFT could be claimed to be `more sample-efficient`.

- Also about the artificial spurious correlation amplification experiments in Lines 257-263: Does Figure 4 show the experimental results (there's no reference in the paragraph)? Does the `D_{π}^{+} spurious` correspond to the construction of `for each question in D_{syn} we sample “spurious steps” from π_{sft}`? Does this ""100% spurious correlation injection"" represent the authentic ""spurious steps"" ratio in RFT data (and what is the authentic ""spurious steps"" ratio in RFT data?)? Does D_{syn} contain no spurious correlation steps? As the RFT data scaling leads to negative performance, will those RFT data with a less percentage of spurious correlation steps lead to a not-so-negative performance? If this is true, can we just use the advantage defined in Eq.(3) as a stronger filtering rule and apply on RFT data? A more in-depth quantitative study is required here, so that the importance of the use of negative examples in Section 6 is better revealed.

- Is the `per-step DPO` method used in the paper proposed in the cited [23] paper? If it is true, is it also true that no new method is proposed in Section 6? Note that it is fine to focus on analysis instead of proposing new techniques, but the author should make it more clear about this in the paper. It would also be helpful if the authors briefly introduce the per-step DPO method used in the paper in a background/methodology section.

- Are the advantage in Eq.(3) and the per-step DPO method applicable to other types of reasoning tasks in addition to MATH and GSM8K? For example, ARC requires LLM reasoning (LLM often conducts this task by writing codes). Are the methods and the analysis applicable to ARC?

- Lines 343-344: `As such, we needed to tune β in Equation 1 for DPO but could not fully avoid
344 performance degradation.` Where is the evidence to this claim? Experimental results with a sweep of β would be supportive. 


- Similarly, Lines 377-386 also require more experimental evidence. It would be helpful if quantitative observation is added to the claim like `When the initial advantage of a spurious step is incorrectly over-estimated, negative data algorithms up-weight the likelihood further. This only leads to further memorization. `, `leads to test-time model collapse`,  and `On the other hand, when \tilde{π} = BoK(π_{sft}) for a higher value of K, the Monte-Carlo advantage estimator has a lower variance (and error). This discussion also justifies the choice of K=5`.

- The paper lacks a Conclusion section, which I believe is because of the length limit of a submitted paper. But it is still better to add such a section and properly refactor the previous contents.


I encourage the authors to address my concerns, and I would consider raising my score provided that the issues I raised are resolved properly.

Others: Line 322: ""instantation"" --> ""instantiation""

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
lD7ziaMHbf;"REVIEW 
Summary:
• This paper proposes ECP-ViT, which optimizes the ViT model by introducing the core-periphery (CP) principle.
• The Core-Periphery Principle Guided self-attention mechanisms successfully reduce memory bandwidth by eliminating data transformation.
• By applying pruning and removing data transformation, the optimization, which considers both software-level design (algorithm) and hardware-level design, achieves real-time performance on a mobile GPU (Snapdragon 8 Gen 2 SoC) with an average speedup of 4.8 times.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
• The performance generality of the proposed ECP-ViT is demonstrated across various datasets (STL-10, CIFAR-100, TinyImageNet, and ImageNet) and hardware environments (OnePlus 11, Xiaomi 6).
• The justification for the proposed Core-Periphery Guided Self-Attention is well-explained, including background information, and effectively illustrated with figures and equations.
• ECP-ViT achieves speed improvements by completely eliminating data transformation, which impacts memory bandwidth and causes network overhead.
• (Table 8) The compiler speed is significantly improved compared to the MNN and TVM frameworks.
• (Table 9) The overhead of data layout transformation and computation for the mobile GPU is analyzed in detail.

Weaknesses:
• The comparative experiments lack consistency.
• Overall, the performance improvements are marginal compared to previous research.
• The implementation uses Fixed Point 16-bit, but most current mobile environments typically utilize 8-bit or higher quantization, and there are no experiments addressing this. Data compression through quantization is essential for mobile environments.
• More fair comparisons could be made if experiments were conducted in specific NPU environments.
• Although a compiler for mobile GPUs is proposed, detailed information about the compiler itself is lacking.
    o (Figure 4) There is insufficient explanation on how the specific allocation of the core/periphery nodes is determined, and there is no criterion for specifying core nodes.
    o The detailed operation of the proposed dimension reduction heuristic algorithm is not explained. More detailed descriptions and algorithms are needed to optimize data layout.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ECP-ViT, a framework that accelerates Vision Transformers on mobile devices using a core-peripheral guided self-attention mechanism. This approach reduces computational demands and achieves up to 16.1x speedup on a OnePlus 11 GPU, enabling efficient real-time deployment of ViT models while maintaining high accuracy.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The compiler and model architecture were co-optimized, significantly improving computational efficiency on actual devices.
- Based on the Brain Neural Networks, this paper cleverly distinguishes between important and less important parts of the attention mechanism. 
- The slice and transpose reshape operations are eliminated through the hardware-level design.

Weaknesses:
- Overall, I think this paper is above the acceptance bar. However, the comparison (or discussion) between ECP-ViT and other lightweight ViT methods is not comprehensive enough, for example, NAS for lightweight ViT [1][2], and advanced token optimization method [3]. 
- Showing the results on ImageNet seems like a reasonable choice, but insufficient to show that the method can be generalized to other domains, such as detection/segmentation. One possible way to do this is to directly transfer the weights for another downstream task and provide some comparison. By the way, the performance improvements in Tab.11 seem a bit marginal compared to the main results. 

[1] Elasticvit: Conflict-aware supernet training for deploying fast vision transformer on diverse mobile devices, ICCV 2023

[2] Nasvit: Neural architecture search for efficient vision transformers with gradient conflict-aware supernet training, ICLR 2022 

[3] Diffrate: Differentiable compression rate for efficient vision transformers, ICCV 2023

Limitations:
Please refer to the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents ECP-ViT, a real-time framework for deploying Vision Transformers (ViTs) on mobile devices. Inspired by the brain's core-periphery principle, this method guides self-attention in ViTs to reduce computational demands and eliminate data transformation operations. ECP-ViT integrates algorithm-system co-optimizations, achieving a speedup of 4.6× to 26.9× on mobile GPUs across various datasets while maintaining high accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The motivation data in Table 1 is interesting. 

2. This paper targets an interesting problem: eliminating the expensive transform operation in ViTs.

Weaknesses:
1. The paper writing can be improved.

2. Some figures and their elaborations are not clear enough.

3. The rationale behind the design is not well-explained.

Please see my comments.

Limitations:
Please see my comments.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ECP-ViT, a framework designed to improve the performance of vision transformers (ViTs) on mobile devices. The authors observed the intensive and irregular memory access involved in the data transformation for self-attention layers, which significantly slows down transformers compared to traditional CNNs. To address this, they propose a hardware-friendly self-attention pruning technique motivated by the core-periphery structure in brain networks, thereby reducing the computational and memory access burdens. ECP-ViT also incorporates compiler optimizations to fuse and eliminate transformation operators to further boost the performance. These combined optimizations enable the work to speedup in ViT inference on mobile GPUs by 4.6x to 26.9x without sacrificing the inference accuracy.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
+ The problem is well-motivated with clear benchmarks in terms of accuracy, MACs, and latency comparisons across various ViTs.
+ The core-periphery concept borrowed from brain networks for self-attention is interesting.
+ The proposed compiler optimization effectively eliminates unnecessary data transformation operators, leading to significant performance improvements.
+ The evaluation is conducted on real-world mobile phones, and includes both high-end and low-end devices.

Weaknesses:
- The evaluation section could be improved:
    - It would be beneficial to discuss the memory access pattern differences before and after applying ECP to showcase its effectiveness in improving memory efficiency.
    - The power and energy consumption evaluation should also be included, given the limited battery capacity of mobile devices.
- There are several presentation issues; for example, the space between Table 3 and Table 4 is too dense, and the font size seems to change abruptly starting from the “Evaluation environment” in Section 4. Please ensure consistent font sizes throughout the text.

Limitations:
The authors have mentioned the plan to evaluate other model architectures in future work, which addresses some limitations. However, it would be beneficial to include additional evaluations such as memory studies, power/energy consumption analysis, and resource utilization metrics.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a ViT accelerating framework, ECP-ViT, to deploy ViT models on smartphones. This framework consists of two parts: 1) Core-Periphery Guided Self-Attention (reducing the computational and bandwidth cost of ViT) and 2) Data Layout Selection based on compiler optimizations (removing the time-consuming data transformation). Specifically, Core-Periphery Guided Self-Attention partitions the tokens in K, Q, and V matrices into core and periphery components. Tokens in the core component exchanged messages with all tokens, and tokens in the periphery component only exchanged messages with tokens in the core component. To fully eliminate the data transformation operations, this work attempts to find a common data layout that works efficiently for both contiguous operators. But the search space is large. They propose the dimension reduction heuristic algorithm to shrink the search space of the data layouts. ECP-ViT obtains competitive results on STL-10, CIFAR100, TinyImageNet, and ImageNet, achieving lower smartphone latency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The Core-Periphery Guided Self-Attention, which is inspired by Brain Neural Networks, is interesting. 
- The Data Layout Selection based on compiler optimizations can effectively speed up ViTs on Smartphones, as demonstrated by comprehensive experimental validation.

Weaknesses:
- Some details need to be elaborated on. The description of the dimension reduction heuristic needs to be more detailed, and it is encouraged to provide its pseudocode. 
- There seems to be a discrepancy in Equation 1. The conventional form of self-attention is softmax(qV)K, while in ECP-ViT, it appears to be softmax(qVK). This discrepancy should be carefully reviewed and corrected.

Limitations:
Not applicable.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
w28i9oe9Xr;"REVIEW 
Summary:
The paper addresses the issue of weak convergence of stochastic processes, whereby evolving information is generally unaccounted for. This can lead to discontinuities when applying these processes to multi-period decision-making problems. Prior work has proposed the concept of extended weak convergence, as introduced by Aldous (1981), but practical numerical implementations have been challenging. To address this, the authors introduce a novel metric called High Rank PCF Distance (HRPCFD) which is shown to overcome computational issues encountered in previous attempts. The paper then demonstrates the utility of HRPCFD via experiments on hypothesis testing and generative modelling of time series data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Unfortunately this paper lies well outside my area of expertise and I am unable to review it effectively. The mathematical framework around extended weak convergence is not an area I’m familiar with, and I consequently found it challenging to grasp the nuances of the problem statement, the significance of the proposed HRPCFD metric, and the potential implications for applications in finance and economics.

So as not to negatively impact the paper’s chances of acceptance, I have defaulted to a mid-range score in my review, which reflects my assessment that the paper could nevertheless still be made more accessible for readers who are less familiar with the domain.

Weaknesses:
See above.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
Time series is ubiquitous in machine learning. They are modeled as stochastic processes and therefore notions of distance between stochastic processes and more generally convergence of stochastic processes are fundamental ideas. Weak convergence of probability measures occupies a central position in this area, but for many settings, like optimal stopping, or the one studied in this paper, it is not sufficient. Extended weak convergence, defined via weak convergence of prediction processes, is the right notion. This paper introduces a metric on the space of filtered processes that metrizes the topology of extended weak convergence, proposes statistical procedures to compute it, and then tests these ideas on GANs for time series.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a paper on a very nice topic and I learnt a lot while reading it. Some of the ideas are very abstract and the paper does a good job of organizing the topics and defining everything precisely so that a meticulous reader is not left confused. It is welcoming to see a rigorous paper in ML conferences. The ideas introduced in the paper are novel and the proofs of all the claim are carefully done, although I can't claim to have read every section in appendix in detail.

Weaknesses:
Although as mentioned above the paper defines everything clearly, the exposition on PCF and HRPCF could be improved. It took me quite some time after re-reading the paper multiple times and some other referenced paper to develop an intuition for these concepts, even though I have some background on probabilistic notions like weak convergence and extended weak convergence. I understand this is difficult to do well in a conference paper with page limits, but I think having a more detailed appendix on PCF and HRPCF would help.

I should mention here that I didn't find the experiments super convincing, but I am viewing this paper as a theoretical contribution, and thus any experiments it has as an added bonus and not a weakness.

Limitations:
N/A

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper constructs a computationally-implementable metric which metrizes an ""extended"" weak convergence for stochastic processes, which more plausibly accounts for the convergence of the process with respect to their filtrations.
The result can apparently more effectively account for similarities between controlled processes, at least in the class of linearly-interpolated stochastic paths considered here.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The method seems to construct high-rank analogues of classic tools, such as the characteristic function, such that the prediction processes arising from projection onto the filtration at each moment can be quantified and divergence between them metrized, without density evaluations, using empirical measures.

The classical SDE reasoning seems sound. I confess that I am less familiar with the rough path theory component, but there are no obvious red flags in the material.

Weaknesses:
The paper is long;
The results seem to be an improvement both theoretically and empirically over the main antecedents

* [18] Hang Lou, Siran Li, and Hao Ni. PCF-GAN: generating sequential data via the characteristic  function of measures on the path space. Advances in Neural Information Processing Systems,  36, 2023. 
* [19] Cristopher Salvi, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. The signature 392 kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873–899, January 2021.
* [20] Cristopher Salvi, Maud Lemercier, Chong Liu, Blanka Hovarth, Theodoros Damoulas, and  Terry Lyons. Higher order kernel mean embeddings to capture filtrations of stochastic processes.  Advances in Neural Information Processing Systems, 34:16635–16647, 2021

But it is not clear whether the increment is ""important"" in practice; Is the increased performance ""worth"" the implementation effort and/or computational cost? The answer is probably problem-dependent.

Limitations:
No particular issues noted.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes High Rank Path method, motivated by the extended convergence notion and the rough path theory, to generate (conditioned) time-series data. A new metric HRPCFD is introduced, and experiments are conducted for Brownian motion, GANs with applications in finance.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is rigorously written, which introduces a new metric HRPCFD on the path-valued processes based on various ideas from probability theory -- extended convergence, rough path, signature... I checked most proofs, and they are correct.

Weaknesses:
Weakness and comments:

(1) The paper may be too heavy for the Neurips audience (though I enjoyed reading it). It seems to be more suitable for a rigorous mathematical or statistical journal (e.g., Annals of Statistics). 

(2) Many proofs of the results (e.g., Thm 3.3) are purely measure-theoretical, and I think the authors may shrink some proofs to keep the idea concise. 

(3) The authors may want to explain why the proposed HRPCFD outperforms others (e.g., signature...) Is there any possible theoretical guarantee?

(4) The authors may have a discussion on the computational efforts of the proposed method (e.g., computational complexity and running time). The path-space optimization (or signature-type methods) often suffer from computational efficiency.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
kZfxICBXd1;"REVIEW 
Summary:
The paper studies an attractive reconfiguration problem in the context of multi-winner elections. Suppose we have an n-voter m-candidate approval election and work with a committee-scoring voting rule outputting size-k committees. Fix such a rule r (in the paper, this is either approval voting, proportional approval voting, satisfaction approval voting, or Chamberlin-Courant). The goal is, given two committees W != W’ such that their score under r differs by at most some number x, and given a number y, to compute (or decide the existence of) a chain of committees W = W_0, W_1, …, W_t = W’ such that any two consecutive committees’ scores differ by at most x under r and the symmetric difference between them has size at most 2y (more precisely, the number of added and removed elements between any two consecutive committees are both at most y). The authors study the classical and parameterized complexity of this problem, almost tracing the complete landscape of its complexity with respect to a large number of natural parameters and combinations thereof. The paper also provides an experimental appendix on computing such chains in practice.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is quite well-written and motivated. I enjoyed reading it. The parameterized complexity analysis is carefully executed, with well-chosen parameters and interesting findings. The paper’s initial segment introduces concepts at an appropriate pace, giving examples where they are due. I was particularly happy with the existence of this sentence: “We will see that all intractability results already hold for the restricted case (x = 0, y = 1), while all algorithmic results hold for the general case.” I think a paper like this could be a nice read for a general AI/ML audience (which is relatively rare for parameterized complexity papers). Moreover, I think the findings will be interesting to the specialized audience interested in multi-winner elections and/or voting in general. The experiments in the appendix clearly add a bit more color to the paper, which would otherwise be a pure complexity paper (I believe these should be more prominently featured earlier in the paper, even if just for a few sentences).

Weaknesses:
- One could argue that such “heavy” parameterized complexity papers shouldn’t appear in AI venues. While I tend to agree with this statement in general, as I already said, this paper could be a welcome exception.
- The model permits chains of committees where a certain alternative is included/excluded multiple times throughout the chain. In clearer terms: it would be strange if, say, Netflix repeatedly added and removed a certain movie throughout the course of 1 month (extreme example: the movie is available only every second day). I’d say it’s more realistic to ask that once a certain alternative in W \setminus W’ is removed, it should never be added back. (**)
- L131-138: {I am familiar with these definitions and I understand what you meant to write.} The way you work here with multi-winner rules implicitly assumes that they are irresolute (i.e., nondeterministic = return a set of solutions), since otherwise neutrality is almost meaningless; e.g., say m = 2 and k = 1 and we have two identical candidates, which one would a neutral resolute rule choose? This makes me also wonder about anonimity as well (outcome does not depend on the name of the voters). What do you actually assume here? (*) I am positive none of these omissions break your results in any way, but I think this should be clarified and revised in the paper.
- L190-191: I think the correspondence only holds for paths that start at W (the vertex set of the graph is defined in terms of the score of W, so the made statement can’t be true). I understand the idea you wanted to convey though, i.e., we care about subpaths that are to be used in a longer path that starts at W.
- I had great difficulty understandign the first half of the proof of Proposition 1 (the one making appeal to previous work). This is not because the rough idea is unclear to me, but because of the details; e.g., “determining whether a committee is on a reconfiguration path” seems as hard as answering the whole question. Instead you might have meant “whether it’s in the vertex set of the graph”. I would advise to revise this part.

Minor:
- You should highlight more in the first few pages that the path need not necessarily be a shortest path. In hindsight, it’s never stated otherwise, and there is no reason why one would believe that, but I still somehow managed to think it’s shortest for a few minutes (maybe because one of the parameters you introduce early on is the length of the shortest one!).
- L56: It can’t be PSPACE-complete because it’s not a decision problem. I assume you wanted to say PSPACE-hard.
- L39: The “But…” sentence reads a bit strange. Consider adding a comma or combining with the previous sentence.
- Abstract: “minor yet impactful modifications” sounds a bit weird in this context. Instead of helping me, the addition of the word “impactful” made me start pondering what would “impactless” changes even mean in this model. The “meaningful” you use on L24 is a bit better, but really I wonder whether an adjective is even needed here.
- L128: “If not” -> “Unless”
- L129: Maybe use \emph{} over the word “type” to show that this is a definition.
- L165: “Let” -> “Assume”
- L178: “we use” -> “we write”
- L194: Remove “the”.
- L195: “there is no” -> “hence no”
- I understand the need for them, but LaTeX is just not very appropriate for imbricated proofs (i.e., Claim 1.1 inside the proof of Theorem 1). The \qed symbol above L234 looks slightly strange if the reader is not accustomed to it.
- L245: “remains” seems inappropriate here since the former part of the statement doesn’t concern W[1]-hardness. Another comment I will make here, but more generally applicable: it might be worth saying that the proof uses an optimal committee in the statement of the theorem (this is announced earlier in the paper, so it would be nice to have this clearly mentioned also in the theorem statements). I understand that this might have the side-effect of overburdening some statements, but I would ask that you consider clarifying this when revising the paper.
- L267: “approach [to] the reconfiguration”
- L294: some early intuition as to why this distinction will be needed would be nice to have.
- Footnote on page 7: “is” -> “was”
- In general: I would vote to use the same counter for Theorems, Lemmas and Propositions (it looks a bit strange at times, like on page 7).
- L390: Remove “as to”.

Limitations:
In general, the paper is honest about the quality of the work and where the unknown starts. I have no complaints in this regard.

No potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the multi-winner reconfiguration model in the approval setup. They focus on the following rules: AV, SAV, PAV, and CC. While AV and SAV can be solved in polynomial time, CC and PAV cannot. Therefore, the authors provide a more refined analysis of these latter two methods using the FPT approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The first contribution of this paper is the introduction of the new model. The second important contribution is a detailed study of four voting rules, particularly CC and PAV, since they cannot be computed in polynomial time.
Besides, the paper is overall clearly written (although a bit dry).

Weaknesses:
It is a bit worrisome that there are side comments from the authors in the appendix.
I also find one sentence in the “Conclusion” section a bit unusual. The authors wrote, “Our preliminary experimental investigations (see Appendix C) indicate that for most randomly generated committees, a reconfiguration path not only exists but can also be efficiently determined using a straightforward heuristic.” There is quite a large section regarding the experiments in the appendix. I think either this experimental part should be completely removed, or there should be a section (or subsection) about it in the main body. It is not a big issue, but it is a bit strange. To be sincere, I don’t know if in such a case I should comment and review these experiments or not.

Limitations:
.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the multi-winner reconfiguration problem. The goal is to find a transferring path between two winner committees and not change/decrease the score too much in the path. An example of this problem is switching products for streaming providers. This paper study this problem under for voting rule, CC, PAV, AV, and SAV. They show that this problem is solvable under AV and SAV, and do detailed complexity analysis for CC and PAV on multiple groups of parameters.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Novelty: Firstly propose reconfiguration problem into a social choice setting. 
+ Well-motivated with real-world examples. I am persuaded by the streaming provider case. 
+ Thorough and solid complexity analysis.

Weaknesses:
- Section 3 is not well-organized. It's almost a list of theorems and proof without any high-level implications. 
- I can't follow Proposition 4's proof.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the multiwinner setting with approval preferences. The authors propose a new framework of multi-winner reconfiguration, where the goal is to select a sequence of committees such that (1) the subsequent pairs of committees do not differ too much from one another, (2) the final committee is better or, at least, not ""too much worse"", from the initial one. The quality of a committee in the second part is measured using either Chamberlin-Courant scores, Proportional Approval Voting scores, Approval Voting scores or Satisfaction Approval Voting scores.

The paper solves the following computational problem: having two committees W and W', and having fixed bounds on the maximal number of candidates that could be changed between committees and on the maximal score loss in comparison to the initial committee, decide whether it is possible to reconfigure W into W'. The question is polynomial for AV and SAV scores. For CC and PAV scores it is PSPACE-complete, while the authors propose several FPT algorithms and other complexity results for special cases where certain parameters of the model are constant.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is very well-written, its structure and motivation are clear. I especially appreciate the real-world examples presented in the Introduction. The obtained results are technically challenging - as far as I checked, they are sound to me.

Weaknesses:
The only weakness that came to my mind is that the current definition of the multi-winner reconfiguration path is somewhat limiting --- for now it is impossible to apply it for other multiwinner rules that are not based on maximizing some kind of score (like the Method of Equal Shares, or the Phragmen's method), which otherwise would be a natural follow-up question of the paper. Hence, if anyone would like to further analyze different rules in this context, there would still be a need to reinvent the whole reconfiguration setup (and potentially, analyze all the four rules studied by the authors again, under the reinvented definition). This could limit the significance of the contribution, but it does not change my overall positive opinion about the paper.

Limitations:
The limitations are adequately addressed and there is no potential negative societal impact of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Y9aRCjuhnV;"REVIEW 
Summary:
This paper proposes a variant of Multi-Armed Bandits (MAB) named EdNetRMABs that model the interdependencies between learning contents to meet the real-world education scenarios. Subsequently, the authors introduce EduQate, an interdependency-aware Q-learning algorithm to optimize content recommendation given the EdNetRMABs. The paper demonstrates the theoretical and empirical effectiveness of EduQate with the experimental results of synthetic and real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work uses RMABs in education to model the learning process with interdependent educational content as the different knowledge concepts have relevance.
2. The paper introduces EduQate employs Q-learning to make decisions on arm selection that do not require knowledge of the transition matrix to compute an optimal policy and provides related theoretical analysis.

Weaknesses:
1. As this work considers interdependency awareness in content recommendation in educational scenarios, the generated group in the experiment is not clear. The authors need to clarify how to capture relevance between the selected exercises of each topic in different datasets. Furthermore, does the different number of topics influence the experimental results?
2. The definition of state space representing a student's knowledge states a binary value. However, in real-world scenarios, the knowledge states are multi-level. A fine-grained knowledge state estimation result is essential for the content recommendation in adaptive learning.
3. The paper lacks the details of EdNetRMABs. Some illustrations would help to understand.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces **EduQate**, a system that generates adaptive educational curricula using restless multi-armed bandits (RMABs). This method aims to efficiently achieve mastery across multiple interdependent educational contents. Unlike traditional methods that assume learning contents are independent, EduQate acknowledges and leverages the interdependencies between different educational concepts.

The paper addresses key challenges in modeling and optimizing the learning process in educational environments where the learning of different topics is interconnected. By considering these interdependencies, the proposed EduQate system ensures more accurate and effective personalized learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality** The paper introduces the concept of EdNetRMABs, a model that considers the interdependencies among learning contents in educational settings. This approach contrasts with existing methods that typically assume independence among different educational topics. By employing the Whittle index and Q-learning to develop the EduQate algorithm, the paper further demonstrates originality in adapting and integrating these techniques to address the specific challenges of personalized education.

**Quality** The authors provide clear definitions and a well-structured EdNetRMABs model. The optimality guarantees of the EduQate algorithm are well-explained and supported through rigorous proofs. Additionally, the empirical results are compelling, demonstrating the effectiveness of the proposed method over benchmark strategies using both synthetic and real data.

**Clarity** The paper is clearly written and well-organized, making complex concepts easier to understand. The authors systematically introduce the problem, the proposed solution, and both theoretical and empirical validations. The use of figures helps to elucidate the model and results. However, the clarity could be further enhanced by including more detailed explanations of the algorithm's implementation and practical applications.

**Significance** The significance of the paper lies in its potential impact on the field of educational technology. By addressing the interdependencies among learning contents, EduQate offers a more accurate and efficient approach to personalized education.

Weaknesses:
1. **Clarity of Explanations**: Due to the limited length, many explanations in the main text are not very clear. For instance, the survival and design of the student model and the content and information of the datasets are not thoroughly explained. For example, there is a dataset mentioned later that lacks similarity content, which should have been noted when introducing the dataset.

2. **Inconsistency in Notation**: In Section 4.1, ""Analysis of EduQate,"" the variable k in the second line has a different font from the k mentioned later. Additionally, the content related to k mentioned earlier is too far from this section, making it difficult for readers to understand and potentially leading to misunderstandings.

3. **Simplified Modeling**: The model uses only one pseudo-state, where if any content within a group is learned, all unlearned content in the group is marked as pseudo-state. This approach seems overly simplified. For example, in Case 1, if only one piece of content in a group is learned, all other content is marked as pseudo-state. In Case 2, if many pieces of content in a group are learned, the remaining content is also marked as pseudo-state. Although both cases result in a pseudo-state, their actual significance differs, and it seems the authors did not consider this distinction.

4. **Bidirectional Relationships**: The relationships between knowledge points in the paper are bidirectional, with only grouping relationships. However, in practice, many relationships between knowledge points within the same group are unidirectional. Recommending subsequent courses without recommending prerequisite courses first can lead to students needing to self-study prerequisite courses when learning subsequent courses, increasing their learning burden. The modeling in the paper does not seem to account for such unidirectional structures, instead using groups to link knowledge points, which could lead to this reverse learning issue.

5. **User Experience**: The discussion on user experience for educators and students is insufficient. Including qualitative and quantitative feedback from pilot implementations would be useful. The authors should conduct user research to gather insights on the intuitiveness and user-friendliness of the system for the target users. This could include surveys, interviews, and usability testing, focusing on ease of use, effectiveness, and areas needing improvement.

6. **Scalability of the EduQate Algorithm**: Although the paper mentions the efficiency of the EduQate algorithm, it lacks an in-depth exploration of its scalability in large-scale educational environments. Detailed performance benchmarking of the algorithm, including analysis of computation time and resource utilization, would be beneficial. The authors should provide a comprehensive performance analysis of the algorithm as data scale and complexity increase, including potential optimization techniques for handling large datasets.

Limitations:
The authors have explicitly acknowledged the limitations of their work, providing explanations for these constraints and discussing their impact on the research. The paper does not pose any negative social impact, as it aims to improve educational practices through a more personalized approach to learning.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes EduQate, an innovative framework that leverages EdNetRMABs to achieve interdependence among knowledge points. By using Q-learning, EduQate implements optimal strategies for personalized learning, offering optimality guarantees without needing explicit knowledge of transition functions governing student learning states. This approach dynamically adapts educational content to individual progress, optimizing learning experiences and outcomes. The effectiveness of EduQate is validated using three real-world datasets, demonstrating its capability to identify and implement the most effective teaching strategies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, this article provides valuable insights and has a good overall quality. It has a certain impact on the AI education domain. The proposed approach, EduQate, adapts the Q-learning method of the multi-armed bandit model and introduces active and passive arms to establish correlations between knowledge points, which demonstrates a level of innovation. The article ensures the validity of the approach at the theoretical level, making it more persuasive. In terms of narrative, the article maintains logical coherence and is comprehensible. The discussed topics are indeed important in the field of education.

Weaknesses:
The formatting of the article is messy, and the images are not very clear. The layout on the sixth page is problematic, with formulas and text mixed together, making it confusing. The images on the seventh page and some of the appendix images are blurred. I suggest using a different format to redraw them.

There are too few baselines in the experimental section, and there are very few validation experiments. Merely presenting outstanding performance in one experimental metric is insufficient to demonstrate the superiority of your method. It would be beneficial to include some reinforcement learning baselines for comparison. Other reinforcement learning methods, such as the Bayesian network approach mentioned in the citations, can also accomplish the task. Therefore, the necessity of your method is unclear in the paper.

Your method does not consider the specific circumstances of practical problems. It raises fairness concerns. Ensuring that all knowledge points are fairly selected in a specific educational context is a crucial issue. Although the article presents a good framework, I believe it is not feasible to use it.

Limitations:
As mentioned above, there are many factors to consider in the specific field of education. These factors include fairness, mapping each question's knowledge points to the ARMs mentioned above, and the presence of cold-start problems, among others. Time is also a crucial factor to consider during real-time usage, although the paper does not mention specific algorithmic time information in practice.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a solution to generate personalized learning curricula in educational settings, focusing on the challenge of accounting for interdependencies between learning topics. It argues that existing approaches, often based on the Restless Multi-Armed Bandit (RMAB) framework, fall short by assuming independence of learning content which is unrealistic in educational settings. To address this the paper introduces a new model Restless Multi-armed Bandits for Education (EdNetRMABS) enabling the capture of relationships between independent learning items. Building upon this model, the authors propose a new algorithm called EduQate, which leverages Q-learning and the Whittle index to compute an interdependency-aware teacher policy for recommending educational content. Notably, EduQate doesn't require prior knowledge of the transition matrix, unlike traditional Whittle index methods.

The authors provide a theoretical analysis demonstrating the optimality of EduQate for the case of recommending a single item at each time step (k=1). While finding the optimal solution for recommending multiple items (k>1) is proven to be NP-hard, a heuristic greedy algorithm is proposed to find solutions.

Through experiments on synthetic and real-world datasets (Junyi and OLI), the paper demonstrates the superiority of EduQate over baseline policies, including Threshold Whittle (TW), WIQL, Myopic, and Random. EduQate consistently achieves higher intervention benefits and average rewards across all datasets. Further analysis reveals the effectiveness of the replay buffer in EduQate, mitigating the ""cold-start problem"" common in reinforcement learning applications.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized and very well written with comprehensive explanations of the model, algorithm and the experimental setup.
2. Addressing the crucial issue of interdependency in learning content is a significant contribution. EdNetRMABs offer a realistic model for educational settings, moving beyond the simplifying assumption of independence prevalent in traditional RMAB approaches.
3. The paper presents a rigorous theoretical analysis of EduQate, proving its optimality for the k=1 case and providing complexity bounds for the k>1 case.
4. The authors address the cold-start problem by incorporating experience replay and perform ablations to show its effectiveness. This is a practical enhancement relevant for real-world applications.

Overall, this paper presents a strong contribution to the field of adaptive learning by introducing a novel and effective approach for generating personalized curricula that account for interdependencies in learning content.  The theoretical analysis, empirical results, and focus on practical considerations make this work both insightful and impactful.

Weaknesses:
The assumption of fully observable knowledge states is a significant limitation. Future work should explore extending the model and algorithm to handle partial observability, a more realistic scenario in education.
While the complexity analysis is provided, further investigation on the scalability of EduQate to larger datasets and more complex networks would strengthen its practical applicability.
The current work primarily focuses on maximizing long-term rewards. Further analysis on balancing exploration and exploitation in EdNetRMABs could offer valuable insights for curriculum design.

Limitations:
1. The experiments are based on simulated students and existing datasets. Real-world classrooms involve numerous factors not accounted for in the model, such as student motivation, engagement or diverse learning styles. This may limit the practical utility of this method.
2. The paper does not address the interpretability of EduQate's recommendations.
3. The assumption that the student states are fully observable is a major limitation of this work. This can result in overconfident recommendations.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a way to extend RMAB with Q-learning to accommodate the fact that some items/arms belong to a group. RMAB can be considered as a weakened version of contextual bandit CB but also a strong version of CB since it considers state transitions (explicitly defined on arms).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very solid paper with quite a few reasons to accept it:
1. The contribution of education simulators. Can the authors comment on whether they will release their simulators publicly for other education or bandit communities to use in the future? Currently, there is a lack of high-quality education simulators that are based on real-world data. This could be a great contribution. I hope the authors make an effort to make such resources public.
2. The technical contribution of introducing a pseudo-action that extends RMAB and WIQL.
3. Clean writing and presentation.

MAB has been used in many different real-world settings and is the main algorithm behind many learning platforms. Any innovation in this space will have a huge impact on students around the world. Unfortunately, this area is very niche and requires high technical sophistication. Unless someone can convince me that this paper's algorithm has already been published elsewhere or is fully derivative of some other work, I think it's a great paper to present at NeurIPS.

I might also be a bit concerned if the authors decide not to share their code/algorithm/simulators to encourage more future research in this direction.

Weaknesses:
1. Can add some more ablation studies.
2. Some clarifications (see question section).

Limitations:
The authors discussed limitations in a section.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
MFKfm5scHi;"REVIEW 
Summary:
This work develops efficient algorithms to approximate the Pareto-optimal set for different bi-objective clustering problems with solid theoretical guarantees. The problem can have different clustering objectives (e.g., k-separation, k-center, k-diameter, k-median, k-means, k-MSR) and/or different metrics (e.g., different distance measures).  

Two different types of algorithms are considered in this work to (1) approximate the whole Pareto set for problems that combine k-separation with various k-clustering minimization objectives or combine k-center/k-diameter with a k-clustering minimization problem, and (2) approximate the convex Pareto set (e.g., the convex hull of the ground truth Pareto set) for problems that combine k-median and k-means, where the Pareto set can be exponentially large.

Thorough and comprehensive theoretical analyses are provided to support and demonstrate the efficiency of the proposed algorithms. Experimental results show that the proposed algorithms can achieve promising performance on different bi-objective clustering problems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
+ This paper is well written and easy to follow.

+ The multi-objective clustering problem is valuable and important for many real-world applications. This work would be impactful and inspire many follow-up work on this research direction.

+ The proposed algorithms and their corresponding theoretical analysis are comprehensive and cover a wide range of different problem settings in a systematic way, which is a solid contribution.

+ The experiments are well-designed to nicely show the benefits of multi-objective clustering.

Weaknesses:
I enjoy reading this paper and do not see any obvious weakness of this work. Below are some minor concerns and questions.

**1. Centers Chosen from the Point Set** 

One limitation of the proposed algorithms is that the clustering centers can only be chosen from the point set. I can understand the necessity of this requirement when we hope to approximate all Pareto solutions (up to $O(n^2)$ with this requirement) in subsection 2.1 and subsection 2.2. 

However, it is not clear whether this restriction is still necessary when we want to approximate the convex Pareto set in section 2.3. What is the challenge for this case if we are allowed to choose the center from an ambient metric space? 

**2. Extension of the LP Rounding Algorithm**

As mentioned in the related work section, Alamdari and Shmoys (2017) leverage the LP rounding algorithm by Charikar et al. to handle the bi-objective clustering problem with k-center and k-median, while this work uses the primal-dual algorithm by Jain and Vazirani (2001). I am curious whether the LP rounding algorithm can be extended to tackle (some of) the other settings considered in this work.

In addition to the incomparable approximation factor, what are the pros and cons between the LP rounding algorithm and the primal-dual algorithm used in this work for multi-objective clustering?

**3. Generalization to more than Two Objectives**

This work focuses on bi-objective clustering, but many real-world applications might involve more than two objectives. What makes it hard to generalize the current methods to handle more objectives for approximating the Pareto set or convex Pareto set?

**4. Minimum Cardinality Set**

It seems that the proposed methods in this work do not consider minimize the cardinality of the approximate solution set. Can these methods be further extended to find the smallest (minimum cardinality) approximate Pareto set as discussed in Diakonikolas (2011)? 

**5. Finding the Most Suitable Solution**

The number of solutions found by the proposed methods for a given method can still be large. How can we efficiently find the best solution for each application (e.g., the best clustering in Figure 4)? Since the ground truth clustering is unknown, we cannot calculate the quality metric (e.g., Normalized Mutual Information and Rand Index) for each solution. Will the users have to check all the solutions themselves and then choose the most suitable one?

In addition, how can the clustering number $k$ be properly chosen for real-world multi-objective clustering problems?

**6. Order of the Problem Settings**

There is a mismatch between the order of problem settings in the main paper and the appendix. In the main paper, the order or problems is 1) k-separation + k-clustering minimization, 2) k-center/k-diameter + k-clustering minimization, and then 3) combination of k-median and k-means. However, in the appendix, the order is A) combination of k-clustering minimization , and then B) combinations involving k-separation. What is the reason for this choice?

**7. Publication Venue**

*(This part will not affect my decision.)*

This work conducts a comprehensive study on multi-objective clustering, which is not possible to be compressed into a 9 (or 10 in camera-ready) page conference paper. Many important materials (e.g., algorithms, all theorems, and theoretical analyses), which are actually the crucial contributions of this work,  have to be put in the supplemental materials. There is also no room left for a conclusion section. 

I think a journal like JMLR should be a more suitable choice to publish this work.

Limitations:
This work has discussed its limitations in the introduction (page 2, objectives subsection), but I think an explicit limitation subsection could be much more helpful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel framework for clustering pareto optimization. This paper studies the approximately pareto-optimal solutions for bi-objective k-clustering problems. The authors focus on the computationally very efficient single linkage / must-link constraints and the computation of pareto-optimal solutions. They first establish that it is not possible to simultaneously approximate k-separation and any of the minimization problems to constant factors. By iterating through all pairs of possible separation and possible radius/diameter, they obtain the approximate Pareto set. The authors also give the results combining k-center or k-diameter with a k-clustering minimization problem, and give the results combinations of k-median and k-means.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors give a novel approximately pareto-optimal approach for bi-objective k-clustering problems.The authors gives the results for combining k-separation with various objectives with (1,\alpha) approximate Pareto set with respect to sep with metric d_1 and f_2 with metric d_2. 

2. The paper produces an informative ensemble. These algorithms achieve provable approximation guarantee. The theoretical evidence are solid and the experiments are well-established.

3. The authors verify that the approximate pareto front contains good clustering which cannot be found by considering a single objective.

Weaknesses:
1. This paper is not very motivated. The authors seem not explain the motivation for studying the pareto-optimal algorithm for the clustering problem. Moreover, the authors did not give the motivation why the combination of these clustering objectives is studied.

2. The authors seem not conduct experiments on large-scale datasets, such as 100 million points. Moreover, there are also faster version of k-means++ method, such as the rejection sampling method proposed in [1] and random projection based k-means++ method proposed in [2]. The author did not consider comparative experiments with these algorithms.

3. The algorithms have polynomial running time. As far as we know, the pareto optimization of other problems can be done in linear time.

4. Some sections, particularly those involving heavy mathematical notation and proofs, could be made clearer with additional explanations or visual aids. This would make the paper more clear.

[1] Cohen-Addad V, Lattanzi S, Norouzi-Fard A, et al. Fast and accurate k-means++ via rejection sampling[C]//Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020: 16235-16245.

[2] Charikar M, Henzinger M, Hu L, et al. Simple, scalable and effective clustering via one-dimensional projections[C]//Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023: 64618-64649.

Limitations:
This paper is mainly a theoretical result, and there is no negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces novel algorithms to approximate the Pareto-optimal solutions for bi-objective k-clustering problems. The authors focus on combinations of clustering objectives such as k-center and k-means, or k-center with two different metrics. Usually, these objectives are conflicting and cannot be optimized simultaneously, making it necessary to find trade-offs. The algorithms provide provable approximation guarantees and are demonstrated through experiments to yield good clustering that single-objective approaches fail to capture.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper addresses the complex issue of multi-objective clustering. The motivation with k-diameter and k-separation problems accurately captures the difficulty of optimizing multi-objective optimization settings. The work provides a novel solution to approximate the Pareto front for bi-objective k-clustering problems.
2. The authors validate their approach through extensive experiments, showing that the approximate Pareto front includes superior clustering compared to those obtained by single-objective methods.
3. The paper has been written very well with detailed approaches.

Weaknesses:
1. It is not entirely clear the main novelty aspect of the work. In the setting of sec 2.1 where k-sep is combined with other objectives, the main takeaway seems to be that the authors were able to integrate existing state-of-the-art guarantees into their framework. Similarly for other sections for Pareto-optimal solutions were discussed, the authors leverage relies heavily on already existing approaches.
2. Most results are either incomparable with respect to the related work, because of bicriteria guarantees or translate to the existing guarantees. The results section should succinctly describe the main technical contributions (even if a few) for a better understanding of technical innovations.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
KgGhxmQFFy;"REVIEW 
Summary:
introduce a molecule tokenizer based on Codebooks which gets integrated into UniMoT, a unified molecule-text LLM

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
strong performance on a variety of benchmarks; outperforms/on pair with 3D-MoLM

Weaknesses:
- Table 1: include CLAMP [1] as well as standard molecular fingerprints (incl. in said reference); further KV-PLM linear probing for ToxCast results in 66.3 AUROC compared to 55.03 in your paper

[1] Seidl, P., Vall, A., Hochreiter, S., & Klambauer, G. (2023, July). Enhancing activity prediction models in drug discovery with the ability to understand human language. In International Conference on Machine Learning (pp. 30458-30490). PMLR.

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propsed Uni-MoT, a unified structure to align molecules with texts with a VQ tokenizer and the Q-Former in BLIP-2. By treating molecules as new word tokens in the codebook, Uni-MoT aligns the discrete token representation for molecules and texts, while also following the autoregressive manner of LLMs. The training of Uni-MoT follows four main stages, Causal Q-Former Pretraining, Molecule Tokenizer Pretraining, Unified Molecule-Text Pretraining, Task-Specific Instruction Tuning. The experiments demonstrate that Uni-MoT can achieve better performance compared to the selected baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The performance of Uni-MoT is overall good and better than the baseline models.
2. Uni-MoT provides a alternative solution for aliging molecules with texts.

Weaknesses:
1. Although the authors claim that Uni-MoT follows a different structure as shown in Figure 1 c, it turns out that it still follows the BLIP-2 [1] structure, which has been widely used to align 2D molecule graph [2] and 3D molecule information [3]. Thus, **the technical contribution and novelty of this paper are extremely limited**. Especially when VQ-VAE [4] is also a well-developed structure adopted in computer vision. This paper is more like simply swapping the input of the Q-former in BLIP-2. 
2. The experiments are only conducted on a single serie of LLM, Llama-2, **which is not sufficient to demonstrate the model agnosticsm of Uni-MoT**. In fact, LLMs like Mistral [5] and Meditron [6] might possibly achieve a better performance. Meanwhile, the selection of Llama-2 is also not convincing, as Llama-2 is not specially pre-trained on chemistry or biomedical corpora. 
3. **The seletion of the datasets is also worth discussion**. The result on ChEBI-20 is presented in Appendix, while the main experiments are conducted on PubChem. I am wondering why not also conduct the remaining experiments on ChEBI-20, as the data scale of ChEBI-20 is much larger than PubChem. At the same time, the reverse task, text-based molecule generation, on ChEBI-20 and PubChem is surprisingly not presented.
4. **The comparison with the baselines is not fair enough**. For example, MolCA adopts Galactica-1.3B as its backbone model, which is much smaller than Llama-2-7B. Thus, the proposed method can not demonstrate its superiority compared to previous methods. Notably, since the authors mentioned MolCA and 3D-MoLM, it is necessary to discuss the possible affects of the modalities. Furthermore, several SOTA baseline models like BioT5 [7] and BioT5+ [8] are not discussed.
5. **The ablation study is also not sufficient without providing the naive fine-tuning performance of Llama-2**. Besides, as Uni-MoT incorporates the VQ tokenizer, it is also important to discuss the size of the codebook.
6. During the pre-training stages, it should ensure that the **data leakage** is avoided. Considering the pre-training dataset adopted has overlaps with the fine-tuning dataset [7, 8], the performance gain could possibly come from the data leakage.
7. **Some claims and definitions are confusing**. e.g. In Line 44, ""a unified molecule-text LLM"", I do not agree with the claim of an ""LLM"". The ""LLM"" is still the Llama-2. It should be like ""structure"" or something.

#### References
[1 ]Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.
[2] Liu, Z., Li, S., Luo, Y., Fei, H., Cao, Y., Kawaguchi, K., ... & Chua, T. S. (2023). Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798.
[3] Li, S., Liu, Z., Luo, Y., Wang, X., He, X., Kawaguchi, K., ... & Tian, Q. (2024). Towards 3D Molecule-Text Interpretation in Language Models. arXiv preprint arXiv:2401.13923.
[4] Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., ... & Wu, Y. (2021). Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627.
[5] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.
[6] Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., ... & Bosselut, A. (2023). Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079.
[7] Pei, Q., Zhang, W., Zhu, J., Wu, K., Gao, K., Wu, L., ... & Yan, R. (2023). Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. arXiv preprint arXiv:2310.07276.
[8] Pei, Q., Wu, L., Gao, K., Liang, X., Fang, Y., Zhu, J., ... & Yan, R. (2024). BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning. arXiv preprint arXiv:2402.17810.

Limitations:
Yes. They have discussed the limitations as not enough tasks, data scarcity, and real scenarios.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work presents a new molecule LLM that uses a pretrained tokenizer to replace the projection layer. The tokenizer consists a Q-Former and a VQ module, which are trained with consistency loss. The model is evaluated on molecular understanding and generation tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The authors provided a novel framework to align text representation in LLMs and the molecules. 
- The authors conducted experiments on many datasets.

Weaknesses:
- The claim that adapter-based methods cannot do text-to-molecule generation tasks in not accurate (Sec.1). They can always adapt text-encoder to a pre-trained SMILES or graph generator. This may make this work not well motivated.
- In the tokenizer (Fig.2), it seems that multiple alignment methods are required to train the model, including: (1)the molecule and text contrastive in Q-Former, (2), the aligning MSE loss, (3) the SMILES decoder reconstruction. It's not clear about the design reasons, and if all of them are required.
- Given the learnable query has a fixed size, it may not perform well for larger molecules.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to use a vector-quantized tokenizer that incorporates a Q-Former to connect pre-trained molecule encoder, SMILES encoder, and SMILES decoder so that a language model can integrate molecule and text modalities. Based on the proposed tokenizer, the authors introduce a four-stage training strategy to train UniMoT, a unified molecule-text LLM proposed in this submission. The performance of the UniMoT is evaluated empirically with 7 tasks in the areas of molecule comprehension and molecule generation. Some ablation studies are also conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well-organized and well-written.
- Using a vector-quantized tokenizer and a Q-Former to connect different modalities could be somewhat novel.
- The proposed UniMoT outperforms baselines in most cases and reach comparable performances in others.
- The authors provide many implement details which increases the reproducibility.

Weaknesses:
- All the components are borrowed from existing works. Besides the pre-trained molecule encoder, SMILES encoder and decoder, both vector quantization and Q-Former are proposed in previous works [1][2]. The Q-Former part of this paper (Appendix A) is very similar to Q-Former's original paper. Even Figure 4 in this paper is very similar to Figure 2 in Q-Former's original paper [2].
- When generating molecules the proposed method relies heavily on a pre-trained decoder. In the decoder's original paper, the reported validity is 99.9 and no guarantee is provided that the generated SMILES string will be always valid [3]. The 100% validity reported in this paper could be attributed to overfitting.
- Some hyperparameter choices are not well justified and studied. For instance, the molecule codebook size is set to 2048, but there is no explanation why 2048 is chosen. How the molecule codebook size affects the performance is also not studied.
- There are some existing works about molecule tokenizers [4][5], the paper lacks the comparison of the performance using different tokenizers.
- The robustness of the model is not studied. Each molecule has many synonyms, how the proposed method performs given different synonyms of the same molecule is desired to know.

[1] Van Den Oord, Aaron, and Oriol Vinyals. ""Neural discrete representation learning."" Advances in neural information processing systems 30 (2017).

[2] Li, Junnan, et al. ""Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models."" International conference on machine learning. PMLR, 2023.

[3] Irwin, R., Dimitriadis, S., He, J., Bjerrum, E.J., 2021. Chemformer: A Pre-Trained Transformer for Computational Chemistry. Mach. Learn. Sci. Technol.

[4] Li, Xinhao, and Denis Fourches. ""SMILES pair encoding: a data-driven substructure tokenization algorithm for deep learning."" Journal of chemical information and modeling 61.4 (2021): 1560-1569.

[5] Schwaller, Philippe, et al. ""Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction."" ACS central science 5.9 (2019): 1572-1583.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
VXxj3XZ1X8;"REVIEW 
Summary:
This work demonstrates that overparameterized neural network models, which have many non-unique solutions, can lead to inconsistencies in representing the mouse visual cortex. It suggests a novel approach called ""adaptive regularization,"" where the regularization parameters in the loss term are learnable rather than fixed. The study also examines other approaches, such as normal regularization and pruning, and finds that these methods are beneficial for improving representation consistency. The proposed method significantly enhances consistency. This contribution is significant to the computational neuroscience community.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This work addresses a critical problem in computational neuroscience: reproducibility. It proposes solutions to mitigate this issue, which have significant impact on the computational neuroscience community. 

2. The authors rigorously define the consistency of computational models and thoroughly examine how regularization strength affects neuronal properties.

Weaknesses:
1. The study is limited to only one type of model, the Rotational Equivariance CNN.

2. In the Methods section, the terms ""Embedding"" and ""Mask"" are not defined. Moreover, Lp and L1 are described as functions of model parameters, but no parameters are defined in the text. This lack of clarity poses an issue for understanding. For instance, Lp could be interpreted in two different ways: as a function of the core but not the readout, or as a function of both the core and the readout. The same ambiguity applies to L1.

3. There are two types of computational neuroscience models regarding their outputs. The first type is task-optimized models, whose outputs are task-related, such as object class, object representation, or action. These models are trained to perform downstream tasks, such as supervised object classification, reconstruction, or playing games [1,2,3]. The second type is neural response fitting models, whose outputs are predicted neural responses, and they are trained to predict these responses directly. However, the reasons why the authors chose response fitting models over task-optimized models are not mentioned.

Reference

[1] Performance-optimized hierarchical models predict neural responses in higher visual cortex

[2] Unsupervised neural network models of the ventral visual stream

[3] Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments

Limitations:
This study investigates the consistency of neuronal properties and the prediction performance of regularized models. However, the models used by the authors are trained to predict neural responses. There is another type of computational model that is trained to perform downstream tasks, with neural-like representations emerging from the training. Future work could consider these latter models.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a systematic investigation of the use of deep neural network fits to biological neurons as the basis for neuron cell type classification. The authors explore how various factors such as regularization and model pruning can influence both the predictive model fit and the consistency of the neural clustering.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper presents a well-motivated, calculated set of experiments for evaluating how well deep neural networks can be used as models of mouse visual cortex. With increasing interest in this area and increasingly bold claims, work like this is a breath of fresh air. The controlled experiments and attention to detail provides compelling evidence that though DNNs may be our best working computational model of visual cortex, the are likely not a global optimum.

Weaknesses:
Overall, the biggest weakness I think is some lack of explanation. As someone who is very familiar with work in computational neuroscience but not a neuroscientist myself, there were a few things that felt under-explained and it wasn't clear if it was because I'm not the correct audience or because the authors did not provide enough context in the text. The authors do a good job of providing clear motivation for some of the fundamentals (L24-53) but then leave out context when things get a little more technical specialized (i.e. comparisons between readout mechanisms, significance of biological properties and tuning indices, etc) as a result it feels unclear what the reader is expected to know before reading.

- The font in the figures is a little unprofessional. Comic sans? In this economy?
- Figure 2G is hard to parse. I recommend splitting up the histograms
- The intuitive explanation of the factored vs gaussian vs adaptive readouts is not super clear to me. It might be worthwhile to spend more time on it, especially to motivate your proposed adaptive mechanism.
- minor typos (""initialiation"" in L183)
- L216 is nonsensical. I think some words are out of order
- Confidence intervals in Table 1 would be nice to gauge significance. Also, the table format makes it a little hard to visualize. Perhaps a graphical representation would be better.

Limitations:
Limitations are clear.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies models trained to predict the responses of neurons in visual cortex. The model has a shared multilayer network core followed by a final layer which maps the core features into individual neuron responses. The key question that this paper asks is, How reproducible are the individual neuron properties and ""cell-type"" clusterings inferred by such techniques? The main conclusion is that sparsity-inducing regularization is important for finding consistent cell properties. Pruning certain channels in the core is also shown to improve consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Consistency and reliability of these methods for understanding visual cortical processing is important for the scientific interpretation of these models. The design of the approach seems sound. A number of metrics are used, both scores as well as response properties used in neuroscience.

Weaknesses:
* I found the comic sans font and cartoon diagram style distracting in Figs 1-4.
* It isn't clear that much is gained from the t-SNE plots in Fig 2. Some evidence of different clusters, yes, but it's known that these kind of plots can be deceiving.
* How do you pick the optimal regularization parameters? By what metric/cross-validation procedure? (Line 242 gives $\gamma=10, \sigma=0.1$) Validation performance is reported but I'm unclear of the splitting procedure and whether there was a train/valid/test 3-way split or not.
* Minor points are made in ""questions"" section

Limitations:
The authors mention that their work is limited to a particular type of core architecture. It's also limited to being applied to just one dataset, whereas open data in other animals (or collected by other research groups, e.g. the Allen Institute's work) are available. I am not suggesting the authors do this in their revision but that they mention it as a limitation.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""Reproducibility of predictive networks for mouse visual cortex,"" explores the reproducibility of neuronal embeddings in the mouse visual cortex using deep predictive models. By introducing adaptive regularization and iterative feature pruning, the authors address key issues related to model overparameterization and provide a robust framework for achieving consistent functional representations of neurons. The work lays the groundwork for future research aimed at developing reliable and interpretable models for understanding neuronal function. 

The primary goal is to investigate the stability and reproducibility of neuronal function embeddings derived from deep predictive models. These models aim to predict neuronal responses to sensory inputs and have been proposed to define functional cell types via unsupervised clustering. The paper addresses the concern that deep models are often highly overparameterized, leading to multiple solutions that can represent the same neuronal function, thereby questioning the reliability of embeddings for downstream analysis.

The paper demonstrates that L1 regularization, which was used in early models, is crucial for obtaining structured and consistent neuronal embeddings when newer readout mechanisms are used. A novel adaptive regularization scheme is introduced, which adjusts the strength of regularization for each neuron. This method improves the consistency of neuronal embeddings across different model fits while maintaining predictive performance. The paper proposes an iterative feature pruning strategy to reduce the dimensionality of performance-optimized models by half without losing predictive performance. This pruning improves the consistency of neuronal embeddings concerning clustering neurons.  The consistency of neuronal embeddings is evaluated using the Adjusted Rand Index (ARI) of clustering partitions across models, correlations of predicted responses across models, and the consistency of tuning indexes describing known nonlinear functional properties of visual neurons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Adaptive Regularization Scheme: The introduction of an adaptive regularization method that adjusts the regularization strength per neuron is a significant novelty. This approach helps achieve better consistency in clustering neuronal embeddings without compromising on predictive accuracy.

- Iterative Feature Pruning: The feature pruning strategy to address overparameterization in deep models is another novel contribution. By systematically reducing the model’s dimensionality, the authors enhance the robustness and consistency of the neuronal embeddings.

- Comprehensive Consistency Evaluation: The paper provides a comprehensive evaluation of model consistency across different dimensions (embedding clustering, predicted responses, and tuning curves). This thorough approach highlights the robustness and reproducibility of the proposed methods.

- The paper is well-written with generally sufficient referencing to the previous methods.

Weaknesses:
- Bias from Regularization and Pruning: While the adaptive regularization and pruning strategies improve consistency, they may introduce biases that affect the biological validity of the neuronal embeddings. Over-regularization, for instance, can reduce the model’s expressive power and lead to less biologically plausible representations.

- The study focuses primarily on models with a rotation-equivariant convolutional neural network (CNN) core. The authors acknowledge that other core architectures, such as regular CNNs or Vision Transformers, were not evaluated. This limitation means the findings may not generalize across different model architectures.

- Despite the improvements in clustering consistency, there is a trade-off between consistency and predictive performance. The pruning and regularization strategies, while improving consistency, sometimes result in a drop in predictive performance, which is not ideal.

- The introduction of adaptive regularization adds another layer of hyperparameters (e.g., the log-normal hyperprior parameter) that need to be carefully tuned. This increases the complexity of the model training process and may require significant computational resources.

- The focus on achieving high consistency in clustering and embeddings may overshadow other important factors, such as the interpretability and biological relevance of the model outputs. Balancing consistency with these factors is crucial for developing useful predictive models in neuroscience.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
IxazPgGF8h;"REVIEW 
Summary:
This paper introduces ChatCam, a system that enables camera operation via natural language interactions. This system has two key components. CineGPT is proposed for text-conditioned trajectory generation and an Anchor Determinator for precise camera trajectory placement. Experimental results illustrate ChatCam’s effectiveness in text-conditioned trajectory generation and show the potential to simplify camera movements and lower technical barriers for creators.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	ChatCam is a novel system to generate camera trajectory via natural language. 
2.	The paper is well-written and easy to follow.
3.	The proposed GineGPT and Anchor Determinator are technically solid.

Weaknesses:
The system is only verified on several limited 3D static scenes, and the generated trajectory is relatively short. 

Some components are not fully verified. For example, what’s the effect of the text prompt “smooth panning speed” in Fig. 3? How does this text prompt affect the trajectory? 

Since the method was only evaluated on a small dataset, and the trajectory seems short, it's not clear whether the generations overfit to a small set of text prompts. 

Some experimental details are unclear.
1.	How about the training dataset? L114 suggests there are 1000 trajectories to train the trajectory generation. I would suggest introducing the training dataset in detail in Sec 4.
2.	What’s the time cost of Anchor Determination in inference?

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose ChatCam, a pipeline that utilizes GPT to translate the natural language into professional camera trajectory, which enhances the video production process for common users.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper divide the task into three steps: observation, reasoning and planning. The first two steps is completed by GPT and get the instruction of every subtasks. In the third step, the author proposes Anchor Determinator and CineGPT to generate the initialization point and camera trajectory.
2. The qualitative and qualitative results and video demo proves the efficiency of this method.
3. The paper is well written and easy to follow.

Weaknesses:
The design of this paper is clever and makes good use of the advantages of LLM. But I still has some concerns. If the authors can address my concerns, I would change my rate.
1. In the planning part of Figure 2, how can you ensure there is no collision with 3D scenes in every sub-step, such as “an S-shaped path, smooth panning speed”. If the  structure of the room is complicated, it is easy to collide with the scenes.
2. Is natural language really the best way for users to interact？Maybe user can import the 3D GS of scenes into Unreal Engine, where Luma AI provide this plugin (https://www.unrealengine.com/marketplace/en-US/product/luma-ai), and generate the camera trajectory by adding camera key points in the 3D space by UI interface, with position, rotation, camera intrinsics. The frames between key points can be obtained by using bilinear interpolation. You can preview whether collision has occurred in real time, and you can also adjust camera parameters by drag. (https://dev.epicgames.com/documentation/en-us/unreal-engine/creating-camera-cuts-using-sequencer-in-unreal-engine)
3. Can you show more complicated camera trajectories, such as dolly. The “straight forward and roll”, “S-shaped path”, “from left to right”,  which just need coarse control, seems can be easily obtained by the item 2 I propose above (UE).

Limitations:
The collision in the 3D scenes is not explicitly and hardly constrained in this pipeline.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for generating camera trajectories for rendering a 3D scene, conditioned on natural language. This problem statement is novel and original, very useful, and the paper demonstrates convincing results. The method uses two components: a language-conditioned camera trajectory generator, and a language-conditioned camera anchor generator. An LLM takes a high-level language query as input and creates a plan based on these two components.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The problem statement is novel and seems to be very useful with high potential impact. 
The method is creative and novel. 
Results are excellent. They demonstrate wide range of trajectories on complex scenes with several interesting elements.

Weaknesses:
The paper fails to demonstrate the planning output of the LLM. Is Fig. 2 an actual output of the method, or just an illustration? I would expect to see many more results of the actual plan used to generate the trajectories. It is very difficult to understand the workings of the method without these. 

The method seems far from reproducible. Sec 3.3 talks about several important components of the method, but only at a very high-level that would not enable anyone to reproduce the results. The details of the finetuning proposed in L142-146 are also unclear. 

The quantitative evaluations rely on ground truth trajectories. Shouldn't there be many possible GT trajectories consistent with a language query? How would this diversity impact the evaluations? The same holds true for output of the system. Can the method generate diverse outputs? If so, the paper should include such results, and ideally, should also update the metrics to reflect this. 

In the output in Fig. 2, there does not seem to be an end anchor point. How would the method know the extent or volume of space the S-shaped path should cover? Are there many cases where the volume of the space is undetermined based on the plan? 

Comparisons to baselines should also be included as videos, especially as the paper talks about their trajectories.

Limitations:
Limitations are not mentioned

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method to generate camera trajectories from user prompts. The idea is to pass on the prompt to an LLM, find a starting anchor location (searched and refined using initial images used to construct the radiance field) and then use CineGPT (a cross-model transformer) trained for the next token prediction on quantized camera trajectories. Compared with some minimal baselines, the proposed approach attains favourable results in terms of MSE errors (on translation and rotation) and user-reviewed visual quality and alignment metrics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and well presented. I was able to understand the parts ""explained in the paper"" quickly. The figures are well made, and the videos in the supplementary section were easy to consume. 

- The idea is well-motivated and I felt the work is in a good direction.

- I liked the idea of next token prediction on quantized camera trajectories and it appears novel to me.

Weaknesses:
- Baselines could be better; I feel the comparisons were extremely weak

- Many details in the paper are missing, making it difficult to comprehend the approach fully. The major misses are the details on the dataset construction, the proper explanation of baselines, and the LLM prompts. I give more detailed questions on the same in the next section.

- related work can be strengthened as well. They can cut a bit on the literature of radiance fields and 3D scene understanding and probably  add more on trajectory learning/optimization

Limitations:
The authors did not discuss the limitations properly. I do not see an explicit limitations section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
B3jt0Ran2t;"REVIEW 
Summary:
The paper describes the problem of matching students to daycare centers, with each family allowed to express preferences about the joint allocation of all siblings within the family. The authors present a modified notion of a stable matching, in which a family may choose to withdraw one of its children from a daycare in favor of a different child, so long as the daycare still prefers this assignment among alternatives with the first child removed. Under this stronger notion of stability, they show that the existing SDA algorithm may produce unstable outputs. They present an extension to the algorithm ESDA, whose successful outputs meet the new stability condition. They also show that the algorithm will be successful with high probability for a particular distribution over problem instances. In this distribution, children populate a daycare preference order by selecting from a fixed distribution over the daycares, and families aggregate these preferences into preferences over joint allocations, using an arbitrary aggregation function. Daycares sample a preference order over children from a Mallows model, with low dispersion. 
Finally, the authors show some empirical results from real Japanese municipalities in which ESDA produces stable matchings in all cases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The algorithm is a heuristic (for good reasons of computational hardness of the problem). Hence, I characterize the contributions as follows:
* The authors define the problem, generalizing from couple-matching instances that may be expressed as families of size at most 2
* The authors present a new notion of stability in the matching which seems to be justified, given that families are generally empowered to prevent one of their children from attending a particular daycare.
* The authors present the ESDA algorithm, which internalizes the new stability notion in the details of the algorithm execution, and produces stable matchings whenever the algorithm succeeds
* Empirical analysis shows the algorithm succeeding on real-world instances
* Finally, the authors show that real-world instances have some strong properties in terms of the similarity across daycares of the preference ordering for students. They also incorporate this observation into the algorithm design, and are able to show that under a certain random model of problem instances, the algorithm succeeds with high probability. In the real-world examples, the preferences of the daycare are largely provided by the municipality, so the assumption is very likely to hold.
* As a smaller point, I appreciate that the authors presented some analysis of the behavior of the algorithm when the dispersion of the mallows models becomes high.
* An additional smaller point: earlier results that operate with a vanishing fraction of couples in the population seem unsatisfying. The theoretical results in this paper instead allow a constant fraction of the families to have siblings, but place stronger constraints on the similarity of orderings of the daycares, which seems better justified.

Weaknesses:
My first question is about goodness of fit of the paper to NeurIPS. The best fit from the CFP is:
* Social and economic aspects of machine learning (e.g., fairness, interpretability, human-AI interaction, privacy, safety, strategic behavior)
specifically for strategic behavior. However, I'm not sure this should be called economic aspects *of machine learning* specifically. I'll leave this issue with the area chair---my personal view is that it's not a great match. There are some related papers that have appeared in past conference instances (for instance, on deferred acceptance variants, but with more of a focus on computational complexity of an algorithmic approach, such as https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html).

Second, I'm concerned about the family preference model in section 4.1. In particular, the title of the paper says ""Large Markets,"" and as the size of the market grows beyond a small geographic area, it seems like that geographic preferences (for nearby daycares) will play a role. However, the random model is based on a single global distribution of preferences that applies to all families across all locations. This distribution is then further constrained to place similar probabilities on all daycares. There is no analysis of the empirical data to justify this uniformity assumption. Additionally, the model assigns independent preferences to two siblings of the same family, which seems to miss a) the fact that a family may have certain specific desires, and b) the family's geo preferences will apply similarly to all children, and c) sending multiple siblings to the same daycare may provide complementarities such as less logistic overhead for transport.

Limitations:
I think the authors have done a good job here.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study a variant of the many-to-one matching problem called the stable matching problem with siblings, which generalizes the stable matching problem with couples. In this problem, some families $f \in F$ may have more than one and at most $k$ siblings, ordered by age $(c_1,\dots,c_k)$. Each family $f = (c_1,\dots,c_k)$ expresses a joint linear preference order for daycares, denoted as 
$>_f \subseteq D \times D \dots \times D$,

 where $>_{f,j} = (d_1,\dots,d_k)$ represents the $j$th preference of family $f$, and $d_i$ corresponds to the preference for child $c_i$. Note that $>_f$ is an ordered set—a tuple. Each daycare $d \in D$ expresses a linear preference order $>_d \subseteq C$ for a subset of children and a maximum capacity $Q(d)$.

The objective is to find a (stable) matching such that no blocked pair exists. In essence, a blocked pair is a tuple (of edges) $(x_1,\dots,x_\ell)$ and $(y_1,\dots,y_\ell)$ such that swapping $x_i$ with $y_i$ results in a new matching that assigns children to daycares with higher priority for at least one family while not negatively impacting the assignment of any other family or daycare preferences.

A stable matching might not exist for restrictive settings of the problem, as the authors illustrate with a simple example in Appendix B.3. My understanding is that if the preferences form cycles, it becomes impossible to find a feasible solution that satisfies these preference constraints. However, in a daycare market where priorities are generated from a specific distribution, particularly random, the authors demonstrate that the probability of a stable matching existing converges to $1$ as the number of children $n$ approaches infinity.They present algorithms to solve the problem and conduct experiments on synthetic and real-world datasets, demonstrating that they can find feasible solutions in most instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses a challenging and relevant variant of the many-to-one matching problem, the stable matching problem with siblings. The authors provide a comprehensive approach by defining the problem, presenting algorithms to solve it, and conducting thorough experiments on both synthetic and real-world datasets. Their work not only demonstrates the feasibility of finding stable matchings under specific conditions but also highlights the practical applications and implications for real-world daycare allocation scenarios.

Weaknesses:
While the paper makes significant contributions, there are some areas that could be improved. The writing is occasionally imprecise, making it challenging to follow the arguments and understand the definitions clearly. In particular, the choice of notation can be confusing (see detailed comments and questions). The structure of the paper is somewhat disorganized, with most of the proofs deferred to the appendix. Considering the strict page limits, this may be reasonable. However, Sections 3 and 4 could be compressed and written more concisely, and some proofs (or at least proof sketches) can be included in the main paper. I have only reviewed the proofs at a high level and have not verified the claims in sufficient detail. Given the strict reviewing timeline, this is the best I can do.

Limitations:
The authors do not discuss limitations and potential negative social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the problem of daycare matching with siblings, an extension of matching with couples. Here, children in families (of size 1 or larger) are matched to daycares. Families have ranked preferences over the tuples of daycares their children end up at (since their preference for one child at one daycare may affect their preference of another child at some daycare), and daycares have preferences over children. In most cases, daycares do not differentiate between children within a family. This is an important problem to solve in Japan, and the authors actually worked with the Japanese daycare matching market in order to produce this work.

Their contributions are: 1) introduce the problem along with notions of rationality/stability/assumptions/etc, 2) propose an extended sorted deferred acceptance algorithm and prove that it will only return stable matchings and will fail to recognize a possible stable matching with probability approaching 1 as the problem grows, and 3) run experiments on their algorithm.

Their model is defined in a pretty standard way according to stable matching literature. The novelty, of course, is the introduction of families generalizing the size of couples. Their stability definition uniquely allows children in the same family to pass along seats to each other, so that a family may use that to their advantage in forming a blocking coalition. They assume that daycares have similar rankings over children and that they are drawn according to the Mallows Model, and that families only have few daycares they are interested in.

The algorithm itself works much like deferred acceptance. First, single children can propose to daycares per usual. Then, families with multiple children begin proposing, presumably according to their full ranking of matching tuples. When a single child is unseated from a daycare, they can simply propose to their next choice. When a family f has an unseated child when family f' is processed, the algorithm attempts again under a new order where f' goes before f. This can cause many iterations.

In the experiments, they use real datasets from Japan as well as larger synthetically-generated datasets. They compare their algorithm to a baseline constraint programming solution, showing that their algorithm returns the same solution faster.

Quick note: diameter is introduced in the main body but only used in the appendix. Perhaps move it to the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Stable matching is a very well-respected area of research, and this seems like a very natural formulation of the problem. It is particularly interesting that the authors are working directly with the market in need and they seem to have been given positive feedback about their work, so this work will almost definitely have a valuable use case. For the most part, the paper is written very well and it is very easy to get a high level understanding of most aspects of the project. Overall, I am very pleased with this paper and would be excited to see it at NeurIPS.

Weaknesses:
I am a bit concerned about the literature review provided. I am aware there is much more research that has been conducted on matching markets with complementaries (I am not knowledgeable enough to know what papers would be most useful), and I know there are various papers in this field. However, very few previous works are cited in this paper. It would be great if the authors could clarify the place of their work in the context of current literature and give confidence that this problem or a generalization of it has not already been studied. In fact, this is very important to motivate the paper.

Otherwise, there are a few points in the paper that are unclear. Much of it is very high level and lacks details, which is okay because it writes a narrative, but it comes at a cost of understanding the details of the proofs. More notably, I think the authors didn't spend enough time explaining their algorithm. I found it somewhat vague and I was uncertain about how it worked, and yet it is an integral part of the paper. This definitely needs to be improved.

Limitations:
Everything seems adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the existence of a stable daycare-children matching in the presence of siblings from the same families with same preferences over the daycares. The authors particularly study the case when the daycares have similar preferences over the set of children, and the market size is large. They propose a variant of the Sorted Deferred Acceptance algorithm to compute the stable matchings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well motivated by the real-world observation that stable matchings exist in the markets as opposed to what the theory suggests. This observation allowed the authors to make necessary adjustments to the assumptions that are sufficient for the theory to work out.
2. The authors take a systematic approach to the problem. They first define a new notion of stability that takes siblings into consideration and show that stable matchings may not exist in the presence of siblings and that the previous algorithms do not work for this new notion of stability. They then consider a specific random daycare market, mention the drawbacks of the existing methods of computing stable matchings, and then prove that a modification to the existing algorithm can find stable matchings with the new definition of stability.
3. The results by themselves are quite interesting; that stable matchings exist even in the presence of siblings with complementaries.
4. The analogy is drawn between the related work in stable matchings with couples and stable matchings with siblings

Weaknesses:
1. The assumption that day cares have similar priorities over children is slightly unrealistic.
2. The random daycare market for which the results are derived is somewhat restrictive.

Limitations:
Limitations sufficiently addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RdAfUp4LcD;"REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
MDdOQayWTA;"REVIEW 
Summary:
The paper derives a new   time-independent inequality for likelihood ratios in the Generalized Linear Model. The proof is based on a PAC-Bayesian approach with a well-chosen prior. The result is applied to GLM bandit models, improving a variant of GLM-UCB. The resulting regret bound removes a exponential dependency in the norm of the unknown parameter.
Several examples of GLM are discussed, and minimal numerical experiments are reported. The comparison with OFULLog+ is convincingly exposed, both in theory and in practice.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, I consider that the submission is technically sound, somewhat incremental, but will be of interest to the community.

Weaknesses:
The submission is correctly written, but with a number of clumsinesses (some are listed below). 
The main text contains not much more than the statement of the results and the (unsurprising) description of GLM-UCB+, while the supplementary material is written a collection of appendices that are not very well presented. For example, the first appendix is called ""Missing Results"", which is pretty unspecific. I did not check all the details, but the main lines look correct.

l.69 undefined notation \mathcal{B}^d(1)
l.77 give a reference
l.80 could you precise what is R_\mu in those examples?
l.95 the sentence is grammatically wrong, something is missing
Thm. 3.1: replace ""where"" by ""for the choice""
l.129 it is not the log-likelihood but the likelihood
l.583 is it not possible to suggest a proof instead of a reference to ""WoframAlpha"" (to a plot?)

Limitations:
This section does not really seem relevant to this mostly theoretical submission

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Confidence intervals for GLMs using a PAC-Bayes approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Confidence sequences and bandit algorithms for GLMs are a hot topic, and the authors contribution to that topic is solid, both because of the strength of the result itself, and because the proof itself is very simple and easy to verify---something that cannot be said about some of the previous work on the topic.

Weaknesses:
The paper is written in a style that might be described as ""being written for the reviewers"". It contains misleading and inaccurate claims which are meant to, presumably, impress the reader with the quality of the results. I give some examples shortly. Its unfortunate, because the ideas in the paper are good, and if only the authors had simply written about their work in a neutral, descriptive manner, I would absolutely recommend that the paper is accepted. However, I do not believe the overclaiming to be some sort of an accident---which if it were, the authors might fix upon my pointing it out---and so without any system for ""revise and resubmit"", must recommend rejection.

Some examples of problematic claims (emphasis mine in all quotes):
1) Lines 1-2, the authors claim that ""[...] for any generalised linear models (GLMs) that is guaranteed to be convex and numerically tight"". To my understanding, the authors have no guarantee that their result is numerically tight for every GLM, and thus the first sentence of the abstract is false (is there even a guarantee in the paper that there exists a single GLM for which it is numerically tight?).
2) Line 53, ""Our main novely lies in __cleverly__ using [...]""; whether the approach is clever or not is perhaps something for readers to judge, not for the authors to proclaim.
3) Line 107, ""we completely remove the poly(S)-dependency from the radius, resolving one of the __open problems__ posited by Lee et al. (__2024__)."" An open problem is an well-known unsolved problem in a field (that is, a problem that numerous other researchers have attempted and failed to solve). Your ""open problem"" is not that. Rather, it is a single sentence in the middle of a paper published at a conference that took place less than 3 weeks before this paper was submitted that simply states that the authors leave it for future work. 
4) Line 111-112 authors claim that ""perhaps more importantly, [ellipsoidal confidence sequences allow] one to equivalently rewrite the optimistic optimization in the UCB algorithm as a closed-form bonus-based UCB algorithm"". Is this true? The claim appears to be that, if $\mathcal{X}$ is a subset of the unit ball and $\Theta$ is an ellipsoid, then the quantity $b(\Theta) = \\max_{x \in \mathcal{X}} \max_{\theta \in \Theta} \langle x, \theta \rangle$ has a closed form expression. Could the authors state the closed form expression they have in mind for, say, $\mathcal{X}$ being an irregular polytope with $2^d$-many vertices? And if the resulting expression requires $O(2^d)$ time to evaluate... then the statement is trivial, and ellipsoidal form of the confidence sequences is unimportant. (and of course, fit a spline to those 2^d vertices, and now you cannot even write the solution as a sum over the vertices!).
5) Lines 307-308: in the conclusion, the authors state that ""[their algorithm] is numerically verified to be the best performing algorithm."" The authors compared against __one baseline__ on a __single experiment__ varying only __one experimental parameter__ between __two values__, and the experiment was __two dimensional__. *Really?* 

Other than that, section 3.3 seems like it could contain interesting insight, but I'm worried that the way it is written, the only persons that might be able to understand it are either the authors themselves, or someone that has spent just as much time poring over the paper as the authors have done. I would suggest that the authors either expand on it and explain it better, or cut it.

Some minor typos:
- Line 312-313 you talk of ""Bayesian randomized (exploration) algorithms for GLM bandits"", but then cite 5 papers for these algorithms, out of which the four I am familiar with are frequentist algorithms, not Bayesian.
- Line 95-97, second half of sentence seems to be missing
- Line 129, the definition looks like the likelihood ratio, not the log-likelihood ratio as claimed
- Theorem 3.2, second line
- Line 82, the set in probability is wrong
- Line 75 rewards should be in filtration

Limitations:
.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Likelihood ratio based confidence sequences for generalized linear inference, who's width only depends logarithmically on $S$, the bound on norm of the parameter vector. This is achieved by utilizing a pac-bayesian change of measure inequality, with the prior and posterior distributions chosen very carefully. Tightness of the bounds are compared with some prior work and a small numerical experiment is provided to demonstrate potential benefits of the confidence bounds, for the application of logistic bandits.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- Key contribution is reducing the $\mathrm{poly}(S)$ dependency of the width of the confidence sets to $\log S$. Practitioners typically do not use state-of-the-art CS prescribed by theory because 1) they are tough to compute 2) depend on unknown parameter. This paper takes a step towards mending this gap by addressing the second issue.
Since dependence of the width on $S$ is logarithmically, then the practitioner can choose a conservatively large upper bound on $||\theta_\star||$ and not suffer from loose/uninformative CS.
- The work benefits from versatility of LR-based CS and automatically gives results that are applicable to multiple data/noise models.
- Gives convex CS for logistic bandits that have rate optimal dependence on $\kappa$ parameter. I might be wrong, but as far as I know, prior work with such dependency on $\kappa$ resort on non-convex sets [Faury 2022]. *I am not aware of the latest results, particularly [Lee 2024].*
- The proof is light and combines simple ideas from not-so-connected areas. Compared to prior work on parallel topics (on GLM, logistic, or poisson models), this seems like a more elegant approach to prove anytime validity of confidence sequences.

Weaknesses:
I think improving dependency on $S$ gains significant relevance, once it yields a practical algorithm. 
If there is no computationally efficient/stable way of calculating the confidence sets, then the relevance of the results is limited to a subset of the bandit theory community. Maximising the UCB on the proposed sets (Thm 3.1 and Thm 3.2) does not seem to be a computationally feasible task, particularly for higher dimensions. Further, I'm afraid that practical relaxations/approximations, would blow up the width, and loosing the current theoretical edge.

- The ellipsoid sets are proposed as a easy-to-implement alternative (which actually might not be the case, due to the hessian in the norm). But it is not clear to me if they are similarly tight. Depending on the parameter $R_s$, these sets may again scale with $\mathrm{poly}(S)$.

- While the paper makes a valuable theoretical contribution, I think it would need more experiments to appeal to a broader community. For instance, a proper benchmarking of the CS against practically common choices (vanilla GP-based CS based on a very loose Linear regression model that uses a sub-Gaussian noise with a large variance) and showing that the statistical gains are worth the computational effort by reporting the regret, and the computational efficiency (e.g. number of flops).

- Experiments do lack comparison with relevant baselines and are only in the logistic case. In particular, Emmenegger 2023 and Faury 2022 are two algorithms that should be compared to. IIRC, Emmenegger 2023 does not work well for logistic bandits, so this would help strengthen the message of the paper. However, Faury 2022 seems to make a strong case on the joint computational and statistical efficiency of their algorithm (for logistic bandits).


This is not a weakness, but I should mention that I am not personally aware of the common rates and parameter dependencies in LR-based confidence sequences. Therefore, I *could not* verify the dependence of the width on certain parameters, so I don't know if everything is optimal, or we are sacrificing something else on the path to $\log S$.

Limitations:
The proof technique might be limited to linear setting, with in my opinion can be viewed as a limitation, if one of the contribution points of the paper is the proof technique. 

Practical limitations and applicability are not adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work considers generalized linear models where the distribution of observations, conditionally on a context vector $x$ and an unknown parameter $\theta^\star$, are generated from a (known) exponential family of distribution. Their main contribution is a new confidence sequence for online estimates of $\theta^\star$, leading to improved regret in the context of Generalized Linear Bandits when used to calibrate a UCB algorithm corresponding to the model for the observations.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well written and easy to follow, and the presentation of the technical arguments is very pedagogical. The literature review seems extensive, and the authors carefully separated the literature that inspired the design of the confidence sequence and the literature related to GLM and bandits. The results are also well presented, with explicit constants so that it is not difficult to reproduce the results from the paper. In that regard, I appreciated how the authors cared about facilitating future implementation of their approach. I also appreciated the precise derivations proposed for some specific families of distributions, that are good illustrations of the results.

Weaknesses:
I am not very familiar with a large part of the literature invoked by the authors, basically the literature presented in Section 3.3. Hence, it is quite difficult for me to assess the technical contribution of the paper (not a weakness, but I'm using this space to say it). For a non-expert reader, Section 3.2 is a bit hard to follow. In particular, I did not see the connection between the result and Theorem 3 of Foster et al. (2018).

Regarding the bandit part, it might be beneficial to extend the ""proof sketch"" part to establish the main arguments that are different from previous works. In particular, l. 215 the authors say ""one needs extra care in the analysis to ensure that the regret bound is also tight"", but after that it seems that all arguments are standard. It might be interesting to provide more details or to remove that sentence.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
aeYNVtTo7o;"REVIEW 
Summary:
This paper introduces scCello, a Transcriptome Foundation Model (TFM) which incorporates prior information from cell ontology graphs denoting the relationships between different cell types in order to guide cell representations. The pretraining objective in the scCello framework encapsulates masked gene expression prediction, a supervised contrastive objective for aligning cell representations to cell ontology terms, and relational alignment of similar cell types, which helps guide representation learning during pretraining to reflect  taxonomic relationships between cells. The foundation model, scCello, is pretrained on 22 million cells from the CellxGene public data repository, and demonstrates improved performance over other single-cell foundation models in downstream tasks, including cell type clustering, cell type identification, marker gene prediction, and batch integration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This work presents a novel foundation model for single-cell transcriptomics, scCello, that incorporates prior information about cell ontology in order to guide representation learning over a large pretraining corpus. The methodology and motivation for the paper is clear, and the work represents a significant step towards training foundational models that incorporate prior information, which subsequent works may build and improve upon.

For pretraining, the authors introduce a multi-component loss function, encapsulating: (i) masked gene expression prediction, (ii) a supervised contrastive objective to align representations to of cells with similar cell ontology terms, and (iii) a relational alignment objective which enforces cell representations to follow the structural similarity of their cell types in the cell ontology graph. For quantifying cell type similarity, the Personalized PageRank (PPR) algorithm is used to derive structural similarity of pairwise cell types in the cell ontology graph, which is then used to guide representation learning.

The single-cell RNA seuqencing data preprocessing methodology seems sound, and follows common techniques for data preprocessing used by other foundation models. The authors also include a hyperparameter and compute resource comparison of scCello against other established single-cell foundational models - Geneformer, scGPT, scTAB, UCE.

Overall, the paper is well-written, motivated well, and clearly explained. Adequate details are provided for the loss objectives and methodology, and additional dataset acquisition and preprocessing details are provided in the appendix.

Weaknesses:
The authors mention that cell type ontology identifiers were obtained for the 22 million cell pretraining dataset from the CellxGene database, to enable mapping between individual cells and cell ontology. While this allows for additional priors from cell ontology graphs to be used during pretraining of scCello, it also necessitates labeled data during pretraining, which may limit the scalability of scCello in terms of available pretraining data compared to other methods which do not require annotated pretraining single-cell data.

Limitations:
The authors have adequately addressed the limitations and potential negative impacts of their work, and agree to do a code implementation release upon acceptance of the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduced a transcriptome foundation model (TFM)  named scCello to resolve the current problem of most of the TFMs they treat cells as independent samples and ignore the taxonomic relationships between cell types. By integrating cell ontology information as well as incorporating three key objectives in the pretraining framework: masked gene prediction, cell type coherence loss, and ontology alignment loss, the model can learn gene coexpression patterns, cell type-specific representations, and structural relationships between cell types. The authors performed a series of experiments to show that the model can predict the unseen cell types, integrate datasets across diverse batches, cluster cell types, predict the cancer response to the drug, and predict the marker genes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript is well-written and well-structured, and it is easy to read. 
2. The authors effectively incorporate cell type ontology information at the pertaining stage, demonstrating in the comprehensive experiment results that this ontology information enhances the model's ability to learn cell type taxonomic relationships, thereby producing more biologically meaningful cell representations.

Weaknesses:
1. In the zero-shot cell type clustering experiment, it is not clear that the authors run the Louvain algorithm with default settings or the optimal setting on Seurat, Harmony, and scVI.
2. In the marker gene prediction part, what is the difference between predicted marker genes based on scCello and the traditional marker genes based on the differential expression level? 
3. There is a paper (A Deep Dive into Single-Cell RNA Sequencing Foundation Models) indicating that the L1 logistic regression model achieves decent performance in cell type annotation task. Could the authors add the L1 logistic regression model for comparison in Table 2?

Limitations:
The authors have addressed all limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents scCello, a single-cell, Cell-ontology guided Transcriptome Foundation Model (TFM) that leverages cell-type relationships from cell ontology graphs to enhance cell representation learning. scCello incorporates three levels of objectives during pre-training: masked gene prediction, cell-type coherence, and ontology alignment. This approach improves the model's generalization and transferability capabilities, leading to superior performance in tasks such as cell type identification, novel cell type classification, marker gene prediction, and cancer drug response prediction. scCello is also robust to batch effects and demonstrates high parameter efficiency compared to existing TFMs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.Integrating cell ontology graphs into TFM pre-training: This is the most innovative aspect of the paper. scCello improves model understanding of biological relationships between cells by incorporating cell-type relationships from cell ontology graphs into TFM pre-training, enhancing the model's generalization capability and transferability.
2.Introducing a multi-level objective function: scCello employs a multi-level objective function that encompasses gene-level masked gene prediction, intra-cellular cell-type coherence, and inter-cellular ontology alignment. This multi-level approach enables the model to learn complex relationships between genes, cells, and cell types, leading to more precise and robust cell representations.
3.Validating the model's effectiveness and advantages: The paper presents a comprehensive set of experiments demonstrating the effectiveness and superiority of the scCello model in tasks such as cell type identification, novel cell type classification, marker gene prediction, and cancer drug response prediction. Additionally, the paper highlights scCello's robustness in handling batch effects and its parameter efficiency, suggesting the model's strong potential for real-world applications.

Weaknesses:
1.Lack of Theoretical Justification: The paper heavily relies on empirical results to showcase scCello's effectiveness. However, it lacks a rigorous theoretical analysis of the proposed approach. A deeper theoretical understanding of the objective functions, their impact on learning, and how they contribute to generalization capability would significantly strengthen the paper.
2.Limited Comparative Analysis: The paper primarily focuses on comparing scCello with existing TFMs but doesn't thoroughly analyze its performance against methods specifically designed for downstream tasks like cell type identification, marker gene prediction, and cancer drug response prediction. A more robust comparison with specialized methods would better illustrate scCello's strengths and highlight its relative advantages.
3.The paper's current model size might be insufficient to fully capture the complexities of single-cell transcriptomic data. This limitation could hinder the model's ability to achieve optimal performance on tasks requiring deep understanding of complex biological processes, especially when dealing with large and diverse datasets.

Limitations:
1.The cell ontology is constantly evolving, and the paper acknowledges that scCello currently requires retraining the entire model for any updates. This limitation hinders the model's adaptability and its ability to keep up with the latest knowledge in cell biology.
2.The paper states that they aim to scale up the model size in future work. This indicates that the current model size might not be sufficient to fully capture the complexities of single-cell transcriptomic data, potentially limiting its performance on certain tasks or datasets.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new transcriptome foundation model (scCello) to generate cellular representations from single-cell RNA-seq data. The key contribution is the integration of known cell type labels (previously annotated by CellxGene submitters) within two novel objectives. First, the authors introduce a cell-type coherence loss to minimize the distance between a cell's representation and the learned representation of its associated cell type. Second, the authors introduce an ontology relational alignment loss to ensure that the similarity between the representations of two cells matches the similarity of their corresponding cell types in an ontology graph. The authors benchmark their model against previously pre-trained transcriptome foundation models and traditional (non-foundation) models. They evaluate performance on tasks including zero-shot cell clustering, fine-tuned cell type classification, and marker gene discovery and find that scCello is state of the art on almost all tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- *Conceptually simple but effective*: Utilizing cell type annotations, which are almost always available in published scRNA-seq datasets, to enhance the model's representations is smart and relatively straightforward. Other methods have proposed incorporating cell type labels before, but a cell type classification loss that treats each cell type as independent has shortcomings. The ontology relational alignment loss presented here which uses pre-existing ontology graphs to determine the similarity between cell types seems intuitively effective and empirically turns out to be too. 

- *Meaningful task selection*: The authors do a good job benchmarking their model on meaningful tasks. For example, they evaluate clustering performance not just on heldout IID data, but also in unobserved cell types, tissues, and donors. The model's improved performance on novel cell type classification and marker gene prediction suggests that it has captured biologically meaningful information. 

- *Appropriate baselines*: The authors do not just compare to existing transcriptome foundation models, which have been heavily criticized for not being competitive with traditional methods. 

- *Algorithm for novel cell type classification*: Their method of comparing the similarity vector between a cell and prototype representations of each cell type to the similarity vector between a cell type and all other cell types, as derived from the ontology graph, is clever.

Weaknesses:
- *Lacking analysis on ontology relational alignment loss*: While it seems intuitive that the ontology relational alignment loss is beneficial, the significant performance improvements are unexpected and probably warrant further analysis. For example, understanding which cell types show improved clustering performance could help provide some intuition. 

- *Missing some baselines*: Since the model assumes that cell type annotations are available, it should be compared against traditional methods that utilize these annotations (e.g. scANVI).

- *Limited to 10x data*: The batch correction results, as is, are impressive, but it would be interesting to see if this method could handle data from different sequencing technologies.

Limitations:
The authors are forthcoming about the limitations of their model.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces scCello, a new transcriptome foundation model (TFM), which learns cell representations over RNA gene expressions. Apart from using Masked language modeling (MLM), masking random gene expressions in cells, the work leverages structural knowledge from ontologies to improve the learned representations. It enforces cell type coherence, where cells of the same type should be close in representation space. Similarly, the representation of structurally close cells, measured over a PageRank measure, should be close. These structurally inferred objectives are encoded as separate contrastive losses. The pretaining objective tries to minimise the sum over the MLM loss and the two contrastive losses together with a regularisation term, which tackles class collapse. scCello is backed by a transformer-based encoder-only model with roughly 10.5 min parameters, which is rather small compared to competitors. The evaluation covers multiple downstream tasks and an ablation study over the optimisation objectives. The results show that scCello is either competitive with state of the art performance or improves it and that the aggregation of the losses is beneficial.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Data: It's great that the work leverages diverse data sources (structural and token-based) and tries to incorporate the strengths of both of them by using custom losses. 
- Experiments: The evaluation is extensive comparing the performance of the proposed model with multiple competitors and across multiple tasks. I also appreciated the ablation study and the analysis of overall performance wrt. #parameters. 
- Model: With a reasonable size the produced model can surpass many larger-sized competitors.

Weaknesses:
**Incorporating structural knowledge**

GNN have been used for incorporating structural knowledge into pretraining of transformer-based models. For instance, take a look at GraphFormers (https://arxiv.org/abs/2105.02605). Such methods can be more elegant than the proposed PPR metric and the contrastive loss, since everything can be captured inside the model and no custom metric is necessary. While the author show that their proposed approach is beneficial for overall performance, such methods should be discussed or compared to, since they overlap in their goal of fusing structural and textual data with a major contribution of the paper.

**Limited Novelty**
The paper does not advance any method or approach. It is an application of existing methods for creating a TFM model.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
B1tCaKP5nB;"REVIEW 
Summary:
The authors proposes a method for testing conditional independence in presence of discretisation.
They assume the variables to be jointly Gaussian, and that some of them are accessible only after discretisation; thus the data contain a mix of discrete and continuous variables.
Discretization might remove some conditional independencies. Assume X1 to be independent from X2 given X3.
It might be that X1 becomes instead dependent on X2 given \tilde{X3}, where \tilde{X3} is the discretised  X3.
The authors develop a way to infer the latent correlation on the real value variables and they propose a novel test for conditional independence for  the setting mixed continuous and discrete variables.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Testing conditional independence with mixed types of variables is an interesting topic and the work is original. 
The presentation is good, even though I could not follow the development of the bridge equations (this might be because I am not familiar with the adopted techniques).

Weaknesses:
* I am skeptical about the specific research question addressed.
X1 and X3 are independent given X2; yet they might be not independent given the discretised version of X2.

For instance, with ref to  Fig 1a,   X1 and X3 are formally dependent given \tilde{X2}; yet the induced dependence might be very weak.
I argue that the strength of the induced dependence depends on how the discretisation is done.
Example: X2 is human height, discretized into bins of few centimeters; then \tilde{X2}  is practically as informative as X2 and the induced dependence is likely to be negligible, in which case it might be sensible not rejecting H0.

The author did not discuss the impact  of the adopted discretization approach on the induced dependence.
Also, there is no compelling example in which discretisation induces a strong dependence.


* In the first set of experiments the test is better calibrated than the competitors, but it has by far less power. Overall, these results are not very strong.


* The competitor tests (Z-test and chi-square) are not modern.
There is no comparison against  existing tests for mixed variables; I can cite for instance Bayesian Independence Test with Mixed-type Variables, Benavoli et al. 2021.
Another simple baseline which I think should be present: test conditional independence having discretized all variables and use modern test for discrete variables (as a starting point, I suggest  those available in bnlearn https://www.bnlearn.com/documentation/man/conditional.independence.tests.html )

Limitations:
No potential negative societal impact.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a test for conditional independence in case of discretized variables, i.e. variables that originally were defined over a continuous domain and are then mapped to a discrete domain. In this case, a binary domain. Authors propose to bridge the unobserved continuous variables with the observed discretized variables with equations modeling the original covariance/precision matrix coefficients. Both theoretical and experimental evidence support the proposed testing  methods. 

Typo at page 13, line 491, equation 17, missing closed bracket.

Citation 3, 4 are the same reference.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper contributes in a significant way on a topic which is crucial in the context of structure learning/causal discovery. Specifically, the proposed theoretical framework is solid and sound, explaining the logical steps that lead to the conditional independence test. The major strength points of this contribution are:

- The self-contained graphical representation of the discretized variables and their original continuous ones,
- The flexibility of the bridge equations, that can be adapted to specific cases without compromising the theoretical soundness.
- The performance of both the unconditional and the conditional independence test.

Weaknesses:
The only weakness is that I would do more experiments.

Limitations:
The only limitation, that is also discussed by authors, is that the ""discretized"" variables are in fact ""binary"" variables, which limits the applicability of the proposed test.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel statistical method for testing conditional independence (CI) when some of the data is discretized. Initially, the authors introduce bridge equations to estimate covariance and establish asymptotic normality, facilitating an unconditional independence test. For the conditional independence test, they employ nodewise regression to recover precision coefficients. Theoretical analysis and empirical validation are provided to showcase the method’s effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper introduces a conditional independence test tailored for scenarios with discretized data, which often encountered in financial analysis and healthcare due to data collection or measurement constraints. The CI test is highly adaptable, capable of handling situations where both variables are discretized, both are continuous, or one is discretized. Numerical experiments on both synthetic and real-world datasets demonstrate superior performance in various scenarios.

Weaknesses:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Limitations:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses a critical issue in Conditional Independence (CI) testing methods, specifically when the available data is a discretized version of the original continuous data. Traditional CI testing methods often assume that discretized observations can directly substitute for continuous variables, leading to erroneous conclusions. To overcome this limitation, the authors introduce a novel CI test tailored for discretized data. The key innovation lies in using a bridge equation and nodewise regression to estimate the precision coefficients that reflect CI relationships among latent continuous variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper tackles a highly relevant and important problem within the realm of statistical analysis and CI testing.
The paper is well-written and presents the concepts clearly.
The proposed method is novel, and the theoretical contributions are solid, providing a robust foundation for CI testing in discretized data settings.

Weaknesses:
Assumption of Multivariate Normality: A primary limitation is the assumption that the data follow a multivariate normal distribution. This assumption simplifies the derivation of bridge equations for unconditional independence testing and the use of nodewise regression for the CI test. However, it restricts the applicability of the method to this specific class of variables. It is unclear how the method would perform with unknown or non-normal variables.

Discretization Modeling: The paper models discretization as a binarization operation applied to observed variables. This assumption may not hold in all practical scenarios. The performance of the proposed method on datasets with different types of discretization (beyond binarization) remains unexamined and is an important consideration for real-world applications.

Empirical Results: According to the empirical results, the proposed Discretized CI Test (DCT) shows smaller power compared to baseline methods. This indicates that while the method is innovative, its practical effectiveness in terms of power may be limited in some scenarios.

Limitations:
See above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
2Bef9YxSJc;"REVIEW 
Summary:
The paper studies an important and open question, how much user behavior knowledge (generally captured by collaborative filtering models) are present in large language models. This has been a topic attracting significant research interest in recent years. The authors propose that simple linear mappings done on top of LM encoder representations are sufficient to capture collaborative filtering signals in recommendations, and propose a new recommendation method, AlphaRec, which takes pretrained language model content embeddings as input, transforms them via MLPs and lightweight graph convolutions, followed by a contrastive loss. The authors conduct experimental analysis for AlphaRec in both standard settings and zero-shot settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**S1**: the topic studied is important. It is generally believed that language model and collaborative filtering (recommendations) learn different representation spaces, and methods to bring the two spaces closer is of significant interest to the large community working on search, recommendations, ads, and related topics.

**S2**: the particular approach proposed (linear mapping from textual space to collaborative filtering space) is understudied in prior work on LLM and (Generative) CF, despite numerous papers in recent years.

Weaknesses:
**W1**: the writing in this paper, esp. recommendation system paradigm related discussions, misrepresents (or ignores) significant prior work done in the field. e.g., ""AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm."" and related writings. 

- Content signals and/or embeddings have been used as the dominant recommendation paradigm in the field, even well before the seminal YouTube DNN paper [1] was published (see e.g., Pazzani and Billsus, 2007 [12]). For recent examples of related work, see e.g., [10, 11] from Pinterest and Meta in KDD'22 (but one should be able to easily find similar papers in WWW KDD etc in prior years as well). 
- Replacing ResNet/ViT- or GPT-/BERT- generated embedding with LLaMa- or Mixtral- generated embeddings cannot and should not be viewed as a paradigm shift, especially given the core architecture of AlphaRec is not substationally different from prior work.

**W2**: AlphaRec needs to be compared with stronger baselines. This applies to many major experiments in the paper. Here are some examples of baselines missing, which may significantly change conclusions obtained and discussions etc:

- vs ID-based recommenders (Table 3).
    - Equation (2) and line 171-173 for $N_u$ already captures set of items that a user is related to (""user interaction history"" / ""user engagement history"") to a large extent. Thus, the authors should compare AlphaRec with at least some SotA sequential/generative recommenders, such as SASRec, BERT4Rec, TIGER, HSTU [3, 4, 6, 7]. All of them are missing in the current paper.
    - Given AlphaRec uses the transposed item id representation - the one layer $N_i$ formulation (equation (2)), relevant work in recent years include Dual contrastive network [8] and User-centric ranking [9]. The authors should compare with or at least discuss some work in this category as related work. 
- Zero-shot performance. (Table 4)
    - ""Book Crossing"" is not a commonly used benchmark dataset. The ""Industrial"" dataset (per citation [1] on line 273) seems to be a small-scale ""Yelp"" dataset, and should be renamed to avoid confusions.
    - For ML-1M, the SotA approach one year ago (LLMRank [14]) already achieved 53.73 NDCG@20, significantly higher compared with 32.15  (AlphaRec) in this work. 

**W3**: many other formulations/experiments/writings could be significantly improved. Examples include:

- The proposed task formulation does not reflect how recommendation systems work in practice. e.g., ""Line 97-99. Personalized item recommendation with implicit feedback aims to select items i ∈ I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1 (yui = 0) indicates user u ∈ U has (has not) interacted with item i [58]."" -- here ""selecting the item that user will interact with"" is not the same as ""selecting the item with the highest reward"", as the interaction itself can be negative (e.g., disliking a recommendation, abandoning session, etc.). See [1, 2] for references.

- A key contribution of this work should be the linear mapping finding. But Table 1 uses a questionable set of baselines for both LMs and CF baselines, which weakens linear mapping related claims.
    - To claim ""Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models"" -- I would expect the authors to compare with a single set of models (e.g., LLaMa-2 7B 13B 70B or GPT-3 1.3b 2.7b 6.7b 13b 175b) trained on identical data. As it stands, all models are trained and/or finetuned with different data, so a simpler hypothesis explaining the LM (Linear Mapping) trend is that people are including more and more data into LLM pretraining/finetuning stages, which happen to capture more and more aspects relevant to recommendations. 
    - On the CF side, ""MF"" ""MultiVAE"" and ""LightGCN"" do not represent SotA baselines on Amazon Review datasets (see W2). 

- Table 1. Please highlight the particular K used for Recall and HR metrics (hard to find in the paper, applies to other tables too).   Most work on recommendation models also report HR/NDCG/etc. over at least 2-3 Ks to help readers understand how metrics vary with different approaches.


- Table 4. [1] should not be labeled as an ""Industrial"" dataset. The cited paper (per line 273) is Ni et al. ""Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"" which in turn seems to be an publicly available review dataset provided by Yelp. Please use appropriate language as the current writing leads readers to think that AlphaRec is an industrially deployed system. Please refer to industrial papers (e.g., KDD ADS track papers [2, 9, 11, 12]) for how to describe testings done on publicly available industrial sampled datasets (like Yelp), vs real deployments. 

- Misc: Contrastive loss is widely used and should not be viewed as a contribution of AlphaRec. See [15, 11] etc.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper states that LLM encodes collaborative signals that make it easy to connect language representation space with an effective recommendation space. Thus, it proposes an effective collaborative filtering model AlphaRec that takes as input only the transformed LLM representations of textual descriptions of items and is trained by InfoNCE loss and graph neural networks. The proposed method outperforms traditional ID-based models and other LM-enhanced methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written and easy to follow. 
2. The paper conducts extensive experiments validating the effectiveness of the methods and proves the validity of the design through ablation study and anlysis.
3. The proposed method exhibits significantly good zero-shot recommendation performance

Weaknesses:
1. One of the most important motivations of the work is that the paper declares large language models encode collaborative signals which indicates the advantage of using representations of large language models for recommendations compared to id embeddings. However, how the preliminary experiments prove this point is insufficiently discussed in the paper. Advanced large language encodes more semantics of the textual descriptions and thus yields better performance. Why this alone doesn't fully explain the performance gain of LLMs should be more explicitly discussed in the main paper.
2. The novelty is limited. Using semantic embeddings of items has been widely used in recommendations. The novelty mostly lies in using the representations of large language models and the implementation details of how to make it effective when combined with traditional recommendation frameworks like non-linear transformations.
3. The paper states that language representation-based methods have low training costs. Still, if taking into account the costs of generating language representations, the computational cost is much higher than ID-based methods.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes AlphaRec, a novel method to incorporate both knowledge from pre-trained language models and collaborative signals. Authors firstly reveal the advantages brought from pre-trained embedding model, and then propose three modules within AlphaRec. An MLP layer to transform pre-trained embedding to item-representation. A graph convolution to aggregate neighbor’s information, and the InfoNCE loss to train introduced parameters within the MLP for each dataset. Overall, the novelty of this paper lies within exploration of NLP encoded embedding on RecSys. The graph convolution and InfoNCE loss are already widely used techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A good exploration on new direction (language-representation-based) RecSys
2. Experiments are conducted from different angles for analyzing their model.

Weaknesses:
1. Insufficient baselines.
2. Uncleared model name definition.

Limitations:
See Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AlphaRec, an LLM-based recommender system that utilizes language representations of item textual data for recommendations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Investigating ID paradigm and LLM paradigm is important.
+ The method is simple but seems to be effective.

Weaknesses:
- In this paper, what most confuses me is the usage of the terminology ""collaborative filtering"" throughout the paper. In traditional recommender system, collaborative filtering information means the interactions among users and items. The authors find that using LM as feature extractors to get user/item embeddings from meta-data can achieve similar results as if CF is used for recommendation. However, this seems to be fundamentally different than LM has the ""collaborative information"", as for most online service platforms, the interaction data should be confident and open source LMs won't be able to train on that data. Therefore, the main claim in the paper seems questionable.

- It would be beneficial if we could have results on more diverse datasets.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3sWghzJvGd;"REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
xM5m7J6Lbl;"REVIEW 
Summary:
This paper defines social markov decision processes (SMDPs) as an MDP generalization incorporating a population of individuals with distinct utility profiles aggregated by a social welfare function. It provides a novel quantitative definition of alignment in this context, then leverages this definition to characterize probably approximately aligned policies and safe policies, prove the conditions under which they exist, and relate them to the accuracy of the reward model.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well written, and the background is particularly clear.
2. The definitions and theoretical results are thorough and rigorous. This paper precisely relates the probability of aligned behavior to the world model accuracy, which I believe is valuable.
3. This paper acknowledges that realistic inaccuracy in the world model could cause intolerable uncertainty in the PAA policy, and shows a more practical approach (safeguarding a black-box policy).

Weaknesses:
Even the more practical approach of safeguarding a black-box policy may have severe limitations. I believe the paper would be strengthened by a discussion of the feasibility of this -- in particular, what is computational complexity of computing $\mathcal{A}_{safe}$ for a SMDP?

Typo: On line 277, I believe ""expansive"" should be ""expensive"".

Limitations:
This paper includes an excellent discussion of the limitations of these results, including the theoretical conditions under which PAA and safe policies will be unreliable. The paper also discusses further practical and philosophical limitations in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper applies ideas from the Probably Approximately Correct framework to agent alignment. The paper defines a new idea of a policy which is Probably Approximately Aligned and explores the existence of such policies under certain assumptions of social welfare and models of the world. The authors show that probably approximately aligned (and approximately aligned) policies exist when there is a sufficiently accurate world model. However, to compute this policy is quite expensive. Thus, the authors also develop the idea of a safe policy which can be derived using a PAA policy and seems to be a policy that will probably not result in a catastrophically bad state.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper appears to be a very reasonable application of a well established form of analysis into a novel domain.

The main idea of providing bounds for the quality of an agent's policy is very important and will likely be the focus of much work in the near future. This is quite useful work and appears to me as the potential basis for work that can eventually have significant beneficial impact on the world.

The paper is generally well written and the motivation is clear. In places the math is a little dense but it seems to be as approachable as it can be for this sort of analysis. I do certainly appreciate that you've put a moderate amount of the work into the actual paper rather than stuffing all the important stuff into the appendix.

Weaknesses:
Not a weakness, but my disclaimer: I was not able to thoroughly review every detail of the math due to time constraints so my understanding of the paper is limited.

The primary (and minor) issue I see with the paper is that it is quite abstract and doesn't give a clear idea of how close this is to being useful. While obviously difficult to fit into a conference paper, an experimental section may give some intuition for details such as how accurate a world model really needs to be, how beneficial PAA/safe policies are, etc.

It seems that Sec 3.2 is constructive in a sense and provides a PAA policy. Some further commentary on the practicality of this policy (is it entirely impractical to use it for synthetic experiments, or simply impractical in any useful setting/world model?) would help to contextualize the paper.

Limitations:
Limitations are well stated.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to define alignment quantitatively and ensure AI agents' actions are predictable and safe. The paper start by outlines the basics of utility and social choice theory, focusing on quantifying social satisfaction and the conditions under which it is measurable. Next, the paper defines probably approximately aligned (PAA) and approximately aligned (AA) policies and provides a modified sparse sampling algorithm to achieve these policies under certain conditions. The paper also presents the idea of ""safe policies"" and a method to ensure AI actions are verifiably safe for society.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Originality: This paper introduces a novel, quantitative definition of alignment in social decision-making contexts, drawing from utility and social choice theory.

- Quality: The paper primarily focuses on theoretical contributions rather than empirical experiments. It is well-structured.

- Clarity: The paper provides detailed mathematical derivations and proofs to support the existence of PAA and safe policies. It includes extensive references and context, including foundational works in utility theory, social choice, AI safety, and reinforcement learning, emphasizing the interdisciplinary nature of aligning AI with human values.

- Significance: This work has a significant impact. While primarily theoretical, the work aims to provide a foundation for developing AI systems that could be safely used in critical applications like social governance, policy-making, or resource allocation.

Weaknesses:
The safeguarding method is described in a general context, with limited discussion of its applicability to specific real-world problems. Consider adding examples of real-world applications where the safeguarding method could be particularly beneficial. For instance, discuss its application in autonomous vehicle systems, healthcare decision-making, or financial trading algorithms.

Limitations:
The authors discuss various limitations of their approach, including computational complexity for large state spaces and strong assumptions about the availability and accuracy of information. The paper also highlights challenges in building reliable world models and the philosophical questions surrounding the informational basis of utilities.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the potential for AI agents to safely make critical decisions, such as those in a government setting, by examining the concept of alignment. It introduces Probably Approximately Aligned (PAA) policies, which are policies that are nearly optimal in aligning with social welfare objectives. The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society. They also discuss the practical challenges in implementing such policies and suggest future directions for research in this area. The focus is on developing a theoretical framework that could eventually be applied to AI governance and decision-making processes.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors draw from utility and social choice theories to provide a quantitative definition of alignment and propose methods to ensure AI actions are verifiably safe for society.

Weaknesses:
I think the problem is not well presented.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
VR2RdSxtzs;"REVIEW 
Summary:
This paper proposes a universal prompting method for solving complex reasoning problems such as mathematical problems and the 24-point game. The method first abstracts the conditions and objectives of the problems and then progressively discovers new conditions until enough information is gathered to solve the problem. Experiments on MATH, the 24-point game, SciBench, and TheoremQA indicate that the proposed method is effective and universal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-structured and easy to follow.

- The experimental results are solid, showing significant improvements.

- The proposed method outperforms other prompting methods with a comparable number of responses.

Weaknesses:
- In Table 1 and Table 2, the number of responses for each problem should be highlighted to indicate whether the comparison is fair.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a prompting method called Multi-Agent System for conditional Mining (MACM). MACM involves three agents, Thinker, Judge, and Executor, who maintain a condition list and try to solve the problems when the conditions are sufficient.

The paper conducts experiments with GPT and Llama series on MATH, 24-points game and sequece sorting. The results show MACM’s superiority to prompting methods like ToT and GoT under these settings.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper does show significant improvement of models’ performance with MACM unber specific settings.

Weaknesses:
- MACM seems to lack novelty. MACM’s idea of maintaining a more free-form context for planning has been explored by previous works like Cumulative Reasoning.
- The experimental setups in the paper seem confused and lack rigor, making the results less convincing. Examples include but are not limited to:
  - In section 4.2, It is unreasonable to state that ToT and GoT are incompatible with MATH and quit comparing with them. Algorithms like ToT have been applied to open-form QA tasks by previous works like LLM reasoners.
  - Across most experiments, it is confusing to distinguish between 0-shot/IO and CoT for GPT-4 on reasoning tasks, for GPT-4 seems to always use CoT on reasoning tasks if not specially prompted.
  - Especially, experimental setups described in Appendix B seem rather unreasonable:
    - The paper states that it always uses “$Top_{k}=1$, and the temperature $t=0$”, i.e. greedy decoding, but this seems incompatible with SC-CoT (though still possible with minor randomness existing).
    - The meaning of “length of the chain $l$” in CoT seems not explained across the paper.
    - The `max_tokens` seems too small for complex tasks like MATH.
- The writing lacks clarity and is hard to follow:
  - The paper tends to list many settings that are different in multiple dimensions and their results in a line (e.g. Figure4,7, Table2,3), making attribution of the results difficult.
  - The paper spends much space on specific examples (\~2 pages across pages 4-6) but seems to show few special things.

Hopefully, the authors could put more work into experimental setup and presentation to improve the quality of the work.

# **After rebuttal and discussion**

The discussion period brings up 

- 2 new weaknesses:
    1. According to Figure 4, MACM's gains on open-source models are not clear and may be marginal. Could you compare MACM with 0-shot and Majority voting, at least on LLaMA3-Instruct 8B?
    2. To test the impact of different models on MACM as **a new method**, it is not sufficient for GPT-3.5 to only test with MACM -- comparisons with baselines are needed. However, this could also be achieved with open-source models, which should be much cheaper and faster.
- 1 new question: The exact versions of GPTs in the paper are unclear. You should at least annotate the exact versions and better compare the methods with the exact same version.

However, most concerns have been resolved by the authors in their replies.

Considering the improvement in the rebuttal and discussion, I believe that the submission has a contribution for **proposing a new effective and general prompting method, which is limited in being only applicable to strong models**.

However, the submitted manuscript is indeed not good enough because it **lacks many details and is not well-formed enough, actually bringing an unnecessary burden for the community to follow the work**, as shown in the long discussion. I tend to take it as not ready for publication in its current form. Hopefully, the authors will add all the necessary content to future versions.

In summary, I would like to keep my final rating as 4 as a reminder of the presentation problem and leave it to AC to decide whether this submission should be accepted.

Limitations:
See above in weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel prompting technique MACM, which utilises multiple agents to cooperate and perform backtracking for mathematical reasoning problems.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The prompting method seems to work well for mathematical reasoning tasks and shows a degree of generalisation.

Weaknesses:
My main concern is with the evaluation method. As Appendix B suggest that the authors used GPT4 Turbo as a judge and only tested on a rather randomly selected set of MATH. This is very worrying, because (a) MATH has its own evaluation protocol, and the Minerva paper [1] also gave a good evaluation protocol. The reliance on GPT4 Turbo as a judge seems unjustifiable. (b) Why randomly select a subset, instead of using the whole MATH test set?


[1] Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., ... & Misra, V. (2022). Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35, 3843-3857.

Limitations:
The paper did not discuss much about the approach's limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to address the limitations of existing methods that require distinct prompt designs for different mathematical problems. The authors propose a general Multi-Agent System for conditional Mining (MACM) method that uses three LLM agents (Thinker, Judge, Executor) to iteratively propose the conditions for a problem, verify whether the existing conditions can reach the objective, and execute calculations. Experimental results clearly show the accuracy improvements of MACM.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The main strengths include:

1.	The motivation to design a method that does not require prompt design for different mathematical problems is important and makes sense.
2.	The authors clearly explain the details of MACM, which makes it easy to follow.
3.	The authors conduct experiments on various datasets, and the accuracy improvements are significant.

Weaknesses:
The main weaknesses include:

1.	The essential technical contributions of this paper may be limited, and an explanation is needed as to why the method is effective. (please refer to question 1 below).
2.	The theoretical analysis is a little weird and needs further explanations. (please refer to question 2 below).
3.	More experimental results are needed to demonstrate the effectiveness of the proposed method. (please refer to questions 3-4 below).

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
AM1znjeo9l;"REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fs28jccJj5;"REVIEW 
Summary:
The paper introduces a novel method for converting Swin Transformer and BERT into SNN without additional training, achieving high accuracy and energy reduction. Key innovations include fully spike-based encoding, trace-driven matrix multiplication, and an exponent-free spike-based softmax using winner-oriented spike shift. The method maintains the original attention architecture, achieving state-of-the-art accuracy on ImageNet with a 42% energy reduction and only 0.3% accuracy loss on GLUE benchmarks while reducing energy consumption by 58%.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SpikedAttention allows for direct transformer-to-SNN conversion without requiring additional training, preserving the original attention architecture and simplifying the deployment process of energy-efficient neural networks.

2. SpikedAttention significantly reduces energy consumption by 42% compared to baseline models like Swin Transformer for image classification and by 58% for NLP tasks on the GLUE benchmark.

3. The method achieves state-of-the-art accuracy for SNNs on the ImageNet dataset (80.0%) and maintains high performance with minimal accuracy loss (0.3% on average) when converting BERT models.

Weaknesses:
1. SpikedAttention requires a longer timestep to maintain high accuracy compared to directly trained SNNs, which could limit its efficiency and responsiveness in real-time applications.

2. The method does not currently support functions like GeLU and LayerNorm, making it difficult to generalize to all types of language models and potentially limiting its applicability across diverse neural network architectures.

Limitations:
1. SpikedAttention requires a longer timestep to maintain high accuracy compared to directly trained SNNs, which could limit its efficiency and responsiveness in real-time applications.

2. The method does not currently support functions like GeLU and LayerNorm, making it difficult to generalize to all types of language models and potentially limiting its applicability across diverse neural network architectures.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents SpikedAttention, a novel method for converting pre-trained transformers into spiking neural networks (SNNs). The method introduces two key techniques: trace-driven matrix multiplication and winner-oriented spike shift (WOSS) for softmax. These techniques enable the conversion of attention modules into spike-based computations without altering the original transformer architecture or requiring any additional training. The authors demonstrate the effectiveness of SpikedAttention by converting Swin Transformer for image classification and BERT for natural language processing tasks, achieving state-of-the-art accuracy and significant energy reduction compared to previous SNN-based transformers and the original ANN models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel techniques. The paper introduces innovative methods for implementing attention mechanisms in SNNs, addressing the challenges of softmax computation and matrix multiplication between spike-based matrices.

2. State-of-the-art accuracy and energy efficiency. The proposed SpikedAttention achieves impressive results on both vision and language tasks, demonstrating superior accuracy and energy savings compared to existing SNN-based transformers.

3. Direct conversion without training. The method enables the direct conversion of pre-trained transformers into SNNs without requiring any additional training or architectural modifications, making it a practical and efficient approach.

4. Applicability to both vision and language tasks. The authors showcase the versatility of SpikedAttention by successfully converting both Swin Transformer and BERT models, highlighting its potential for broader applications in various domains.

Weaknesses:
1. Longer timestep compared to directly trained SNNs. The paper acknowledges that SpikedAttention requires a longer timestep than directly trained SNNs to maintain high accuracy, which could impact its latency and efficiency in certain scenarios. Could the authors draw a figure to show the accuracy and energy with different timesteps?

2. Limited support for certain functions. The current implementation of SpikedAttention does not support GeLU and LayerNorm, limiting its applicability to most models.

3. There is no ablation study in this work. What if trace-driven matrix multiplication is absent? What if WOSS softmax is missing?

4. Limited discussion on hardware implementation: While the paper mentions the potential for hardware implementation, a more in-depth analysis of the hardware implications and optimizations would be valuable.

Limitations:
The paper discusses limitations in the final section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a transformer-to-SNN conversion method without modifying its attention architecture. To minimize the energy consumption, the authors apply one-spike phase coding, Trace-driven matrix multiplication, and winner-oriented spike shift for softmax. They evaluate their conversion method on vision and NLP tasks including ImageNet classification and GLUE Benchmark.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Solid paper structure and intuitive figures. Easy to follow the paper.
- The proposed methods are well-structured for energy efficiency.
- Comparable performance with low energy consumption
- As well as theoretical energy consumption, the authors provide hardware-realistic energy including data movement and the update of membrane potential.
.

Weaknesses:
- In the vision task, the authors evaluate their method on only the ImageNet dataset. It would be helpful if there are other type of datasets such as neuromorphic datasets.
- In Table 1, the meaning of without ReLU is ambiguous. Please elaborate the details.
- In figure 2(b), weights should be $W_Q$, $W_K$, $W_V$.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
5ClpGA0u9K;"REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Oo7HY9kmK6;"REVIEW 
Summary:
Mean-Field Langevin Dynamics (MFLD) framework is used to solve optimization problem over manifold. The main contribution appears to be reducing a general optimization problem over signed measures to probability measures using lifting or bilevel approaches. Convergence rate of MFLD, when applied to both approaches, are investigated.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
It sounds interesting to use lifting or bilevel ideas to reduce MFLD to solve general optimization problems over signed measures.

Weaknesses:
The scope of lifting or bilevel ideas should be made clear. For instance, lifting idea seems to be applicable when the signed measure can be represented as projections of probability measures; does the projection representation always exist?
Also, reduction of the optimization problem over probability measure may change to a more difficult optimization problem, e.g., the objective functions (3.1) and (3.3) which might be more difficult to deal with. Some discussions on how to deal with these functions empirically will be helpful.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies extension of mean-field Langevin dynamics (MFLD) to perform convex optimization over the space of signed measures. The paper considers the lifting and bilevel approaches and shows that the latter guarantees better convergence properties at a wider range of hyperparameters. MFLD-bilevel is shown to be amenable to a previously studied improved temperature annealing schedule over the standard $1/\log t$ rate. Moreover when learning a single index model, MFLD-bilevel is shown to achieve a local convergence rate which depends polynomially on the dimension and inverse temperature via an improved LSI constant.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper rigorously compares the lifting and bilevel approaches, and the result that the latter leads to stronger guarantees (at least in the low temperature regime) is quite surprising to me as many influential works on shallow NNs have built on the lifted formulation.
* The paper is overall well written and detailed, and the analysis utilizes novel techniques (namely local LSI bounds) to go beyond the standard results in MFLD.
* While the paper has some similarities with [Takakura, Suzuki, 2024], the setting is more general with a focus on more abstract optimization as explained in Appendix A.

Weaknesses:
* The parameter space $\mathcal{W}$ is assumed to be a compact manifold. Does this mean compact without boundary, which precludes subspaces of Euclidean space? Can this assumption be removed by e.g. adding a confinement term and considering relative entropy regularization with a log-concave distribution? While the non-necessity of these elements are presented as advantages in Section 1.1, studying $\mathcal{W} =\mathbb{R}^d$ is simply necessary for certain optimization problems and it would be very nice to fill out the details.

Limitations:
Discussed in corresponding sections.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the well-known and recently extensively studied mean-field Langevin dynamics (MFLD) to optimization problems over signed measures (instead of probability measures). This has applications and relevance to the training of NNs or other problems in data science, such as sparse deconvolution, which are intrinsically of such form.
As an example, the learning of one neuron is presented at the end of the paper.

In order to fit the setting of signed measures into the classical probabilistic framework of the MFLD, the Authors leverage and explore two classical strategies. Namely, the lifting and bilevel reduction.
They observe that the bilevel reduction behaves more favorably compared to the lifting reduction, allowing to obtain stronger and faster convergence.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Extending the MFLD via a bilevel approach to optimization problems over signed measures seems to be an interesting and relevant direction given that this captures several problems of interest in machine learning and data science.
- After discussing two potential directions to approach the problem (lifting and bilevel), the paper adapts and employs the convergence analysis of [Chi22b] to the new setting of the bilevel MFLD, which transpired to be the way to go due to the assumptions holding in more realistic scenarios.
- Despite the paper being of purely theoretical nature and in parts quite technical, it is overall well-written and good to follow. The organization and structure of a general introduction, revisiting the classical MFLD as well as the two reduction strategies in Section 2 and 3, respectively, helps in this regard.
- Overall, the technical contributions seem to be rigorous and convincing.

Weaknesses:
The experimental exploration of the proposed approach is a bit limited. The experiment in Figure 1 seems very academic.
I was wondering, if the Authors could comment on whether there are experiments already conducted in the literature that could prove the practicability of the approach. A remark on that instead of the experiments in Figure 1, which could be presented in the Appendix, would be appreciated.
Yet, as the paper's contribution is predominantly of theoretical nature, I _do not_ see the limited experimental investigation as a reason for a lower score.

Limitations:
I do not see any unadressed limitation.

Moreover, I like how the lifting approach is discussed, despite turning out not to be the way to go.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Mean-field Langevin dynamics (MFLD) has been developed for optimizing convex functionals over the space of probability measures. This work extends MFLD to convex problems defined over the space of signed measures. The authors consider two approaches: lifting and bilevel approaches, and prove the superiority of the bilevel approach. The lifting approach cannot satisfy the two required conditions for MFLD convergence under weak regularization, whereas the bilevel approach satisfies both conditions simultaneously. Additionally, an improved convergence rate is given for the enhanced annealing schedule of the temperature.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This work successfully extends MFLD to convex optimization problems over the space of signed measures. Although recent work [TS24] also considered a similar approach, their method is a special case of the MFLD-bilevel proposed in the paper.
- This work proves an improved convergence rate for MFLD-bilevel with annealing temperatures. Although the rate still exponentially depends on the regularization strength, it may be difficult to avoid such dependence.

Weaknesses:
- Since this work considers (compact) Riemannian manifolds as the particle space, time discretization is essentially challenging. I’m curious about how to discretize the dynamics in time and guarantee the convergence rate.
- I guess the lifting approach satisfies both (P1) and (P2) once we limit the range of $r$ to be bounded, like $r∈[−R,R]$. If this is true, it is worth considering whether the bilevel approach is truly superior to the lifting approach.
- As the authors commented in the paper, [TS24] studies a quite similar method. I would like to see any additional technical challenges or difficulties compared to [TS24] if possible.

Limitations:
Limitations are addressed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
476zUsqFZB;"REVIEW 
Summary:
This paper attempts to address the problem of low interpretability of reaction prediction methods by proposing modeling step-wise polar reactions. To model such mechanisms it uses an existing dataset PMechDB. The authors propose an approach to model such reaction by first selecting the right atoms to react from the input molecules using learned models and then react them.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Several existing chemistry reaction prediction models are benchmarked on the PMechDB dataset.
 - A way to integrate reaction mechanism information is introduced.

Weaknesses:
- The paper has substantial clarity problems: 
  - Table captions are insufficiently informative, requires going deeper into the text to understand what results are actually presented (e.g. 'Table 3: Top-N Accuracy of Trained Models').
  - Figures 5 and 6 are formatted inconsistently with the rest of the file.
- Citation quality is poor:
  - Could provide more references to prior work overall. e.g. section 3.3 describes prior work on sequence to sequence modelling without any references. 
  - PMechDB is introduced in a way that makes it unclear, whether the database is a contribution of this work or not.
- Novelty is not prominent. Method in [18] (OrbChain) is already working with similar task on a similar dataset.
- Evaluation is insufficient:
  - Source code for reproduction has not been provided.
  - The resulting models have not been evaluated on the global datasets, making it unclear whether the fine tuning as specified in this work improves the performance in general rather than on the test set of PMechDB.
  - Error bars are not provided.
  - Not benchmarked against a comparable method, referenced in [18].

Limitations:
The authors address some of the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Current reaction prediction models lack interpretability for chemical reaction prediction. This paper evaluates the various machine learning models on the PMechDB dataset which contains polar elementary steps. Besides, this paper proposes a new system: PMechRP, which achieves the highest top-5 accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:  
1. A new benchmark has been introduced, which improves the interpretability and causality of a chemical reaction.

2. Several methods are evaluated.

Weaknesses:
Weaknesses:

1. This paper seems like a technique report.

2. The main conference track is not suitable for this paper. I think the dataset & benchmark track is more suitable.

3. Writing is poor.

Limitations:
N/A

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Previous reaction prediction models formulate the forward chemical reactions in an end-to-end manner, which only considers the input state and output state while ignoring intermediate states describing the electron redistribution changes. This work tries different models on a new benchmark dataset PMechDB. Experimental results demonstrate the effectiveness of the transition state information in the new benchmark dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The motivation is quite clear. This reviewer agrees with the importance of the exploration of intermediate electron transfer. This is particularly important for the chemical reaction simulation, benefitting the understanding of reaction mechanisms.

Weaknesses:
(1) The technical contribution of this work is very limited. This reviewer does not see enough improvements from the algorithm side. Also, it seems the dataset is not proposed by this work. The contribution of this work is overall limited.

(2) If this work intends to propose a new benchmark, then much more comprehensive reaction models should be covered. Currently, two important reaction models are not discussed: ""non-autoregressive electron redistribution modeling for reaction predictions"" and ""A Generative Model For Electron Paths."" In addition, the evaluation metric and the new task are not clearly described. More detailed descriptions should be provided for clarity.

(3) The presentation of this work is not very clear. This reviewer does not fully understand how the multi-step information helps the reaction modeling. A good example illustrating the significance of the intermediate step information is required. At this stage, this reviewer thinks the multi-step transition information can be easily captured by recursive modeling of single-step reaction models. Currently, this reviewer does not see what new challenges are brought by the intermediate step.

(4) This work is very similar to the published paper ""AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning."" This reviewer does not see many differences between the submitted work and this prior work.

Limitations:
Constructing the benchmark dataset with ground-truth multi-step electron transition states is very hard. This may hinder the further development of this direction.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
the paper describes a new approach to predict polar reaction mechanisms, which is the most important class of chemical reaction mechanisms. this can be quite useful for chemical reaction prediction.


this reviewers rating is based on the current presentation of the manuscript, if the authors are willing to enhance the clarity of the manuscript, this reviewer is willing to increase their score.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Addressing an underexplored but important problem
- decent results
- interesting results with pre-trained methods, that in some cases are surprising (T5 seem to work not as well despite multi-task pretraining)

Weaknesses:
- Model and data processing descriptions are quite short and should be expanded, and presented coherently in one location in the manuscript. From the description in the manuscript I would likely not be able to re-implement the method
- It is not immediately clear which ensemble is shown in table 4
- maybe not so much innovation from the ML side?

Limitations:
ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
oEmyoy5H5P;"REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cdYIL6OQr6;"REVIEW 
Summary:
This paper introduces a novel mixture of experts model that applies local differential privacy to the gating mechanism. Their methods leverages the one-out-of-n gating mechanism and provides specific generalization bounds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall insight of the paper is clear and strong.
2. Improve the tightness of bounds on the risk for mixtures of experts models.
3. Reduce complexity by relying on fewer parameters.

Weaknesses:
1. The section 3 talks about PAC-Bayesian bounds for mixtures of experts, but it lacks insight into why PAC-Bayesian bounds are applied instead of other bounds. More explanation is needed here, similar to the explanation needed in section 4 regarding Rademacher bounds.

2. In the experiment section, it is unclear why the chosen dataset is used for the experiments and why only 5 epsilon values were selected.

3. Even though there are very few existing guarantees, the experiment should include other methods as baselines and compare the results.
4. For the experiment section, only consider mixtures of n linear experts in binary classification tasks seems easy. Need to add other classification tasks.
5. There is no description about the datasets used in experiments.

Limitations:
1. Lack many reference as I motioned in questions part. 
2. The paper lacks a smooth flow, making it difficult to follow. Specifically, there is no clear insight or reasoning provided to explain why the existing mechanisms or bounds were chosen, as highlighted in weakness 1.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to regularize mixtures of experts by imposing local differential privacy (LDP) on the gating mechanism. The authors provide theoretical justifications and derive PAC-Bayesian and Rademacher bounds tailored to this approach. Experiments conducted on various datasets demonstrate that using LDP as a regularizer improves the generalization ability of the models, especially in cases prone to overfitting. The method offers a balance between leveraging neural networks for gating and maintaining robust theoretical guarantees, making it a valuable contribution to the field of machine learning.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper demonstrates originality by integrating local differential privacy (LDP) into the mixture of experts model, addressing privacy concerns while improving model generalization. The theoretical contributions, including PAC-Bayesian and Rademacher bounds, are rigorously derived and tailored to the new approach. The clarity of exposition makes complex concepts accessible, and the experiments validate the practical benefits of the method. The significance lies in enhancing the robustness and scalability of mixture of experts models, making them more applicable to real-world scenarios prone to overfitting.

Weaknesses:
The primary concern regarding this paper lies in its significance. There have been previous works that incorporated differential privacy (DP) into the construction of mixture of experts models with privacy considerations. This paper, however, utilizes local differential privacy (LDP) to analyze the theoretical aspects of mixture of experts models. The introduction of LDP significantly alters the generalization behavior of these models because both LDP and DP are methods that inherently enhance algorithm robustness, thereby affecting generalization. If the main goal of the paper is to enhance privacy, it is imperative to compare this approach with existing DP-based methods and highlight what specific aspects LDP protects that traditional DP cannot. Without this comparison, the added value of using LDP over existing DP methods remains unclear. On the other hand, if the focus is on analyzing the generalization of mixture of experts models, the paper must justify the rationale behind incorporating LDP for this analysis, as LDP is not inherently required for mixture of experts models. The paper needs to elaborate on why LDP is a suitable and necessary tool for this analysis and how it fundamentally impacts the generalization properties of the models in a meaningful way. Additionally, while the theoretical contributions are substantial, the practical implications need to be demonstrated more robustly through experiments. Comparing the results directly with models using traditional DP methods would strengthen the paper by showing the practical improvements and specific scenarios where LDP outperforms DP. Furthermore, using a broader range of datasets could better illustrate the claimed benefits in robustness and scalability. By addressing these concerns, the paper can more convincingly argue the necessity and advantages of using LDP in mixture of experts models, thereby enhancing its significance in the field.

Limitations:
No.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides generalization bounds for a particular type of mixture of experts (MoE) networks. They focus on MoE architectures where an input $x$ first goes through a gating function $g$, and then gets routed to a single (one out of n) expert $i \in [n]$ according to the $g(x) \in [0,1]^n$ distribution. The final output of the MoE network is the output of the expert $h_i(x)$.

The authors observe that when the gating function $g$ has certain regularization properties, which correspond to local differential privacy (LDP), then the resulting network has better generalization bounds than what appears in the existing MoE litterature. The authors provide such bounds. 

Finally, the authors evaluate LDP-regularized routing on binary classification tasks with mixtures of linear models. The results show that LDP regularization outperforms an un-regularized baseline.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Mixture of experts models are still understudied, especially from a theoretical standpoint, so I appreciate the new analysis provided by this paper. The connection with differential privacy is creative and seems fruitful, although I have some reservations about it (see below). 

The bounds do improve on existing generic bounds for this type of MoE models. The paper is well-written and easy to follow, and the experimental code is available.

Weaknesses:
First, it is worth emphasizing that the MoE networks in this paper do not satisfy local differential privacy themselves. The gating network is not even *trained* with differential privacy. This paper only uses local differential privacy as a regularization condition on the *gating network only* at *inference* time, which does not provide meaningful privacy guarantees. That is not immediately clear from the title of the paper. To be fair, this work does not try to achieve any privacy goals, and is entirely focused on generalization bounds. But if privacy is not needed, then it is not clear why DP is the right tool for the job. The paper directly uses LDP (it could have been called something like ""exponentially regularized routing"") but does not motivate this choice. Are there other forms of regularization that could achieve similar or better bounds? While there are some known connections between robustness, differential privacy, and generalization, they are not mentioned here. In the context of MoEs, some large models already regularize their gating functions (e.g., the Switch Transformer adds some ""jitter"" noise to the routing logits). 

Next, the paper motivates the study of MoE models by mentioning recent progress with LLMs such as the Switch Transformer, which uses multiple layers of experts (deep MoE) and combines the output of different experts. All the modern LLM MoE models I am aware of are such deep MoEs. Meanwhile, the paper focuses on simple shallow MoE models with a single gating network followed a single layer of experts, which limits the potential impact of the paper in my opinion. While theoretical bounds may be of interest even on shallow MoE models, I would appreciate at least some discussion about whether the authors' approach can generalize to deeper models. 

Finally, the experiments have some limitations, which mostly stem from the two previous concerns. 
* The authors only evaluate a single, rather simplistic (3-layer MLP gating network followed by linear experts), MoE architecture. More concerningly, they use a fixed number of experts (n = 100), thereby missing an opportunity to evaluate their claim that ""we can have many more experts with almost no penalty from the theoretical point of view"".
* The only baseline is ""No LDP"", which I think is a quite weak baseline. It is not entirely surprising that adding some regularization, in the form of LDP routing, improves generalization compared to a completely un-regularized baseline. How about other forms of regularization, such as dropout, clipping, or jitter noise (which already exists in the context of MoEs)?
* Another baseline would be a non-MoE model, with a comparable number of parameters, e.g., even a simple, dense, multi-layer perceptron. Showing that shallow MoEs outperform dense models would alleviate concerns about the practical relevance of this work.

Minor comments:
* Table 1 might be more readable as a graph.
* It is quite surprising to see MNIST being qualified as a ""large"" dataset, for which a 4-layer network takes 3 hours to train on a GPU, in 2024.
* Also, it is unclear why MNIST has to be broken down into 3 binary classification tasks.

Limitations:
The authors adequately addressed the limitation they identified (difficulty of tuning epsilon), even though this is not the main limitation of this work in my opinion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the mixtures of experts models, in particular the one-out-of-n gating mechanism for ease of theoretical analysis, and show that applying a soft-max, which is also the exponential mechanism, on the gating mechanism gives LDP and can improve generalization. The privacy techniques are largely the same as previous work, PATE, but specifically applied to mixtures of experts. The authors then provide theoretical analysis showing generalization bounds for this approach.


Unfortunately, I’m not familiar enough with the mixture of experts literature to evaluate the novelty of applying the soft-max and the corresponding theoretical guarantees. I am rather surprised though that the soft-max has never been applied and am still somewhat confused upon what the previous techniques were in the one-out-of-n mixtures of experts.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors show that choosing an expert through the soft-max provides better generalization. The privacy guarantee is then essentially proportional to the regularization factor (\beta) for the soft-max application. Further they give theoretical generalization bounds for this approach.

Weaknesses:
Unless I am mistaken (please correct me if I’m wrong) the authors are not providing privacy guarantees for the model itself, but only one inference call to the model. In particular, if feature vector x is input to the model, then the expert is chosen randomly according to the soft-max / exponential mechanism, which is \epsilon-LDP. If inference was then run again, suppose even on the same feature vector, then the random draw from experts would occur again. This is known as composition in the privacy literature, and the privacy guarantees	would now be 2*\epsilon. 

Providing privacy guarantees on only one inference call to a model is both not very useful nor interesting due to the composition properties.

Limitations:
See weaknesses section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors  take the first step (I though so at first) to MoE under LDP theoretically.  I read again and found that the author seems to have raised the utility lower bound of existing studies. Few experiments could be found. Perhaps, I am not an expert in MoE, but it really leave a  hard time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Important poblem.

Weaknesses:
1. Perhaps I am not an expert in MoE, and I cannot tell from the author's introduction that there are any challenges. 
2. The experimental results and application scenarios are not clear.

Limitations:
see weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
4vp0edVY4o;"REVIEW 
Summary:
This paper tackles continual learning by addressing interference between tasks. For the interference between different tasks, the authors propose a method called ‘global alignment’ to align the data representations using task-specific compositions of pre-trained token representations. Then the authors conduct extensive experiments to verify the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to understand.

2. The motivation is clear. From the perspective of cross-task interference, this paper provides some simple but effective methods to avoid the interference of the data representation and classifier.

3. The proposed methods are easy to follow and experiments verify their effectiveness.

Weaknesses:
1. The analysis of interference in Section 3.2 does not consider the activation function between the two layers of the network. Multiplying the two linear weight matrices is equivalent to using only one weight matrix, so the analysis of this linear case is different from that of the non-linear case and I’m curious about the inference in the non-linear case.

2. About wiring with neighbor attention:

    a) The author needs to explain the rationality of neighborhood tokens and the impact of the size of K on the model.

    b) This method requires matching the most similar K tokens for each token given a sample. This process may require a large amount of computation.

    c) The author needs to further explain the difference between this method and controlled-LoRA. Both these two methods add a learnable term to the original token representations but adopt different generation strategies. Specifically, this method adds a term composed of neighborhood tokens, while controlled-LoRA uses low-rank matrices. So what are the advantages of this method?

3. About Controlled-LoRA:

    a) Computational burden and parameter size: As the number of tasks increases, linearly increasing model parameters is not in line with the spirit of CL, and the increased parameter size should be limited even if LoRA only adds small parameters compared to the large model.

    b) Why does Controlled-LoRA perform worse than other methods? In the case of task-IL, given the task ID for each test sample, we can directly use the best LoRA model.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In Continual Learning (CL), the interference caused by the constant modification of the representation is the leading cause of catastrophic forgetting. Motivated by this and the idea that gradients in opposite directions are one cause of this interference, the authors proposed new ways of adding knowledge to a model. The authors motivate the proposal with a toy model that exemplifies where and why the interference occurs, concluding that it comes from the discrepancy between the representations of the different tasks and the relationship between the class vectors. These conclusions lead the authors to propose three proposals that slightly modify the transformer architecture, specifically adding learnable weights in the self-attention layers to generate a task-specific attention representation. The authors also propose using the probing and then fine-tuning approach presented in the past to help initialize class vectors. The results are presented in task and class incremental learning benchmarks in sequences of text datasets.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The authors present a motivation from which they exemplify the problem in a simple way and where they want to aim their solution. This motivation may help the reader understand the context and problem to be solved.
- The paper introduces a method that alters the architecture of a transformer model. While the paper does not explicitly state this, the provided motivation helps to understand why certain parts of the self-attention layer (the k matrix) are modified.
    - The authors must describe why only the K matrix is modified and not the others.

Weaknesses:
- What is the actual contribution of applying wiring or C-LoRA? The results show that applying FP can benefit the methods much more than adding the proposals presented.
    - What are the implications of applying FP to the Adaptation or CL methods? The performance gain from the proposed methods is relatively low (compared to L2P), and this could further decrease with the application of FP. This requires a deeper exploration of the proposed methods and the alternatives.
- The writing is unclear and often unnecessarily complex, making reading difficult for someone unfamiliar with the subject. The notation used is only sometimes in line with the literature, which can confuse readers. In addition, there are problems explaining some terms and easily avoidable problems. For example:
    - Line 46, FP, is mentioned but needs to be correctly defined. However, the document defines the abbreviation FP multiple times after that when only one should be enough.
    - Line 182, there is an extra “192”.

Limitations:
As the authors mention, this method heavily relies on a pre-trained model with a similar distribution to the incoming tasks. This assumption is only sometimes true and can be more complex in scenarios with a more complex input data distribution.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the problem of Task-Incremental-Learning (TIL) with pre-trained transformer in the context of NLP. The author extended their experiments in the Class-Incremental Learning (CIL) scenario. The authors identify potential forgetting causes as (1) negative correlation between data representations and (2) negative correlation between class prototypes. After theoretically justifying their claim, the authors propose to align the model representations with either a) learning new key matrices for pre-trained queries and values b) learning new key matrices for with additional token chosen as the nearest neighbors of the considered token c) learning new queries and values from the original ones with a low-rank adaptation strategy. The author additionally align prototypes (class vectors) leveraging a probing and fine-tuning strategy. This paper shows superior performances on various dataset in CIL and briefly discuss the effect of various components.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- clear and understandable paper
- the equations are well derived and clear
- the proposed approach has interesting theoretical justifications
- The obtained performances on the TIL settings are compelling

Weaknesses:
1. While I believe the evaluation to be sufficient, it would improve the paper to include more recent prompt-learning techniques such as CODA [1] which showed stronger performances than L2P. I believe it would equally be interesting to see this approach applied to Computer Vision problems, even though I understand this paper focuses on NLP.

2. Although the proposed approach is theoretically justified, it would be interesting to quantify such alignment through experiments, by computing data representations covariance/correlation between transformed class vectors; for each alignment strategy.

3. I appreciate the effort to include results in CIL scenarios. However, I think more discussions as to how the proposed approach performs poorly compared to ER-ACE could be introduced.

4. What is the link between the findings of this paper and previous work on orthogonal subspaces in continual learning [2, 3]? In these works, amongst others, it seems that the correlation between hidden representation should be zero, and not positive. I think such work should be included in related work section.

5. A discussion regarding the extra computation induced by the alignment strategies and the PF would be welcomed as well, as it seems to increase it considerably.

6. The code is unfortunately not accessible to the reviewers.

If the authors can address most of the above points I would happily increase my score.

**Typos and presentation**

- LoRA l44 and PF l.46 are not defined. Please either define the acronyms or cite the corresponding paper.
- l.114 $h_{\tau}$ should be h_{i}
- I do not like the use of RHS l.163 and 164.
- l. 182: ""grounded192""?

[1] Smith, James Seale, et al. ""Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[2] Wang, Xiao, et al. ""Orthogonal subspace learning for language model continual learning."" Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10658–10671

[3] Chaudhry, Arslan, et al. ""Continual learning in low-rank orthogonal subspaces."" Advances in Neural Information Processing Systems 33 (2020): 9900-9911.

Limitations:
Limitations have been partially addressed in the main draft. I believe a discussion regarding the poor performances on CIL scenarios should be included, as well as a discussion on the potential computation overhead.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the cause of cross-task interference in class-incremental learning of transformer-based language models. The authors disentangle the cause into the correlation (i) between data representations and (ii) between class vectors in the linear classifier. To tackle (i), the authors propose three ways to construct data representations at each layer by learning an attention over the pretrained token representations. To tackle (ii), the authors propose to only train the classifier for a new task (to obtain a good initialization) before jointly training both the classifier and the encoder.

The authors perform experiments with the pretrained BERT-base model and various text datasets. Training is in the task-incremental setup (where task labels are provided and the model predicts over in-task classes); the model is evaluated on both task-incremental and class-incremental (where task labels are not provided and the model predicts among all classes) setups. The authors found that their methods, ""alignment models"", outperform existing adaptation and continual models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and I did not find major technical flaws.

2. I enjoyed reading the motivation of the paper in Sec. 3 where the authors examine the causes of cross-task interference.

3. I find that the initialization of class vectors may influence cross-task interference interesting, although I have some related questions.

Weaknesses:
1. While the main goal of the paper is reducing cross-task interference, the main results (Table 1) is using the task-incremental setup, where task labels are known during prediction. The class-incremental learning results are only in Fig. 4, comparing the proposed method only with LoRA and ERACE.

2. I find it a bit hard to infer where the performance improvement comes from from the results. I wonder if it is possible to do some fine-grained ablation analysis that verifies that the proposed method indeed helps by reducing overlap in data representations and in class vectors' features for different tasks. For example, one could measure the accuracy on the task level or look at the confusion matrix, which may reveal some information about cross-task confusions.

3. Is $\Delta W$ shared across tasks? If so, I don't understand why the proposed method does not forget. The authors hypothesize that this is potentially due to referencing pretrained representations. However, since the [CLS] representations are constructed involving $\Delta W$, I'd expect the model to lose some ability to generate good representations for past tasks. Could the authors elaborate on this point?

Limitations:
The authors have addressed the limitations clearly in Sec. 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
7ug4oSmN7l;"REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
0uGlKYS7a2;"REVIEW 
Summary:
The authors present a model where an optimizer plays with a learning agent and aims to extract better rewards from the sequential decision-making game by anticipating what the learner will do selecting a strategy that outperforms the value of a game. They study two settings: a zero-sum game, where they show a polynomial time algorithm that can extract an advantage for the optimizer, and a general-sum game, where they show the problem is NP-hard by reduction from the Hamiltonian cycle problem. They show that for a learner using Replicator Dynamics, the optimizer can get an advantage related to the time horizon by using a constant strategy.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper tackles the important problem of using information about other agents to maximize utility in two-player games. The authors present various theorems and proofs to show how strategies for the optimizer can be devised to gain an advantage. Further, they also show a novel hardness proof for the general-sum game by reduction from the Hamiltonian cycle problem, when playing against a mean-based learner.

Weaknesses:
My primary concern with this paper is in the organization and clarity of presentation. The authors do not contextualize and clearly state their results, or talk about the importance of their results. Further, a large portion of the theoretical information is included in the introduction, with various theorems being re-stated later on. Then, there is an awkward interlude with the related work section before continuing with the remaining theoretical analysis. This makes the paper very hard to follow, without any appropriate structure to guide the reader. I would expect a much shorter introduction, which introduces the problem setting and introduction without going into theoretical detail and summarizes the paper's contributions, following which related work or background for the problem is provided. In general, main Theorems and Propositions should not appear in the Introduction.

Some important details which could be highlighted are missing from the body. For example, the authors don't clearly state the benefit that can be achieved in a zero-sum game by the optimizer, except for the informal statement of proposition 1. This seems to me one of the main contributions of the paper, other than the hardness proof.

Theorems and propositions are referred to by multiple names and repeated. If possible, authors should use the same numbers for the same theorem or proposition. 

The authors also focus only on the MWU and Replicator Dynamics models for the learner. However, there is no justification for why this model is chosen as opposed to any other kind of learner. The setting described is general, however all proofs rely on the learner using this model. Some discussion on why MWU and replicator dynamics are chosen will add to the concreteness of the paper.

Some minor/specific comments below:
It would be helpful if n and m are described explicitly as the cardinality of the action spaces of the two players. As it is right now, it is easy to miss when they are first introduced, and later equations (e.g. Eq. 1) rely on the readers knowing what they stand for.

What is \Delta, as used in the definition of the value for zero-sum games? It seems to be used as a function, but it isn't clear from the context what it is supposed to be. This same notation is used in line 163, which suggests \Delta(A) might be a set?

Line 164: i1, i2 are not described in Proposition 4.

The replicator dynamics is defined as a continuous time analogue of the MWU algorithm, but the MWU algorithm itself is not described until later in the paper. This can be confusing for readers, if MWU is not being used, then it should not be introduced earlier. If Replicator Dynamics needs to be described as a generalization or adaptation of MWU, MWU needs to be described first.

Limitations:
The authors don't specifically have a limitations section addressing the limits of the work, though they do propose future work building on the current results. The authors should address why only one model of the learner is considered, as that can be seen as a limitation.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The authors consider the problem of optimally exploiting a learning agent in zero-sum games and general-sum games.
- For zero-sum games, they look at a specific continuous time learner, replicator dynamics, and at an analogous discrete time learner, multiplicative weights updates. They provide an explicit expression for optimal utility of the exploiter agent in the continuous case, as well as a construction for a polynomial-time algorithm that achieves that utility.
- In the discrete case, they show that the discretized optimal strategy from the continuous case achieves at least as much utility. Moreover, they show that this utility can be exceeded, and they bound this difference between the continuous and discrete cases from below and from above.
- In the context of general-sum games, the paper shows that determining the optimal utility of the exploiter against a Best Response Algorithm (in discrete time) is NP-hard.
- The paper concludes by discussing open problems, in particular whether there are some learners and classes of general-sum games where one can say more about the optimal utilities for polynomial-time optimizers.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper makes a significant contribution by analyzing the general setting in which an optimizer is trying to exploit a mean-based learner. In my view, both the impossibility result in the general case as well as the characterization of the zero-sum case are conceptually interesting.
- The mathematical analysis appears sound and rigorous. The paper discusses an interesting connection to optimal control. (Though I have not checked the proofs in the appendix and this is not my main area of expertise).
- I found the paper was well written and well motivated.
- I appreciated the discussion of societal impact. I agree with the authors that this work is relevant since it is important to study how exploitable commonly used learning algorithms are.

Weaknesses:
- The paper doesn't include any empirical simulations. It would have been interesting to see some example numerical computations of the optimal strategy in a game and resulting simulated utilities. How much better does the optimal strategy do compared to simple baselines, in some canonical games?
- Focusing on MWU and related algorithms makes sense to make the analysis tractable, but it also limits the applicability of the results. Moreover, it would be especially interesting to have an impossibility result for a no regret algorithm.
- While the computational hardness result is interesting, it does not appear very surprising given typical hardness results in game theory.
- While I found that the authors generally explained their approach well, I thought it might have been interesting to give some more explanation, potentially via a concrete example (in the zero-sum case). If this takes up too much space, one could include an example in the appendix.

Limitations:
The authors adequately address limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies a control problem where an optimizer plays with a mean-based learner in a zero-sum or general-sum game. The problem follows a previous line of work that aims to understand how to play optimally with no-regret learners in repeated games. The paper shows several new results. For zero-sum games, an optimal algirithm is provided for a continuous-time setting, where the learner uses a MWU-like learning dynamic. The authors show that this algorithm, when extended to the discrete-time setting, results in an algorithm that guarantees the optimizer the value of the corresponding one-shot game. For general-sum games, the paper provides a negative result, showing that no FPTAS would exist for computing the optimizer's optimal strategy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem studied follows a line of very well-motivated problems. Understanding how to play optimally with mean-based agents is very interesting.

- The paper is overall technically sound and presents some solid results.

- The zero-sum and continuous-time settings are interesting and look like reasonable choices for the problem. The paper presents non-trivial results for these settings.

Weaknesses:
- The informal statements of results in Section 1 could have been made clearer. The current presentation does not allow an easy comparison of results in different settings.

- The results only apply to an MWU learner, not general mean-beased learners, and it relies on the knowledge of $\eta$.

- In the discrete-time setting, the algorithm only guarantees a lower bound. There is no matching negative result.

- The negative result for the general-sum case only implies it is hard to get reward T-1, so it doesn't preclude the optimizer from obtaining sublinear regret overall. Moreover, fictitious play is a bit simplisitc as it is not no-regret. (However, I think the reduction itself is quite interesting.)

---

Minor:

- It may be clearer to state Assumption 1 as a condition or a property. Stating it as an assumption may make it sound like your result requires additional assumption, while I think the purpose of Proposition 4 is more about showing the existence of games with a utility gap.

- Line 362: closed form

Limitations:
The authors addressed the limitations properly.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper outlines conditions under which, in repeated two-player zero-sum and general-sum games between a learner with an online learning strategy and an optimizer that knows the learner's strategy and utility function, the optimizer can learn a policy to achieve a higher average utility than the value of the one-shot game. For the two-player zero-sum game setting, assuming the learner selects actions following Replicator Dynamics in continuous time, this paper proposes an algorithm in which the optimizer can take a constant (over time) optimal strategy that provably maximizes its utility. In discrete time with the learner following a Multiplicative Weights Update (MWU) strategy, the continuous time optimal utility of the optimizer is shown to lower bound the discrete time optimal performance following the same strategy. For general-sum games, this paper proves a computational hardness result showing it is NP-hard to approximate the optimal utility for the optimizer against a Best Response learner. This is proven via a reduction from the Hamiltonian cycle problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper does a good job at highlighting the motivating questions, and the main ideas of the proposed approach are in general clearly explained. The proof for the reduction from the Hamiltonian cycle was quite intuitively explained too, and I really appreciate it.  

2. The theoretical contributions seem sound and relevant to the optimizer-learner setting in two-player zero-sum and general-sum games. Please note that I have not thoroughly checked the proof details in the appendix.

Weaknesses:
1. This paper primarily focuses on theoretical bounds on optimal utility and computational hardness guarantees for learning an optimal strategy for the optimizer against mean-based learners in the zero-sum and general-sum games. But there are no empirical evaluations of the proposed algorithms. In section 1.2 Related Work, authors point out prior work in contracts and auction design, which could also be broadly categorized as mechanism design. Some recent related papers (eg. [1],[2] ) have proposed experimental frameworks to analyze interactions in similar optimizer-learner frameworks, which could be referred to for similar experiment design with repeated games and bandit agents. 

[1] Guo, W., Agrawal, K.K., Grover, A., Muthukumar, V.K. and Pananjady, A., 2022, March. Learning from an Exploring Demonstrator: Optimal Reward Estimation for Bandits. In International Conference on Artificial Intelligence and Statistics (AISTATS).

[2] Banerjee, A., Phade, S.R., Ermon, S. and Zheng, S., MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning. Transactions on Machine Learning Research.

2. The connection to optimal control could perhaps be better motivated and explained.  

3. Confusing notation that can be improved:
- Line 255 and 256: Should it be $R_{cont}(x, h(0), T, A, -A)$ and $R^*_{cont}(h(0),T,A,-A)$?
- Line 308: ""for each node $v_i$ of the graph, the learner has two associated actions $v_i$ and $v_i'$"" - the overloaded use of $v_i$ is confusing. 
- I might have missed it, but what is the relation between $h_i(t)$ and $h(t)$? Is it explicitly defined somewhere in the paper?

Limitations:
Yes, authors have discussed the limitations and potential future directions for their approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
opaRhDvQRD;"REVIEW 
Summary:
This paper identifies two previously overlooked challenges in online continual learning (CL): model ignorance and myopia. In response, it introduces a new framework called Non-sparse Classifier Evolution (NsCE). NsCE features non-sparse maximum separation regularization and targeted experience replay techniques designed to quickly learn new globally discriminative features. Experimental results show significant enhancements in both model performance and throughput.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-motivated by the shortcomings of existing OCL methods. It tackles a significant learning problem, and the focus on model throughput from both empirical and theoretical perspectives is a pertinent challenge in many practical applications involving data streams.
2. The strength of the paper lies in its thorough empirical evaluation, which convincingly illustrates the concepts of model ignorance and myopia. Additionally, the introduction of the term ""myopia"" is intriguing, and the accompanying theoretical analysis offers convincing insights into this issue.
3. The analysis of model parameter sparsity is interesting, and the proposed method with pre-trained initialization aims to address the issues of model ignorance and myopia, ultimately achieving good empirical performance.
4. It's the first time theoretical results have included discussions of model throughput, which represents a noteworthy contribution.

Weaknesses:
1. For the model's ignorance, following the general expectation of transfer learning that pre-trained models facilitate fast learning, it is not surprising that using pre-trained models improves the performance of OCL. So how the pre-trained model improve the learning speed is still not clear
2. As highlighted in Section 4.1 of the article, many existing methods struggle with large-scale volatile datasets. While employing pre-trained models can partially mitigate the issue of model ignorance, it is still uncertain whether this approach offers a definitive solution. Although Appendix B.2 addresses this concern, a definitive and clear solution to fully resolve this problem has not yet been established.
3. It's better to illustrate why to claim that using $max()$ function is easily affected by a small number of outliers in the parameters.
4. The paper claims that a smooth classifier can help mitigate the model's myopia, but it hampers the model's ability to perform rapid classification on the current task. Are there any insights into why this occurs?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper revisits the core challenges of Online Continual Learning (OCL) in high-speed data stream environments, identifying two significant obstacles: the model's ignorance and myopia. It then introduces a non-sparse classifier evolution framework (NsCE) designed to effectively address these issues. Additionally, the authors offer some theoretical guarantees from a PAC-Bayes perspective, providing insights into the robustness and generalization capabilities of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The analysis of factors beyond forgetting in OCL is enlightening, especially in highlighting the suboptimal performance of current approaches.
2. The inclusion of model throughput as a factor in OCL is crucial, and the analysis from a Pac-Bayes perspective offers compelling insights.
3. The examination of sparsity and the proposed Non-sparse Classifier Evolution (NsCE) in continual learning is noteworthy. The method is straightforward, easy to implement, and has proven effective in experiments.
4. The paper is generally well-structured and easy to understand.

Weaknesses:
1. While the authors have imposed constraints on the number and frequency of memory buffer accesses, these conditions still mimic laboratory settings rather than real-world applications. It would be beneficial to provide examples of real-world scenarios where such restrictions are applicable and realistic.
2. Despite that the analysis of model throughput as a factor in OCL is intriguing, it seems that the authors did not give a perfect strategy. More analysis on how to improve the model throughput and the relationship with pre-trained models is needed.
3. Some experiments in the Appendix is better to be concluded in the main text as they also serve as some important validations on the proposed methods like results in Table6, Table11 and Table12.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper identifies and formalizes main challenges specific to OCL. Notably the authors highlight the need for a stronger focus on ignorance (the inability of the online learner to fully converge) and throughput. Similarly, the authors identify Myopia, which corresponds to learning sparse feature, as a potential issue. Therefore they propose a simple yet effective non-sparse regularization strategy, combined with a maximum separation and targeted experience replay strategy to solve myopia with minimal computation overhead. A comparison with state-of-the-art approaches and pre-trained models shows that the proposed approach outperforms existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The experiments seem to have been realized with care
- The paper is well written
- The experimental results are compelling
- I appreciate the defined challenges as I would also agree that Ignorance is an important topic in OCL as the focus is shifting away from forgetting in recent studies
- I appreciate the effort of providing a theoretical analysis
- the code is available

Weaknesses:
**Major Weaknesses**

1. What is the justification behind this limited request on the memory buffer? Why not include experiments in the traditional setup with $Freq=1$?
2. Some references to related work on budget CL are missing. I would suggest the authors to clarify what is the difference between their analysis with regard to the throughput and the analysis of computationally budgeted continual learning [1]. It would be beneficial to include such methods in the comparison table.
3. What is the justification behind the currently used metric $A_{AUC}$? I would like the authors to report the Average Accuracy, which should be the metric of interested in continual learning.
4. A graph showing the weight sparsity with and without the sparsity-regularization term could also showcase better the impact of the loss. At the moment, there is no clear demonstration of how much myopia has been solved. Overall the presentation of how the proposed approach solves the introduced challenges could be improved.
5. The authors should clarify the relation between the defined challenges and the usual stability-plasticity trade-off challenges of continual learning [6]. What is the relation between ignorance and plasticity? Is Myopia related to stability?
6. Figure captions must be enlarges. Confusion matrices are also barely readable.

**Minor Weaknesses**

7. I believe the findings of figure 2 right hand side have been discussed in previous studies such as [5]. Such references could be included in the discussion.
8. Table 1 readability could also be improved (number size specifically).
9. figure 2, right hand side has incorrect markers.
10. If I understand correctly, the ""single task setting"" is just training for one epoch. If so, I would advise to introduce it as such.
11. equation 3 should be reference in figure4 caption.
12. l184 : ""This trend towards simplification is illustrated in Figure 4(Right), where there is a noticeable increase in the sparsity of parameters associated with older tasks as new ones are introduced."" You might want to define the sparsity as $1-s(w)$ as current definition can be misleading. A high sparsity is obtained with a low $s(w)$ value.
13. I wonder how would the sparsity be affected by methods such as SS-IL [2], ER-ACE [3] or GSA [4] , which focus on re-arranged last layer weight updates.

**Typos**

- caption of Figure 4 : ""class 0 corresponding to class0""
- l167: Appednix

**References**

[1] Prabhu, Ameya, et al. ""Computationally budgeted continual learning: What does matter?."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[2] Ahn, Hongjoon, et al. ""Ss-il: Separated softmax for incremental learning."" Proceedings of the IEEE/CVF International conference on computer vision. 2021.

[3] Caccia, Lucas, et al. ""Reducing representation drift in online continual learning."" arXiv preprint arXiv:2104.05025 1.3 (2021).

[4] Guo, Yiduo, Bing Liu, and Dongyan Zhao. ""Dealing with cross-task class discrimination in online continual learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[5] Buzzega, Pietro, et al. ""Rethinking experience replay: a bag of tricks for continual learning."" 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021.

[6] Wang, Maorong, et al. ""Improving Plasticity in Online Continual Learning via Collaborative Learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
The limitations have been correctly discussed in appendix.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ACIDDnTbSJ;"REVIEW 
Summary:
This paper proposes a method to generate feint behaviors and strategies so that the agent can obtain temporal and spatial advantages when competing with opponents. Specifically, this paper first describes the characteristics of feint behaviors at action-level and proposes a Feint behavior template generator called Palindrome-directed Generation that extracts subsets of semi-symmetrical actions from an offensive behavior and synthesizes them as a Feint behavior. Then, the paper proposes a dual-behavior model, which considers the physical constraint and effectiveness when constructing effective combinations of feint behaviors and follow-up actions. Moreover, the paper formalizes the feint implications at strategy-level and proposes rewards reflecting the temporal, spatial, and collective impacts. Finally, the paper provides an implementation scheme of feint behaviors that can be integrated into the existing MARL frameworks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper provides the formalization of feint behaviors and the corresponding palindrome-directed feint behavior template generator. The analysis of feint behaviors and their combination with follow-up actions makes sense.

2. The proposed feint behavior implementation scheme can be easily integrated into existing MARL frameworks.

3. The paper is organized well and the presentation is easy to follow.

Weaknesses:
1. This paper claims that most offensive behaviors can be decomposed into three action sequences, which are Stretch-out Sequence (Sequence 1), Reward Sequence (Sequence 2), and Retract Sequence (Sequence 3) and it proposes a feint behavior template generator based on palindrome structure. This design should be effective for many feint behaviors composed at the action level. However, more complex feint behaviors belonging to the strategy level may not consist of the above three parts and thus the palindrome structure will not work. This paper may specify more clearly the scope of feint behaviors to which the proposed scheme can be applied.

2. The discussion in the experiment section is relatively weak. For example, according to Appendix E, when applying the feint behavior implementation to MARL frameworks, the implementation trains a feint policy model to generate feint behaviors. The experiment results show that the feint behaviors can increase task returns. However, it is unclear whether the actions generated by the feint policy model are really feint behaviors. It is also possible that the feint policy model trained based on $Rew_{collective}$ manages to find certain good actions to beat opponents. This paper may have more discussion about this based on a clear definition of feint behaviors.

Limitations:
Please see the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a comprehensive formalization of feint behaviors in competitive multi-player games. The paper introduces a method for the automatic generation of feint behaviors using Palindrome-directed templates and combines them with high-reward actions in a Dual-Behavior Model. This formalization is incorporated into Multi-Agent Reinforcement Learning (MARL) frameworks. The authors conducted extensive evaluations using various MARL models in both two-player and six-player scenarios, demonstrating that their formalization significantly improves game rewards.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The paper introduces a new formalization of feint behaviors, in which automatic generation of Feint behaviors via Palindrome-directed templates, and combine them with intended high-reward actions in a Dual-Behavior Model
* The methodology addresses Feint implications on game strategies in terms of the temporal, spatial, and their collective impacts via the implementation in existing MARL frameworks.
* In the experiments using various MARL models in both two-player and six-player scenarios, the authors demonstrate that their formalization significantly improves game rewards.

Weaknesses:
* The paper may lack clarity and consistency in the definitions and notations of key concepts, such as the reward functions in Section 4.2.1 and the Policy Occupancy Measure for model-free policies in Section 4.2.2. Additionally, the inconsistency in describing the payoff matrix dimensions in Section 4.2.3 creates confusion about the interaction between different agents' policies.

* The paper also does not provide sufficient methodological details for calculating crucial measures, such as the Policy Occupancy Measure for model-free policies and the structure of the new policy occupancy measure.  

* The evaluation results focus primarily on gaming rewards without a thorough analysis of the types and distributions of observed Feint behaviors. A more comprehensive assessment, including the underlying reasons for focusing on gaming rewards and the detailed impact of Feint behaviors, would enhance the understanding of the study's contributions.

Limitations:
The authors have not adequately addressed the limitations and potential negative societal impact of their work. While the paper presents a novel approach to generating and integrating Feint behaviors in MARL frameworks, it lacks a detailed discussion on the limitations of the proposed methods, such as the scope of applicability and potential challenges in real-world implementations. To improve, the authors should include a section discussing these aspects, providing a balanced view of the strengths and weaknesses of their approach, and consider potential negative outcomes and mitigation strategies.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a comprehensive approach to formalizing and implementing feint behaviors in multiplayer games. It introduces a new method for the automatic generation of feint behaviors using Palindrome-directed templates and combines these with high-reward actions in a Dual-Behavior Model. The paper further explores the implications of feint behaviors on game strategies, focusing on temporal and spatial impacts, and provides a unified implementation scheme. Experimental results demonstrate significant improvements in game reward gains, diversity, and minimal time overheads when incorporating feint behaviors.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The introduction of Palindrome-directed templates for generating feint behaviors is a novel concept that adds value to the field of game strategy formalization.
* The paper offers the first detailed formalization of feint behaviors at both the action and strategy levels, addressing a gap in existing literature.
* The integration of feint behaviors into common MARL frameworks and the use of diverse experimental scenarios enhance the practical applicability of the proposed methods.
* The implementation scheme is designed to be adaptable across various MARL models, which increases its versatility and potential for widespread adoption.

Weaknesses:
* While the paper evaluates several MARL models, it would benefit from a broader comparison with more diverse baseline strategies and models to establish the relative performance gains more comprehensively, such as [1-2]. 
* The paper does not sufficiently address potential scalability issues when applying the proposed methods to larger and more complex game environments beyond the tested scenarios.
* The focus is primarily on multiplayer games, and the applicability of the proposed methods to other domains with different characteristics is not explored in depth.
* It is recommended to apply appropriate smoothing to the curves to enhance the readability of the figures. This is particularly important for Fig 4, where almost all the curves without feint are nearly overlapping.

[1] Yu, Chao, et al. ""The surprising effectiveness of ppo in cooperative multi-agent games."" Advances in Neural Information Processing Systems 35 (2022): 24611-24624.
[2] Yang, Tianpei, et al. ""ASN: action semantics network for multiagent reinforcement learning."" Autonomous Agents and Multi-Agent Systems 37.2 (2023): 45.

Limitations:
The author mentioned ""Limitation discussed in the Discussions,"" but there is no section called ""Discussion"" in the manuscript. I also couldn't find any discussion about limitations in other parts of the document.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the first comprehensive formalization of Feint behaviors in multi-player games. The authors present a novel approach to automatically generate Feint behaviors using Palindrome-directed templates and combine them with intended high-reward actions in a Dual-Behavior Model. The formalization addresses both action-level and strategy-level aspects of Feint behaviors, considering temporal, spatial, and collective impacts.

The authors provide a unified implementation scheme to incorporate Feint behaviors into common Multi-Agent Reinforcement Learning (MARL) frameworks. They evaluate their approach using multiple MARL models in a custom boxing game scenario and a strategic real-game simulation.

The results demonstrate that incorporating Feint behaviors can significantly increase game rewards and improve the diversity of multi-player games. The authors show that their method outperforms existing approaches in several scenarios, with minimal computational overhead.

While the work presents a novel and potentially impactful approach to modeling deceptive behaviors in games, it is limited by its focus on specific game scenarios and lack of theoretical analysis. Nevertheless, this paper contributes a valuable framework for enhancing the realism and strategic depth of AI agents in multi-player games.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper demonstrates significant originality by providing the first comprehensive formalization of Feint behaviors in multi-player games. The authors address a notable gap in the literature, as previous works have only touched on Feint behaviors superficially or as proof-of-concept. The formalization at both action and strategy levels shows a depth of thinking that goes beyond existing approaches.

The quality of the work is evident in the thorough development of the formalization, from the Palindrome-directed templates for generating Feint behaviors to the Dual-Behavior Model for combining them with high-reward actions. The authors have clearly put considerable effort into creating a robust framework that can be applied across different game scenarios.

In terms of clarity, the paper is generally well-structured, guiding the reader through the complexities of Feint behavior formalization in a logical manner. The use of illustrative examples, particularly in the boxing game scenario, helps to ground the abstract concepts in concrete applications.

The significance of this work lies in its potential to enhance the realism and strategic depth of AI agents in multi-player games. By incorporating Feint behaviors, the authors have shown significant improvements in game rewards and diversity, which could lead to more engaging and challenging game AI. Furthermore, the unified implementation scheme for common MARL frameworks suggests broad applicability of this approach.

Weaknesses:
While the paper presents a novel approach, there are several areas where it could be strengthened:

1. The primary experimental validation is conducted on a custom boxing game scenario. While this provides a good test case, it may not fully demonstrate the generalizability of the approach. The authors should consider including experiments from a wider range of game types (e.g., strategy games, team sports) to show the broad applicability of their formalization.

2. While the paper provides a detailed formalization, it lacks rigorous theoretical analysis or proofs of the properties of the proposed approach. For instance, the authors could provide theoretical bounds on the performance improvements or convergence guarantees for their method.

3. The paper primarily compares the performance of agents with and without Feint behaviors. However, it would be valuable to see comparisons against other existing methods for modeling deceptive or strategic behaviors in games.

4. The formalization and implementation details are quite complex, which could make it challenging for others to replicate or build upon this work. The authors could consider providing a simplified version or pseudocode of key algorithms to improve accessibility.

5. While the paper focuses on game simulations, it doesn't adequately address how this approach might be applied to or impact real-world game design or AI systems beyond simulations.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
cCOpatbXFU;"REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TKozKEMKiw;"REVIEW 
Summary:
This paper formulated the problem of finding an optimal decision tree as Markov Decision Problem and solve the scalability problem using an information-theoretic test generation function. This method provides a trade-off between the train accuracy and tree sizes, the decision tree naturally offers interpretability over ML algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written and well-organized, it combines RL and decision tree generation, and building MDP before constructing the decision tree.

Weaknesses:
1. The paper lacks sufficient novelty. The approach of constructing a Markov Decision Process (MDP) and using Decision Trees (DT) to generate actions is not new. Specifically, Algorithm 1 appears to still rely on Classification and Regression Trees (CART) for splitting criteria, which diminishes the originality of the proposed method.
2. The evaluated scenarios in the paper are not clearly articulated. The algorithm has not been tested against well-known benchmarks, unlike other optimal DT algorithms. This makes it difficult to assess the comparative performance and robustness of the proposed approach.
3. The advantages of using this algorithm instead of CART are not clearly demonstrated. Both algorithms control tree size and depth. However, CART is known to converge faster and offers a simpler implementation. Without clear evidence of the benefits, it is hard to justify the use of the proposed method over established techniques like CART.
4. The definition of actions generated by the tree is ambiguous. It is not clear whether the actions are discrete or continuous. If the algorithm is designed to build an MDP, it should be tested on general reinforcement learning (RL) tasks to validate its effectiveness and applicability in broader contexts.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors pose binary decision tree construction within the framework of Markov Decision Processes. They first propose methods for constructing an MDP from a decision tree construction problem, exploring varying test generating functions that trade off the coverage of the search space vs the size of the search space. They then apply Dynamic programming to solve the resulting MDP and show this learnt method can both create binary trees that minimise the loss over a dataset but that it can also be used to add additional losses a user may have over decision trees such as the prior that trees should be small, making them interpretable. They evaluate their proposed method, comparing with other high performing methods such as Quant-BnB, MurTree and a DeepRL method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Well written paper and easy to understand the method
- Clearly an important direction of research
- Thorough experiments with appropriate baselines and good range of datasets to ensure the conclusions generalise to a wide range of datasets
- Code fully provided, along with implementations of baselines used

Weaknesses:
- Various versions of Reinforcement Learning for binary tree construction have previously been explored. While the implementation in this paper is ultimately different and appears to significantly improve performance, there is limited novelty of the approach. Novelty largely comes down to the test generating functions explored and the addition of extra losses (interpretability) in addition to just the dataset accuracy.

- Small formatting issue
     - Table 1 is too small

Limitations:
- Limitations are appropriate addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper models the construction of decision trees as a reinforcement learning problem. Currently SOTA algorithms for constructing decision trees have the drawbacks that 1) they take long to compute at depths > 3, and 2) the trees constructed are complex and difficult to interpret. By modelling the problem as an RL task, the authors hope to make the construction of decision trees scale to larger sizes. They present Dynamic Programming Decision Trees which models tree construction as a MDP solved using dynamic programming. They evaluate the accuracy of trees produced by their approach empirically against other commonly used approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The approach presented is a novel approach to constructing dynamic trees.

**Significance:** As datasets become larger and interpretability becomes more important, having an approach that scales DT construction to larger trees is needed now. This makes this work rather significant.

**Clarity:** The first half of the paper (up to Sec. 4) was clear. It becomes harder to understand after. Providing an intuitive explanation certain equations would be helpful. E.g., why is probability $p_l = |X_l| / |X|$?

**Quality:** The technique designed is sound and the experiments chosen were the correct ones to demonstrate their claims.

Weaknesses:
The experimental evaluation is weak. From what I understood, the algorithms being evaluated were run only once and evaluated once. The results do not statistically back up the authors' claims. Multiple runs with statistical significance testing is needed.

There is no actual analysis on the interpretability of the trees produced, only the complexity of the trees.

Limitations:
The authors mention one limitation (test generation) as a problem. It seems that that would make it difficult for DPDT to actually scale to larger trees. Is that not so? Is scalability not a limitation then?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use an approach for learning interpretable
decision trees using markov decision processes. The results are shown
to be competitive with branch and bound methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
None of notice, given the listed weaknesses.

Weaknesses:
There exists extensive experimental evidence challenging the claims
about the interpretability of decision trees, while simultaneously
demonstrating the need for decision trees to be explained, since these
can otherwise exhibit arbitrary explanation redundancy.

As a result, and at present, there is no practical justification
whatsoever to learn so-called interpretable optimal decision trees.
It is absolutely unclear that optimal decision trees will provide any
advantage, regarding computed explanations, over decision trees
induced with heuristic algorithms.

Given the above, I cannot recommend acceptance.

Some references on the necessity of explaining decision trees.

Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, João Marques-Silva: On
Efficiently Explaining Graph-Based Classifiers. KR 2021: 356-367

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the Computational
Intelligibility of Boolean Classifiers. KR 2021: 74-86

Yacine Izza, Alexey Ignatiev, João Marques-Silva: On Tackling
Explanation Redundancy in Decision Trees. J. Artif. Intell. Res. 75:
261-321 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the explanatory power of
Boolean decision trees. Data Knowl. Eng. 142: 102088 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On Preferred Abductive
Explanations for Decision Trees and Random Forests. IJCAI 2022:
643-650

João Marques-Silva, Alexey Ignatiev: No silver bullet: interpretable
ML models must be explained. Frontiers Artif. Intell. 6 (2023)

Limitations:
These were listed above. I believe the paper is solving a non-relevant
problem given practical and theoretical evidence regarding the
non-interpretability of decision trees, be these optimal or not.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
4kCr61XYQJ;"REVIEW 
Summary:
This work extends Poisson-Gamma Dynamical Systems (PGDSs) by considering non-stationary transition dynamics to effectively capture the evolving dynamics of observed count sequences.

The authors propose a model where the underlying transition matrices evolve over time, based on three (gradually more complex and flexible) Dirichlet Markov chains.

For inference of the model, the authors make use of the Dirichlet-Multinomial-Beta data augmentation to derive a fully-conjugate Gibbs sampler.

Experiments showcase improved data-smoothing and forecasting performance of the proposed method across several real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Extending PGDS models to accommodate time-varying transition dynamics is of interest and significant

- The proposed variations of Dirichlet-Markov chains provide flexibility in capturing different modeling assumptions

- Devising a closed-form Gibbs sampler for posterior inference of this model is significant.
    - The attained expressions seem correct to the best of my knowledge, although I did not carefully double-check the mathematical details of the derivation.

Weaknesses:
- The main limitation of this work is the assumption that the transition kernel is static within each sub-interval: i.e., the authors consider that the kernel can only change at discrete instants, while is constant within each sub-interval.

Limitations:
The authors address the main limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing PGDS models struggle with capturing the time-varying transition dynamics seen in real-world data. To address this, the submission proposed a non-stationary PGDS, allowing the transition matrices to evolve over time, modeled by Dirichlet Markov chains. Using Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed for posterior simulation. Experiments demonstrate that the proposed non-stationary PGDS achieves improved predictive performance compared to related models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed non-stationary Poisson-Gamma Dynamical System offers several notable advantages. 

Firstly, its ability to allow transition matrices to evolve over time addresses the limitation of state-of-the-art PGDS models in capturing time-varying transition dynamics, making it more suitable for real-world count time series. 

Secondly, the use of specifically-designed Dirichlet Markov chains to model the evolving transition matrices enhances the model’s capacity to learn non-stationary dependency structures. 

Thirdly, the application of Dirichlet-Multinomial-Beta data augmentation techniques facilitates the development of a fully-conjugate and efficient Gibbs sampler for posterior simulation.

Weaknesses:
I did not find any obvious weaknesses.

Limitations:
The authors discussed some future work directions in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work extends  Poisson-Gamma Dynamical systems (PGDS) to model non-stationary dynamics by replacing the constant transition matrix $\Pi$ with a time dependent one $\Pi^{(t)}$ and the original Dirichlet prior on the columns with three different Dirichlet Markov chain constructions. The manuscript describes am efficient Gibbs-sampler for inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The work addresses a relevant problem of modeling non-stationary dynamics in count time series. The provided extension relative to the original PGDS is sufficiently novel. I lack deep enough understanding of some parts related to the sampler, therefore I cannot assess if the construction of the sampler required new ideas or was a mechanical extension of the sampler for PGDS  (this being the main reason for my lower confidence score.). I tend to assume new ideas were necessary.

Weaknesses:
My major problem is the experiment evaluation. In Table 1 in the NIPS dataset we can see results like $14.014 \pm 4.387$ bolded, over values like $14.706 \pm 4.414$, or $17.105 \pm 6.449$. 
In ICEWS values like $0.214 \pm 0.008$ over $0.215 \pm 0.007$ , in USEI $4.596 \pm 0.562$ over $4.703  \pm 0.538$, in COVID $6.969 \pm 1.107$ over $7.566 \pm 1.095$. These are mainly smoothing results. In the light of this I am not confident in the statement “As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks.”.  We do not know how the confidence interval was computed, or how many repeats were made. The lack of statistical rigor in the evaluation stands in striking contrast with the sophisticated Bayesian model presented. 

Besides this, other possible problem with the evaluation is that the manuscript states that default paramerters were used for the benchmark methods “GP-DPFA, PGDS, GMC-RATE, GMC-HIER, BGAR”  while the present method used specific K based on the dataset. It is very hard to tell if this is a fair comparison or not.

Limitations:
No specific limitation section was provided. The part on future work in the Conclusion can be interpreted as pointing out some limitations of the current model, but a specific limitation statement would be preferable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces non-stationary Poisson-Gamma dynamical systems, an extension of Poisson Gamma dynamical systems with a dynamic transition matrix. Decomposing the time steps into equally spaced subintervals, the transition matrices evolve between sub-intervals, remaining static within sub-intervals. The authors introduce three options for transitions to occur. The authors derive a Gibbs sampling scheme for exact posterior inference using data augmentation techniques and showcase the effectiveness of their method through a series of predictive and qualitative results.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a well-written, organized paper that is easy to read. The proposed method allows for exact posterior inference through Gibbs sampling. The authors exhibit extensive predictive results across 4 datasets, although their method only exhibits marginal improvement as compared to Poisson Gamma dynamical systems.

Weaknesses:
I'm not convinced that the magnitude of the author's contribution, nor the significance of the paper is strong enough to warrant acceptance, and the methods produce only marginally better results than that of Poisson Gamma dynamical systems. The qualitative results are not groundbreaking.

Limitations:
The authors address the limitations of their work, stating intention to address these limitations (e.g. constant sub-interval lengths) in future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
kQ9LgM2JQT;"REVIEW 
Summary:
The paper ""QGFN: Controllable Greediness with Action Values"" introduces a novel approach to enhance Generative Flow Networks (GFNs) by incorporating action-value estimates (Q-values) to control the greediness of sampling policies. This method, called QGFN, includes three variants—p-greedy, p-quantile, and p-of-max—each designed to balance the generation of high-reward samples with the maintenance of diversity. Through comprehensive experiments on tasks like molecule generation and RNA design, the authors demonstrate that QGFN significantly improves the generation of high-utility samples while preserving diversity, providing a practical and effective solution for deploying safe and helpful LLMs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- **Innovative Approach**: The introduction of Q-values to control the greediness of sampling policies in Generative Flow Networks (GFNs) is a novel and creative solution to the challenge of balancing high-reward sample generation with diversity.
- **Comprehensive Evaluation**: The paper includes thorough experiments across various tasks, such as molecule generation and RNA design, providing strong empirical evidence for the effectiveness of QGFN.
- **Ablation Study**: The paper provides in-depth ablation studies on the hyperparameters of GFNs.

Weaknesses:
- **QGFN variants matter**. This paper does not provide a method to select different QGFN variants. Different variants of QGFN have very different performances in different environments. In Section 5, the authors claim that p-of-max is suitable for small action spaces, and p-quantile is suitable for large action spaces. What if different states have different action spaces? For example, in the graph combinatorial optimization problems [1], earlier states have a much larger action space than later states, and the action space size will change (decrease) during the sampling. I think compared with a fixed action space, this setting probably requires the use of different QGFN variants on different states during the sampling. 
- **Lack of more complex environment**. Therefore, I wonder about the QGFN's performance on graph combinatorial optimization problems, such as MIA. [1]
- There are some missing recent works that deal with sampling high-reward candidates with diversity, such as [2, 3] 
- I am happy to raise my scores if my concerns are resolved. 


[1] Zhang, D., Dai, H., Malkin, N., Courville, A., Bengio, Y., & Pan, L. (2023). Let the flows tell: Solving graph combinatorial optimization problems with gflownets. arXiv preprint arXiv:2305.17010.

[2] Chen, Y., & Mauch, L. (2023). Order-Preserving GFlowNets. arXiv preprint arXiv:2310.00386.

[3] Jang, H., Kim, M., & Ahn, S. (2023). Learning Energy Decompositions for Partial Inference of GFlowNets. arXiv preprint arXiv:2310.03301.

Limitations:
See **Weaknesses**

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on improving high-reward sample collection, i.e., exploitation, in training GFlowNets. The motivation stems from the fact that the flow may pursue states that lead to many low-reward states rather than focusing on states that lead to high-reward states. To this end, the authors propose incorporating Q-value estimation, i.e., expected future reward, and interpolating between flow and Q to make a transition. Experimentally, the proposed method shows promising performance in fragment and atom-wise molecule generation and RNA-sequence generation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow, with a well-developed motivation and idea.
- It seems interesting that the compositional nature of the generation model can be considered to improve the performance of GFlowNets for practical applications.
- This paper discusses various design choices for incorporating Q-value estimation, which offer different perspectives on ""greediness.""
- Overall, the experiments are well-done, and the reported results demonstrate the effectiveness of the proposed model.

Weaknesses:
No major weaknesses. See questions below.

Limitations:
The authors have acknowledged the limitations of their approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes jointly learning a $Q$ function and a policy network $P_F$ to improve the search for high-valued states when training GFlowNets. To achieve this, the authors develop three sampling strategies for composing $Q$ and $P_F$, $p$-greedy, $p$-of-max, and $p$-quantile, and show that the resulting algorithm, termed QGFN, often leads to faster discovery of modes relative to GFlowNet baselines.

In spite of QGFN’s notable performance, I believe the paper would greatly benefit from a more clearly described solution and from the inclusion of stronger, optimization-focused baselines. I will be happy to increase my score if the enumerated weaknesses and questions below are properly addressed during the rebuttal period.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. **Well-described problem**. Section 3 highlights the issues of an exclusively GFlowNet-oriented approach to the search of high-valued states with clarity, namely, the large probability mass associated with low-reward states.

2. **Intuitively sound method with strong empirical results**. When the $Q$ function is accurately learned, QGFN — which interpolates between a GFlowNet (p = 0) and a DQN (p = 1) — should yield samples with higher rewards for larger $p$. This behavior is experimentally confirmed. However, the distribution from which QGFN samples, even when $Q$ is perfectly estimated, is mostly unclear. See weakness 4 below.

3. **Extensive experimental campaign**. (however, important baselines are missing) Experiments include four commonly used benchmark tasks for GFlowNets, and QGFN often outperforms alternative methods by a large margin.

Weaknesses:
1. **Missingness of important baselines**. FL-GFlowNets (Pan et al., 2023) and LED-GFlowNets (Jang et al., 2024) exhibited strong performance for the optimization-focused tasks of molecule and set generation. Notably, contrarily to the greediness $p$ of QGFN, these methods do not require tuning of a hyperparameter that may unpredictably modify the sampling distribution. Authors should include FL-GFlowNets and LED-GFlowNets in Figures 2, 3, and 4. Have the authors considered increasing the sharpness of the distribution by modifying the temperature in LSL-GFN?

2. **Training of the $Q$-network is insufficiently described**. Lines 163-175 are hard to follow and Algorithm 1 in Section B does not provide sufficient details for understanding the training of QGFN. If I understand correctly, the adoption of multi-step returns with the horizon size set as the maximum trajectory length implies that the each $R(s_{t})$ in Equation (2) corresponds to the expected reward among $s_{t}$’s children under the chosen sampling policy. In any case, an unambiguous equation representing the learning objective for $Q$ should be included in Section 4.

3. **Difference between each sampling strategy is unclear**. In practical applications, a sampling strategy would have to be chosen, as selecting among the proposed approaches can be excessively time-consuming — even without retraining the model. However, $p$-greedy, $p$-of-max, and $p$-quantile seem to perform relatively different when the benchmark task is modified. A general approach for choosing a sampling strategy should, then, be proposed by the authors. Otherwise, the method is hardly usable in real applications.

4. **QGFNs lack theoretical guarantees**. Important questions are left unanswered. What if $Q$ is not properly trained? How do we know that $Q$ is inaccurately learned? What is the sampling for a given $p$ and sampling strategy? To improve QGFN’s reliability, a theoretical analysis should be included. Does a sufficiently trained QGFN finds, in average, more rewarding than standard GFlowNets?

5. **Rationale for choosing $p$ is not well-defined**. Experiments are filled with cryptically chosen numbers. In Table 1, the $p$ value for $p$-of-max and $p$-quantile are respectively set to $0.9858$ and $0.93$; in Figure 3, a “mode” is defined as a molecule with Tanimoto similarity smaller than $0.70$ and a reward larger than $1.10$. What is the rationale behind such numbers? How can a practitioner choose $p$? Also, if the choice of $p$ relied on a grid search over $N$ values in $[0, 1]$, a fair comparison with GFN-TB and GFN-SubTB in Table 1 should allow these methods to sample $N$x times more trajectories than QGFN. However, it is unclear whether this observation was considered for the experiments defining Table 1.

6. [Minor] Typos. Algorithm 1 refers to $\mathcal{L}\_{flow}$ and $\mathcal{L}\_{huber}$, which are undefined. I assume $\mathcal{L}\_{flow}$ is $\mathcal{L}\_{TB}$, but I could not find a definition for $\mathcal{L}\_{huber}$.

Limitations:
Authors claim that the computational overhead induced by learning a Q network and the sensitivity of sampling to the accuracy of $Q$ are the main limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
4dmwvbs4Ea;"REVIEW 
Summary:
This paper studies the offline policy optimization problem, i.e. to find a policy whose value function is close to the optimal value function using offline samples.  Under the assumption of linear MDP, they proposed a gradient ascent algorithm. 

The sample complexity of the algorithm only depends on the feature coverage of the best policy and does not require coverage over any other policies.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well written. The algorithm, theorems and lemmas presented in this paper are all very clear.

The problem studied in this paper is offline policy optimization, which has significant value to the community, both empirically and theoretically. 

The algorithm in this paper is simple and computationally tractable.

In contrast to other offline RL paper, this paper does not require that the offline data is sampled i.i.d. or to be admissible, and they can handle arbitrary offline data as long as the data has sufficient coverage to the best policy.

Weaknesses:
Compared to Zanette (2021), the algorithm idea is somehow similar. Specifically, both algorithms use the actor-critic update, and the optimization in the algorithm of this paper is similar to the pessimism estimation in Zanette (2021).

The assumption made in this paper is the linear MDP assumption, which is stronger than the assumption in Zanette (2021).

Limitations:
Yes. The authors addressed all the limitations listed in the guidelines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDP, which achieves a strong sample complexity under the weakest data coverage assumption. Moreover, their algorithm is easy to implement and computationally efficient. Their algorithm design is based on a reduced version of linear programming formulation of MDP, which approximately transforms the original problem into a unconstrained saddle-point optimization problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm proposed in this paper has many nice properties: it is computationally efficient, easy to implement, and works under the weakest data coverage assumption.
2. The developed techniques and the observations on transforming the optimization problem is insightful.
3. The paper is also well-written, with clear proof sketch and is well-positioned among related work.

Weaknesses:
The paper is pretty notation heavy and a bit hard to follow. I would suggest including a table of notations with descriptions. The choices of notation can also be optimized. For example, I found the mixed use of $D_{\pi}$ and $D_{\theta}$ confusing, and they looks like they are dependent on the value of $\pi$ and $\theta$.

Limitations:
Nothing necessary stands out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an approach for solving Offline RL problems in domains where the underlying MDP problem has reward and transition models that are linearly realisable under a known feature map. The paper is well presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Some of the tricks employed in the approach are quite interesting. 
2. The analytical results are good.

Weaknesses:
1. This work is only for MDPs where rewards and transitions are linear. Many of the moderately interesting problems are not linearly realisable, so it is quite important that authors provided a detailed discussion on how it can be addressed. 
2. I am not entirely confident about this, so will wait for inputs from authors. The approach seems to be built based on works by Hong and Tiwari, and stabilisation trick [Neu and okolo, Jacobson et al.]. I was not sure on the key significant contributions of this paper on top of those works. 
3. For me the biggest concern is that there are no experimental results. How would such an approach work for an MDP where the transition and reward models are not linear? Also, the approach is still approximate (given the bound), so would have been important to show the real results.

Limitations:
There is no limitations mentioned.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
1SmXUGzrH8;"REVIEW 
Summary:
The paper tackles the problem of single-shot fine-tuning of text-to-image models for diverse subject-driven renditions. It first discusses the problem of image-text alignment present in the few-shot fine-tuning paradigm for text-to-image models. It then presents FineStyle. More specifically, it introduces 

* a novel data augmentation technique to synthetically increase the number of image-text pairs from just a single pair
* concept-oriented masking during the (parameter-efficient) fine-tuning phase

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper identifies the problems present in the existing few-shot fine-tuning frameworks for text-to-image models. 
* The proposed method is simple and is well demonstrated. 
* The derivation of segmentation maps from cross-attention maps for concept-oriented masking is beautiful.

Weaknesses:
* Minimal details available on the pre-trained model being used. The paper just mentions MUSE. It didn't mention its capacity. Similarly, it didn't provide any details on the VQGAN being used. 
* Lack of references provided to the works that leverage parameter-efficient fine-tuning for controlled generation in the domain of text-to-image models. Some examples include [1], [2], and [3]. I believe this is relevant since the authors use parameter-efficient fine-tuning as well. 
* FineStyle was demonstrated for masked models like MUSE. But the image generation community doesn't use MUSE that much. So, I am a little concerned about its adoption at scale. It would be very nice if the authors could also showcase some results obtained from applying FineStyle to open text-to-image models such as [4] and [5]. 
* Timing information would have been nice to include as this study aims to avoid the limitations of the iterative fine-tuning scheme introduced in StyleDrop. 

## References

[1] https://github.com/cloneofsimo/lora

[2] Using LoRA for Efficient Stable Diffusion Fine-Tuning, https://huggingface.co/blog/lora

[3] Implicit Style-Content Separation using B-LoRA, https://arxiv.org/abs/2403.14572

[4] SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, https://arxiv.org/abs/2307.01952

[5] PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation, https://arxiv.org/abs/2403.04692

Limitations:
No comment here.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a few-shot fine-tuning paradigm called FineStyle for controllability-enhanced style personalization that requires only a single reference image. A concept-oriented data scaling scheme and a parameter-efficient adapter are two key components of the proposed method to achieve this goal.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper is well-motivated and well-organized.
2. This idea of scaling the number of training images by creating multiple sub-images and corresponding separate concepts is interesting and reasonable.
3. The controllability of the proposed method is good.

Weaknesses:
1. Only one baseline model (i.e., StyleDrop) is adopted to compare with the proposed method. Many highly related SOTA methods are not introduced or compared in this paper, such as DreamStyler [1*], ControlStyle [2*], StyleAligned [10], and IP-Adapter [40]. They can perform the same task as the proposed method. \
[1*] DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models. AAAI 2024. \
[2*] ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors. ACM MM 2023.

2. The proposed method requires very detailed and complex text description for each input style reference image, which is inconvenient and needs human intervention. Moreover, as the authors say, ‘it is often challenging to faithfully describe the visual look of a style in pure text form.’ In contrast, many other methods do not require additional text description of the input style image, such as DreamStyler [1*], ControlStyle [2*], StyleAligned [10], and IP-Adapter [40].

3. The proposed method is inferior to previous method StyleDrop in style learning, which can be observed from both qualitative results and quantitative results. 


4. Detailed information about human evaluation is not provided. How many image-text pairs and participants are involved in the conducted human evaluation? In addition, the sum of the user preference proportions reported in Table 2 is not 1.

5. I am curious about the running time of the proposed method. Is it comparable or superior to previous methods in speed at inference?

Limitations:
The limitations and potential negative societal impact are discussed in the supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing style-tuning based style transfer methods often result in content leakage because of the coupled style and content. To address this, FineStyle proposes a decomposition conception of style and content in images and fine-tune a kv adapter in cross-attention on MUSE. FineStyle demonstrates better fine-grained control in visual results compared to other methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.FineStyle focuses on fine-grained style transfer and achieves excellent control effects, showcasing the future potential of style transfer.
2.This paper is well written, easy to follow.
3.The experiments are comprehensive, and the appendix provides detailed supplementary information. 
4.This paper shows that the kv adapter on cross-attention is better than the feat adapter in hidden states for more fine-grained style or content control.

Weaknesses:
1.The author mentioned in Section 4.1 that T2I models require a very large image-text dataset for concept learning.Therefore, some style tuning methods use human feedback to scale up the dataset to achieve better learning outcomes. However, the differences between these two datasets are still substantial. For instance, human feedback often adds only a few images, whereas the dataset for a large T2I (Text-to-Image) model typically contains millions of image-text pairs. Although scaling up such a small dataset can theoretically reduce overfitting and enhance the style transfer effect, it is insufficient to achieve the concept learning emphasized by the author.
2.The author mentions in line 50 that the adapter is fine-tuned using clearly defined pairs of content and style concepts, anticipating the learning of associations between text and image concepts. However, in the methods section, it is described merely as a data augmentation technique. The clip scores for variant (a) in Table 1 also suggest that this data augmentation technique does not significantly enhance performance relative to StyleDrop. Furthermore, the paper lacks a cost-time analysis for the concept division.
3.This paper demonstrates that the KV adapter is more effective than the feat adapter in providing fine-grained control. However, it lacks additional experimental analysis to substantiate this claim, such as visualizations of attention maps.
4.The concept pair data scaling up  seems like un-necessary, which reduces the innovativeness of this paper.

Limitations:
The authors have already discussed limitations and societal impact in the appendix.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
NhP8MTJzT5;"REVIEW 
Summary:
The paper introduces SyntheOcc, a framework utilizing diffusion models to synthesize photorealistic images for autonomous driving simulations. The proposed method addresses limitations in the existing 2D diffusion model to generate multi-view driving videos by integrating detailed 3D geometric data.
The authors effectively employ 3D semantic multi-plane images (MPIs) for precise geometric control, enhancing the realism and utility of generated images for training perception models. The paper also proposes re-weighting strategies to address the imbalance problem between foreground, background, and object categories. The experiments prove the effectiveness of the proposed MPI encoder and the reweighting strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces an innovative approach by incorporating 3D semantic Multi-Plane Images (MPIs) to capture both geometric and semantic details of a scene. This approach allows for the precise modeling of 3D environments in a 2D image synthesis context, enhancing the photorealism and depth accuracy of the generated images. 
- The design of the MPI encoder is very effective in handling the input conditions with a large number of channels while maintaining spatial consistency to the latent features of diffusion UNet.
- Additionally, SyntheOcc incorporates sophisticated reweighing strategies to address class imbalance and ensure focus on critical features. These include foreground enhancement, depth-aware reweighing, and class-balanced reweighing.
- The paper outlines a comprehensive set of evaluations to demonstrate the effectiveness of the proposed method. Qualitative evaluations visually demonstrate the photorealism and environmental accuracy of the generated images compared to real scenes from the nuScenes dataset. Quantitative analyses leverage metrics such as Frechet Inception Distance (FID) to measure image quality and evaluate perception model performance, offering solid empirical evidence of the framework's effectiveness. Ablation studies further dissect the impact of various components and design choices in the proposed method. Additional robustness tests are conducted to evaluate how changes in the MPI settings (like variations in depth or semantic labeling) affect the output quality and the training effectiveness of perception models.

Weaknesses:
The contributions for reweighing strategies seem to be minor improvements over 
existing methods (Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan 
Yeung. Integrating geometric control into text-to-image diffusion models for high-quality 
detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023, Benjin Zhu, 
Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection (arXiv preprint arXiv:1908.09492), which limits the perceived novelty of the paper's contributions.

The paper focuses on scene editing capabilities, but there is a noticeable underrepresentation of object-level editing in the experiments. 

Magic drive’s data augmentation is evaluated on two perception tasks BEV segmentation and 3D object detection, with CVT (Zhou & Krahenbuhl , 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Hence, evaluations on same downstream tasks are encouraged for better comparisons to the state-of-the-art baseline.

The paper doesn't provide any evaluations comparing the re-weighing solution proposed by GeoDiffusion.

There are also several noticeable view inconsistencies, eg: Fig 14 - row 2 column 2-3 (clouds seem different), row 7 column 4-5 there is a mismatch in building structures, which are not discussed in the paper.

Limitations:
The authors acknowledge some key limitations in the proposed method. First, it relies heavily on existing data for generating scenes, which means it doesn’t create as much variety as it could. This limits how well it can train models to handle different driving conditions. The paper also struggles with complex scenes, like crowds, where it fails to accurately identify individual people. This is a big deal for autonomous driving, where accurate representations of the scene are crucial for predictions. The authors suggest that future improvements could include better methods for creating diverse scenes and making the model more capable of handling dynamic environments, which would help make the system more practical and effective for real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SytheOcc, a method that employs a diffusion model with 3D occupancy as conditions to generate street view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Unlike previous methods that use box conditions, this paper proposes the use of 3D occupancy, resulting in finer geometric control ability.
2. The paper further suggests the use of 3D semantic multi-plane images to represent the 3D occupancy.
3. The text and figures are well-presented, and the provided examples are very promising.

Weaknesses:
1. The main concern is the inconsistency between views and frames. Despite using Cross-View and Cross-Frame Attention and 3D occupancy as conditions, the spatial and temporal consistency results are unsatisfactory (e.g., Fig 5 (b) and the video demos). This is not the expected outcome, as incorporating 3D occupancy as a consistent world representation should result in better spatial and temporal consistency. Additionally, it would be preferable to have metric results such as FVD for the temporal experiments.
2. In Table 1, why does SytheOcc-Aug show worse results for certain categories (e.g., bicycle, moto)?
3. Table 1 lacks experiments for ControlNet-Aug or ControlNet+depth-aug.
4. Some discussions regarding 3D occupancy as a 3D geometry condition:
   4.1. The field of view (FOV) for 3D occupancy is limited as it is generally generated using lidar, which leads to inconsistency issues for high-rise buildings when considering cameras of larger FOVs.
   4.2. The current annotation of 3D occupancy has limited category coverage. It would be beneficial to explore open-vocabulary approaches.
   4.3. When using only 2D semantic masks as conditions, the paper mentions the presence of ambiguity (i.e., Fig 6 a0). Can the use of instance-level semantic masks alleviate this problem?

Limitations:
The authors have provided a comprehensive list of limitations and future work of their paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper propose a new 3D semantic multi-plane images (MPIs) based image generation pipeline, which enables finer geometric control for 3D editing, dataset generation, and long-tailed scene generation. Through extensive experiments, the work demonstrates substantial advancement in generation quality and better alignment between condition and synthesized images.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work explores a new 3D semantic Multi-Plane Images (MPIs) as a condition, which provides better spatial alignment compared with baselines and enables 3D editing.

2. The comparison results are comprehensive and demonstrate the effectiveness the proposed method, the ablation is relatively complete to validate the MPI Encoder and the reweighting strategy.

3. The paper is well-written, and the experimental results are presented clearly.

Weaknesses:
1. The MPI encoder, which is the major contribution, is not novel for me. Although the proposed 3D MPI enables finer control than BEVGen, but the diffusion model also operates on the 2D domain and generates each view and frame separately without strict geometry constraints.

2. The importance of reweighing is tricky and hard to tune, considering many hyperparameters. Are the m and n in Eq. 6 the same for different datasets?

3. It’s hard to decide how the method works without a supplementary video, I doubt the view-consistency of the generated video across frames and views.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new controllable diffusion-based image generation method named SyntheOcc, which takes an occupancy map as input and generates camera images. SyntheOcc enables the application of scene editing and long-tail corner case generation and shows a strong capability of data augmentation for autonomous driving systems.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Compared with previous controllable image generation methods for traffic scenarios like Panacea or MagicDrive, the occupancy map contains more 3D spatial information than the BEV layout.
2. The paper is well-organized and easy to follow.
3. The extensive experimental results demonstrate the effectiveness of the proposed data generation pipeline.

Weaknesses:
1. The control signal in Panacea or MagicDrive is BEV layout, which only contains lanes and foreground objects and is more easily acquired than occupancy. However, the SyntheOcc relies on sophisticated collected occupancy.

Limitations:
The proposed SyntheOcc faces challenges in real-world application scenarios. For instance, to generate planning-level long-tail corner cases, other methods like Panacea or MagicDrive simply require editing the object's trajectory. However, SyntheOcc demands not only inputting the background occupancy but also constructing a pseudo occupancy for the foreground object. This raises the question of whether using occupancy as a control signal is an advantage or a disadvantage.

The crux of the issue lies in the complexity of this method compared to alternatives. While other approaches need some adjustments to object trajectories, this technique necessitates providing comprehensive background and foreground occupancy data. This additional overhead prompts us to ponder whether occupancy-based control offers tangible benefits or introduces unnecessary complications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
VzoyBrqJ4O;"REVIEW 
Summary:
The authors propose a technique that utilizes unlabeled 360-degree data to improve previous methods, which includes two main stages: offline mask generation for invalid regions and an online semi-supervised joint training regime.  Experimental results indicate that the proposed method outperforms previous methods on the Matterport3D and Stanford2D3D datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well written and clearly structured;

The performance is promising;

The experiments are thorough and the proposed method is validated on multiple datasets.

Weaknesses:
My major concern is that the technical contribution seems limited. The core idea of utilizing unlabeled data has been proposed by DepthAnything. The proposed method is more like an application of this idea to the 360 data;

As shown in Tables 2 and 3, previous 360 depth estimation methods typically use the metric loss for training. While the affine-invariant approach employed by the authors enables training on multiple datasets, it also impedes real-world applications due to the lose of metric scale.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method to improve 360 monocular depth estimation using perspective distillation and augmentation with unlabeled data. It introduces the concept of ""perspective distillation,"" which leverages the available 360 monocular depth maps and their corresponding equirectangular images to generate pixel-wise depth supervision signals. This technique helps to address the lack of ground truth depth data for training in the 360 domain. Additionally, the paper presents an unlabeled data augmentation approach that utilizes the geometric properties of 360-degree images. By exploiting the spherical geometry, the authors generate synthetic stereo pairs to augment the training dataset without requiring paired depth information. The proposed method is evaluated on benchmark datasets and achieves significant improvements in terms of depth estimation accuracy compared to existing approaches. The results demonstrate the effectiveness of perspective distillation and unlabeled data augmentation in enhancing the performance of 360 monocular depth estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative techniques: The paper introduces novel approaches, such as perspective distillation and unlabeled data augmentation, to address the challenges of 360 monocular depth estimation.

2. Improved depth estimation: The proposed method achieves significant improvements in depth estimation accuracy compared to existing approaches, as demonstrated through rigorous evaluations on benchmark datasets.

3. Use of unlabeled data: By leveraging unlabeled data and synthetic stereo pairs, the method reduces the reliance on paired depth information, which is often difficult to obtain in the 360-degree domain.

Weaknesses:
1. Complexity: The proposed method introduces additional complexity, such as perspective distillation and synthetic stereo pair generation, which may require more computational resources and training time.

2. Dataset dependency: The effectiveness of the proposed method heavily relies on the availability and quality of the benchmark datasets used for evaluation, which may affect its generalizability to real-world scenarios.

3. Limited scope: The paper focuses specifically on 360 monocular depth estimation, which may limit its applicability to other depth estimation tasks or domains, such as 360 monocular depth completion. 

4. Insufficient related work: Adding the latest panoramic depth estimation and panoramic depth completion methods is preferred.

Limitations:
This paper proposes an interesting solution for panoramic depth estimation task, ie.e,  perspective distillation and unlabeled data augmentation. It could contribute a lot for the community. The reviewer suggests introducing more related works, including the latest panoramic depth estimation and panoramic depth completion approaches.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper effectively utilizes unlabeled data by employing the SAM and DepthAnything models to generate masks and pseudo-labels respectively. When projecting data onto a cube, the authors use random rotation techniques to minimize cube artifacts, thereby enhancing the accuracy of 360-degree monocular depth estimation. Additionally, the method was tested in zero-shot scenarios, demonstrating its effective knowledge transfer.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper introduces a training technique for 360-degree imagery that enhances depth estimation performance by generating pseudo-labels with the DepthAnything model to leverage information from unlabeled data. Additionally, it employs the SAM model to segment irrelevant areas such as the sky in outdoor panoramic images. Furthermore, the method uses random rotation preprocessing to eliminate cube artifacts.

Weaknesses:
- The paper stated that ""As depicted in Figure 2, the rotation is applied to the equirectangular projection RGB images using a random rotation matrix, followed by cube projection. This results in a more diverse set of cube faces, effectively capturing the relative distances between ceilings, walls, windows, and other objects."" However, from observing Figure 2, the unlabeled data seems to be entirely outdoor panoramas, which makes the mention of indoor elements such as ceilings, walls, and windows confusing. If there are indeed indoor panoramic images in the unlabeled data, what elements might the SAM model need to segment in such indoor panoramas? The description in the paper appears somewhat unclear.
- The paper mentioned, ""We chose UniFuse and BiFuse++ as our baseline models for experiments, as many of the aforementioned methods did not release pre-trained models or provide training code and implementation details."" However, methods such as HRDfuse [1], EGFormer [50], BiFuse and BiFuse++ [35, 36], UniFuse [11], and PanoFormer [29] have all made their source codes available, making this reason in the paper seem insufficient. Additionally, ""EGFormer is not included since its experiments are mainly conducted on other datasets and benchmarks"" appears to be an inadequate reason for not including it in the experiments.
- In Table 3, despite introducing more unlabeled data on the Unifuse training set, i.e., SP-all (p), the performance of the method is not significantly improved. This phenomenon seems to be only effective on the BiFuse model, but not on other methods.
- In Table 3, there is a typographical error in the recording of the Abs Rel value; it should not be 0.858. Additionally, in Table 2, ""UniFuse"" is incorrectly written as ""UniFise.""

Limitations:
- The paper stated, ""Cube projection and tangent projection are the most common techniques. We selected cube projection to ensure a larger field of view for each patch."" This is merely a theoretical assertion, with no experimental evidence to prove which projection method is superior. Additionally, using cube projection directly can lead to cube artifacts. Following the suggestion in [1], setting each panoramic image to have 10 or 18 tangent images using more polyhedral faces, rather than the standard six faces, might reduce the artifacts caused by direct cube projection.
[1]  Cokelek, M., Imamoglu, N., Ozcinar, C., Erdem, E. and Erdem, A., 2023. Spherical Vision Transformer for 360-degree Video Saliency Prediction. BMVC 2023.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a novel depth estimation framework specifically designed for 360-degree data using an innovative two-stage process: offline mask generation and online semi-supervised joint training. Initially, invalid regions such as sky and watermarks are masked using detection and segmentation models. The method then employs a semi-supervised learning approach, blending labeled and pseudo-labeled data derived from state-of-the-art perspective depth models using a cube projection technique for effective training. This framework demonstrates adaptability across different state-of-the-art models and datasets, effectively tackling the challenges of depth estimation in 360-degree imagery.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method employs models trained for traditional pinhole cameras to enhance 360-degree depth estimation, a first in the field. Moreover, the main motivation of this paper is reasonable.
2. It outperforms conventional methods by incorporating pseudo labels from foundational models into the loss function.
3. The paper demonstrates the model's generalizability to real-world scenarios, indicating its practical utility.

Weaknesses:
1. The proposed method is straightforward and the performance gains provided by the proposed method are described as marginal.
2. The method's effectiveness is demonstrated only with specific models, UniFuse and BiFuse++, limiting evidence of its broader applicability.
3. Inconsistencies in the decimal points used in quantitative results tables make direct performance comparisons challenging.

Limitations:
See the weakness and the question parts.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a training strategy for single-image depth estimation on 360-degree equirectangular images. The strategy centers around leveraging strong pre-trained models for perspective images as teacher networks. It does not depend on any particular network architecture and therefore can benefit any 360 depth estimation networks. Experimental results validate that the strategy improves models otherwise trained only with limited ground-truth annotations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality and significance**: 360-degree imagery is becoming increasingly critical for many computer vision applications. However, there is still a significant gap in GT depth annotations compared to their perspective counterparts, and any solution to this issue can have a significant impact. The authors demonstrate that leveraging strong perspective depth models, e.g., Depth Anything, is a simple yet effective solution. 

**Quality**: The main idea and the several supporting procedures (e.g., random rotation processing, valid pixel masking, mixed labeled and unlabelled training) are all well-motivated and reasonably designed. Experimental results show consistent improvement by incorporating the proposed pseudo GT. Additional results including zero-shot and qualitative evaluations further help with understanding and make the approach overall more convincing. 

**Clarity**: Paper is well-written with good structure, clear expressions, and adequate details.

Weaknesses:
1. A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, and avoid generic remarks. 

Despite focusing on a slightly different task (stereo depth), FoVA-Depth (Lichy et al. 3DV 2024) presents a few very similar concepts: 
  - leveraging abundance of perspective depth GT
  - cube map as a intermediate representation to gap between 360 and perspective images
  - random rotation augmentation
It is worth some discussion regarding similarities and differences. 

2. A very simple yet critical baseline is missing: directly project pseudo GT on cubemap to equirectangular images. A good stitching strategy may be challenging, but with something simple or even without any additional scaling, it should help clarify how much the pre-trained depth anything model contribute to the overall performance. 

3. The paper addresses only relative depth estimation. Since several baselines (e.g. upper section of Tab.2) already have metric counterparts, and depth anything has metric variants, I feel some experiments and analysis in that regards should be straightforward and nicely complement the relative depth results. 

4. As the premise of the work is the usefulness of the abundant pseudo GT compard to limited 360 depth GT, it is necessary to understand how does the benefit from pseudo GT scale (how many pseudo GT can be as useful as a real 360 GT?), and how does the student network compare to the teacher network in terms of estimation quality (is the quality of teacher network already a bottleneck?). The paper offers little insight in these questions.

Limitations:
The only limitation the authors bring up (quality of unlabelled data) already has a solution in the paper, so not really a limitation? 

I do think there are a few other things worth mentioning: 
- 360 data, even without requiring GT, is still scarce compared to perspective data. The fact the authors can only evaluate with two such datasets is an evidance. This is a limitation since it prevents further scaling up training. 
- Only equirectangular images are supported (though it seems that, in principle, the approach should work in more general camera models).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
TLUGoShY30;"REVIEW 
Summary:
This paper proposes a comprehensive inverse rendering method that incorporates indirect illumination to enhance the decomposition quality of environment lighting and BRDF materials. Specifically, this work uses multi-time Monte Carlo integration to model light transport and devises algorithms to accelerate computation by precomputing the diffuse term and leveraging an SDF-based geometry for initialization. Both qualitative and quantitative results are promising, demonstrating realistic shadows on glossy surfaces and higher PSNR values.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The results exhibit a decent quality. By incorporating indirect illumination, the modeling of shadows and glossy surfaces achieves a remarkable degree of photorealism.
2. The underlying theory is sound to me. The GGX appearance model is de facto in industry, and the approximation applied for the diffuse term is standard and commonly used in real-time rendering.
3. As mentioned, I believe that some insights are derived from real-time rendering techniques and I personally appreciate it as accurately modeling indirect illumination is also important for effective inverse rendering.

Weaknesses:
1. The optimization time is relatively long compared to other methods. 
2. Due to computational limitations, the number of bounces is restricted, thus limiting the ability to model shiny surfaces and confining the results to glossy surfaces.

Limitations:
Limitations and potential negative social impact are well discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an inverse rendering method that handles multi-time Mante Carlo integration which models indirect illumination. It reduces the computational cost by pre-computing the diffuse map based on a Lambertian model. It also proposes to use spherical Gaussian encoding to improve the initial SDF reconstruction of scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The presentation is clear and easy to follow.

2. Modeling indirect illumination is so far under-explored in inverse rendering

Weaknesses:
1. Overclaim of novelty: multi-bounce MC integration has already been employed in earlier inverse rendering work [1]. On the other hand, the diffuse Fresnel term (Eq. 7) is not commonly used in recent inverse rendering works. Most related works (TensoIR, nvdiffrec/nvdiffrecmc) use exactly Eq 9, so the claim of transferring from Eq. 7 to Eq. 9 should not be counted as a contribution.

2. Continuing from (1), using a pre-computed diffuse map also prevents correct modeling of diffuse shadows, this is not discussed nor mentioned in the paper.

3. Inaccurate/incomplete description of the geometry module: IPE is used for encoding input positions to MLPs, while judging by the text (line 181) spherical Gaussian should be used for encoding viewing/reflecting directions. However, in this work, spherical Gaussian is actually used to replace IPE, which is very confusing. Also in Figure (2b) the entire Diffuse MLP is replaced with ""SG"" and there is no explanation of what this stands for. Also in the same figure, the term ""SGE"" is not explained (maybe spherical Gaussian encoding? Then what is the difference between SGE and SG?)

4. Weak evaluation: there is only a single table in the entire paper. Only PSNR and training time are reported. It is not clear how the PNSR metric is computed (is it novel-view synthesis? Is it relighting?). There is no evaluation of other intrinsic properties that are commonly evaluated in other inverse render papers, e.g. albedo, normal error, etc. The proposed spherical Gaussian encoding is not properly ablated - only qualitative comparison is presented.

5. Missing datasets: it would make the evaluation more complete if the method could evaluate more standard datasets (NeRFactor dataset, TensoIR dataset) where many other works have been evaluated. Also, there is no real-world dataset tested, making it questionable whether the proposed algorithm can work in real-world scenarios.

Factual errors: GGX has nothing to do with the diffuse term used in Disney BRDF. GGX is a microfacet normal distribution function that models specifically the specular part of the Disney BRDF. The diffuse Fresnel term in Disney BRDF is invented by the original Disney BRDF paper and is often omitted in real-time rendering engines and replaced with the Lambertian model.

[1] Mai et. al. Neural Microfacet Fields for Inverse Rendering, ICCV 2023

Limitations:
A crucial limitation, i.e. diffuse shadows are not handled by the proposed method, are omitted and not properly discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose an inverse rendering method that reconstructs the geometry, materials, and lighting of 3D objects from 2D images, effectively handling scenes with multiple reflections and inter-reflections. To address the high computation cost of Monte Carlo sampling, the authors propose a specularity-adaptive sampling strategy, significantly reducing the computational complexity. The authors also introduce a reflection-aware surface model to initialize the geometry and refine it during inverse rendering, to improve the inverse rendering reconstruction quality. Experiments show the proposed method outperforms other inverse rendering methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to understand.
2. The proposed methods are intuitive and the results look nice.

Weaknesses:
1. The validation on real data is missing. The paper only shows its results on synthetic data. It would be better to test this work on challenging real data, such as the specular real data captured by NeRO.

2. The selected baselines are incomplete. There exists a strong baseline candidate, NeRO, which can also reconstruct the geometry and materials of specular objects. (Although its main focus is geometry reconstruction). Since it has released all of its code, it would be better if the authors could also compare this work with NeRO.

3. Need more analysis on the SG encoding. SG encoding looks like a very important part of the final quality(because it plays an important role in the geometry reconstruction). However, the authors only use a small part of Fig. 2 to showcase its effectiveness. More visual comparison and quantitative comparison would be appreciated. In the meanwhile, SG encoding seems to be very similar to [1], which was released on Arxiv last year. The authors should explain the difference between its SG encoding and [1]'s encoding. Since the authors didn't cite [1], I assume that SG encoding should be the author's original contribution.

4. The quantitative comparison is insufficient. More quantitative comparisons on the relighting results and reconstruction of BRDF and geometry should be included.

[1] SpecNeRF: Gaussian Directional Encoding for Specular Reflections

Limitations:
The limitations have been discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for learning disentangled scene representations from images. The proposed pipeline has two stages: the first recovers geometry using an SDF-based model to learn geometry from the images through differentiable rendering. The second applies differentiable ray tracing to predict material parameters and environment lighting. The proposed method differs from previous approaches in that it renders inter-reflections between objects which create indirect lighting rather than only reflections of the environment and shadows. This is shown to produce better results than existing methods in scenes where these inter-reflections are present.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The core idea is solid, and seems to have the expected effect on improving both recovery of environment lighting and rendering quality. I think it would also be fairly applicable to other pipelines which use ray tracing to recover lighting, most likely with similar benefits.

The explanation is clear, and I think the method should be reproducible from the paper, and the authors have also said they will release code.

Generally, I think this is a good contribution tackling a quite difficult problem. The strategies of diffuse pre-computation and spherical Gaussian radiance models will likely be useful to future efforts in this direction.

Weaknesses:
It would have been nice to see some results on real data. The synthetic examples seem to show a clear improvement, but it would help to see the difference in a more practical setting.

Limitations:
The authors are clear about limitations, and I think they cover them well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
fPBACAbqSN;"REVIEW 
Summary:
This paper presents a sparse calculation technique for the attention mechanism in long-context large language models during the pre-filling stage. Specifically, the technique builds on the authors' observation of three patterns in the attention map: the A-shape pattern, the Vertical-Slash pattern, and the Block-Sparse pattern. In the proposed method, the authors first determine the optimal pattern for each attention head offline and then dynamically decide the hyperparameters for each pattern (e.g., sparse indices) on-the-fly. It is worth noting that the A-shape pattern is more regular in terms of the distribution of non-zero indices compared to the Vertical-Slash and Block-Sparse patterns. To provide a low-cost estimation of the non-zero indices in the Vertical-Slash and Block-Sparse patterns, the authors predict these indices using the matrix multiplication results between the last query vector and the key matrix and the matrix multiplication results between a mean-pooled query matrix and the key matrix, respectively. Results on three different pre-trained LLMs and four datasets indicate that the proposed method effectively maintains information from long prompts while reducing pre-filling latency compared to current state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Impressive results: Based on the reported experimental results, the proposed method can reduce the pre-filling latency for a 1M context by ten times without compromising accuracy.

2. Well-motivated: I appreciate the analysis and high-quality figures in Section 2, which explicitly illustrate the key takeaways for readers and effectively motivate the proposed method.

3. Comprehensive summary in the related works section.

Weaknesses:
1. How to ensure that the observations on the three LLMs in this paper are generalizable to different LLMs? Since the entire method is built on the observation of the attention patterns in these three LLMs, the generalizability of this observation is crucial for the robustness of this work. For example, the block-sparse pattern is the most dynamic among the three. What if there are even more dynamic attention maps in other LLMs with specific inputs? How is the search space determined, and how general is it? The offline Kernel-Aware Optimal Sparse Pattern Search determines the pattern for each head offline, and the pattern does not change during inference regardless of the input. Is it always true that the attention pattern for a specific head remains nearly the same?


2. Lack of evaluation on larger models: For longer contexts, larger models seem to be a better option to ensure quality. However, the models evaluated in this paper are relatively small (<10B parameters). Thus, it is unclear whether the improvements still hold for larger models.

3. Lack of discussion on usage for the generation/decoding stage: Pre-filling is important because it determines the first token latency, but generation/decoding is also important because it determines the throughput. It would be beneficial to add some discussion on whether the observations and the proposed method are still applicable for the generation/decoding stage. For example, while the A-shape pattern might still exist, the sparse indices approximation in the vertical-slash pattern might not work for generation/decoding.

Limitations:
The authors discussed the limitation and social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A key challenge for LLM inference with processing long context lengths is time-to-first token for long prompts. This paper introduces a sparse attention method designed to accelerate prefill with long context lengths. They utilize a strategy that incorporates three different types of sparsity (A-shape, vertical-slash, and block-sparse). They calibrate for which sparsity pattern to apply to different heads, thereby adapting to the varying sparsity patterns across LLM heads. Their method is training-free and obtains significant latency gains in terms of time to first token.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Time to first token is a significant problem that is under-addressed in existing KV cache compression works, as many applications have long input prompts and short generation lengths
- Their approach is able to adapt dynamically to different inputs for certain attention patterns (which have different sparsity patterns), as well as to handle differing behaviors across attention heads
- They provide efficient online kernel implementations for each sparsity pattern (both for constructing the sparsity pattern for the two dynamic patterns as well as for sparse computation)
- They observe significant prefill speedups attained with minimal accuracy degradation
- They provide detailed evaluation across a range of long context tasks, as well as ablation for each type of sparsity pattern (and additional analysis showing the distribution of sparsity patterns across attention heads, etc.)

Weaknesses:
- Their approach is heuristic-based (as it is based on fixed types of attention patterns), which may not generalize to new models that are released (some of these patterns could be specific to models that employ particular positional encodings or other model architecture features, which may limit generalizability)
- The offline kernel search assumes that each head will play the same role regardless of the target task (or alternatively that calibration data will always be available for the target task in order to pre-determine which general attention pattern to use and what budget to allocate to that head)
- The writing in some sections is poor (eg. end of Section 2)

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Targeting at the time-consuming profiling stage of long contexts, this paper proposed an efficient prefilling stage spare attention mechanism. It's based on the observation to common attention patterns. The proposed method can be integrated into most existing LLMs, such as LLama3 and Yi-9B. It achieves good performance compared to the original model while significantly reduce the computation times.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1: The motivation is clear and the paper is easy to understand. 

2: The proposed method seems simple yet effective.

3: The experiments are comprehensive.

Weaknesses:
1: It’s tricky to claim that “we found the three patterns”. Those patterns have been found many years ago, since the years of Bert / transformer models were proposed. Even the idea of leveraging such sparsity of attention has been invented many times for different transformer based models. 

Some other major concerns can be found in the ""Question"" section.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes MInference, a method to accelerate the pre-filling stage for long-context LLM generation. The key method leveraged by MInference is dynamic sparsification, which consists of three sparse patterns observed in attention matrices: the A-shape pattern, the Vertical-Slash pattern, and the Block-Sparse pattern. A sparse pattern search method is developed to minimize the sparsification error for each attention. MInference has been tested across various LLMs and benchmarks, demonstrating that it significantly accelerates LLM generation in long-context settings with no performance drop.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and well-motivated.
- Studying how to accelerate LLM generation for long-context settings is of great practical importance.
- The experimental results of the proposed MInference method are promising.

Weaknesses:
- The experimented model scales are relatively small (e.g., LLaMA-3-8B and Yi-9B) and are only constrained to dense LLMs (i.e., not MoE).
- It is not clear how general the observed sparse patterns are across LLMs of various scales.
- The implementation details on how the proposed MInference interacts with existing CUDA kernels (e.g., flashattention) are not very clear.

Limitations:
The limitations of the proposed method have been thoroughly discussed. And there is no potential negative societal impact of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zuWgB7GerW;"REVIEW 
Summary:
This paper introduces Accordion Networks (AccNets), a novel neural network structure composed of multiple shallow networks. The authors propose a generalization bound for AccNets that leverages the F1-norms and Lipschitz constants of the subnetworks, demonstrating that these networks can break the curse of dimensionality by efficiently learning compositions of Sobolev functions. The paper also provides theoretical insights and empirical validation, showcasing the superior performance of AccNets in learning complex compositional tasks compared to shallow networks and kernel methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The introduction of Accordion Networks (AccNets) as a novel neural network structure is a creative and original contribution. The paper provides a thorough theoretical analysis supported by empirical evidence, ensuring the soundness of its claims. The ability of AccNets to break the curse of dimensionality by learning compositional functions efficiently addresses a fundamental challenge in high-dimensional learning tasks.

Weaknesses:
1. The practical implementation of the proposed regularization methods might be challenging, particularly the first one requiring infinite width. 

2. The paper mentions the difficulty in optimizing Lipschitz constants, which could be a limitation in practical applications.

3.  Additional experiments on more diverse real-world datasets could further demonstrate the robustness and generalizability of AccNets.

4. Although the author has discussed the differences between DNN and AccNet, there is still not enough information for me to be sure in which settings to use AccNet and in which settings to use DNN. More clear differences and applicable conditions, especially the shortcomings of each need to be pointed out.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization bound for deep neural networks that describes how depth enables models to learn functions that are compositions of Sobolev functions. To do this, they both prove a generalization bound for compositions of accordion networks (densely connected networks with a low-rank weight structure) and for compositions of Sobolev functions. They then present a sample efficiency result for different kinds of regularization on accordion networks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I really liked this paper and would like to see it accepted to NeurIPS. It addresses an important question: how does depth change generalization bounds for deep neural networks? To my knowledge, not many papers so far have addressed this question and I found the findings presented here very interesting and well embedded within prior methodology.

I also found the paper very well written. I found it easy to follow along despite the highly technical nature of the results (note that I did not check the proofs in particular detail). I especially appreciated the remarks explaining different potential extensions and limitations.

Finally, the theory appears to be able to explain certain empirical phenomena (in networks trained under realistic paradigms) at least qualitatively (though note that I had a few questions I will mention under weaknesses and questions). This indicates to me that it is a promising way for thinking about generalization in deep neural networks.

Weaknesses:
1. I would like to see a more thorough comparison with shallow networks and generalization bounds, as this comparison is a central argument for the usefulness of the presented theory. While it is clear how the findings for the shallow network are a special case of the findings on the deep networks (as presented in Thm. 1), it remains a bit unclear to me how the theory can explain improved generalization in deep compared to shallow networks. The authors certainly present different several pieces of evidence on this: both Fig. 1 and Fig. 3 demonstrate that shallow networks exhibit worse scaling. I also appreciated the theoretical explanation of a particular contrast in l. 256-261. However, I think it would be really useful to provide a general theoretical explanation for this difference and test it empirically: would it be possible to extend the theoretical comparison in l. 256-261 to the general experimental setup studied in the figures --- and if so, would this theoretical comparison predict the conditions under which deep networks have the strongest advantages over shallow networks (or perhaps the conditions under which they don't perform that much better)? Not only would this serve as a useful validation of the theory, I think it would also provide a more extensive intuition for the authors' findings.

2. I appreciated the fact that the authors compare their findings with related work wherever this becomes relevant. However, I think a (potentially brief) section comparing the results here to other theoretical investigations of depth in deep networks (perhaps using different approaches) would be useful. 

3. The linked codebase does not contain the notebooks indicated in the README as far as I can tell and therefore currently can't be used to directly reproduce the findings.

4. I believe the figures would still benefit from error bars or some other indication of the overall statistical error in the findings. I agree that the main contribution of this paper is theoretical, but since the experiments test the empirical validity of the theory, I believe it is nevertheless important to get a sense for the overall deviation in these findings (e.g. across model seeds). If the authors are concerned about a lack of clarity, they could leave the bars out of the main figures but add supplementary figures with error bars. Moreover, some of the lines in Fig. 1 do contain error bars and it would be good to clarify what these error bars represent.

Limitations:
The authors adequately discuss the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce accordion networks (AccNets), which are compositions of multiple shallow networks. By leveraging prior workthat computes norm-based generalization bounds for shallow two-layer networks, the authors bound the complexity of a deep AccNet (as measured by its F1 norm) but the sum of the complexities of the individual shallow networks. They empirically observe that the rates predicted on real-world data are roughly representative of the trained networks, and are indeed much better than those for kernels trained on the same tasks. They put forth a nontrivial scaling law for the excess risk: $N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{mid})}$ for an Acc Net compared to $\mathcal L \sim N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{in})}$ for a kernel in terms of the dimensionalities $d$ and Sobolev constants $\nu$ of the respective spaces and functions. From this, the authors obtain predictions of several phases, that they put forth experiments to verify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper tackles a very important open question in the theory of deep learning, for which not much progress has been made. By creatively leveraging results for shallow network in composition, the authors arrive at a nontrivial bound for deep nets. The empirics are a very compelling and welcome part of the paper. The phase diagrams illustrate the nontrivial predictivity of the theory, especially at the level of the rates. This may have important implications for scaling laws. Modulo minor revisions in discussion and exposition, the whole paper is quite readable for a relatively broad audience.

Weaknesses:
I am not sure how compelling the phase plots in Figure 2 are. The bounds in general are extremely loose, however the comparison of the rates in Figure 2c and Figure 3 is very promising. In general, however, it is the experience of the reviewer that measuring a rate is an extremely finicky business. It is therefore important to add a section in the appendix explicitly stating how the rates were obtained and measured. I also strongly encourage the authors to make the code for all figures public. 

Because they are used very early on throughout the paper, it is the opinion of the reviewer that the notions of F1 distance and Sobolev norm should be defined earlier on in the paper. Without this, it seems like the audience will be constrained to the set of learning theorists familiar with these terms. However, if these terms are defined early on, the paper becomes remarkably accessible to a much broader audience.

Limitations:
Given the theoretical nature of this work, it is unlikely to have major social implications.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
HBj86RMdZ8;"REVIEW 
Summary:
This paper focuses on the optimization and learning methods for ``online'' RLHF and contrastive offline methods (DPO, IPO). The authors aim to understand the separation between these two type of methods, where they are different in terms of whether new responses can be sampled or not. The authors state that the difference in the reward parameterization is key to the separation and propose global coverage and local coverage to capture such a difference. 

The theoretical insights also motivate a novel approach called Hybrid Preference Optimization (HyPO) that combines offline data with online samples, where the online samples are used to control the KL divergence. Empirical results are provided to verify the effectiveness of the proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors study the theory of RLHF under the KL-regularized target, which is more close to the practice, as compared to many previous works using the non-regularized reward;
- The notion of local coverage condition is novel in the literature and is very natural in the analysis of KL-regularized target. I appreciate the paper writing at the beginning of section 4, which is easy to follow and informative.  
- Building open the notions of local convergence and global convergence, the authors show a clear separation between the offline algorithms like DPO and online RL-based algorithms. This aligns with the recent observations that the online algorithms outperform their offline counterparts with a large margin. 
- This paper takes a step further to study the differences between the DPO and RLHF. While the original DPO paper states that the learning of DPO is equivalent to the RLHF, the empirical results do not align with this. The discussion related to the parametrization of reward for assumption 4.4 explains such a difference in practice. 
- Overall, I feel that the story of this paper is complete. Lemma 4.1 and the discussion  around assumption 4.4 clearly show that the offline algorithms can search for the policy with a large KL (possibly due to the parameterization of reward). This not only aligns with the separation between global coverage condition and the local coverage condition, but also motivates the practical algorithmic design to explicitly control the KL divergence.

Weaknesses:
This is not a weakness but some clarification on the terminology. In the literature of RL theory, particularly for the preference learning paper, the learning (online exploration) is mentioned with querying the human preference so as to learn the $r^*$. In the setup of this paper, the online data is only used to compute the KL loss without querying the human feedback. Therefore, it is more related to an intermediate setting that we use the offline preference dataset to construct a proxy reward function and we are allowed to query the model to get new responses but not the human feedback. See the RSO [1], some discussions in [2] for proposing multi-step RSO, and some discussions in [3]. 

[1] Statistical rejection sampling improves preference optimization.
[2] Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint
[3] Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data.

Then, it would be interesting to see the empirical results in terms of the ONLINE setting where we can query the new response and the new human preference signals because a lot recent works show that even with the DPO, the online framework outperforms the original one with a large margin. Moreover, this is standard in the PPO literature like Chat-GPT, Claude, and LLaMA2.

[4] Training language models to follow instructions with human feedback
[5] Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
[6] Llama 2: Open Foundation and Fine-Tuned Chat Models

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work considers the statistical separation between contrastive algorithms (DPO and IPO) and RLHF. It proves that DPO/IPO requires global convergence assumption which is in general a very strong assumption while on the other hand RLHF only requires local coverage. This separation stems from the explicit KL constraint in the objective of RLHF while the implicit reward in contrastive algorithms can results in unbounded KL. Given the observation, a hybrid (offline + online) algorithm is proposed: the preference optimization remains offline DPO however it draws online samples to enforce KL constraints on DPO policy, recovering the statistical guarantee of RLHF while attains the practical computational strengths (w/o value and reward models) of DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Well written and easy to follow; understanding DPO and RLHF is important towards better preference optimization algorithms.

- Good technical qualities and offer viable insights into DPO.

- The proposed hybrid algorithm recovers the statistical guarantee of RLHF while keeps the practical computational strengths of DPO (w/o using value and reward models).

Weaknesses:
I only have a couple of minor comments:

- Second sentence of L562 should be appended to L231 to make the proof sketch immediately clear.

- Theorem E.1 could be relocated to Section 5 to be more self-contained.

Limitations:
Overall, I am convinced this is a good submission with good technical quality. However, as I did not extensively follow the theoretical results on preference optimization, I am not able to comment on the novelty of this work, hence I give confidence score of 3. 

It would also be beneficial if the authors could create a table summarizing previous works and highlighting their contributions in comparison to prior research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on the paradigm of fine-tuning large language models (LLMs) using human preference data. It delves into two primary techniques: online reinforcement learning (RL) and offline contrastive methods. The authors challenge the previous notion of these techniques being equivalent by conducting a theoretical analysis through the lens of dataset coverage. They introduce the concepts of global and partial coverage conditions, proving that the former is necessary for offline methods using forward KL like DPO to converge to the optimal policy, while the latter is sufficient for online RL methods using reverse KL. The paper proposes a hybrid preference optimization (HyPO) algorithm, demonstrating its empirical superiority over DPO while maintaining some computational efficiency. The authors also provide a coverage-based explanation for why RL and offline contrastive methods might decrease the probability of preferred responses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper provides a rigorous mathematical analysis of the different conditions under which offline and online methods have provable performance guarantees, contributing to the theoretical foundation of preference learning in RL.

2. The authors introduce the HyPO algorithm and support their theoretical findings with empirical results, demonstrating the effectiveness of HyPO on the TL;DR dataset.

Weaknesses:
1. The experiment is not sufficient. Since it is done with Pythia 1.4B and the TL;DR dataset, it is unclear if the proposed method is still valid on larger models and other datasets. While theoretically interesting, it is unclear if using reverse KL instead of forward KL can lead to a significant performance gain in practice. 

2. The proposed HyPO method only uses online samples to calculate the KL loss rather than to collect new preference feedback. While simpler, it may fail to fully leverage the benefits of online methods.

Limitations:
The authors discussed the limitations well in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies learning from human preference feedback for aligning large language models (LLMs). Many existing works share the same observation that offline alignment methods such as DPO underperforms their online counterpart such as PPO. But this phenomenon has not been well understood. This paper studies this difference through the lens of coverage. It unveils that the KL constrain in offline alignment methods can be violated due to incomplete coverage and thus offline methods may find bad solutions. Inspired by this insight, the authors propose a new method, hybrid preference optimization (HyPO), which combines the offline preference update with the online KL regularization. Experiments on the summarization task demonstrate that HyPO is better than DPO. This paper also sheds light on a counterintuitive observation that DPO often decreases the likelihood of preferred responses through an illustrative example.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This work addresses an important matter and provides insights on a widely discussed question - why do offline alignment methods often underperform online methods? The paper is well written and easy to follow. The theoretical claims are sensible and easy to understand. I believe the community will benefit a lot from this work.

Weaknesses:
The computational cost of the proposed HyPO algorithm is not sufficiently discussed. Usually the most computational cost in online alignment methods comes from the sampling process of LLMs. In practice, the computational cost plays an important role in comparing algorithms. Thus it will provide useful guidance if the computational cost is well discussed in the paper.

The discussion in Section 6 is bit handwavy. While Example 6.1 provides good intuition, a more rigorous analysis is needed to justify the claim.

One minor suggestion on presentation: It will be good to provide the PPO numbers in Table 1 to help the readers compare HyPO to PPO more easily.

Limitations:
This paper includes a fairly thorough discussion on the limitations at the end. I would like to add one point: the analysis in this work only focuses on the solution space. It explains why offline methods may find bad solutions, but doesn't explain why they often find the bad ones in practice. As far as I know, people often use a very low learning rate for DPO in practice. For example, the original paper used 1E-6. One possible reason for such low learning rates is that it compensates the ineffective KL regularization. Despite such efforts, DPO still finds bad solutions. It will be insightful if we get a better understanding of why bad solutions seem inevitable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
cA9gLXFaRo;"REVIEW 
Summary:
In this paper, the author introduce Instruction-guided Visual Masking (IVM), a generic and powerful visual grounding method that enhances broad multimodal instruction following tasks in a plug-and-play way. By masking out all instruction-irrelevant image regions, IVM effectively injects superior visual grounding ability to downstream LMMs non-intrusively, significantly boosting both commercial and open-sourced LMMs and achieving state-of-the-art results across numerous challenging multimodal benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the motivation of this paper is commendable, as it addresses an interesting problem and provides rich illustrations that facilitate understanding. Furthermore, the work is supported by a substantial number of experiments to validate its effectiveness.

Weaknesses:
1. The routine of retraining models by preparing new datasets, as described in this paper, can be considered somewhat old-fashioned. 
2. The technical insight of this paper is relatively weak, and no strong novelty technical methods are proposed.

Limitations:
see weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Instruction-guided Visual Masking (IVM), a versatile visual grounding model designed to improve alignment between textual instructions and specific image regions. It outlines the development of a visual masking data generation pipeline and a new learning technique, Discriminator Weighted Supervised Learning (DWSL), which prioritizes high-quality data samples to enhance performance on multimodal tasks.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper introduces instruction-guided visual masking (IVM). The IVM-enhanced multimodal models can focus on task-relevant image regions to better align with complex instructions. This implies that the model can become more sensitive to instructions.
2. Figures 2 and 3 are helpful to understand the method.
3. This paper has collected a richer and more complex visual grounding dataset.

Weaknesses:
1. The IVM model architecture shown in Figure 6 is not conducive to understanding the approach.
2. It is recommended to compare more methods on a multimodal benchmark.

Limitations:
Limitations and broader impact have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents IVM (Instruction-guided Visual Masking). The key idea is that we could mask out the instruction-irrelevant regions in the given image. The trained model is tasked to mask out the irrelevant regions, enforcing the multimodal model to focus on the task-related visuals. Such grounding-centric approach is effective in enhancing different multimodal models.

The paper also details their solution to create a large number of reliable pixel-level labels. The paper presents a MoE pipeline with various visual grounding models to collect reliable labels.

The paper further introduces DWSL for IVM training. DWSL helps IVM training to focus more on higher quality training data.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The proposed IVM is sound and effective. I found the problem formulation of generating heatmap interesting. The experiments clearly demonstrate the effectiveness of the proposed method.
- The MoE pipeline is well-developed and can be applied to both labeled and unlabeled data.
- The IVM architecture with discriminator training can effectively reduce the impact of low-quality data.
- The paper is well-written and solid.

Weaknesses:
I don't find significant problems in the paper. One possible improvement for this paper is that it would be interesting to provide some typical failure cases of the proposed method.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
For the purpose of precise instruction following performance in LLM, this paper proposes a versatile grounding model that is compatible with diverse multi-modal models. Leveraging the LLM, a visual masking data generation pipeline is built and 1 million image-instruction pairs are constructed. On top of it, an Instruction-Guided Visual Masking (IVM) model is trained to focus on task-relevant regions that align with complex instructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. An IVM model is proposed to enhance multimodal instruction following via nuanced surgical visual grounding. Overall, this model is simple yet effective. Such a model can be seamlessly incorporated into a multimodal model to boost the performance of downstream tasks.
2. A dataset creation pipeline with a mixture of experts is carefully devised and along with it an 1-M dataset is built.
3. To effectively utilize the dataset, a discriminative weighted supervised learning training strategy is devised to select the high-quality dataset pairs.
4. Extensive experiments have been conducted to validate the effectiveness of the proposed approaches on various tasks, e.g. visual understanding, reasoning segmentation model and real robot control.

Weaknesses:
1. This work proposes a visual grounding model. However, it seems that the comparison with state-of-art visual grounding methods (VG) is missing. I understand this task involves slight differences with reasoning segmentation (RS) tasks. But will the VG task evaluation be more direct?
2. This approach relies heavily on the human manually labeled data, without which the performance will significantly drop. Therefore, it is more like a dataset creation work. Finetuning on this dataset, other compared works might also achieve similar or even better performance.
3. The derived framework is a little bit heavy, with LLM and heavy visual rendering head involved . There might be a more efficient network framework to achieve similar performance.

Limitations:
Authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
fu0xdh4aEJ;"REVIEW 
Summary:
This paper investigates the sample efficiency problem in continuous control. The authors propose the BRO algorithm, i.e., Bigger, Regularized, Optimistic. The authors find that strong regularization allows for effective scaling of the critic networks, which, paired with optimistic exploration, leads to quite good performance. BRO achieves strong performance on numerous continuous control benchmarks, and is the first model-free reinforcement learning algorithm that can learn meaningful performance in DMC dog and humanoid tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- this paper is easy to follow and easy to understand

- the studied topic is important to the RL community. It is vital to develop stronger and more powerful model-free RL algorithms for continuous control problems

- despite that this work combines numerous previous well-developed tricks and strategies, the authors selectively incorporate them into one framework and demonstrate that such design choice incurs quite good performance.

- the experiments are extensive and solid

- the developed BRO algorithm is the first model-free RL algorithm that can achieve meaningful performance in DMC dog and humanoid tasks

Weaknesses:
this paper has the following drawbacks,

- the quality of the figures could be significantly improved. Please try to export pdf with matplotlib instead of taking screenshots by convention.

- BRONet seems to be a simplified version of ResNet. I do not seem to observe any significant network architecture difference between them. The authors should not over-claim on the network architecture. Any clarifications here?

- It is often unclear how the figures are plotted and which environments they cover, e.g., Figure 4, Figure 6. This should not be vague and ought to be clearly stated in the main text

- I am a bit concerned with the claim that *Algorithmic improvements matter less as the scale increases* (Line 184). Do you think that this is always correct? One should focus more on scaling instead of algorithmic improvements in the context of RL?

- missing baselines and references. The authors should compare against some other recent strong model-free RL algorithm, e.g., TD7 [1]. Meanwhile, the authors should cite the REDQ [2] paper when referring to replay ratios. REDQ should be included as a baseline approach in the paper. Moreover, a recent paper introduces sample multiple reuse (SMR) [3] that updates a fixed batch multiple times to boost sample efficiency. I think this can be a very relevant work and should be included and discussed in the paper. Also, the authors write that they introduce another actor network, and incorporate regularization techniques into critics, it reminds me of another work, DARC [4], where they leverage double actors for exploration and introduce critic regularization for better performance. It should be included in the paper.

[1] For sale: State-action representation learning for deep reinforcement learning

[2] Randomized Ensembled Double Q-learning: Learning Fast Without a Model

[3] Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via Sample Multiple Reuse

[4] Efficient Continuous Control with Double Actors and Regularized Critics

Despite the aforementioned drawbacks, this is a solid paper that may have a potentially large impact on the RL community. I would be happy to reconsider the score if the aforementioned flaws are addressed during the rebuttal phase.

Limitations:
The authors have a good discussion of the potential limitations of this work in the main text.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates how reinforcement learning (RL) can benefit from parameter scaling. The authors introduce BroNet, a variant of ResNet with LayerNorm, as a well-regularized network structure for the critic that improves performance when scaled. They also find that when the critic is properly regularized, the common Clipped Double Q-learning trick can be replaced with optimistic exploration, further boosting sample efficiency. These findings are combined into a new algorithm called BRO.

To demonstrate the efficiency of their approach, the authors conduct extensive experiments on 40 challenging tasks across 3 benchmark suites. They also provide comprehensive ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper addresses the important and timely topic of scaling in model-free off-policy RL.
- It offers extensive large-scale studies on various design choices, providing valuable insights to the RL community.
- The proposed BRO algorithm shows promising results across a wide range of tasks.

Weaknesses:
As acknowledged by the authors, the study primarily focuses on state-based off-policy RL. The transferability of their conclusions to other domains such as image-based problems and offline RL remains unclear, which limits the paper's broader impact.

Limitations:
Yes, the authors have discussed the limitation in section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies how to scale up RL algorithms in the continuous domain and introduces the BRO (Bigger, Regularized, Optimistic) algorithm, designed to enhance sample efficiency with (relatively) large models. The authors conduct extensive experiments to verify the effectiveness of factors like replay ratio, regularizations, optimistic exploration, and quantile Q-values when scaling up RL algorithms. The findings from these extensive experiments lead to the novel BRO algorithm, which consists of a novel architecture with proper regularization and exploration. Empirical results demonstrate that BRO achieves state-of-the-art performance on 40 complex tasks from the DeepMind Control, MetaWorld, and MyoSuite benchmarks, outperforming leading model-based and model-free algorithms, especially in the challenging Dog and Humanoid tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. This paper tackles an important problem of scaling up in reinforcement learning, especially in continuous action domains.

2. The authors conduct extensive experiments on the effects of different methods on scaling up, which I found very informative.

3. The proposed algorithm, BRO, achieves strong empirical performances on various domains, especially on the Dog & Humanoid domains.

4. The paper is well-structured and well-written.

Weaknesses:
Usually, scaling up benefits more when a large amount of data is available, where large models can lead to positive transfer or generalization across different tasks. However, the current setup is the same as the standard setting, where the agent is trained for 1M steps on each task separately. The work would be more significant if the model is trained on and can be transfered across different tasks.

Limitations:
The authors discussed their limitations well in the paper.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
1PCsDNG6Jg;"REVIEW 
Summary:
Replicability is a notion of stability for learning algorithms, recently proposed by Impagliazzo et al. [ILPS22] to address the replicability crisis pervasive in scientific studies using statistical procedures. A learning algorithm $A$ is a function that takes as inputs a dataset $S \in (\mathcal{X} \times \mathcal{Y})^*$ and a (random) string $r \in \\{0,1\\}^*$, and outputs a hypothesis $f: \mathcal{X} \to \mathcal{Y}$. The algorithm is replicable if for independent draws of same sized datasets $S, S’$ and random $r$, $A(S, r) = A(S’, r)$ with high probability. Put simply, if two scientific labs use the same replicable algorithm to analyze independent datasets S and S’, and arrive at different conclusions, they will have a hard time blaming statistical fluctuations in the data.

This paper presents a variety of results that connect replicability to other well-studied concepts in learning theory, such as (realizable) online learnability and private learning. In particular, the authors show a computational separation between replicability and **online learnability**, assuming the existence of one-way functions. They also present a lifting procedure that transforms an efficient replicable PAC learner for a hypothesis class $\mathcal{H}$ over the uniform distribution on $\\{\pm 1\\}^n$ to replicable learners over any marginal distribution whose sample and time complexity depends on the complexity of the marginal. This lifting procedure is then used to design an efficient replicable algorithms for **learning parities over distributions that are far from uniform**. Furthermore, they show that any **pure DP learner** can be turned into a replicable one in time polynomial in all relevant problem parameters, except for the ""representation dimension"" of the hypothesis class.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very well-written paper with solid technical contributions. Though the paper touches on various concepts in learning theory, the polished presentation ensures that the reader is not overwhelmed by the breadth of coverage. As replicability is a relatively new concept in learning theory, I was not familiar with it. However, the connections between replicability and other concepts are rather surprising and pleasing to someone new to the topic. The clear and well-organized proofs, each employing diverse techniques, were relatively easy to follow.

I found the application of the lifting procedure to replicable parity learning and the distinction from Gaussian elimination particularly interesting. For the uniform distribution, it is straightforward to see that Gaussian elimination is replicable for data labeled by parity functions. However, one can define simple distributions on which Gaussian elimination is *not* replicable (though the zero-one loss *is* small with high probability). This highlights the necessity and the non-triviality of the lifting procedure for efficient replicable learners over the uniform marginal. It also shows that computational separation between replicable learning and statistical query (SQ) learning extends to non-uniform marginal distributions.

Weaknesses:
Given that this is a solid and well-polished paper, I did not find any significant weaknesses, only a few minor points below.

- **Elaborating replicability with simplified, realistic statistical examples.** The paper begins by addressing the replicability crisis in scientific fields, but once the authors define replicability within the learning theory setup, the initial narrative gets lost. What would the random strings model in a real-world scientific study? Does it model the PRG seed number that experimenters use in their PyTorch code? Taking a simplified but realistic example (e.g., a hypothetical FDA approval procedure based on clinical data collected by independent labs), and mapping the A's, S's and r's to real-world concepts would be helpful.

- **Ambiguity in the big questions.** I found some of the ""big"" questions Q1-Q4 [page 2-3] too generic to be useful. In particular, questions like ""How does replicable learning *relate to* online/private learning?"" are extremely underspecified because ""relate to"" can have multiple interpretations. It would have been more helpful if the authors posed more specific motivating questions, such as ""Is there a computational separation between replicably learnable classes and online learnable classes?""

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In the paper, the authors discuss the connection and differences among replicability, a novel stability condition for learning algorithms proposed by Impagliazzo et al. [2022], online learning, and differential privacy from a computational perspective. Their first contribution is a computational separation between online learning and replicable PAC learning. In particular, under standard cryptographic assumptions, there is a concept class that can be replicably learned, but no efficient online learning algorithm exists. The second contribution is a method to extend a replicable PAC learner that works under the uniform distribution to ones that work under more complex distributions, whose probability mass functions can be computed by decision trees. The final result is a way to transform a purely differentially private learner into a replicable one. Combining these with some existence hardness/equivalence results provides a figure (Figure 1.1) referred to by the authors as the ""computational landscape of stability.""

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Figure 1.1 is an excellent summary of all known relationships between different notions of algorithmic stability. I believe the communities behind all three areas (replicable, online, DP) could benefit from such a roadmap. All the results are clean and well-motivated. 
The one I like the most is the lifting result for replicable learner. Intuitively, it feels like replicable learners and statistical query algorithms are almost equivalent as most replicable algorithms we know are based on the idea of making each statistical query fired by the algorithm replicable. The only exception known before is learning parities under the uniform distribution over boolean hypercube. The authors demonstrate the possibility that there may potentially be much larger gaps between SQ algorithms and replicable learners.

Weaknesses:
While the results are conceptually novel and interesting, the techniques used are more or less standard. For example, the lifting result follows from building a replicable version of the routine from Blanc et al. [2023] for learning the decision tree structure of the distributions, and the core of it is just to estimate the influence of distributions in a replicable manner (via random statistical query rounding).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the computational relationship between *replicable* learning algorithms, a recent notion of algorithmic stability [ILPS22] promising two runs of the algorithm over independent data output the same hypothesis with high probability assuming shared randomness, and other classical notions of algorithmic stability such as SQ-learning and differential privacy (as well as the closely related topic of online learning). The equivalences between such notions are well understood statistically, but this is the first paper to make a systematic attempt to study *computational* aspects of the problem.

More formally, the authors present both negative results (separations), and positive results (efficient transformations) between these notions:

On the negative side, the authors show:

1. A concept class based on one-way sequences which has no efficient online learner but can be learned replicably in polynomial time. This result is based on work of Bun separating private and online learning, but requires a new idea on the replicable side. In particular, two independent runs of the algorithm (with fresh samples) must identify a shared early element in the one-way sequence (using their shared randomness) to output the same unique hypothesis on the remainder. This is done via a new replicable quantile estimation lemma that leverages a variant of [ILPS22]'s randomized rounding methods.

2. A separation between SQ learning and replicable learning based on affine parities. While this class was already known to separate the two under the uniform distribution, no separation (or even efficient replicable algorithm) was known for more general distributions. Based on work of Blanc, Lange, Malik, and Tan, the authors give a general procedure to lift an efficient replicable algorithm for the uniform distribution for certain classes to a replicable algorithm for general distributions whose computational efficiency scales with the decision tree complexity. They then give examples of distributions with low decision tree complexity where the trivial `gaussian elimination’ based algorithm for the uniform case fails but their lifted algorithm runs in polynomial time (they also observe these distributions remain hard for SQ). This makes progress on a question of ILPS'22 about replicable algorithms where Gaussian elimination fails.

On the positive side:

1. There is an algorithm transforming any pure private learner for a class C to a $rho$-replicable learner in time exp(rep-dim(C))*poly(eps^{-1},\rho^{-1}), where eps is the accuracy parameter and rho is the replicability parameter. Their algorithm uses a classic procedure of Beimel Nissim and Stemmer which generates a finite approximation of C using a pure private learning, then learn this representation with the finite learner of [BGH+23]. To get polynomial dependence, this is run for constant error then boosted afterwords using replicable boosting.

Soundness:
1: poor

Presentation:
3: good

Contribution:
3: good

Strengths:
Replicability is a critical notion in machine learning and the sciences in general, and has recently been a fruitful notion more generally in the study of algorithmic stability, leading e.g. to advances in differential privacy. Prior works in the area largely focus on the statistical complexity of replicable learning. Understanding the *computational* cost of replicability, both in general and as compared to other notions of stability, is a critical open problem and clearly of interest to the NeurIPS community. This paper is the first to initiate a systematic study of this problem, and makes initial progress on understanding connections with pure privacy, SQ, and online learning, standard notions in the literature.

Related to the above, the authors make progress on an open problem of [ILPS’22] to design efficient replicable algorithms for parities beyond the uniform distribution (namely in this case for distributions with constant decision tree complexity). This takes some work to formalize and is a reasonable contribution on its own from a computational standpoint.

Weaknesses:
The work has two main weaknesses.

First, the authors seem to have misunderstood prior separation results in the literature, and as a result, the presentation of `computational separations’ with respect to privacy in the paper (namely in Figure 1.1 and the exposition) is wrong. Namely, the authors claim that there is an “efficient transform from pure DP learning to replicable learning” and “no efficient transform from apx DP learning to replicable learning”, where “efficient” is in terms of eps and rho (error and replicability) but not the underlying dimension of the problem, but this seems false.

In particular, there actually *is* a transformation from approximate DP to replicability that is “efficient” in this sense. Correlated sampling can be run in time roughly scaling with the output domain of the private algorithm, then boosted via ILPS from constant accuracy/replicability in polynomial time in these parameters. In fact, *any* learning problem that is solvable replicably can be solved in time polynomial in eps and rho by boosting/amplifying, so “efficiency” in these parameters is not very meaningful. The question of efficiency instead should be one of domain-size/dimension, which (as the authors to their credit highlight several times) is not efficient in the given reduction.

Part of the confusion here seems to stem from the result of [BGH+23] giving a computational separation between apx DP and replicability. In Prop 5.1, the authors state [BGH+23] exhibit a PAC problem which is efficiently learnable under apx-DP, but cannot be replicable learned assuming one-way functions exist. As far as I can tell, this is not shown in [BGH+23]. First, the separation given by [BGH+23] is not for PAC-learning, it is for a somewhat contrived statistical task; separating the two in the PAC setting is open. Second, the separation has nothing to do with one way functions (which seems to be a different result in their paper), and relies on public key encryption. Third, the separation is in terms of the dimension/size of the space, not accuracy or replicability parameters.

The second weakness of this paper is that, while extra technical work is certainly required for several of the results in this paper (namely the online bound, and generalizing BLMT23), the ideas in this paper do not go substantially beyond known methods in the study of replicability. The online vs replicability result is not too much of a jump from its use in work of Bun separating privacy and online learning, (the new replicable quantile estimation method takes work but is fairly straightforward from techniques in ILPS22). The SQ/distribution-lifting result also follows largely from combining techniques of [ILPS22] with [BLMT23]’s non-replicable method which already relies mostly on statistical estimation sub-routines.

Overall, the authors have identified an important problem and made some nice partial progress in this front (including progress on an open problem of ILPS22), but combined with the issues above and without introducing substantially new ideas to the study of replicability I cannot recommend the work for acceptance in its current form.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work contributes to the recently evolving area of replicable learning. This work establishes three main results

1. It is known that online learning algorithms can be replicable and replicable learning algorithms yield online learning algorithms. The work focuses on the computational complexity of these transformations and establishes a negative result--there exist concept classes that are efficiently, replicably, and learnable but there are no efficient online learning algorithms (assuming the existence of one-way functions).
2. Recent work showed that PAC learning algorithms under the uniform distribution can be black-box converted into distribution-free PAC learning algorithms. This work shows the transformation can be made replicable.
3. It is known there exist concept classes that are approximately DP learnable, but not efficiently, replicable learnable.  This work shows that if the concept class is pure DP-learnable, then it is efficiently replicable learnable.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This solid work clarifies several questions on replicable learning in the context of computational efficiency. The paper is very well written and the proofs are rigorous.

Weaknesses:
Main weakness is that obtained by combining known works. It is not clear to be that there is not that much novelty in the proofs.  for example, 

1. Proof of Theorem  2.1 proceeds in two parts.  a) existence of a concept class that is not online learnable, b) designing a replicable learning algorithm for this class. (a) is known by prior works, (b) replicable algorithm follows by the (now) standard technique of random thresholding/rounding. 
2. Please see Q2. 

Though the proofs may not be novel, the final results that the authors obtain are interesting.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
SKCbZR8Pyd;"REVIEW 
Summary:
This paper proposes to apply preference optimization techniques (which have proven useful in aligning text language models’ outputs to users’ preferences) to speech language models that generate sequences of discrete audio representations and then speech. The particularity of the preference dataset is that it does not rely on truly human preferences but is simply made up of audio (AR) tokens obtained from natural speech and synthetic speech (the ones coming from natural speech being the preferred ones). 

Different preference optimization (PO) strategies are investigated: RLHF-PPO, DPO, etc and human evaluation is conducted to evaluate the synthetic speeches obtained. DPO seems to be the most performant PO method. An iterative process where the updated speech generation model is used to create a new and more challenging preference dataset (with synthetic / natural speech pairs) further improve the speech generation quality.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The integration of human feedback to align speech outputs to human preferences is a new topic addressed in this paper with convincing results on speech generation.

Weaknesses:
-the preliminary analysis on ‘distribution gap’ is interesting in itself but it is not clear how it really relates with SpeechAlign (in other words: why SpeechAlign solves this distribution gap observed earlier)

-the way Best-of-N Sampling (BoN) is presented is confusing: it is at the same level of PO methods but while reading its description it looks more like  a decoding approach than a model alignement approach => ??

Limitations:
-I don't see so many limitations to this paper

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method to improve speech generation in a speech language model via preference optimization. The method relies on creating a dataset of ""gold"" speech tokens produced by a neural codec model from a speech sample, contrasted with synthetic tokens produced by a speech generating model from text. The model is then trained to preferably generate tokens closer to the ""gold"" ones, via a number of  preference optimization methods, and evaluated using both automatic metrics (WER and speaker similarity) and human preference judgments. The results show that the model can iteratively improve its performance based on the preference dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The method is simple yet non-obvious.

The existence and impact of the distribution mismatch between gold and synthetic tokens is demonstrated via preliminary analysis and experiments.

A number of different preference optimization algorithms are tested.

The evaluation is carried out on two datasets, and accompanied by a thorough analysis and several ablations.

Weaknesses:
The description of the preference optimization algorithms (COH, DPO, RLHF-PPO) is hard to understand without already being familiar with the relevant papers. 

The results are presented without showing their variance, even though the underlying data should be available as the paper mentions evaluating each  model 10 times. It would be good to have the spread of the scores available in addition to the mean.

Limitations:
No discussion of limitations within the body of the paper.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study analyzes the training-inference mismatch that occurs in codec language models, a branch of personalized speech synthesis research, and mitigates it through preference optimization methods. By avoiding the labor-intensive process of collecting human preference test results, the researchers efficiently gathered data and used it to further fine-tune the model, achieving improved results in personalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Preference optimization has not yet been sufficiently explored in personalized speech synthesis, and this study demonstrates its effectiveness for this purpose.
2. They conducted thorough observations and analyses of the problem they defined.
3. The illustrations and figures added to the paper aid in comprehension, and the writing is clear and easy to understand.
4. They evaluated various components and details they used within the paper, effectively demonstrating the impact of each component.

Weaknesses:
I believe the evaluation with other baselines might be insufficient. I have included additional evaluation questions below for consideration.

Limitations:
They addressed their limitations in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ""SpeechAlign,"" a method aimed at improving text-to-speech (TTS) performance by aligning speech generation with human preferences. It addresses the distribution mismatch between ground truth AR tokens and predicted AR tokens in neural codec language models. The proposed method involves preference optimization and iterative training, which has shown to enhance the performance of TTS systems.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* Insightful Analysis: The paper provides a valuable insight into the distribution mismatch between ground truth AR tokens and predicted AR tokens, identifying it as a key issue affecting TTS performance.
* Effective Methodology: The proposed preference optimization and iterative training strategies are well-reasoned and demonstrate a clear improvement in TTS performance.
* Comprehensive Evaluation: The experimental results are extensive and include both subjective and objective evaluations, providing strong evidence for the effectiveness of the proposed method.

Weaknesses:
* Performance Gap: Despite the improvements, the performance of the proposed method still falls short of the ground truth.
* Limited Scope: The method is specifically tailored to the VALL-E based TTS model and does not present a general framework for other types of speech language models.

Limitations:
* The method is more like a specific method to alleviate the domain mismatch or error propagation of the pipeline codec-based TTS model.
* The method relies on ground truth tokens as the chosen samples in the preference data, potentially overlooking other high-quality token sequences that can also serve as the chosen samples.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
21tn63ee15;"REVIEW 
Summary:
This work focuses on the task of ""worldwide geolocalization"" with an effective and adaptive framework based on large multi-Modality models. A novel framework, i.e., G3, is proposed, including Geo-alignment, Geo-diversification, and Geo-verification. This work also releases a new dataset MP16-Pro. The experiment results show that G3 has superior performance on two well-established datasets IM2GPS3k and YFCC4K.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The task of ""worldwide geolocalization"" is very important and quite interesting.
2. This paper is easy to follow, it is well-written.
3. The experiment results are solid. The G3 model achieve better results than GeoCLIP and Img2Loc on IM2GPS3k and YFCC4K.

Weaknesses:
1. In the Geo-diversification part, there is no ablation study on different LMMs and different RAG templates. I wonder whether it still works on open-source LLMs.
2. Minor:

    a. Figure 1 is often set as a teaser figure, which could show the basic design of the whole work. It would be better to indicate the solution, instead of only showing the limitations.

    b. Table 1 needs citations for each previous work. And GeoCLIP should be noted as NeurIPS 2023 instead of arXiv.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes three steps, i.e., geo-alignment, geo-diversification, and geo-verification to optimize both retrieval and generation phases of word-wide geo-localization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clearly stated.
2. The experimental results show the effectiveness of the proposed method.
3. The proposed method achieves state-of-the-art performance.
4. Code is publicly available.

Weaknesses:
The experiments are not insufficient. 
The model seems too large, so the author should provide the number of parameters and gflops experiments.

Limitations:
The authors discuss the limitation in the paper, but not in enough depth.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In the paper, authors propose a novel framework, G3, for worldwide geolocalization of a given photograph anywhere on Earth. The authors address the challenges of capturing location-specific visual cues and handling variations in image data distribution across the globe. G3 utilizes a three-step process: Geo-alignment, which learns location-aware image representations, Geo-diversification, which employs multiple retrieval-augmented prompts for robust location prediction, and Geo-verification, which combines retrieved and generated location data for final prediction. The authors also introduce the MP16-Pro dataset to support location-aware visual representation learning. Experiments on the IM2GPS3k and YFCC4K datasets demonstrate the superiority of G3 over existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* All the modules in the G3 framework: Geo Alignment, Geo Diversification and Geo Verification seem logical and rational. Three kinds of embedding coming from the vision encoder are used for retrieval. LLM is used to generate a set of plausible coordinates by providing positive and negative examples.
* The method achieves superior performance over several baselines at various levels of granularity on IM2GPS3k and YFCC4K.
* Overall, the method is interesting and novel, the writing and flow of the paper is meaningful.

Weaknesses:
* The only limitation discussed is regarding the efficiency of inference. However, there is no mention of how much compute time and memory (in number) is required to geo-localize a given input image. 
* There are no concrete qualitative example of failures reported in the paper. Can the system be fooled easily? For example, if an image from Italy contains a human with a flag of The Netherlands, is the system capable of correctly geolocalizing the image? How does the RAG system along with the LLM perform in such a case?
* Limited evaluation considering the state-of-the-art. No mention of recent works such as Pigeon [1] or a GeoReasoner [2].
* Why did the authors choose to use CLIP vision encoder for extracting image features? Recent works have shown that purely image-based pretrained models such as DINO-v2 are better feature extractors than CLIP. No ablation study done for the choice of the vision encoder.
* Overall, from discussion in **L316-L333** and Figure 4, it looks like the number of references provided to LLM highly depends and varies based on the image content. The performance is highly sensitive to this hyperparameter and a single value cannot guarantee optimal performance. This can make the framework highly unreliable for practical use cases.

[1] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024

[2] Li, Ling, Yu Ye, Bingchuan Jiang, and Wei Zeng. ""GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model."" In Forty-first International Conference on Machine Learning.

Limitations:
Limitations are included but failure cases are missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces G3, a RAG framework for geo-localization. By introducing a three-step process, the G3 framework achieves superior performance against other SoTA methods. To improve the expressiveness of the image embeddings, the paper proposed a new dataset, MP16-Pro, which adds textual descriptions to the existing MP-16 dataset. By comprehensive experiments, the paper demonstrates the necessity and merits of the G3 framework.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed method, G3, achieved competitive performance in geo-localization against existing classification-based, retrieval-based and RAG-based methods.
2.	Compared to the original MP16 dataset, the proposed MP16-Pro dataset additionally provides textual geolocation-related data, which could be beneficial to the community if fully open-sourced.
3.	Comprehensive experiments and explanations are provided to prove the effectiveness and necessity of each G3 component.
4.	The authors have open-sourced their project in a clear and instructive manner, which is very positive for reproduction.

Weaknesses:
1.	The necessity of the Geo-alignment module in the G3 framework has been indicated by experiments results in Table 2. However, the authors did not address the motivation for their particular design choice of the alignment module. Why do the image features have to align with both text features and gps features? Does aligning with simply one modality work just as well? The authors should clarify that by conducting the corresponding ablation study.
2.	One of the main contributions claimed by the authors is the introduction of the MP16-Pro dataset. However, the description of the construction process for the MP16-Pro dataset is not sufficiently detailed.
3.	The G3 framework was not the first work to incorporate RAG into geolocalization, nor was it the first work to use retrieval-based models. While the proposed method achieved superior performance, it lacks certain novelty to the field.
4.	In the experiment setup in section 5(line 226), no retrieved coordinate is considered when evaluating G3 on IM2GPS3K, which is inconsistent to figure 2 of the paper. If no retrieved coordinate works better in certain cases, I wonder about the necessity and applicability of such design.

Limitations:
The authors of this paper have addressed the limitation of the G3 framework by pointing out its high computation cost. The introduction of alignment and diversification brings on more computation cost and latency compare to existing methods, which limits the retrieval and inference speed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a RAG-based framework for worldwide geo-localization. The first Geo-alignment stage projects input images to embedding spaces to align with GPS coordinates, and text description with contrastive learning. Given new input image, the system is able to retrieve similar GPS and text description. Then the retrieved candidates GPS and text prompts are fed to GPT4V with a pre-defined prompt template to generate GPS coordination. The final stage conducts a similarity-based verification based on multi-modal representations. The method is evaluated on two worldwide geo-localization datasets, i.e., IM2GPS3k and YFCC4K, with state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The RAG-design for geo-localization is interesting and promising.
+ The writing is easy to follow.
+ The performance is much better than previous methods. 
+ Ablation result is provided for the three stages. 
+ The case study and failure cases are informative and interesting.

Weaknesses:
- My major concern is that the current pipeline is highly dependent on the LMM which is the powerful closed-source model, GPT-4V. This model is expensive for large-scale applications and also hard to reproduce due to unannounced updates for API across time. It would be better to provide the results with open-source large multi-modal models, for example, LLaVA. I would expect a lower accuracy with open-source models. 
- The proposed MP16-Pro dataset is also claimed as a contribution, but there is no guarantee that the data will be released. Hope this will be provided in final version.

Limitations:
The limitation is included in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
bJddXCyosA;"REVIEW 
Summary:
The paper introduces VisMin, a new benchmark for assessing fine-grained understanding in VLMs. VisMin evaluates the ability of models to distinguish between minimally different images given a caption, testing recognition of changes in objects, attributes, counts, and spatial relationships. The benchmark is created using an automated pipeline and human verification. Empirical results show that current VLMs have notable deficiencies in spatial relationships and counting. Fine-tuning models like CLIP and Idefics2 with a large-scale dataset generated by this pipeline leads to improvements in fine-grained understanding and image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This paper introduces VisMin, a benchmark that challenges models to detect subtle semantic differences between similar images, revealing significant shortcomings in VLMs and MLLMs.
2.This paper uses minimal-change image-text data to finetune VLM and MLLM models, improving the fine-grained understanding capabilities.
3.Table 2 illustrates that current open-source MLLMs perform poorly in fine-grained image understanding. This paper suggests using minimal changes in images to enhance fine-grained image understanding, and I believe this idea is promising.

Weaknesses:
1.After fine-tuning on VisMin data, MLLMs showed improved performance on fine-grained understanding benchmarks. However, their performance on POPE and MMMU decreased, which contradicts the authors' claim of improved text-image alignment in MLLMs. The authors should provide results on additional benchmarks such as TextVQA, MathVista, and MMBench.
2.The authors attribute the poor performance to the binary-choice task and limited computational resources preventing training of an 8B model. This explanation seems insufficient; given the computational constraints, the authors could reduce the model size, such as using Qwen-1.8B, to validate the method's effectiveness.
3.For VLMs, the authors should also provide some zero-shot image classification results, such as on ImageNet. I am curious to know if fine-tuning on VisMin data affects zero-shot classification performance.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the fine-grained understanding of objects, attributes, and relationships between objects for VLMs and MLLMs. Specifically, it focuses on the capability of VLMs and MLLMs to distinguish between two very similar images given a caption. Firstly, by leveraging an image generation diffusion model and an LLM, this paper introduces a new benchmark termed Visual Minimal-Change Understanding, which requires models to predict the correct image-caption match given two images and two captions. Secondly, it performs evaluations on multiple VLMs and MLLMs and analyzes the performance in terms of different types of changes: object, attribute, count, and spatial relation. Thirdly, it fine-tunes existing VLMs and MLLMs on the generated datasets and shows improvement in fine-grained understanding.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The motivation is clear, and the problem addressed in this paper is well-explained. 
- The proposed dataset is interesting and benefits the research of fine-grained understanding of foundational VLMs. 
- This paper is well-written and easy to follow. 
- The evaluation protocol and the methods for fine-tuning are technically sound. 
- The analysis of VLM and MLLM performance on the proposed benchmark is interesting.

Weaknesses:
- Some notations are confusing, and implementation details for the evaluation are missing: how are two captions chosen in text scores for VLMs? It appears from line 247 that captions chosen from a single image are paired instead of being randomly sampled. What's the difference between C_0 and C_1 in lines 247 and 248? The notation here is a bit confusing. Do the notations C in line 247 and T in line 294 both refer to the captions for the images? What's the difference? 


- Implementation details for fine-tuning are missing: How do the authors construct the VisMin Instruct-IT dataset? In line 344, some rule-based approaches are applied, but the reviewer cannot find details of these rule-based methods. It would be great to show some examples of how VisMin is converted to VisMin Instruct-IT. It also appears that the authors fine-tune the model using QALora and instruction fine-tuning techniques. How many instruction-image pairs are in this dataset? 


- Insufficient dataset analysis: The authors mention in this paper that after human filtering of the proposed dataset, there is still some noise, such as deformation and mismatches. Conducting a human evaluation on the proposed dataset to determine the percentage of these noise data would be great. This would provide a valuable reference for future works using this dataset.

Limitations:
- The authors discuss limitations and potential solutions in Section 7, such as the possibility of noisy data being included in the proposed dataset. The reviewer suggests conducting human evaluations on some samples from the dataset to provide statistics on the noisy data and to offer visualizations of these noisy data samples. 
- The social impact should be discussed in this paper. This paper propose fine-tuned VLMs and MLLMs which are able to detect visual minimal-change and can have potential social impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a new benchmark, VisMin, which mainly challenges models to detect semantic differences between visually similar but semantically different images. It uses an automated data curation pipeline and human verification to create dataset. The authors benchmark the dataset with current VLMs and MLLMs, showing that the proposed dataset can help improve image-text alignment and overall performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The motivation of the paper makes sense and the paper writing is clear.
2. The curated dataset contains hard negative samples that will be beneficial to improve fine-grained understanding of current models. The dataset curation pipeline is well designed and the data quality looks quite good.
3. It indicates huge performance on CLIP and Idefics2 after finetuning with the proposed dataset.

Weaknesses:
1. The paper only uses T, I, G to evaluate the performance of different models on the proposed dataset, which is insufficient to show the ability of understanding attribute, count and spatial relation. I think some basic metrics like accuracy are also needed. 
2. The models being benchmarked are not sufficient. It lacks some latest or popular models, like Flava, CogVLM, etc.
3. Another concern is that the finetuned CLIP/Idefics2 models do not demonstrate improvements in all aspects, indicating that the benchmark may introduce some side effects. I am curious about the underlying reasons for this and recommend that the authors propose a new baseline method alongside the benchmark.

Limitations:
The authors use editing models to modify original images. I suggest that the authors carefully review the curated dataset to ensure there are no ethical issues.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
WpEaUIBIWH;"REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3Rtn1OMTC4;"REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Vvcnqs8091;"REVIEW 
Summary:
The paper addresses the inefficiencies in current pipeline parallelism schedules used for training large-scale models, particularly focusing on high activation memory and pipeline bubbles. The paper proposes a new framework to decompose pipeline schedules into repeating building blocks and introduces V-shape building blocks (V-Min, V-Half, V-ZB) with controllable activation memory. Such building blocks reduce peak activation memory compared to the traditional 1F1B approach and even achieve higher throughput with reduced pipeline bubbles. The evaluations demonstrate substantial improvements in throughput and memory efficiency over 1F1B and 1F1B-I methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors decompose pipeline parallelism into repeating building blocks, providing a systematic methodology for designing more efficient schedules. This approach helps in understanding and optimizing pipeline parallelism in a structured manner.
- The V-Shape building blocks demonstrate the structured understanding of pipeline parallelism.
- The authors provide experimental evaluations demonstrating the practical benefits of the proposed methods.
- By reducing pipeline bubbles, the paper demonstrates that PP can become a much more preferred option over TP at practical large-scale training.

Weaknesses:
- It would be helpful to have experimental results for long sequence lengths. 1024 and 3072 sequence lengths are too short compared to what SOTA LLMs can handle.

Limitations:
No societal impact exists.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes systematic methodology for designing pipeline parallelism schedules and analyzing their performance (e.g. peak memory and pipeline bubbles).
The major observation is that a pipeline schedule can be viewed as repeating a building block, and the peak memory critically depends on the lifespan of a block.
Based on these insights, the authors design a family of novel V-shape building blocks and pipeline schedules that are memory efficient and achieve higher throughput than strong baselines like 1F1B (though at the cost of more communication), which is validated both theoretically and empirically.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This work proposes a systematic and unified perspective for pipeline parallelism schedule,
with some novel insights that might foster the design of better schedules in the future.
- Analysis and experiments are extensive and well executed, confirming the efficacy of the proposed methodology.
- Overall, writing and presentation are clear.

Weaknesses:
There is still room for improvement in writing. For example:
- There are quite some typos, some of which are listed here: Line 22 ""tensor"" -> ""Tensor""; Line 189, ""V-Min"" -> ""V-ZB"" (?); Line 193, ""serious"" -> ""series"".
- Line 210, ""while other methods' memory is similar"": one exception is 1F1B-R, whose activation memory is much smaller.

Limitations:
Limitations are discussed throughout the manuscript.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a way to identify the repeated building block a pipeline schedule is built from. By relating the peak activation memory of the schedule to the lifespan of the building block the authors show that existing schedules do not optimally use memory, and design higher-throughput schedules that use the same activation memory as the baseline. The methods demonstrate up to 55% improvement in throughput for pure pipeline parallelism settings and 16% improvement for large language models over the 1F1B baseline. The paper also provides a systematic methodology for designing pipeline schedules, emphasizing memory usage balance and reduced pipeline bubbles.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper introduces a novel framework for decomposing pipeline parallelism schedules into memory-efficient building blocks. This approach addresses inefficiencies observed in existing schedules. The systematic methodology for designing pipeline schedules emphasizes memory usage balance and reduced pipeline bubbles.

Discussion of peak memory and new memory efficient building blocks is the core of the paper and the part which is most clear. The authors provide technical details, including the proposed building blocks, their implementation, and experimental results.

Weaknesses:
The paper suffers from quite a few clarity issues and could use some copyediting (there are lots of small grammatical errors, etc.). Main clarity issues in the paper are, for example, a lack of useful captions in figures, a lack of substantive discussion in some of the appendices (i.e., “we leave the discussion to appendix X” but there is not much discussion in appendix X).

A lot could be done to improve clarity of the discussion in section 3. For example, although the paper explicitly details asymptotic behavior (d -> infinity?) this is really only mentioned in lines 166-167, which somewhat confuses the issue. Some brief discussion of what the effect is in low-d situations would be nice (not new experiments – just a qualitative idea). Fig. 3 and Table 1 contradict each other due to the presence/absence of the asymptotic limit, which is mentioned in the title of Table 1 and briefly in the text but is not very clear, especially since Fig. 3 and Table 1 are supposed to be read together (?).

The figure captions should at minimum restate what is shown in the figures and what relevant terms (d, l, etc.) mean – this makes it much easier to refer to the figure without searching through the text for definitions and explanations. This should be doable in 1 or 2 sentences per caption at most and should not take a lot of space.

Some plots are hard to read. For example, Figure 4 and Figure 5 shows detailed pipelines using various colors of blocks and fonts without proper definition or explanation. It is also not clear the shown pipeline is the actual setting or a high-level demonstration of the design. 

V-Half seems to be a heuristic method based on V-Min and V-ZB. It may not be an optimal solution for the pipeline. Other configurations regarding the trade-off of memory and throughput are not evaluated.

Section 4.4 is confusing. Table 3 is not referenced anywhere and not explained in this section.  The results mentioned in this section (Table 6) do not clearly show the combined performance of the proposed approach and existing techniques.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
6YKMBUiIsG;"REVIEW 
Summary:
The paper investigates whether speculative sampling is compatible with watermarking for LLMs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. This paper is original in the landscape of LLM watermarking.

S2. This paper shows an interesting ""no-go"" theoretical result (Th. 1).

S3. This paper proposes  2 designs sustaining either the sampling efficacy or the watermark strength.

S4. I like very much the proposed measurement of the watermark strength: the average exponent of the p-value.

S5. The presentation is crystal clear.

Weaknesses:
W1. Speculative sampling.
I am not an expert in sampling for LLM. I do not know how ""speculative sampling"" is key compared to more common methods like nucleus or top-k sampling, which prevents me from judging this paper's impact.

W2. More comments on the experimental results
Say at least that MWS is much better than MSE in the sense that the MWS loss of sampling efficiency is barely visible, whereas the MSE loss of watermarking strength is significant. The LLMs used in the experimental protocols are old and their entropy is bigger than more recent ones. It might be worth stating that this choice gives high ANLPPT.

Limitations:
No limitation is given.
A limitation about the lack of practicality of the experimental protocol would be welcome.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the inherent trade-off between watermark strength and speculative sampling efficiency in large language models. A no-go theorem is presented, proving that it is impossible to maintain the highest watermark strength and sampling efficiency simultaneously. This paper also proposes a framework called the ""two reweight framework"" and develops two practical methods that focus on either maintaining watermark strength or sampling efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ **New framework**. The proposed framework allows for the integration of unbiased watermarking and speculative sampling techniques without altering the output distribution, thereby improving generation efficiency.
+ **Theoretical proof**. This paper rigorously proves a no-go theorem that demonstrates when the vocabulary size exceeds two, it is impossible to maintain both watermark strength and sampling efficiency simultaneously.

Weaknesses:
- **Limited experimental dataset and models and experimental coverage**. The experiments are conducted only on specific datasets (e.g., CNN_DAILYMAIL) and models (e.g., Llama-7b and Llama-68m). Additional benchmarks on different datasets and models would strengthen the generalizability of the findings. The experiments only cover a few tasks (text summarization and open-ended text generation)
- **Algorithm clarity**. The pseudo-code provided for the algorithms could be further detailed, with clearer explanations for each step to improve reproducibility.
- **Lack of analysis on the robustness of watermarking**. ""On the Reliability of Watermarks for Large Language Models"" mentions paraphrasing attacks, and copy-paste attacks, etc. Could this article potentially evaluate the robustness of the watermark under the two reweight framework?

Limitations:
The authors have discussed the limitations and potential negative societal impacts in Appendix F and G.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the trade-offs between sampling efficiency and watermark strength to see if LLMs can generate watermarked output efficiently. It is proven in this work that it is not possible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Therefore, upon the no-go theorem, the paper provides two methods to maintain either one of them and conducts experiments to validate the effectiveness methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper provides the first study into the relationship between sampling efficiency and watermark strength, which is a practical common interest.
2. This paper provides proof of the no-go theorem that it is not possible to simultaneously maintain the highest watermark strength and the highest sampling efficiency.
3. From experiments, the effectiveness of proposed methods are validated clearly with visualizations.
4. Figure 1 provides an overview of the paper, which is clear and informative.

Weaknesses:
1. Experiment section is relatively short and can include more analysis of the proposed methods on aspects such as ablation study.

Limitations:
Although the experiments demonstrated the no-go theorem and effectiveness of proposed methods, maybe more analysis can be provided.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes it is impossible to maintain the highest watermark strength and sampling efficiency simultaneously for content generation by considering integrating an unbiased watermarking method [1] and speculative sampling strategy [2] [3], where they provide rigorous theoretical analysis and empirical results.

[1] Hu, Zhengmian, et al. ""Unbiased watermark for large language models."" arXiv preprint arXiv:2310.10669 (2023).
[2] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. ""Fast inference from transformers via speculative decoding."" International Conference on Machine Learning. PMLR, 2023.
[3] Chen, Charlie, et al. ""Accelerating large language model decoding with speculative sampling."" arXiv preprint arXiv:2302.01318 (2023).

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper focuses on an interesting direction integrating the watermarking method and speculative sampling to accelerate the sampling efficiency while maintaining the watermark strength. It helps us understand the interactions and tradeoffs between watermarking and sampling for content generation in LLM, which is significant.

Weaknesses:
1. Why do the authors mean by ""naively applying speculative sampling to a watermarked target distribution may significantly reduce the overlap probability with the draft distribution Q."" as in the 112-nd line?

2. In Fig.2 (a) (b), for MWS, the sampling efficiency (AATPS) is only a little smaller than that of VSpS and MSE. For example, for K=2 in (a) in terms of U score, the AATPS of MWS is smaller than that of VSpS or MSE by about less than 0.1, which should be acceptable since it achieves comparable watermark strength with VUW. 

It seems inconsistent with the claim that simultaneously accelerating the sampling efficiency while maintaining the watermark strength is impossible. May the authors explain this result?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
sKEhebkEdz;"REVIEW 
Summary:
This paper introduces a novel method to address the knowledge transfer challenge in multi-omic single-cell data. Specifically, it focuses on scenarios where annotations are available partially in one modality, namely scRNA-seq data, and aims to infer annotations in another modality, the scATAC-seq data, without requiring paired datasets.

The authors proposed to use a shared encoder to project the two modalities, with additional compoenents including: optimal transport-based dataset expansion for scRNA-seq data, divide and conpuer for target scATAV-seq data, and cross-omic multi-sample mixup. 

The author conducted comprehensive comparison with SOTA method and showed that the proposed method achieves the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, the proposed approach is innovative, and the methodology section is well-structured. An ablation study confirms that each proposed component contributes significantly to the overall superior performance of the methology.

Weaknesses:
I'm not entirely convinced about the prevalence of the problem setting described by the author, where only a small subset of the scRNA-seq data is annotated. It may occur in scenarios such as large experiments conducted in batches, where only certain batches are annotated. It raises questions about the experimental setup. Are annotations removed randomly or based on specific conditions or batches? I wonder how does the availability of annotated subsets (randomly or by batch) affect the overall study results.

Limitations:
The authors have addressed limitations such as only applicable to open-set settings.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a label transfer method from scRNA-seq to scATAC-seq data. Based on the heterogeneity of single-cell data, this work partitions data into several groups and designs effective strategies to tackle them respectively. Experiments demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is technically sound, outperforming existing methods on two scRNA-seq and scATAC-seq integration datasets.
2. Ablation studies demonstrate the effectiveness of each component in the proposed method.
3. The paper is well written and clearly organized in general, which is easy to read and follow.

Weaknesses:
1. In the Introduction, the authors claim that ""a small fraction of scRNA-seq data with cell types annotated"" aligns more closely with practical scenarios. Do you have any evidence to support the claim? You may provide an example to show that scRNA-seq data are not processed and annotated as a whole, but only a small portion is annotated.
2. The uniform distribution assumption in the optimal transport process could be wrong, as most scRNA-seq data are unbalanced across different cell types.
3. In Eq. 8, the negative cell types are removed with a significant difference from the cell type with the highest probability. However, does a threshold of 1e-3 represent significant differences?
4. What does $S_i$ in Eq. 10 mean?
5. What is the motivation of mixing scRNA-seq data to form scATAC-seq data? Is such an operation biologically reasonable?
6. Typo: In line 253 cBridge -> scBridge
7. Currently the author split the MouseAtlas dataset into several subsets with different combinations for evaluation. I recommend the author provide the experimental results on the full dataset.

Limitations:
No significant limitations of this work are found.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a semi-supervised knowledge transfer framework called DANCE, designed to effectively transfer cell type annotations from scRNA-seq data to unannotated scATAC-seq data under conditions of label scarcity. It is similar to the unsupervised domain adaptation task in computer vision. DANCE addresses the challenge of heterogeneous multi-omic data by generating pseudo-labels based on optimal transport, employing a divide-and-conquer strategy, and using cross-omic multi-sample Mixup to reduce cell heterogeneity. Extensive experiments demonstrate DANCE's superiority over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-motivated and seems to be reproducible.
2. The paper is well-structured and easy to follow.
3. The theory involved in the method seems relatively solid.
4. The experiment fully proves the effectiveness and superiority of the DANCE.

Weaknesses:
1. The compared methods, especially the DA method, are relatively old. Comparison with newer methods (e.g., [a], [b], etc.) helps to understand the performance of the methods.
2. Repeated use of symbols may cause confusion and misunderstanding among readers. Eq. (1), (3), (16), and Theory 3.2 all contain $\lambda$. As far as I understand, the meanings of these symbols may be different.
3. The method includes too many empirical hyperparameters, and the ‘crucial’ parameters selected by parameter analysis seem not comprehensive enough. A more comprehensive description of each $\lambda$ and threshold $\tao$ will help to understand the method and promote future work.
4. The quality of images seems to be low, affecting comprehension. To be specific, the method process is not clearly and comprehensively shown in Figure 1. The text in Figure 2 is too small.

[a] Semi-Supervised Domain Adaptation with Source Label Adaptation
[b] COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport

Limitations:
The author has explained the limitations in Sec. 5.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the challenge of knowledge transfer across multi-omic single-cell data under label scarcity. The proposed semi-supervised framework, DANCE, uses optimal transport to generate pseudo-labels and a divide-and-conquer strategy for handling scATAC-seq data. The framework demonstrates superior performance on benchmark datasets, offering a practical solution to label scarcity in multi-omic data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Pros:**
1. The introduction of optimal transport (OT) into the single-cell domain effectively addresses label scarcity and imbalance issues, supported by ablation studies.
2. The divide-and-conquer strategy for scATAC-seq data and Mixup to alleviate cellular heterogeneity are well-handled.
3. The performance gain is impressive, showing improvements as label availability increases.
4. The paper is well-written and organized, making it enjoyable to read.

Weaknesses:
**Cons:**
1. The exclusion of OT in scATAC-seq data due to cellular heterogeneity needs practical examples or quantitative analysis to substantiate its significance.
2. The initial prediction's dependency in the divide-and-conquer strategy could lead to misclassification. Discussion on handling wrongly divided samples or providing statistics is needed.
3. Exploring if DANCE can be conducted in the opposite direction (scATAC-seq with OT and scRNA-seq with divide-and-conquer) would be beneficial.
4. Discussion on the complexity of OT and divide-and-conquer in terms of memory and time compared to existing studies is required.
5. Including a discussion on the use of scCLIP [1] in scenarios with scarce labels would enhance the paper.

[1] https://openreview.net/forum?id=KMtM5ZHxct

Limitations:
I do not observer any potential negative societal impact of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
47CdPNiWUB;"REVIEW 
Summary:
This paper proposes a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The method is architecture-independent and integrates concepts from adversarial training to address dataset imperfections robustly. Through theoretical justifications and a series of experiments on standard datasets like MNIST and Toxic Comments, the paper demonstrates that RR can significantly improve the performance of neural networks trained under various corruption levels. The paper’s contributions are particularly valuable as they provide a new tool for improving training accuracy in the presence of label noise, enhancing the robustness and applicability of machine learning models in diverse and error-prone real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper's approach to using Rockafellian Relaxation for addressing labeling errors is innovative, especially the combination with adversarial training concepts.

Quality: The method is grounded in solid theoretical justification, and the empirical results show marked improvements over existing methods.

Clarity: The explanations of the methodologies and the algorithms are clear and detailed, making it easier to understand the operational aspects of the proposed solution.

Significance: The significance of this work lies in its potential to improve training robustness across various domains and dataset imperfections, which is highly relevant for deploying machine learning models in error-prone real-world environments.

Weaknesses:
Computational Complexity: The added complexity might limit the practical application of the method in scenarios with constrained computational resources.

Limitations:
Generalization to Different Noise Types: While the method is tested against uniform label noise, its effectiveness against other types of noise is not thoroughly investigated.

Dependence on Hyperparameter Tuning: The effectiveness of RRM is likely sensitive to the choice of hyperparameters, such as the regularization term and the parameters controlling the adversarial component. The paper does not provide extensive guidance on hyperparameter selection, which could affect the reproducibility and ease of application in different scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a loss reweighting scheme to train models in the presence of label errors. When training an NN with empirical risk minimization in this setting, one would want to assign a weight of zero to all datapoints that are mislabeled and a weight of one to all datapoints that are correctly labeled. This paper presents an automated method for accomplishing this weighting, called the Rockafellian Relaxation Method (RRM). It is noted in Theorem 3.1 that the inner minimization objective of RRM reduces to a linear programming problem, despite RRM being non convex in general. After relating RRM to distributionally robust optimization techniques, the adversarial variant of RRM is introduced (A-RRM), which includes adversarial perturbations to induce adversarial training as well as loss reweighting. Experiments on four datasets show that RRM and A-RRM outperform other methods in both adversarial settings and settings with high proportions of noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work addresses two different types of robustness: robustness to label noise and robustness to adversarial feature perturbation. It should be of interest to those who are generally interested in robust and trustworthy machine learning. Furthermore, the proposed training method has strong theoretical foundations, and its relation to other optimization formulations is discussed in detail. The theoretical results are validated in experiments that cover different datasets and types of data corruption.

Weaknesses:
The experimental section lacks a relevant baseline for comparison. As it stands, it is unclear how this compares to other noise-reduction techniques. The relationship to other techniques is discussed in the related work section, it would be nice if the purported benefits of this approach were borne out empirically.

The introduction of adversarial training in section 3.5 is under-motivated. Based on the earlier sections, it is unclear how label and adversarial feature corruptions are related to each other, why we would want to achieve robustness to both, and whether previous approaches have attempted this before. I would suggest explicitly motivating this earlier in the paper.

Limitations:
The limitations are briefly discussed in the paper. As noted above, one main limitation is that it only studies $\ell_\infty$ bounded FGSM attacks. Furthermore, this paper only considers the uniform label noise model, and does not consider the case when label corruption might be correlated with features.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents Rockafellian Relaxation (RR), a new method to address labeling errors in machine learning datasets. RR is a loss reweighting technique that enhances neural network robustness against labeling errors and adversarial attacks, working across various data domains and model architectures. The key contribution is an approach that mitigates label corruption and class imbalance without needing clean validation sets, offering a practical solution for training robust models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper introduces Rockafellian Relaxation (RR), a novel loss reweighting methodology that addresses learning with noisy label problems

- The authors provide a solid theoretical basis for RR, relating it to optimistic and robust distributional optimization formulations. RR is also designed to be architecture-independent, making it a versatile tool applicable across different neural network architectures.

- The method does not rely on having clean validation data, which is of advantage in many real-world applications.

Weaknesses:
- While not explicitly mentioned, the iterative nature of the RR algorithm could potentially be computationally intensive, especially for large datasets.

- The method assumes a specific model of label noise (e.g., uniform label noise), which may not hold in all real-world scenarios. 

- The paper could benefit from a more comprehensive comparison with other state-of-the-art methods for handling noisy labels, such as GCE [1], ELR[2], to better position RR in the existing literature.

[R1] Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

[R2] Early-Learning Regularization Prevents Memorization of Noisy Labels

Limitations:
Authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
6qJKrOulTr;"REVIEW 
Summary:
The authors propose a mathematically rigorous methodology based on Riemannian geometry for attributing network importance of tokens in a transformer models input space (e.g. image patches, or ~words in the textual domain). The proposed methodology—whilst based on sound theory—translates into an intuitive algorithm involving what appears to be a relatively inexpensive eigendecomposition. Experiments on 3 datasets across both the image and NLP domains explore how the features correlate with ground-truth inputs in the text domain, in addition to first steps towards exploring how the features affect the networks’ output logits.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A major strength of the paper is the mathematically solid approach in attempting to identify regions of the **input** space that explain transformers’ model decisions. This is an important area of study: in contrast to many recent mechanistic interpretability methods finding latent network representations (that are intrinsically hard for humans to interpret by default), salient features in the pixel/text input space are much more readily interpreted by humans.
- Whilst I am unfamiliar with geometric deep learning, the authors do a fantastic job of presenting the technical content in a digestible manner without sacrificing depth or rigor.

Weaknesses:
# [W1] Feature importance comparisons

Feature importance-based explanations are motivated on [L303] as quantifying the contribution of features `""to a model prediction""`. More concretely, around [L180], the authors motivate the eigenvalues of the pullback metric found using their method as ultimately deducing the importance of each segment (e.g. image patch)  `“with respect to the the final prediction”`. 

Consequently, a major weakness of the paper is how there is no comparison with related work for how well the proposed method’s identified important features alter the **output** logits (e.g. upon ablation).

I am slightly confused by why the authors did not adopt the established “perturbation test” experimental protocol in the baseline [1] against which they compared, to provide experimental evidence in favor of this. Currently, the only comparisons made around [L274] measure the features’ importance as they correlate to the *input’s* labels.

Concretely, the authors could, for example, ablate particular patches of MNIST and observe that the resulting performance drops correlate with the pullback metric’s eigenvalues. This would provide stronger evidence of the authors’ claims about the features affecting the networks’ output, and (crucially) ground the results in contrast to those achievable by existing methods.

# [W2] Limited experimental results & improvements

There is a lot of interesting theory here, but ultimately this is a paper with a concrete applied goal of feature attribution in transformer models. With such a new methodology with many technical details, I believe there is an extra burden of proof on the authors to demonstrate this somehow leads to additional insights / practical gains. As such, it is a relative weakness of the paper that so few experiments are performed to justify the methodology.

Beyond toy datasets, it would be interesting to see how the method performs on more complex ones (not necessarily larger ones), such as TinyImageNET. Here, we could visualize much more easily if the method helps identify salient features of animals’ body parts (for example) as being important features for classification. MNIST experiments alone in the image domain are hard to interpret given the similarity of all the input data. 

Furthermore, the method provides an almost insignificant increase of just `0.07` cosine similarity (over the baseline in [1]), on just a single dataset (and with just two baselines—for example, how does GradCAM perform here?). This is not sufficient evidence to convince me as a reader that the proposed methodology should be adopted. 

---

- [1]: Chefer, Hila et al. “Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021): 387-396.

Limitations:
Some limitations are indeed addressed throughout. However, (unless I have missed something, in which case I apologise!) I can only find the limitations of the small number of datasets used stated in the NeurIPS checklist. This needs to be stated explicitly in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work attempts to find the set of inputs that generate the same neural network predictions. To this end, the authors interpret the layers of the network as transformations of the input manifold. This interpretation is used to defined equivalence classes over the inputs and to define feature importance. Finally, the tools are used to identify equivalence classes for MNIST digits and for hate speech detection with BERT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Section 2 does a thorough job of introducing a manifold interpretation to neural networks.  This introduction is then used to motivate multiple algorithms for finding equivalence classes of the inputs---or the setup of inputs that result in the same prediction---and identify features that are important.

Weaknesses:
The main contribution of this work is to introduce a tool for analyzing which set of inputs produce the same output. However, this is exactly the Fisher information matrix (with respect to the inputs) and has also been introduced in prior work (https://arxiv.org/abs/2104.13289). Could the authors clarify what the differences are and what the additional novelty. If the ""local data matrix"" introduced in https://arxiv.org/abs/2104.13289 is identical to the tools in this work, I think it severely diminishes the contributions of this work. Furthermore the experiments are extremely similar (such as Figure 1). 

The second weakness is the limited number of experiments. The work does not show any quantitative results: Figure 1, Figure 2 and Figure 3 is just 1 example and is not indicative of why the tools are useful. The experiments in section 4 primarily discuss wall-clock time. It would significantly help if claims such as ""(Line 266) we notice that the perturbation-based algorithm ends up producing monochrome ..."" are substantiated quantitatively. Overall, the work doesn't provide novel tools and the experiments lack a novel usage of these tools and do not reveal any new insights.

Limitations:
The authors address limitations of their work but it can be expanded upon. For example, the authors can discuss the time required to compute Eigenvalues, and other limitations such as not having any Eigenvalues to be 0 in Algorithms 3 / 4. Furthermore, their algorithm should work (in theory) for infinitesimal steps in the input manifold.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a method for exploring equivalence classes in the input space of Transformer models using a solid mathematical theory. By analyzing the Jacobian of the model, the method reconstructs and navigates these classes, offering a powerful tool for understanding Transformer interpretations and enhancing explainability. The proposed method is expected to solve problems in Computer Vision and Natural Language Processing tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I write both strength and weakness. 

First, I must disclose that I have no prior study or background in both Transformers and manifolds. While I am conceptually aware of them, my knowledge is limited to that extent, and I lack confidence in reviewing the technical details. Therefore, please consider my review comments as feedback from a layperson in this field, focusing on the overall mathematical consistency and readability of the paper.

This paper describes mathematics in a clear and understandable manner that even a layperson like myself can grasp. Each definition and theorem is stated accurately, and I believe that the general concepts can be understood with basic knowledge.

I personally feel that the objective of this paper is not clearly conveyed. While the paper claims to contribute to explainability and sensitivity analysis through the analysis of input manifolds, the logic behind this was not clear to me in the Introduction and Preliminaries. Although the concepts of explainability and sensitivity analysis become clearer in the later chapters, it might be beneficial to provide a bit more explanation in the Introduction.

Additionally, it might be helpful to clearly define the equivalence class mathematically.

Since I am not familiar with the existing literature, I was unable to judge the novelty of this work.

Weaknesses:
See above.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops a novel theoretical framework grounded in Riemannian geometry for analyzing the input space of Transformer models, and introduce two algorithms, SiMEC and SiMExp, which facilitate the exploration and interpretation of equivalence classes within this input space. These methods offer new insights the internal mechanisms of Transformers, and provide new understanding of how these models perceive and process input data which can be very useful in the field of explainable AI.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1 novelty: This paper provide an innovative application of Riemannian geometry to analyze the input spaces of Transformer models, which is very novel in the area.

2 Theory: This paper establishes a solid mathematical theory on how Riemannian geometry is applied to Transformer models. Based on this theory, SiMEC and SiMExp are developed to explore the input spaces of Transformer models.

Weaknesses:
In experiment, the MNIST dataset is a little bit trivial, as the pixels of the background is essentially zero. It is nice to see the application of the proposed algorithm on natural images like CIFAR.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
CECVgSZbLW;"REVIEW 
Summary:
The authors explore the use of distributional reinforcement learning within Monte Carlo Tree search. They propose two algorithms CATS and PATS a categorical distribution and particle distribution based approach respectfully. They perform a theoretical analysis of the methods and show analysis of regret. They then evaluate on a synthetic planning tasks and evaluate it in combination with a pre-trained network on the atari benchmark.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Thorough theoretical analysis
- The authors address distributional RL applied to planning which is a clear important direction of research

Weaknesses:
- Lack of referencing of existing work and novelty relative to existing work
    - Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E. and Mannion, P., 2023. Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning. *Autonomous Agents and Multi-Agent Systems*, *37*(2), p.26.
- Unjustifiable statement: “For example, CATS is significantly better than other methods in Breakout, Enduro” There is no significance testing performed so this statement cannot be made and in fact the Confidence intervals overlap
- Key results in appendix and lack of empirical results in the main paper
- CATS never outperforms fixed depth MCTS on the synthetic tree task
- Unable to find code despite checklist saying it is provided

- Small issues
    - Figure 2 algorithms alignment off
    - Indication of Atari results in section 5 which are not there
    - Adding bold to best performing method in the Atari table would be useful for readability

Limitations:
- Limitations are not included in the main body of the paper which they should be especially considering there is space. The limitations are also not thoroughly discussed for example
    - “faces challenges in managing computational demand” : this does not say anything meaningful
    - “Our approach’s performance is slightly influenced by hyperparameters”, this can be said for essentially any method
- It seems that the distributional approach has an additional memory cost which if correct should be added to the limitations
- Given CATS and PATS do not massively outperform all baselines on the synthetic task I think the limitations should be where this is addressed and perhaps some insight given into why this is and why performance on Atari is also not particular strong relative to methods such as MENTS.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose two algorithms, Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). These algorithms extend Distributional Reinforcement Learning (RL) to Monte-Carlo Tree Search (MCTS) by modeling value functions as categorical and particle distributions, respectively to improve the performance of MCTS in highly stochastic settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The integration of Distributional RL into MCTS using categorical and particle distributions is innovative and addresses a critical need in handling stochastic environments (Sections 3.1-3.3).
- **Quality:** The theoretical analysis is rigorous, with well-defined proofs and clear methodology (Sections 4.1 and 4.2).

Weaknesses:
1. **Empirical Validation**: While the paper presents a comprehensive set of experiments demonstrating the efficacy of the proposed methods (CATS and PATS) in synthetic scenarios, there is an evident lack of diversity in the benchmarks used. 

2. **Algorithm Complexity and Overhead**: Both CATS and PATS introduce additional complexity by incorporating distributional approaches and Thompson Sampling into MCTS. The paper does not sufficiently address the computational overhead or the scalability of these methods when applied to environments with larger state or action spaces. This could be crucial for understanding the practical deployment of these algorithms in real-world applications.

Limitations:
### Computational Demands
The authors recognize that the Categorical Thompson Sampling (CATS) distributional Monte Carlo Tree Search (MCTS) involves increased complexity due to the management and updating of probability distributions. This acknowledgment is crucial as it highlights a potential scalability issue, especially in environments where computational resources are limited or real-time responses are required.

### Fixed Precision
The approach used in the Particle Thompson Sampling (PATS) to manage the growth in the number of particles by fixing the float precision is a practical solution to prevent computational overload. However, this method may introduce limitations in the precision and adaptiveness of the model, potentially affecting the accuracy of value estimations in environments with high variability.

### Number of Atoms
The performance sensitivity to the number of atoms indicates a hyperparameter dependency, which could impact the effectiveness and robustness of the model. The authors mention that suboptimal choices in this hyperparameter may affect performance, suggesting a need for careful tuning and validation to optimize the model's accuracy and efficiency.

### Addressing Limitations
While the authors have outlined these limitations, the discussion could be expanded to include more detailed strategies for mitigating these issues, particularly the computational demands and fixed precision aspects. For instance, strategies to optimize computational efficiency or adaptive techniques to dynamically adjust precision based on the context could further strengthen the approach.

### Societal Impact
The paper does not explicitly address the potential negative societal impacts of the research. In the realm of reinforcement learning and AI planning, concerns such as the deployment in sensitive or critical environments, where errors may have significant consequences, should be considered. Discussions around ethical implications, misuse, and long-term effects would be beneficial.

### Suggestions for Improvement
1. **Enhanced Computational Strategies**: The authors could explore methods to reduce computational overhead, such as parallel processing or optimizing algorithmic efficiency, to make the model more practical for real-time applications.
   
2. **Dynamic Precision Adjustment**: Introducing mechanisms to adjust the precision of particle distributions dynamically based on the observed variability in the environment could help maintain balance between computational efficiency and model accuracy.

Overall, the authors should be commended for their upfront discussion of the limitations, but there is room for deeper analysis and additional strategies to address these limitations comprehensively.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces distributional return estimates to MCTS-based planning. For this the authors borrow from work on distributional Q-Learning and show how to adapt the MCTS value back-up and action selection steps to compute and utilise these distributions. They formulate two approaches based on different distribution representations (quantile and particle based) for which they provide some theoretical convergence analysis as well as first experimental results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
The paper combines two well-established ideas in MCTS and distributional value approximation resulting in a new algorithm with better theoretical guarantees. The overall approach and implementation of this combination makes sense and should at least in theory overcome limitations in stochastic environments.

Though I was unable to verify all proofs in detail, the theoretical analysis seems to make sense and establish the advantages of the proposed methods.

Weaknesses:
Despite the soundness of the overall proposed method, I found the paper very hard to follow and felt details were missing due to an overall lack of focus. Contributing to this were the following issues:

1. Empirical Evaluation
Experiments are limited to a toy domain and results on the Atari benchmark reported in the appendix.
The toy domain is a tabular environment that is being generated randomly and contains stochasticity in both the final reward and transitions.
For an illustrating example this makes it hard to judge the combined effect on the overall return distribution to be approximated. 

How the combinations of branching factor and depths that were plotted were chosen is unclear to me. Beyond this I am not sure how meaningful these plots are. In the right most plot it appears as if the PATS approximates the root value almost correctly in under 100 simulations - at which point it could not even have tried all k = 200 actions available to it.

Also CATS appears to be doing consistently worse than some of the other methods despite having the same theoretical properties as PATS.

For the Atari baseline the authors make use of Q-networks and point to a related paper. However, the exact implementation details and hyperparameters are not discussed making it hard to reproduce this work based on the paper alone.

While stochasiticity and the exploration challenges this causes form one of the main motivations for this paper, no further ablations how the proposed methods improve here are presented.

2. Content division
The author devote a significant amount of space to the summarisation of MCTS and distributional RL. While the theoretical analysis is arguably the strongest part of the presented work only the main theorems are found in the main body of the paper with very little contextualization.

3. Overall presentation
There are several presentation issues in overall formatting, grammar and spelling. The former includes, but is not limited to overlapping lines, inconsistent / in-text section headers and wrong section references.

Limitations:
The discussion of limitations is restricted to a short paragraph in the appendix listing generic points such as increased computational demand and sensitivity to hyperparameter choices. However, no further investigation or explanation as to their severity is provided.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS) algorithms, which incorporate distributional reinforcement learning into Monte Carlo Tree Search (MCTS) to handle value estimation in stochastic settings. By modeling value functions as categorical and particle-based distributions and applying Thompson Sampling for action selection, the proposed algorithms aim to improve the robustness and accuracy of value estimates. The paper proves the theoretical effectiveness of these methods by achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is interesting and original and the non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$ significantly advances the state-of-the-art from the previous $O(n^{−1/2})$.

Weaknesses:
1- While using distributional RL in MCTS to do Thompson sampling is interesting, it introduces much computation complexity hindering the applicability of the proposed algorithms.

2- The numerical experiments for the stochastic environments that are the main motivation of this work are done on a toy problem.


Minor comments

1- The presentation of the paper can be improved, specifically the parentheses () citation style can be confused with equations reference. 

2- Line 42, the authors mention V node for the first time without properly defining what is a V node.

Limitations:
1- The added high computational complexity from maintaining a distribution for each node in the MCTS.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
LnJ2EGKTXh;"REVIEW 
Summary:
This paper introduces a framework for generating paired instruction and robot code for further fine-tuning LLMs for robot-specific tasks. A symbolic simulator is used to check the correctness of the generated code and an LLM is prompted with chain-of-thought reasoning to align generated instruction. The resulted dataset was used to fine-tune a robot specific LLM and later tested on benchmark tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper tackles two challenges of automated data generation for robot code synthesis, one is to check the correctness of the code by grounding it to logical state of objects and the other is to align the generated the instructions to provided robot capabilities. By designing principled and general modules that tackle each of these problems, RoboInstruct is shown to generate useful data for finetuning general purpose LLM to robot specific code generation applications.

Weaknesses:
RoboSIM can only check for semantically meaningful steps of the code and may not catch lower-level error that requires spatial/geometric reasoning, or even reasoning about physics, including commands that take in numerical parameters e.g. move(0,0.2,0), rotate(0.75). This seem to limit the usefulness of RoboInstruct to certain types of robot APIs.

Limitations:
see weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ROBO-INSTRUCT, a novel framework designed to generate synthetic training data for fine-tuning small language models to create domain-specific robot programs. The framework features two main components: ROBOSIM, an algorithm that validates programs using angelic execution, and INSTALIGN, which aligns instructions with the generated programs.
The key contributions of this work include the development of ROBO-INSTRUCT to enhance the code generation performance of small open-weight language models for domain-specific robot programs. This framework introduces ROBOSIM, which features a dynamic world synthesis and evaluation process for generating relevant world states and performing automated code checks for diverse tasks. Additionally, it includes INSTALIGN, a procedure that refines instruction-code pairs to improve alignment between instructions and the code generated by SELF-INSTRUCT. By fine-tuning the Codellama-Python-7B model using ROBO-INSTRUCT, the model significantly outperforms several other open-source and most proprietary models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper demonstrates strong clarity and organization, making complex concepts accessible to readers. Each section flows logically, and technical details are explained effectively, ensuring that the methodology and findings are easy to follow.
- The paper thoroughly reviews and incorporates current literature.
- The paper meticulously validates all its claims through experimental results and detailed analysis. The effectiveness of ROBO-INSTRUCT, ROBOSIM, and INSTALIGN is demonstrated convincingly through empirical data and comparisons with existing models and benchmarks. This empirical validation ensures that the contributions are not just theoretical but substantiated with practical evidence.

Weaknesses:
- While ROBO-INSTRUCT offers significant advancements for fine-tuning language models in robot programming, there are some weaknesses to consider. Firstly, it heavily relies on SELF-INSTRUCT for generating initial programs, potentially introducing biases from the base model's training data. This could limit the diversity and quality of the generated programs.
- Moreover, while ROBO-INSTRUCT shows promising results on benchmarks like ROBOEVAL, its application to real-world robot programming tasks requires thorough evaluation and validation. Real-world robot environments often present unpredictable challenges that benchmark datasets may not fully capture, necessitating further testing to assess the framework's robustness and generalizability in practical applications.

Limitations:
The limitations of the work are clearly stated

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to improve the performance of small open-sourced LLMs to generate code that can run successfully on a service robot simulator to solve tasks. The idea is to use another small model to generate program data using SELF-INSTRUCT and fine-tune a 7B model for the robotics domain. The authors note that data generated by SELF-INSTRUCT may have good diversity but lack correctness. To this end, they build a simulator RoboSIM that takes the programs generated by SELF-INSTRUCT and verifies the correctness of the execution in addition to syntax errors given a predefined robot API. They further modify the instructions to align with the verified programs better. Overall they show the 7B LLM fine-tuned on this clean data can outperform a GPT3.5-Turbo in the robotics domain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and motivated. Figures are easy to understand and helpful in conveying high-level ideas. The idea to build a pseudo-simulator that tracks world states without running the generated programs through an actual simulator is novel. It is amazing that synthetically generated data filtered by simple heuristics such as requiring programs to pass the world state tracking RoboSim is sufficient to help improve the performance of an actual simulator.

Weaknesses:
While the paper is well-written for the scope it sets for itself, I am not sure if the contribution is significant enough. There are many works using LLM to generate data and fine-tune domain-specific models, so the idea behind this paper is not super novel. The performance gain is also limited by the rather heuristic method considering the gap between the best model presented by the paper and GPT-4 it sought out to beat. In fact, given the 17% performance gap, simple baselines could be using GPT-4 to generate the programs and fine-tuning small models or using GPT-4 as the critic to filter programs. These simpler heuristics may yield better results and prove the Robo-Instruct method (which is also rather heuristic) proposed by the paper unnecessary. For reference consider 
[1] Improving Small Language Models on PubMedQA via Generative Data Augmentation
Therefore, I am not sure the contribution of this paper is all that significant.

Limitations:
Building RoboSim to track world states might work only for simple pick-n-place or task-planning problems. How to generalize this heuristic of tracking world states to more complex problems is unclear. Some analysis of the scope (suitable for what kind of problem class) of this approach is needed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ROBO-INSTRUCT, a novel framework designed to improve the code generation capabilities of smaller open-weight language models (LLMs) for domain-specific robotic tasks. ROBO-INSTRUCT leverages two key components:

1. ROBOSIM with DYNAMICEVAL:  A task-agnostic simulator that dynamically synthesizes a consistent world state based on the robot's actions within the program. This allows ROBOSIM to identify execution errors and validate generated programs even for diverse and complex tasks.
2. INSTALIGN: An instruction-program alignment procedure that utilizes Chain-of-Thought reasoning to refine the generated instructions. This ensures that the instructions better reflect the intent of the generated robot program, improving alignment between the two.

The paper evaluates ROBO-INSTRUCT by fine-tuning a Codellama-Python-7B model and testing its performance on ROBOEVAL, a benchmark for service mobile robots. The results demonstrate that the ROBO-INSTRUCT fine-tuned model significantly outperforms other open-weight models

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novel framework: Introduces ROBO-INSTRUCT, a unique approach to generating training data for fine-tuning smaller LLMs on domain-specific robot tasks.

Dynamic world synthesis: ROBOSIM's ability to dynamically create relevant world states allows it to validate diverse programs generated by SELF-INSTRUCT, overcoming the limitations of traditional simulators.

Instruction-program alignment: INSTALIGN effectively refines instructions to better reflect the program's intent, improving the quality of the training dataset.

Strong empirical results: Demonstrates that ROBO-INSTRUCT significantly improves the performance of small open-weight LLMs, enabling them to surpass even some proprietary LLMs.

Cost-effective and private:  Provides a potential alternative to deploying proprietary LLMs for local robot deployment, offering cost-effectiveness and privacy benefits.

Weaknesses:
Limited novelty: The idea of using a sim/emulator to verify the generated program has already been explored in previous works such as Chain-of-code, which is not mentioned by this work. 

Limited scope: The paper focuses on a specific domain (service mobile robots), and it is unclear how well ROBO-INSTRUCT generalizes to other robot domains.

Lack of real-world evaluation:  The paper only evaluates ROBO-INSTRUCT on a synthetic benchmark. Real-world deployment and testing are required to further assess its practicality.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RGnjY6l2HT;"REVIEW 
Summary:
The paper proposes UniEdit, a framework that allows the editing of videos. More specifically, UniEdit allows manipulation via text prompts to change the visual style or the motion pattern that is visible in the video. Moreover, it also targeted steering, e.g. via segmentation masks. They achieve this by introducing an additional reconstruction branch and a motion-reference branch into a u-net based diffusion network and share the values of the attention layers which are party designed specifically for this work. The method allows editing videos without retraining and creates very good results.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The core idea is straightforward and well-presented. There are just a few hyperparameters to select. They have a large amount of visual content showing the quality of their contribution. Moreover, they performed extensive human experiments to rate the videos.

Weaknesses:
The implementation may not be entirely reconstructable. Hopefully, this issue will be fixed when they publish the code as promised.

Even if the method description is understandable, the math is sometimes not entirely correct. For example in line 212/212, M is a matrix but the notation says that it is a scalar from a set $\{ -\inf, 1 \} $ or $\{ 0, 1 \} $. The authors should be encouraged to revise the math present in the paper.

**Minor weaknesses:**
Sometimes the English writing is a bit weak. For example: 
 - 100-110: Some parts are not forming complete sentences, e.g. ""Other improvements like efficiency [1], training strategy [19], or additional control signals [16], etc.""
 - 187: I think it should be ""an additional network"" or ""additional networks""

Limitations:
Unfortunately, there is no benchmark to evaluate the method. This is not the author's fault and they tried to do their best to create baselines. However, this makes it harder to rate the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper suggests UniEdit, a tuning-free method for editing the motion of a given video. The authors use a pre-trained text-to-video diffusion model and utilize its motion prior, to performing motion editing on a video while keeping the appearance of the original video. During the denoising process, they apply structural/content features injection from the reconstruction branch of the original video to maintain the input video's structure or content. The motion is edited according to a text description used to denoise another reference branch which is then used for injecting features into the editing videos. The results improve over the existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Successfully applying feature injection for video diffusion models. 
* Impressive results.

Weaknesses:
1. Novelty.  Feature injection for image editing is a known technique [56].  Applying injection to video models is important and challenging, but not novel enough in my view.
Showing that the injection of motion features from the reference motion branch into the edited video, constrains the output motion, is important, but not surprising given the observation of [56]. 

2. Given that the main insight of the paper is that “the temporal self-attention layers of the generator encode the inter-frame dependency”, there is not enough analysis of this besides the visual results and Figure 6, which shows the relation between the optical flow magnitude and the temporal attention on one example qualitatively. Showing a quantitative analysis, and analyzing the features during the denoising process for different layers, could support this claim better and show the importance of this insight.

Limitations:
Yes, the authors discuss both, limitations, and broader impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on developing a tuning-free framework capable of editing both the motion and appearance of videos. They introduce UniEdit, an approach designed for text-guided motion editing that maintains the original content of the source video. By utilizing two branches—an auxiliary reconstruction branch and an auxiliary motion-reference branch—they achieve both content preservation and effective motion editing.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper pointing out the problem of existing methods of not being able to keep the non-edited area and propose using spatial self-attention module, spatial cross-attention module and temporal self-attention model to solve the problem. From the experiment results, it shows that the edited results exhibit the editing task correctly while maintaining the unedited area.
2. The paper is overall clear and well-written
3. This paper provides versatile applications like motion editing, stylization, rigid/non-rigid object editing, and background editing.

Weaknesses:
1. The number of the participants in the user study might not be representative enough.

Limitations:
This method is inherently influenced by the T2V model used.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TFAG9UznPv;"REVIEW 
Summary:
The paper presents an empirical study on how synthetic data can help improve the robustness accuracy and clean accuracy of certified adversarial robustness. While existing studies have shown promising results in using synthetic data to improve empirical adversarial robustness, the effectiveness of synthetic data on certified robustness has never been explored. This paper bridges this gap by designing experiments on $l_2$ and $l_\infty$ robustness of multiple models over the CIFAR-10 and CIFAR-100 datasets. Experiment results show that synthetic data improves certified robustness but in a different way than their effects on the empirical one. The paper also provides ablation studies and guidance on different hyperparameters, such as dropout rate, epochs, model sizes, learning rate schedulers, quantity, and ratios between original and synthetic data.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper conducts experiments with multiple models and two datasets, demonstrating the generalizability of their study.
2. The ablation studies consider several hyperparameters such as dropout rate, epochs, model sizes, learning rate schedulers, quantity, and ratios between original and synthetic data.
3. The paper provides guidance on comparing certified accuracy among different approaches. This guidance is important and should be prompted in this research field.

Weaknesses:
1. My biggest concern is that the paper misses a large chunk of related works in section 2 and its experiments.
    * In section 2, when discussing the deterministic approaches of certified robustness, the paper writes, ""one deterministic approach consists of ..."" There are other groups of deterministic approaches using convex bound propagation, such as IBP, SABR, TAPS [1, 2, 3]. The paper focuses only on approaches bounding the Lipschitz constant of each neural network layer. This narrow focus hurts the generalizability of the paper and should be justified. For example, the paper mentions that this group (bounding the Lipschitz constant) of approaches generates a robust guarantee by computing the distance between the highest two logits in the output space. This sentence justifies why not focus on the convex bound propagation approaches.
    * In experiments, the paper's generalizability can be enhanced by comparing against or combining state-of-the-art convex-bound-propagation approaches, such as SABR and TAPS, SOTA, to see how the synthetic data can help.

2. In section 4.3, the paper discusses the correlation between the generalization gap and certified accuracy. The reasoning in this section makes sense. However, the paper might neglect the fact that the model trained with dropout and synthetic data hurts the certified accuracy. This is a weird result. Could you provide the generalization gap of the model trained with $\rho=0.85$ and synthetic data to try to explain this wired result?

[1] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models

[2] Certified training: Small boxes are all you need

[3] Connecting Certified and Adversarial Training

Limitations:
Although the paper mentions some limitations in their checklists, one significant limitation is whether synthetic data can help other deterministic training methods, such as convex bound propagation, as mentioned in the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores advancements in certified defenses against adversarial attacks in deep learning models by leveraging data from state-of-the-art diffusion models during training. It addresses the current challenges where empirical methods, such as adversarial training, augment data but face difficulties with new attacks, contrasting with certified approaches that provide robustness guarantees within predefined threat models but often exhibit lower overall robustness. By integrating additional data from diffusion models, the study achieves state-of-the-art robustness certifications on CIFAR-10 and CIFAR-100 under L infinite and L2 threat models. This approach not only enhances deterministic defenses but also improves accuracy on clean data. The paper also performs extensive ablation studies to examine the effects of various design choices on certified robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The writing of the paper is clear and easy to follow.
2. The paper explores a novel approach by leveraging data from diffusion models to improve certified defenses, which are typically more reliable than empirical methods.
3. The approach achieves state-of-the-art results on CIFAR-10, demonstrating a significant improvement in robustness certificates for both L infinite and L2 threat models.
4. The experimental findings are comprehensive and solid.

Weaknesses:
1. The paper evaluated four architectures selected from the certified robustness leaderboard [9]. However, the achieved clean accuracy and robustness accuracy on these neural networks are notably low. For instance, compared to architectures like WideResNet, which can attain at least 95% clean accuracy and over 80% robustness accuracy on CIFAR-10, the accuracies achieved by the approach in this paper remain insufficient.
2. While evaluating the impact of using generated data from diffusion models to bolster certified robustness is novel, the idea itself represents a somewhat incremental advance. Previous research has already explored the use of such data to enhance adversarial robustness  within empirical methods.
3. Despite the appeal of certified robustness due to its robustness guarantees, its lower accuracy compared to empirical methods diminishes its practicality. While this paper enhances certified robustness, its results have limited practical applicability.

Limitations:
1. The results primarily focus on CIFAR-10 and CIFAR-100 datasets, with limited exploration of generalization to other datasets and real-world scenarios.
2. The results are based on four selected neural network architectures, and the generalization to a broader range of architectures is not thoroughly investigated.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes using data augmentation with diffusion models to improve the certified robustness of image classification models. The authors analyze the training and certification behavior of different Lipschitz-bound-based machine learning models when the training data is supplemented with additional generated samples.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**The Method Improves Certified Robustness**

The experiments demonstrate that adding generated data can enhance the certified robustness of several certification methods. Three out of the four tested methods show improvements when trained on additional data, and the fourth method also shows improvements when dropout is removed. The authors suggest this is due to a generalization gap in existing models, which can be bridged by training on additional (generated) data.

**Extensive Evaluation**

The evaluation section is comprehensive, with tests conducted across two datasets, four certification methods, and two threat models. It also includes numerous ablations, such as varying model sizes and percentages of generated data. This provides a well-rounded understanding of the proposed data generation method, with findings effectively summarized in the takeaway list in Section 4.6.

**Code Included and Well Documented**

The supplementary material includes code for running the experiments, which enhances reproducibility. The repository is well-documented, providing clear instructions for setup and reproduction of the paper’s results, tables, and figures.

Weaknesses:
**Limited Novelty and Insights**

The idea of supplementing training data with generated data is not new and has been applied in various contexts. Specifically, it has already been used to improve Lipschitz certification by Hu et al. [22]. While this work offers a more thorough evaluation across different threat models and model architectures, the additional insights gained are limited. The paper mainly serves as an evaluation without providing new theoretical insights or methodological advancements.

**Missing Comparison to a Key Related Approach**

A very similar work by Hu et al. [22] also uses diffusion models (DDPM) to augment the training of Lipschitz-based certification methods. Although this work is listed in the references, it is neither discussed nor compared to, despite being the most related approach. This omission leads to several issues:
- The claim in L105 that certified guarantees on ImageNet are “close to random guessing” is inaccurate. Hu et al. report 35% CA for $\ell_2$-norm with $\epsilon = 36/255$, which is significantly higher than the 0.1% of random guessing.
- The claim in L36 that “[generated data] has not yet been combined with deterministic certified training methods” is incorrect, as Hu et al. have done this.
- The claimed improvement over SOTA methods (L13, L145) is not entirely accurate. For $\ell_2$-norm with $\epsilon = 36/255$ in CIFAR-10, Hu et al. report 70.1% CA, which surpasses the 69.05% reported in this work.

**Unclear Meaningfulness of Improvements**

The improvements over prior methods are minor, in the low single digits. As the authors note (L261), a change in hyper-parameters or different seeds can have similar effects on the model. Therefore, it is difficult to judge if the measured improvements are meaningful, especially considering the significant overhead of training a generator, generating the data, and additional model training on the larger dataset. This issue is compounded by the fact that all reported numbers are from single runs with one seed, without error bars or standard deviation. While the authors argue that the large computational cost makes multiple runs infeasible, it remains unclear how meaningful the improvements are.

Limitations:
The limitations are not adequately addressed. The paper should include a discussion of limitations, such as the fact that only four Lipschitz-bound-based certification methods are evaluated. It is unclear if these results generalize to other Lipschitz-based certification methods and are likely not applicable to certification methods from different families. Furthermore, only mid-scale datasets like CIFAR are considered; the generalization gap may not exist in larger-scale datasets. Additionally, the results are limited to image classification, and the resulting improvements are small.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
9OHXQybMZB;"REVIEW 
Summary:
The paper connects conformal risk control to define prediction sets containing results that satisfy a property. In short, assume that function $f$ (a trained model) does not satisfy some property $\mathcal{P}$, we can define a prediction interval around results of $f$ s.t. those intervals satisfy $\mathcal{P}$ with $1 - \alpha$ probability. The authors first generalize conformal risk control to multidimensional conservative parameter, through a proxy from $\mathbb{R} ^d \mapsto \mathbb{R}$, and use that in addressing the property testing. They define the risk as if there is no output in the prediction interval that satisfies the property.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The idea is novel. Defining conformal property satisfaction based on property testing and conformal risk control opens a new area of work based on black-box post-hoc modifications of the model toward a desired property. I believe it can be further used in model-explainability, fairness, or tweaking the model toward constrained predictions.

The idea is nicely developed and nicely presented. It can easily engage an audience how is not familiar with conformal risk control or property testing. I would recommend an additional introduction to the aforementioned topics in the appendix, however the current version is also really nice.

I also find the way the authors address fairness in section 5 (through bias in measurement error) really interesting.

Weaknesses:
1. Notations in the paper can be introduced in a better way. For instance a mathematic notation of $\mathcal{P}$, which can be a subset of the function space.

2. There are minor typos in the mathematical notation e.g. in line 88, $x \sim \mathcal{D}$ but $f(X) \neq g(X)$. Also I do not understand why the authors used $\mathrm{Min}$ instead of $\min$; I believe that might have to relate to the fact that they are minimizing over vectors but still it could be indicated by $\min_{\boldsymbol{\lambda}\in\boldsymbol{\Lambda}}$. Also shouldn't we find the minimal $\boldsymbol{\lambda} _\mathrm{min}$ instead of $\boldsymbol{\Lambda} _\mathrm{min}$?

Limitations:
To the best of my understanding, the paper defines the guarantee over a property, and a joint guarantee of property + accuracy is not defined in its framework. I see that their extension of risk control supports multi-dimensional set constant not a multi-dimensional risk. However, I believe this is not a shortcoming of the method but a possible open problem to solve.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This article offers an alternative approach to alignment by testing whether trained models follow specific properties. This is done, for instance, on monotone or concave functions. A small modification for the use case is made to the Conformal Risk Control setting to allow vectorial parameters. Using CRC and property testers (POTs), a guarantee is obtained that the predictor approximately satisfies the property.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The topic of the paper is interesting, and the approach is novel, to my knowledge. 
This approach's applications are quite broad and should interest communities interested in alignment, robustness, or, for instance, linking to certain physical applications. A few applications are presented in experiments.
Finally, I find the paper, overall, well written.

Weaknesses:
However, several parts could be improved. Section 2.1 is unclear, and the introduction of lambda without details is bound to confuse readers not familiar with CRC. 
Moreover, I found section 3.2 to be overly verbose and lacking clear, explicit mathematical formulations of the loss, particularly but not exclusively.
In case it is a misunderstanding, I believe a rewriting and clear formalism would help in understanding.
Moreover, I want to emphasize the lack of links between different sections of the papers. Notations, like the function ""g"" are reused with different meanings, and the threshold ""epsilon"" disappears from the method.

Additional typo: line 147 ""setof"".

Limitations:
I believe it's worth mentioning that this multidimensional  CRC is essentially one-dimensional, as it replaces the non-increasingness in lambda by one in a mapping of lambda to R. 
Moreover, the monotonicity in the l1 norm of lambda is often not verified (think of different slopes for the different dimensions), and several experiments do not use this result.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a way to solve the problem of alignment in Machine Learning using Conformal Risk Control. They first expand the previous work of conformal risk control to multidimensional parameters $\boldsymbol{\lambda}$, then used this extension to propose a way to test if a function belongs to a certain class of functions $\mathcal{P}$ that we assumed are aligned with user interests. Finally, they demonstrate how this method can be used to test monotonicity of functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The theoretical foundations of the paper seems pretty solid. The methodology is clear and the paper is quite easy to follow
* The monotonicity and concavity examples are pretty convincing
* Detailing the linearity case help understand the theoretical foundation and reasoning of the method

Weaknesses:
* There is only a single example with results. Presenting a method for concavity is interesting but it would be nice to see it applied to a real world examples
* There is still a significant gap between monotonicy/concavity and alignment as we understand it in AI.

Limitations:
* The authors advertise this paper as being useful in GenAI. What are examples of potential class of functions that would satisfy alignment with a generated text / image for example. It is indeed a good first step to test the belonging to a class, but defining that class can be very hard in the most important application

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method to post-process a pre-trained model to align with a subset of hypotheses on which the specific desired behaviors can be attained. The proposed method relies on proximity oblivious testers to give detection for the misalignment, based on which a conformal risk control process is used to calibrate the prediction interval to guarantee the farness to the desired subset of hypotheses.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The considered problem is a general and important. The properties to be aligned can include many possible instances.
2. The paper is overall well written and easy to follow.

Weaknesses:
1. The proposed aligning method is largely built on conformal risk control to calibrate the prediction interval, so the original technical contribution needs to be highlighted.
2. The generalization to multi-dimension conformal risk control is a bit straightforward and an immediate result from the original conformal risk control. The technical challenges need to be highlighted also.
3. The proposed method depends on the reliability of the POT, as mentioned in Line 181. However, to guarantee a rigorous prediction, it is important to consider the failure and errors from POT and how to guarantee the valid prediction in this case in details.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2vhkjOdlc8;"REVIEW 
Summary:
The paper ""Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection"" introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The framework focuses on four main components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance compared to state-of-the-art multi-class and even some class-separated UAD methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Using simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction to achieve superior performance is highly original. This is a significant departure from traditional methods that rely on complex designs and multiple modules. It challenges the conventional views: more complex architectures are necessary for better performance in anomaly detection tasks.
2. The methodology is well-detailed, and the experimental design is robust. The authors conduct extensive experiments on three well-known datasets (MVTec-AD, VisA, and Real-IAD), providing comprehensive performance metrics and comparisons with SOTA methods. The result is convincing, showing that Dinomaly not only outperforms existing MUAD methods but also surpasses some of the best class-separated UAD methods. 
3. The paper is generally clear, well-organized, and relatively reproducible.
4. The significance of this work is substantial, and makes a valuable contribution to anomaly detection, as it addresses a major challenge in UAD—achieving high performance in multi-class settings without resorting to complex, specialized architectures, and is potentially scalable.

Weaknesses:
1. The paper provides a detailed explanation of the proposed framework but lacks important justification and discussion, it is difficult for readers to realize the novelty and improvements brought by Dinomaly. The author may need to compare Dinomaly to specific previous methods, highlighting the differences and improvements. Discuss how the minimalist approach contrasts with more complex architectures and why this improvement is significant.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored. For instance, explain in more detail why Noisy Bottleneck helps prevent identity mapping.
3. The paper claims simplicity but there was no discussion of parameter number, computational complexity, or time complexity in the experiment. 
4. In Loose Constraint, the author claims that 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. How to group the features into the low-semantic-level group and high-semantic-level group in 2-group LC?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the Multi-class Unsupervised Anomaly Detection task and proposes a minimalistic reconstruction-based anomaly detection framework — Dinomaly that consists of only vanilla Transformer blocks. In this framework, four key components (Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction) are introduced to alleviate the performance gap between multi-class and class-separated models. The paper conducts extensive experiments on three major datasets: MVTec-AD, VisA, and Real-IAD. Results show that Dinomaly outperforms current state-of-the-art methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1)The paper is well-written and has clear statements which make it easy to understand. 

(2)The design of Dinomaly is straightforward but innovative. The use of foundation transformers, noisy bottleneck, linear attention, and loose reconstruction is well-justified. 

(3)The paper generally outperformed existing SOTA methods and did enough experiments and comparisons.

Weaknesses:
(1)The method relies heavily on transformer architectures, which might limit its applicability to other types of models.

(2)Transformers can be resource-intensive, and the paper does not fully address the computational cost of training and inference.

(3)The method's generalization to other domains or types of anomaly detection is not fully explored.

Limitations:
Limitations are discussed in Supplementary Sec. A.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Dinomaly, a simple yet effective anomaly detection framework using pure Transformer architectures. It identifies four key components essential for multi-class anomaly detection: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance, surpassing both state-of-the-art multi-class and class-separated anomaly detection methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors have conducted extensive experiments to validate the effectiveness of their method across multiple anomaly detection tasks.
2. The authors have proposed a simple yet effective framework that approaches and even surpasses the results of state-of-the-art methods in single-class anomaly detection tasks.

Weaknesses:
1. L53-55 'In addition, previous...': The authors should provide relevant evidence rather than subjective assumptions.
2. L77: Placing the Related Works section in the appendix is unconventional.
3. The first component proposed by the authors, Foundation Transformers, was already introduced in the ViTAD paper, which diminishes the overall contribution of the paper. 
4. The input resolution used in the authors' experiments is 448x448, while other comparison methods use 256x256 or 224x224. This is an extremely unfair comparison. Please include results with a 256 resolution in table for a fair comparison.
5. In the proposed Loose Loss, 90% of the feature points were selected. How was this 90% hyperparameter determined? Please provide ablation study results.
6. The authors have employed Linear Attention to reduce computational load while maintaining similar performance. It is recommended that the authors compare the parameter count and FLOPs of their method with those of the baseline methods to demonstrate its efficiency. Additionally, it is suggested to conduct ablation studies to verify the computational efficiency of Linear Attention. 
7. Other methods, such as RD4AD, SimpleNet, and UniAD, perform under the proposed settings. The authors can conduct a more equitable comparison.

Limitations:
It is recommended that the authors evaluate the performance of different methods under fair settings.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Dinomaly, a minimalistic unsupervised anomaly detection (UAD) method designed to bridge the performance gap between multi-class UAD and class-separated UAD. Utilizing pure Transformer architectures with key components such as Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction, Dinomaly achieves superior performance on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Dinomaly effectively bridges the performance gap between multi-class and class-separated UAD, achieving superior results on popular benchmarks such as MVTec-AD, VisA, and Real-IAD. 
2.  It utilizes a simple, straightforward approach with pure Transformer architectures, avoiding complex modules or specialized tricks.
3. The detailed ablation study demonstrates the effectiveness of each component—Noisy Bottleneck, Linear Attention, Loose Constraint, and Loose Loss—in enhancing anomaly detection.

Weaknesses:
1. The method might be perceived as too application-oriented, lacking broader theoretical contributions.

Limitations:
See the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Dinomaly simplifies the anomaly detection process by eliminating the need for complex designs, additional modules, or specialized techniques. It relies solely on basic Transformer components such as self attention mechanisms and multi-layer perceptrons (MLPs) to perform anomaly detection for multi class images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has a clear motivation and contribution. The paper effectively proposes the viewpoint of ""less is more"" in multi class unsupervised anomaly detection, emphasizing how the simplicity of model architecture can achieve or surpass the performance of more complex systems.

Weaknesses:
I hope to provide a specific explanation of the information provided by the decision-making process for identifying anomalies in the model.

Limitations:
The author candidly acknowledged the limitations of the work and provided the problems that need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
UTwuDTpdNO;"REVIEW 
Summary:
The paper addresses the vulnerabilities of Federated Learning (FL) systems to various adversarial attacks, including model poisoning and backdoor attacks. The proposed solution is a Meta Stackelberg Game (meta-SG) framework designed to offer robust and adaptive defenses against these attacks. The approach formulates adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) and employs a meta-learning method to find optimal defense strategies. Theoretical analyses show the algorithm's efficiency in convergence, and empirical results demonstrate its effectiveness against different attack types.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed framework considers both untargeted and targeted attacks.

2. The paper uses RL to realize online adaptation which is close to the real world.

3. Inspired by meta-learning, the method is robust to unknown attacks.

4. In the pre-training phase, the paper uses generated data to decrease the concern of privacy leakage.

5. The paper provides sufficient experimental results.

Weaknesses:
1. Could you explain more about the necessity of adding the gradient adaptation (from BSE to meta-SE)? Although BSE is ex-ante when knowing the distribution $Q$, the model $\theta$ is changed during training and it could capture emerging information. Could you provide empirical results comparing BSE and meta-SE to show the advantage of meta-SE?

2. Considering adaptive/mixed attacks, the paper misses two relevant frameworks: MixTailor [1] and RobustTailor [2]. They can adjust aggregation methods during training. Especially, RobustTailor also simulates a game between the defender and the attacker, and it proposes a mixed attack. This kind of method could be included in experiments as a baseline.

3. Because the whole method is complicated with 2 stages, comparing computational cost with other baselines is necessary.




[1] Ramezani-Kebrya, Ali, Iman Tabrizian, Fartash Faghri, and Petar Popovski. ""Mixtailor: Mixed gradient aggregation for robust learning against tailored attacks."" arXiv preprint arXiv:2207.07941, 2022.

[2] Xie, Wanyun, Pethick, Thomas, Ramezani-Kebrya, Ali, and Cevher, Volkan. ""Mixed Nash for Robust Federated Learning."" Transactions on Machine Learning Research. 2023.

Limitations:
The authors mentioned limitations in Section 5. The main one is the privacy concern.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper titled ""Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks"" proposes a new approach to enhancing the security of Federated Learning (FL) systems. The paper identifies that existing FL defenses are inadequate against adaptive and mixed attacks. To address this, they introduce a Meta Stackelberg Game (meta-SG) framework, which employs a Bayesian Stackelberg Markov game (BSMG) and a meta-learning approach. This framework aims to provide a robust and adaptive defense mechanism against various poisoning attacks, including model poisoning and backdoor attacks. The proposed method is theoretically proven to converge to an equilibrium efficiently and is empirically validated to perform well against strong adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Introducing the Meta Stackelberg Game (meta-SG) framework is innovative, offering a new perspective on defending against adaptive and mixed attacks in FL. 

+ The paper provides a solid theoretical foundation, proving that the proposed algorithm converges to a first-order ε-equilibrium, proving the method's efficiency. 

+ Extensive experiments demonstrate the effectiveness of the meta-SG framework, showing significant improvements in defense against various attack types compared to existing methods. 

+ The meta-learning component allows the defense mechanism to adapt dynamically to different attack scenarios, enhancing its robustness in uncertain environments.

Weaknesses:
- The proposed approach seems computationally intensive, requiring significant resources for pre-training and adaptation, which might limit its practicality in real-world applications. Although it proves that it can converge in Theorem 3.3, it would be beneficial to have empirical evidence, such as the method's run-time overhead. 

- While the framework is tested against several attack types, the scope of attacks considered might not cover all possible real-world adversarial strategies, limiting the generalizability of the results. The paper especially makes it unclear what attacks are used in pre-training, whether they are the same, and how different they are compared to the real FL environment when testing and generating results. 

- The proposed method's scalability to larger and more diverse FL environments remains unclear, especially given its computational demands.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a game theoretic model for robust federated learning. The technique is composed of pre-training and online adaptation. During pre-training, a meta-policy for the defender is solved as a Bayesian Stackelberg Markov game. The defense policy is further polished during the online adaptation stage.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The Stackelberg game is designed to counter unknown/uncertain attacks by an adaptive adversary. 

Theoretical bounds for sample complexity are provided. 

Empirical results demonstrate the effectiveness of the proposed technique.

Weaknesses:
The main weakness is the slight violation of privacy as the technique needs a portion of ground truth data from the clients. This has been disused as the limitations in the paper. 

Minor comments:

On page 2, ""including mixed attacks ,"" ---> extra space

Limitations:
Privacy violation is mentioned as a limitation of the technique. It's unclear whether future development can remove the dependence on the client-side ground truth.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a defense mechanism in federated learning that has adaptability inspired from meta learning. The authors formulates a Bayesian Stackelberg Markov game (BSMG), focusing on addressing poisoning attacks of unknown or uncertain types. The authors propose an equilbrium inspired by meta learning and then look at a local version of that. Empirical evaluations are done on MNIST and CIFAR.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper seems to have interesting results.

Weaknesses:
The writing is sloppy in many places and quite a few things are unexplored.
Federated learning's primary motivation is privacy, so the privacy loss from a core small dataset must be analyzed. The authors do acknowledge the loss but IMO that is not enough.

Behind all the motivation of federated learning, the core idea is in the equilibrium proposed - IMO, exploring this equilibrium in more detail would make the paper stronger (in fact, the problem makes sense even in simpler adversarial learning problems).

Is Def 3.1 just a differential equilibrium, in the style of ""Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study"" but missing second order conditions?
Why are there no second order conditions? Just first order may not induce a local equilibrium, which is a meaningful equilibrium to achieve. This is where even more meaning needs to be discovered for the first meta-SE. The authors compare it to PBE in the appendix, but the notations of belief consistency and sequential rationality is what makes PBE (SE) convincing. Without any such (or similar) notion, a new equilibrium in a dynamic setting is not principled.

I do not understand why the ball $B(\theta^*)$ is used with a bound of 1? What is special about 1? (same for the other ball)
Proposition written informally (and no explanation) in the main paper does not make sense (e.g., Prop 3.4).

There are many typos when I started to look in appendix:
1) Eq F6, $\tilde{l}$ should have two inputs
2) Line 959, the parameters of $\tilde{l}$ is lost, without this the equations with $\theta'$ on left and no $\theta'$ on right is not well-defined.
3) I do not understand how the equation in line 964 came about - first it is said that it is an inequality but what is written is an equality.

Overall, typos do not inspire confidence.
Also, any defense mechanism should itself be subject to attack with the adversary possessing knowledge of defense mechanism - I do not see that in experiments.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the problem of backdoor/poisoning attacks in federated learning (FL). In this setting, a single attacker has control over all malicious clients trying to employ different attack types (on each controlled client). This paper aims to create a defense mechanism against such adaptive attackers. To this end, the paper proposes a game-theoretic approach, which contains two stages: (1) Pre-training stage: before the FL environment, the defender first learns a good defense policy by simulating that environment using a small amount of truthful data against a simulated attacker with known possible actions (e.g. attack types used), and (2) Online-adaptation stage: the defender leverages the pre-trained policy and adapt it against the attacker at the real FL environment. This paper demonstrates the effectiveness of their proposed mechanism, as well as considers ablation studies where the previous assumptions do not meet: adaptation to attack methods at real FL environment are different (but similar) from ones seen at the pre-training stage.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-written and easy to follow.

Weaknesses:
I have some concerns, mostly about the practicality of the assumptions:

1. Assumption on the accessibility of data: the paper assumes that in the pre-training phase, the defender has access to a small amount of data, which is used to model the data distribution of clients using generative models. (1) It goes against privacy. (2) It is not clear that a small amount of data is representative enough to model client data distribution. (3) Things will be even more complicated if each client has its sub-population (data) that is (reasonably) different from each other. In this case, which malicious clients the attacker has control over will matter. 

2. Assumption of the similarity of attack types at the online adaptation stage: the paper assumes that the attack types in a real FL environment, though unseen, should be similar to those of the pre-training phase. This seems impractical, especially in a white-box setting, where the attacker will try to leverage the property of the defense mechanism to create an adaptive attack that is specified for that defense mechanism (see Carlini's works).

I also have concerns about the experiments:
1. Datasets and models used: Would it be possible to use more practical datasets and models instead of MNIST/CIFAR10 and ResNet-18? I would like to see results when the data distribution is complex enough that generative models cannot easily learn with a small amount of data. Right now, the amount of data used is still considerable, considering the dataset used. This would make the assumption of the accessibility to a small amount of data in the pre-training phase more persuasive.

Some comments on writing/paper organizations.
1. In Table 1, please highlight which results are the best. It would be more readable and easier for comparison. 
2. In Figure 2, it might be better to show smoothed curves.
3. In Appendix F, before each theorem/lemma/assumption, it would be better if an intuition/proof sketch for each one is provided. Also, if the proof technique/assumption is standard, please mention the corresponding references.
4. In the Conclusion, I think it would be more honest if explicitly stated that the major limitation of this paper lies in the practicability of the assumption. Right now, I only see privacy concerns mentioned, which is misleading. 

(Not really a weakness) It could be nice if source code is included (might be using an anonymous repo). 

Overall, I think this is a good paper if ignoring the practicability of the assumptions on the attack types/data (on the accessibility in the pre-training phase/distribution of accessed data in the pre-training phase/distribution of data in each client). I also did not find an experiment where the attacker leverages the information about defense mechanisms to instantiate a better attack scheme (white box attack). It is known that many proposed defense mechanisms failed in this scenario, though it seems obvious that it will go against the similarity assumption on the attack types at the online adaptation phase. However, I am also aware of the hardness of defense tasks in adversarial machine learning and it is good to have some initial results even under strong assumptions. I will try my best to be reasonable. Maths were not checked carefully, I will try to go into the details in the rebuttal phase.

Limitations:
See Weaknesses above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";0
qLtLQ4KUCq;"REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEvnCuviMO;"REVIEW 
Summary:
Metric distortion is a framework to evaluate the ""accuracy"" of social choice rules, by considering a worst-case candidate and voter embedding in a metric space, and by assuming that reported votes are derived from distances in the metric space. So far, votes were assumed to be a deterministic function of the distances. The paper investigates the case where they are probabilistic functions of the distances, in the asymptotic limit of a large number of voters. The key finding is that this inverts the evaluation of some voting rules, in particular Copeland and Random Dictatorship, for highly noisy voting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The model introduced by the authors is an insightful generalization of previous work which, remarkably, provides a markedly different view on social choice rules. Given the growing importance of social choice in machine learning, as well as accounting for noisy inputs while considering embedded vector spaces, I believe that this work scores high in significance.

Additionally, the analysis is quite thorough, with matching lower/upper asymptotic bounds for Plurality upper bound for Copeland and lower/upper bounds for Random Dictatorship.

The paper is also fairly well written.

Weaknesses:
My main concern is Lemma 3. The proof seems to argue that the constraints $\forall i, | b_i - w_i | \leq b_i + w_i$ (i.e. inequality constraint on $(i, W, B)$), but the optimization problem (7) has a constraint $\max_i | w_i - b_i | \leq \min_i b_i + w_i$. It is not clear to me why these constraints would be equivalent. Note that the latter implies the former set of constraints. Thus if $\mathcal E_{\alpha}'$ was defined with all voter-wise constraints, then it would be a minimum over a smaller set, and thus $opt(\mathcal E_\alpha') \geq opt(\mathcal E_\alpha)$. Since Lemma 3's proof actually says $\frac{SC(W, d)}{SC(B, d)} \leq 1 / opt(\mathcal E'_\alpha)$, using this inequality seems to imply the actual Lemma 3. Am I reading this correctly?

It is disappointing that the upper-bound for Copeland. It would be helpful if the authors can point out where the argument gets loose.

Limitations:
The paper stated its results very clearly and factually. I have no concerns about unaddressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the framework of metric distortion, measuring how well voting rules minimize the social cost in a given metric space, to probabilistic voting scenarios where the preferences of voters are drawn from a probability distribution defined by the relative distances between candidates and each voter's ground truth position in the metric space.

The authors base their analysis on three different axioms that the induced marginal probabilities of relative preferences must verify, namely *scale-freeness*, *independence of other candidates* and *strict monotonicity.* They define a general class of marginal probabilities that verify the three axioms, and show that it encompasses the widely used *Plackett-Luce* model.

They then provide upper and lower bounds for the distortion of the *Plurality* rule, both linear in the number of candidates and matching asymptotically when the number of voters grows to infinity.

They then provide a upper-bound for the distortion of *Copeland* rule and show that it is independent of the number of candidates in the limit of a large number of voters.

Moreover, they give upper and (non-matching) lower-bounds for the distortion of *Random Dictator.*

They finally compare their results under both the *Plackett-Luce* and *Pairwise Quantal Voting* models (the latter being inspired form Quantal Response Theory), and show that the classical bounds of the metric distortion literature are recovered in the limit of vanishing randomness (although not for Copeland rule, hinting at a loose analysis).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper, in extending the metric distortion analysis to probabilistic voting models perhaps closer to reality, is rather original and proposes a more optimistic view of metric distortion where randomized dictator is beatable in a worst case distortion sense.
The paper is fairly well written and the proofs seem correct.

It is an interesting idea, with an interesting result.

Weaknesses:
The main problem of this paper is that it is not a good match to NeurIPS. This paper would work very well at an algorithms conference like SODA or ICALP, or a CS econ conference like EC (okay, probably one tier down like WINE), or possibly even at the those AI conferences that have a history in social choice theory like AAAI or IJCAI. And I also understand that NeurIPS has accepted such papers in the past. However, is it really a good fit for NeurIPS 2024? 

- There is no mention of the proof of Lemma 1 being in Appendix A.
- In the proof of Theorem 2, $\zeta$ is hardly introduced (also not in the Appendix).
- Formatting may be improved in place: e.g. Equations 6, 7, 10, or Theorem 3.
- l.312 ""converges to 9 instead of 5"". This part is not very clear, reminding the general bound in the deterministic case would improve readability.
- Typos:
	- l.220 ""and is by solving"".
	- Equation 24 showcases $(d)$ instead of $(a)$.
	- Equation 34: $\geq$ should be $=$.
	- l.618 ""LEt"".
	- l.641 should probably be deleted (equivalent to l.642).
	- l.660 weird grammar.
	- Footnote 6: missing index $\gamma_j$.
	- l.670: missing $(\hat{g}_{MID} +\hat{g}_{OUT}  )^2$ in inequalities $(a)$ and $(b).$ Furthermore, Equation 63 is used in $(a)$ rather than in $(b)$.

Limitations:
- The existence of distributions on rankings that generate pairwise order marginals of the form described in the paper is assumed and left for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of metric distortion in single-winner elections. The key assumption is that the voters' preferences are not exactly compatible with the metric space, but they rather agree with it with a certain probability. The authors propose several axioms that formalize the requirements for the probability distribution for it to make sense in the context of distortion. Then they provide upper bounds of distortion in the probabilitic setting for Plurality, Random Dictator and Copeland (in case of the first two rules, they provide also lower bounds).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work is the first one to combine probabilistic voting with metric distortion, hence the novelty is clear. The paper is overall of good quality, the axioms proposed in their work make sense to me, and the results are sound. The research direction introduced in this paper can be continued in further follow-up papers.

Weaknesses:
The paper could have been more clearly written --- for example, the formal notation should be introduced at the beginning of Section 2   (together the model) rather than in the middle of the introduction.

Besides, I think that Axiom 2 (Independence of Other Candidates) could have been better motivated. I can imagine that it was crucial to obtain the authors' results, but it seems rather natural to me that  in the real-life scenarios that motivated the research, the presence of additional candidates can impact the voter's probability of ranking one candidate over another.

Another weakness is that the authors only consider three rules, and the analysis of only two of them is complete --- many important rules (like Borda or PluralityVeto) are not considered at all. This could raise a question whether this amount of technical contribution is enough for a top conference like NeurIPS.

Limitations:
The authors adequately addressed the limitations and there are no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers metric distortion in probabilistic models of voting. In the metric distortion framework the voters and alternatives are embedded in a metric space, and given the ranked preferences the goal is to find an alternative with low distortion. In this setting these rankings come from a probabilistic model.

In the first part of the paper, there axioms are introduced and authors show which axiom is satisfied by which probabilisitic model. In the second half of the paper the goal is to find the distortion of Plurality and Copeland rules for a specific class of probabilistic models. The results show matching upper and lower bound of $m$ for plurality and constant upper bound for Copeland.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
Defining a model for distortion in stochastic models is a useful idea in future analysis of voting systems.
The idea of the paper is novel and it uses novel techniques in the second half.
I like the definition of the three axioms. I find them natural and easy to understand.

Weaknesses:
My main concern is about the presentation. The preliminaries section is incomplete. The definition of distortion is hard to understand for a general audience and you made it harder by just putting the formula there. You have to add a description in words and give some intuition on why this definition makes sense. 

It's not clear how the probabilistic model works and how you define distortion on it until section 3 where you define it for a specific class. You have to formally define your probabilistic approach in Section 1.1 and also define distortion in this model. Not knowing the exact definition makes following the first paragraph of section 2 really hard. Before reading the rest of the paper I didn't understand why $P$ is a function of $d$ or why the preferences may not be consistent with the distances.

I think you have to add more intuition on the probabilisitic models. For instance you mention ground-truth in the definition of Mallows model but you have to explain in more details that how this model distributes the probabilities based on the distance to this ground-truth. The same explanations are needed for PQV.

My understanding is that the analysis that you provide works for any member of $G$, but currently the only members for which we have the final bound are PL and PQV. Is that true? If so is there any other interesting member of this class?

Limitations:
-

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oEVsxVdush;"REVIEW 
Summary:
The authors propose the use of the tensor product to model the interactions between object properties and their values, in contrast to the usual concatenation-based fusion for compositional representations. Extensive experiments on a large variety of image datasets are performed, where performance gains are often significantly higher than those of the baselines considered.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Modeling “multiplicative interactions” has proven to be a powerful mechanism for deep learning [1], but is largely neglected in modern times. This paper proposes an interesting way of fusing compositional representations of object properties and their values through the tensor product (and subsequent summation). Beyond the TPR’s success in this paper, I imagine the insights here through the proposed model form could help the design of more expressive deep learning architectures in future work more generally.
- The experiments are exhaustive, the technical exposition clear and crisp, and for the most part, every architectural design and decision is justified in great detail. Overall, the paper is remarkably digestible given the technical sophistication, yet offers many insights.

---

- [1]: Jayakumar, Siddhant M. et al. “Multiplicative Interactions and Where to Find Them.” *International Conference on Learning Representations* (2020).

Weaknesses:
# [W1] Multiplicatively large dimensionality of the TPR representations & issues scaling to larger settings

My main criticism of the paper is the resultant TPR's dimensionality. In particular, the soft TPR representations live in a $D_f\cdot D_r$ dimensional space. Due to the use of the Kronecker product, the dimension of the TPR representations grows multiplicatively with the two terms. 

Whilst the datasets studied in the paper are relatively simple and only 10 factors of variation are modeled, it seems prudent to acknowledge that the TPR size could grow prohibitively large for increased values of $D_f, D_r$ for more complex datasets outside of controlled settings, where significantly larger number of FoVs exist. In particular, the NeurIPS checklist states: `The authors should discuss the computational efficiency of the proposed algorithms`.

For example, I see from [L1008] that $N_R:=10$, and $N_F$ is as high as $106$ for Cars3D. Even in this regime with a very small number of roles, the TPR representations are (presumably?) larger than the concatenation-based representations of the baselines. A comparison of FLOPs/multiply-adds needed for the compared methods would be appreciated to better understand the methods’ drawbacks through the use of the (often computationally expensive) tensor product.

Limitations:
Limitations are discussed well throughout (and in further detail throughout the appendix), but an extra discussion of the potential drawbacks of increased computational costs should be made.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work explores compositional representations -- considered a crucial capability underlying intelligent human behaviour in deep learning systems. It argues that there is an incompatibility between discrete symbolic compositional representations—e.g. as obtained through traditional disentanglement approaches—and the continuous vector spaces underlying deep learning systems. To address this, the authors introduce a novel continuous compositional representation that builds on the Tensor Product Representation (TPR) approach, akin to a soft approximation to the TPR. They introduce a method for learning soft TPRs with weakly supervised data called the Soft TPR Autoencoder, and apply it to visual representation learning. This model demonstrates state of the art disentanglement for representation learning and improved sample efficiency for downstream models using those representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- importance: compositional representation learning, and compositional generalisation and sample efficiency is an important and relatively under-explored area in deep learning research. By focusing on an approach for representation that aims to be more compatible with deep learning this work makes important contributions to this area.
- novelty: the method presented is interesting & new but builds on the well established TPR framework.
- clarity: the method is explained well and in enough detail to understand and reproduce it.
- evaluation: the paper provides a thorough evaluation of the model, including comparisons with the relevant baselines, convergence rates, and performance in low sample regimes. The better performance on downstream tasks w.r.t. number of training examples is particularly interesting and supports some of the motivations for compositional representation.
 - interested to see where this work might develop in future work exploring hierarchical compositional representations.

Weaknesses:
- motivation for the approach & model could be unpacked a bit more (see questions)
- other domains: the authors focus on applying the work to the visual data, outside the typical domain of TPR models. In general, a strong feature, enabling them to tackle the messier world of complex visual data (and weaker super vision) and compare the model to the many disentanglement approaches that have previously been applied in the visual domain. However, it could be interesting to see how the _Soft_ TPR approach compares to traditional TPR in its typical domain of language.
 - no high impact is shown for downstream utility. the improvements seen for downstream tasks are certainly interesting, particularly in the low data regime, however only two tasks are explored. are there any more good downstream tasks to evaluate the utility of their learned representations? could there even be more speculation as to where future work might really leverage what can be learned with this approach?
 - need for supervision (albeit weakly): could the work be extended to use different forms of supervision or less/no supervision. noting that their ablation showed the importance of the supervision, perhaps the authors could explore more variants on this ablation, and speculate on extensions of the model that could reduce the need for supervision?

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel framework for representation learning known as Soft Tensor Product Representations (Soft TPR), aimed at capturing the compositional structure of data more effectively. The authors propose a continuously-valued compositional representation that contrasts with traditional symbolic methods. The paper makes several key contributions in the realm of representation learning, including the conceptualization of Soft TPR, the Soft TPR Autoencoder, and the demonstration of the benefits of this representation for both representation learning and downstream models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of Soft TPR brings a fresh perspective to the field of representation learning, offering a new way to represent compositional structures in a continuous manner. It provides a thorough theoretical exploration of Soft TPR, complete with detailed mathematical proofs and framework extensions. The paper clearly articulates the differences between Soft TPR and existing methods, holding significant potential for enhancing model interpretability and improving robustness to covariate shift.

Weaknesses:
1. No MPI scores for disentanglement (in Table 1) are reported. 
2. The construction of the model's loss function involves a multitude of hyperparameters, suggesting that the model might require a complex and intricate tuning process. Compared to other models like VCT, the process of adjusting parameters to achieve optimal results could involve additional complexity and such a requirement might pose challenges in terms of computational resources and time.
3. The reason why *Soft TPR is better than TPR is not fully explored.

Limitations:
I don't see any negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
znBiAp5ISn;"REVIEW 
Summary:
There's a large performance gap for graph tasks, especially graph classification tasks, between the spiking neural networks and artificial neural networks. The authors proposes the problems as the neuron's under starvation and illustrated the reason of the problem. To solve the problem, TAS-GNN was proposed.

The main contributions of the paper are as follows:
1: Starvation problem of spiking neurone in GNNs in graph classification tasks are identified.

2: A strategy was proposed to address the spike frequency deviations on the basis of the correlation between graph topology and spike frequency patterns.

The authors conduct experiments on 5 popular datasets and use several different designs of GNN layer. The results show competitive potential of the TAS-GNN.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1:This is a well-written paper, from the formulation of the problem to the solution. The author's motivation for the use of graph topology is clear.

2:The method of using topology-awaregroup-adaptive neurons shows competitive results compared with other baselines. The ablation study makes the result more persuasive. 

3: The Figures in the paper are quite straightforward, easy to follow.

Weaknesses:
1: The name of the paper is ""Topology-Aware Spiking Graph Neural Networks"". However, as I can tell the only graph topology used in the method is nodes degree, which is used to group the neurons. I wonder if it is appropriate to name it as ""topology aware"", or the author can explain it more.

2: The analysis regarding the performance of the method is lack of discussion. For instance, in some datasets, such as MUTAG and IMDB-Binary, the proposed method achieve quite competitive results while in PROTEINS it doesn't. It's betted to explain what cause the phenomenon, like the characteristics of the datasets? Also, in table 2, the results of GAT and GAT+TAG in IMDB-Binary are the same. It's better to make an explanation about them.

3: There're several typos and basic grammar mistakes in the paper that will affect the presentation of the paper. In line 120 "" and apply is to""; The sentence in line 123 is hard to understand

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper primarily discusses integrating Spiking Neural Networks (SNNs) into Graph Neural Networks (GNNs) to address several key challenges in graph classification tasks. Specifically, the paper proposes a new method called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) which leverages the topology of graphs to improve the performance of spiking neural networks in graph classification tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
（1）The authors clearly articulate the performance gap between existing Graph Neural Networks (GNNs) and Spiking Neural Networks (SNNs) in graph classification tasks.
（2）The authors conduct an in-depth analysis of the performance degradation of spiking neural networks in graph classification tasks and introduce the ""neuron starvation"" problem.
（3）The authors propose topology-aware group-adaptive neurons (TAG) based on the graph's topology, a novel approach that helps address the neuron starvation issue.
（4）The authors provide a detailed description of how to convert input graphs into spike representations, perform message passing, and classify the graphs.
（5）The authors validate the method's generalizability and effectiveness by using multiple public datasets (such as MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY) in the experimental section.

Weaknesses:
（1）The authors mention several application areas and challenges, but the references and comparisons to existing literature are not sufficiently comprehensive.
（2）Although the methodology section describes the main steps, it lacks detailed descriptions of some key aspects such as threshold initialization and the specific training process.
（3）Although there are some ablation studies, the analysis of the individual contributions of each component is insufficient, making it difficult to determine the specific impact of each component on the overall performance improvement.

Limitations:
(1) While the paper discusses the neuron starvation problem and the sensitivity of initial thresholds, it does not explicitly outline the broader limitations of the proposed TAS-GNN method. It would be beneficial to include a dedicated section that explicitly lists and discusses the limitations of the current work.
(2) The paper does not thoroughly address how TAS-GNN scales with extremely large datasets or very high-dimensional graphs. Including an analysis of computational complexity and memory usage for larger graphs would provide a clearer understanding of the scalability limitations.
(3) While multiple datasets are used, the paper could further discuss the generalizability of TAS-GNN to other types of graph-based tasks beyond classification, such as regression, clustering, or even dynamic graphs.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the performance gap between spiking neural networks (SNNs) and artificial neural networks (ANNs) in graph classification tasks. The authors identify a ""starvation"" problem in spiking neurons within GNNs, where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set, unlike in transductive or inductive learning settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	This paper identifies a critical ""starvation"" problem in spiking neurons within Graph Neural Networks (GNNs), where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set
2.	The paper proposes a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the graph classification problem.

Weaknesses:
1.	The authors use the node degree instead of the concept of topology, there’s a large gap between the graph topology and node degree.
2.	The authors solve the graph classification task as a contribution, which is not a significant challenge for spiking graph neural networks.
3.	The advantage of Spiking Neural Networks (SNN) is their low energy consumption. However, the paper does not mention the feature, so it is unclear why graph neural networks should be combined with SNN. The motivation behind TAS-GNN is not clear.

Limitations:
The authors adequately addressed the limitations.  The authors should discuss more details of the potential negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes topology-aware spiking graph neural networks with adaptive thresholds based on a group of neurons for graph classification. The paper first diagnoses the poor performance as the existence of neurons under starvation caused by the graph structure. Then the paper proposes the adaptive threshold among neurons partitioned by degrees, as well as the learnable initial threshold and decay rate to reduce the sensitivity. Experiments on several datasets show superior performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes the first SNN design to target graph classification.

2. This paper identifies the starvation problem and proposes a novel topology-aware group-adaptive technique.

3. Experiments show superior performance on several datasets, some outperforming ANNs.

Weaknesses:
1. The proposed method seems to be a hybrid ANN-SNN model rather than a pure SNN design. The paper did not discuss how this will affect the deployment of the model on potential neuromorphic hardware, since SNNs mainly target those hardware to obtain energy efficiency.

2. The paper did not discuss the (theoretical) energy efficiency estimation, which is a major motivation for considering SNNs as stated in Introduction.

3. Or if the motivation is to get models with better performance than ANN, then Table 1 does not include state-of-the-art ANN results for comparisons.

Limitations:
The authors discussed limitations in Appendix A.1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
MDgn9aazo0;"REVIEW 
Summary:
The paper introduces the Channel Clustering Module (CCM), a novel approach to enhance time series forecasting models. CCM addresses the limitations of traditional Channel-Independent (CI) and Channel-Dependent (CD) strategies by dynamically clustering channels based on their intrinsic similarities. This approach allows the model to balance individual channel treatment with capturing essential cross-channel dependencies, leading to improved forecasting performance. CCM is adaptable to various time series models and demonstrates its effectiveness through extensive experiments on multiple real-world datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The originality of CCM lies in its novel approach to channel clustering for time series forecasting, addressing the limitations of existing strategies.
- The quality of the work is evident in the well-designed experiments and the clear presentation of results, showcasing the effectiveness of CCM across different datasets and models.
- The paper’s clarity in explaining the concept, methodology, and results enhances its readability and understanding.

Weaknesses:
- The paper could benefit from more detailed discussions on the selection of similarity metrics and the impact of hyperparameters on performance.
- The computational efficiency of CCM, especially in large-scale applications, is not extensively discussed.

Limitations:
The limitation of the Channel Clustering Module (CCM) outlined in the paper includes its scalability to extremely large datasets and the computational overhead introduced by the clustering and embedding processes. While CCM shows improvements in forecasting, its efficiency in real-time forecasting scenarios with limited computational resources remains to be optimized. Additionally, the clustering and embedding processes in CCM introduce additional computational overhead, which could be a concern in scenarios where computational efficiency is critical.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Time series forecasting has been a topic of interest, with previous studies exploring different strategies. The Channel-Independent (CI) strategy treats channels individually, improving forecasting performance but lacking generalization and ignoring channel interactions. On the other hand, the Channel-Dependent (CD) strategy combines channels indiscriminately, leading to oversmoothing and reduced accuracy. A channel strategy is needed that balances individual treatment and essential interactions. Based on the correlation between performance and channel mixing, a novel Channel Clustering Module (CCM) was developed. CCM groups channels with intrinsic similarities and utilizes cluster information, combining the advantages of CD and CI. Experimental results show that CCM enhances the performance of CI and CD models, enables zero-shot forecasting, and improves interpretability of complex models by uncovering intrinsic patterns among channels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed model-agnostic method CCM achieves optimal performance between single-channel and cross-channel modeling, and it can be integrated into existing time series prediction models to enhance their performance.
2. By learning prototypes from clusters, CCM facilitates zero-shot forecasting on unseen samples, whether in univariate or multivariate scenarios.
3. The author integrated CCM into four mainstream time series prediction models on multiple different datasets. The experimental results demonstrate that in most cases, CCM can bring about significant performance improvements.

Weaknesses:
1. The experimental section involves a limited number of baseline methods, for example, SOTA LLM-based time series prediction models [1, 2] were not selected.
2. I noticed that CCM introduces additional model complexity, as it has an independent Feed Forward layer for each cluster. When the value of K is large, this may result in an excessive number of Feed Forward layers, leading to a significant increase in space complexity. On the other hand, the time complexity of CCM is linearly related to K and C. For certain datasets with a higher number of channels (e.g., Traffic), CCM may noticeably increase the time complexity of the base model. Some of the results in Figure 5 of the Appendix hint at this issue.
3. According to Table 14, the improvements brought by CCM to the base model are sometimes overshadowed by the perturbation caused by randomness. This may indicate that CCM has certain limitations.

[1] Zhou, Tian, et al. ""One fits all: Power general time series analysis by pretrained lm."" Advances in neural information processing systems 36 (2023): 43322-43355.
[2] Jin, Ming, et al. ""Time-llm: Time series forecasting by reprogramming large language models."" arXiv preprint arXiv:2310.01728 (2023).

Limitations:
The author elaborates on the limitations of their work. The main limitations are as follows: CCM does not outperform CI/CD in a few cases in the long-term forecasting benchmark; CCM’s scalability to extremely large datasets remains to be tested.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new Channel Clustering Module (CCM) for time series forecasting, which dynamically groups channels based on intrinsic similarities to balance the strengths of Channel-Independent (CI) and Channel-Dependent (CD) strategies. CCM improves forecasting accuracy by enhancing model generalization and capturing essential cross-channel interactions, achieving significant performance gains in both long-term and short-term forecasting. The module also supports zero-shot forecasting and improves the interpretability of complex time series models. Extensive experiments demonstrate CCM's effectiveness and adaptability across various mainstream time series models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces a new Channel Clustering Module (CCM) that balances individual channel treatment and cross-channel dependencies, combining the strengths of Channel-Independent (CI) and Channel-Dependent (CD) strategies.

2. CCM enables zero-shot forecasting, leveraging learned prototypes to handle unseen samples effectively.

Weaknesses:
1. The introduction of CCM increases the model's complexity, particularly for original CD models, which may result in higher computational overhead.

2. The paper acknowledges that the scalability of CCM to extremely large datasets remains untested, which could be a limitation for practical applications requiring the processing of large-scale data.

Limitations:
1. CCM does not outperform CI/CD strategies in a few cases, possibly due to the underlying channel relationships in certain real-world domains not aligning well with the similarity metric used by CCM.

2. The increased model complexity introduced by CCM may lead to higher computational costs, particularly for models with numerous channels and clusters.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new Channel Clustering Module and a corresponding Cluster Loss to group similar channels using a cross-attention mechanism. This creates a hybrid between channel independent and channel dependent approaches. Experiments in the paper compare both with and without the proposed module on a variety of existing state of the art methods and are run on time-series datasets in different domains on both short and long-term time horizons and demonstrate that this approach is generally applicable regardless of the domain. These experiments show an improvement in performance over prior work while also enabling zero-shot forecasting. The clustering produced by this module surfaces relationships between channels which are useful for feature analysis and interpretability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written and easy to comprehend and has good mathematical background. The CCM and the cluster loss is well motivated and utilizes the Gumbel-softmax to represent cluster assignment in a differentiable manner. The technique is evaluated on a variety of time series datasets across diverse domains in both a short and long-term setting. This demonstrates that the method is useful across different models and generally reduces error. The computational complexity of the CCM in the inference setting is also included.

Weaknesses:
The paper is evaluated on a variety of real-world datasets but evaluation on some synthetic data to highlight scenarios where existing methods produce a higher error but adding the clustering module reduces the error would help solidify the claims of the paper.

Limitations:
The authors list that the work needs to be tested on large datasets and that there is an additional performance overhead of the proposed CCM module.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
tVO3b68Oyp;"REVIEW 
Summary:
This paper proposes a two-stage speech language model with semantic tokens and acoustic tokens similar to AudioLM ([Borsos et al., 2022]).
-   The semantic tokens come from a speech tokenizer that can group a variable number of frames into a single token. To train such a speech tokenizer,
    1.  This paper first takes inspirations from syllable-like structures uncovered from HuBERT, and produces an initial segmentation (Section 3.1).
    1.  An iterative process is then applied to improve the segmentation (Section 3.2).
    1.  Finally the tokens are obtained by clustering of the mean-pooled frame features (Section 4.1).
-    The acoustic tokens are identical to the HuBERT-based tokens in ([Hassid et al., 2023]), referred to as ""mHuBERT"" in this paper.

Experiments with the proposed model demonstrate the following when compared to previous work,
-   Better unsupervised syllable segmentation
-   Lower speech reconstruction WER
-   Better or competitive accuracy in speech language modelling tasks (sWUGGY, sBLIMP, tStoryCloze) with lower compute
-   Better speech continuation quality

[Borsos et al., 2022]: https://arxiv.org/pdf/2209.03143 ""AudioLM: a Language Modeling Approach to Audio Generation""
[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
-   Originality: This paper proposes an original method for producing syllable-like segmentation of speech in an unsupervised manner.
    -   It uses conditional probabilities from a masked language model instead of feature similarity ([Peng et al., 2023]) to detect initial syllable boundaries.
    -   The use of an iterative process to further improve the segmentation quality is also original.
-   Quality: This paper is well-motivated. The experiment design is sound. Ablation studies included in the experiments provide valuable insight to various modelling choices.
-   Clarity: The experiment results are reported in an easy-to-interpret manner.
-   Significance: The proposed model is an competitive speech language model with a lower inference computational cost.

[Peng et al., 2023]: https://arxiv.org/pdf/2305.11435 ""Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model""

Weaknesses:
I think the method and results in this paper would make a good paper for NeurIPS, however I cannot make a recommendation for acceptance because this paper needs substantial revision to improve its readability. A non-exhaustive list of issues making the paper hard to follow includes the following,
-   References to items not yet introduced
    -   Lines 153-154, the phrase ""our loss"" make it sound like a referrence to the masked language model loss discussed in the previous sub-section, whereas in fact it is referring to Equation (3), a yet-to-be-introduced loss for SylBoost.
    -   Lines 159-162 give a very vague description of the ""similarity matrix"" and the ""cut algorithm"" which can only be known if the reader has already seen the subsequent Section 3.3.
    -   Starting at line 188, Section 4.2 makes repeated references to ""mHuBERT"". ""mHuBERT"" appears to be name given to the acoustic tokens in ([Hassid et al., 2023]) by this paper (line 241). ([Hassid et al., 2023]) itself does not use this name, so an ordinary reader would not be able to tell what an ""mHuBERT"" model is when they work through Section 4.2.
-   Confusing terminology
    -   ""pretraining"": This paper makes a liberal use of the term ""pretraining"" to the point it's very difficult to tell which is the model being ""pretrained"". For example,
        -   Line 113 mentions a ""pretrained HuBERT teacher model"", then line 119 says ""during pretraining, the **student** model ..."". The teacher and the student are presumably not trained at the same time, yet the use of ""pretraining"" in this context make it appear that the contrary is happening.
        -   Line 225 says ""for all pretraining experiments"". A reader will have to look really closely to see this means ""training of the speech LM"", not ""pretraining HuBERT, etc"".
    -   ""Agglomeration"" vs ""SylBoost"": This paper appears to use these two terms interchangeably. Agglomerative clustering is apparently also used  (line 183). This makes it difficult for the reader to tell when ""agglomeration"" is mentioned, whether the authors intend to refer to SylBoost or just the clustering.
-   Confusing equation
    -   The unnumbered equation between line 126 and line 127 defines the similarity matrix from MLM probabilities. It makes reference to
$Y_t$ without specifying which $t \in M$ is used to define $C_{r,c}$. As a result, after having read the paper 6 times over, I still do not know how to compute $C_{r,c}$.
-   Writing style
    -   Overall the writing style of this paper is very wordy, inconcise and disorganized. Often the same message can get through with far shorter sentences. Most of the paragraphs read like a dump of the stream of consciousness of the author instead of a technical document intended for actual readers. For example,
        -   Lines 102-112 would be a lot easier to understand with formal notations and a concrete example.
        -   Lines 127-131 appear to be a mere repetition of the equation above, without any new information.
        -   Lines 242-255 contain a large amount of disorganized modelling details.

[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Limitations:
The authors adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first introduces an algorithm named LossPred that generates syllable-level speech segmentation without any training or supervision. The algorithm works by analyzing the prediction loss of speech tokens under different mask positions.

With the initial boundaries proposed by LossPred, the paper proposes further training a pretrained HuBERT / data2vec2 model by minimizing the sum of squared distances between feature vectors of each token and the average of feature vectors within the corresponding segment. This process is called SylBoost, and it further improves syllabic segmentation performance and efficiency.

Finally, the paper proposes training a Generative Spoken Language Model (GSLM) with the speech tokens obtained from quantized SylBoost units. Compared to existing GSLMs trained on other discrete representations, SylBoost encodes speech into much shorter sequences, significantly boosting training and inference efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The proposed speech representation learning and unit discovery algorithms, LossPred and SylBoost, are novel. While the idea of improving computational efficiency through dynamic or fixed-rate downsampling of speech representation is not new, this paper appears to be the first to successfully apply dynamic-rate downsampled representations with a very low sampling rate of 5Hz to Generative Spoken Language Models (GSLMs).
2. The presentation of the paper is of high quality and clarity. The authors report extensive experimental results, which effectively demonstrate that the proposed method outperforms various state-of-the-art (SotA) methods.
3. The topic addressed in this paper is significant, as very low sampling rate speech representations can benefit various tasks, including speech understanding and generation.

Weaknesses:
1. As pointed out by the authors, the proposed LossPred and SylBoost methods seem to be restricted to speech representation learning. It might be difficult to apply these methods to music, singing voice, speech with noisy backgrounds.
2. LossPred is slow in evaluating the loss prediction matrix. Each sentence requires about 200 Transformer network evaluations.
3. LossPred is highly heuristic. There seems to be no theoretical guarantee that the HuBERT model combined with LossPred reveals syllabic boundaries instead of revealing only phoneme or word boundaries.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies learning low bitrate speech units that preserves semantic information. As presented in the paper, the proposed approaches achieve SoTA performance on tasks like ASR and ZeroSpeech. The proposed approach also shows benefits in terms of compute resources — as claimed by the authors, 30x faster to train, and also benefits in terms of inference and transmission due to low bitrate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the proposed multistage approach — first using the HuBERT like model to extract syllable-like noisy segmentation, then bootstrapping pseudo-syllabic units iteratively makes sense to me. The proposed approach also shows clear benefits in terms of performance and efficiency. 

Good performance: Compared to baseline approaches like SD-HuBERT, the proposed method achieved higher accuracy on syllable boundary detection and clustering, ASR, and also shows better continuation metrics as shown in Table 7 for generative spoken language modeling experiments. All those evaluations all positively demonstrate the strong associations with syllables of the generated speech units, while it does show lier-bitrate compared to the baselines compared in the paper.
The authors also conducted ablation studies to further demonstrate a couple design choices.



Efficiency: As claimed in the paper, the proposed technique is capable of achieving extremely low-bitrate compared to the counterpart speech units, while still being able to achieve good performance in a wide range of tasks, with the efficiency in both training and inference phases.

Weaknesses:
Demonstrating efficiency: As efficiency is also one selling point of the paper, it would be great if the authors can demonstrate the training efficiency and low-bitrate benefits in a more comprehensive way, like visualizing the GPU training time vs Performance, and also bitrate vs unit quality for certain tasks.



Limited use cases: The proposed approach focuses on learning semantic units for speech applications. It’s unclear if the proposed methods can be applied to other important non-speech use cases like understanding acoustic environment, and understanding speaker’s identity and emotion. 



Understanding Unit Quality: To demonstrate the unit quality for synthesizing the audio and for generation, should the author also compare with other related works (like [1] and [2]) in terms of reconstructing the original signal? Like in [1] (see Table 1), the authors compare the different approaches in terms of reconstruction performance using a couple of metrics like MEL, STFT and ViSQOL score, and also semantic task performance.

[1]: https://arxiv.org/abs/2405.00233
[2]: https://arxiv.org/abs/2306.06546

Limitations:
Not aware of potential negative societal impacts

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an approach for extracting syllable-like units from speech SSL models for use in a transformer-based language model. The motivation is that, compared to baseline acoustic units, which tend to mimic phonetic units in their time resolution, syllable-like units have lower time resolution, which makes them easier to model using techniques from the language domain. The authors propose an adaptation of the SD-HUBERT approach to extract units that can be used in Generative Spoken Language Modeling.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors identify an important limitation of why using language modeling techniques is a challenge in the speech domain, and their proposed approach seeks to address the limitation.

Weaknesses:
Overall, I found the submission difficult to follow. Please see my additional comments below.

line 2 -> Transformers do not require the inputs to be tokenized. The tokenization step is performed so that we can use language modeling techniques in speech.

line 18 -> Generally speaking, there is no requirement for the SSL representations to be powerful or abstract. 

Line 20 -> I don't see how the example of young children motivates your SSL description from the previous sentence; the transition is incoherent.

line 22 -> What does performant mean in this case? What is the connection between composing highly realistic text and the ability of a model to provide features for a downstream task? You seem to conflate the two goals, even though they are not necessarily the same.

line 23 -> The statement on this line is not clear. Several successful speech language model methods were introduced in the literature, what about previous approaches that make them fail? Please consider clarifying.

line 31 -> The temporal resolution impacts the LM part of the problem. Why is it important if we want to extract features for a downstream task?

line 37 -> What does ""syllable-like"" mean in this case? Can you elaborate on the time resolution it represents? Why is it important to start with a ""syllable-like"" unit? What makes it suitable for GSLM? What challenges from prior work are you addressing when using ""syllable-like"" units?

Line 38 -> I would refrain from using words like ""breakthrough"" and instead let the reader decide if the improvement is indeed a ""breakthrough.""

line 48 -> I disagree with labeling your method as ""train-free"" since it relies on a pre-trained HuBERT model. 

line 51 -> The distinction between the first and second contributions needs to be clarified. If the boundaries from the first contributions are not good on their own, then why mention them as a contribution? 

Line 102 -> It is not clear how/where you do the masking. Do you do it on the raw input, mel-spectrogram, or the extracted features? 

line 113 -> Shouldn't the approach be ""train-free""? Why do we have a student/teacher model that we are training?

line 147 -> The authors must refine the motivation for why syllabic units are useful for this application. Why not use word units instead? 

line 189 -> Superior compared to what?

line 198 -> I suggest leaving any experimental details to the experiments sections.

Table 1 -> Can you try any non-neural baselines for boundary detection? What would the performance be if we used heuristics based on energy, zero-crossing rate, or changes in Prosody to get rough boundaries?

Table 1 -> What makes Data2Vec2 better than HuBERT for extracting boundaries?

Table 1 -> What happens if you apply SylBoost to Feat-Sim?

Table 1 -> Please describe the metrics and abbreviations in the captions.

Table 2 -> What does the underline represent?

line 221 -> Implement what exactly? Please re-write the sentence.

line 237 -> typo

Table 3 -> What does the underline represent?

*Estimated.-> What does estimated mean? If prior work does not explicitly give this information, then it is better to leave it out.

line 263 -> What is the R score?

line 281 -> Please present the tables in the order they are referenced in the text; you currently jump from Table 1 to Table 4 and then go back to Tables 2 and 3.

line 340 -> Communication is not the last name of the first author from [16]

Limitations:
Can the authors comment on the trade-off between resolution and ease of modeling (and quality)? What do we lose/gain using syllable-like speech units in a language modeling paradigm?

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qNYYb4nOKA;"REVIEW 
Summary:
* This paper studies the evaluation of information extraction, particularly LLM-based IE, in scenarios where human-annotated data is unavailable.
* The proposed evaluation framework relies on the `Needle in a haystack` evaluation. That is, an LLM is first used to generate a piece of information (needle) given the original text; then, the needle is infused into the document, and the quality of IE is assessed by whether the needle can be successfully extracted. 
* In addition to this evaluation framework, the authors also discussed several aspects to be considered when using LLM-based IE for processing long documents.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An interesting application of `Needle in a haystack` evaluation in information extraction.

Weaknesses:
* The writing quality is not great, and several areas require further clarification
	* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach
	* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 
* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)
* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the quality evaluation of information extraction (IE) performed by large language models (LLMs). It discusses the methods to handle the input/output size limitations of the LLMs and their performance in IE. It also introduces additional scores to evaluate the extraction quality and discusses how to interpret them.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper analyses the technical limitations of LLMs complicating the extraction of information from a long context.
2. This paper presents to insert a needle into the data to evaluate the performance of IE without labeled data.

Weaknesses:
1. The analysis of the performance of LLMs in IE is not new and has various analysis, such as in the following papers:

> [1] Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness (Li et al., 2023)

> [2] Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors (Han et al., 2023)

> [3] When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks (Peng et al., 2023)

Among the papers, the authors in [3] also analysed LLMs' limitations in long context understanding, which is similar to the conclusion of this paper. 

2. This paper lacks a thorough literature review in LLM for IE as well as new evaluation formats, such as [1, 2, 3] and the following paper:

> [4] Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Fan et al., LREC-COLING 2024)

3. This paper only focuses on the NER task but lacks the other IE tasks, e.g. relation extraction and event extraction. Additional experiments are required to test the generalisability of the method. The number of samples tested is also limited (see ""# entities used for evaluation"" in Table 3).

Limitations:
See ""Weaknesses"".

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a framework to capture information extraction quality in the absence of humanly labelled and curated datasets. It explains how an approach on how to include the schema, and the role and limitations of LLM's (specifically gpt-4-1106-preview).

Experiments are done (I guess), by ""extracting information"" from long business documents originating from the healthcare sector. Several scores are presented according to the SUSWIR metrics. It delves into the ""lost in the middle"" phenomenon. It introduces the MINEA score, a newly proposed metric.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It tries to address a relevant problem in the field (curated benchmark data is hard to come by).

Weaknesses:
- The paper is from the start extremely vague and misses concrete statements and explanations about the work done. The contributions are unclear, the data is essentially undefined, for most of the work what exactly is being done is simply unclear.
	
- Even the task of ""Information Extraction"" is not concretely described in a way that is reproducible.
	
- Line 7-8: ""The framework focuses on information extraction in the form of entity and its 
	properties"". 
	
- Table 1: it is completely lost upon me what is being presented here.

-  ""We extract information from several long documents from our business case"". What are these documents? What are they originating from?

-  The scores mentioned are ""redundancy"". How is this measured? What do these scores represent? Is lower or higher better? Even these basic questions are not answered. All of this in the appendix (where it shouldnt be), and the further tables are not better.

- The work is very dry. There are no figures that explain or examplify what the problem is, or how this framework is supposed to fit.
	
- The related work section is short and doesn't address the original point (evaluation in absense of benchmark data).

- It is unclear to me how this work should contribute in any form to evaluation in the absence of benchmark data.
	
- The introduced MINEA score is ""explained"", but not examplified or mathematically defined.
	
- All examples are screenshots of data in JSON format rather than helpful explanations.

Limitations:
No. The paper does not concretely address the limitations of this metric. There are no good, bad examples provided.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an automated framework for evaluating the quality of IE tasks using LLMs. The framework introduces a scoring method called MINEA, which creates evaluation criteria by injecting artificial data (""needles"") into documents. The paper also discusses how to deal with the limitations of LLMs when processing large amounts of data, and introduces an iterative extraction process to improve the completeness of the extraction and reduce repetition.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
s1. The introduction of the MINEA score is somewhat innovative.

s2. The paper is clear explanations of the proposed framework.

Weaknesses:
w1. Lack of Originality: The originality of the paper is insufficient. Related work has already mentioned using the ""needle"" method to evaluate the information extraction capabilities of LLMs. While this paper adds the use of large models to help create the needles, the contribution is still lacking.

w2. Insufficient Experimental Description: The description of the experimental setup is missing, including the experimental environment, data sources, and dataset sizes. However, the paper spends too much space on toy examples.

w3. Unreliable Conclusions on Length Limitations: For the experiments on the input and output length limitations of models, the paper only tested one model, making the conclusions unreliable.

Limitations:
L1. The paper should provide a comparison to existing work to highlight the improvements.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zDYXdR3ClP;"REVIEW 
Summary:
This paper introduces a universal image restoration framework UIR-LoRA based on multiple low-rank adapters. UIR-LoRA employs the pre-trained text-to-image diffusion model SD-turbo as the shared component. It utilizes a LoRA composing strategy based on the degradation similarity predicted by CLIP encoder to combine different LoRA modules. Experiments show the effectiveness of the proposed method.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed LoRA-based Universal IR method is easy to understand and follow.
2. The motivation of this paper is very clear to me.

Weaknesses:
1. UIR-LoRA adopts SD-turbo as the pre-trained backbone for image restoration. However, SD-tubo utilizes VAE with high compression rate to encode input images, resulting in severe detail distortion for image restoration. This issue has been widely discussed in recent published works [1,2]. However, the paper ignores this very important issue in the Method Section and only mentions the skip-connections for VAE in Line 223.
2. The degradation-aware router seems to be unreliable. I do not believe that the original pre-trained CLIP Text Encoder can distinguish between different degradations through degraded text representations, such as ""rain"" and ""raindrop"". Therefore, DA-CLIP fine-tunes the original CLIP. But this paper doesn't contain any discussions about this.
3. This paper does not provide complete technical details, such as how the LQ image is used as a condition for SD-turbo. Is ControlNet used, or is it directly concatenated? I do not see any information about this in the paper. 
4. Tab. 1 only reports the trainable Param for UIR-LoRA. I think it's necessary to report the overall Param of the model. In addition, the reported PSNR for DiffBIR is very low. Did the authors add skip-connections to the VAE of DiffBIR for a fair comparison?
5. The visual results in Fig. 3 seem strange. The visual results of Restormer show noticeable artifacts between patches. Do the authors test Restormer using a tiled mode? As far as I know, using a single A100 GPU (Line 251), Restormer can restore the entire image without encountering out-of-memory issues.

[1] Wang, Wenjing, et al. ""Zero-Reference Low-Light Enhancement via Physical Quadruple Priors."" In CVPR, 2024.

[2] Geng, Zigang, et al. ""Instructdiffusion: A generalist modeling interface for vision tasks."" In CVPR, 2024.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to perform universal image restoration via multiple low-rank adaptation. The key idea is to leverage a pre-trained stable diffusion model as the shared component and transfer it to specific degradations with LoRA adaptation. A degradation-aware router is further proposed to generate weights for LoRA combination based on degradation confidence. In experiments, the authors evaluated their method on multi-degradation and mixed-degradation datasets and conducted several ablation experiments on their core components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of applying LoRA to a pre-trained SD for multi-task image restoration is promising and interesting.
- The overall presentation is easy to follow.
- The experimental results are good and the ablation studies make sense.

Weaknesses:
- ControlNet is the most popular approach to adapting SD models to other tasks. I'm curious why the authors chose LoRA? As far as I know, LoRA is often used for large language models (with billions of parameters). It would be great to provide more detailed motivation in the introduction.
- In line 123, maybe it's better to use ""concatenate"" or other operators instead of ""add"" to present the unified parameters. Here, the weight $s_k$ can be ignored.
- Can the authors use other SD models as the base model? I believe applying LoRA to a multi-step diffusion process can further illustrate its efficiency.
- In Eq. (4), $s_0 \cdot M_k$ is used in both numerator and denominator, which seems weird and confusing.
- The mixed degradation experiment is cool. It would be interesting if the authors could apply their model to real-world degraded images.
- Line 45: proposed -> propose

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission proposes a transfer-learning based strategy to address challenges related to image-degradation restoration. The premise is that a pre-trained generative model can be employed as a common starting component for multiple degradation types, upon which distinct sets of trainable parameters (ie. low-rank adaptors) can be added in order to address specific-degradation restoration tasks. Mixed-degradation restoration is enabled through a top-K hyperparameter, that affords a mixture of (degradation) experts to be active. The experimental setup considers multi and mixed image restoration problems where average results are offered across image-degradation datasets and appropriate standard quantitative metrics, qualitative examples, are reported in comparison with alternative approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The technique described for piping specific samples down specific low-rank adaptor chutes is relatively easy to understand and yet reportedly results in competitive restoration accuracy for investigated datasets. 

* Nascent investigations into mixed-degradation image restoration problems provide a promising seed to be followed.

* The writing is of a reasonable standard.

Weaknesses:
* The key idea of leveraging pretrained VLM features (and specifically CLIP) for the task of image restoration from multiple degradations, pre-dates the current submission [R1]. While authors clearly go to some length to highlight their alternative CLIP-based scheme, which amounts to envoking specific (pre-existing [R2]) low-rank adaptors, the core technical contributions here can be regarded as somewhat limited.  

* The phrase 'Universal Image Restoration' may not be a sufficiently accurate (or modest) description for the proposed method. The submission collates ten different image restoration tasks which, despite vague statements in the abstract, remains a 'multi-task' not a 'universal' setup. Samples for all ten degradation tasks are shared between train and test (Sec. A.1) and individual task adaptors appear to be trained independently on task-specific datasets (L188--196). Generalisation ability to previously unseen degradations is also not considered. Suggest method description requires reworking.

* The claim that multi-task learning (MTL) frameworks, designed to handle image restoration for multiple degradations, share all parameters across different degradations (L029) is incomplete and somewhat misleading. Several existing MTL works (eg. [R3,R4]) make use of both shared and task-specific parameter subsets for multiple image restoration tasks. Indeed 'which proportion of parameters should be shared and which should be task specific' can be considered a fundamental (and long standing) MTL question. The idea of benefiting from commonalities between image restoration tasks is well understood and my concern is that this casts doubt on a core premise of the submission. 


References

R1. Controlling Vision-Language Models for Multi-Task Image Restoration. ICLR 2024.

R2. LoRA: Low-rank adaptation of large language models. ICLR 2022.

R3. All in One Bad Weather Removal using Architectural Search. CVPR 2020.

R4. Pre-Trained Image Processing Transformer. CVPR 2021.

Minor:

L076: 'draining' --> 'deraining'

L099: 'mim' --> 'min'

L238: 'aspects' --> 'aspects.'

Limitations:
Half of one sentence (L293) is apportioned to discussing method limitations. See above for suggestions on components that might make for valid additions here.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes universal image restoration framework using multiple low-rank adapters that learns task specific weights from to perform multi-domain transfer learning. the proposed method leverages the pre-trained generative model weights as the shared component and adapts it task specific low-rank adapters. At each layer in the restoration pipeline the proposed method uses the degradation similarity to combine LoRA adapters outputs, this enables the proposed to handle for mixed degradation restoration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes LoRA adapters to learn task specific weights and proposes a strategy to combine the adapter outputs using degradation similarity measure
- extensive experiments are performed showing the proposed strategy works better than random and average in table 3.
- extensive experiments are performed to show the proposed methods performance against the sota methods in table 1 for mutliple degradation task.
- Extensive experiments are performaed showing impact of LoRA rank and prediction accuracy

Weaknesses:
- In table of the paper authors compared proposed method against sota on REDS and LOLBlur datasets, both these datasets have mixed degradations  of blur, jpeg compression, noise, and low light. Although these comparisons performed on mixed degradations, it would be helpful to how the proposed method performs on mixed weather conditioned images (MID6), which is comparatively challenging than REDS and LOLBlur  datasets. 
MID6: Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration, CVPR, 2024.

- Can authors confirm, whether network re-trained seperately for each experiment in table-1,  and table-2 separately, i.e. table-1 and table-2 trained network weights for proposed method are different.

Limitations:
- authors have addresed limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a framework to improve image restoration across various degradation types using Low-Rank Adapters (LoRA). The proposed method adapts a pre-trained generative model to each degradation type. It performs a weighted sum of the output of adapted models using the estimated degradation of input images. The proposed method performs impressive results in restoration accuracy and resources.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proposed method is interesting and reasonable.
Experimental results support this paper's contributions and the proposed method's effectiveness.

Weaknesses:
In Table 3, the 'Top-1' strategy performs almost the same as the 'All' strategy, which limits the motivation of the weighted sum of the adapted models.
Table 6 presents the restoration performance comparisons for each degradation. The proposed method underperforms previous works in significant degradation types such as blurry, low-light, raindrop, and rainy.
The average scores might mislead the evaluation performances.

Limitations:
The proposed method is simple and effective, but evaluating average scores on multiple degradations can mislead its contribution.
The proposed method achieves near-best performance by selecting a single adapted model but underperforms in many major degradation types.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
ShJWT0n7kX;"REVIEW 
Summary:
The paper is concerned with sampling trajectories with a terminal condition. For stochastic processes governed by a Brownian Motion, Doob's h-transform gives an posterior SDE that leads to samples with the final condition. However, estimating the h-function that is needed for the posterior SDE usually involves simulating trajectories, which is inefficient if the terminal condition is rarely reached.
The authors propose a simulation-free variational optimization method to estimate the h-function based on a least action principle and Gaussian approximations to the marginal densities.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors provide a solid and clear background on Doob's h transform that gets supported by the provided proofs in the appendix.

2. The paper clearly highlights the challenges of the optimization problem and proposes an efficient solution adressing these challenges.

3. The related work section gives a good overview and nicely connects to related topics.

Weaknesses:
1. I found the path histograms in Figure 2 to be too cluttered. I would propose adding less samples.

2. The paper misses a learning curve. It would in general be interesting to have more training details.

Minor Weaknesses:

In Chapter 3 and in the appendix, the authors change from trajectory length T to the unit interval. There should be a sentence that explains this change.

Limitations:
The authors adequately adressed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a variational formulation of Doob's h-transform, which characterizes the distribution over paths with a given endpoint. Instead of relying on potentially wasteful sampling approaches, the authors propose directly optimizing a tractable variational distribution over transition paths which satisfy the initial and terminal conditions by design. This approach reduces the search space over trajectories and avoids trajectory simulation. Experiments on real-world molecular simulation and protein folding tasks demonstrate the applicability of the approach.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper clearly motivates the problem it tackles, describes the challenges well and nicely introduces the idea behind their method in an illustrative fashion. Overall the paper is very well-written and structured. In particular, it builds up the method piece-by-piece explaining the choices along the way. As far as I can judge the related work seems to be exhaustive. The experiments are done on interesting problems as far as I can tell and I also appreciated that the authors made their code publicly available.

Weaknesses:
My main concern comes from trying to interpret the experimental results and in particular judging the performance compared to MCMC. It seems quite clear from Tables 1 and 2 that the variational approach requires fewer calls to the potential energy function than MCMC, however their performance differences are harder to judge in my opinion. 
More specifically, in Table 1 the standard deviations are so large that there is basically no meaningful statistical difference between the shown results especially for the Max Energy, but also the Log-likelihood. Now this might mean that MCMC and the presented method both perform well, but its surprising to me that there is that much inherent variation. Similarly in Table 2, the variance for the Max Energy of MCMC in the first line is huge. 
I'm also a bit confused by the Max Energy increases when using a mixture in Table 2.
To better understand how the performance of the variational approach improves during training, it would be nice to see a plot that shows the Max Energy as a function of the training epochs.

Finally, I would have liked to see a discussion of the limitations of the approach. Section 6 has Limitations in the title, but does not actually discuss them in any way beyond extensions of the proposed method.

Limitations:
Section 6 has ""Limitations"" in the title, but limitations are not discussed, only future work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The Authors of this paper tackle the problem of sampling conditioned SDEs with a specific interest in ""transition path sampling"", i.e. sampling a Langevin-type SDE undergoing a transition between an initial state (or set of states) $A$ and a final target set $B$. Sampling transition paths efficiently can provide a tremendous boost to research in catalysis or drug design. In this paper, the Authors model the transition path distribution with either (i) a parametrized Gaussian process or (ii) a parametrized mixture of Gaussians processes. These families of priors are then optimized by leveraging a variational formulation of the problem developed by the Authors starting from Doob's $h$-transform.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The Authors tackle a challenging problem with a novel variational formulation, which is apt to be optimized by leveraging techniques developed by the generative modeling community within ML. A clear strength of the paper is the sound theoretical analysis justifying the proposed method. Interestingly, the choice of Gaussian process (or a mixture of Gaussian processes) variational priors allows the Authors to simplify the algorithm using analytical results and sidestep lengthy calculations at the deployment phase.

Weaknesses:
I find the paper overall clearly written, but I had to go through section 3.2 (Computational Approach) several times to grasp how the method can be deployed in practice. Specifically, I think that it can be improved the explanation of the fact that, because of the Gaussian prior, you only need to model the transition path probability and not the $h$-transform itself, as well as the explanation of the actual optimization step (Reparametrization of Gradients).

The subscript $0,T$ is introduced for the first time in Eq. (6) without explanation and used throughout the manuscript to label variables related to the conditioned process. It might be useful to clarify this notation explicitly. 

The experimental evaluation is somewhat limited, especially concerning the Chignolin protein. Over the years, many transition path sampling strategies have been developed, as well as very much related enhanced sampling techniques. It would be nice to have a comparison also to different baselines, as well as a discussion on the computational complexity and actual running times of the baselines.

Limitations:
Limited experimental evaluation

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submitted manuscript presents a variational formulation of Doob’s h-transform, leading to a novel (simulation-free) computational approach for rare event sampling in transition paths. The task of interest involves conditioning a dynamical system driven by Brownian motion with a known drift term to reach a given endpoint. In theory, this terminal condition can be addressed using Doob’s h-transform, resulting in an associated SDE for the conditional dynamic. However, practical implementation requires knowledge of the h-transform. The authors propose a variational problem formulation that provides the necessary information about the h-transform as its solution. Solving this variational problem results in a computational approach for simulating the conditional dynamic. Compared to existing methods, it is claimed that the proposed approach avoids importance sampling estimators and expensive trajectory simulations. The method is tested on both synthetic and real datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces a novel variational objective to describe Doob’s h-transform, leading to a promising and innovative computational approach for simulating the conditional dynamics of transition paths. Although I appreciate the general approach presented in Section 3.1, I am not convinced by the computational approach proposed in Section 3.2 due to several reasons pointed out below. If these issues can be addressed, I believe that the approach can be further utilized to construct highly efficient sampling strategies.

Weaknesses:
- The proposed approach in Section 3.2.1 introduces certain issues. The connection to Doob’s h-transform established in Theorem 1 is only guaranteed if an exact solution to the optimization problem (9) or (11) is found. However, by introducing the Gaussian parametrization of $q_{t|0,T}$, this guarantee is lost. There is a lack of discussion regarding the implications and potential effects of this parametrization on the overall accuracy and validity of the method.
- Moreover, the formulation of the task of solving equation (12) given $q_{t|0,T}$ is misleading. In reality, you are solving an inverse problem here. Given the solution of the PDE in (12), your goal is to reconstruct $u_{t|0,T}$, which is generally an ill-posed problem. For instance, introducing uncertainties into the model description via the drift $b_t$ or diffusion $\Xi_t$ can lead to significant challenges. This scenario is likely when applying the approach in practice under model misspecification. A critical question to address is the robustness of the proposed framework against model misspecification, such as small perturbations in $b_t$.
- The claim that the approach is simulation-free is not entirely clear. In line 173, it is stated that the Gaussian parametrization allows for the generation of arbitrary samples. However, this assumes that the states $x_{t|0,T}$ are independent for all $t$, which is not true when $x_{t|0,T}$ is a solution of the SDE in (10). The dependence between the states needs to be accounted for in the sampling process, and this aspect seems to be overlooked. 
- I am wondering what the measured quantities in the numerical experiments actually reveal about the correctness of the proposed approach. Evaluating the maximum energy and the likelihood might not provide sufficient information about the accuracy of the estimated distribution. If the sampled paths only follow high likelihood regions, this does not necessarily indicate that the correct distribution has been captured. The objective should be to estimate quantities of interest related to the conditioned SDE in (6). However, it is not clear whether the simulated paths correspond to accurate simulations of (6). For instance, how does the method perform when estimating rare event probabilities or other Monte Carlo estimators with respect to (6)? This aspect needs to be thoroughly addressed to validate the effectiveness of the proposed approach.

Limitations:
- There is no discussion about error propagation arising due to the Gaussian parametrization.
- The approach is limited to conditioning the sample path on point sets $x_T = B$.
- Limited statistical evidence is presented in the numerical experiments.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a variational formulation of the Doob's h-transform to change the problem from the expensive simulations of trajectories to an optimization problem over possible trajectories from given initial point and end point. The model parameterization introduced imposes the desired boundary conditions by design and uses a mixture of Gaussian as the variational family. The variational formulation offers an alternative to expensive simulations and MCMC which is hard to scale. The proposed framework is applied on a simulated case study and real world problem in material science/computational chemistry of transition path sampling  which is a study of molecule transitions betweeb local energy minima and metastable states under random fluctuations. The results are along expected lines where it is possible to scale the solution on computationally intractable problems with MCMC and other sampling based methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well written, with sound math, a lot of references on recent work and older literature on the subject. I found the introduction to be a great summary of the work. 
2. The idea is sound and through the use of variational framework to solve a complex problem of estimating rare event probabilties by sampling forward trajectories brings out the desired qualities of sample efficiency, reducing search space and matching a complex target distribution with an easier to sample variational distribution. As also observed in Table1 and 2, MCMC cannot be scaled to certain large scale experiments where Variational inference can give good results. 
3. The work finds application for solving transition path sampling problem which is important in material science and chemistry from what it seems(I am not so well acquanited with those domains.) The case study on protein is well documented and explained. 
4. The figures and illustrations are clean and support the narrative.

Weaknesses:
Minor things
- Increase font sixe for Table1, bold the important results.
- I would have liked to see a bit more dicussion on the dimensionality, what are the typical values of D ..

Limitations:
The authors have addressed the challenges quite well and what the future research directions could be taken.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Ehsd856Ltb;"REVIEW 
Summary:
This paper proposes a novel tokenization method for DNA sequence foundatnion model. According to the authors' experiments, their model has similar performances by comparing with other foundation models with larger scale, while their scalability looks better.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The tokenization process is interesting, and the paper contains theoricial proofs. It seems that their-TNF outperforms other methods in 1 out of 6 datasets.

Weaknesses:
Major:
1. Their proof majorly focus on arguing the problems of k-mers tokenization. However, some baselines like DNAHyena utilizes single nucleobase as tokens. Is it possible to justify the superority of the authors' method comparing with other tokenization approach (e.g., single nucleobase token, and BPE used by DNABERT-2).

2. According to their experiment result, it seems that their approach does not show comparable performances across all the datasets, which makes me hard to believe its superority. Moroever, some methods like HyenaDNA is only trained based on human genome, the testing dataset might be not suitable for benchmarking with these methods. Is it possible to select some genome overlapped with all of these methods? Also, it will be interesting if the authors can include NT [1] in the comparison.

3. In section 4.1, the authors utilize two datasets to analyze the effect of hyper-parameters, are there any reasons for not using other datasets?

4. The rest of benchmarked methods are all in Million level not billion level, and thus the superority of scaling it not very interesting. It will be helpful if the authors can discuss and try to scale their method to 1B or 7B level, which will be a breakthrough in this area. NT is 1B at max.

[1] https://github.com/instadeepai/nucleotide-transformer

Minor:

1. It seems that there is a block near line 176 and I cannot click it, is it a typo?

Limitations:
Please see the weaknesses, as well as their limitation section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the use of k-mer profiles in genome representation for metagenomic binning. The authors propose a new, scalable model that leverages k-mer profiles to represent DNA fragments efficiently. This model is theoretically grounded and empirically validated against state-of-the-art genome foundation models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Theoretical Contributions**: The paper broadens the theoretical understanding of k-mer. By establishing theoretical bounds and providing a detailed analysis of DNA fragment identifiability, it contributes valuable insights that can guide future research and applications in genome analysis.
    
2. **Scalability and Computational Efficiency**: The proposed model addresses the computational demands associated with large genomic datasets. By demonstrating that their k-mer-based model requires significantly fewer resources compared to more complex genome foundation models, the paper appeals to practical needs in genomic research, particularly in resource-limited scenarios.
    
3. **Empirical Validation**: Through extensive empirical testing, the paper substantiates the theoretical claims by showing that the k-mer-based model performs comparably to genome foundation models in metagenomic binning tasks. This not only validates the model but also emphasizes its practicality for real-world applications.

Weaknesses:
1. **Comparison Basis**: The paper positions k-mer profiles as a competing approach against DNA language models (LMs). This comparison is somewhat misleading as k-mer is fundamentally a tokenization method (i.e. one initial step of DNA FM), and a more appropriate comparison would be between different tokenization strategies (k-mer vs. BPE) rather than against the entirety of DNA LMs.
    
2. **Experimental Clarity**: The experimental setup lacks detailed discussion, particularly on how the DNA foundation models are integrated and utilized in the experiments. There is a concern about potential unfair comparisons, especially if the study only uses the embeddings from DNA LMs for classification without fine-tuning the full models, which is the standard practice for such models. This could lead to skewed results favoring the proposed method.
    
3. **Technical Details and Minor Errors**: The paper has several minor but notable issues, such as the lack of clarity in the definition of the loss function at line 199 and missing labels for equations post Equation 3. These issues, while minor, could impede the understanding and reproducibility of the research.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors describe an embedding approach for binning metagenomic reads via a k-mer approach. The authors motivate the need for scalable solutions for metagenomic analysis and provide theoretical motivation for k-mer based approaches, which have value on their own. Then they motivate a strategy to learn non-linear embedding of short, correlated k-mers for binning purposes. They compare their approach to several large language models that have been used for that purposes and while having much higher complexity and foodprint, these do not provide better performance on commonly used standard benchmarking data.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* very well written manuscript, nicely incorporating algorithmic and learning approach for a relevant application
•	Impressive results (considering the scalability of the solution and the importance of scalability for the problem) in a well chosen benchmark
•	Good theoretical foundation of the provided solution (which has value beyond the proposed solution)

Weaknesses:
* The authors may want to acknowledge a little more in depth that there are also excellent algorithmic solutions for metagenomic binning that are directly evaluated on the CAMI2 data (in the CAMI2 publication).
* Related, there is also no comparative evaluation to algorithmic approaches available (although very good ones to other machine learning based approaches).

Limitations:
Limitations are well addressed in the manuscript

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provides a theoretical analysis for k-mer-based representation of genomes and then propose a lightweight and scalable model for performing metagenomic binning at the genome read level, relying on the k-mer compositions of the DNA fragments and achieve pretty good results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The logical is exceptionally clear and well-structured. 

2. The issues examined in this paper are interesting and relevant.

Weaknesses:
1. Limited comparison with the current methods. Zhang[1] has propose a novel k-mer natural vector for representing biological sequences with the consideration of positional information in sequence. What is your strengths compared with the novel k-mer natural vector. Could you make comparison with it in the task of Metagenomics Binning. In addition, there are various pseudo features extracted from biological sequences directly [2], why not compare with them in terms of performance.

2. Limited experimental results. The article just shows the results just on one task of Metagenomics Binning, which may not fully demonstrate the effectiveness of k-mer. If possible, please design more downstream tasks.

3. The poor framework. We all know k-mer feature can work, but how to design a framework to its strength. As shown in the Figure 3, DNABERT-S outperforms ‘Ours-linear’ and ‘Ours-nonlinear’ in all datasets. In addition, METABAT2, VAMP and SEMIBIN2 are the common binning algorithm, why not compare with them to highlight the effectiveness of “Ours-linear” and “Ours-nonlinear”.

4. Lack of guidance from theory to experiment. The proof of Theorem 3.1 is a wonderful process, but how to use it to guide the experiment, how to select a suitable k according the special dataset. And if we consider the position of k-mers in sequences, then we can assure that any sequences are identifiable unless they are exactly the same.

5. Lack more detailed explanation for the effectiveness of k-mers. The DNABERT family builds a vocabulary based on k-mers and then uses a nonlinear network to extract the embedding for each tokens, which is also a nonlinear representation, but what is the difference between this and ""Ours nonlinear"". Apart from the dimensional difference, where is the disadvantage.
 
Reference:

[1] Zhang, YuYan, Jia Wen, and Stephen S-T. Yau. ""Phylogenetic analysis of protein sequences based on a novel k-mer natural vector method."" Genomics 111.6 (2019): 1298-1305.

[2] Pse-in-One: a web server for generating various modes of pseudo components of DNA, RNA, and protein sequences B Liu, F Liu, X Wang, J Chen, L Fang, KC Chou - Nucleic acids research, 2015

Limitations:
Yes, they Have.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
nmUkwoOHFO;"REVIEW 
Summary:
This work focuses on understanding supervised finie-tuning (denoted SFT, where parameter weights are changed) and in-context learning (denoted ICL, where weights remain frozen and the adaptivity is done through a few shots in prompts), in the context of modern LLM. In doing so, the authors propose to apply the Advanced Density Peaks algorithm (ADP) to layers in the transformers that are used in a question answering task. The authors presented a wide range of plots and discussion, observing that both few-shot learning and supervised fine-tuning shows complementary behavior where earlier layers and late layers focus on different levels of information.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This manuscript could gain importance due to the recent advances in modern LLMs and the widely adapted practice of supervised fine-tuning and  in-context learning. The authors make great efforts showing a wide range of analysis (although in a less clearly organized manner).

Weaknesses:
I have a few major concerns regarding the manuscript in its current form.

This manuscript has several issues in clarity. First, the description of the ADA algorithm, which is the cornerstone of this work, remains largely hard to follow, and I have to refer to the original ADA paper to figure out what the authors propose. Also in the results section, a wide range of observations are discussed, but such discussions are largely scattered and I found it hard to gain insight from them.

There are also issues with the originality and significance of this manuscript. This is partially due to the issues in clarity mentioned above and also the lack of significant contributions. Technically, this manuscript adapts ADA algorithms in understanding the representation, in each layer of the transformer. This is technically not a significant contribution. This, alone, is not a concern, since many great analysis works use simple methods to reveal important insight. However, in this manuscript’s case, the insights are less clear and a reader may find it difficult to draw inspiration from the discussions.

In an ideal form, the manuscript could contain clear, organized discussion revealing interesting insights. A non-exhausting topic that can be studied includes semantics, fine-tuning dynamics, theoretical bounds, practical rule-of-thumb, or the lack thereof in modern models. I would encourage the authors to consider such aspects.

Nit-picketing:
Table A1: last columns for Llama-3-70b have formatting issues (should uniformly use percentage or decimal).

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores the internal dynamics of LLMs when subjected to ICL and SFT strategies. Despite achieving comparable outcomes, the paper reveals that these methods result in distinct representation landscapes within the model. ICL fosters a hierarchical organization of representations based on semantic content in the initial layers, whereas SFT yields a more diffuse and semantically mixed landscape. In the latter stages of the model, fine-tuning leads to representations with sharper probability modes that better capture answer identities compared to the less defined peaks produced by ICL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper takes a novel approach by analyzing the geometric properties of the hidden representations in LLMs, offering new insights into the workings of ICL and SFT. The use of the Advanced Density Peaks algorithm and the systematic investigation of data transformations across hidden layers demonstrate a insightful methodological framework.

Weaknesses:
See Questions.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors analyze the probability landscapes of the hidden representations of LLMs when they perform in-context learning (ICL) and supervised fine-tuning (SFT) using the MMLU question answering benchmark, and the Llama-3, Llama-2, and Mistral-7B LLMs are used for this purpose. The Advanced Density Peaks (ADP) algorithm is used for this analysis. The experiments conducted recover fingerprints that showcase the distinctions between ICL and SFT, specifically that representations in the earlier layers of the network are aligned with the subjects (topic of the question) during ICL and that later layers are better aligned with the final answers for SFT. The authors conduct a number of different experiments analyzing the probability landscape and draw interesting conclusions from the results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper has a number of strengths, namely the extensive set of experiments that lead to interesting findings. The findings are useful to the community-- with the main takeaway being the two-phased nature of the hidden layer representations' probability landscapes.
- Additionally, I appreciate the findings obtained that demonstrate the fingerprints obtained for ICL and SFT in the early and later layers respectively. More specifically, the results imply that during ICL the representations in the earlier layers of the network are more aligned with the subject partitions and that during SFT representations of the later layers are better aligned with the answer/label distributions of questions. This finding showcases the distinctions between ICL and SFT from the perspective of the representation landscape.
- The experiments on the hierarchical organization of the cluster density peaks via linkage clustering with respect to the subject relationships show how information in the internal model layers are organized.

Weaknesses:
- My main issue with this work is that the experimental analysis is only conducted on the MMLU benchmark. This detrimentally restricts the impact of the work, especially as the authors draw general conclusions. Is it possible to extend some of this experimental analysis to other benchmarks, perhaps just even to other question answering based benchmarks?
- Can the authors provide more details on the average linkage experiments that demonstrate the hierarchical organization of the cluster density peaks? For instance, how do the authors map the subjects at a granular level to each of the dendograms/leaves? Overall, I believe the subsection ""Hierarchical organization of density peaks"" can be further improved in terms of writing and readability for future readers.
- There are a number of typos throughout the paper that need to be corrected. For example, the authors write ""LLama"" in a number of places although it should read ""Llama"" (e.g. line 86), ""assignation"" -> ""assignment"" on line 140, among others. These can be fixed in the revision.

Limitations:
The authors discuss limitations sufficiently. Note that owing to the strengths and weaknesses mentioned above, the paper's contributions currently constitute a technically solid, moderate-to-high impact work, which forms the basis for my score.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
AO5MjDuHpr;"REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
CcNw4mVIxo;"REVIEW 
Summary:
This paper proposes to use spiking neural networks (SNNs) to slice the event stream in an adaptive manner before passing the voxelized events to the downstream inference model. The first step of the proposed method divides the input event stream into voxelized event cells with the same temporal interval. An SNN, constructed to have a scalar output, then takes the event cells recurrently. The timestamps when the SNN generates spikes are considered to be the slicing positions. The slicing SNN and the downstream inference model are trained together using the membrane potential-driven loss and the linear-assuming loss. The feedback-update strategy allows the two networks to be trained end-to-end. Extensive experiments on a toy example, object tracking, and recognition demonstrate that the proposed method can be easily integrated into existing models, bringing a noticeable performance improvement.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper focuses on an interesting yet underexplored problem, which is to use a data-driven model to adaptively construct the event voxels. The proposed method is intuitive, and the key idea is convincing. It is clear that the authors have put in great effort in preparing this submission.

2. The key technical contribution involves two parts. First, the feedback-update strategy allows supervision signals from the downstream ANN to back-propagate to the SNN. Additionally, the membrane potential-driven loss and the linear-assuming loss control the spiking time through the supervision of the membrane potential value. The two parts complement each other, leading to an end-to-end trainable model.

3. Additionally, the paper also discusses how the hyperparameter $\alpha$ can be tuned together with the SNN weights and analyzes the implication of different $\alpha$ values to the spiking behavior. 

4. The experiments are very extensive. The SpikeSlicer has been validated on several event-based applications, demonstrating its prediction quality, efficiency, and the fact that it can be easily incorporated into existing models.

Weaknesses:
1. Despite the strengths above, the key design appears to be a bit simple. As a potential NeurIPS paper, this work is relatively weak on the technical sophistication and theoretical insights. However, this is complemented by extensive experimental evaluation and empirical analysis. 

2. While SNNs are efficient and consume less energy than ANNs, SNNs are also less capable than ANNs. Since the speed of the entire SNN+ANN prediction pipeline is going to be slow anyway, it may be worthwhile to investigate whether using an ANN as an event slicer can lead to better prediction quality.

3. While the proposed losses are justified by proposition 1 and empirical analysis, it is unclear if the proposed feedback-update strategy is the best way to identify the desired trigger time $n^*$. In particular, it is unclear if the argmin operator can return any meaningful signal during the initial training stages.

Limitations:
Yes, the authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proposes a novel method for adaptively sample event data and subsequently preprocess it, utilizing a spiking neural networks (SNNs) as module.

The sampling method involves a feedback mechanism that triggers the activation of the SNN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Tests are done on dataset with different lighting conditions. Being robust to different event rates.

Weaknesses:
The experiments conducted do not contain tasks such as optical flow, object detection, or image reconstruction. The type of tasks tested is limited.

Limitations:
The implementation of the code appears to be challenging, which may affect its reproducibility.
The application of this algorithm in embedded systems seems to be constrained due to the use of Spiking Neural Networks (SNNs).
Furthermore, it is unclear whether the code will be made publicly available.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors designed a plug-and-play event processing method, SpikeSlicer, to split event streams with an adaptive amount. The proposed method is a lightweight SNN, constrained by a custom Spiking Position-aware Loss (SPA-Loss) to regulate neuron states. Additionally, a downstream ANN refines the slicing decisions using a feedback-update training strategy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed plug-and-play event processing method, SpikeSlicer, based on SNN representation, which can be used for various vision tasks with event cameras. 
* The downstream ANN refines the slicing decisions using a feedback-update training strategy, and the performance of the downstream ANN provides feedback to adjust the representation.
* Experimental results demonstrate that SpikeSlicer can effectively enhance the performance of object tracking and recognition with event cameras, while also leveraging the advantages of neural computation in processing speed and power consumption.

Weaknesses:
* The comparison algorithm for event-based object tracking, DiMP, is from 2019. Why not try the latest methods? In recent years, many studies have focused on improving the effectiveness of event stream representation to enhance the performance of event vision tasks. 
* Many methods for object detection and tracking with event cameras have not been compared.

Limitations:
There is not much discussion on this aspect.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Nb5xlelV0C;"REVIEW 
Summary:
This paper proposes a training-free method for generation interpolation of diffusion models with attention manipulation. Targeting on the layout transition inconsistency and nonuniform step-wise transition, the paper proposes to extend the attention interpolation from previous cross-attention to self-attention, and adopts a beta distribution to ease the nonuniform transition. Additionally, three metrics are proposed to evaluate the interpolation qualities, involves consistency, smoothness and fidelity. Experiments show the effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The beta distribution for interpolation weights instead of linear weights seems promising.
2. The proposed metrics seem sound to evaluate the interpolation quality.
3. Experiments show the effectiveness of the proposed attention interpolation.

Weaknesses:
1. The application of the generation interpolation seems somewhat restricted, and the practical meaningness of the task is doubted, is the interpolation could benefit other generative applications, or isolated. 
2. Basically, the manipulation of the attention map has been broadly explored for image editing, such as [Masactrl; ICCV 2023], [InfEdit; CVPR 2024], etc., and the self-attention corresponding to the layout and the cross-attention corresponding to the semantic are basically common sense, the proposed AID adopts a similar fashion with reformulated interpolation task, where the contribution is somewhat insignificant.
3. The discussion with training-based interpolation methods is suggested to be provided, such as [UniHDA; 2024].

Limitations:
The authors have adequately discuss the limitations and broader impacts of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Summary and Contributions: The authors introduce a novel task called conditional interpolation, which is to generate interpolation images with various conditions like text or pose. They propose an attention-base (and prompt-guided) method to achieve conditional interpolation, and three evaluation metrics to assess the consistency, smoothness, and fidelity of generated images. Extensive experiments shows their method achieve the best performance in image interpolations of diffusion models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The propose task(Conditional interpolation) is unexplored and interesting. Comom image interpolation task focus on generate transition images between two real-world images, but limit to one condition. The author introduce both text-embedding and attention mechanism to achieve better performance. 
2. The authors do a detail analysis of conditional interpolation, specifically text-embedding interpolation. They prove that text-embedding interpolation is equivalent to manipulating the keys and values in cross-attention module and find that doing similar operation in self-attention layer can significantly improve spatial consistency. 
3. The method can be used in image interpolation with distinct conditions like “a truck” and “a cat”. The authors also show their method can improve image-editing results when use editing methods like p2p.
4. The authors introduce a third prompt to guide the interpolation between two prompts, which is interesting and useful.

Correctness: 
the claims and method are correct.
Clarity: 
The paper is well written
Relation to Prior Work: 
The paper is clearly discussed how their work differs from previous contributions.

Weaknesses:
1.The paper lack of a clear and detail definition of the conditional interpolation. The authors claims that they formulate a new task call conditional interpolation, which is doing interpolation under various condition, such as text and pose. But I am confused that the method they proposed only condition on text, how could it be various condition? What the definition about various condition? 
2.The comparations between baselines are inconsistent. The table 1 shows the result of TEI and DI, but the table 2 only show TEI.
3.The qualitatively compare between baselines（TEI and DI）is lack, while the quantitative result of this two baseline are exist.

Limitations:
see weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose Attention Interpolation via Diffusion (AID), a novel, training-free technique for improving image interpolation under specific conditions like text or pose. Traditional methods using linear interpolation often produce inconsistent, low-fidelity images. AID enhances image consistency and fidelity with a fused interpolated attention layer and selects interpolation coefficients using a beta distribution for smoother results. An advanced variant, Prompt-guided Attention Interpolation via Diffusion (PAID), treats interpolation as a condition-dependent generative process. The authors include the experiments to demonstrate AID's consistency, smoothness, and efficiency in condition-based interpolation. The work also includes user study to show AID better aligns with human preferences and aids compositional generation and image editing control.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work includes user study to compare performance of different models which is appreciated. 
2. The work is motivated to improve existing image interpolation methods based on their drawbacks.

Weaknesses:
1. The work proposes a few metrics to evaluate the quality of interpolated images. However, these metrics could be biased and fail to evaluate the quality of samples. More details are discussed in Questions. 
2. Though image interpolation can generate interesting visual effects. I feel the actual applications for such technique could be limited.

Limitations:
The work sufficiently addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper the authors explore the task of interpolation between images in conditional diffusion modal. First the authors list out the three desirable properties of successful interpolation: perceptual consistency, smoothness, and image quality. The authors first introduce a method AID that incorporates three ideas: (1) interpolation should be done with both cross attention and self attention, (2) instead of a simple interpolation a fused interpolation should be performed, (3) the interpolation coefficients are sampled with a beta distribution instead of sampling uniformly.

The authors also introduce a second variation of their method called PAID (Prompt guided conditional interpolation) where the users can optionally add a prompt representing the intermediate image.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and well organized. The authors do a good job first analyzing the issues with interpolating just the text prompt and then proposing a method that addresses them.

- The visual results shown in the paper look impressive and the authors show that the method can also be applied to other tasks like image editing and compositional generation.

- The authors use a comprehensive set of metrics that capture the 3 different aspects of good interpolation and show that the proposed method is helpful for each of the three metrics.

Weaknesses:
- One aspect of diffusion models that the authors have not considered here is the classifier free guidance and the use of negative prompts, which is standard practice for generating high quality images. Appendix H in supplement mentions that there are some results shown with negative prompt. But it would be useful to have a more detailed discussion of this in the main paper.
- The authors show an ablation study in Table 1(b). However the discussion for these experiments is very brief. It would be useful if the authors could show some visual results corresponding to these ablation experiments and a more thorough discussion.

(Minor):
- “text-to-diffusion” → “text-to-image” L94

Limitations:
The authors have included a limitations section in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
AAN46kUPXM;"REVIEW 
Summary:
In this paper, the author proposes ""Expressiveness,"" a metric that measures the dissimilarity of feature maps produced by different filters. Subsequently, the author introduces NEXP, a technique to prune filters based on their expressiveness. The proposed method is tested on tasks such as image classification and object detection.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The overall structure of the paper is clean and easy to follow.
2. Although I am not very familiar with the structured pruning literature, the application of the concept of representation power of neural networks in this field appears novel.
3. The experiments are comprehensive, including pruning at initialization (PaI), pruning after training, and other tasks like object detection.

Weaknesses:
1. The notation is very confusing. For example, in the section 'Generalization of concepts at a structural level,' $\ell$ is the index of a certain layer, but the upper-case K is the total number of layers, and the lower-case k is the index of a filter/channel in a certain layer.
2. The concept of expressiveness is not new in the context of pruning [1] and neural architecture search [2,3,4].
3. The performance improvement by NEXP is not prominent. For example, in Tables 1 and 2, NEXP often shows a higher compression ratio but lower accuracy. This makes it unclear if NEXP offers a significant advantage over other methods.

***
**Minor Mistakes:**

Line 163: ""where k the is"" should be corrected.

[1] Tanaka, Hidenori, et al. ""Pruning neural networks without any data by iteratively conserving synaptic flow."" Advances in Neural Information Processing Systems 33 (2020): 6377-6389.

[2] Lin, Ming, et al. ""Zen-NAS: A zero-shot NAS for high-performance image recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[3] Wang, Haoxiang, et al. ""Global convergence of MAML and theory-inspired neural architecture search for few-shot learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. ""Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective."" arXiv preprint arXiv:2102.11535 (2021).

Limitations:
The authors have discussed limitation in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on weight pruning for CNNs. It proposes an evaluation metric, i.e., ""expressiveness"", to evaluate whether a neuron/groups of neurons should be pruned or not. The metric focuses on the neurons' ability to redistribute informational resources. As the evaluation of expressiveness requires data samples, the paper includes studies on arbitrary data or limited dataset’s representative samples. The experiments are conducted for image classification tasks on ResNet architectures, and the object detection task on YOLOv8m.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Weight pruning is an effective manner in reducing the redundancies in DNNs. Instead of focusing on weight importance, this paper considers the expressiveness of neurons in the information flow within a network. The proposed evaluation metric can also be combined with existing strategies with importance evaluation metrics as a hybrid pruning approach.

Weaknesses:
1. Limited practicality of the approach. The method is mainly focused on removing redundant filters from CNNs. Though CNN is one category of DNNs, recent works have shifted to more advanced model architectures such as transformers and Mamba, which are mainly composed of FC layers instead of CNNs. The practicality of the approach is highly restricted to SOTA model architectures for image classification tasks.   

2. Limited performance improvements. This is a major concern. The performance gain of the proposed method is not obvious compared with baselines. For instance. on CIFAR-10 VGG-16, SCP and reduce the parameters 15.28$\times$ with a 93.85\% accuracy while the proposed method can only reduce the parameters 5.62$\times$ with a slightly better accuracy 93.87\%. HRank also provides better performance than the proposed method with higher accuracy 93.96\% (0.09\% higher than NEXP) and higher reductions of FLOPs (4.26$\times$ (HRank) v.s. 4.01$\times$ (NEXP) ). On DenseNet-40, Hrank also shows better performance across all metrics. Hrank v.s. NEXP: Accuracy 95.05\% v.s. 94.64\%, parameter reduction 3.31$\times$ v.s.3.12$\times$, FLOPs reduction 3.38$\times$ v.s. 2.51$\times$.

Limitations:
Please refer to weaknesses and questions. The major concern is that the method does not show better performance than baselines, and is also limited in model architectures.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new structured pruning approach NEXP. It works by computing the dissimilarity score of the feature activations across samples and removing those filters with smaller variances. Experimental results on several models and datasets demonstrate the effectiveness of the proposed approaches.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written with clear motivation and many of the important technical details are included.  
2. The authors have presented the experimental results well and the additional discussion provides deeper insights on the effectiveness of the proposed methods.  
3. The authors evaluated models beyond image classification, i.e., the proposed methods work well on YOLOv8 object detectors.

Weaknesses:
1. The conclusion section is too short and fails to characterize the main contribution of this paper. What does it mean as to “when” and “how” to prune? The authors should elaborate on these further.   

2. I think that this is not a scalable approach for computing the pruning metrics. The pruning metric proposed in the paper requires computing a N by N matrix for each filter in the network, where N is the number of samples. This could grow quite computationally infeasible for large networks and batch sizes. The authors also fails to discuss this aspect on the pruning efficiency in the paper.  

3. In terms of experiments, I am not sure why the authors compare each methods under different compression ratio? If the pruned models in each method have different parameters, it can be hard to compare the accuracy numbers.  

4. I feel a lot of the content in section 3 is not necessary and they can could go into a separate preliminary section. Section 3.1 and early parts of Section 3.2 takes up a lot of space and in the meantime do not provides us with the motivation and insights for the later introduced methods.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to handle network pruning problem. Specifically, it proposes to use a new importance measurement, called expressiveness, to decide the pruning process. It jointly considers the model state to leverage on the proposed measurement. In addition, it can also combined with typical importance based pruning methods to improve the model efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Network pruning is a valuable research direction to study on, espeicailly for current large-scale era where efficiency matters a lot.
2. The proposed new metric to measure the network importance is interesting.
3.  Empirical results show the method superiority.

Weaknesses:
1. Adding more discussion in the conclusion part helps to improve the paper readability.
2. Since this paper proposes a new pruning metric, it is better to show more visualization and network behavior analysis to illustrate the intuition.
3. The compared baseline models are relatively old, adding more recent publications helps to support this paper.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
SQicD307Oh;"REVIEW 
Summary:
The paper proposes a black-box approach to make any no-regret algorithm to a state-free algorithm. The topic this paper works on is a very important topic. The theoretical results in this paper are significant, if correct. That said, the algorithm description is hard to follow, and hence, I could not verify if the results are correct or not.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
Introduction and Related Work are both well-written and useful to understand what the issue in current RL theory is. The topic this paper works on is a very important topic. The theoretical results in this paper are significant, if correct.

Weaknesses:
That said, the algorithm proposed in this paper is not easy to understand. I tried to understand it by reading its description multiple times, but still I do not really understand it. It is because there are some undefined symbols and ambiguous definitions. For example
- What is $π^\perp$?
- What do ""compatible with $\Pi$"" and ""it extends the pruned policy $\pi^\perp_t$ to $\pi_t$"" in Line 185-187 mean?
- What is the initial set of $\mathcal{S}^\perp$?
- What are inputs and outputs of $\mathrm{ALG}$?
- Figure 1 is not really helpful to understand how the pruned space, dual trajectory, etc are obtained.

**Minor Comment**
- A paper by [Fruit et al. (2018)](https://arxiv.org/abs/1807.02373) seems related in that it also find unreachable states. I think it should be cited.

# After rebuttal review
I read the paper again and now understand the algorithm. The contribution of the paper is significant, and I highly recommend acceptance to the AC.

Limitations:
The paper is mostly theoretical, and it has no potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a kind of parameter-free reinforcement learning where the algorithm does not need to have the information about states before interacting with the environment. To achieve this, the authors design a black-box reduction framework which can transform any existing RL algorithm for stochastic or adversarial MDPs into a state-free RL algorithm. The paper focuses on establishing the theoretical regret bound of such a black-box approach.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper proposes an interesting problem and a black-box framework for transforming any existing RL algorithm into a state-free algorithm.  The theoretical analysis seems sound and the technique may be of interest for the theoretical community.

Weaknesses:
While I understand that the focus of this paper is on theories, it could still be informative to include some toy experiments. For example, how would a non state-free algorithm behave if given ""wrong"" or insufficient knowledge/estimates of the state space, and how would the corresponding state-free algorithm (via the reduction) behaves.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of online reinforcement learning in the tabular setting when no prior knowledge about the size of the state space is available. Unlike existing algorithms, that usually require the size S as input parameter, the proposed algorithm is fully adaptive and its final performance scales with the set of reachable states.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The algorithmic solution is quite elegant since it can be applied to any ""basic"" RL algorithm with regret guarantees.
* The final result achieves the desired removal of the dependency on S, which is replaced by the size of the reachable states.
* The result holds for both stochastic and adversarial settings and it can be extended to removing the dependency on the horizon H as well.

Weaknesses:
* I would encourage the authors to provide a clean comparison of the final bounds in the stochastic setting with the best available bounds. In particular, I'm wondering whether the restart leads to extra log terms.
* Related the previous point, I suggest the authors to make explicit the bounds for simple doubling trick strategies, so as to have a point of comparison.
* What is exactly the role of epsilon? It looks like it can be directly set to 0 and everything works the same.

Additional references of interest

* “Layered State Discovery for Incremental Autonomous Exploration” https://arxiv.org/pdf/2302.03789 This paper extends the seminal work of Lim and Auer on “Autonomous exploration” where the state space is possibly infinite (countable). In this paper, the authors managed to resolve an issue in the original paper and removed any dependency on the total number of states, where making the bound completely adaptive to the set of reachable states. Given the similarity between finite horizon and bounded distance exploration, I wonder whether there is any connection to draw between these two works. My impression is that there is quite a strong resemblance between the concept of pruned states and L-reachable states. The main technical difference is that in autonomous exploration the agent needs to explicitly restart to avoid getting “lost” in long episodes, whereas in finite horizon, the reset naturally happens each H steps.
* “Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes” https://arxiv.org/abs/1807.02373 This paper considers the case where the state space is somewhat “misspecified” (i.e., the set of available states is actually larger than the set of reachable states). In this case, the authors still require knowing the possibly large set of available states, so I’m referring to this paper more for completeness than for strict comparison.

Limitations:
See above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
oMHpejyGdx;"REVIEW 
Summary:
The paper presents Prompt-Agnostic Adversarial Perturbation (PAP), a method to enhance privacy and security in customized text-to-image diffusion models like Stable Diffusion.  These models, while enabling high-quality image synthesis, pose risks of privacy breaches and unauthorized artwork usage.  Existing adversarial methods, limited by their reliance on specific prompts, fail with unseen prompts.  PAP addresses this by modeling the prompt distribution with a Laplace approximation, generating robust perturbations effective against diverse prompts. Experiments on datasets such as VGGFace2 and Wikiart show PAP's superior performance and robustness.  This method provides a significant improvement in protecting images from unauthorized use and manipulation in diffusion models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. I appreciate the idea of designing a prompt-agnostic adversarial perturbation for privacy protection.
2. The method is concise and paper organization is logical.
3. The formulas, tables, and figures in the paper are easy to follow.

Weaknesses:
1. Although the motivation for designing a prompt-agnostic adversarial perturbation is reasonable, the method seems too complex. That is, maybe we can expand the prompt with LLM to get a lot of prompts (different scenes) containing the keyword. Then we can use all these prompts together as a set to calculate a gradient for adversarial attack at each iteration.
2. For definition 3.1, the assumption of Z = p(x0, c0) as a constant lacks evidence.

Minor suggestion:

1.	For tables 1, 2, 3, and 4, there are some left parentheses that have no space between the previous metric names.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel adversarial training method for text-to-image diffusion models, enhancing robustness against prompt-agnostic attacks. Specifically, the authors utilize prompts (embeddings) from a prompt distribution rather than a specific prompt for adversarial training. The proposed method is evaluated in the contexts of face privacy and artistic style protection.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper aims to bridge the gap between prompt-specific protection and prompt-agnostic protection, which is essential for text-to-image diffusion models.
2. The proposed method is principled and effective.
3. The empirical evaluation is comprehensive and convincing.

Weaknesses:
See Questions.

Limitations:
The authors discussed the limitations in Appendix H.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- This work is about a method to craft perturbation images to protect users/artists from personalized text to image diffusion methods (specifically DreamBooth and Textual inversion), that generalize better to unseen prompts than previous works.
- The core algorithm is ""Prompt-Agnostic Adversarial Perturbation"" (PAP). Instead of crafting the perturbation with a fixed condition prompt, PAP model the prompt distribution as a Gaussian dist and sample prompt embedding from it during the perturbation crafting process.
- The prompt distribution is modeled as a Gaussian using Laplace approximation. The mean is estimated by minimizing the diffusion loss starting from a reference prompt. The variance is approximated using a simplified formula based on the difference between the reference and estimated prompts, and their respective loss values.
- Experiments show PAP outperforms previous prompt-specific methods on metrics like FID, CLIP similarity, and LPIPS across different datasets (CelebA-HQ, VGGFace2, Wikiart) and generalization to unseen test prompts.

(after rebuttal)
The authors addressed my main concerns by including results from newer methods with PAP, and clarifying the inconsistent implementation. Despite some issues regarding efficiency and application to more recent personalized techniques/model, I've increased my score from 4 to 5.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is generally well-written, with consistent improvement on unseen prompts when compare with previous works.
- PAP is simple and easy to use, not very costly to add on top of other protection method
- The explanation of the method is easy to understand
- The paper also includes performance under DiffPure, and the result under DiffPure seems to be positive
- The supplementary is informative, with extensive evaluation

Weaknesses:
The experimental evaluation need to be improved:
- Apply PAP to other methods like AdvDM + PAP, not just Anti-DreamBooth ASPL, and other recent method such as Diff-Protect [1] to better demonstrate that PAP is a good plug in to the current methods.
- Lacking a simple baseline of adding Gaussian noise to $c_N$ to show the value of approximating H.
- Questionable metric choices, as using LAION aesthetic score for Celeb-HQ and VGGFace2 datasets is not well-justified for face images. And images generated from protected model often contain high-frequency and colorful patterns, which may lead to unreliable LAION aesthetic scores.
- Also evaluation need to include identity score (as used in Anti-DreamBooth) for face datasets, which is crucial to ensure generated images don't contain user's identity.
- A suggestion would be to add an assessment against recent encoder-based personalized techniques, such as InstantID [2], for a more comprehensive comparison.

Limitations:
Since PAP still build upon anti-dreambooth, 4-6 minutues A800 when crafting a set of images is still very impractical. Again, I want to see performance of PAP on top of more recent/efficient methods

[1] Xue, Haotian, et al. ""Toward effective protection against diffusion-based mimicry through score distillation."" The Twelfth International Conference on Learning Representations. 2023.

[2] Wang, Qixun, et al. ""Instantid: Zero-shot identity-preserving generation in seconds."" arXiv preprint arXiv:2401.07519 (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a prompt-agnostic adversarial perturbation (PAP) method for customized diffusion models. They first use Laplace approximation to model the prompt distribution. Then they derive the attacks by maximizing the disturbance expectation. Extensive experiments on three datasets validate their performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Experiments on three datasets.

Interesting topic.

Mathematical Proof.

Weaknesses:
Potential semantic gap between the estimated prompts and natural language.

Experiment Scope: only do experiments on SD 1-5.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
kOMrm4ZJ3m;"REVIEW 
Summary:
This paper presents a method using sequence-to-sequence transformers to find Lyapunov functions for dynamical systems, mainly polynomial systems. Even though it illustrates some new advances in LLMs in solving mathematical problems, the reviewer doubts the mechanism and the necessity of using large models such as transformers to learn Lyapunov functions for simple polynomial systems, as well as the fairness of the comparisons made in the paper. This work could be a very interesting and good workshop paper to show the advances of LLMs, but the reviewer doesn't think there are significant academic contributions to the math community.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
This paper finds a way to explore or show the capacity of transformers, or LLMs in general, to solve existing mathematical problems, that is, finding the Lyapunov functions for nonlinear dynamical systems in this paper. Also, the backward data generation method is interesting.

Weaknesses:
Even though the authors tried to make the story interesting and appealing, it seems that a deeper understanding on Lyapunov stability analysis is needed when improving this paper in the future. For example, Lyapunov functions are useful for providing stability guarantees for equilibrium points for dynamical systems, but there are no such equilibrium points in the three-body problem (line 29). Please see the introduction of chapter 3, Lyapunov stability in 'Nonlinear systems' by Khalil, H. K., one of the references of this work. 

Moreover, it seems that the authors didn't understand the stability concept fully when writing this paper. In line 27, they stated that the goal is ""discovering the Lyapunov functions that control the global stability of dynamical systems"", while in Def. 3.2, they had the definition of 'stable'. Stable and globally stable are different, which should be precise as an academic paper. Also, that's different from asymptotically stable. Meanwhile, 'control' is definitely not the correct wording here.

Last but not least, finding a Lyapunov function (candidate) for a nonlinear system is not a very hard problem now, even for high-dimensional (polynomial) systems, but how to verify it is indeed a valid Lyapunov function satisfying the Lyapunov conditions is the bottleneck, which is difficult to address for non-polynomial or high-dimensional systems. As shown in the paper ['Neural Lyapunov Control'](https://papers.nips.cc/paper_files/paper/2019/hash/2647c1dba23bc0e0f9cdf75339e120d2-Abstract.html), one can easily use a one-hidden feedforward neural network with 6 hidden neurons to learn a Lyapunov function for many 2- or higher-dimensional nonlinear systems which are not necessary to be polynomials. That said, the results shown in this work are not impressive at all, and the Lyapunov function candidates they found using transformers still lack correctness, which needs to be verified against the Lyapunov conditions, given a new nonlinear system.

Limitations:
The fairness of the comparisons and the correctness of the learned Lyapuonv functions, as discussed above. Also, even though the authors claimed that their methods worked well with non-polynomial system, the comparisons and examples are mainly for polynomial systems.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors propose a new method of generating synthetic training samples from random solutions, i.e. Lyapunov functions for dynamical systems in the forms of ordinary differential equations. They demonstrate that the sequence-to-sequence transformer training on such data can produce Lyapunov functions better than both existing algorithmic solvers and humans. More speficially, for polynomial systems, the method proposed in the paper can find Lyapunov functions for 10.1% of the randomly generated polynomial systems where the state-of-art algorithm can only find 0.7%, furthermore, the method can produce a Lyapunov function for 12,7% of randomly generated non-polynomial systems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem of Lyapunov function problem is an important problem. The performance of the method proposed in the paper is impressive and promising.

Weaknesses:
The novelty of the data generating methods, those described in Sec. 5, is not clearly stated and explained. Currently, it reads as if the results are obtained accidentally.

Limitations:
As I have pointed out, the paper does clearly describe the difference between what it proposes to some of existing methods, thus make it quite difficult to see the main reasons for the rather remarkable experiments results.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper trains a transformer to predict Lapunov functions for both polynomial and nonpolynompial dynamical systems. They generate a dataset combining ""forward-generated"" problems — solutions discovered from existing solvers on randomly generated problems — with ""backward-generated"" problems — problems generated by generating a random solution, and then building a function solved by the solution. The trained transformer achieves great performance. On a test dataset of randomly generated problems, it solves 12% of problems, compared to 1% solved by the state of the art solver. In addition, the model solves 84% of 75 problems in the FSOTOOLS dataset where human experts only solved 9.33%.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper is well-written, and clearly described, so that I could understand it even without having previously encountered Lyapunov functions. 
- The related work section is good
- The dataset generation is very carefully described, and lots of important design decisions are included. There is even more detail in the Appendix. Despite not being an expert, it seems like the authors really know what they're doing, and have rigorously tested different interesting choices of what to include in the dataset. 
- The results are really cool! The authors test out of domain generalization well, by showing performance on a randomly generated dataset. Indeed, some of the models evaluated were only trained using ""ground truth"" solver solutions from a different randomly generated dataset. But  the model gets 10x higher performance than the ground truth solver on this random dataset.  
- The big question for papers like this is how well the models generalize out side the train set. Due to the difficult nature of the problem, it's hard to have really good test sets, but the authors are aware of the importance of this, and do lots of OOD generalization tests of their systems, and report positive results.
- Again, the authors are quite thorough with their documentation and evaluation of their system. The paper feels very polished and professional and the results are impressive.
- Progress on this problem could lead to other work using deep learning for mathematics.

Weaknesses:
1. The algorithmic approach is not new — training a transformer to do math problems that a large dataset can be generated for — but this is okay given the careful effort needed and given to the dataset creation process, and the importance and performance of the problem achieved. 

2. I find all the different dataset variants included to be a bit confusing. The paper could be easier to read with a reduced the number of variants presented in the main section of the paper. 

3. I think the discussion section goes a little far in making suggestions about LLM reasoning ability given success on this dataset. The sentences are true, but it's not necessarily true that performance here is equivalent to reasoning. the transformers are probably doing sometihng more like ""intuition"" given the amount of data they are seeing and the quickness at which they generate an answer.

Limitations:
The limitations are addressed well in the discussion section.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study addresses the problem of designing the Lyapunov function of dynamical systems via learning. The existence of the Lyapunov function is a sufficient condition of the dynamical system's stability. However, its design is not established except for systems with sum-of-square polynomials. The Authors propose a novel dataset construction framework on which Transformer models can be trained. Extensive experiments demonstrate that Transformer models successfully learn to design Lyapunov functions on several types of datasets with moderate generalization ability across datasets. It is also shown that the injection of out-distribution samples in the training set significantly boosts the success rate. Yet restricted to small systems, this study suggests that the learning approach is a promising direction for addressing hard problems of symbolic mathematics.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This study clearly presents the problem and the approach. Overall, the writing is clean and accessible to diverse readers. 
- A novel backward dataset generation algorithm is proposed to train Transformer models for Lyapunov function design. 
- The experiments validate the proposed approach from various aspects. Particularly, the generalization across datasets is tested well.

Weaknesses:
I appreciate the overall contributions of this study. I'd like to raise two weaknesses, one for the methodology (backward generation) and one for the experiments. 

**Major comments**

Backward generation. While the Author claims that the backward generation is the key innovation, the explanation of the generation process is very limited in the main text. I understand the restriction of pages, but it would be better to elaborate on the method a little more to give intuition to the readers. While the details are presented in Appendix A.2, it mainly provides the procedures, and the rationale is not much given. I was not able to fully understand the procedures either because of the unclarity of several notations (see Minor comments below). Plus, many mapping and transformations appear, but their intention and necessity are unclear to me. I encourage the authors to discuss the range of the Lyapunov function and $f$ covered by the proposed generation method and whether this is large enough or not, including, if any, the important class of functions that cannot be covered for now.

As for the experiments, assuming a practical use, it is important to know whether the Lyapunov function given by Transformers is really the Lyapunov function of the targeted system, and if not, whether this is because of a simple failure or the existence of such function. Discussion and experiments seem missing in this part. 

**Minor comments**

*(Markdown math rendering somehow fails in the first item below, so I wrote it in a bit tricky way.)*
- Some notations are unclear and inconsistent. For example, vectors $\mathbf{z}$ and $\mathbf{z}_{j}^{i}$ are in bold font, while other vectors such as $x$ are not. The subscript notations $A$ [line 177] 
and $B$ in Eq. (10) are not defined. Assuming that it refers to the $i$-th entry of vector and $C$ is $D$, the first term of the equation in [line 177] appears to be a scalar while the second term is a vector. The Authors may represent a vector by $E$ in the first term, but still, this is a row vector while the second term is a column vector. 

$A = (\nabla V)_i(x)$

$B = (\nabla V(x))_{\tau_2(i)}$

$C = (\nabla V)_i(x)$

$D = (\nabla V(x))_i$

$E = (h_{\pi(i)})_i$

- Forward/backward methods and Forward/backward generations are both used. I recommend that the Authors keep terminology consistent.
- [line 183] and the is --> and there is 
- [line 224] .In -> . In
- [line 489] What does $Id$ mean (the identity map?)?
- [line 492] ""$g$ is positive"" souds a bit wierd as $g$ is a function.
- [line 489] Written $g= 1$ but $g$ is supposed to be a function.
- [Eq. 10] Does $\mathbf{e}^i_j$ refers to the $j$-th entry of $\mathbf{e}^i$? Then, it should be unbold as it is a scalar.

I also encourage the Authors to include more related works that exploit Transformers to address hard math problems. 

**Shortest Vector Problem (a series of works by the same group)**
- ""SALSA: Attacking Lattice Cryptography with Transformers,"" Emily Wenger, Mingjie Chen, François Charton, Kristin Lauter

**Gröbner basis computation**
- ""Learning to Compute Gröbner Bases,"" Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, Kazuhiro Yokoyama

If not restricted to Transformers, 

**Detection of terminal singularity**
- ""Machine learning detects terminal singularities,"" Tom Coates, Alexander M. Kasprzyk, Sara Veneziale

**Integer programming**
- ""Machine Learning for Cutting Planes in Integer Programming: A Survey,"" Arnaud Deza, Elias B. Khalil

Limitations:
Limitations are adequately presented. For the potential improvements, see Weakness and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
5jRU8ufi8H;"REVIEW 
Summary:
The paper proposes token-level generalization bounds for large language models (LLMs), such as LLaMA2-70B, using less restrictive compression techniques like Monarch matrices, Kronecker factorizations, and post-training quantization. The authors argue that traditional document-level bounds are vacuous at this scale and introduce a method leveraging martingales for deriving tighter bounds, which not only hold theoretically but are also demonstrated through empirical validation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Originality**: The paper introduces a novel approach to computing generalization bounds at the token level, which is a significant departure from the document-level bounds prevalent in prior works.
2. **Technical Soundness**: The use of martingales and non-restrictive compression methods to derive generalization bounds is both innovative and robust, providing a solid theoretical framework backed by empirical results.
3. **Significance**: The ability to provide non-vacuous generalization bounds for LLMs as large as 70 billion parameters is highly significant, as it pushes the boundary of what is understood about LLM generalization in practical settings.
4. **Clarity**: The paper is well-written, with clear explanations of the methods and their implications, making it accessible to readers who may not be experts in the specific sub-field of machine learning.

Weaknesses:
No major weaknesses

Limitations:
The paper lacks intuitive explanations for the proposed bounds, which might hinder understanding for readers not familiar with advanced statistical concepts in machine learning.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops nonvacuous generalization bounds for modern language models. Specifically, this paper proves a token-level generalization bound, and applies different techniques (LoRA, 2 Kronecker Product, Monarch Matrices, and post-training quantization) to control the capacity of model class.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper is well-written: Theorem 3.1 is clean, and many compression techniques (parameter-efficient tuning, post-training quantization,etc.) and modern language models (GPT, LLAMA, etc.) are analyzed. I believe these are nice contributions.

Weaknesses:
The main weakness in my opinion is the left-hand side of eq. (2), since it uses contexts from the training data. I agree it is still a meaningful result, but it is also a little hard to interpret. Line 190 claims that ""This figure confirms our intuition that the next token distribution is particularly diffuse at the beginning of a sentence, while it decreases for later tokens but remains relatively high. Given how diffuse the distribution is and the large number of possible sentences, it is broadly infeasible to make predictions on new resampled tokens from the empirical distribution alone."" I am not fully convinced by this, since we can add some fixed prompt and only measure generalization error for the generated part.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel approach that computes non-vacuous compression-based generalization bounds for LLMs at the billion-parameter scale. Prior works could only achieve vacuous bounds for these large-scale models and rely on the assumption of IID documents. By leveraging the vast number of tokens in LLM training sets and properties of martingales, the authors derive non-vacuous bounds for LLMs that generate high-quality texts. Further, they showcase the tightness of the bounds by examining compression schemes including Monarch matrices and Kronecker factorizations, and post-training quantization techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper tackles an important problem that aims to give guarantees on the generalization abilities of LLMs, which are getting more powerful but the good performance is extremely hard to interpret and assess. 
2. Prior works compute non-vacuous bounds on LLMs but rely on assumptions of IID documents and therefore can only be applied to those that generate poor text quality. This work presents a novel approach based on properties of martingales and gives much tighter bounds on LLMs of much more practical capabilities. Further, it does not require altering the pretraining pipeline of the LLMs being analyzed.
3. It investigates the generalizations by examining compression schemes including Monarch matrices and Kronecker factorizations, and post-training quantization techniques. The results also give interesting insights for practitioners.

Weaknesses:
1. Since the utilization of martingales is one main theoretical contribution of the work, I feel some background and proof sketch on how they are being used would be better included in the main text. 
2. The main models being examined are of the LLaMA and GPT-2 family of models. In particular, the experiment on the chat version of LLaMA is interesting as the generalization gets worse from the supervised finetuning. It would be interesting to see if this is generally true and why this is the case. More experiments on other finetuned LLMs would provide more evidence.
3. From table 2 and 3, it seems that given larger model sizes, the derived bounds get closer to random guess performance. Why is this the case and does it mean the bound would potentially be no-longer meaningful if the model gets large enough? Maybe I misunderstood something and would appreciate some clarification on this point.

Limitations:
The authors note the limitations of the current work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
8HeUvbImKT;"REVIEW 
Summary:
The paper introduces WeiPer, an post-hoc method for out-of-distribution (OOD) detection that leverages weight perturbations in the final layer of neural networks to enhance detection performance. 

Contributions:

1. Introducing linear projections of the penultimate layer by perturbing the final layer's weights to improve OOD detection.

2. Discovering a fingerprint-like nature of in-distribution (ID) samples in both penultimate and newly perturbed spaces, leveraging this structure for a novel detection method.

3. Proposing a KL-divergence-based scoring function and evaluating it alongside MSP and ReAct methods, showing state-of-the-art performance on near OOD tasks using the OpenOOD benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Simple methods with good empirical results. Technical novelty is limited but this is not a concern if it outperforms previous methods.

Weaknesses:
**Related Work Missing Strong Baselines: Data Depths and Information Projections**

Data depths and information projections are gaining significant interest in the OOD detection community. However, these approaches are notably absent in the related work section. It is essential to address these gaps to provide a comprehensive overview of the field.

Relevant works include:

M. Darrin. ""Unsupervised Layer-wise Score Aggregation for Textual OOD Detection.""

M. Darrin. ""Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data.""

M. Picot. ""Adversarial Attack Detection Under Realistic Constraints.""

M. Picot. ""A Simple Unsupervised Data Depth-based Method to Detect Adversarial Images.""

M. Picot. ""A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection."" TMLR 2023.

P. Colombo. ""Beyond Mahalanobis Distance for Textual OOD Detection."" NeurIPS 2022.

P. Colombo. ""Toward Stronger Textual Attack Detectors.""

**Lack of Strong Baselines for Empirical Comparison**

The current work lacks strong baseline comparisons, particularly those involving data depth methods. This omission undermines the robustness of empirical evaluations. Incorporating these baselines is crucial for a fair and thorough comparison with existing methods.

Key references that should be considered for baseline comparisons include:

E. Gomes. ""A Functional Perspective on Multi-Layer Out-of-Distribution Detection.""
M. Picot. ""A Simple Unsupervised Data Depth-based Method to Detect Adversarial Images.""
M. Picot. ""A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection."" TMLR 2023.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a component, WeiPer, that can benefit OOD detection. WeiPer adds random perturbations (sampled from standard normal) to the class projection weight vectors and essentially expands the output dimension (compared to the original logit space). WeiPer can be combined with multiple existing OOD detection scoring functions (e.g., MSP, ReAct). The authors further propose a KL-divergence-based scoring mechanism that works particularly well with WeiPer. Experiments show that WeiPer+KLD yields state-of-the-art results on the challenging OpenOOD benchmarks, including the near-OOD one on ImageNet-1K.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed WeiPer is to my knowledge novel. It also has generality since it can be combined with many existing OOD scoring functions.

2. Extensive analyses are presented along the introduction of the method, which well justifies each design choice and makes the underlying intuition/insight clear.

3. Most importantly, unlike many other papers that use arbitrary or easy OOD benchmarks for evaluation, this work demonstrates notable improvements on the challenging OpenOOD, especially with the ImageNet-1K near-OOD benchmark (with ~2% AUROC increase over the previous SOTA ASH).

Weaknesses:
A few weaknesses have been discussed by the authors in Sec. 4 and 5, e.g., WeiPer is less powerful on ViT and could induce higher memory consumption.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript proposes a post-hoc OOD detection method, which is broadly applicable and can improve existing OOD detection methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The methodology is post-hoc, making it more practical.

- Results for near OOD evaluation are promising.

- The method can be combined with existing OOD detection strategies and improve their performance, thus increasing the potential impact of this work.

Weaknesses:
Since memory usage is a limitation of the proposed methodology, the manuscript should present a comparison of computational cost (time and memory) between the multiple OOD detection methodologies that are considered.

Limitations:
The authors clearly stated the method's limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces ""WeiPer,"" a method that improves existing out-of-distribution (OOD) detection techniques by perturbing class projection weights. This method leverages the class-discriminative ability of pre-trained neural network classifiers by introducing weight perturbations in the final fully connected layer, creating richer representations of the input. The authors demonstrate that this technique significantly enhances OOD detection performance across multiple benchmarks, especially in challenging near-OOD scenarios. Additionally, a KL-divergence-based scoring method is proposed to utilize the properties of the augmented WeiPer space, supported by theoretical motivations and empirical observations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. WeiPer introduces a novel and effective technique for OOD detection by incorporating weight perturbations, broadening the scope of current methods.

2. The paper provides extensive experimental evidence demonstrating the superior performance of the WeiPer method across multiple benchmarks and includes detailed ablation studies to validate the contributions of each component.

Weaknesses:
1. WeiPer introduces additional computational complexity and memory requirements, which might limit its applicability in resource-constrained environments.

2. The proposed method involves several hyperparameters that need tuning, potentially increasing the difficulty of practical implementation.

3. Compared with other methods, the superior performance is not very stable. While the method performs well across several benchmarks, additional validation on a wider variety of datasets would further demonstrate its generalizability.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vBKoEZ1PG3;"REVIEW 
Summary:
This work proposes a novel video-language framework, HAWK, aiming at understanding video anomalies, which incorporates motion modality to enhance its capability. The work generates rich language descriptions for seven different video anomaly datasets, and also generates question-answer pairs to tackle potential user inquiries. The proposed framework demonstrates SOTA performance for video anomaly understanding and question-answering across multiple scenarios, which will advance the open-world anomaly understanding field.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work proposes a novel vision-language framework to address open-world video anomaly understanding, which is a very different pipeline from previous classification-based anomaly detection pipelines. To build a vision-language model for open-world video anomaly understanding, the work adopts seven different video anomaly datasets for training, where rich language descriptions and question-answer pairs are generated. Experiments for video anomaly understanding and question-answering across multiple scenarios demonstrate the effectiveness of the proposed framework. I believe the work will advance the field of open-world video anomaly understanding.

Weaknesses:
1. The work adopts Gunnar Farneback’s algorithm to obtain the motion modality. Is this algorithm efficient? 
2. The Ubnormal dataset consists of virtual anomaly videos. Do the authors consider the gap between real and virtual anomaly videos, which is mentioned in previous works [1, 2]?

[1] Ubnormal: New benchmark for supervised open-set video anomaly detection, CVPR 2022

[2] Generating Anomalies for Video Anomaly Detection with Prompt-based Feature Mapping, CVPR 2023

Limitations:
The paper has discussed the limitations and potential impacts of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The manuscript proposes a new framework that uses an interactive large-scale visual language model (VLM) to accurately explain video anomalies. The framework can explicitly integrate motion modalities to enhance anomaly identification. The manuscript also constructs an auxiliary consistency loss to guide the video branch to focus on motion modalities. The manuscript annotates more than 8,000 anomaly videos with language descriptions, enabling effective training in different open-world scenarios, and creates 8,000 question-answer pairs for users' open-world questions. The final results show that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question answering.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This manuscript proposes a novel video language framework HAWK, which aims to understand video anomalies, and combines motion modalities to enhance its capabilities.
2.This manuscript collects seven video anomaly datasets from different scenarios and generates rich language descriptions for each video. At the same time, considering the diversity of open-world questions, question-answer pairs are generated to solve potential user queries.
3.The proposed method achieves state-of-the-art performance on three public anomaly detection datasets.

Weaknesses:
1. There are already many studies focusing on the background information in abnormal events. Have the authors considered describing the background information when describing the action information?
[1] Scene-aware context reasoning for unsupervised abnormal event detection in videos[C]//Proceedings of the 28th ACM international conference on multimedia. 2020: 184-192.
[2] Few-shot scene-adaptive anomaly detection[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16. Springer International Publishing, 2020: 125 -141.
[3] Hierarchical scene normality-binding modeling for anomaly detection in surveillance videos[C]//Proceedings of the 30th ACM international conference on multimedia. 2022: 6103-6112.
2.The dataset proposed in the manuscript also supports question answering in open scenes, but the method does not reflect how to use these questions and answers to improve the model's anomaly detection and understanding capabilities.
3.When constructing a dataset, if a description is generated for each second in the dataset, it may result in a lot of repeated descriptions. In order to avoid this redundancy problem, have you considered using keyframes instead of second-by-second descriptions to reduce the complexity and computational cost of data processing?
4.Most previous studies pay equal attention to various parts of the video, such as background, motion information, character appearance, etc. This paper focuses on motion information. Section 4.3 mainly extracts language descriptions related to motion. Whether background information is not considered. Moreover, the 7 datasets proposed in the manuscript contain more abnormal scenes. I am very curious why the scene information is not paid attention to.

Limitations:
Yes, the authors address possible limitations of their study.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a new variant of the video anomaly detection. Prior methods were vanilla classification methods, and this paper proposes more descriptive anomaly description and also QA along with that. The paper first gives the dataset creation strategy and then introduces the motion model and the video architecture to provide a text description of the anomaly. The authors also have baselines to compare against recent methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem setup is novel and fills the gap in current VAD methods.

2. The dataset creation strategy is very intuitive and the quality looks good from the examples provided.

3. The evaluation protocol is correctly explained.

Weaknesses:
My main concern is regarding the experimentation and the lack of some crucial baselines.

1. The proposed method benefits from the training data, whereas all the baselines are zero-shot. It is difficult to now evaluate the contribution of your motion model and the training data. It is essential to show how the baseline methods like VideoChat will behave when they are fine-tuned with the training data the authors propose. Without these baselines, it is difficult to evaluate the effectiveness.

2. The GPT-4 output shown in Fig. 2 (a car with red lights has lost traction....) is already pretty good. Due to this, the authors must show that the trained model is able to perform better than the basic dataset creation strategy. It is understood that the dataset is created using off the shelf models and expected to perform worse than the trained video and motion model, but a baseline that uses the pseudo-dataset creation strategy is important. (Or an appropriate justification of why it cannot be added)

Limitations:
Yes, limitations are discussed and they correctly reflect the shortcomings that I see.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents HAWK, a framework that uses large Visual Language Models (VLMs) to accurately interpret video anomalies. HAWK integrates motion and video modalities through a dual-branch framework, enhanced by an auxiliary consistency loss to focus on motion-related features. The authors annotated over 8,000 anomaly videos with language descriptions and created 8,000 question-answer pairs to train the model across diverse scenarios. HAWK demonstrates state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The framework benefits from extensive annotations and question-answer pairs, improving training quality. 
2. HAWK achieves superior results in video description generation and question-answering tasks, outperforming existing baselines.
3. The paper is well-written and easy to follow.

Weaknesses:
1. In Table 1, it would be better to indicate the LLM backbone of the methods for a fair comparison.
2. From the ablation study in Table 3, it appears that even without the motion modality, the baseline model achieves comparable performance to other methods without motion modality. This may be because the high-quality dataset and strong LLM backbone contribute more to the performance, which weakens the perceived technical contribution of the motion modality.

Limitations:
See the weaknesses.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
kXErlJSZ84;"REVIEW 
Summary:
This manuscript provides a new approach to line recognition of offline handwriting and printed documents with regard to multilingualism.

This method can recognize a line based on its simultaneously recognized features based on transformers.

This approach is interesting for document analysis.

Promising results have been obtained for different datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well written and organized. Since the method recognizes the characters simultaneously, this approach is efficient. This paper explores the impact of using a synthetic dataset to improve accuracy in practice.

Weaknesses:
This is not a whole system for the field of document analysis. This task is a part of document analysis that depends on line segmentation. This means that if the previous part (line segmentation) of the process has errors, these are transferred to the next steps. Therefore, this is an essential task for the work. What happens if you have a whole document for the task? What happens to your results if you use existing methods that segment lines? 

This method is not generally suitable for multilingual contexts. If there are several characters in different languages, this paper has no clear solution. You should therefore have several models for each language.

It is better to explain the method for your synthetic data in more detail in the Appendix. It is better to clarify this with an illustration.

Please check your text again for some errors, e.g. line 263 (annotations annotations).

Is line 230 correct? ""the English Volume 0002""

Limitations:
They showed some limitations in Figure 4. But it needs to add more samples in an Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel transformer-based character detection for text line recognition. The authors use a diverse set of synthetic data to enable localization part of the detection network to generalize to unseen characters during training. The transformer-based detectors can identify all characters in a text line in parallel and a masking strategy has been adopted to encourage detection interactions. The methods proposed by the authors also includes a process to fine-tune the detection network using only line-level annotations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1: The approach is novel for character detection for text line recognition using transformer-based models.
S2: The authors demonstrate strong performance by the model across several datasets and outperformance for cipher recognition.
S3: The model can generalize well to unseen characters and variations in text as the authors utilize synthetic data.

Weaknesses:
W1: Several typos throughout the paper. Ex: typo in the Fig 1 caption: “Our model is general can be”, 
W2: The authors can provide additional information on the fine-tuning process and the adaptability of the masking strategy. Did they explore other strategies?
W3: The performance on real-world datasets that contain some-to-large amount(s) of noise has not been extensively discussed by the authors.

Limitations:
L1: The efficiency of the proposed method in real-world datasets with noises has not been fully explored.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel detection-based approach to text line recognition for both printed (OCR) and handwritten text (HTR), covering Latin, Chinese and cipher characters. Traditional detection-based methods have been largely neglected in HTR due to the difficulty of reading characters separately and the high cost of character-level annotation. The authors propose a solution to these challenges through three main insights: (i) using synthetic pre-training with diverse data for character localisation across different scripts; (ii) using modern transformer-based detectors to handle multiple character instances simultaneously, using a masking strategy to ensure consistency; and (iii) fine-tuning a pre-trained detection model with approximate character localisation using line-level annotations on real data, even with different alphabets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Originality: The method is highly original, proposing a detection/classification approach to character recognition that differs from most state-of-the-art methods, which typically rely on autoregressive decoding from images of lines or pages.
- Synthetic training data: The authors propose a way to train models using fully synthetic data, eliminating the need for actual character-level annotations.
- Model efficiency: The model is relatively small (~40M parameters), requiring only 100k synthetic line samples for training.
Error analysis: This approach allows for a better understanding of errors, distinguishing between detection and classification errors.
- Computational cost: Characters can be independently predicted in parallel, reducing computational cost, especially without a language model.
- Adaptability: The method can be easily adapted to any alphabet with minimal training data.
- Code release: The authors are committed to releasing the code, facilitating further research and application.

Weaknesses:
- Lack of new technical contributions: Despite the originality of the approach, the paper lacks novel technical contributions. The model, training strategy, fine-tuning with CTC, data augmentation and synthetic data generation are not novel.
- Performance on handwritten documents: The method is not competitive on Latin handwritten documents and is outperformed by existing methods on Chinese handwritten documents. It does outperform on cipher recognition, but this is a more specialised and less researched area.
- Scope of evaluation: The framework is only evaluated on perfectly segmented lines of text, raising questions about its performance on full pages. Full page processing would require additional steps such as text line detection and reading order retrieval, which could impact performance.
- Impact of line detection: There are concerns about how the quality of line detection would affect character detection and recognition, particularly in cases where vertical or horizontal lines merge.

Limitations:
- Conclusion: The conclusion lacks depth and provides no insight into future improvements to the method.
- Paper and writing:
    - Figure 2: The figure is unclear and needs to be made clearer.
    - Table 1: The table is misplaced, it appears on page 6 but is referenced on page 7.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper treats text line recognition as an object detection task and proposes a two-stage training approach based on DINO-DETR. In the first stage, synthetic data with bounding box information is used to predict the bounding boxes and categories of text, due to the absence of character-level annotations for text line datasets. In the second stage, real text line data is employed to sequentially fine-tune the classifier and the entire model. The proposed method achieves state-of-the-art performance in cipher text.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well-written and easy to follow. 
- The evaluation is well done and compares against several strong baselines.

Weaknesses:
- Abalation study is easy. 
- Utilizes more engineering skills than methodological innovations.

Limitations:
The proposed method performs well only on the cipher text recognition task.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
96gXvFYWSE;"REVIEW 
Summary:
This paper introduces a threshold-based auto-labeling (TBAL) method called Colander to maximize TBAL performance by finding the optimal labling confidence function and thresholds. In order to find the optimal confidence function, Colander treats the auto-labeling objective as an optimization problem that maximizes the coverage under label error constraint. They use a neural network as the confidence function, and design a surgate for the optimization problem which can then be optimized using gradient-based methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: The paper turns auto-labeling into an optimization problem, and proposes a disciplined solution that can be solved by gradient methods.

S2: The proposed optimization surrogate may be adopted by other auto-labeling methods.

S3: The paper presents extensive experiments to compare with existing methods, and the results are promising.

Weaknesses:
W1: the paper is overall well-written, but the discussion of the thresholds is confusing or missing details. Specifically:
- In line 89, it says ""the vector _**t**_ denotes scores over _k_ classes"", and I suppose score means the predicted scores (or probabilities) of the _k_ class, but later _**t**_ is defined as a vector of thresholds.
- In line 144 (P1), it is unclear what _T^k_ stands for, and how the set of threshold, _T_, is determined. 
- In Algorithm 1, the Colander produces the estimated confidence function and thresholds hat _ti'_ (line 14), but the threshold is not used, and it relies on Algorithm 2 to estimate the threshold for each class. There is no discussion what the difference between these two thresholds.

W2: There is no discussion of the computation overhead of Colander.

Limitations:
The computation cost of Colander increases linearly with the size of the set of confidence thresholds _T_, but it seems the process requires finer granularity in _T_ in order to get a better estimate of the optimal confidence function and thresholds.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel auto-labeling method, called Colander. In contrast to existing works, Colander models the objective of finding an optimal confidence function as a constrained optimization problem (the confidence function should have maximum coverage whilst obtaining a sufficiently low error rate; both components are controlled with a penalty term). Colander is evaluated and compared to baselines on four datasets covering vision and language tasks: MNIST, CIFAR-10, Tiny-ImageNet and 20 Newsgroups. The obtained results clearly indicate that the method improves upon existing baselines, both in terms of coverage and error rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper proposes a novel approach to identifying confidence functions for auto-labeling. As indicated in the paper, finding confidence functions for auto-labeling is challenging, and this paper tackles this problem elegantly via constrained optimization.
* The paper’s results are promising, showing that the introduced method improves upon existing baselines.
* Overall, I believe that this paper provides a solid contribution to the research area of auto-labeling.

Weaknesses:
* The paper focuses heavily on formally introducing Colander, and the experimental content is comparatively thin. To provide the reader with a better notion of robustness, it would for example have been useful to provide additional details on the hyperparameter search in the main paper, and how selecting those differently affects both the coverage and error rate. Likewise, it would have been useful to provide additional details on the impact of the introduced penalty term.
* It would also have been interesting to better understand the relationship between chosen model architecture for a dataset and auto-labeling performance.
* The paper scarcely discusses limitations in the conclusion section, yet I would have expected a more detailed discussion of where and how this approach is limited.
* Related to that, the paper does not touch upon future work / research questions that the obtained results create.

Limitations:
As discussed in the Weaknesses, the paper mentions a limitation in the conclusion section but I'd encourage the authors to further elaborate on how and where their approach is limited, and how such limitations can potentially be addressed in future work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses threshold-based auto-labeling functions aimed at identifying a large subset of unlabeled instances where the auto-labeling error remains below a specified threshold. The authors observed that standard temperature-scaled softmax scores are inadequate for effectively thresholding labeled instances. To address this, they propose learning a confidence scoring function and a threshold based on a subset of the validation data. This confidence function is a two-layer neural network trained on the logits from the last two layers of the base model. Extensive experiments were conducted on both image and text datasets using various training-time strategies to optimize the base model, demonstrating the efficacy of their approach.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. The paper is well written, with a variety of experiments conducted to demonstrate the applicability of their proposed method. The choices of strategies and hyperparameters are documented well. 
2. The performance of the proposed method is quite significant, achieving lower error rate and higher coverage than the other methods, especially on the harder datasets.

Weaknesses:
1. The whole TBAL procedure is a hybrid mixture of iterative training/self training and active learning. While the authors explained the differences between TBAL and self-training + active learning (ST+AL), the difference seems small (mostly, TBAL aims to identify a low-error auto-labelled dataset and ST+AL aims for a good classifier). These two goals do not seem fundamentally different, and can be easily translated to each other. Given the similarity of the framework, I feel it is still necessary to include some ST/AL works in the experiments. 
2. A lesser weakness is that due to iterative training, the proposed method and compared methods all went through rounds of data selection, which makes the comparison indirect. For example, it would be hard to tell the exact improvement on thresholding quality the method brings.

Limitations:
The authors acknowledge that the limitation of their work is the requirement of validation data.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenges of overconfident scores in threshold-based auto-labeling (TBAL) systems. It critiques existing confidence scoring and calibration methods and introduces Colander, a new post-hoc method tailored to optimize TBAL performance.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
-	The paper has a good identification of the problem which is over confidence of existing TBAL confidence functions, and it proposes a novel framework to find the optimal confidence function. 
-	The paper conducts extensive experiments on both image and text data. 
-	The paper compares various post-hoc functions. 
-	The paper is well written with great details in Appendix

Weaknesses:
-	The authors rightly mention limitations (“A limitation of Colander is that similar to other post-hoc methods it also requires validation data to learn the confidence function. Reducing (eliminating) this dependence on validation data could be an interesting future work.”) in the conclusion. So, does this mean that they use Gold data for validation? If that is the case, that might be the weakness of TBAL in general. The following paper talks about it in detail: https://aclanthology.org/2023.acl-long.796/

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a Colander for threshold-based auto-labeling (TBAL) pipeline. Colander trains a flexible confidence function using latent information of the classifier model, instead of fixed confidence functions. The optimization problem of Colander is based on the objective of TBAL, that is, maximizing coverage while minimizing auto-labeling errors. The authors consider differentiable surrogates for the 0-1 variables using the sigmoid function to make this problem tractable using gradient-based methods.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper is well-motivated. The proposed method is explained with simplicity and clarity.

Weaknesses:
Contribution: Although this paper proposes the Colander for the TBAL pipeline, there is no theoretical analysis and understanding of mathematical properties.

Completeness:
- The paper includes unknown notation, which is used without pre-defined. 
- In Section 3.3, an undefined procedure exists: RandomQuery, RandomSplit, and ActiveQuery. In the case of Random method, it is possible to guess by readers, but in the case of ActiveQuery, a detailed explanation is needed.
- There are a few expressions that make it difficult to figure out the intent. Additionally, some keywords used in the paper need to be unified. 
   - What is the difference between the score (or scoring) function and the confidence function?
   - What is the difference between ‘inherent tension’ (in line 33) and trade-off?
   - What is the difference between post-hoc method and calibration method?
- The reviewer is unsure if Figure 1 is necessary for the reader: it contains acronyms that are not defined in advance, such as ERM, and certain formulas (h) are left undefined in the introduction.
- Line 174, line 186: The reader does not know which Appendix to look at because Appendix information is omitted.
- Appendix, Algorithm 2, line 8: The formula where C_1  weight is considered is not defined in the text.
- Appendix, Table 3: there are typos.
- This paper is considered to have no significant contributions compared to previous research about TBAL[1].
[1] Vishwakarma, Harit, et al. ""Promises and pitfalls of threshold-based auto-labeling,"" Neural Information Processing Systems, 2023.

Limitations:
In this paper, there was not enough discussion about the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ASA2jdKtf3;"REVIEW 
Summary:
This paper extends the framework of multi-agent influence diagrams (MAIDs) to explicitly capture complex forms of reasoning corresponding to Theory of Mind (ToM) as required for the interaction of Multi-Agent Systems with human users.  It introduces the framework of incomplete information MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. Using results connecting EFGs to MAIDs, the authors demonstrate a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents’ subjective models.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The approach is well situated within the state-of-the-art of related work in agent models with a game theory component and, as far as one can judge, appears technically sound within the broad remit of causal and influence diagrams (IDs). 
The paper is very well structured and the authors did their utmost to keep it relatively accessible by alternating formal sections with intuitive descriptive summaries. It remains somehow tedious to read, owing to the large number of definitions, whose numbering alternate with that of theorems. 
The rationale for building a framework on top of MAIDs rather than EFGs is well introduced, together with the mapping between strategies in MAIDs and EFGs and the choice of working at the interim stage. This culminates with Theorem 20, until section 5.1 raises some issues around the relevance of Nash Equilibria.

Weaknesses:
The major issue I would raise for this paper is one of relevance to NeurIPS, even in the extended sense. While a major rationale for the paper appears to be its potential application to AI Safety, in the NeurIPS context there does not seem to be enough outreach to current AI models, at least in a way in which they could be interfaced to the proposed ID model. This means some consideration of how current models may form ‘beliefs’, and this was not entirely obvious from the paper’s Title and Abstract. Perhaps my expectation was unrealistic, but I had imagined an attempt to unify formal ToM issues with ToM properties that are known to be associated to LLM, under a framework where this approach would federate or wrap formal agentic methods around, say Agentic LLM. With this comment I am not criticising the authors for not having written another sort of paper, I am simply pointing the perceived gap that may exist between this approach and the NeurIPS constituency. Further evidence would be the absence of references to NeurIPS paper and the relative dearth of mainstream AI venues in the references (to the notable exception of AIJ). Overall, it appears that AAMAS might be a better venue to host this type of paper. 

The paper does not really clarify its ToM framework which references both “multi-agent interactions” as well as “higher-order intentional states” but these aspects are not part of further formal developments. It also mentions “belief hierarchies of arbitrary and infinite depth” and this raises the issue of whether such a formal approach is realistic when it comes to ToM, in particular in the interactions between agents and human users. 

Despite an early reference to AI Safety and a mention in the paper’s abstract, there is little in the paper that actually progresses the discussion on AI Safety, which is only used marginally through ID examples, such as the one of Figure 2.

Limitations:
The limitation section begins with a number of upbeat statements that would better be placed in the conclusion or parts of the abstract. The main identified limitation, which echoes the discussion of section 5.1 is verbatim: “The main limitation of our work is the lack of a useful solution concept.” appears a quite severe restriction. While not affecting the solid grounding of the approach it considerably restricts its impact at its current stage of development.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework Incomplete Information Multi-Agent Influence Diagrams (II-MAIDs) for modeling complex multi-agent interactions involving theory of mind (ToM) and higher-order beliefs. The authors prove the equivalence between II-MAIDs and Incomplete Information Extensive Form Games (II-EFGs) at the interim stage. The paper also shows the existence of Nash equilibria in II-MAIDs under certain conditions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The II-MAID framework fills a gap in existing game-theoretic models by allowing for inconsistent beliefs and higher-order reasoning. The paper is built on solid mathematical foundation with formal definitions and proofs.

Weaknesses:
* From my perspective, the proposed II-MAID framework appears overly complicated for modeling Theory of Mind (ToM), which is fundamentally a straightforward psychological mechanism observed in daily human interactions. The paper's approach may overcomplicate a concept that should be more intuitively represented.
* The paper introduces numerous assumptions and definitions without clear explanations which hinders the readability. As a non-expert in the field, some details in the paper are difficult to read. 
* It is unclear whether the model can be scaled and applied to larger, more realistic scenarios, where ToM takes place more frequently.
* The paper lacks experiments that validates the model.

Limitations:
N/A, see weaknesses.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends the theoretical framework of multi-agent influence diagrams (MAIDs) with incomplete information (II-MAIDs) to explicitly capture this complex form of reasoning. The primary theoretical contribution is the proof of the existence of Nash equilibria, although, in general, these equilibria are impossible for agents to identify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This work is game-theoretic in nature, and overall, the presentation quality is good and smooth to the best of my knowledge.

2. Although I think the assumption made in this work generally makes sense to me: agents have consistent beliefs as part of our commonsense, which can be derived from a common prior distribution, I agree that there are settings with no common prior available. The setup is a less constrained setup.

Weaknesses:
1. One of my major concerns is the audience of this work. Given that this work is submitted to the safe ML track of NeurIPS, I expect more discussion on the relevance of this framework to AI safety. The author should elaborate on what they imply by “safety” rather than making a very brief claim about its relevance in the related work and conclusions sections.

2. The discussion of theory of mind is also lacking, given that this is well-motivated. There have been extensive studies on machine theory of mind, ranging from early studies [1-2] to recent studies on LLMs [3-4]. There has also been research connecting Theory of Mind to Game theory [5] and Interactive POMDP [6]. See the survey [7] for details. Overall, this work needs significant improvement in discussing related work for readers to evaluate its contribution and relevance to NeurIPS.

[1] Rabinowitz, Neil, et al. ""Machine theory of mind."" International conference on machine learning. PMLR, 2018.

[2] Jara-Ettinger, Julian. ""Theory of mind as inverse reinforcement learning."" Current Opinion in Behavioral Sciences 29 (2019): 105-110.

[3] Sap, Maarten, et al. ""Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

[4] Ma, Ziqiao, et al. ""Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models."" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.

[5] Yoshida, Wako, Ray J. Dolan, and Karl J. Friston. ""Game theory of mind."" PLoS computational biology 4.12 (2008): e1000254.

[6] Çelikok, Mustafa Mert, et al. ""Interactive AI with a Theory of Mind."" Computational Modeling in Human-Computer Interaction. 2019.

[7] Albrecht, Stefano V., and Peter Stone. ""Autonomous agents modelling other agents: A comprehensive survey and open problems."" Artificial Intelligence 258 (2018): 66-95.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B1vGiSgELw;"REVIEW 
Summary:
This paper introduces a new concept called Matryoshka Query Transformer (MQT) that brings in the concept of Matryoshka information packing to make visual tokens flexible and can be used in multimodal vision language models like LLaVA. The reduced tokens tend to reduce the quadratic complexity of the language model due to potential flexibility in the processed visual tokens. 

Owing to this, there are significant efficiency benefits all the while retaining accuracy across various benchmarks. The paper also showcases analysis and further discussion on the technique. 



----------------------------
The review will be short and does not reflect the time put in for the review or the quality of the paper. When the ideas are simple and clear -- I tend to write shorter reviews to the point.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I will go sequentially

1) The paper is extremely well-written and easy to follow. 
2) The core ideas, while not completely novel as mentioned by the authors, Matryoshka and the Query transformer, making them work together and showing incredible benefits is commendable and is a worthy contribution. 
3) The modelling very clear and the mechanism to achieve and the details are very well fleshed out. 
4) The experiments are extensive, with a good analysis. 
5) Great job with visualizations and nice to see some TFLOPs measurement. 
6) Good analysis beyond benchmark numbers. 

Overall, this is a solid work with practical utility. I have a few questions about the paper mentioned in the weaknesses and would appreciate answers for them in the rebuttal, however, I am happy with the paper and am willing to champion it unless there is something I am missing found by other reviewers.

Weaknesses:
Most of these are not weaknesses, but rather questions

1) How are you measuring TFLOPs? What does it include, the cost of vision encoder? LLM processing and generation? I might have missed this in the paper and any pointer would be great.
2) While I understand the when you are picking a fixed # token for MQT, you make that choice before in hand. What happens if you actually just obtain 256 tokens and take the first 8? What will be the performance in that case? I know softmax etc make huge differences, but would be good to see that. 
3) Any thoughts on sampling vs joint training?
4) Why is log-based (actually exponential) spacing so poor (not 3.5% as mentioned in the paper but 2.1% from the table) so much worse than fine granularity? This is a bit surprising to me because 256 token performance should not be affected because of the lower token counts.
5) In visualizations, what do you mean by one random token? 

I am looking forward to your answers on these things.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Authors try to use Matryoshka mechanism to guide the learning process of LVLMs such as llava. 

Authors show a pretty good scaling curve with different amount of visual tokens.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea is interesting. 
2. The presentation is clear. 
3. The attention visualization is interesting.

Weaknesses:
1. For a given image, which scale should I choose to achieve best trade off? Is there an answer?
2. Authors use more compute (2 epoch) to get the similar performance as LLaVA, is there any reason for this?
3. All designs are center upon the qformer. There is little study on this. How many layers do we need? Do we really need it?

Limitations:
See comments above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers multimodal vision transformers, in which we have a stream of both visual and textual tokens. Current architecture typically assume a fixed number $m$ of visual tokens. In contrast, the proposed `MQT` aims to achieves a dynamic number of visual tokens. This is done by integrating a form of compression+dropout mechanism during training. Specifically, the image tokens are compressed to $M$ tokens using cross-attention. To vary the number of tokens, the model only needs to use only  $m < M$ queries from te cross-attention mechanism. At training time, this is done by randomly selecting $m$ and feeding the **first** $m$ queries.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The proposed method is a simple idea that can be added as plug-and-play to any model
* generally the idea of using some form of dropout also often has benefits for regularization so it may also benefits more than efficiency
* The paper is well written and has comprehensive ablation experiments

Weaknesses:
- **Lack of comparison with dynamic baselines**: in the field of image-only ViT, there is a flourishing literature on dynamically reducing the number of tokens using merging or pruning or scale selection. I am not as familiar with this literature for multimodal models, but there does seem to be similar existing approaches: for instance, *LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models* is one, though it might be too recent for the authors to have considered it for the submission. However, more generally, having a simple off-the-shelf token merging/pruning approach for the ViT encoder would be interesting.

- **No improvement over baselines in the low FLOPs regime**. From Figure 1, it seems that the benefits of the proposed `MQT` decrease in the low FLOPs regime, when increasing the sparsity ratio of $M / m$.  This limits the usefulness of the methods in the realm of model efficiency, though it seems to be a useful and simple training strategy for the high FLOPs regime.

Limitations:
The paper has a limitation section but in my opinion it does not fully address some limitations of the paper (e.g. no improvement in performance in the low FLOPs regime)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of achieving flexibility in the number of visual tokens to suit different tasks and computational resources. Inspired by Matryoshka Representation Learning (MRL), the authors propose the Matryoshka Query Transformer (MQT), which allows for any number of visual tokens during inference. Experimental results demonstrate considerable performance across varied visual token lengths and show promising results even with extreme token numbers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of handling an arbitrary number of visual tokens is highly valuable. The authors propose a novel MRL-based method to tackle this issue.

2. The experimental results are promising, highlighting the trade-off between performance and computational resources.

3. The analysis of the impact of visual tokens is valuable, providing meaningful conclusions that deepen our understanding.

Weaknesses:
1. Can you explain the choice of the MQT transformer design? Does it need to maintain sufficient capability for extracting information from varied tokens? For example, Q-Former has multiple layers to achieve effective information extraction.

2. The extraction process V=Q(Z,G) lacks correlation with textual information. In Fig 4, different text prompts should result in the same attention map due to this. Therefore, this design might be limited when dealing with complex images, where visual representations should maintain different semantics.

3. How does the model perform in text-rich scenarios, such as TextVQA? The sophisticated recognition ability might be significantly compromised in these cases.

4. Since the number of visual tokens affects performance, how does the model perform when the number of visual tokens exceeds the original 576 tokens?

5. It would be better to provide a comparison with the LLaVA baseline, using adaptive pooling or vanilla Q-former to adjust token numbers.

Limitations:
Please consider answer the questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
NC0Bjl4uTf;"REVIEW 
Summary:
The paper presents a novel Chinese inertial generative adversarial network (CI-GAN) designed to generate high-quality training samples for Chinese writing recognition using inertial sensors. The CI-GAN integrates Chinese Glyph Encoding (CGE), Forced Optimal Transport (FOT), and Semantic Relevance Alignment (SRA) to enhance the quality and authenticity of generated inertial signals. The approach addresses the challenge of collecting diverse and extensive training data for Chinese character recognition, showing significant improvements in classifier performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces innovative methods in the form of CGE, FOT, and SRA, contributing significantly to the field of inertial writing recognition. The release of a new dataset further enriches the community's resources.

Weaknesses:
1.Lack of Detailed Baseline Configuration: The paper compares CI-GAN with a traditional GAN in the appendix, but fails to provide detailed settings for the baseline method. This lack of information hinders the ability to fully understand and replicate the comparative effectiveness reported.
2.Insufficient Comparison with Other Augmentation Techniques: The study does not compare CI-GAN with other data augmentation methods, such as random perturbations. It remains unexplored whether applying random disturbances to the data could also substantially improve classifier performance.

Limitations:
The authors have discussed limitations related to the variability of writing styles and the potential impact of environmental factors on sensor data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes CI-GAN to acquire unlimited high-quality training samples, alleviating the data scarcity in the inertial signal recognition of Chinese characters. By utilizing these generated data, the performance of recognition models is highly improved.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is easy to follow.
- The proposed methods may help disabled people.

Weaknesses:
- The pipeline lacks novelty. The employed technologies are widely used in CV and NLP, and the proposed pipeline merely reuses them for the inertial signal domain without any innovative design. Furthermore, the author fails to cite relevant studies such as [1][2] and does not discuss their differences.
[1] Wasserstein GAN (WGAN)
[2] Efficient Estimation of Word Representations in Vector Space

- The proposed CGE is simply a learnable embedding to represent Chinese characters, lacking innovative design for glyph information. The author introduces GER to enhance the orthogonality of character embeddings but does not provide an ablation study to verify its effectiveness.

- The author uses Wasserstein distance in GANs. What is the difference between this approach and WGAN [1]? Additionally, the author proposes using FFM to supervise the signal in feature spaces. These measures are also similar to some works, such as perceptual loss using VGG and identity loss using ArcFace, but the author does not cite these and discuss the difference. 

- The dataset used for training and testing is too small, which could not effectively verify the effectiveness of the proposed method.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address an important probem in human computer interaction: making computers accessible to vision impaired people. The paper address this my collection paired data of text and imu signals. First, the paper address the issues of limited data by training a generative model, to resample/bootstrap more data and then train recognition model on both real and generated data to archive high performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
the paper addresses an important social problem, and accessibility should be focused on all groups. 

The data collected for this paper, the paired data on text and imu is very useful, hope the authors will open-source it. 

paper is well written and the figures are clear and convey the ideas.

Weaknesses:
My main concern is, that it is very unlikely that we get more than we give to the system, the generated samples are a function of real samples. 
I would like to see, a competitive baseline with good data augmentation, and maybe on a low data regime gan generated samples are better than augmentation, but this has to be shown, otherwise, I don't see the value of extra effort to train a generative model to get data augmentation.

Limitations:
I wouldn't say this is a major limitation, but on the scale axis, this problem can be solved by collecting more data. Unlike annotations like explaining an image or video, handwriting signals are more easy to collect on the long term. would be nice if the authors can address this, also please explain the issues with data augmentation.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
QgMC8ftbNd;"REVIEW 
Summary:
This paper proposes an abstract model of RL with information structures. Under this mode, the paper investigates the necessary conditions for sample-efficient learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The results of this paper are very general, in the sense that they encompass the results of several previous works.

Weaknesses:
(1) It is hard to identify the technical novelty & originality of this paper. The POSTs and POSGs are special cases of ""generalized PSRs"", but the so-called ""generalized PSR"" is largely a direct abstraction of the analysis framework of Liu et al. 2022b. The algorithm and the proofs are also largely patching the analysis in the previous papers (in particular, Liu et al. 2022b).

(2) It is not clear what new concrete tractable class does this paper identify. Tractable POMDP classes and POMG classes have already been studied in previous works. Several examples of information structure are provided, but there isn't a definition of a new, meaningful, and tractable problem class.

Overall, this paper indeed provides a general framework, but it seems that generality is the only advantage of this framework. However, ""general framework"" cannot be an excuse. After all, there have already been several general frameworks for sequential decision making. For example, for model-based learning, the so-called Decision-Estimation Coefficient characterizes the statistical complexity of *any* model class [1,2,3,etc]. The SAIL framework of Liu et al. (2022b) is more concrete and it also applies to a broad class of sequential decision making problems. The authors have to justify what is the significance of this framework other than its (vacuous) generality.

[1] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making.

[2] Dean Foster, Dylan J Foster, Noah Golowich, and Alexander Rakhlin. On the complexity of multi-agent decision making: From learning in games to partial monitoring.

[3] Dylan J Foster, Noah Golowich, and Yanjun Han. Tight guarantees for interactive decision making with the decision-estimation coefficient.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new representation of sequential decision making problems with an information structure. They show that the information structure can be used to estimate the complexity of the decision-making problems and can guide learning algorithm development.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors present a good study on complexity of decision-making problems in finite state and action spaces
1. They identify scalable decision-making problems for learning applications.
1. The work appears to be technically sound, but I haven't checked all the proofs

Weaknesses:
1. If I understood correctly, the authors are dealing with finite state and action spaces and the complexity measure is effectively a rank of a certain matrix that they construct. If the state or the action spaces are infinite or continuous then the measure becomes infinite for all systems and we cannot use the measure for judging complexity. If this is so, I see it as a major weakness
1. Presentation can be improved. It’s hard to understand what different concepts mean, say core tests. Can a “layman” summary be given after these kinds of definitions?
1. While I appreciate that the paper is theoretic in nature, it is hard to judge it's practical implications in present or even in future. 
1. Some claims need for further clarification or justification. For example, I am not sure that Figure 1 is fully substantiated. Hopefully, the authors can clarify

Limitations:
limitations are discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the tractability of modeling and learning general sequential decision-making problems with an explicit representation of information structure. This gives a unifying framework that captures a range of commonly studied RL models. Through a graph-theoretic analysis it characterizes the complexity of observable dynamics in sequential decision-making problems. It then gives an upper bound on the sample complexity of learning these problems characterized by their information structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and well-structured. The proposed framework of partially-observable sequential teams (POST) and partially-observable sequential games (POSG) incorporates more general system dynamics than classical MDP/POMDP models through much more general information structures. The model is also practically relevant for application with complex and time-varying dependence on the past or time-varying observations dynamics. 
- The paper provides a thorough theoretical foundation to understanding the complexity and tractability of POST and POSG problems. This includes characterizing the complexity of the observable system dynamics, as well as upper bound of on the statistical hardness of learning both through the lens of the information structure. Moreover the results recovers known sample complexity results in POMDPs. 
- This paper gives a key insight that the complexity of the dynamics and the statistical hardness of sequential decision-making problems can be characterized in terms of its information structure. The proposed method of observation generalized predictive state representation is also of independent interest for other adjacent problems.

Weaknesses:
- The m-step $\mathcal{I}^{\dagger}$ -weakly revealing condition can be stringent. As the paper points out, the condition will be harder to satisfy when $\mathbb{I}_h^{\dagger}$ is large. It would be helpful to address the sensitivity of claimed results when this condition is not satisfied. 
- The results of the paper are theoretical in nature, and it could benefit from empirical results to demonstrate the effectiveness of the proposed algorithm and framework on experiments / simulations.

Limitations:
The author(s) have adequately addressed the limitations of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
YZWvf58dBS;"REVIEW 
Summary:
This paper introduces the TransLLM framework to transform English-centric chat LLMs to non-English languages, addressing the challenges of transferring advanced abilities without supervised data and preventing catastrophic forgetting of original knowledge. Key contributions include using the Translation Chain-of-Thought (TCOT) to divide the transfer process into sub-tasks, employing Low-Rank Adaptation (LoRA) and Recovery Knowledge Distillation (KD) to maintain original LLM parameters and recover knowledge. TransLLM's effectiveness is demonstrated through experiments transforming LLaMA-2-chat-7B to Thai, where it outperformed strong baselines and ChatGPT in multi-turn conversations and safety benchmarks, highlighting significant improvements in helpfulness and safety without extensive supervised data. This framework offers a solid foundation for developing safe and useful non-English LLMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of TransLLM, which combines Translation Chain-of-Thought (TCOT) and recovery knowledge distillation, provides an effective method for transforming English-centric LLMs to non-English languages. This approach addresses both the transfer of advanced abilities and the prevention of catastrophic forgetting.
2. The method shows notable improvements in rejecting harmful queries and maintaining human preference alignment, as evidenced by outperforming GPT-4 and ChatGPT on the safety benchmark AdvBench, highlighting the robustness of the model in safety-critical applications.

Weaknesses:
1. The experiments are primarily conducted on transforming LLaMA-2-chat-7B to Thai, which may limit the generalizability of the findings to other non-English languages and other models. Further validations on other models (not necessarily bigger than the current one) or other size of Llama-2 would have strengthened the paper.
2. While the proposed method exhibits promising results in MT-bench and AlpacaEval, it is not tested on other traditional NLU benchmarks, just like MMLU in English. Incorporating more diverse evaluation benchmarks would provide a more comprehensive assessment of the model's performance across various language tasks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a transfer pipeline, TransLLM, which uses a translation chain-of-thought (TCOT) to adapt English-centric large language models (LLMs) to low-resource languages. This pipeline consists of pre-training and supervised fine-tuning (SFT) phases. During the pre-training stage, the authors select monolingual data in the target language and translation parallel data for language modeling. This pipeline also uses an external translation model to construct TCOT dialogue data, which is used in the SFT phase. In the SFT phase, to avoid the catastrophic forgetting problem caused by continual learning, the authors adopted the LoRA PEFT method and further proposed the recovery Knowledge Distillation (KD) method. Specfically, the recovery KD method mixes responses generated by the original model during the SFT-based transfer phase. The effectiveness of this pipeline was demonstrated by successfully transferring the Llama-2-7B model from English to Thai. Compared to other baseline models based on open-source LLMs, the transferred model exhibited great multi-turn dialogue capabilities and safety on the Thai MT-bench and AdvBench benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Originality**: Compared to other machine translation-based transfer methods in the community, the proposed pipeline introduces the concept of Translation chain-of-thought and specifically addresses the catastrophic forgetting problem that might arise from SFT-based transfer methods, proposing effective solutions.
2. **Evaluation Protocol**: I appreciate the efforts made by the authors in the experimental section to ensure the effectiveness of evaluations for low-resource language (Thai). The authors employed professionals for evaluation and also validated the agreement between GPT-4's automatic evaluation and human evaluation on the MT-bench. Additionally, they used human translation in constructing some of the test data.
3. **Significance**: Currently, training data and resources for LLMs are predominantly English-centric. The proposed method helps build strong chat LLMs for low-resource languages and minority groups.

Weaknesses:
1. **Flexibility of Methodology**: Although the proposed method reduces the dependency on instruction-following data in the target language, it still relies on parallel corpora and external translation models for data construction (Translation pre-training data, TCOT data). If high-quality parallel corpora or models are not available, the proposed method might be infeasible. For example, even commercial translation systems cannot support translations for some endangered languages or dialects.
2. **Scope**: While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline. I also noticed that the authors emphasized ""non-English"" in the title, but a broader range of non-English languages still requires exploration.
3. **Effectiveness**: The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline.
4. **Quality of MT data**: The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact. In the absence of reference translations, quality estimation methods like CometKiwi [1] and TransQuest [2] could be used.
5. **Typos**:
    - Line 45: instruct tuning -> instruction tuning

References

[1] Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

[2] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020. TransQuest: Translation Quality Estimation with Cross-lingual Transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5070–5081, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Limitations:
The authors discussed some limitations after the conclusion.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors present a method for transforming a chat-based English LLM to Non-English (Thai is the only language experimented with) based on a series of steps that teach the LLM to take in a non-English query and respond in non-English for that query. The methods presented is referred to as TransLLM pipeline which comprises of extending the based model vocab and finetuning with LoRA in multiple stages -- comprising of target language pretraining (on monolingual target language data), translation pretraining and transfer finetuning. The experiments are done on LLaMA2-Chat-7B with Thai as the target language and show the model performs well on both translation into and out of Thai and obtains good performance wrt baselines on MT-Bench & Alpaca-Eval.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper presents a simple and scalable solution pipeline to adapt chat LLMs to new languages.

Weaknesses:
1. The experiments are done on only one LLM and on only one language (Thai). This severely constrains the extend to which the results could be verified.
2. The novelty of the proposed method is very thin and very limited analysis is done to motivate that novelty.

Limitations:
Limitations have been adequately addressed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on a scenario as transforming a English-centric **chat** large language model to a non-English chat large language model (or not just en-centric). The authors want to address the catastrophic forgetting problem where further tuning on the original En-chat LLM without reusing their original SFT data will hurt their original chat abilities mainly in English. In terms of this, they introduce several techniques, including translation chain-of-thought, low-rank adaptation, as well as recovery KD, and they claim that with only single-turn Thai data, they can successfully transform English LLama2-chat-7B to Thai, while the performance in MTBench and some others remain competitive.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose a pipeline to tune the chat English language model to a non-english language model. The pipeline looks interesting and the performance looks good in MTBench, Th-translated MTBench, etc.

Weaknesses:
1. To be honest, I am not sure whether this paper has good starting point. They focus on a scenario where you want to further fine-tune the chat model in English to a chat model in other languages. Could you provide more concrete application scenarios in your introduction? I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?
2. I suggest the authors to further polish the paper abstract. I am getting confused several times, for example, in your statement ``Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method'', why can you simply say this is a resource-efficient method? As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive.

Limitations:
See the weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
XYDMAckWMa;"REVIEW 
Summary:
The paper proposes a loss for training flow matching (rectified flows, stochastic interoplants) that is based on integrating the target velocity over the available data as the regression target. This reduces the variance of the gradient estimator and can also be applied in the stochastic variant of flow matching.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
To the best of my knowledge, the idea is novel to use a larger batch of data for the velocity target in the flow matching objective than as the input for the network. This reduces the variance in the loss estimate.

The additional compute over the FM loss seems negligible, since only the target velocity field is adapted, but the network still receives batches. This makes it an attractive improvement to flow matching training.

Weaknesses:
I genuinely like the idea, but cannot recommend acceptance in the current presentation. I am happy to be corrected on any point and adjust my score.

Below I grouped the weaknesses by category.

## Theoretical contributions can be simplified and potentially reveal existing result (update: largely addressed, but presentation to be improved)

I think the theoretical derivation can be greatly simplified:

1. The notation is overly complicated. Why not use the standard notation for joint $p(a, b)$, conditional $p(a|b)$ and marginal $p(a)$ probabilities instead of index notation, where the index sometimes means ""joint"", ""conditional"" or ""marginal"", and sometimes indexes time $t$ or a condition such as $x_1$? (or $\\rho$ instead of $p$).
2. I think this reveals that the new loss is simply obtained by writing the target velocity in the ExFM loss as the expectation over the training data $\\rho_1$: Eqs (8, 10) say that the target velocity is given by $\\int w(t, x_1, x) \\rho(x|x_1, t) dx $, that is just average the velocities over the entire training data, weighted by the probability that the path actually  (where $x$ is sampled from the linear conditional paths). Given this observation, it seems to me that the actual contribution of the paper does not lie in this new loss, but how to efficiently estimate this integral, which is currently hidden in Appendix B.

In fact, I think that sections 2.1 and 2.2 can be merged to a simple importance sampling argument in the original flow matching argument.

I also think that the authors are missing that their third contribution has already been derived in the same form by their reference [10] in Eq. 4 and I think the second contribution is a simple extension to different conditional flow fields resulting from different inversions $\\varphi^{-1}$.

## Evaluation on tabular data is wrong (update: fixed)

The NLL defined in Appendix H.5.3 does not contain the volume change, which is an integral part of the negative log-likelihood. Did you use this equation for evaluation? My current score reflects the belief that volume change was accounted for.

Also, in Table 3, sometimes the highest values and sometimes the lowest values in each row are marked bold. Which model is better and does this use the incorrect formula? Please update without e-notation, adapting -1.29E+02 to -129, this is hard to read.

## Toy data evaluation (update: fixed)

It is easy to construct a very good approximation for the moons distribution by taking a Gaussian mixture of values for Table 4.

## Typos (has no influence on my recommendation)

Here is a list of what I found:

- l. 30: introduced -> introduce
- l. 33: base -> based
- l. 65: $rho$ -> $\\rho$
- l. 70: need -> needed
- l. 92/93: using map -> using the map
- l. 100: we return to end of the standard CFM loss representation -> ?
- l. 105: just (unknown) -> just the (unknown)
- eq. 7: the integral shares variables with the outside expression, e.g. add tilde on the integration variables
- l. 124: inevitable -?> invertible
- l. 126: have -> has
- eq. 16: consider moving numerical tricks like using softmax from the theory section of the paper, page 6 already introduces a lot of notation.
- throughout: dispersion -> variance

Limitations:
I do not think that the limitations of the work are properly addressed in the theoretic part, in contrary to the statement of the authors in the paper checklist. In particular, I did not fully understand how accurately Eq. (10) (which is part of the loss) can be estimated. One question that can be a way towards addressing this is by expanding on when the assumption in line 172/173 is valid (and why this Jacobian is even a problem).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an analytic formula for the vector field satisfying the continuity equation for the given change of density which interpolates between two distributions in the sample space. This is a common setting in the Flow Matching model [6] (Rectified Flows [1], Stochastic Interpolants). 

The authors apply the formula for several special cases like linear interpolation between samples and propose to use this formula for the training. They study the proposed training procedure empirically for synthetic examples and CIFAR-10 images.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The presentation of the proposed method is clear.

Weaknesses:
The proposed formula is already well-known in the community. It was proposed in [1] (see eq. 4, Def 3.1, Sec 5.1 for demonstration) and [2] (see Appendix D). In [3] (see eq. 4), the authors propose to use an analogous formula for training a generative model and conduct a much more exhaustive empirical study. Moreover, [4,5] develop different applied methods building upon this formula.

The downside of this formula is also very well-known in the community, i.e. one has to use large batch sizes to accurately estimate the vector field, which comes with a significant computational cost. Moreover, for any finite size, the estimation of the vector field is biased and the estimation of the loss is biased which would deteriorate generation quality when used to train large-scale models. This limitation is not adequately studied in the paper, e.g. the models are not compared in terms of the training time and memory used.

Given that the main contribution of the paper has already been proposed and the empirical study of this formula is unsatisfactory, I cannot recommend this paper for acceptance.

Minor comments:

- The authors refer to the original FM framework [6] as Conditional Flow Matching, which was proposed in [7].
- The objective proposed in Flow Matching is already tractable, so it is not clear what “a tractable form of the Flow Matching objective” means.
- There is a typo in line 65.
- The derivative notation used is inconsistent with its description in the text.
- From eq. 19, I assume that “dispersion” means “variance”.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[2] Neklyudov, Kirill, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. ""Action matching: Learning stochastic dynamics from samples."" In *International conference on machine learning*, pp. 25858-25889. PMLR, 2023.

[3] Xu, Yilun, Ziming Liu, Max Tegmark, and Tommi Jaakkola. ""Poisson flow generative models."" *Advances in Neural Information Processing Systems* 35 (2022): 16782-16795.

[4] Scarvelis, Christopher, Haitz Sáez de Ocáriz Borde, and Justin Solomon. ""Closed-form diffusion models."" *arXiv preprint arXiv:2310.12395* (2023).

[5] Xie, Tianyu, Yu Zhu, Longlin Yu, Tong Yang, Ziheng Cheng, Shiyue Zhang, Xiangyu Zhang, and Cheng Zhang. ""Reflected Flow Matching."" *arXiv preprint arXiv:2405.16577* (2024).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

[7] Tong, Alexander, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. ""Improving and generalizing flow-based generative models with minibatch optimal transport."" *arXiv preprint arXiv:2302.00482* (2023).

Limitations:
The authors do not provide a necessary literature review nor study the limitations of the proposed approach (see Weaknesses section above).

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a novel approach to training flow-based generative model by deriving the conditional flow matching objective function with respect to the flow function. The author argues that this new method of training will reduce variance, add stability, and ultimately lead to faster convergence. Additionally, the reformulation allow derivation of the exact vector field expression, and in some simple cases, enables the computation of the oracle trajectory solution.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- If the derivations are correct, this methodology could potentially add some innovations in the field of flow matching.

Weaknesses:
- The paper is difficult to follow and lacks clear writing and organization. Specifically, the authors' use of notations is very confusing.
- The mathematical computations do not appear to be very rigorous and some assumptions seem very incorrect. I may have misunderstood some derivations, so please correct me if I'm wrong (see Questions section).
- The experimental results are not very robust or convincing. For instance, while the paper proposes that one of their contributions is the reduction of variance during training, many of the results demonstrate larger variance across numerous, different metrics.
- Overall, the paper feels like it requires substantial revisions and is far from being polished.
Typos:
- Line 65, rho_1$\rightarrow$ $\rho_1$.
- Line 130, practical $\rightarrow$ practical form.

Limitations:
The author has adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This works aims at producing a lower variance loss for flow matching. This is done by using the formula for the ground truth marginal velocity field, estimating it using self-normalized importance sampling, then regressing onto this estimated marginal velocity field.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Variance reduction for CFM is a useful avenue of research

Weaknesses:
- This paper lacks polish. I felt this paper is cumbersome to read, with a lot of heavy notation that can be drastically simplified, whereas the proposed algorithm is quite simple.
- The proposed approach is not exactly novel. The ""Explicit Flow Matching"" objective is just the original ""Flow Matching"" objective where we regress onto the optimal velocity field. The practical implementation being proposed is a simple application of self-normalized importance sampling.
- Importantly, the proposed approach leads to a biased objective (when estimated through a minibatch) where the optimum is not guaranteed to be the correct velocity field. This is not a problem for the low-dimensional experiments but importance sampling becomes more problematic in high dimensions.

Limitations:
The proposed ExFM objective is equivalent to the original FM objective, and the paper should not over-claim this contribution. This objective is intractable, so the use of smart estimation methods is interesting in its own right.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
C3t6GMPnC5;"REVIEW 
Summary:
This paper explores the capabilities of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) in various downstream learning tasks. Despite Mamba's success in some areas, the paper identifies challenges and limitations in achieving performance parity with Transformers on standard benchmarks, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study demonstrates that while Mamba models have robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning techniques, their downstream learning improvements still lag behind those of Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to read.

2. The study shows that Mamba’s recurrent dynamics are robust to small input changes, which is validated both theoretically and empirically. This robustness ensures stability in training and fine-tuning processes.

3. Despite initial shortcomings in ICL performance, extensive experiments demonstrate that mamba models exhibit strong potential for improvement through efficient fine-tuning. The models can achieve up to 81.5% of the ICL performance improvement, highlighting their adaptability with appropriate tuning methods.

Weaknesses:
I appreciate the authors for providing a theoretical analysis to demonstrate the controllability of implementing AMP on Mamba blocks, and the experiments indicate that PEFT is also suitable for Mamba. However, I have several concerns:

1. The authors define the Mamba process as a generalized operation: $x_t=F_{\theta}(x_{t-1},u_t)$, but the actual output of Mamba is $y_t = \bar{C_t}x_t$.Therefore, the theoretical analysis provided in the paper pertains to the stability of the hidden state under small perturbations. Is it possible to extend this analysis directly to the output $y_t$? Since the stability of the hidden state does not necessarily imply the stability of the output.
2. Theorem 1 ensures the feasibility of implementing LoRA on Mamba blocks but focuses on the $W$ matrix, neglecting the consideration of the most crucial transition matrix $\bar{A_t}$ in Mamba. Does this mean that the $\bar{A_t}$ matrix was not subjected to LoRA during fine-tuning? If so, is it possible to consider applying PEFT to the $\bar{A_t}$ matrix as well?
3. As an empirically-driven paper, would it be possible to include more backbones for comparison in future versions? Currently, the only baseline for comparison is Pythia.

Limitations:
The authors raise some limitaions, for example, the lower-precision method is not explored in this paper, but the authors claime to solve them in the future.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Mamba's downstream learning capabilities through two primary aspects: (i) fine-tuning and (ii) in-context learning. Specifically, it examines the training stability and robustness of fine-tuning when mixed precision is applied, as well as Mamba's ability to perform in-context learning. The contributions of this paper include:

- Theoretical analysis of the stable dynamics of Mamba.
- The theoretical analysis is corroborated by the experiments.
- Experimental demonstration of Mamba's limitations on real datasets in terms of ICL.
- ICL performance improvement of Mamba through fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The theoretical analysis section, although I did not verify the proofs, offers valuable insights and paves the way for more complex analyses in future work.
- This paper is well-motivated.

Weaknesses:
The weakness is majorly from the ICL part. 

In fact, the authors show that pretrained Mamba cannot learn well via ICL, but can learn well after fine-tuning. This fact indicates that this limitation does not come from Mamba architecture itself, which is also consistent with the observation of other works such as [1]. Therefore, the limitations observed in this paper is just a general limitation caused by training recipes, which is not Mamba-specific and has been studied in many works. The solution is also standard, and the improvements are also expected since once trains well, Mamba should be able to perform in-context learning as shown in [1]. 

Moreover, many questions are still unclear. For instance, why does Mamba suffer from such limitations? 

Therefore, the study in terms of the ICL part is lack of depth, novelty, and technical contribution. 

In terms of the mixed precision part, there are also many places that are unclear to me. I suggest the authors to use more space in discussing the speciality of Mamba compared to Transformers, and what structure of Mamba caused this problem. If recurrence is the main cause of the problem, having more experiments of similar models such as GLA, linear attention, etc would also help readers to understand more about the phenomenon. 

---

References

[1] Park, Jongho, et al. ""Can mamba learn how to learn? a comparative study on in-context learning tasks."" ICML 2024.

Limitations:
The work is well-motivated, but the study lacks depth.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks at improving start space models or Mamba by enabling mixed precision handling to improve inference and fine-tuning. The results show similar performance with a significantly reduced memory requirement

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
There are extensive results compared to full-precision models
The authors provide a proof of the theorem to back up their claim
The change to the mamba block is clear and easy to implement by others

Weaknesses:
The actual change is relatively minor in quantity but does deliver the author's required memory reductions.
The works don't use the larger models available due to limitations on memory requirements still

Limitations:
the limitations are well discussed at the end of the paper and generally relate to LLM or transformers in general too

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
GxpkSJjbQR;"REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
IGAN7RldcF;"REVIEW 
Summary:
This paper studies a model of content creation and consumption on arbitrary online user-generated content platforms (e.g., YouTube, TikTok). It focuses on a type of Cournot competition in which creators mainly modify their creation volume. The paper provides a description of this model, a theoretical analyses of the Pure Nash Equilibria in this setting, an analysis of how platform designers might use mechanism design to balance consumer and creator utility, a framing of this balancing problem as an optimization problem solvable via (approximated) gradient descent, and experiments using purely synthetic data (sampled ""users"" with Gaussian preferences) and empirical data (users with preferences from the MovieLens dataset, popular in recommender systems).

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this paper provides a strong overall contribution and number of results and insights that will be of interest to a number of different communities -- researchers interested in UGC and online communities, mechanism design, ML for social media, etc.

The clarity is high throughout. The paper begins with strong and well argued motivation, the organization is helpful, and in general the overall narrative of the paper is clear.

In terms of novelty, this paper directly builds on a previous modelling work, but is very upfront about highlighting what the main differences and additions are in terms of contribution. The experiments seem to especially build off the design of [40] (esp. in terms of the synthetic data + MovieLens combination), which might be worth mentioning if that is intentional.

Overall, the potential significance of this work seems potentially high.

Weaknesses:
Overall, I expect readers won't have any major concerns with the theoretical results or experiments (see some minor questions below in the Questions section). 

Rather, the main threat to the significance of this paper is making the case that that a Cournot-style is actually common in the UGC platforms being invoked here. Of course, even if only a few platforms really end up being well-described by the model, the contribution is still very meaningful. That said, a few specific concerns with the current draft:
- a number of specific platforms are mentioned by name: YouTube, TikTok, Netflix, Spotify, and MovieLens. 
- Only data from MovieLens is used (which is very reasonable -- it's a very popular dataset for academic work for good reason).
- However, the named platforms vary quite a bit in terms of their actual creator competition, i.e. one would expect the incentives of a platform like Netflix (which also acts a creator agent, sometimes with substantially higher budget than other creators) to differ quite a bit from TikTok

See ""Questions"" section below for some specific questions about this concern that I think are likely to be in scope of a revision.

With this critique in mind -- that certain platforms might violate the assumptions needed for the model to work well -- I think the current draft may overstate the generality of the conceptual insight.

Limitations:
I do think the current draft could do more to justify the strength of the conceptual claims and/or hold a bit more space to explicitly discuss limitations (primarily, how well requisite assumptions hold across the platforms of interest). See above (Questions).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of the tradeoff between users’ satisfaction and creators’ engagement. Authors first define the traffic competition of creators on user-generated content platforms as a Cournot Content Creation Competition (C4) and establish corresponding PNEs. Based on PNEs, this work identifies the tradeoff between users’ and creators’ engagement and proposes the offline optimization solution to achieve the maximum social welfare by adjusting the exploration level of matching. Theoretical and empirical results are provided to support the effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Authors theoretically model the traffic competition among content creators as a C4 game, identify the tradeoff of user and creator engagement based on their theory, and finally find the optimal platform intervention to maximize the social welfare with the optimization method. Necessary proofs are provided with details. 

2.	Based on the synthetic and real-world datasets, authors validate the phenomenon of the user-creator tradeoff (Figure 1) and the benefit of optimizing \beta (Figure 2). Authors also provide the results in Appendix with different $\lambda$ in the objective $W_\lambda$ to investigate the sensitivity of their solution when the target is changed. 

3.	The manuscript is well-organized and easy to follow.

Weaknesses:
1.	Some assumptions are too strong, including (a) basic setups: Creators are producing contents with the same frequency and the same cost (only relate to the frequency) all the time. (b) platform intervention: all users contribute one unit amount of traffic, neglecting the dominant position of active users. 
2.	Although the effectiveness is guaranteed by the theory and empirical study on small datasets, authors should also present the potential of the solution to be applied in the practical scenarios, e.g., how is the efficiency of the optimization, how to conduct the daily update of intervention strategy. 
3.	Existing works have studied the C3 game. Authors may declare their unique contribution and improvement by considering “Cournot Competition” in their theory establishment and compare with previous methods in empirical validation.

Limitations:
1.	Limited practical value. The assumption is too strong, and the experiments are constrained on small dataset with 1,000 users.
2.	Unclear distinct contributions compared with previous works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new game-theoretical model Cournot Content Creation Competition ($C^4$), that studies the relation between the matching strategy of user-generated content (UGC) platforms and the production willingness of the platform’s content creators. Under certain assumptions, the authors show that the game has a unique Pure Nash equilibrium, and show that increasing matching accuracy elevates user satisfaction but also decreases the overall volume of content creation. Building on this tradeoff, the authors propose an optimization approach that balances the two objectives, providing both theoretical analysis and empirical simulations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Overall well-written.
- Interesting insight on the tradeoff between user satisfaction and creator engagement shown by theoretical analysis.

Weaknesses:
- (Main) Model might be too simplistic - authors assume users consistent produce work of same topic & quality and only changes the production volume.

- (Minor) The authors associate user satisfaction as a short-term goal for the platform and creation volume as a long-term goal for the platform. The authors make an argument for this in line 188-197, although I’m still not fully convinced:
   - The main imbalance that I feel comes from the fact that when I think of *long-term* goals of a platform, it's fundamentally intertwined with the ability for platforms to attract new and keep existing users, which comes from a user standpoint and not from a ""content volume"" standpoint. I get the authors argument when they mention how content creation frequency might harm user satisfaction (line 192 “users can hardly be satisfied by their previously consumed material”). However, given individual's limited attention span, I think this only happens when the number of creators are quite limited, and that it's unclear that a decrease of production frequency from say 2 weeks -> 3 weeks will result in a significant harm to the *long term* viability of a platform causing users to drop out in the long-term.
   - In general, this seems to point to an alternative model where the content volume *comes in* the user's utility model, where users’ utility are not only determined by how they liked the recommended content (which is the utility considered in the paper) but also by the availability of content on the platform, and they might drop out of the platform when their utility falls below a certain level. From this lens, it's less clear that this is a short-term v.s. long-term issue.

- Typo: i =1 -> j=1 in line 92, third and fourth -> second and fourth in line 347

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
cEtExbAKYV;"REVIEW 
Summary:
This paper introduces a new perspective on quantization in diffusion models. It views the quantization error as causing a ""stepback"" in time within the latent space during the denoising process. The paper proposes StepbaQ which adjusts the sampling steps to correct the sampling path and reduce the buildup of quantization errors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well-organized, making it easy for readers to understand and follow the main ideas.

Weaknesses:
* This paper does not provide solid theoretical guarantees or an in-depth analysis of how the temporal shift impacts the scheduled sampling trajectory. 

* The study lacks a comparison of computational efficiency with other quantization approaches.

Limitations:
The method assumes quantization errors follow a Gaussian distribution, it actually requires the scaled latent variable to share the same mean as the quantized latent variable. Thus, it necessitates both a tailored diffusion process design and an accurate error model, which may restrict its applicability or effectiveness.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel perspective of quantization error in diffusion models being equivalent to a ""step back"" in the denoising process. The authors are show that one can effectively quantify this stepback using a small calibration dataset, and continue the diffusion process as usual after correcting for the new effective timestep. This technique doesn't make many assumptions, and hence is very general and can be integrated with many off-the-shelf quantized diffusion solvers. The authors also show convincing experimental evidence on effectiveness of their method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main ideas introduced in the paper are novel and elegant. The perspective of thinking of quantization error as stepback enables the author to devise simple, clean, general techniques to help correct for step size and error accumulation. The empirical evaluation is convincing.

Weaknesses:
While the experimental results (like the ablation study) are helpful and convincing, the paper could benefit from a more thorough analysis/interpretation of the method. For instance, how hard is to estimate the quantization error variance for different models/denoising methods? What does this say about the Gaussian assumption in the first place? What does it mean (in terms of making progress) to take one denoising step if the calculated stepback is >=1 step?

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the StepbaQ, a general strategy designed to enhance the performance of quantized diffusion models which employs a sampling step correction technique to realign the sampling trajectory and eliminate the accumulation of quantization error. Experiments show that the proposed StepbaQ improves performances of diffusion models quantized by off-the-shelf tools.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ This paper is well-written and the topic seems quite interesting and is of high practical value.
+ The numerical results are convincing (but limited).
+ Ablation studies are provided to show importance of different components.

Weaknesses:
- It is not clear that how the definition of stepback will impact performances if the stepback consists of multiple steps.
- It would be good if the authors can provide computational cost and corresponding comparisons with baselines.

Limitations:
Not applicable.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel method to address the issue of accumulated quantization errors in quantized diffusion models. It attributes the quantization error to a ""stepback"" in the denoising process, and it introduces a sampling step correction mechanism to mitigate the adverse effects of accumulated quantization error. Experimental results demonstrate significant performance improvements in terms of FID on the SDprompts dataset, particularly under challenging quantization settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents a novel perspective by interpreting quantization errors as a temporal shift or ""stepback"" in the denoising process. This insight provides a clear understanding of how quantization errors impact model performance.

2.  By calibrating the sampling trajectory, the StepbaQ effectively addressed the accumulated quantization error.

Weaknesses:
1. The author should provide more justification to the argument in Equation (5) that the quantization error can be modeled as a Gaussian random variable.

2. Can the calibrated statistics obtained from one model be directly adapted to different models?

3. Does the method work in consistency models, which perform sampling with only a few steps?

Limitations:
Based on the standard metrics, the algorithm performs well. But further justification on Equation (5) is needed. Furthermore, it would be interesting to see how this method works in the setting that only a few sampling steps are taken, such as in the context of consistency models.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper is well organized and logically understandable. The author reconsidered the quantization error as a ""stepback"" problem and provided concrete theoretical illustration. The author looked into latent variables behind general quantization errors in sampling trajectories, which impressed significantly when first reading. Qualitative experimental results proved the effectiveness of the proposed method.  This is literally a creative, persuasive and well-completed work.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Contents are well organized leading to an easy-reading form. The author shows mature writing style that makes this work logically understandable. 
The core insight that quantization error can be reviewed as ""stepback"" is interesting and creative. Authors have presented detailed proof for their theories and conduct extensive experiments.

Weaknesses:
Despite varies experiments on 2 mainstream diffusion models are conducted, the comparison with other existing methods seems to be slightly thick.  Experimental results on sd v1.5 and sdxl-turbo only takes native PTQ and PTQD into consideration, which makes the result less competitive.

Limitations:
Despite the author gives an interesting insight behind the quantization error arisen in sampling stage of diffusion models, experiments seems to be slightly coarse as some details are not well presented.  More comparison between existing methods and more model setting details are strongly recommended.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Z2f4Laqi8U;"REVIEW 
Summary:
Authors propose a greedy structure learning algorithm in the context of ancestral graphs. The score function is based on the normalized likelihood. The score is decomposable using the concept of ac-components. Limited experimental results are presented as experimental evidence.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The multivariate cross-entropy decomposition is explained in details, especially the link between its definition and its application in the context of ancestral graphs. Leveraging this framework allows a flexible and effective definition of scoring criteria for the class of graphical models.
- The usage of ac-connecting paths and ac-connected subsets for the likelihood decomposition (theorem 1) is the main result of the paper and allows to map the structure of an ancestral graph to a specific estimand.

Weaknesses:
- In the case of ancestral graphs, it's crucial to understand the differences between edge patterns. I would suggest to refactor Figure 1 to improve readability, e.g. isolating different edge patterns combinations with horizontal lines.
- The performance of the proposed solution is poor. For instance, Figure 2 most of the metrics overlap with existing solutions.
- The experiments keep the number of samples fixed even is the number of the parameters of the model increase, reducing the information available for the biggest models. Moreover, a sample size of 100 for BARLEY (114,005 parameters) is rather unrealistic.

Limitations:
- The experimental evidence is poor: few reference models with rather sparse hyperparameters space exploration.
- The search and score algorithm limits the number of possible exploration paths to the barely minimum.
- It is unclear what is the actual computational burden of the proposed score.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors present a novel likelihood decomposition of ancestral graphs, which is based on a novel concept ac-connected subset. With this decomposition, the authors present an efficient hybrid causal discovery method. But it is worthy to note that in the implementation of the causal discovery method, they use the approximate local scores limited to the closed surrounding vertices of each node and edge, but not the exact likelihood score.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The likelihood decomposition of ancestral graphs is very novel and could be useful in the future.
2. For me, I encourage the studies on the MAG/PAG learning methods. It is a hard and fundamental problem.
3. The paper is clearly written.

Weaknesses:
See questions.

Limitations:
No.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper defines a score for ancestral graphs through an inclusion-exclusion expansion and implements it into a hybrid search approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The likelihood decomposition (Theorem 1) is nice and there seems to be a slight improvement in the empirical testing.

Weaknesses:
There are close overlaps to published and preprint works by Hu and Evans, so it is hard to assess the novelty here. For example Theorem 1 (in another form) is from reference 14. 

Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work.

Limitations:
The limitation on the depth of the inclusion/exclusion is discussed in the limitation section, but maybe this wasn't so clear throughout the paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a greedy search-and-score algorithm for causal structure discovery in ancestral graphs which allow for both directed and bidirected edges.The paper provides an explicit decomposition of the likelihood function of ancestral graphs
in terms of multivariate cross-information over relevant ‘ac-connected’ subsets of variables which is related to head-and-tail factorization developed in prior work. In the main theoretical result authors show that cross-entropy and the likelihood function of an ancestral graph can be decomposed in terms of multivariate cross-information for ac-connected components. As a corollary this implies that two ancestral graphs are Markov equivalent if and only if they have the same ac-connected subsets of vertices.
Authors use the likelihood function decomposition in terms of cross-information of ac-connected componenetsto propose a search-and score algorithm that computes scores for nodes and edges and probes orientations of edges to minimize the score.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters)

The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm.

Weaknesses:
The proposed algorithm seems to be only empirical and lacks theoretical guarantees and relies on MIIC as a base of the algorithm. To my understanding, the paper does not provide a guarantee that the algorithm always converges in a reasonable number of steps (even if not to a global minimum). Multivariate cross-information over a large set of variables may be very sensitive to noise, which I think is a weakness compared to algorithms that use cross-information only over small number of (e.g. triples) of variables.

Also as the authors point out Theorem 1 is equivalent to the decomposition established in prior work, which makes novelty of the main theorem somewhat questionable.

I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used.

Limitations:
na

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
tnh4LK72yj;"REVIEW 
Summary:
This work proposed a novel spatiotemporal learning framework CMuST. In CMuST, MSTI is devised to dissect complex multi-dimension data correlations, to reveal disentangled patterns. And RoAda is proposed to extract the task-wise consistency and task-specific diversity. In addition, this paper introduce a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper achieves the best or second-best results in most experiments, validating the feasibility of the proposed method.

2. The paper proposes a continuous learning mechanism that enables the model to continuously learn from tasks. As claimed by the paper, it is ""the first continuous multi-task spatiotemporal learning framework, CMuST, to jointly model learning tasks in the same spatiotemporal domain.""

Weaknesses:
1. The paper considers one of its major contributions to be the proposal of a benchmark. However, after reviewing the code, it seems that the authors did not provide details on how they processed the data.

2. The proposed MTSI module largely uses the Attention module from Transformers, but the authors did not provide any references to Transformers. Additionally, using the attention mechanism to capture relationships is a relatively straightforward design and lacks significant innovation.

3. One of the main problems this paper addresses is the cold start problem for new tasks. However, the paper still involves task-specific refinement, i.e., training is still required. Perhaps the superiority of the proposed module can be validated by comparing the adaptation time to new tasks.

4. In the experiments conducted by the authors, it can be observed that when the number of tasks is one, the model's performance is not superior to many models. This might indicate that the proposed MTSI module is not sufficiently effective.

5. The authors use a simple layer to obtain task summarization $S$. Compared to common designs in many MAEs, using just a linear layer with an activation function seems somewhat simplistic for MAE.

Limitations:
This paper adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a Continuous Multi-task Spatio-Temporal Learning Framework CMuST to facilitate the task-level cooperation in spatiotemporal predictions (mainly for traffic related tasks). The model is composed of three components . Data representation and integration module processes and standardizes diverse urban data into a harmonized format. MSTI modules reduce the complex interactions within spatiotemporal data. RoAda modules iteratively captures the task-wise consistency and task-specific diversity.

This study is generally fine with a relatively novel method for spatiotemporal multi-task learning through prompting (although prompting studies in this field is being rapidly developed). The main contribution is how the prompting is handled. The main problem I found is that the text is not that easy to follow with many jargons, coined (complicated) phrases, e.g., continuous multi-task spatiotemporal learning is a bit confusing -  is it something related to continual (lifelong) learning?

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
S1: The paper provides open access to data and code.
S2: The paper introduces the first continuous multi-task spatio-temporal learning framework for joint modeling of tasks within the same spatio-temporal domain, which is generally a novel method.
S3: The paper validates the proposed model on multiple datasets and tasks, demonstrating its generalization ability.

Weaknesses:
W1: During the model construction process, the paper does not clearly address the inconsistency issue of feature C across different task data. It is recommended to provide detailed explanations either in the data preprocessing stage or within the ""Data representation and integration module"" of the model.
W2: For continuous task rolling, specific operational details of each task model from training to convergence (such as the number of epochs) are not mentioned in the paper. 
W3: Typos such as ""Compressedd"" (line 235).
W4: Modify proprietary terms that lack detailed explanations, such as ""PECPM"" on line 124.

Overall, the paper falls short in its presentation. Hope that it could be revised to ease the understanding.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to enhance urban intelligence. CMuST introduces a Multi-dimensional Spatio-Temporal Interaction network (MSTI) for capturing complex data interactions and a Rolling Adaptation training scheme (RoAda) to iteratively update the model, simultaneously maintaining task uniqueness and leveraging shared patterns across tasks. The framework is validated through extensive experiments on datasets from three cities, demonstrating superior performance against existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. Well presentation. This paper is well-presented and well-organized, providing a clear and comprehensive overview of the proposed methods and their implications.

S2. Good significance. The  proposed CMuST can jointly model different tasks of spatiotemporal forecasting  within the same spatiotemporal domain. This approach not only reinforces individual correlated tasks from a collective perspective, but also helps  understand the cooperative mechanism within the dynamic spatiotemporal system. 

S3. Sufficient and qualified technical contribution. The contribution and innovation of MSTI network lies in that effectively dissecting interactions across multiple data dimensions for improved spatiotemporal representation and commonality extraction, and RoAda training scheme  for ensuring model  to adapt to new tasks and continuously learn commonality and personalized patterns. The coupling of these two major components can well contribute to the ST learning field. 

S4. New benchmark construction and good experiment designs. The construction of benchmark datasets for three cities enriches the research field and provides a solid foundation for evaluating the framework performance. Extensive experiment designs including robustness in data-scarce scenarios, visualized attention scores, and performance variation with task increasing, demonstrate the framework's superiority in enhancing individual tasks with limited data and providing insights into task-wise continuous learning.

Weaknesses:
1. In Section 4.4, there is missing detailed description on how to avoid catastrophic forgetting during task rolling adaptation. It would be beneficial if the authors could provide more experimental details in this regard.

2. Lacking comparison baselines. More baselines which are argued for unified spatiotemporal/time series learning, such as UniST, UniTime should be added for comparisons.

Limitations:
The limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a multi-task spatiotemporal learning framework that helps the model understand the relationships between multiple tasks. The specific contributions lie in proposing MSTI to model the multidimensional spatiotemporal data and RoAda to capture the commonality and personalization among multiple tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The author attempts to construct a spatiotemporal model for continuous multi-tasks, which is an attractive motivation with development potential.
2.	RoAda provides an executable technical solution for spatiotemporal continuous learning.

Weaknesses:
1.	The author mentions multi-task and multi-domain problems several times in the introduction, but these concepts are not intuitively introduced in the paper. Multi-task and multi-domain do not always have a unified consensus in the ST community. For example, does multi-task include regression tasks and classification tasks? Does multi-domain refer to different cities or different modes of transportation? The author should provide more specific scopes for these terms in the introduction.
2.	The author's method of handling ST dependencies is not novel. Using cross-attention techniques to model from the perspectives of temporal dimension, spatial dimension, and spatiotemporal relationships separately is a common processing paradigm in the ST community. The contribution of MSTI can be considered overstated.
3.	The research on ST prediction and continual learning in the related work section is insufficient, lacking analysis of advanced ST prediction models and continual learning models in recent years.
4.	The author argues that continual learning can help spatiotemporal models enhance generalization ability. The theory and experiments in the paper are insufficient to support this argument.
5.	The author neglects experiments on cold start problems.
6.	The comparative experiments did not achieve the best results on all tasks; reasons for this should be analyzed.

Overall, I believe this work proposes a very attractive challenge, but in the end with the old problem, it only solves a standard problem, i.e. the ST multi-task problem. The author proposes Rolling Adaptation to solve this problem, which is a contribution that cannot be ignored. However, this work is incomplete for many issues mentioned in the introduction are not addressed, the definition of tasks is confusing at the beginning, and there is a lack of a pseudocode algorithm to help readers understand the proposed method more accurately.

Limitations:
The limitations have been addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
PAWQvrForJ;"REVIEW 
Summary:
In this work, the authors propose an MIA for LLMs that utilizes prompt calibration to measure variation on model behavior for neighboring inputs. The authors also make a connection with the neighborhood attack from Mattern et al and show how their framework encapsulates such neighborhood-based attacks. Performance on multiple domains demonstrate a clear bump in performance using this attack.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Attempts to improve black-box privacy auditing are useful and needed, especially with more and more model trainers where access is available only via wrappers or APIs.
- Figures and diagrams in the paper are very well made and helpful, and complement the writing well.
- Assessing the method's robustness for different sources of prompts is useful and helps understand worst-case performance when prompt data is completely unrelated to the member distribution.

Weaknesses:
- The core idea here is not very different from distillation-based stealing [1], followed by a slightly modified neighborhood attack.

- L240: There is no clear relationship between ""modest"" paraphrasing in token space, and a corresponding $h$ in the Equation 10, let alone plus/minus. For a language model, $x$ is in the embedding space, so small perturbations to the embeddings will probably not correspond to any actual tokens. Moreover, inputs are sequences and thus perturbations to multiple tokens makes the assumption around a small $h$ even less plausible. Replacing 20% of all tokens is in no way a ""modest"" replacement.

- In Table 1: why not include a version in the baseline that uses the reference models that you train but does not use the neighborhood logic. This would help better understand the contribution of both these steps, serving as an ablation study. 

- L631-633: This is not a grounded statement. Making the connection not only seems unnecessary, but incorrect. While it is nice to have theoretical connections, it is by no means necessary for a good paper. I would urge the authors to rethink the use of theory to make connections in places where they do not seem valid without handwavy justifications. 

## Minor comments
- L32: '[44]' is merely a presentation abstract. [2], for instance, provides an actual systemization of memorization in LLMs
- Figure 1b - what is ""memorization""? Figures should be labeled clearly. 
- The authors seem to be confused about which works introduced LiRA. The authors seem to suggest that Mireshghallah et al introduced LiRA (L45, L102), whereas it was proposed by Carlini et al. [3].
- L49-50: While overfitting means higher leakage, it is by no means an ""assumption"" for MIAs.
- L59: ""Exponential"" is a bold claim here; there are only 3 datapoints. It is not surprising that it declines, as it is explained by theory [4]
- L81: Computing a second-order derivative in an LLM is just not feasible practically. 
- L84: The neighborhood-based MIA requires another masking model, which need not be an LLM.
- L163: If it is over-represented in dataset, it is needless to say a member. The statement here feels vacuous.
- L164: Please also cite appropriate work that introduces this formal dependency on reference models for difficulty calibration [4].
- L168: Missing citation
- The dot on top of $\theta$ in Equation 5 (and other related equations) is barely visible- please use better notation.
- L182: Missing citation
- L229: Why is a negative-definite Hessian problematic? In practice, the (empirical) Hessian is very low-rank and most of its entries are close to zero.
- L235: ""..can be interpreted as kind of"" - this is very handwavy
- Table 1: Please also include TPR@FPRs (like 1% and 0.1% FPR). The table is also missing a lot of attacks like Min-K%++ [5] and MoPE [6]
- Figure 3 Why is Y axis going above AUC = 1.0?


### References
- [1] Galichin, Andrey V., et al. ""GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation."" arXiv preprint arXiv:2405.07562 (2024).
- [2] Hartmann, Valentin, et al. ""Sok: Memorization in general-purpose large language models."" arXiv preprint arXiv:2310.18362 (2023).
- [3] Carlini, Nicholas, et al. ""Membership inference attacks from first principles."" 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 2022.
- [4] Sablayrolles, Alexandre, et al. ""White-box vs black-box: Bayes optimal strategies for membership inference."" International Conference on Machine Learning. PMLR, 2019.
- [5] Zhang, Jingyang, et al. ""Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models."" arXiv preprint arXiv:2404.02936 (2024).
- [6] Li, Marvin, et al. ""Mope: Model perturbation-based privacy attacks on language models."" arXiv preprint arXiv:2310.14369 (2023).

Limitations:
There is no proper discussion about limitations, apart from a 1-2 sentences in the Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a membership inference attack against causal language models, addressing the limitations of previous attacks, such as the inaccessibility of appropriate reference datasets and heavy reliance on overfitting. To overcome these limitations, the authors propose a self-prompt approach to extract reference datasets from LLMs and introduce a membership signal based on memorization. They compare their method with other methods using the AUC metric.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper effectively identifies two key limitations of previous membership inference attacks against LLMs, providing a clear motivation for the proposed targeted solutions.

- The proposed probabilistic variation assessment is interesting and has potential applications beyond LLMs.

- The paper is well-written with a clear logical structure, making it easy to follow.

Weaknesses:
- The main evaluation metrics need to be revised. As established by Carlini et al. [R1], membership inference attacks should be evaluated by computing their true-positive rate at low (e.g., ≤ 0.1%) false-positive rates, rather than using average-case metrics like AUC. Such evaluation metrics have become the de facto standard in evaluating membership inference attacks and are used by many existing works [R2] [R3]. I suggest using two metrics: the Full Log-scale Receiver Operating Characteristic (ROC) Curve to highlight low false-positive rates, and the TPR at a low FPR, which measures attack performance at specific FPRs (e.g., 0.1%, 0.01%).

- The experimental settings are not clearly described. For example, the paper uses a self-prompt approach to extract reference datasets, but it does not specify how many datasets are extracted for each case, which directly relates to the attack costs. This is especially important for commercial LLMs since attacking commercial LLMs will incur high costs in collecting such datasets. Additionally, details on dataset splitting (e.g., how many datasets are used for training the target model) and whether the approach requires training shadow models are missing. If it does not require shadow models, how is the threshold τ in equation (2) determined?

- The paper argues that the proposed probabilistic variation assessment is based on memorization rather than overfitting, but it does not clearly explain the key differences between these two concepts or how the approach improves from the perspective of memorization.

- The reference-based baselines used in the paper are limited and somewhat outdated. Including more advanced baselines, such as [R4], would strengthen the comparison.

- An ablation study to investigate the contributions of the proposed self-prompt approach and probabilistic variation assessment separately would highlight the individual contributions of the paper.


[R1] Carlini, Nicholas, et al. ""Membership inference attacks from first principles."" 2022 IEEE Symposium on Security and Privacy (2022).

[R2] Bertran, Martin, et al. ""Scalable membership inference attacks via quantile regression."" Advances in Neural Information Processing Systems (2023).

[R3] Wen, Yuxin, et al. ""Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries."" International Conference on Learning Representations (2023).

[R4] Shi, Haonan, et al. “Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks”. IEEE European Symposium on Security and Privacy (2024).

Limitations:
Please see my above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes self-calibrated probabilistic variation (SPV)-MIA, a membership inference attack. The novel ideas that SPV-MIA introduces in the space of LLMs: 1) using paraphrasing to obtain samples around the target sample text in the sample domain, and using paraphrased texts to compute probabilistic variation of the target model around the target, and 2) using self-prompting to generate reference data that SPV-MIA uses to train a reference model which is then used to calibrate the probabilistic variation of the target model on the target samples. Experimental evaluation on three benchmark datasets show that SPV-MIA outperforms existing MIAs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Multiple interesting ideas that result in a strong, practical MIA on LLMs
- Good and easy to follow MIA

Weaknesses:
- Some parts of the paper need support, e.g., how PVA is more general than neighborhood attack.
- Datasets evaluated with are tiny; larger datasets should be used
- Writing can be improved for better comprehension

Limitations:
- Writing of the paper is poor; there are too many grammatical mistakes (even in abstract) and many sentences are not properly constructed making it difficult to read the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the membership inference attack on large language models finetuned on private data. Instead of reusing other pre-trained public large language models, the paper proposes a way to generate a reference dataset and finetune this dataset to attain a reference model. With this reference model, the paper further calculates the score by probabilistic variation assessment: symmetrically rephrasing the target data and calculating the average score as the final score. As evaluated in the experiment, the proposed attack has a large margin above the baselines. The paper also conducted ablation studies to understand the design of important and demonstrates the robustness of the algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm is novel. It proposes a way to generate the reference dataset via self-prompting, which leads to a high-quality reference model for a better membership inference attack.
2. The results seem very promising. The performance of the proposed attack has a large margin over the baselines.
3. The algorithm is robust to assuming the domain-specific public data. Even if it is relaxed to irrelevant data, the proposed attack still achieve good performance.

Weaknesses:
1. It is not fully in the black-box setting, because it assumes the knowledge of the same pre-trained model and the access of its parameters.
2. The algorithm has two novel parts: propose a way to generate the reference dataset for learning a reference model and a novel score similar to probabilistic variation assessment. It might be important to study how much each component contributes to the final improvements.

Limitations:
The paper has well-discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
FFJFGx78OK;"REVIEW 
Summary:
This paper proposes to combine the consistency model (CM) with diffusion denoising bridge models (DDBMs) to build a consistency diffusion bridge model (CDBMs), which includes two paradigms of consistency bridge distillation and consistency bridge training. The experiments are conducted for image inpainting and image-to-image translation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper is well written and organized, with a clear structure for readers to follow up easily.

+ The unified view of design space of several different DDBM models is an interesting summary for a clear comparison over existing works.

+ The experiment's results especially with NFE=2 are promising to be comparable with other models with larger NFE steps as shown in Figure 4.

Weaknesses:
- The proposed method is reasonable but may not be very surprising. This work looks more like a direct combination of CM and DDBM, which models have been introduced a lot in previous works, including consistency distillation and consistency training. 

- Question about the motivation to combine CM and DDBM. CM is designed to reduce NFE steps requirement for diffusion sampling. DDBM models can also be able to achieve this goal by learning transformation between two data distributions, such as what is claimed in I2SB, while sacrificing the advantage of unsupervised learning. What is the key motivation and necessity to combine these two together?

- In experiments results, some common metrics like PSNR and SSIM are not reported. In Table 3, the compared methods are most unsupervised learning approaches which are not trained with paired data. In this case, more NFE steps are expected. Therefore, for a fair comparison, the NFE for the compared baselines in Table 2 and 3 may also need to be noted. 

- For qualitative results, the comparison with previous DDBM method under the same NFE would be appreciated, since some DDBM method such as I2SB and CDDB [1] also claimed enabling image generation or restoration within a very few NFE steps from 1-4. It would be important comparison to compare with these baselines under the same experiment settings including NFE.

- Due to the requirement for paired data training, the proposed model needs to be retrained to different tasks, which are limited compared with diffusion-based methods. What is the computational complexity and parameter generalization robustness to retrain the model on a new task? 


[1] Chung, Hyungjin, Jeongsol Kim, and Jong Chul Ye. ""Direct diffusion bridge using data consistency for inverse problems."" Advances in Neural Information Processing Systems, 2023.

Limitations:
The author discusses the limitations of the proposed method in the last section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on efficient sampling of the denoising diffusion bridge models (DDBMs) or similar. In particular, the paper proposes to extend the consistency models (CMs) to DDBMs.

CMs are generative models developed in the context of improving the sampling cost of diffusion-based generative models (DBGMs). Note that sampling of the probability flows of the DBGMs runs the integrators up to a few hundred steps or more. Similar to common DBGMs, assume we have a forward diffusion whose initial distribution—data—is denoted by $x_0$. In particular, when we rewrite the forward diffusion in the probability flow (PF) form, we get an ODE whose marginal is equivalent to the original forward diffusion. We further assume that we have the ODE-integrators that run $t$-decreasing direction starting from any given $t \in (0, T]$ to $0$. Then, the CMs train to predict the output of the integrators, similar to denoising autoencoder, but the perturbation here is deterministic. Training such models uses the consistency condition. Once we train the CMs, we can use them instead of running a few hundred steps of DGBMs.

DDBMs are input-conditional bridge models. More specifically, assume we have two distributions and their joint distribution, e.g., one is an image distribution, and the other is its edge images. The bridge models learn a diffusion to connect one distribution to the other. Unlike common bridge models, however, DDBMs conditions on its initial point, e.g., an edge image, so that the path generated by the learned diffusion preserves the information of the initial value. For several applications, such as image-to-image translations, these DDBMs show several benefits, unlike common Markovian models; in particular, the initial coupling of the joint distributions is preferred. Nevertheless, DDBMs are also required to run the SDE-integrators (or ODE-integrators for its PFs) a few hundred steps, similar to any other diffusion-based models.

In order to overcome sampling inefficiency in DDBMs, the paper proposes to extend CMs to DDBMs.

Finally, the authors demonstrate the proposed method's effectiveness via various experiments, including image-to-image translations and image inpainting.

---------------------------------------
Updated the rating from 4 to 7 after the authors' rebuttal

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
One of the paper's key contributions is the introduction of a novel solution to improve the sampling of the DDBM-like models. The proposed method addresses one of the significant challenges in diffusion-based models and offers a fresh perspective on how state-of-the-art techniques can be extended to the DDBM-like models. Furthermore, the proposed method exhibits comparable performance and scalability compared to popular diffusion-based generative models with a fewer number of evaluations, further underscoring the practical effectiveness of the proposed method.

Weaknesses:
The authors tackle a highly interesting problem and reach out for an excellent solution. However, I find that the paper requires a major revision due to a few theoretical loopholes in extending the consistency models (CMs) to the denoising diffusion bridge models (DDBMs).

For example, in expressing CM in the form of a stochastic differential equation (SDE), the paper uses a definition that is not mathematically well-defined. Particularly, in Equation 2, the notation of a reverse-time standard Wiener process is mentioned. However, a reverse-time standard Wiener process is not defined in general; more precisely, it is tricky to define such a concept. The reason is that the definition of the It\'o integral is highly sensitive to the direction of time. Consequently, the change of variables used in calculus cannot be applied to SDE directly. As a result, we need to define an SDE-specific chain rule called Ito's lemma. Accordingly, one cannot rewrite an SDE by changing the sign of time. In order to apply the chain rule of calculus, the SDE literature uses the Stieltjes integral instead of the It\'o integral, and a corresponding SDE notation needs to be defined. If this approach were taken for the submitted paper, however, the denoising diffusion and consistency model would need to be defined accordingly.

In conclusion, this paper's development deviates from the fundamental assumptions of the It\'o integral-based SDEs. Note that the original CM circumvents this issue by using only ordinary differential equations (or partial differential equations). Therefore, unlike the It\'o integral, it is free from concerns about the time directions. However, this does not apply to SDEs.

In my understanding, one would be able to reach the same conclusion as the submitted paper while sticking to the fundamentals of SDEs (related to It'o\' diffusion). However, such modification would require a major revision from the current submission. 

In addition, a few statements need to be addressed. In particular, to get an unbiased estimation of the expectation in Equation 15, one needs to sample $x_t$ and $x_T$ first and then sample $x_0$. However, the authors state that we can achieve this by sampling $x_0$ and $x_T$ first and then $x_t$. Such an estimator is biased except when we use a single sample.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors combine Denoising Diffusion Bridge Models (DDBMs), that build a transport map between two arbitrary target distributions (that can be coupled) through a stochastic process, with recent advances on consistency techniques [1], that were originally designed for denoising diffusion models. The motivation herein is to speed up the inference process of DDBMs, that turns out to be costly due to a high number of neural network evaluations. Following the approach from [1], the authors propose to learn the *consistency function* of the probability flow associated to an arbitrary DDBM, which outputs a prediction of the target distribution given any input of the DDBM process $x_t$, for any time $t$. To do so, they derive a consistency loss, which can be seen as the natural extension of the loss of [1] to the bridge setting. Then, they propose two paradigms for this approach: either *consistency bridge distillation*, which assumes to have access to a pretrained DDBM model, and *consistency bridge training*, where no model is available. Their formulation encompasses popular designs of diffusion bridges such as the Brownian brigde, the Image-to-Image Schrödinger bridge [2], the original bridges designed for DDBMs [3]... The authors provide practical guidelines on the neural network parameterization and preconditioning, the design of the ODE solver and the consistency schedule. Finally, they conduct numerical experiments on high-dimensional datasets that demonstrate that the proposed consistency bridge model is faster than the original model while having equal or better generative performance. They notably find out that fine-tuning a pretrained DDBM with consistency training provides better performance under lower computational budget than distillation.

[1] Consistency models. Song et al. 2023.

[2] I2SB: Image-to-Image Schrödinger Bridge. Liu et al. 2023.

[3] Denoising diffusion bridge models. Zhou et al. 2023

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- Although this paper adapts lots of elements from [1] to the bridge setting, which may amortize the novelty of this contribution, I think that this contribution may be impactful for practical usage as it encompasses a large formulation of DDBMs and demonstrates great empirical performance, see Section 4.
- The paper is well written and easy to follow. In particular, the mathematical statements are clear to understand.
- The presentation of the background is well done and helps the readability of the paper.
- The comparison with related work in the numerics section is well conducted. 
- All details on the experiments are available in the paper, which is a really good point for reproducibility.

[1] Consistency models. Song et al. 2023.

Weaknesses:
- The current paper does not exhibit any particular novelty compared to the setting from [1], but this is not a major weakness to me.
- I think that the paper does not provide enough intuition on the design of a good consistency schedule.

[1] Consistency models. Song et al. 2023.

Limitations:
The limitations are clearly indicated in Section 5. The authors notably acknowledge that bridge consistency models suffer from numerical instability (same as classic consistency models).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Motivated by faster sampling time, the authors investigate consistency model techniques applied to denoising diffusion bridge models, a particular formulation of conditioned diffusion model whereby the forward process is a mixture of diffusion bridge SDEs, with a conditioned terminal point.

In particular, the authors adapt typical consistency model training regimed known as ""consistency model training"" - training the model from scratch -  and ""consistency model distillation"" - distilling from a pretrained diffusion (bridge) model.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
As far as I am aware consistency methods have not been applied to diffusion bridge models, though the significance of this is unclear vs performing consistency training on diffusion models or flow matching models.

The empirical performance showcased by the author's implementation appears competetitive to the baselines the authors consider.

Weaknesses:
Denoising diffusion bridge models are a particular formulation of conditioned diffusion model whereby the forward process is given by the SDE of a diffusion bridge, with a conditioned terminal point. This has the benefit of data to data translation, but in my opinion the authors exaggerate the significance of this and I would not consider it ""a new family of generative models"" [line 4]. Given the consistency methods applied to diffusion models transfer trivially to diffusion bridge models, I am of the opinion that the novelty of the methodological contribution is quite limited.

Furthermore I would also question the usefulness of data to data generative models which are deterministic. For any ill-posed inverse problem, practitioners would want to be able to sample from many possible uncorrupted sample given a single corrupted sample, including some example provided here such as for inpainting.

For consistency models or indeed any flow type model, a Gaussian is typically chosen as marginal which adds stochasticity and permits the PF ODE to match the diffusion model in marginal distributions. This is not the case for diffusion bridge models with conditioned terminal point, there is no longer any stochasticity and hence the ODE derived cannot match the marginals of the stochastic generative backward process. Hence I wonder if PF-ODE of a diffusion bridge model even makes sense in the same way as a diffusion model? Given all the consistency approaches rely on the deterministic component, I believe this is quite fundamental and needs to be addressed.

There are a number of related works that could be discussed. In particular Augmented bridge matching (https://arxiv.org/abs/2311.06978) which draws an equivalence between denoising diffusion bridge models and conditioned bridge matching.


Other:
>  tractable class of Schrödinger Bridge and simulation-free, non-iterative training procedure [32, 6]

[32,6] consider regular bridge matching without any connection to an optimal coupling and hence although tractable just a bridge matching model and not a schrodinger bridge. This is a common mistake.

Limitations:
See weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
r7mj17BKzw;"REVIEW 
Summary:
This paper introduces SuperEncoder, a novel approach to Quantum State Preparation (QSP) that aims to combine the scalability of Approximate Amplitude Encoding (AAE) with the speed of traditional Amplitude Encoding (AE). SuperEncoder uses a pre-trained neural network to directly estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state, eliminating the need for iterative parameter tuning during runtime. The authors explore different loss functions for training SuperEncoder, finding that state-oriented training using fidelity as a metric (L3) performs best. They evaluate SuperEncoder on synthetic datasets and downstream tasks like Quantum Machine Learning and the HHL algorithm, comparing it to AE and AAE. Results show that SuperEncoder achieves runtime similar to AE while maintaining the scalability of AAE, but with some degradation in fidelity. The impact of this fidelity loss varies across applications, being more tolerable in QML tasks than in precise algorithms like HHL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a novel approach to Quantum State Preparation with SuperEncoder, which innovatively combines the strengths of existing methods (AAE and AE). The idea of using a pre-trained neural network to directly estimate quantum circuit parameters is a nice solution to the QSP problem.

Quality: The research demonstrates high quality through its comprehensive experimental design. The authors explore different loss functions, provide detailed analysis of their landscapes, and evaluate the method on both synthetic datasets and real-world applications. The comparison with existing methods (AE and AAE) across multiple metrics (runtime, scalability, and fidelity) shows a rigorous approach to validation.

Clarity: The paper is well-structured and clearly written. Complex concepts are explained in an accessible manner, with helpful diagrams (like Figures 2 and 3) to illustrate key ideas.

Significance: SuperEncoder potentially represents a step towards more efficient QSP, which is crucial for many quantum algorithms.

Weaknesses:
1. The gradient evaluation of the loss function (e.g. Eq. 1) requires computing the derivative of the state $\rho$ with respect to model parameters. As the authors acknowledge, this could become complicated on real devices due to the enormous cost of quantum state tomography. The authors work around this by using the parameter-shift rule to compute the gradient. However, the parameter-shift rule does not scale as well as classical backpropagation with autodiff (see https://openreview.net/forum?id=HF6bnhfSqH -- I guess a citation to this work would be relevant here). This casts doubts on the whole scalability of this method.

2. Again related to scalability, the number of input neurons to the model has to be $2^n$. This again doesn't look too scalable past 20 qubits, which can already be realized experimentally.

Limitations:
Limitations have been discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a model, namely SuperEncoder, to solve the quantum state preparation problem. Instead of evolving the parameterized gates to generate the target quantum state, they train a model to predict the rotation parameters from the target states.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Solve the quantum state preparation problem from a new perspective.

Weaknesses:
1. Poor results. The results seem ok with four qubits but decrease way too fast when increasing the number of qubits. The proposed method is not comparable to previous methods.
2. It is actually impossible to use an ML model to predict the parameters. Since training the AAE ansatz is a non-convex optimization problem, finding the optimal parameter is indeed an NP-hard problem. There are infinitely many pairs of quantum states and parameters, and I wonder how the size of the training set would scale with the number of qubits. 
3. The training overhead is non-negligible. If we are preparing a quantum state that is beyond the simulation power of classical devices, the evaluation methods based on state fidelity would need an enormous number of quantum circuit executions, which I suspect would not be much less than training the AAE.

Limitations:
Naive ideas with poor experimental results.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the problem of Quantum State Preparation (QSP), which is critical for quantum computing but requires a circuit depth that scales exponentially with the number of qubits, making it impractical for large-scale problems. The authors propose SuperEncoder, a pre-trained classical neural network model designed to estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state. This approach eliminates the need for iterative parameter tuning, making it a significant advancement towards iteration-free approximate QSP. 

Contributions

1. Introduction of SuperEncoder, which pre-trains a classical neural network to estimate PQC parameters directly, bypassing the need for iterative updates.
2.  Provides empirical evidence that SuperEncoder significantly reduces the runtime for quantum state preparation compared to traditional methods, thus enhancing the efficiency of quantum algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See  Contributions.

Weaknesses:
1. [Scalability Issue]
The most significant drawback of this work is its poor scalability. Since the input to the SuperEncoder is $2^n$ dimensional, the number of qubits cannot be too high, such as exceeding 20 qubits. This limitation severely restricts the applicability of the SuperEncoder to larger quantum systems. Discussing potential strategies to overcome this drawback would greatly enhance the practical value of the SuperEncoder.

2. [Barren Plateau Problem]
Another major issue is that, even within a reasonable range of qubit numbers (e.g., 10-20), training the SuperEncoder is challenging due to the barren plateau problem. Consequently, the SuperEncoder is likely only suitable for situations involving fewer than 10 qubits. In these cases, the time difference between AAE and SuperEncoder is not as significant as one might expect, which greatly limits the potential impact of this work.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
7VNvM9SnRE;"REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Ktx95ZuRjP;"REVIEW 
Summary:
This paper introduces a novel framework called Proactive Infeasibility Prevention (PIP) to enhance the capability of neural methods in addressing complex Vehicle Routing Problems (VRPs) with interdependent constraints, such as the Traveling Salesman Problem with Time Window (TSPTW) and TSP with Draft Limit (TSPDL). The authors propose integrating the Lagrangian multiplier method and preventative infeasibility masking into the solution construction process to improve both the feasibility and optimality of solutions. Additionally, an enhanced version, PIP-D, employs an auxiliary decoder to predict infeasibility masks, potentially reducing computational costs during training. The paper presents extensive experiments demonstrating the effectiveness of PIP in reducing infeasible rates and improving solution quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The PIP framework represents an innovative approach to integrating constraint awareness and preventative measures within neural VRP solvers.

The use of an auxiliary decoder for predicting infeasibility masks is a good try.

The paper provides extensive empirical validation demonstrating the effectiveness of the proposed methods.

The paper is well-organized and clearly presents the problem, proposed solutions, and experimental results. The use of figures and tables effectively supports the textual content, aiding in the reader's comprehension.

Weaknesses:
The ideas of using the Lagrangian multiplier [1] and preventive mask functions[2] are not new.

The code is not provided, which limits the reproducibility of the work.

It's not clear how to obtain the training label of infeasibility mask.  The computation could be expensive to get a global and accurate infeasibility mask.

Some related work from recent AI and OR fields are missing.

The paper could benefit from additional experiments on a wider range of VRP variants and real-world datasets to further validate the generalizability of the proposed methods.

The presentation could be improved by providing more context on how this work relates to and advances the current state-of-the-art in neural VRP solvers.

The computational efficiency of the PIP-D framework should be more thoroughly analyzed, especially in terms of how it scales with problem size.

The paper may lack a deeper discussion on the theoretical underpinnings of the proposed methods and their potential implications for the broader field of combinatorial optimization.

[1] Qiaoyue Tang, Yangzhe Kong, Lemeng Pan, and Choonmeng Lee. Learning to solve soft-constrained vehicle routing problems with lagrangian relaxation. arXiv preprint arXiv:2207.09860, 2022.

[2] Hou, Qingchun, et al. Generalize learned heuristics to solve large-scale vehicle routing problems in real-time. The Eleventh International Conference on Learning Representations. 2023.

Limitations:
The authors have acknowledged some limitations, such as the potential inability of PIP to improve performance on all backbone solvers and VRP variants. However, the paper could benefit from a more detailed discussion on the robustness of the PIP framework when faced with different levels of constraint hardness in real-world scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of predicting feasible solutions for VRPs with complex constraints. It introduces two novel methods to enhance existing algorithms: i) integrating constraints directly into the optimization objective using the Lagrangian Multiplier Approach; ii) PIP framework, which proactively excludes potential nodes during solution prediction to prevent future infeasibility. Besides, a neural decoder PIP-D is proposed to reduce computational complexities.
Experiments on two VRP variants demonstrates significant advancements in feasible prediction compared to the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-structured with a logical flow.
2. The feasibility issue investigated are critical, and the proposed PIP framework address it well.
3. Extensive experiments and detailed clarifications are made to demonstrate effectiveness.

Weaknesses:
Major concerns:

- **Computational cost.** From the numerical results, PIP incurs approximately twice the computational time compared to baseline neural solvers (e.g., AM and POMO). Despite attempts to mitigate this by introducing neural decoder PIP-D, the reported results don't exhibit a corresponding reduction in time. The authors may clarify this more.

- **Lack of comparisons** between different step numbers in PIP. The authors only conducted experiments of one-step PIP. It is crucial to include comparisons with different step numbers to assess the impact of the ""looking ahead"" masking strategy. For instance, additional results from zero-step PIP (directly masking nodes violating constraints) and two-step PIP. Here, I emphasize zero-step PIP since its masking strategy differs from the ones of the baseline neural solvers, as I understand it. 

- Only two TSP datasets are considered. The paper is titled ""Learning to Handle Complex Constraints for **Vehicle Routing Problems (VRPs)**"", however both datasets are **variants of TSPs**. The key difference between VRPs and TSPs is **capacity constraints**. I would like to see VRPs with various complex constraints (such as time windows, pick-up and delivery, split delivery etc.), otherwise, the authors should change the title and consider more TSP variants. 

- Heuristic algorithms for TSPs and VRPs with complex constraints have been well developed (such as LKH3). I do not see that the proposed algorithm outperforms the state-of-the-art heuristic algorithms (not neural algorithms).

Limitations:
The author has discussed the limitations in section 6: one potential limitation is that it may not improve performance on all backbone solvers and all VRP variants.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper  propose a novel Proactive Infeasibility Prevention (PIP) framework to advance the capabilities of neural methods towards more complex VRPs, and further investigates the Lagrange multiplier method for soft objective in VRPs, presenting the PIP (& PIP-D) for the hard case where Lagrange multiplier method difficult to find feasible solution. And the experiments show that the PIP is prior to Lagrange multiplier method and has generality to TSPTW and TSPDL.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper contributes to the solution of the soft constraint objectives of VRPs.
The author's perspective on the problem is also good; that is, the constraint itself is an NP-hard problem, which was not mentioned in reference [8], and the description of why the constraint itself is NP-hard is very clear, that is, because of the irreversible impact of the first selected node on the selection of subsequent customer nodes.

Weaknesses:
The author's method actually addresses the shortcomings of the Lagrange multiplier method, but it is not mentioned in the Introduction, and the problems raised in the Introduction can actually be solved only using the Lagrange multiplier method.   
The method involves complex calculations, particularly when integrating the Lagrangian multiplier and PI masking, which can be computationally intensive​. 
The paper does not thoroughly address the scalability of the proposed method when applied to extremely large datasets, which may limit its practical application in some real-world scenarios​​.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Proactive Infeasibility Prevention (PIP) framework to enhance the ability of neural methods to handle complex constraints in Vehicle Routing Problem (VRP).
The PIP framework integrates the Lagrangian multiplier to enhance constraint awareness and introduces preventative infeasibility masking to proactively guide the solution construction process. Additionally, an extended version called PIP-D employs an auxiliary decoder and two adaptive strategies to learn and predict masking information, reducing computational costs during training.
These methods were extensively tested on different levels of constraint hardness in the Traveling Salesman Problem with Time Window (TSPTW) and Traveling Salesman Problem with Draft Limit (TSPDL) variants. The results demonstrate that the proposed methods enhance the capabilities of neural methods, significantly reducing infeasibility rates and improving solution quality.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	Adaptability to Complex Constraints: The paper devises a method to apply machine learning to constraint-based problems, which have been challenging to handle with deep learning due to the difficulty in finding feasible solutions. 
2.	Experimental Validation: Experimental results demonstrate that the proposed method can compute feasible solutions more effectively compared to traditional methods.
3.	Integration with Constructive Methods: The proposed approach can be combined with constructive methods, enhancing its practical applicability and flexibility.

Weaknesses:
1. Generality of the Trained Model: It is unclear whether the trained model generalizes well to unseen instances, raising concerns about its robustness and applicability to different problem settings.
2. Lack of Theoretical Justification for PIP: The paper does not provide a theoretical justification for the superiority of using the Proactive Infeasibility Prevention (PIP) method, leaving its theoretical advantages unproven.
3. Lack of Classical methods with time limit: There are no comparison with the classical methods (e.g., LKH3) with a time limit. I am concerned that LKH3 could find ""good"" feasible solutions within a short time (e.g., within 5 minutes).

Limitations:
The limitations of this work are discussed in the last section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
cEb305kE1V;"REVIEW 
Summary:
The authors introduce the idea of implicit optimization, and coupled feature extraction for images, to achieve robust image registration. I liked the overall idea and was eager to gain insight into how implicit optimization

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I really liked the overall ideas here. 

An implicit optimization layer does indeed address a DL shortcoming that hs been pointed out before (several DL methods show that some fine-tuning of the output deformation field will improve registration for many domains/moels). 

I like the idea of using this in combination with a feature extractor and using those learned features to drive the optimization, with potential benefits down the line. 

The paper is also pretty clear and concise, which I like.

There are extensive experiments, with measures of variance, which is a great start.

Weaknesses:
Unfortunately, I found the rest of the paper (beyond the core idea) lacking and having several weaknesses. 

Importantly, the authors mischaracterize important relevant literature and conceptual ideas, that I think are incorrectly described in the motivation, and not compared to properly in the experiments. This makes it challenging to assess and gain insights from the contribution.

Incorrect characterization: the authors say a few statements that to me seem directly false (and are important to the paper). For example, on line 50 they say ""Moreover, design decisions like sparse keypoint learning for affine registration [103, …] do not facilitate dense deformable registration"" -- they repeat this in several parts of the paper (e.g. line 107). This is wrong -- their first citation, for example, 103, uses keypoints for *deformable* registration (along with affine). A few lines lower they say ""Current DLIR methods are not robust to minor domain shift like varying anisotropy and voxel resolutions, different image acquisition and preprocessing protocols [62, 53, 70, 43]."" -- which is incorrect and not supported by the citations. First, citations 62, 53, 70 are from before 2012 and do not discuss DLIR methods at all (but just general registration), whereas citation 43 *is explicitly tackling and achieves robustness to domain shift*. It may well be that the authors' method does better (I am not sure, see below), but the claim is incorrect. Many other papers tackle distribution shift in DL registration -- see Mok et al, 2023. Another crucial omission is related to the authors' claim that DL methods may not output local minima results -- which is true, but plenty of works propose to take the output of neural networks and perform a bit of instance-specific optimization of the resulting field to get to that local minima -- essentially 'fine-tuning' the field for a couple of seconds (e.g. VoxeMorph TMI 2019, but plenty of other methods as well after that for example from Matthias Heinrich's group). This is crucial to the current paper, since the proposed method essentially does the same thing at inference -- runs a forward neural network (albeit just as a feature extractor), and then performs an optimization for the image pair -- and while it's done differently (and more elegant in some sense) than the existing literature, these approaches are very related. Overall, I was excited about the method but overall found the motivation/related-works either misleading or lacking rigor -- perhaps the authors are simply not aware of the abilities of existing literature mentioned above, but this does  limit the novelty and insight substantially

In the experiments, it seems to me that some obvious results are missing:
. I am not sure why methods used in Figure 3 (e.g. [43]) are missing from Table 1. It seems like a crucial comparison. Deformable KeyMorph [103] is missing from the whole experiments section, and is close to the existing method in that it separates the feature extraction (there via keypoints, but using a parallel net) from the optimization. Training keymorph on oasis and testing it seems like an important comparison if we are to extract some insights into how to decisions in the proposed method (the feature extractor and the implicit optimization) improve our insights in the field.
. Overall the results in Table 1 do not seem impressive -- comparable at best with existing methods. This is totally okay in my book, if the authors are able to communicate other interesting insights. Unfortunately, I do not believe this is the case.
. In the domain-shift section the authors show that their method tens to outperform the DL methods. However, their method gets the benefit of doing instance-specific optimization (the proposed layer) after feature extraction, at the cost of some GPU work for each pair. This is what instance-specific optimization does at the end of DL methods (as discussed below), which was employed in several papers, but this is not included in the comparison! This is a peculiar omission to me -- it should be included for completeness, but importantly it is also crucial to understand whether the proposed method behaves differently -- perhaps there is some advantage, in several situations, to the proposed method, or perhaps it offers more guarantees, etc -- we simply don't know. Minor: it would also be interesting to understand what are the limits of this domain shift of this model -- does it generalize to more substantial variations in modality, or 7mm slice spacing found in clinical sequences?
. I also find the claim that DL methods do not work without crops peculiar - most DL methods are convolutional and hence not size specific, and some (e.g. SynthMorph, which the authors refer to here) does not even require both images be of the same size.
. Since one of the contributions of the paper is the parallel feature extraction (to be used with the optimizer), it seems to me that it would be an important ablation to take the features of some robust method (does not have to be registration, even a domain-shift-robust segmentation network will do, or a 'robust foundation model', etc) and see if that can be combined with an optimizer. This would help provide insights if the formulation of the proposed model and the end-to-end training is useful. 
. The claim in 4.4 is also missing some reaosnable comparison -- would it not be possible to take the displacement field of any other DL method, initialize your favorite parametrization (freeform, diffeomorphic, etc) with that, and run the (any) optimizer? It seems like this is easily doable and a reasonable comparison?

Limitations:
Yes the paper has a limitations section. It would be nice if the authors could comment on how this method can be used on CPUs -- by far the standard hardware available to non-ML users (neuroscientists, clinicians, etc).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel image registration framework that aims to bridge the gap between classical and learning-based approaches. It incorporates fidelity optimization directly into the neural network as a layer. The framework employs end-to-end implicit differentiation through an iterative optimization solver, ensuring that the features learned are both registration and label-aware. Additionally, the warp functions derived are guaranteed to represent local minima of the registration objective within the feature space. The authors report that this framework performs exceptionally well on in-domain datasets and remains robust against domain shifts, such as anisotropy and variations in intensity profiles. Furthermore, the framework is designed to allow seamless switching between different transformation representations at test time without the need for retraining.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper's motivation is both clear and innovative, effectively merging classical optimization techniques with neural networks by embedding an optimization layer within the network. This enhances data consistency and steers the optimization toward local minima.
2. A significant technical achievement of this paper is the backpropagation of gradients through the optimization layer using the implicit function theorem. This demonstrates the paper's technical depth and is the key contribution of the paper.
3. The analysis of loss landscapes provided in the paper is insightful. The flattening of the feature space by neural networks introduces a wider range of possible gradient directions, which, when combined with fidelity loss, enhances overall performance.

Weaknesses:
1. A major limitation is the framework's registration accuracy, as measured by the Dice score, which does not demonstrate clear advantages over neural-network-only methods. For instance, in Table 1 on the OASIS dataset, the proposed LKU-Net variant achieves a Dice score similar to that of TransMorph-Large. This calls into question the practical benefit of integrating classical optimization into the network.

2. The absence of specific smoothness measurements in comparison with other methods, such as the percentage of negative Jacobian determinants (||J||<0) or the standard deviation of the logarithm of the Jacobian determinant, is a significant oversight. Without these metrics, it's unclear whether the observed increase in Dice score represents a genuine improvement or merely a trade-off with the deformation field's smoothness.

3. The paper lacks clarity in the reproducibility of results compared to other methods. For example, the learn2reg OASIS leaderboard indicates that LKU-Net can achieve a Dice score of 88.5 on the OASIS dataset without explicitly optimizing for smoothness. Similarly, TransMorph scores 88.5. Additionally, in Table 4, the performance of LKU-Net with Dice supervision is paradoxically worse than without, which is counterintuitive and raises concerns about the implementation fidelity and methodological consistency.

4. While the introduction of an optimization layer with a fidelity function is a key contribution, the paper falls short in comparing its method to other registration methods that also combine learning with optimization. This lack of comparative analysis leaves unanswered questions regarding the true effectiveness and novelty of the proposed method compared to existing approaches.

Limitations:
The paper commendably integrates a fidelity function with an optimization layer into a neural network, but there are several limitations:
1. The optimization layer has not demonstrated significant performance improvements compared to traditional neural network methods on OASIS leaderboard, and the implementation issues in the other datasets/methods raises concerns about the completeness of the result. Clarifying scenarios where this integration is advantageous could enhance the paper's impact.
2. The absence of quantitative smoothness metrics for registration performance is a critical gap. Introducing these metrics would strengthen the comparative analysis with existing methods.
3. The paper lacks a comprehensive comparison with major competitors combines optimization with learning. Expanding this analysis would provide clearer insights into the framework's unique contributions.
4. Concerns about implementation and reproducibility persist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced DIO, a differentiable implicit optimization layer to a registration network that aimed to bridge the gap of classical-learning-based image registration, considering the incorporation of weak supervision like anatomical landmarks into the learned features. The authors decoupled feature learning and optimization and trained a deep network to predict multi-scale dense features registered through a black box iterative optimization solver.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**S1.** The proposed model significantly improves over SOTA models on domain shift experiments.

**S2.** The paper is well-written

**S3.** Multi-scale optimization seems an interesting approach that might be a possible module that can be integrated in deformable image registration.

Weaknesses:
**W1. Possibilities of getting artifacts from different voxel sizes.** How do different voxel sizes ensure that the velocity or transformation field is differentiable or invertible? I believe this approach might introduce artifacts and lose fine details when propagating the source image to match the target image. And how processing the image features independently will preserve the diffeomorphism property in generating the transformation field? Can the image matching term efficiently capture the intensity differences likewise treating the input images as pairwise? Overall, treating the input images separately from the feature extractor raises several questions regarding the credibility of the transformation field $\phi^*$. 

**W2. Motivation for using multi-scale optimization.** I found the motivation of using multi-scale is somewhat underdeveloped. What is the rationale behind using such kind of optimization considering different source/target image features? 


**W3. Applicabilities of learned multi-scale dense features from sparse images.** In Sec. 4 the authors tried to show that DIO learned interpretable dense features and compared to the classical methods DIO preserved the gradient in the loss function. On the other hand, the authors also discussed that deep networks recovered affine transform with $~90$% overlap. I wonder what is the advantage of capturing multi-scale dense features compared to existing DLIR methods such as VoxelMorph, TransMorph, etc. 

**W4. Experimental supports.** Though the authors are getting comparable performance in image registration (Tab. 1), they are achieving improved results in testing out of domain/distribution datasets. However, the authors might want to show their model's performance without adapting their proposed multi-scale optimization. Basically, is the optimization scheme or the multi-scale features helping the complete registration model in achieving those bits of improvements? and the important question is why? Two interesting sets of experiments that validate the domain shift hypothesis can be the following - 
*(i) train on some of the other datasets (excluding OASIS) and test on the rest,* and 
*(ii) train on multiple datasets, including OASIS, and test on the rest.* 


Overall, I appreciate the authors for working in the domain of image registration which is very relevant as well as important in the medical imaging domain. However, the current version of the manuscript lacks some important experimental justification and further experiments. With that being said, the current version of the manuscript is under the threshold of acceptance. However, I am open to reconsidering the initial rating if the above concerns are adequately justified.

Limitations:
Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fzdFPqkAHD;"REVIEW 
Summary:
The paper presents ATS (Agent-To-Sim), a framework to enable agent behavior modeling from multiple casual video captures in indoor scenarios captured during long spans of time. The proposed pipeline consists in (1) 4D reconstruction of the scene geometry and observer and agent motion, and (2) controllable agent behavior learning and generation.

For the first stage, multi-video registration uses coarse-to-fine registration to globally align the cameras to a shared canonical space derived from DINOv2 per-frame features (initialized with a walkthrough clip of the environment) and then jointly optimizes the 3D structures while adjusting the cameras locally with novel featuremetric losses (which makes the optimization robust to changes of lighting and appearance and improves alignment accuracy) and standard photometric and regularization losses. With the proposed (annealed) swapping of latent per-video codes during optimization, missing information is shared across videos, while video-specific details are kept.

For the controllable agent behavior modeling, in order to generate plausible interactive behaviors, the generated behavior conditions on an encoding of the scene, observer, and past from the agent's egocentric perspective, which avoids overfitting to specific locations in the scene. Then, the ego-perception-conditioned generation of full body motion proceeds hierarchically via diffusion: Generated goals Z condition generated paths P, which finally condition generated body motions G.

The included experiments reflect the quality of the 4D reconstructions achieved by the proposal, the improvements in displacement errors compared to two baselines (as well as ablations of the proposed method), and a qualitative analysis of the effects of the behavior conditioning signals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Great technical achievement to reconstruct agent behavior in indoor settings, exploiting the shared information across different videos captured at different times via robust alignment based on semantic features from foundational image models (DINOv2) and diffusion-based short-term hierarchical motion generation.
- Plausible long-horizon generation of agent motion for different bodies, conditioned on the environment, observer, and past trajectory.
- Despite the complexity of the system, the description is relatively brief and complete, whig, along with the rest of the paper, is excellently written.

Weaknesses:
- The paper focuses on environment-aware motion of agents in the presence of a (human) observer. Even if out of scope for this paper, it would be interesting to discuss more complex agent-environment interactions (see my questions below).
- I believe the current experiments use a small number of environments/scenes, which makes it hard to justify considering the system for larger-scale deployment, but I'll be happy to update my score if the authors correct me.

Limitations:
The authors reasonable address limitations and social impact in the appendices.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses using an iPhone's RGBD camera to collect several hours of videos within a room over a time span of one month. Through these multi-view videos, a 4D reconstruction of the room is generated. A collection of rigid bodies is used to simulate agents (such as cats, dogs, etc.) in the room. Utilizing goal-conditional path generation technology, users can ultimately control the movement of these agents by setting goals.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The video presented in this paper is very effective; it reconstructs 4D video from a single view and reconstructs a complete room from multiple views.
2. In addition to reconstruction, the paper also discusses how to control the movement of the agent through goal-condition path generation.
3. Intuitively, I think this is a good paper and may inspire researchers in the field of 4D reconstruction.

Weaknesses:
1. While I am not an expert in 4D reconstruction, I find the presentation of this paper rather unclear, particularly the methodology section, which is extremely difficult to understand. My confusion began around lines 126-127. What are the color and feature descriptors of the video? I later noticed that ψ is described as the DINOv2 [40] feature of the input image. So, is ψ a feature of an image? How to obtain it? The paper should clarify this. Additionally, what is X, and is it a point cloud obtained from a mobile phone? If so, how does the point cloud acquire its color in Equation 2?

2. I suggest using a table to explain each symbol in detail. If the explanation of a symbol requires context from the paper, ensure it is as understandable as possible. For technical terms, provide detailed explanations within the paper. A comprehensive symbol table in the appendix would significantly enhance the paper's clarity.

3. The paper lacks detailed quantitative experiments to demonstrate the effectiveness of the method.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Agent-to-Sim, an approach to learn a 3D agent in a 3D environment from casual videos of the same agent captured over a long horizon. ATS first conducts 4D spatio-temporal reconstruction from the set of videos, including a deformable agent, the background scene, and a moving observer. This is done with a coarse-to-fine video registration method. Then, given the 4D reconstruction, ATS learns a hierarchical diffusion model over the agent's goal, path, and pose trajectories.  The overall approach is tested on a dataset of iPhone videos for over several types of agents and motion patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I am not a subject matter expert in this field. However, the paper was clear and well-written such that even a non-expert like myself can understand the proposed high-level approach. The attached supplementary materials give a great visual overview of the paper.
- The paper outlines several limitations of the proposed approach and future directions to address them. The limitations are meaningful and help the reader better understand the problem setting, modelling assumptions, and future directions.
- The paper tackles a challenging problem on the path towards building scalable and realistic simulators.

Weaknesses:
- Certain technical details are not clear for readers unfamiliar with the related literature. This limits understanding and reproducibility. See questions.
- Evaluation of the method seems limited and is mostly limited to qualitative comparisons. I suppose this is inevitable given that ATS tackles a new problem setting than related work. However, it does limit the reader's ability to evaluate the significance of this methodology.
- For behavior generation evaluation, I don't understand why certain baselines were selected. In particular, FaF seems like a detection + multi-agent motion forecasting paper for self-driving, so it's not immediately clear how it can be adapted to this setting.

Limitations:
The authors have adequately addressed limitations and potential social impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a method for learning interactive behaviors of various agents, including humans, cats, dogs and a bunny, by leveraging unstructured videos captured casually. The various videos are registered together in a common frame, offering a 4D reconstruction of the agent and the environment. Based on this reconstruction, the multi-modal distribution describing different agent behaviors is learned by using diffusion models and Control UNets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses the very challenging problems of learning agent behaviors from a collection of unstructured videos captured over different sessions. To learn interactive behaviors, both the trajectories of the agent and the surrounding environment need to be reconstructed, as to have relevant context of the behavior. Additionally, the motion of the camera/observer need to be reconstructed as well, to allow the registration of the videos in a common frame. As the videos are collected over a potentially large period of time, change in the environment can occur, complicating the tasks of registration and reconstruction.

The idea of using ego-perception encoding for the learning and generation of plausible interactive behaviors is another strong point. After the agent and the environment are reconstructed, ego-perception encoding is learning perception codes of the scene, the observer and past trajectory, factors that condition the generation of the agent's body motion.

Behavior generation considers the generation of the goal and the conditioned generation of the path, taking into account the goal.

Weaknesses:
There are numerous models employed in the proposed framework. Due to the limited space available, few details are provided about their motivation and their implementation. This makes both understanding of the work and its reproducibility very challenging. 

A particular aspect which is not addressed in detail is the modeling of the agents, especially of animals like cats that are quite challenging due to their non-rigid nature. In particular, it is not clear how eq.2 is combined with eq.3, and why the same number of ""bones"" (b=25, L.137) is used for all agents. Also, the nature of G^b is not discussed in detail. 

Additionally, details on how NeRF-type reconstructions are combined with feature descriptors, and how this helps in handling layout changes is not discussed in detail.

More examples like the previous can be given for different aspects covered in the paper, like camera localization (eq.6), scene alignment (eq.7) and behavior learning (eq.10 and 11). Each of these aspects would certainly require more space for describing in detail the corresponding models and support the relative claims in the experimental evaluation. 

Regarding experimental evaluation in particular, only high-level results regarding the agent behavior prediction are provided, while it would be crucial to quantitatively assess the quality of 4D reconstruction and, importantly, to include a detailed ablative study.

Overall, although some very interesting ideas are proposed in this work, both for 4D reconstruction of agent behaviors and behavior learning and generation, I think that the paper is too densely packed without having enough space to describe the paper contributions in sufficient detail. In my view, even describing in detail one of the 4D reconstruction or agent behavior modeling parts alone would be challenging in the space available. This affects also the experimental evaluation, as not all claims are supported by the results.

### Minor comments
- L.35: ""Such systems do not scale well""
- Figure 1, caption: incomplete sentence ""conditioned different observer trajectories""
- L.88: ""whiling accounts""
- L.113: what ""longitudinal videos"" are?
- Figure 3, caption: what does ""low latency"" means in this context?
- L.215: ""we collect the a""

Limitations:
Limitations of the work are discussed in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
OGaZVSS0Cx;"REVIEW 
Summary:
The authors present the first mini-batch algorithm for kernel k-means. The algorithm itself is simple and works the way one would expect mini-batch kernel k-means to work. The authors improve the running time of an iteration of kernel k-means from $O(n^2)$ to $O(n(k+b))$ for the mini-batch version of the algorithm. Additionally, they show that using a specific learning rate function, there is an upper bound on the number of iterations of the algorithm.

The main challenge in the design of this algorithm is to keep track of the intermediate centers as storing the points in feature space is infeasible, as they are updated iteratively as in Lloyd's algorithm. For this, the authors design a recursive update rule to keep track of the quantity $\| \phi(x) - C_i^j \|^2$ for each iteration $i$ and each center $j$. They show that in a new iteration this quantity can be updated by considering the distance of each point in the dataset to the centers of mass of the clusters in the mini-batch and the previous centers.

Finally, the authors provide an experimental study of the mini-batch algorithm on four datasets and compare it to a non-kernel mini-batch algorithm and the full kernel k-means algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The algorithm offers improved running time bounds that are interesting to practitioners using kernel k-means in practice. It is also the first algorithm for mini-batch kernel k-means.
- The main theorem's bound on the number of iterations is nice to have and a good follow up to paper [26].
- The theoretical analysis is cleanly written and easy to follow.

Weaknesses:
- The techniques, while elegant are not particularly novel in terms of theory. The proofs mostly follow from analyzing the inner product terms in the k-means formulation. 
- A number of the proofs in the main body of the paper could have been moved to the appendix, as they do not give the reader more of an understand of the big picture and are very detail specific.
- There is no discussion of the experimental results.
- While the authors state the paper is mostly theoretical, I believe this algorithm is mostly interesting to practicioners and therefore a more thorough focus on the experimental evaluation with more parameters, additional datasets and thorough discussion would strengthen the paper in my eyes.

Limitations:
The authors included a checklist in the appendix of the paper, but have not discussed practical limitations in the main body of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the first mini-batch kernel $k$-means algorithm, which significantly reduces running time compared to the previous kernel $k$-means methods relying on the full datasets. With the proposed mini-batch kernel $k$-means algorithm, each iteration can be executed in time $O(n(k+b))$, improving the complexity of $O(n^2)$ for fully-batch methods. The authors also provide theoretical guarantees, ensuring that the algorithm can terminate (reach a convergence) within $O(\gamma^2/\epsilon)$ iterations with high probability, where $\gamma$ is the bound on the norm of points in the feature space. When initialized with the $k$-means++ seeding method, the algorithm achieves an $O(logk)$-approximation. Experimental  evaluations confirm that the mini-batch kernel k-means algorithm performs significantly faster than its full-batch counterpart while maintaining solution quality.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm achieves significant improvements on time complexity compared with full-batch kernel $k$-means methods.

The paper provides theoretical analysis, ensuring the algorithm's termination and performance bounds if initialized with the $k$-means++ seeding method.

Weaknesses:
The techniques used in this paper are largely based on the work of [1]. Mini-batch $k$-means method is not new for clustering problem. The main contribution of this paper is to combine the idea of mini-batch $k$-means with the kernel $k$-means versions. It should be noted that the theoretical bounds given in this paper are not entirely novel.

There are some technical issues in the proofs (details see questions),  potentially undermining the theoretical guarantees.


[1] Gregory Schwartzman. Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. ICLR 2024.

Limitations:
Although this paper mainly gives theoretical results for clustering problems, it lacks discussions on broader impact as required by the NeurIPS guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose the first mini-batch kernel k-means clustering algorithm. It is a variant of Lloyd's algorithm that was introduced by Sculley that takes a batch of random b points instead of the full set of points and a weighted avaerage with the current centers while updating the centers. This paper attempts to translate this idea in the *kernel* k-means setting. The resulting algorithm has the same approximation guarantee as the original k-means but it terminates faster and consumes less time per iteration.

Their analysis follows the recipe of Scwartzman who used an early stopping condition when the improvement on the batch drops below some user-provided parameter. The main challenge in the kernel setting is that the underlying Hilbert space could be large or even infinite-dimensional. This is prohibitive as Scwartzman's bound on the number of iterations depends on the dimension. The authors bypass this by instead giving a bound on the Hilbert norm of the points which can be bounded in practice for example using normalized kernels.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors coduct detailed experiments that compares their algorithm favorably with the prior works. Specfically, the ARI and NMI scores were noticably better across a variety of 4 datasets.

I liked the paper. I think it has a decent theoretical and experimental contribution.

Weaknesses:
New ideas are limited. Mostly an adaptation of Scwartzman's work in the kernel setting

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article presents the first mini-batch kernel k-means algorithm, which significantly improves running time compared to the full batch kernel $k$-means with only a minor negative effect on solution quality. The proposed algorithm runs in $O(n(k+b))$ time per iteration, as opposed to $O(n^2)$ for the full-batch version. The authors provide theoretical guarantees for the algorithm's performance, demonstrating that it terminates within $O(\gamma^2/\epsilon)$ iterations with high probability when the batch size is $\Omega((\gamma/\epsilon)^2 \log(n\gamma/\epsilon))$. Experimental results confirm the efficiency and effectiveness of the algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Improved Efficiency: The mini-batch approach drastically reduces the running time from $O(n^2)$ to $O(n(k+b))$ per iteration, making it feasible to handle large datasets.

Theoretical Guarantees: The algorithm includes a thorough theoretical analysis, ensuring termination within a specific number of iterations and providing an approximation ratio when using $k$-means++ initialization.

Flexibility with Kernels: The algorithm works well with popular normalized kernels (e.g., Gaussian, Laplacian), making it versatile for various applications.

Practical Relevance: Early stopping conditions align with practical machine learning workflows, increasing the algorithm's usability in real-world scenarios.

Weaknesses:
Approximation Quality: While the solution quality is comparable to the full-batch version, the approximation ratio depends on the batch size and initialization, which may not always guarantee optimal clustering.

Parameter Sensitivity: The performance heavily relies on parameters such as batch size and learning rate, which need careful tuning.

Complexity in Implementation: Implementing the recursive distance update and maintaining inner products can be intricate, potentially increasing the implementation complexity.

Potential Issues

Stochastic Nature: The inherent stochasticity of mini-batch algorithms can lead to variations in performance, and convergence to local minima is not guaranteed.

Parameter Initialization: Poor initialization of cluster centers can significantly affect the algorithm's performance and convergence speed.

Data Dependence: The effectiveness of the algorithm may vary depending on the dataset characteristics, such as the distribution and dimensionality of the data points.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Xo1Yqyw7Yx;"REVIEW 
Summary:
This paper applies quality diversity (QD) optimization (evolutionary method) to the problem of diverse task generation for (meta) reinforcement learning (RL). It argues that QD could be used in settings where an open-ended simulator’s parameterization is unlikely to produce tasks that are diverse in high-level features. The method works by handcrafting a set of high-level task features that are relevant to the learning process of the RL agent, then running QD to collect a set of diverse parameterizations that cover the feature space distribution well. A (meta) RL agent is then trained on tasks sampled from a distribution based on the set of QD optimised tasks. The paper’s experiments on GridNav, Alchemy, and Racing tasks show significant improvement in agent performance over existing baselines such as robust prioritised level-replay.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Focuses on high-level features more relevant to downstream tasks that the meta-RL agent should learn to adapt in, rather than focusing on simulator parameters as in prior unsupervised environment design (UED) work.

By shifting the focus in task distribution design for meta-RL training, this paper showed significant improvements on representative existing UED solutions, on a diverse set of evaluation tasks (GridNav, Alchemy, Racing). The evaluations are well-designed with sufficient ablations. 

Makes a connection between QD and UED for meta-RL training, which is novel to the best of this reviewer’s knowledge.

The paper is very clearly written and illustrated. Relevant related works are discussed and contributions of the work are put into appropriate context. Content is well self-contained despite introducing new methods and tasks.

Weaknesses:
It is still necessary to hand-craft the high-level features, which needs expert knowledge (or at least quite high familiarity with downstream tasks) and can be heuristic. 

Evaluation only used one meta-RL method (VariBAD). It would be useful to see performance of other meta-RL methods such as RL^2 on the DIVA task distribution.

Limitations:
Limitations have been adequately addressed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces DIVA, a technique for exploring the parameter space of parametrisable environments. The technique uses a variant of MAP-Elites to explore the environment parameter space, finding exemplar points spread across the parameter space, as measured with respect to some user provided features. The authors show that this generates a usefully diverse collection of environment instances by training a meta-learning agent on the generated levels and comparing it to a number of baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
On the whole the paper is well written and clear.

The authors choose a sensible selection of baseline algorithms to compare against, giving the reader an understanding of the strengths of their approach.

Weaknesses:
The primary weakness from my perspective is that domains that the experiments was conducted on are all very simple “toy” domains. In particular, these domains do not have all have intrinsically complex parameterisations, and for two of them the authors had to, in effect, obfuscate the parameter spaces so the algorithm had a challenge to work against. This leads one to worry that the authors’ results may not be representative of more realistic open-ended domains where the parameter space complexity may manifest in a different way.

Another weakness of the technique is the authors need to hand-select the features used by the algorithm, per domain. The authors mention that this could be automated, but the fact still stands that in this paper, quite extensive hand tuning and selection - as reported in the appendices - of the objectives was made to get their results. The impact of this technique is much more limited if feature sets need to be hand-tuned per domain, so without demonstrating that this is not the case, I think the authors do not demonstrate that this technique is likely to have wide impact.

I have a further query about the way that the VariBAD hyperparameters were tuned, below in the questions section.

Limitations:
The primary limitation is the one regarding hand-tuned feature sets, which I have expanded on above, in the weaknesses section.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper identifies the limitation that hand-crafting a sufficiently diverse set of simulated training tasks to bridge any significant sim-to-real gap is labor-intensive. It then proposes DIVA, a new evolutionary approach for generating diverse training tasks in the absence of well-behaved simulator parameterizations. The paper demonstrates how DIVA outperforms proposed baselines such as ACCEL, PLR, and DR.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper identifies a key limitation in current methods used to bridge the sim-to-real gap.
- The proposed approach is novel.
- There is a good spread of experimental results.

Weaknesses:
- I think the presentation of the paper can be improved. For example, Figure 1 could include more descriptions about what is happening, such as defining E_S(theta) as the structured environment simulator and E_U(theta) as the unstructured environment simulator.
- In line 128, how was the number 80% chosen? And why is it described as “roughly”? More justifications for the choice of these parameters should be included.

Minor things
- Typo in line 184, “the final y location is determine by”

Limitations:
- Yes, the paper sufficiently covers the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper describes an approach for learning a QD-archive to be used as a proxy for samples of test environments, and shows that this results in improved performance in producing a set of meta-learning tasks over DR and UED baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The introduction of QD approaches into UED algorithms is a promising algorithm for improvement. The empirical results appear convincing, and the approach is quite natural.

Weaknesses:
The comparison between DIVA and PLR/ACCEL is a bit of an apples-to-oranges comparison. Regret-based UED methods like PLR and ACCEL are meant to be making decisions under ignorance, where there is no information known about the target distribution. There are other UED approaches designed for the case where there is some information that is known, and thus it is a decision under risk. Specifically, it would be better to compare against SAMPLR, CLUTER, or DRED. Since SAMPLR requires simulator access, the fairest comparison would be against CLUTER or DRED. 

[SAMPLR] Jiang, Minqi, et al. ""Grounding aleatoric uncertainty for unsupervised environment design."" _Advances in Neural Information Processing Systems_ 35 (2022): 32868-32881.

[CLUTER]Azad, Abdus Salam, et al. ""Clutr: Curriculum learning via unsupervised task representation learning."" _International Conference on Machine Learning_. PMLR, 2023.

[DRED] Garcin, Samuel, et al. ""DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design."" _Forty-first International Conference on Machine Learning_. 2024.

That being said, the current results look to have more significant results than these other works (though it is hard to tell across domains), and this approach has not be tried in meta-learning before. It is important to note that UED has been used in meta-learning before, for instance in AdA.

[AdA] Team, Adaptive Agent, et al. ""Human-timescale adaptation in an open-ended task space."" _arXiv preprint arXiv:2301.07608_ (2023).

It occurs to me that DIVA + UED as discussed starting at line 259 could be used as an algorithm for decisions under ignorance, and would possibly be quite a good algorithm. It may be worth running the approach from 259, maybe without any test time data, on the traditional maze, F1, and bipedal walker environments and transfer tasks to check if it consistently outperforms existing methods.

It seems like the assumption about the probability of generating a useful level from the simulator underlying DIVA is similar to the assumption necessary for ACCEL or PLR. Both of these approaches should work if there is a ~0.01 % chance of generating a useful level. The boot-up time would just be a bit slower to get the initial buffer of levels. 

The limitation of online agents evaluations is a real bottleneck for the UED approaches which DIVA avoids, but it is a bit of an odd comparison as DIVA generates the model once offline, and thus isn't aiming to be adaptive towards current agent performance. It seems like DIVA looses the benefits of adaptivity by completely removing online agent evaluations. I would be interested if a DIVA-like approach with limited online evaluations could keep the best of both. 


In Figure 5d, ""unique genotypes is a quite bad metric for diversity, as completely random levels would score quite well even though most random levels are quite qualitatively to each other. 

### Clarity

In the abstract it is not clear to me what ""well-behaved simulator parameterisations"" means, what ""unscalable flexibility"" refers to, or what ""ill parameterised simulators"" means.

It would be good to have a citation on line 46 for how one could learn the axes of QD.

It's not immediately clear what ""genotype"" means, and it seems to be used in different ways on line 82 and 88. In the first case it may be more conventional to call it the ""level generator"" and in the second case it could be more conventional to call it the ""level parameters"".

Limitations:
See Weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
W4pIBQ7bAI;"REVIEW 
Summary:
This paper proposes a novel MEDIQ framework to simulate clinical interaction between doctor and patient. The proposed MEDIQ framework is capable of proactively asking questions and collecting information to make the diagnosis results. Also, the MEDIQ framework incorporates two agents—expert and patient, to formulate a conversation loop. With an abstaining module, the expert agent determines when to stop a conversation. The extensive experiments validate the design of the MEDIQ framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The novel design of the expert and patient modules to formulate a medical conversation
2.	The design of the expert agent to proactively collect information to support the diagnosis decision
3.	The design of the patient agent to extract the information from the patient record and interact with the expert agent

Weaknesses:
1.	A large portion of the algorithm, for example, the abstention module and rationale generation module, rely on the output from ChatGPT, which makes the results less reproducible. Moreover, the outcome from ChatGPT, even though the authors claim they estimate model confidence, is still in a state of ‘black box’. The authors should develop some other methods to better quantify and evaluate the outputs, instead of simply relying on ChatGPT or other LLMs.
2.	The dataset is crafted using LLMs on the medical Q&A dataset. To better validate the performance of the proposed model, the authors should add experiments using the medical conversation dataset. In the context of real clinical conversation, the doctors may make a diagnosis decision not simply based on existing medical records, but also take symptoms, medical testing results, and other information into consideration. This information is not included in the medical Q&A dataset.

Limitations:
1. Need to strengthen the explainability of the proposed framework, especially for the abstention module and rationale generation module.
2. Need to add medical conversation dataset for better evaluation.
3. Need to compare with medical Q&A LLMs.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce MediQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient system that simulates a patient and responds to follow-up questions, and an Expert system that serves as a doctor's assistant and asks questions to the patient before making a medical decision.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes MediQ that stimulates realistic clinical interactions, which is more like a realistic scenario, rather than providing general responses at once.
2. The authors conduct extensive experiments to demonstrate the effectiveness of each component's design.

Weaknesses:
1. While the paper demonstrates its effectiveness via automatic evaluation metrics, it lacks human evaluation of the model's performance. Therefore, the performance of the system in ""realistic"" scenarios cannot be verified.
2. The response of the Expert system relies more on the parametric knowledge of LLMs. Although LLMs have learned knowledge during pre-training, their accuracy and stability cannot be guaranteed.

Limitations:
See ""Weaknesses"".

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes MediQ, a two-LLM-agent-system for interactive patient diagnosis. The authors argue that vanilla LLMs do not autonomously engage in an information extending discussion, but rather directly try to diagnose the patient and therefore oftentimes hallucinate.
The proposed system specifically aims to actively refrain from diagnosis when not sufficient information are available in order to ask more questions. This is embodied by a so-called expert agent. The patient agent, in turn, is prompted to support the expert agent by answering with relevant information. 

The authors evaluate different versions of their system, where the patient agent either responds with the queried information or tries to further select relevant information, and the expert agent uses different abstention modules, e.g., predicting binary abstention, numeric confidences or including reasoning chains.
The evaluation is conducted for MEDQA and CRAFT-MD datasets, where patient information is partially removed to evaluate the interactive setting. The used LLM is GTP-3.5, showing that selecting relevant subset of patient information from the patient agent is superior to the easier strategies.  
The authors also compare their approach to the full information setting for a number of additional base LLMs, including GPT-4 or Llama-3-8b. The results show that the interaction improves accuracy when compared to having only the multiple choice questions or base information about the compaint. The system, however, does not surpass the full information setting.
Finally, the author also provide several meta analyses, providing insights that, e.g., personalized abstention increases accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Relevant topic: The paper deals with a relevant problem setting, trying to improve patient diagnosis using LLMs, where hallucination has bad consequences
- Sensible solution: The proposed system, using two agents which interact and support each other based on abstention and selecting relevant information, is well argued for.
- Good evaluation setup wrt data and base LLMs: The used datasets as well as the performed synthetic removal of patient information allow for a good evaluation of the problem. In addition, different base LLMs have been tested.
- Wide meta analysis: The conducted meta-analyses provide good insights, showing details about conversation lenghts and impacts of different strategies.

Weaknesses:
- Goal of the paper not clear: While the topic is relevant and the proposed abstention module is sensible, the paper sometimes refers to the outcome as simulation framework. Is the purpose then to evaluate other approaches through the simulation system rather than presenting a new abstention module?
- Related work not conclusive: It is not clear to me why mentioned works of the interactive models and agents section do not solve the same or an overlapping problem, which would make it necessary to evaluate against them. Mentioned works include [2, 18, 24], where one would have to argue better why the used approaches for improving interaction and sufficiently different or inapplicable here. In addition, mentioned competing medical diagnosis systen [45] is also not sufficiently described / exluded from being relevant for the evaluation, as only the abstention module seems to be missing. But it might be that the overall performance of used prompts / techniques even supersedes the reported ones here, right? Lastly, I do not understand the referencing of other papers promoting abstention during explaining the proposed method, i.e., [13], but not discussing the differences wrt novelty in the related work section. 
- In addition to the covered works, I see an overlap to works such as [1], dealing with selective predictions of LLMs. It seems these methods have the same goal and should be compared to or discussed. A further elaboration is needed or a better
- As a consequence, at this point, I am not convinced the paper has sufficient novelty.
- Table 2 reports the reached accuracies of the different LLMs and variants. It seems from the table, that the full non-interactive setting is always paramount to the best interactive variant. It is not clear to me why abstention and patient information filtering then would help. 

[1] Chen, Jiefeng, Jinsung Yoon, Sayna Ebrahimi, Sercan O. Arik, Tomas Pfister, and Somesh Jha. ""Adaptation with self-evaluation to improve selective prediction in llms."" arXiv preprint arXiv:2310.11689 (2023).
[2] Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah D. Goodman. Stargate: Teaching language models to ask clarifying questions, 2024.
[13] Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. Don’t hallucinate, abstain: Identifying llm knowledge gaps via multi-llm collaboration, 2024.
[18] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in large language models. arXiv preprint arXiv:2402.03271, 2024.
[24] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. arXiv preprint arXiv:2310.11589, 2023.
 [45] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic ai, 2024.


****** Update after author response *******
I thank the authors for their detailed clarifications. After reading the response and the other reviews, I am open to improve my score towards acceptance. I still think that it would be valuable and maybe a requirement that the framework includes better absention ""modules"", as there are papers who focus on that. The framework, in general, is an interesting and sensible interactive process, but to really add value the evaluation should better reflect that it is possible to gather all information or to abstain otherwise.

Limitations:
Limitations have been discussed in the paper, but I wonder what would happen if the patient LLM would make up facts and how this might be confidently tested at real-time. If relevant, this could be added to the discussion.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The goal of the paper is to develop a dataset on which models can be trained for interactive LLM-patient dialogues that require follow-up questions. The paper adapts MedQA and CraftMD datasets into an interactive setup, uses an LLM to mimic a patient's questions and trains LLMs to ask the necessary follow-ups to answer the patient's primary question.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths (originality, quality, clarity, and significance)

Adapting existing datasets into a conversational setup is a useful contribution
that will likely be used in future work.

The paper is clearly written.

The paper evaluates on both closed-source and open-source LLMs.

The finding that a naive question-asking approach actually decreases
performance is quite interesting.

The experimental evaluation of different methodologies for improved follow-up
questions and abstention is thorough and convincing.

Weaknesses:
The interactive framework means that the evaluation of Expert systems is
heavily dependent on the idiosyncracies of the Patient system.  While it is
clear that different choices of Patient and Expert systems have meaningful
impacts on the overall QA performance, the evaluation is not detailed enough to
understand whether performance on MedIQ would generalize to performance for
human patients. While this limitation is perhaps obvious, it should be
addressed more explicitly and it would be helpful to guide future work to
discuss how the authors would ideally expand this work to include human
patients and/or clinicians. In particular, if the Expert systems are able to
answer the QA tasks with information that would not be sufficient for a human
clinician, that may be a bug rather than a feature.

Limitations:
See weakness above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
08oUnmtj8Q;"REVIEW 
Summary:
The authors developed a few-shot evolutionary optimization framework to effectively solve the multi-objective EOPs and constrained EOPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method can solve the multi-objective EOPs and constrained EOPs with little data, especially for the engineering problems.

Weaknesses:
The learning results may rely on the relation degree of different tasks.

Limitations:
The overhead of the algorithm need to be further decreased.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new surrogate-assistant evolutionary algorithm that utilizes a Gaussian process with Deep Kernel Learning as the surrogate model. The method employs few-shot meta-learning to learn from multiple tasks to construct the surrogate. It is then integrated with the existing MOEA/D-EGO algorithm to create a new approach. Experiments are conducted on the DTLZ benchmark problems and a gasoline motor engine calibration problem to evaluate the performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is instantiated and tested in expensive multi-objective optimization and constrained optimization scenarios.
2. A real-world problem is considered in the experiments.

Weaknesses:
1. Many important algorithmic details are unclear. For instance, the main distinction between the proposed MDKL and the existing DKL algorithms is its ability to learn from a set of related tasks, yet its implementation is not clearly explained. How parameters from different source tasks collectively form the experience, and how parameters from both source and target tasks jointly create this experience, are not clearly addressed.
2. The effectiveness of the algorithm is primarily tested on expensive multi-objective optimization problems, but state-of-the-art algorithms in this field were not selected for comparison.

Limitations:
Even if a clear definition of task similarity cannot be provided, it is recommended to offer some hints to help users apply the algorithm.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a meta-learning framework into few-shot optimization to assist the surrogate modelling in expensive evaluation setting. The authors parameterize a mapping function to get the hidden feature of the solution space and then integrate such mapping into a gaussian kernel function as a deep kernel. They then facilitate meta-training of the proposed deep kernel over a group of related tasks to attain an experience model, by maximizing the posterior likelihood. During the online optimziation of the target task, the experience model is firstly adpated to the new task in the same way above and then updated acoording to its accuracy in terms of the predicted objective value. The experimental results show that the proposed framework achieves competitive performance against some strong baselines over EMOPs and ECOPs benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of integrating meta-learning into the kernel-learning based surrotgate methods is novel, and might improves the surrogate-based optimziation towards generalizable setting.

2. The expriments result show that the proposed FSEO framework is at least competitive with the existing baselines, which is acceptable and should be encouraged for further development.

3. The overall writing is clear.

Weaknesses:
Before the next round of author-reviewer rebuttal, following concerns exist:
1. Given that the likelihhod-based loss function (Eq. 4) should be maximized to fit the samples from all of the related tasks, why its update should follow a gradient descent rather a gradient ascent? Correct me if I was wrong.

2.  line 144 ~ 146, the authors state that the U update interations roots from the smaller number of available related tasks. I can not understand the reason behind, can you explain it more?

3. The neural network $\phi$ used in the deep kernel function is a 2-layer MLP, which limits the FSEO to meta-learn surrogate function among the related tasks with the same slution dimension. However, in practice, related tasks might not share the same dimension. I would appreciate the authors to provide realistic scenarios where FSEO is eefective. Besides, the effectiveness of the FSEO on traditional single-objective tasks should also be verified to make it more convincing that FSEO is a general framework.

4. Although the overall writing of this paper is not bad, it is still difficult for less-skilled readers to understand the whole picture. In particular, the content in Section 3.2 and Section 3.3 should be carefully refined to make sure the potential readers fully understand how the DKL and MDKL operate. For now, it is too simple and ambiguous.

Limitations:
The limitations listed in Conclusion are quite brief. I would appreciate the authors to explain more about the two limitations listed here.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Meta Deep Kernel Learning (MDKL), a new surrogate for SAEAs. MDKL consists of a deep kernel with meta-learning. Empirical studies demonstrate its effectiveness in expensive multi-objective optimization and constrained optimization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. This paper is well-written and easy to follow. The technical details are well presented.
2. This paper extends deep kernel and meta-learning-based surrogates into evolutionary algorithms.
3. This paper investigated multi-objective optimization and constrained optimization.

Weaknesses:
Meta-learned deep kernel surrogates have already been well-studied in Bayesian Optimization [1]. The authors are also aware of this as they mentioned in Related Work. I think this paper does not present significant new advancements based on the previous work.

First, the authors claim that MDKL is specially designed for optimization, while the previous work is not. In this regard, I do not see many differences between MDKL and previous meta-learned deep kernels. The authors claim that the advantage of MDKL lies in continuous adaptation; however, most models support parameter updates or fine-tuning. The authors also did not sufficiently explain the relationship between continuous adaptation and optimization problems, or what significance it has for optimization problems.

Second, the authors propose that one of the novelties of this paper is taking expensive multi-objective optimization problems (EMOPs) and expensive constrained optimization problems (ECOPs) into account. MDKL, as a surrogate, can be integrated into almost any expensive optimization algorithm. It seems to be able to cooperate with Bayesian optimization as well. The authors simply replaced the surrogate in a multi-objective optimization algorithm with MDKL and conducted some experiments, without providing any new analysis, insights, or proposing any new methods specifically for MOPs or COPs. Therefore, I believe this paper does not make a significant contribution to solving EMOPs and ECOPs.

[1] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. ICLR 2021.

Limitations:
No concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
mdK1vhgpa5;"REVIEW 
Summary:
The paper proposes a method to continually adapt a pre-trained classifier to an unlabeled stream of test data. They address the problem of continual test-time adaptation through the lens of Bayesian deep learning. Their method consists of three main components: (1) a variational warm-up strategy to turn any source model into a Bayesian Neural Network, (2) a mixing strategy between the source model and the last posterior to leverage the trade-off between adaptation and forgetting, and (3) a modified entropy term that is symmetric and incorporates data augmentations. The authors compare their method on standard CIFAR-C and Imagenet-C datasets to a set of TTA baselines. The paper also includes ablation studies on the components and an evaluation of uncertainty estimates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper tackles one of the most relevant problems in continual domain adaptation, namely the trade-off between agile adaptation while preventing the forgetting of the source model. It does so by constituting a mixture of the source and last adapted model in a VI framework, which is novel to my knowledge. (originality)
- Further, in a setting where robustness is crucial and therefore uncertainty quantification can be helpful, the combination of Bayesian deep learning and continual test-time adaptation is interesting and insightful. (originality)
- The methodological backbone is accompanied by insightful ablation studies that highlight the significance of the different parts of the paper’s contribution. (quality)
- The paper is, in most parts, pleasant to read. The notation is clear and consistent, and the reader is well guided through the different sections. (clarity)
- The paper presents clear experimental evidence in support of the method. The experiments show an improvement in adaptation accuracy on already quite saturated datasets (up to 1.8% points on CIFAR-10-C). VCoTTA also seems to be advantageous on most corruption types. (significance)

Weaknesses:
The paper presents strong evidence in support of the proposed method. However, it is left unclear to me why the method performs so much better than previous approaches.

- The method consists of a range of specific components. However, in some cases, the specific design of the components is not clearly motivated. In particular, equations 10 and 13 lack supporting citations or explanations. Why have exactly these formulations been chosen?
- I’d like to get more clarity on the difference between this paper and the original CoTTA work, as it seems to me there are certain components in common (e.g., student-teacher approach, EMA). More precisely, could you please highlight the difference in the update equations between the two papers? My understanding is that adding the VI framework notably changes (i) the optimization objective by adding the KL term (instead of solely minimizing entropy) and (ii) the predictions by marginalizing out the model parameters. Where else does the VI framework contribute to differences?

Limitations:
The authors have listed limitations including computational efficiency and the need for access to the source data at adaptation time

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces VCoTTA, a novel variational Bayesian approach to address the Continual Test-Time Adaptation (CTTA) task, which focuses on effective domain adaptation during continuous domain shifts at test time. The authors' main contributions include a method to measure uncertainties in CTTA, addressing the issue of error accumulation due to the use of unlabeled samples. They propose transforming a pretrained deterministic model into a Bayesian Neural Network (BNN) using variational warm-up at the source stage, and employ a mean-teacher update strategy during test time. The approach updates the student model by combining priors from both source and teacher models, with the evidence lower bound formulated as the cross-entropy between student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper demonstrates originality through the novel Variational Continual Test-Time Adaptation (VCoTTA) approach, which creatively utilizes Bayesian Inference for Continual Test-Time Adaptation, and employs strategies like the variational warm-up and prior mixture techniques. The quality of the work is evident in its solid theoretical foundation, comprehensive methodology, and empirical validation on multiple datasets. 

The paper's clarity is apparent in its well-structured presentation, use of visual aids, and explicit statement of contributions. The significance of the research is underscored by its practical relevance to risk-sensitive applications, potential for broad applicability, and the reported improvements in predictive accuracy and uncertainty estimation under distribution shifts. By addressing critical challenges in CTTA, such as error accumulation and uncertainty estimation, and bridging Bayesian methods with test-time adaptation, the paper not only advances the current state of the art but also opens up promising avenues for future research. Overall, this work represents a valuable contribution to the field, offering both theoretical insights and practical advancements in continual learning and test-time adaptation.

Weaknesses:
Regarding the computational overhead discussed in the paper, while it is noted that online Variational Inference is employed to make the approach computationally feasible, a detailed analysis of the computational costs associated with VCoTTA is absent. Table 13 presents a comparison of time and memory costs, but the source of these values is unclear. Could you specify which dataset was used for these measurements? Also, is it possible to clarify whether the time and memory comparisons pertain to training or testing phases?

The manuscript contains several typographical and grammatical errors that need to be addressed. Specifically, brackets are missing in Equation (5) and in the sentence following Equation (13). Could these omissions be corrected to prevent misinterpretation of the mathematical expressions and enhance the clarity of the paper?

There are multiple grammatical issues that require rectification. The sentence ""MT is initially proposed in semi-supervised and unsupervised learning"" is somewhat unclear. Could this be rephrased for better coherence? Additionally, the sentence ""We use the mean entropy derived from a given *serious* data augmentation to represent the confidence of the two prior models, and mix up the two priors with a modulating factor"" appears to contain a typographical error and could be better structured. Could these issues be addressed to improve the readability and accuracy of the text?

The heading for Section 5.7 seems to not accurately reflect the content discussed within. Could this heading be revised to more accurately convey the main topics or findings of the section, thereby ensuring clarity and relevance for the reader?

The explanation of how MT operates in semi-supervised and unsupervised learning settings appears incomplete and potentially misleading. The current statement, ""where the teacher model guides the unlabeled data, helping the model generalize and improve performance with the utilization of large-scale unlabeled data,"" lacks specificity. Could you specify which model (teacher or student) benefits from this guidance and in what manner? Additionally, the phrase ""where the teacher model guides the unlabeled data"" seems incorrect. Could this be clarified or corrected to accurately reflect the operational dynamics of the MT framework?

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a variational continual adaptation method. Where a sequence of test-time domain adaptation problems are shown to a model. More specifically a labeled dataset is given as an initial dataset to learn from, and then afterwards a sequence of unlabelled datasets with domain shifts are presented to the model.

Using traditional variational continual learning methods will result in error accumulations in the posterior over parameters. So the authors propose a scheme that regularizes against the posterior learned from an initial source dataset. 

The authors propose do away with using sequential Bayesian inference. Instead, the authors use a mix of a source prior (learned using labeled data) and a teacher prior. The teacher prior is an EMA of the previous task’s posterior learned with variational inference. The method the authors propose is named VCoTTA.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper is well written and the components of VCoTTA are clearly explained. 
* The authors provide an ablation to demonstrate which design choices worked well, for instance, to demonstrate that the mixture of priors worked well for test-time adaptation.

Weaknesses:
Novelty:
* In terms of novelty I’m not convinced that there are important applications of continual test-time adaptation in the form of classification tasks derived from CIFAR10 datasets. I could be wrong, but some justification in the paper is required and some more realistic benchmarks would be nice.
* Maybe I have misunderstood the variational warm-up procedure. But using the MLE estimates to initialize the BNN mean parameters was done in VCL https://arxiv.org/abs/1710.10628 . So this is not a novel idea. Furthermore, there are better ways to initialize a BNN such as using Bayesian linear regression: https://proceedings.mlr.press/v97/rossi19a/rossi19a.pdf.

Clarity
* Why does the teacher model use EMA updates instead of using the inference variational posterior?
* Why is data augmentation an important component in VCoTTA (Sec 4.2)? This is suddenly presented in the paper without justification.

Notation:
* In Section 4.2, the title is a “Mixture-of-Gaussian prior”, but Eq 11 is an addition of two priors which are Gaussians by design, so this is a “scale-mixture prior” https://arxiv.org/pdf/1505.05424, not a mixture of Gaussians (https://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w9b_mixture_models.pdf)?
* Confusing notation of the source prior: it is denoted as $p_0$ (Fig 2) and $p_1$ (Eq 11), this needs to be consistent.


Empirical weaknesses:
* No standard errors in the experimental results. So difficult to see which method outperforms another.
* Uncertainty estimation is not performed with standard methods like ECE or OOD detection like https://arxiv.org/abs/2102.06571. It is unclear to me whether the Brier Score estimates uncertainties.

Limitations:
There is a good discussion on the limitations of VCoTTA.

One limitation that is not discussed is in the effectiveness of (variational) Bayesian sequential inference methods. Weight space variational inference has been shown to be very difficult to do in practice, https://arxiv.org/abs/2301.01828. So weight space variational inference (without tricks like multi-head networks and coresets) might not be the best choice when wanting to remember a source distribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a variational Bayesian approach to handle uncertainties in continual test-time adaptation (CTTA). The source pretrained model is made Bayesian by variational warm and a mean-teacher update strategy is used at test time. To avoid drift due to uncertainty of priors using only unlabeled data at test time, the paper proposed to update the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method’s effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: Bayesian approach in Continual Learning is a principled and elegant approach to the problem which this paper is relying on. In CTTA, there are additional issues due to the uncertainty of the prior distributions using only the unlabeled data from unknown domains. This paper presents a novel solution by using adaptive mixture prior models and student-teacher update on top of an existing framework.

- Relevance: CTTA is a topic that can interest a general audience, and the  Bayesian and variational framekwork can also be of interest to many.

Weaknesses:
- While Bayesian approach is nice in principle, it can be computationally demanding and offer little benefit in practice. Most of the existing CTTA methods are computationally and memory efficient, whereas this method present an opposite end of the spectrum. While the reported results are impressive, it is unclear to me why the proposed method is superior to other SOTA methods.
- The hyperparameter selection process is not addressed in the paper, which is critical in TTA where all hyperparameters should be predetermined before data access. How are they chosen?

Limitations:
Limitations are mentioned in the paper, albeit very brief. Overall, the proposed method is more complex and demanding (such as requiring a pretrained probabilistic model or source data) for TTA applications. Perhaps the proposed approach may work even better with UDA or other CL scenarios with (partially available) target labels?

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
wWyumwEYV8;"REVIEW 
Summary:
The authors aim to investigate spurious correlations learned by CLIP models. For this, they curate a novel dataset where animals are organized into common and uncommon backgrounds, e.g. a polar bear is more likely encountered in snow than on grass. The authors then perform experiments where they benchmark various CLIP and ImageNet models on the curated dataset. They observe that CLIP models suffer from spurious correlations which stem from changing the background.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I think the issue of spurious correlations is important and one needs to understand how and whether VLMs learn spurious features. The paper presents many experiments and shows that scale or backbone capacity do not improve the effective robustness on CounterAnimal which is interesting.

Weaknesses:
The paper has many issues, both in terms of writing and the methodology which need to be fixed.

### Major:
**The authors missed important previous works**: The paper “Recognition in Terra Incognita” is very related to this work and also proposes a “dataset designed to measure recognition generalization to novel environments” based on camera traps. The dataset is sorted according to difficult environments for different animals, which makes it very similar to CounterAnimal. I think the authors need to cite and discuss this paper. Currently, I do not understand the benefit of having a new dataset in addition to the already present one. The waterbirds dataset is also highly similar and should be discussed (https://arxiv.org/pdf/1911.08731). The authors cite that paper, but do not discuss it in the Related Work section, nor put it into context with CounterAnimal. The backgrounds challenge (https://github.com/MadryLab/backgrounds_challenge) is also highly related and should be discussed. In general, the related work section is very weak, given how extensively spurious correlations and worst-group-accuracy have been studied. Another important work to be discussed would be ""Finding and Fixing Spurious Patterns with Explanations"" (https://arxiv.org/abs/2106.02112).

**The naming of the common vs counter groups is misleading**: 
Line 165: “Photos with the highest CLIP accuracy are assigned to the common group, and those with the lowest CLIP accuracy are assigned to the counter group.” I have a major understanding issue here. As far as I understood the paper before this line, the goal was to put images with common backgrounds into the common group and images with uncommon backgrounds into the counter group. This is also depicted in Fig. 1 or Table 1. The caption in Fig.1 says that “Most ice bears appear in a snow background (i.e., common), while it also is reasonable to find some ice bears in a grassy environment (i.e., counter)”. But here, the authors write that accuracy has actually been used to separate images into these groups? But then the frequency of the co-occurrence of certain backgrounds and classes has not been taken into account, or rather, it is a conjecture that those backgrounds where the CLIP model has higher accuracy on are more “common”? 

**The terms ""effective robustness"" and ""robustness"" are used interchangeably which is wrong and confusing**:
I think the paper conflates the terms “robustness” and “effective robustness” which is confusing. When looking at effective robustness plots, such as in Fig. 2, we are interested in the residual difference between the measured value and the value predicted by the linear fit. As I can see, all plotted markers (CLIP and ImageNet) lie on their respective linear fits, and none of the interventions, such as CLIP-DC or CLIP-DFN offer any effective robustness benefits. It is though true that the **absolute** robustness numbers are overall higher for the CLIP-DFN models, for larger models or models trained on more data. I am however confused by the authors discussion of this observation. On the one hand, they write that larger CLIP models are more robust but increasing the dataset size does not yield improvements. First, I am confused whether they mean “effective robustness” or “robustness” here. Second, I do not see the effect the authors are describing: Both more data and larger backbones have higher absolute robustness but the same effective robustness as the other models. The statement “CLIP models trained on high-quality data are more robust” is also confusing, because it is not clear whether “robustness” or “effective robustness” is meant. 

**Due to methodology issues, results on CLIP models cannot be compared to results on ImageNet models (or other advanced LVLMs):**
Line 60: “d) Spurious discovering: preserving classes and associated data based on the decrease in zero-shot performance (i.e., evaluating based on pre-trained CLIP models without fine-tuning) when shifting the backgrounds.” This step is really unclear. Do the authors curate the dataset based on the zero-shot accuracy of a CLIP model? From the introduction and the abstract, it sounds like the authors want to benchmark the robustness of CLIP vs ImageNet models on this custom dataset. But then, it is strange that CLIP models also seem to be used during the curation process. After reading the more detailed description in line 156, I think the statements made in line 85 are misleading. The authors write “ImageNet models are more robust to spurious correlations captured by CounterAnimal” and “Compared with CLIP models (colored in blue), surprisingly, we find that ImageNet models exhibit a stronger robustness to the spurious correlations in CounterAnimal.” Given that CounterAnimal has been curated based on the performance drop of a CLIP model, I find it very unsurprising that CLIP models perform worse on it compared to ImageNet models. I think that if CounterAnimal had been curated based on an ImageNet-trained ResNet50, the trend would have been reversed. I think all statements comparing CLIP and ImageNet trained models on CounterAnimal need to be relaxed and I think that this comparison is quite meaningless because of the described selection bias. I think that the whole Section 3.3. is misleading for this reason and statements such as the following cannot be made given the methodology issues: “Surprisingly, we find that ImageNet models are more robust to spurious features in the CounterAnimal dataset. This finding may contradict the common belief [Radford et al., 2021, Shi et al., 2023] that the CLIP models tend to be more robust to spurious correlations than single-modal supervised learning.” Similarly, the conjecture paragraph from line 265 onwards is wrong and cannot be made. 

For the same reason, the comparison to advanced LVLMs in line 273 onwards cannot be made.

Figure 1: Are these examples cherry-picked or are they representative of the data points present in CounterAnimal? I am asking this, because of the Winoground dataset [A]. This dataset tests the compositionality of VLMs by forcing a model to match two captions to two respective images. Winoground has later been criticized because the two images in the choice process are not equally hard [B]. For example, the model needs to match “the glass is on the grass” and “the grass is in the glass” to the corresponding images. However, there is much more grass in the image matching to the first caption, and the model likely picks that image for both captions just because there is more grass and it makes the decision in a bag-of-words-manner. To summarize, Winoground did not control for object size, orientation and other confounders. In Fig.1, it appears that the main objects (the polar bears) are equal in size, so size could be excluded as a possible confounder? Did the authors consider this possibility, i.e. that the drop in performance could be explained by other differences in the images from the respective domains?
[A] https://arxiv.org/abs/2204.03162
[B] https://arxiv.org/abs/2211.00768

### General:
Line 25: please cite CLIP

Line 64: “The resulting dataset covers a total of 45 animal classes, ends up with 7,174 common photos and 5,926 counter photos, aligning with the standard size as an evaluation dataset [Recht et al., 2019, Hendrycks et al., 2021].” -> I do not understand this statement. Different test sets have different numbers of test images. ImageNet Val has 50k images for example. In what sense are the presented numbers standard?


Line 94: “Overall, larger CLIP backbones (i.e., larger markers) can improve the effective robustness, implying that scaling up backbones may enhance robustness against spurious features.” -> I do not see this in Fig. 2. The larger markers appear to be on the fitted line, same as the smaller markers. Effective robustness measures the difference with respect to the linear fit, and there is none for the larger CLIP backbones. Please clarify this point.


Line 146: “feature noise involves severe feature corruptions” -> Please be more specific here. What do you mean with feature noise? Do features refer to animals features such as missing ears or such? Or to the images themselves?

Line 147: “clarity issues arise when animal objects are not in major positions” -> unclear formulation: what is a major position? Do the authors mean that the animals are too small or not in the center of the image?

Line 153: “Note that the class space of backgrounds as above is not entirely orthogonal with each other due to the inherent ambiguity of the real-world situations. Nevertheless, we try our best to discern the assigned background labels within each animal class.” -> This is unclear. How many images would be ambiguous? I could imagine that many images would have two backgrounds, such as e.g. grass and sky or snow and water. For example, the last image in Fig. 1 on the left has both snow and water. It is not clear to me that only picking the snow background and ignoring the water is correct here. Further, at least for CLIP, the caption can contain several background keywords.

Further, I imagine animals occur in all kinds of environments, but there are only two backgrounds for each animal. Were the other images also discarded?

Line 214: “Therefore, we conclude that our CounterAnimal dataset possesses some realistic shifts that are generally contained in large-scale pre-training data, regardless of backbones.” This conclusion cannot be drawn from this experiment since the backbone has not been varied here.

### Section 4:
The proposed experiment is very similar to the well-known ShiftMNIST [D] or ColoredMNIST [E] datasets, which test the influence of spurious correlations. The findings here are not novel and should be brought into perspective with previous work. I do not understand how Fig. 11 relates to the text. What is “supervised”, “obj”, “objbkg”?
[D] https://arxiv.org/pdf/1811.00401
[E] https://arxiv.org/pdf/1907.02893

### Typos, grammar:
The quality of the text is poor on some occasions which makes reading and understanding the paper difficult. The manuscript would benefit from a round of proof-reading. Some statements and formulations should be made more precise.
Line 32: “The performance boosts over ImageNet models seem to suggest that CLIP resolves distribution shifts and thus spark a rich discussion about its rationale.” Strange formulation. How can “distribution shifts be resolved”? Please rephrase for clarity.

Line 112: “More specifically, [Yang et al., 2023] report that CLIP models may misaligned frequently co-occured objects with the corresponding texts.”

Line 115: “[Tong et al., 2024] find that CLIP misaligned samples will further cause the hallucination of LVLMs.” I do not understand this statement, grammar errors.

Line 132: “Meanwhile, many existing datasets, e.g., DomainBed and Wilds, do not have overlapped label space with ImageNet, making the comparison between ImageNet and CLIP models hard.” There is a version of DomainBed [C] where the dataset has been filtered to only include classes compatible with ImageNet, such that an evaluation of ImageNet models is possible out-of-the-box.
[C] https://openreview.net/pdf?id=LiC2vmzbpMO

Line 171: “Recalling that, when CLIP models resort to the shortcut of data, the model performance will heavily correlate with the backgrounds presented in the common group yet is compromised when coming to the counter group.” Grammar errors, I do not understand this sentence. What is “the shortcut of data”?

Line 208: “It suggests that the CounterAnimal dataset captures some general spurious shifts that at least commonly present in the pre-train dataset of LAION400M.” grammar

Line 213: “Here, the spurious features degenerate the zero-shot robustness of CLIP models trained on both LAION2B and by OpenAI.” Typo? “degenerate”?

Line 243: “In Figure 7, we consider two pre-train datasets, namely, LAION2B and the close-soured data from OpenAI” typo

Line 297: “Nevertheless, in the following theorem, we justify that CLIP remains learning to use spurious features, aligned with our experimental observations on the CounterAnimal dataset.” grammar

Strange space break between line 310 and 311.

# Summary of the review:
We could fix the naming convention from ""common"" and ""counter"" to something like ""hard"" and ""easy"" since accuracy has been used rather than frequency of certain backgrounds to classify backgrounds into certain groups. Based on my arguments below, I believe we cannot compare CLIP models to ImageNet models on the proposed dataset in any sensible way due to the introduced selection bias. I believe the very title of the paper is misleading since the posed question cannot be answered based on the methodology issues. But if we remove the claims about comparing ImageNet models and CLIP models, then, the main point of the paper is that there exist backgrounds which are harder for CLIP models, given certain classes, and other backgrounds which are easier. I don't think that this observation is particularly interesting on its own. The authors did not relate the hardness of the backgrounds to their frequency in the pretraining dataset or anything else. The observation that backgrounds matter is also not novel but quite well-known and the authors do not offer a solution. Further, the writing is quite poor and confusing on many occasions; I provided many examples of incorrect and confusing sentences below.

Limitations:
The limitations discuss the comparison in performance of the CLIP vs ImageNet models which I believe cannot be made due to the methodological issues in this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents CounterAnimal, an evaluation dataset featuring two subsets: animals with common backgrounds and those with unusual backgrounds. The images were sourced from iNaturalist. Data with high CLIP accuracy are categorized as ""Common"",  while those with low CLIP accuracy are labeled as ""Counter"".  Results shows that CLIP models experience a greater accuracy drop compared to ImageNet models when tested on this dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper analyzes multiple factors affecting CLIP accuracy, including model size and training data quality.
- The paper combines both experimental results and theoretical analysis. The analysis in Section 5 is interesting and novel.
- The paper is well-written and easy to follow.

Weaknesses:
- The proposed dataset is not sufficiently robust to analyze the influence of spurious bias, as this is not the only difference between the common and counter datasets.
  - To analyze the accuracy drop caused by spurious features such as background, the background should be the only difference between common and counter image pairs. Prior work [4,5] has proposed such datasets focusing on background.
  - In the proposed dataset, other factors may influence the model accuracy gap besides background. For instance, as shown in Figure 1, the more varied gestures of ice bears on the right compared to the left could be a contributing factor to the accuracy drop.

- Current experiments cannot conclusively show that ImageNet models generalize better than CLIP.

   - As the common and counter groups are selected according to the CLIP accuracy (see line 165 in the paper),  they indicate easy and hard samples for CLIP.    Since ImageNet models have different training characteristics, it is natural that hard cases for these models may differ from those for CLIP, resulting in a smaller performance drop for ImageNet models. This result cannot support that ImageNet models are more robust than CLIP models.
   -  The accuracy drop from common to counter group can be greatly influenced by the model used to divide the common and counter dataset. Using the combined proposed common and counter dataset, a new Common' and Counter' dataset can be created based on the accuracy of ImageNet models. What is the impact of this dataset division on the accuracy drop for different models?


- Prior studies[1,2,3,4,5,6] have proposed datasets specifically to analyze the influence of background, which are not discussed in this work.  These datasets can be used for CLIP evaluation as they do not overlap with the CLIP training set. Additionally, creating datasets based on model accuracy in this work is similar to the approach in [6].

[1] Noise or Signal: The Role of Image Backgrounds in Object Recognition.

[2] Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models, NeurIPS 2019.

[3] Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation.

[4] ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing, CVPR 2023.

[5] LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images, NeurIPS 2023.

[6] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object, CVPR2024.

Limitations:
This paper has discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors create an evaluation dataset comprising two groups, one with animals in usual backgrounds (common group) and another with unusual backgrounds (counter group). They then evaluate a suite of models of different backbones, model sizes, and datasets. They find that CLIP models do poorly than ImageNet-trained models, and generally high quality data or bigger model size improves counter group accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The CounterAnimal dataset is a nice contribution that can be of value to the community.
2. The authors have evaluated a number of models on the dataset and that too could be of value to the community.

Weaknesses:
Please see questions for more information.

Limitations:
The authors have addressed some limitations. For the rest, please see my questions block.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work asks one interesting question: ""Do CLIP models always generalize better than ImageNet models?"" Driven by this question, this work proposes a new benchmark dataset named CounterAnimal. This dataset consists of a) the common group: comprising animals in common backgrounds, and b) the counter group: including animals in plausible yet unusual backgrounds. The main idea is that the performance drops from the common to counter groups quantify the reliance on spurious background features for animal predictions. The main observation is that CLIP models exhibit notable performance drops when tested on the counter group. In comparison, ImageNet models can be more robust than CLIP models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is always good to see a new and novel dataset proposed for evaluating CLIP and ImageNet-trained models. The proposed dataset CounterAnimal is complementary to existing datasets that cannot reflect the robustness of CLIP models to spurious correlations. 

- The dataset construction is well-presented. The statistics, curation, background labeling, and spurious discovery are well introduced in Section 2

- The analysis around spurious correlation is good. This work tries to give insights from several aspects, such as pre-trained datasets, scaling up, and learning paradigms. The observations are sound to me.

Weaknesses:
-  I found the analysis of why CLIPs rely on spurious features interesting. However, I think the claim is somewhat ""obvious"": there exists a relatively strong correlation between the object captions and the parts of image backgrounds, CLIP will learn to align the backgrounds, i.e., spurious features. If the training dataset contains many examples of spurious correlations, then models will tend to be biased.

- I am curious about why ImageNet models may not be so influenced by the spurious bias in CounterAnimal. Is this because the ImageNet training set does not have too many spurious correlation examples? Or ImageNet has a spurious bias but such bias is different from the one in CounterAnimal? Please provide a discussion or share some insights on this question. 

- This paper adopts absolute performance drop in Section 3.3. Such a metric may not be so robust. For example, model A drops from 40 to 39, and model B drops from 90 to 89. They drop the same but the I would say model B is better. Please comment on this, and discuss the metric of absolute performance drop.

Limitations:
The dataset is proposed, so a discussion on the potential bias/ privacy is needed. I appreciate this work highlights the future improvement of expanding semantic scope, data source, and ImageNet testbed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
2BOb4SvDFr;"REVIEW 
Summary:
The paper proposes to use a new distance, Min-Max-Jump, which is the minimum largest distance on any path between two points, to be used in k-means clustering to learn clusters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The distance can overcomes some demerits of the convex (""spherical"" in the paper) clusters.

Weaknesses:
The distance in the paper is, in fact, related to single linkage clustering that assign give a pair of points a distance at which the pair is joint to one cluster. This need to be analyzed to relate to previous work as well as to compute pairwise distances efficiently. 

Theoretical property of the distance is poor. The paper should review many other density-based distance functions to put this work into the correct context. 

There would be a lot of problems using this distance as many of pairs of nodes would share the same distance. There is no analysis on the  metric property of this distance. 

On evaluation, the methods need to compare to single-linkage clustering (SSL) at the very least as all the advantages of using this distance with k-means are available in SLL in its simplest form.

Presentation-wise, it is hardly up the standard. There are methods/algorithms/concepts that are mentioned as ""a is like b with a difference"" without a formal definition. This mixes up definitions and properties.

Limitations:
The paper uses a new distance without theoretical justifications. It learns nonconvex clusters by using a nonconvex clustering-based distance (without explicitly mentioning it), which is hardly a novelty.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new metric, min-max jump distance. Effectively, say we are given a complete graph with vertex set $\Omega$ and edge weights $d(x,y)$ denoting the distance between $x$ and $y$, where $d$ is a metric. Then $MMJ(x,y|\Omega')$ is the minimum, over all paths between $x$ and $y$, of the maximum weight edge between $x$ and $y$ on the subgraph induced on the vertices $\Omega' \subseteq \Omega$. Explained nicely in the paper, if you started at vertex $x$ and wanted to get to $y$ and $d(\cdot,\cdot)$ denoted the distance required to ``jump'' from one vertex to another, what is the minimum distance you need to be able to jump to somehow traverse from $x$ to $y$?

This is a nice intuitive metric, and has strong connections to the minimum spanning tree. In fact, I suspect there is more literature to draw from minimum spanning tree research that could yield conclusions about MMJ. The MST is also nice in clustering since oddly shaped clusters (non-convex, for instance) can have small MSTs. This is the idea of MMJ: use it as a metric for K-means so that it can identify non-spherical clusters.

The paper proves some notable theory about the properties of MMJ. Mostly, they show how: 1) When adding a new vertex $p$ to a set $\Omega$, $p$'s MMJ within the context of $\Omega+p$ can be computed knowing all pairwise MMJ's within $\Omega$ within the context of $\Omega$. This effectively adds a new point and evaluates it within the complete, updated context. 2) Given the MMJs for this additional point $p$ within the context of $\Omega +p$, expand the MMJ context of all other pairs in $\Omega$ to the context of $\Omega+p$.

This can then be used in a very dynamic programming-like manner to start with just two points, add new points $p$ and find the context of $p$ and all other points, and then update all known existing MMJs to the new context. This is their algorithm 1, requiring $O(n^3)$ time. They also use properties of the MST to bypass unnecessary calculation to yield algorithm 2, which takes only $O(n^2)$ time (to find the MST).

They then evaluate the performance of algorithms using the MMJ measure on irregular shaped clusters to verify that MMJ helps identify these. This makes sense, since they likely have small MSTs, but not small average/sum/max/etc distances within clusters. Notably, they show how MMJ improves K-means.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
3: good

Strengths:
I think MMJ is a very cool metric with nice properties and intuition, particularly that related to the MST (I wish the authors had spent more time discussing this!). Their findings are nice and relatively simple to understand (in spite of the presentation). They show that it helps K-means expand to more complicated cluster shapes, and overall it is a very nice, NeurIPS-worthy result. However, as I will explain in the weaknesses section, I do not think this paper is in an acceptable state for NeurIPS.

Weaknesses:
There are a few notable downsides. In terms of the result, I'm not entirely convinced of its novelty. How much of this is actually a re-iteration of MST-based methods already understood? Is this really better than other MST-based algorithms on irregular clusters (think single linkage)? I know that there is a lot of literature that explores irregular shaped clusters, but I am not an expert in this area and so I cannot place this work in the context of existing results. I wish the authors would explain that. Though even if these algorithms aren't entirely better than state of the art, the novelty of the nice formulation of MMJ is certainly appreciated.

However, the biggest flaw in the paper is the writing quality. There are places where the paper is nice and concise, but most of the time it just lacks exposition to understand the higher level of things or adequate details to fully understand what is happening. Formal proofs are contained in the paper, but the jumps in some of the proofs are too large. Theorems and proofs are placed back to back with no high level explanation. Algorithms are written and pseudocode with only the briefest justifications, and no thorough explanations. This is not an acceptable paper for NeurIPS, and I think these issues are too extensive to simply ask the authors to revise. Though if other commenters disagree, I am amenable to changing my opinion.

And one of the disappointing things about this is how natural this work is and how much it lends itself to nice intuitive explanations and visual depictions! For instance, you could do some very nice visualizations of Algorithm 1, where you depict a matrix and show which indices have been calculated to what context $\Omega_n$ at each time point. This clarifies the different purpose of the two loops.

I hope to see this paper submitted again later in a more cleaned up state!

Limitations:
None notable

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents the Min-Max-Jump (MMJ) distance concept and two calculation methods, focusing on path optimization in data analysis and clustering. The contributions include introducing MMJ distance, proposing efficient calculation methods, discussing its properties and applications, and offering a user-friendly approach for practical implementation. Overall, the paper introduces a new distance metric for path optimization and data analysis, providing useful tools and insights for various applications in the field.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1: The paper demonstrates strength through its meticulous use of theorems and proofs, enhancing the credibility and robustness of the research findings.

S2: Clear visualization of results in the paper aids in effectively conveying complex information to the readers, improving understanding and interpretation.

S3: Extensive literature citations throughout the paper showcase a strong foundation of existing knowledge and research, adding depth and scholarly rigor to the study's presentation.

Weaknesses:
W1: The paper's writing style deviates from academic norms, indicating a need for improvement in writing proficiency.

W2: The extremely brief Introduction lacks a detailed definition of the problem, its significance, and challenges. Moreover, it lacks citation support for the points presented. While Section 2.1 mentions methods like k-NN, UMAP, HDBSCAN, it fails to provide corresponding references, lacking essential academic backing.

W3: The overall structure of the paper lacks clarity, as it introduces different distance metrics in Section 2.1 but introduces a new distance measurement approach in Section 2.4, leading to disjointed logic.

W4: The presentation of various distance metrics in Section 2.1 appears disorganized and lacks coherence.

W5: The extensive definition provided towards the end of Section 6.3 disrupts the logical flow of the paper, suggesting a need to adjust the paper's structural coherence.

Limitations:
The paper does not discuss limitations. The authors seem to perceive their work as solely testing models with datasets without considering the shortcomings of the algorithms themselves.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Different distance metrics have been introduced in the literature for data analysis. In this paper the authors consider the min-max-jump distance and apply it in the context two applications, namely, k-means clustering and as an internal clustering evaluation index. They also present two algorithms for computing the min-max-jump distance.

Experimental comparisons reveal that min-max-jump based k-means clustering is better than standard k-means clustering. Also, the min-max-jump distance is shown to be a better internal clustering evaluation index.

This referee feels that this work is rather incremental. Also, experiments have been conducted only on very small datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate the efficacy of the min-max-jump distance on two different applications.

Experimental results have also been supplied to support their assertions.

Weaknesses:
The work done is incremental with very limited novelty.
Extensive experiments are called for.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
LuqrIkGuru;"REVIEW 
Summary:
The authors propose a fairness attack on GNN through node injection. They propose two node injection principles, the uncertainty-maximization and homophily-increase principle, to make fake node injections lead to a more significant fairness compromise.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This article is well-written and highly readable.

The focus on attacking fairness is interesting.

The author’s discussion on potential extensions of the method, such as different approaches to measure node vulnerability and potential mitigation methods, is encouraging.

The choice of datasets and baselines for validating attack performance is representative, demonstrating the significant effectiveness of the proposed method.

Weaknesses:
1. The main concern is that the proposed node-injection-based fairness attack could potentially be mitigated by existing defenses designed for accuracy-based node-injection attacks. The distinction between fairness-targeted and accuracy-targeted attacks is not discussed, and there is a lack of an in-depth discussion on related work concerning node-injection attacks aimed at accuracy.

2. Another concern is that the theoretical effectiveness of ""the node injection strategy is evaluated by an increase in the node-level homophily ratio"". This homophily ratio does not clearly establish a connection with fairness metrics, i.e., DP/EO.  Providing a theoretical guarantee that the proposed node injection strategy results in more significant improvements in DP/EO compared to random node injection will enhance the validity.

3. Additionally, the motivation of attackers to undermine fairness is not clearly discussed. Adding some real-world examples to demonstrate the motivation of such attacks will be better.

Limitations:
1. The proposed method is designed for binary classification with two sensitive attributes. It should be considered whether and how the proposed method can be extended to multi-class classification and multiple sensitive attributes.

2. The experiments were conducted on a two-layer GCN, which is consistent with the baseline [10]. However, a discussion is expected about the feasibility of the attack  on GCNs with different architectures.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a Fairness GNN Attack method called Node Injection-based Fairness Attack (NIFA). The proposed method aims to increase the bias of GNN models by injecting nodes into the graph. NIFA identifies nodes with high uncertainty to target them, then connects the injected nodes in such a way increasing the homophily of the graph. The features of the injected nodes are then optimized to balance utility with fairness. NIFA is evaluated on multiple benchmark datasets and compared with Fairness GNN attack methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is very well written and easy to follow. 
- The proposed method seems to be technically sound.
- The fairness of GNN models and Fairness attack methods are both important and well motivated problems.

Weaknesses:
- The proposed attack method is not well motivated, could the authors provide concrete application domains/cases where it is not possible to modify/attack the existing edges between nodes in the graph but it is possible to inject nodes into it and connect such nodes to the rest of the graph? 

- Furthermore, if modifying the edges in graph is considered unrealistic, how is this different from the part in the proposed method where injected nodes are connected to the nodes in the graph ? The limitation of previous works claimed by the authors seem rather arbitrary and inconsistent. If the ability to modify the graph structure is not a realistic assumption, then connecting the injected nodes to the real nodes in the graph should similarly be considered unrealistic. Basically, if connecting injected nodes to the real nodes in the graph is permissible, then it should be permissible for previous fairness attack methods to modify the graph structure by adding edges to it (without deleting existing edges). 

- The utilized 1% perturbation rate is rather high, in real-world large graph datasets consisting of millions of nodes, this is equivalent to injecting the graph with tens of thousands of nodes which can hardly be considered unnoticeable. The authors should experiment with significantly smaller perturbation rates and compare the corresponding results against the relevant baselines in the literature. 

- It is not possible to evaluate the effectiveness of proposed method against the relevant fairness attack methods using a single dataset only as 2 out of the 3 fairness attack methods are reported without results on 2 datasets. The authors should include additional datasets and/or additional Fairness attack methods. 

- All 3 fairness GNN Attack methods [11, 13, 40] report results on one or both Pokec datasets. Therefore, if the author do not have access to the computational resources required to run the aforementioned baselines, they should run their proposed method on the setups of those 3 baselines and report the results of the 3 baselines  [11, 13, 40] from the corresponding works. In this manner, we would be able to properly evaluate the effectiveness of the proposed method against the relevant Fairness Attack baselines in the literature.  

- It is unclear how the authors ensure a fair budget across all attack methods given the different nature of the attacks where some methods modify the graph structure and node features, while other methods inject the graph with additional nodes. Could the authors please elaborate on this point and how they ensured fairness across budgets of different types of attack methods ? 

- The train/val/test split utilized assigns half of the nodes to labeled training nodes. However, in most realistic scenarios a significantly smaller percentage of the nodes are assigned to labeled training nodes. How does the proposed method perform when the number of training nodes in the graph is limited ? This is specially important given that targeted nodes with high uncertainty in this work are a subset of the training nodes. The authors should conduct experiments with limited labeled nodes and evaluate the effectiveness of the proposed method under this more realistic scenario. 

- The authors should evaluate their proposed method on additional common benchmark datasets for Fair GNN learning task such as Credit, Bail and NBA datasets.

Limitations:
The authors adequately discuss the limitations of their proposed method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper examines the vulnerability of GNN fairness under adversarial attacks.  A gray-box node injection-based poisoning attack method, namely NIFA, is proposed. NIFA follows the newly designed uncertainty maximization principle and homophily-increase principle. Then,  multiple novel objective functions are proposed to guide the optimization of the injected nodes’ features, impacting the victim GNN’s fairness from a feature perspective. The experiment is extensive and solid.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1: The problem of the vulnerability of GNN fairness is interesting and very important. This paper is well-motivated.

S2: The proposed method is technically sound.

S3: The experiment is solid and extensive. Very comprehensive experimental results are reported in the appendix including hyper-parameter testing, ablation studies, analysis of poisoned graph, etc.

S4: The paper is well-written and very easy to follow.

Weaknesses:
W1: Node injection will change the topological structure of a graph. However, several existing studies work on structural fairness in GNN. I am wondering whether the proposed fairness attacks are applicable to those works. What if the graph structure changes over time and becomes dynamic? Is the proposed attack applicable to dynamic fairness GNN methods?

Some references:

[1] Uncovering the Structural Fairness in Graph Contrastive Learning. NeurPIS 2022.

[2] Tail-GNN: Tail-Node Graph Neural Networks. KDD. 2021.

[3] On Generalized Degree Fairness in Graph Neural Networks. AAAI. 2023.

[4] Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach. KDD. 2024.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes NIFA, a novel fairness attack method via node injection. In particular, the authors use the uncertainty maximization principle to select the target node to attack and randomly connect the injected nodes to the target nodes in the same sensitive group to increase the overall homophily. Finally, the authors use direct optimization to tune the features of the injected nodes to maximize the unfairness and minimize the predictive loss. Experimental results show the effectiveness of NIFA in reducing the fairness of GNNs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper delivers the first node injection attacks on fairness for GNNs.
- The paper is overall well-organized and easy to follow.
- The methodology is basically well-motivated and verified by empirical studies.

Weaknesses:
- Although experiments show that NIFA distinctly increases the bias level of GNNs, the accuracy decreases as well, especially for DBLP. It can be helpful if more discussions on the tradeoff between accuracy and bias level are provided.
- Complexity analysis is not included in the paper. A succinct discussion (theoretical or empirical) on the computational efficiency of NIFA is encouraged.
- There exists a gap between Lemma 1 and the goal of the homophily-increase principle. Increased homophily cannot guarantee the increase of $\Delta_{sp}$ and $\Delta_{eo}$.
- The overall framework of NIFA is simple but kind of incremental. The technical novelty and provided insights are somewhat limited.

Limitations:
The limitations including the potential negative societal impact of this work are sufficiently discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Cth1PyCwZt;"REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nu2Sqrsnr7;"REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cuO0DenqMl;"REVIEW 
Summary:
This paper proposed a new ensemble algorithm, called Wasserstein Gradient Boosting (WGBoost), which is a novel gradient boosting framework that leverages the wasserstein gradient for probabilistic prediction. Specifically, WGBoost fits a new base learner to the Wasserstein gradient of a loss functional on the space of probability distributions. Since we cannot access to the probability distribution at each iteration and we also cannot evaluate the wasserstein gradient at each iteration, the author proposed to implement it using the particle method and approximate the functional gradient by the kernel method. The algorithm returns a set of particles that approximate a target distribution for each input. The main application demonstrated is posterior regression, where WGBoost provides a distributional estimate of output-distribution parameters.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- WGBoost's ability to approximate target distributions with particles offers a robust approach to posterior regression,  capturing predictive uncertainty effectively. 
- The proposed method shows superior performance in empirical evaluations on real-world tabular datasets, both for regression and out-of-distribution detection tasks.
- The implementation is seems easy by utilizing the particle method combined with kernel approximation.

Weaknesses:
The approach seems interesting, but the paper lacks the comparison with existing work. Moreover, there is no analysis or discussion when this algorithm is useful as I will explain in the below. 
- It seems that the proposed method seems almost identical to the stein variational gradient descent (SVGD), but there is no qualitative and quantitative comparison with that. Please explain what is the fundamental difference compared to SVGD. 
- Since the proposed method is very similar to SVGD, I think the comparison with SVGD and its extended methods including [3, 5, 6, 7] (I think there are other many variants of extention in SVGD).

- It has been known that the posterior approximation quality strongly depends on the choice of kernels in SVGD [1, 2, 3, 4]. Since the proposed algorithm is almost identical to SVGD, the quality of approximation by the WGBoost is strongly affected by the choice of the kernel function. However, there is no discussion about this point.

- The numerical experiments are only conducted with respect to the final performance on benchmark dataset and I cannot understand when and what kind of problems the proposed algorithm is suitable to  approximate the posterior distribution. It is known that SVGD suffers from collapse phenomena.
- In addition to the above point, there is no discussion about the computational cost of the proposed algorithm. When I say the computational cost, I point about the computational cost at each iteration and convergence speed. How large computational cost is compared to existing method regarding the number of particles and training dataset size ? I think the proposed algorithm is based on the boosting method, so it suffers from large computational cost with respect to the training dataset size.
- As for the convergence speed, it has been known that the convergence of the SVGD is slow, which does not show the linear convergence [4], and I suspect that the proposed method suffers similar problem. However, no discussion is present about the convergence speed or no numerical comparison exists with existing method.

[1] Stein Points 

[2] Measuring Sample Quality with Kernels

[3] Kernel Stein Discrepancy Descent

[4] On the geometry of Stein variational gradient descent

[5] FUNCTION SPACE PARTICLE OPTIMIZATION FOR BAYESIAN NEURAL NETWORKS

[6] Feature Space Particle Inference for Neural Network Ensembles

[7] Repulsive Deep Ensembles are Bayesian

Limitations:
The limitation is unclear. As far as I read no formal description is presented.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a probabilistic boosting tree algorithm called Wasserstein boosting that uses a smoothed particle gradient to provide probabilistic predictions. Experiments are performed using UCI tabular regression, and out of distribution classification.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: 

-	I like the application of Wasserstein gradients and gradient flow to gradient boosting trees, don’t think I’ve really seen any paper like this before.

Quality:

-	I didn’t check especially carefully but the machinery for the algorithm seems to be well explained and correct.

Clarity:

-	The relevant machinery of Wasserstein gradients and gradient boosting is mostly well explained.

Significance:

-	Making trees more probabilistic with limited modifications to their tabular capabilities would be a quite nice advance.

Weaknesses:
Originality: 

-	Nothing really noted. Being a straightforward application of some machinery is totally fine.

Quality:

-	For a paper on trees, I find the experiments quite small scale and limited. I would have expected NLL experiments on larger scale datasets, such as the xgboost and lightgbm papers. 

-	Comparisons to (approximate) Bayesian neural networks are quite weak – many other methods such as sgmcmc, etc. tend to outperform on datasets of these scales. references: https://arxiv.org/abs/1902.03932, https://arxiv.org/abs/1907.07504, https://arxiv.org/abs/2002.03704, amongst others

-	The natural missing comparison here is to Gaussian processes and other kernel methods, which are naturally probabilistic and similarly nonparametric (like trees).

-	Another missing set of experiments is comparison to quantile regression  / pinball loss using trees, which is implemented directly in lightgbm. Quantile regression itself is also naturally nonparametric in at least some sense.

Clarity:

-	My understanding of Wasserstein particle flows is that the particles should interact in some manner during the gradient step. However, the writing of Algorithm 1 makes this quite unclear. I think that the kernel smoothing in the gradient step for the approximate flow is what makes the particles interact, but it’s overall quite unclear.

         o	The code doesn’t seem to provide any clarity here.

-	Overall, it’s quite unclear which parameters in the loss are actually being estimated. If we’re only estimating uncertainty in regression parameters (or analogously classification), then there’s straightforward two stage approaches. 

      o	For example, one can easily fit a tree predicting the mean and then modify the loss function to predict its variance, or we can modify the loss in classification problems to do something analogous.



-	The writing is extremely passive and non-specific. Suggestions below.

       o	L150: “procedure of exact or approximate” Please use the algorithm box to specifically write or point out which algorithm is used in the experiments. The current algorithm is so unspecific it’s very hard to follow.
       o	L113-115: “Although [32] … originally suggested…” rephrase to something like “Although Friedman [32]  originally proposed using a line search … , Buhlmann and Hothorn [34] recommend against the line search …”

Significance:

-	Part of the strength of trees in my experience is that they scale pretty well to large tabular datasets (e.g. n = 10 million). You also tend to need strong uncertainty quantification on these types of datasets, which is part of the reason why Bayesian neural nets became popular for a while. Yet, these large scale uncertainty quantification experiments are lacking from the paper.

      o	Bayesian neural nets: https://proceedings.mlr.press/v115/izmailov20a.html, https://proceedings.mlr.press/v130/immer21a/immer21a.pdf, 

      o	Gaussian processes: https://arxiv.org/abs/1809.11165,

Limitations:
n/a

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to perform boosting with base learner which are fitted to the Wasserstein gradient of a loss function on the space of probability distributions, which can be useful in particular to capture uncertainty of the models.

Several variants of the algorithm are discussed (by adding a diagonal Hessian preconditioner and with different approximation of Wasserstein gradients for functionals not differentiable on discrete measures). The method is demonstrated on posterior regression tasks where the functionals are KL divergences with respect to some prior, and applied on different real datasets benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is well written and proposes a new interesting boosting method to minimize loss over probability distributions.

- The paper is well written
- A new boosting algorithm guided by functionals on probability distributions
- Application on real datasets outperforming baseline methods

Weaknesses:
The paper is good overall in my opinion, but still has some weaknesses.

- Experiments focus on KL divergence functional.
- No theoretical analysis

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel gradient boosting framework called Wasserstein Gradient Boosting (WGBoost). Unlike traditional gradient boosting methods that fit base learners to the gradient of the loss function, WGBoost fits them to the Wasserstein gradient of a loss functional defined over probability distributions. This approach is particularly useful for probabilistic prediction, where the goal is to approximate the uncertainty in the model prediction. The authors provide a general formulation of WGBoost, its algorithmic implementation, and empirical evaluations on various benchmarks. 

Paper's major contributions include, the introduction of the Wasserstein gradient flow framework into gradient boosting; the development of an approximate algorithm for posterior regression using the KL divergence; and the demonstration of WGBoost's performance on regression, classification, and out-of-distribution (OOD) detection tasks. Authors also propose a second-order WGBoost algorithm built on the approximate Wasserstein gradient and Hessian of the KL divergence.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
S1: The application of Wasserstein gradient flows to gradient boosting is a novel and promising direction, providing a new perspective on ensemble learning algorithms.

S2: Solid theoretical concepts, with detailed derivations and explanations of the Wasserstein gradient and Hessian approximations.

Weaknesses:
Authors did not specify details about the hyperparameter selection for Conditional Density Estimation, Classification and OOD Detection tasks.

Misc:
There is a typo in the line no 284, root is written as room.

Limitations:
NA

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
u1b1dJtyxc;"REVIEW 
Summary:
The paper studies the existing ""brain score"" approach of evaluating how similar LLM representations are to human brain activity. First, they show that when using shuffled train-test splits on the Pereira dataset, which some prior studies use, a trivial temporal auto-correlation model performs similarly to GPT2-XL. Second, they show that untrained GPT2-XL's brain score is simply due to encoding sentence length and position. Third, they show that a trained GPT2-XL's brain score is largely explained by sentence length, sentence position, and static word embeddings, which are all non-contextual features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper studies the important topic of understanding why recent research has shown similarities in LLM representations and human brain language activity. 
2. They highlight issues with existing neural datasets commonly used in the LLM-brain field, e.g., shuffled train-test splits on the Pereira dataset.

Weaknesses:
From most to least significant:
1. The paper writes: ""OASM out-performed GPT2-XL on both EXP1 and EXP2 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a model’s brain-likeness"" (Lines 173-177). 
- I agree this shows that a model that exploits temporal auto-correlation, OASM, can achieve similar neural predictivity on the Pereira dataset as GPT2-XL. However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to temporal auto-correlation, rather than linguistic similarity. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to each factor. Although GPT2-XL can theoretically exploit temporal auto-correlation artifacts, it may not be empirically doing so as it was optimized for language performance instead.
- Furthermore, OASM may be a much stronger method at exploiting temporal auto-correlation than GPT2-XL's architecture is capable of. The paper's results may highlight that the Pereira dataset is easy to ""cheat"" using temporal auto-correlation, but not that GPT2-XL or other LLMs are doing so.
2. The paper evaluates ""brain score"" using a metric they defined, out-of-sample R-squared, whereas the prior research they cite [2, 24] seemed to use Pearson correlation. Although they argue for the advantage of the metric they used, it is challenging to understand how their results relate to prior research. For example, they do not show the Pearson correlation that their OASM model obtains on Pereira, which would make it easier to compare to models in prior research. Furthermore, they only tested a single language model, GPT2-XL, whereas more recent research has used larger or different models.
- Additionally, the metric they defined seems to produce results close to 0 for GPT2-XL and less than 0.05 for all models too. In Figure 2b, the R-squared results cluster around zero, with many negative values. They obtain an average R-squared value that is positive (e.g., Figure 2a?) only because they clip negative values when averaging.
3. The paper provides a theoretical justification arguing that GPT2-XL can encode sentence length and sentence position (Lines 197-201). However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to sentence length/position, rather than contextual/semantic features. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to the two factors.
- They compared GPT2-XL to two ideal models of sentence position (SP, represented as a 4-dimensional one-hot vector) and sentence length (SL, represented as a scalar). However, these ideal models may be a much ""cleaner"" representation of sentence length/position than the perhaps noisy GPT2-XL representation of sentence length/position that may not be cleanly and linearly decodable.
4. The paper writes: ""GPT2-XL only explains an additional 28.57\% (EXP1) and 16.7\% (EXP2) neural variance over a model composed of features that are all non-contextual."" However, the paper does not provide a noise ceiling for the metric they defined, out-of-sample R-squared. Consequently, it is unclear whether the small improvements in neural predictivity is due to hitting the noise ceiling.

Limitations:
Limitations not mentioned in the paper:
1. Please see Weaknesses 1-4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate the simplest set of features that can explain variance in neural recordings (fMRI, ECoG) during language processing. The authors focus on the surprisingly high alignment (""brain scores"") of untrained LLMs, but also investigate trained LLMs. The authors conclude that the predictivity performance of untrained LLMs can be explained by simple features such as sentence position and sentence length. The authors quantify the effect of autocorrelation on shuffled cross-validated train-test splits and find that predictors that account for the temporal structure in the neural data explain the data better than other (linguistic) features. Overall, the study highlights the importance of understanding why LLMs (or, any feature space for that sake) map onto the brain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is generally well-written, and the analyses are well-motivated. The topic is timely.
- The paper is very comprehensive, and provides in-depth analyses of one widely used dataset (from Pereira et al. 2018) for LLM-brain mapping studies. The paper also investigates two other datasets, but in less depth. The authors run analyses across several seeds for the untrained models, and in general, include a good amount of well-motivated control analyses.
- The analyses of trained GPT2-XL are interesting (Section 3.3), and provide a good contrast to the analyses of the untrained models.

Weaknesses:
- The authors motivate the paper with ""attempting to rigorously deconstruct the mapping between LLMs and brains"", but do not really acknowledge other work doing so. The paper lacks a short relevant work section on other studies that ask why artificial models map onto human brain data (from language, e.g., Merlin and Toneva, 2022; Kauf et al. 2023; Gauther and Levy, 2019, ...). 
- I find it very odd that the authors include ""brain scores"" in their title and also motivate the study through Schrimpf et al. 2021, but then do not replicate almost any of the analysis choices in Schrimpf et al. 2021: the feature space pooling is different, the ridge regression, the evaluation metric. For instance, sum feature pooling is motivated because ""it provides higher alignment scores"", but other studies motivate last token pooling because it is conceptually better motivated (Transformers integrate over the context). It does not feel quite right to make decisions based on ""what gives the highest alignment"", because, perhaps sum feature pooling does indeed artificially inflate scores. Either way, it is not very suitable to link the title and most of the motivation of the paper based on one instance of prior work, and then make completely different choices. That being said, the choices are definitely well-motivated in most cases, but it makes the comparison with prior work different -- which is fine, the motivation should just be changed in that case. 
- The authors should make it more clear which voxels are used in which analyses. The authors mention that unless otherwise noted, the language voxels are used (line 75), but it is not always very clear. For instance, Figure 2d clearly includes voxels across several networks.
- Regarding novelty: Kauf et al. 2023 also investigated contiguous splits on Pereira2018 as a supplementary analysis (not to the same extent as in the current paper), and also discusses the problem of temporal auto-correlation.

Limitations:
The authors discuss limitations of their study.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper paper studies the topic of neural - brain representation mappings. They focus on three neural datasetse commonly used in LLM-to-brain mapping studies: Pereira fMRI, EcoG and Blank fMRI. Specifically, the study investigates the assumptions underpinning previous positive reports about the existence of mappings between brain representations and LLM internal representations. The study focuses in particular on GPT2-XL, which was shown to perform well on the Pereira dataset in particular, with which a series of brain-activation prediction experiments are performed.

The first presented result is that when shuffled train-test splits are used, the result is very different than when contiguous train-test splits are used, with opposite patterns on which layer performs best. This is particularly true for fMRI datasets. The authors then train an orthogonal auto-correlated sequences model on the shuffled split, which out-performs GPT-2-XL despite having a completely non-linguistic feature space. The authors take this as a signal that previous results should be challenged on their conclusion that high performance on this benchmark should be taken as an indication of brain-likeness.
 
Next, the authors investigate what explains the neural predictivity of an untrained GPT2-XL model, and they fi
nd that it is fully accounted for by sequence length and position. Following-up on that, they find that these
features are also main drivers for much of the neural predictivity of a trained GPT2-XL model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper presents a detailed study into why LLM activities may be predictive of neural activities, 'debunking' several previous claims. I think there is a lot of value in this
- The experiments seem sound (though I am not an expert in this field)
- The conclusions are interesting, and contain valuable lessons for future work on this topic

Weaknesses:
- The presentation could be improved, in my opinion. I don't always find everything completely clear. For instance, the notion of 'shuffled train-test splits' is quite important for the paper, but it is never really explained how they are specifically constructed, and how they differ from their 'contiguous' counterpart. (I can imagine multiple dimension in which one could shuffle)

Presentation suggestion: I think it may work better if the results of the different datasets are grouped together, result-by-result, rather than dataset by dataset.

Limitations:
The authors adequately address limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There is a large body of research focused on measuring the similarity between language processing in the brain and in language models. Recent studies have shown that representations from Transformer-based language models exhibit a higher degree of alignment with brain activity in language regions. However, the authors mention that this inference is valid only for the subset of neural activity predicted by large language models (LLMs).
The primary aim of this paper is to investigate this question by analyzing three popular neural datasets: Pereira, Blank, and Fedorenko. To achieve this, the authors build voxel-wise encoding models to compare encoding performance between representations from language models and brain recordings in three settings: (i) shuffled train-test splits during voxel-wise encoding, (ii) untrained LLM representations and their alignment with the brain, and (iii) trained LLM representations and their alignment with the brain. The experimental results demonstrate that untrained language models are explained by simple linguistic features such as sentence length and position, while trained language models are explained by non-contextual features (i.e., word embeddings).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This study primarily focuses on understanding the reasons behind the better alignment between language model representations and brain recordings. The exploration of various simple linguistic and non-contextual features, and their contribution to explaining the variance in brain predictivity over contextual embeddings, is valuable for the research community.
2. The authors tested different validation setups, including comparisons between untrained versus trained models and shuffled versus unshuffled data, to evaluate brain scores. 
3. Controlling features with different combinations provided valuable insights into the contribution of each feature to the performance of brain alignment.

Weaknesses:
1. While the main research question aims to investigate the simplest set of features that account for the greatest portion of the mapping between LLMs and brain activity, the insights remain unclear for specific language regions of the brain. For instance, considering language parcels based on the Fedorenko lab, do simple features explain all the variance in these language regions? Or do these features only account for early sensory processing regions?
2. It is a well-known fact that Transformer-based representations consist of both low-level and high-level abstract features. If embeddings from language models predict brain activity and this predictivity is only due to a simple set of features, it should be better interpreted using approaches like residual analysis (Toneva et al. 2022), variance partitioning (Deniz et al. 2019), or indirect methods as suggested by Schrimpf et al. (2021).
3. Shuffling train-test splits is not an ideal scenario for brain encoding, especially for continuous language. All prior studies follow unshuffled train-test splits, i.e., contiguous time points (TRs). Shuffling the train-test split can result in sentences from the same passage being present in both the train and test sets, which is not ideal for model validation.
4. What are the implications of this study for both the AI and Neuroscience communities? What are the final conclusions?

Limitations:
Yes, the authors have presented several limitations in the conclusion. However, these limitations do not have any societal impacts on this work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the similarity between large language models (LLMs) and human brain activity by analyzing brain scores, which measure how well a model predicts neural signals. The authors question the validity of using brain scores as a measure of similarity between LLMs and human cognitive processes. They analyze three neural datasets and find that simple features like sentence length and position explain much of the neural variance that LLMs account for. They caution against over-reliance on brain scores and emphasize the need for a detailed deconstruction of what LLMs are mapping to in neural signals.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study provides a thorough examination of how various features (simple and complex) contribute to the neural predictivity of LLMs, offering a detailed deconstruction of the relationship between LLMs and brain activity. The replication of key findings using RoBERTa-Large, in addition to GPT2-XL, strengthens the validity of the conclusions drawn regarding the generalizability of the results across different LLM architectures

Weaknesses:
1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.
2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zeNwOAcb4q;"REVIEW 
Summary:
In this work, the authors proposed an approach to estimate the instance-dependent transition matrix in order to reliably learn from noisy labels. The idea is to use a condition diffusion model to estimate the transition matrix by using the pretrained extracted image features as the conditions. Once the transition matrices are estimated, the classifier is learned through the corrected cross entropy loss. Experiments are presented to compare the performance of the approach with other baselines using both synthetic and real noisy datasets.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is easy to read and notations are clearly stated

Weaknesses:
The main weakness is the lack of support and discussion in substantiating the idea. Experiments are insufficient to support the claims.

Limitations:
No limitations are discussed

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of supervised learning from noisy labels, where the label noise is modeled using instance-dependent label transition probability matrix. Mainly, this work attempts to leverage conditional diffusion model in order to obtain a generative model of transition matrix conditioned on the sample features. To that end, this work first generate pseudo paired samples $( x_i, T_i )_{i=1}^N$ using existing method (VolMinNet). Secondly, a conditional diffusion model is trained that generates $T_i$ given $x_i$. Finally, the classifier is trained taking into consideration the estimated transition matrix from the diffusion model.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem considered is of interest to the broad ML community
2. Adequate experimental settings, baselines, and ablations are provided for numerical validation.
3. The attempt to apply diffusion model is novel.

Weaknesses:
1. The technical soundness of the proposed method is questionable. Essentially, the proposed method trains a conditional diffusion model using paired samples $(x_i, T_i)$. If we consider the true transition matrix as $T(x)$ for a sample $x$, then the idea of the proposed method is to train a conditional generative model $p( T(x) | x )$. There are several issues with this attempt and the proposed implementation:
   (a) The authors use pseudo transition matrix $T_i$ generated from a sample-independent method (VolMinNet). $T_i$ only depends upon the cluster assignment of $x_i$. The diffusion model, at best, can approximate the conditional distribution $p( T_i | x_i )$. This has no clear relation to $p(T(x) | x)$. Therefore, in principle, the transition matrix generated by the trained diffusion model cannot be better than that returned by VolMinNet.
   (b) Second, the transition matrix is modeled as a deterministic function of sample, i.e., only one $T(x)$ exists for a given $x$. Therefore, it does not make sense to learn a generative model for $p(T(x) | x)$, since it is a degenerate distribution (probability of all other matrices should be zero except the true $T(x)$). 

2. Another hint at why the proposed method should be limited by the pseudo paired sample distribution is that the diffusion model training part (which is ultimately used as transition matrix estimator) does not require available noisy labels. Hence, no extra information can be extracted about the true transition matrix $T(x)$ beyond the information captured by the pseudo paired samples $(x_i, T_i)$. 

2. It is unclear where the performance gain in empirical results is coming from. The manuscript does not provide any intuitive or theoretical explanation to justify the quality of their estimator. Moreover, no rationale for the algorithm design is provided.

Limitations:
Limitations are not adequately discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the estimation of the transition matrix with instance-dependent label noise. They used a diffusion model for this estimation. By applying a diffusion process to the transition matrix, the diffusion model is trained to generate transition matrices from a prior distribution. The instance-wise generated transition matrix is then used to train the classifier with a forward cross-entropy loss. The improvement of the method is demonstrated by experiments on benchmark and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The instance-dependent label noise scenario is a challenging task.

Weaknesses:
* The reason for generating the transition matrix using a diffusion model is unclear.
  * The instance-dependent transition matrix is the target to be estimated, but it is uncertain what role training a diffusion model to generate the transition matrix without a fixed target.
  * In addition, as mentioned by the authors, the transition matrix must be satisfied: the entries are greater than 0, the row sum is to be 1, and the diagonal entry is typically the largest. However, these considerations have not been taken into account in the construction of the diffusion process. Although a transformation method is proposed in Section 3.4, there is no discussion of how this affects the training of the diffusion model.

* Pre-trained features are fed into the diffusion network, but their impact on the diffusion process has not been analysed. This could be seen as providing additional conditional information during the diffusion process, implying that this diffusion model might be a conditional diffusion model. It would be better to discuss these consideration.

* In Algorithm 3, it appears that the diffusion model is trained in order to generate the initialized $T_i$. I wonder if the desired training is for the initialized $T_i$ to be generated perfectly as is. This could lead to a transition matrix that might not contain instance-dependent information, raising questions about the mechanism by which diffusion training introduces variance.

* The diffusion training seems to take a considerable amount of time, which needs to be analysed. If it takes a long time, the performance improvement may not be significant in comparison.

Limitations:
They mentioned the limitations only briefly in the experimental section. I have noted additional limitations that I perceive in the Weaknesses part.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cgb0Tn4uHy;"REVIEW 
Summary:
This paper introduces a method that supplements the traditional estimate of a class-dependent transition matrix, which is popular in label-noise learning. Traditional transition matrix methods are less effective for instance-dependent noise. To overcome the limitation, the proposed method adds a residual term such that it can extend the projection of a class-dependent T on label predictions to fit the true one as if we have an instance-dependent T. Theoretical analyses of the algorithm confirm its convergence and generalization properties under specific assumptions. Experimental results on various synthetic and real-world noisy datasets such as CIFAR-N and Clothing1M show the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The performance is eye-catching.
2. The method is proposed with both theoretical analyses and experimental results.

Weaknesses:
1. The intuition of the proposed residual is not clear. For example, why a sparse structure is preferable in this problem? Why do u and v enable a sparse structure? Why is a Hadamard product employed? Why not simply use a vector u?
2. The theoretical part of the main paper is heavy but the outcome is not convincing. Specifically, there is a huge gap between Eq. (17) and Theorem 3.1.
3. The assumption in Eq. (7) is too strong.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of learning with noisy labels. To handle the instance-dependent noise, the authors propose an extended model for transition matrix-based methods. Specifically, their model combines a class-dependent transition matrix with a sparse implicit regularization term. The authors provide a theoretical analysis of the proposed method. Experiments conducted on both synthetic and real-world noisy label datasets verify the effectiveness of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Theoretical analysis of the convergence and generalization are provided.
2. Experiments are conducted thoroughly, including experiments on synthetic and real-world datasets. The ablation study is also conducted.

Weaknesses:
1. The method proposed in this paper appears to be a straightforward combination of VolMinNet and SOP.
2. The experimental results for TMR are missing for the CIFAR-N, Clothing1M, and WebVision datasets.
3. An important baseline, CCR [1], which is the state-of-the-art among transition matrix-based methods, is absent.
4. The paper lacks an analysis of the estimation error of the transition matrix. It would be beneficial to compare the estimation errors of the transition matrix for TMR against those of other baselines.

**Reference**

[1] Cheng, De, et al. ""Class-dependent label-noise learning with cycle-consistency regularization."" *Advances in Neural Information Processing Systems* 35 (2022): 11104-11116.

Limitations:
I did not find that the authors have discussed the limitations and potential negative societal impact of their work. To improve the paper, the authors can provide a thorough analysis of the limitations of their method in an independent section. For example, scenarios where the method might not perform well can be included.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In learning from noisy labels, existing methods generally focus on class-dependent (but instance-independent) noise that can be modeled by a transition matrix $\mathbf{T}$. Some methods have also been proposed for instance-dependent noise (modeled by $\mathbf{T}(x)$). This work belongs to the latter. In particular, it proposes to implicitly model $\mathbf{T}(x)$ using an extended model based on the transition matrix $\mathbf{T}$ and a residual term $\mathbf{r}(x)$. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm (TMR) are analyzed under certain conditions. Experiments show that the proposed algorithm outperforms baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
**Originality**

The paper studies the challenging problem of instance-dependent label noise, which is less addressed in the literature compared to class-dependent noise. The proposed extended model for transition matrix, which is a combination of a transition matrix with residual terms, seems novel and effective. Related work is adequately cited.

**Quality**

The experiments are quite comprehensive. The paper compares the proposed method with multiple methods (including some state-of-the-art ones) on various datasets. The experimental results show that the proposed method outperforms all those baselines. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm are also analyzed under certain conditions.

**Clarity**

The description of the proposed method is clear. The experiment section is generally clearly written and well-organized.

**Significance**

The proposed method shows significant improvements compared with various baselines. Therefore, it has the potential to be adopted by other researchers and practitioners, advancing the state of the art in learning from noisy labels.

Weaknesses:
**Originality**

- In Lines 120-125, the residual term $\mathbf{r}(x)$ is introduced. However, it is not clear to me how novel it is compared to the previous work [57,25,30,31]. The authors should elaborate on this point.
- I can see why residual term $\mathbf{r}(x)$ might be useful, but why is it modeled as in the form in Line 124? The motivation should be explained.

**Quality**

- The convergence analysis seems very restrictive to me because it requires too many assumptions (Lines 171-175, Lines 183-186, and Appendix B.2).
- The generalization analysis (Theorem 3.2) is w.r.t. the training loss (surrogate loss) under the noisy distribution $\tilde{\mathbb D}$, but the test accuracy under the clean distribution $\mathbb D$ is what people really care about. Is it possible to prove any consistency guarantees?
- Knowledge of the ground truth $R_*$ is required to derive Theorem 3.2, but we do not know $R_*$ in practice.
- Section 3 is not clearly written, and I found it hard to follow and assess its correctness (see below).

**Clarity**

Section 3 is not clearly written, and I found it hard to follow and assess its correctness. Specifically:

- In Lines 173-174, is $R_{\ast}$ assumed to be $U_{\ast} \odot U_{\ast} - V_{\ast} \odot V_{\ast}$?
- In Lines 203-205, $\mathcal F$ is a set of loss functions. What is the exact meaning of ""about the data""? Why is $R$ not considered in $\mathcal F$? Is a fixed $R$ being used here?
- In Lines 206-207, what is the definition of $\epsilon$-cover?
- In Lines 207-208, what are the mathematical definitions of the ""average losses""?
- In Lines 210-213, it seems that here $R_{\ast}$ is fixed. Yet, it does not make sense to me because $R$ should depend on the transition matrix $T$ and the distributions $\mathbb D$ and $\tilde{\mathbb D}$. What is ""ground truth"" w.r.t. here?

**Significance**

The significance of the proposed method could be further enhanced through a more rigorous theoretical analysis (see above).

Limitations:
I did not see where the authors discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In noisy label learning problem, noise is often characterized by confusion matrix. In contrast to instance-independent noise, this work considers a setting where confusion matrices could be different for different samples. Under this setting, the authors proposed to use a global confusion matrix shared by all instances and a residual term for each instance to account for the different between instance-dependent confusion matrix and the global confusion matrix. For learning, an MLE loss combined with an implicit sparsity regularizer is optimized.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work is tackling a challenging yet important setting in noisy label learning. The proposed model is a natural and intuitive extension to instance-independent confusion matrix as it allows for a wider range of noise. The proposed algorithm (TMR) is simple to implement, and demonstrated to be effective under synthetic and real-data experiments.

Weaknesses:
- Motivation for the use of sparsity regularizer is not clear. The authors does not discuss much on why the vector $\textbf{r}$, or matrix $\textbf{R}$ in their model should be sparse. They did point out in page 3, line 117 that the difference when using the global transition matrix and the instance-dependent transition matrix should be small. However, that is not sufficient to promote sparsity, as any other $l$-p ($l>1$) norm could have promoted that goal.
- The use of implicit regularizer is also not clear. And more importantly, since the output of $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$ is a probability vector, $\textbf{r}$(X) has to satisfy certain constraints. This is not discussed nor specified anywhere in the paper. And hence it is questionable how the parameterization of $\textbf{r}(X)$ could produce valid probability vector $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$.
- The analysis might contain flaw. Equation (14) is incorrect: $\widetilde{\textbf{Y}}$ is a matrix composing of one-hot vectors while the RHS is a matrix composing of probability vectors. The two are not equal in general. This equation seems to be the key step to motivate the objective to be analyzed in (17), and also the key step in the proof of Theorem 3.1 (page 15, line 534). 
- The analysis is based on linear model which is not very realistic.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
HbV5vRJMOY;"REVIEW 
Summary:
This paper tried to use Matryoshka mechanism to assign tokens to different experts.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Seems like the proposed approach can learn some effective components in images, shown in visualizations .
2. The empirical performance is good compared to mavit.

Weaknesses:
1. Why there is not comparison with FF which owns a single scale? What I mean is, starting for MaViT, and continue finetune the model with the same amount of compute .
2. Why is imagenet accuracy so low compared to a normal ViT? Such as Figure 3.
3. Why is there no experiments on imagenet1k?

Limitations:
1. Important Tokens: is there a way to measure it?
2. The method description part is not clear to me.
3. Authors should also consider comparing with TokenMerging series of work, since the goal is the same, which is to save compute from the token level.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper builds on the Mixture-of-Experts paradigm for vision transformer, and adds a hierarchical aspect to it, yielding a Mixture of Nested Experts (**MoNE**). More specifically, experts are defined as subsets of nested channels in the Feed Forward Networks, such that each expert has a different compute cost, and the larger one correspond to the full original model. Nevertheless, some operations are always performed at the full model dimension, after padding the tokens that went through smaller experts as needed: The $(QK^T)V$ operation in the self-attention, the layer norms and the residual connections. In addition, a dynamic budget allocation heuristic is proposed to adapted the classical (uniform) load balancing loss to the scenarios of experts with different capacities.
The proposed method is then evaluated on image and video classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The routing strategy is learned in the first transformer layer and propagated to all subsequent layers. This can be better from a hardware perspective to know early on which channels need to be turned on/off.
- Having experts with different compute costs/""capacity"" is well motivated and reasonable to improve the accuracy/efficiency trade-off.
- Good ablation experiments on the location of the router

Weaknesses:
* **Fixed number of experts:** It is not clear to me why `MoNE` uses a fixed number of nested blocks: It is very natural for `MoEs`, since experts are entirely separate blocks, however in the case of `MoNE`, the router could act directly on the channel and output a binary decision (essentially, having as many experts as dimension $D$). I would expect this to have an impact on the load balancing loss, but it would given much more flexibility to distribute the capacity across experts. More generally, there is a large literature on dynamic sparsity (e.g. *(1)* for recent references) which seems closely related to the design of nested experts and is not mentioned here.

* **Baselines:**  The experiment sections lacks baselines, in particular ones which also consider a for dynamic routing (e.g. there are none for the video classification task). For instance:
	  * Simple mixture of experts. The only baseline considered in the paper is MoD. Not only that the paper compared to *the best reported MoD configuration* (line 233); However MoD experiments seem to have been run on very different datasets, so it is not clear why the best configuration would transpose here.
	  * Token pruning; In particular for  image classification, there is a very large body of literature on token pruning (A-ViT, E-Vit, etc.). Since the only tasks considered in the experiments are classification, pruning tokens should be a valid dynamic computing baseline.
	In addition, some of the metrics reported in the paper could be improved:
	  * As far as I know, ImageNet-21k evaluation is not very standard as there is no official train-validation split.
	  * FLOPs are not enough to show the method's efficiency and they should be accompanied by real latency. For instance, they do not take into account extra padding operations.

* **Relevance of the dynamic budget allocation:** Table 1 shows that a simple uniform allocation for the load balancing loss performs as well as the heuristic proposed in Section 4.3. Therefore it is not clear to me whether the proposed dynamic budget allocation really has a significant impact



#### references
  * (1) Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time, Liu et al, 2023

Limitations:
There is  little discussion about some limitations e.g. (i) why rely on a fixed number of experts or (ii) what are the real latency of the method. The only limitation mentioned is that it would be hard to adapt this scheme to decoder-only LLMs, which seems a bit out of scope with the paper itself.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper present a method to select nested portions of a transformer network, using a MoE router-expert assignment method where each expert is a progressively larger slicing of a single underlying model.  A capacity budget determines how many tokens can go to each expert, while a router network scores experts for each token so that the most important tokens (in the sense of benefitting from the larger computations) go to the larger model slices.  Furthermore, when trained using random budget selection, the model can be dynamically scaled to different cost-accuracy tradeoffs at inference time.  The model is evaluated on image and video classification tasks using imnet21k, K400 and SSv2, with excellent flops-accuracy performance compared to baselines, and a set of ablations show the impact of different design choices for router placement.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a well-written paper that describes an in interesting and effective idea.  The approach is evaluated convincingly on three datasets for image and video classification tasks, and in enough settings to profile its performance characteristics and anecdotal visualizations of its behavior.

Weaknesses:
There are a few points that I think were under-explained (see questions below).  In particular the descriptions of the projections around the MLP and SA were a little terse, and I'm still not sure exactly which operations are in the subspaces vs the full dimensional space.

The connection between token redundancy and the operation of the model is also a little tenuous, though likely (see below as well), and while FLOPs are measured explicitly, the impact on both real computation savings and runtime may depend on hardware and distributed computation implementations, which I didn't see mentioned.

Limitations:
addressed in the final discussion section

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the concept of Mixture of Nested Experts (MoNE), which utilizes a nested architecture to process visual tokens more efficiently in visual media like images and videos. MoNE aims to leverage redundancy in data, choosing experts in a priority order to process visual tokens, thereby achieving substantial compute savings while maintaining performance. The approach, demonstrated with MoNE's algorithms like MoNE-21K and MoNE-4K, optimizes adaptive processing on standard image and video datasets, significantly reducing computational demands while using a single trained model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Overall, the paper is well-motivated and the idea of combining nested structers with MoE is very interesting; The problem of information redundency do exist in image or video classification tasks.

2. Empirically, the MoNE models attain very competitive results with less FLOPs and parameters, which could support their theoretical analysis in vision information redundency.

3. The paper is well-written and easy to understand. The method is simple and easy to follow.

Weaknesses:
1. My main concerns are in the actual inference speed of your models. The experimental results reported in the paper focus on comparing FLOPs rather than throughput. Could the authors demonstrate some direct comparisons of training/inference speed?

2. The EPR algorithm (Algorithm 1) proposed in the paper seems to implement routing operations through some loops. Is this process actually implemented through loops or parallel computing? Does this operation take up a lot of inference time?

3. I'm also concerning whether the method can benefit dense prediction tasks such as image segmentation. As in those tasks there are less information redundency, will MoNE result in performance degradation?

Limitations:
Limitations are discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Mixture of Nested Experts (MoNE) framework. MoNE is built on top of the MatFormer architecture which utilizes a nested architecture where smaller, less computationally expensive sub-models are nested within larger models. Similar to MatFromer, MoNE uses structured slices of the model weights (the experts) to process information hierarchically. While MatFormer focuses on obtaining many models on the optimal loss-vs-compute curve using the mix'n'match algorithm (during inference), MoNE learns to dynamically route visual tokens to an appropriate expert based on their significance and the computational budget.

The results show that MoNE achieves a favorable accuracy-efficiency trade-off curve for image and video classification tasks compared to the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow
- The extension of the MatFormer architecture into an MoE architecture with dynamic routing seems quite intuitive. The Matformer architecture is inherently composed of nested subnetworks and enabling dynamic routing in this architecture makes sense.
- Experiments on Image and Video Classification tasks shows MoNE outperforms the baselines.

Weaknesses:
- Baseline comparisons:  It is conceivable that the dynamic routing approach can outperform the mix'n'match policy of MatFormer during inference as smaller sub-networks can focus on simpler/uninformative tokens while the larger experts focus on more complex/informative tokens. Nonetheless, to fully evaluate the potential of the proposed MoE framework, it would be beneficial to compare it with other MoE architectures, especially those like Sparse Vision-MoE.

- The advantage of having a nested set of experts compared to non-overlapping experts remains unclear. The qualitative results suggest that MoNE utilizes the full model for the most critical tokens, while less informative tokens are processed by smaller sub-networks. Given this, the paper could be strengthened by comparing MoNE to other dynamic token processing methods such as AdaViT [1], SVitt [2], and A-ViT [3], which have significantly enhanced computational efficiency in image and video recognition tasks by learning to dynamically skip tokens.

1. Meng, Lingchen, et al. ""Adavit: Adaptive vision transformers for efficient image recognition."" CVPR 2022.
2. Li, Yi, et al. ""Svitt: Temporal learning of sparse video-text transformers."" CVPR 2023.
3. Yin, Hongxu, et al. ""A-vit: Adaptive tokens for efficient vision transformer."" CVPR 2022.

- Specialization of Experts in MoE Architectures: A primary focus in designing MoE architectures is the specialization of experts to address different aspects of the data distribution. Typically, having non-overlapping diverse experts is desirable. In contrast, MoNE and MatFormer consist of partially overlapping parameters, suggesting that the implicit learning bias in these architectures may differ significantly from that in conventional MoEs. Investigating the differences between MoNE experts and conventional sparse MoE architectures could greatly enhance the reader's understanding of the proposed MoE framework's behavior.

- Worse results when increasing the number of routers: Standard MoE architectures employ MoE layers and routers in every(other) layer, enhancing the model's flexibility and expressiveness. However, it is concerning that increasing the number of routers leads to worse performance in MoNE. This suggests potential optimization challenges in this MoE model compared to standard MoEs. Notably, the best results are obtained when the router is placed at the very first layer, indicating that the dynamic decision-making for all layers is based on relatively simple cues at local feature levels. Further exploration into why the performance deteriorates with more routers compared to traditional MoEs, which typically see improved results, would be insightful.

Limitations:
The authors mention one of the limitation of their work regarding extension to sequence modeling tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
VTcGX5HO19;"REVIEW 
Summary:
This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO). BKTF approximates the objective function using a low-rank tensor factorization, with Gaussian process priors placed on the latent factors to capture dependencies and enable uncertainty quantification. The key advantages are the ability to handle complex functions that are non-stationary and non-separable, and the sharing of information across dimensions to enable a more global search compared to standard GP models with local kernels. Inference is performed via MCMC sampling. Experiments on benchmark optimization functions and hyperparameter tuning tasks demonstrate improved performance over GP-based BO, especially when the initial sample size and evaluation budget are limited.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The BKTF surrogate is a novel and creative approach to extend BO to handle more complex objective functions. Modeling the objective as a tensor factorization with GP priors on the factors is an elegant way to introduce non-stationarity and multi-scale correlations in a principled Bayesian framework.

The method is grounded in a clear mathematical framework, with full details of the model specification and MCMC-based inference procedure provided. Positioning BKTF as a type of deep GP offers useful insight into its expressive power.

The paper includes extensive experiments on a range of synthetic functions and real-world hyperparameter tuning tasks. The results convincingly demonstrate the advantages of BKTF over standard GP-BO in terms of optimization performance and sample efficiency, especially in the realistic setting of a very limited evaluation budget.

The paper is clearly written, with the methodology explained in detail and the experimental setup and results presented thoroughly. The authors also discuss the limitations of their work, including the scalability challenges and the restriction to a grid-based search space.

Weaknesses:
The main weakness is that the proposed BKTF method is only compared against basic GP-based BO with standard kernels. To fully demonstrate the advantage of the BKTF surrogate, comparisons should be made to more advanced GP models such as deep kernel learning, deep GPs, and other scalable GP variants. Without these comparisons, it's difficult to assess how much of the performance gain is due to the specific BKTF approach vs. simply being a more flexible GP.

The BKTF model bears significant similarities to existing works on scalable GPs, such as ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also use inducing points on grids to obtain tractable approximations. The relationship of BKTF to these methods is not discussed, and it's unclear whether BKTF provides any substantial advantages over these existing scalable GP approaches.

The experiments on the synthetic test functions are somewhat limited in their dimensionality (only up to 10d). Given that BO is most useful for optimizing high-dimensional black-box functions, testing on some higher-dimensional benchmarks would be valuable. The scalability of BKTF as the dimensionality and grid size increase is not fully investigated.

The MCMC inference procedure may become prohibitively slow for high-dimensional spaces or large evaluation budgets. The paper does not report the wall-clock time of the experiments, which makes it hard to assess the computational feasibility of BKTF in practice, especially compared to alternative approaches.

For the hyperparameter tuning experiments in Section 5.2, the strong performance of BKTF with very few iterations seems counterintuitive and is not fully explained, since the BKTF model has many parameters and would be expected to require a substantial amount of data to train effectively. This seems to contradict the results on the synthetic test functions, where GP-EI performs equally well in the first few iterations.

Limitations:
The relationship of BKTF to existing scalable GP methods is not thoroughly discussed. The paper does not make clear how BKTF differs from or improves upon approaches like ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also exploit grid structure. A more thorough comparison is needed to justify BKTF as a novel contribution.

The cubic scaling of the covariance matrix computations in the number of grid points, which could make BKTF infeasible for high-dimensional or very fine-grained grids. Some discussion of potential ways to scale up BKTF, e.g., by exploiting grid structure or using sparse approximations, would be valuable.

The fact that BKTF relies heavily on a sensible grid specification, and may fail badly if the grid is poorly chosen. Some experiments showing the sensitivity of the results to the grid choice would help assess this risk.

The lack of comparison to a wider range of flexible surrogate models beyond standard GPs, including more sophisticated GP models as well as other probabilistic regression approaches. The current experiments are not sufficient to establish BKTF as the best choice for BO in practice.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate for Bayesian optimization (BO). This model uses a CP decomposition to define a set of random basis functions drawn from a GP prior. These latent functions are then weighted by another set of random variables. This defines a probabilistic model that can perform uncertainty quantification, which can then be trained by performing MCMC sampling over the hyperparameters. The acquisition function is calculated by computing the first and second moments of the samples, and calculating the upper confidence bound (UCB).

This procedure results in a non-stationary, non-separable model that can capture complex functions. This model is tested on a range of synthetic and ML hyperparameter BO benchmarks, each of which are non-separable functions that exhibit high degrees of interaction between variables.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**
This paper is the first to use kernelized tensors in the Bayesian optimization setting. Gaussian processes are a common surrogate in this setting, and this proposes an alternate surrogate with good arguments for its adoption.

**Quality:**
The paper demonstrates the performance of the BKTF surrogate on a wide range of benchmarks. The performance is strong, justifying the claims in the paper. Further, many supplementary results are provided to further investigate the modelling decisions made.

**Clarity:**
The explanation of the BKTF model and fitting process is explained well, providing a clear description of the model. Specifically, the 2D example in Figure 1 provides a clear motivation for modelling functions that have a high degree of interaction.

**Significance:**
The proposed method is a strong surrogate for Bayesian optimization, one that can model functions with mixed input spaces and high degrees of interaction between variables. This presents a good addition to the range of available surrogates in the field.

Weaknesses:
**Comparison to other methods:**
The authors claim that a strength of their method is that their method is non-stationary and non-separable. However, I do not feel that the paper sufficiently justifies this explanation for the model's success, for the following reasons:

I do not believe that the GP ARD is separable. The authors present that the ARD kernel is the product of $D$ independent kernels in 3.2. However, this is not how these kernels are implemented. Instead, (specifically for stationary kernels) they are expressed as functions of weighted distance [5], where $d=\sqrt{\sum_{d=1}^D (x_d - x'_d)^2/l_d}$. These kernels cannot be written as products of 1-dimensional kernels, and are non-separable. The authors also suggest that these experience the curse of dimensionality as the dimension increases, but this effect is not severe in the low-dimension regime studied in the paper.

The authors claim that the flaw of using additive GP kernels is that they:
> [require] strong prior knowledge to determine the proper interactions and [involve] a large number of kernel hyperparameters to learn

I do not find this argument convincing. For low dimensional problems, additive kernels can include all interactions up to order D, and learn the weighting of each order of interaction [1]. In fact, I believe that the number of kernel hyperparameters is of a similar order to the BKTF method. Additive kernels also work well with MAP estimation of the hyperparameters (especially for the low-dimensional problems investigated here), although I do not see why MCMC could not also be used for additive kernels if the number of kernel hyperparameters is considered too large. The additive GP baseline should therefore be order D, not order 1, to provide a fair comparison against existing non-separable modelling methods.

It is unclear why the authors compare to SaaSBO, a technique designed for high dimensional (D>100) spaces that places a strong prior on the lengthscales of the inputs (with the prior belief that few of the inputs are important, which is not the case for the test functions used).

Moreover, the authors do not provide comparisons against methods that are designed for non-stationary settings e.g. [1, 2]. 

(Minor comment) I would like to see MCMC over the GP kernel hyperparameters to obtain a fully Bayesian acquisition function, as in [6].

**Hyperparameter choices with BKTF:**
I disagree with the authors that BKTF is robust to rank misspecification. Figure 13 shows that the CRPS doesn't converge until 30 observations for the rank 2 model. Including the initial 30 datapoints, this model is fit on 60 datapoints, for a 2D problem - the GP model provides a much better fit to the data for <60 datapoints.  Since these models are used in a BO setting where few datapoints is common, this behaviour suggests that rank *is* an important parameter, and further that the model does not fit well with few datapoints. 

I would also want to see this experiment repeated on higher dimension test problems, to see if the problem is exacerbated in high dimensions. Moreover, the CRPS of the rank 2 model seems to converge only to that of the GP models, suggesting the performance over GP may not be solely due to modeling quality: this should be further investigated.

**Experiments:**
It is unclear how the authors handle categorical inputs for the baselines. The authors should be using methods designed for mixed input spaces, such as [4].

Following from the discussion on CRPS, this paper would benefit from some experiments on the quality of the fit of the model.

(Minor comment) It would be interesting to see the (arguably more popular) Matern 5/2 kernel compared to the 3/2 kernel, especially for the GP baselines.


[1] Snoek et al. ""Input Warping for Bayesian Optimization of Non-stationary Functions""    
[2] Eriksson et al. ""Scalable Global Optimization via Local Bayesian Optimization""    
[3] Duvenaud et al. ""Additive Gaussian Processes""    
[4] Ru et al. ""Bayesian optimisation over multiple continuous and categorical inputs""    
[5] Williams and Rasmussen. ""Gaussian Processes for Machine Learning""    
[6] Snoek et al. ""Practical Bayesian Optimization of Machine Learning Algorithms""

Limitations:
The authors provide good discussion on the limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new surrogate model for Bayesian optimization, based on a functional tensor factorization. The approach discretizes the model to a pre-specified grid and uses MCMC sampling for inference. Bayesian optimization is carried out by selecting promising points from the pre-specified grid, as quantified by an acquisition function. The paper includes experimental results on synthetic functions as well as ML hyper-parameter tuning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Nice experimental results on the ML hyper-parameter tuning task.
- Generally well written paper.

Weaknesses:
- Optimization method appears to be restricted to an a-priori defined grid of possible candidate points.

- I am concerned about reproducibility of the benchmarks, as the code submission not complete, e.g. appears to be missing implementations of basic functions like baseline GP fitting (`fitrgp`) and predicting (`predict`), the additive GP model mentioned in line 274 of the paper, the continuous optimization of the acquisition function. 

- Grid-based GP-UCB, GP-EI baselines are missing for ML tuning tasks (Figure 3). This is notable, because these experiments contain discrete variables, which requires a rounding operation if they are relaxed to a continuous space, as is done by the paper. This rounding operation is likely to degrade the performance of ""continuous"" GP-UCB and GP-EI, as it will can be prone to sampling ""between"" integers it has already seen, reducing its sample efficiency.

Limitations:
> A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

An important limitation to highlight here once more that it goes from ""not challenging"" to prohibitively expensive as the dimension increases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for Bayesian optimization where the prior is a low-rank sum of tensor products of GPs. An MCMC scheme is developed for approximate updating and UCB sampling. Several experiments show strong performance relative to baselines on artificial function optimization and ML hyperparam tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a seemingly new approach to BO with a more flexible type of prior, which shows good empirical performance.

Weaknesses:
It's not clear what is new relative to previous BKTF papers [10,11], other than the scheme for using UQ in UCB sampling.

The clearest potential drawbacks to the approach are the memory and compute costs. These should be reported for the experiments.

Limitations:
The paper motivates the approach in part because standard methods assume the generating process is stationary, but the BKTF is also stationary. It’s nonstationary only after conditioning on values of $g$.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEnazjpwOx;"REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Mdd3f8Cui8;"REVIEW 
Summary:
This paper proposes a framework to augment latent features from observed features, with the help of LLM. They frame the problem as a text-to-text reasoning problem.  The method can be adapted to different domains easily.  The method is also validated with a real world dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the presentation and logic flow are smooth and clear.  The methodology is also reasonable to me. And their experiments also validate the effectiveness of their method.

Weaknesses:
The key concern for me is that when using the LLM for inference and text generation, I worry about the social bias and fairness of the problem. Some research has shown that LLM is still biased in some sense, can the author conduct some evaluation on whether the latent feature is biased towards some sensitive attributes like race, gender, etc?

The other thing is that I wonder how much human labor effort and expert labor effort will be needed to have the latent features. 

Typo in line 203

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a unique form of LLM data-augmentation that attempts to generate informative latent variables to improve downstream tasks. They do this by transforming the latent feature mining task into a text-to-text propositional reasoning task.
Validation is performed with a case study in the criminal justice system and latent features align well with ground truth labels + significantly enhance downstream classifier performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Clear, well written paper with descriptive diagrams
- Using LLMs to infer latent space in this way seems to be a novel idea

Weaknesses:
- Type on line 203: ""whic serve""
- Potential for LLM biases in the latent variable finding. E.g. Marijuana usage does not necessarily require Substance Abuse Treatment.
- Lack of evaluation on multiple datasets/domains and no publically released code
- Lack of ablations exploring generalizability with x\% features removed

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a framework that uses LLMs to improve predictive modeling by augmenting observed features with inferred latent features. This approach transforms the latent feature mining task into a text-to-text propositional reasoning task, enabling LLMs to infer unobserved yet crucial factors from available data. The framework is tested through a case study in the criminal justice system, demonstrating improved accuracy in scenarios where collected features are weakly correlated with outcomes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It addresses the challenge of limited data availability by leveraging LLMs to infer latent features, to improve predictive modeling. 

2. The approach of transforming latent feature mining into a text-to-text propositional reasoning task is interesting. 

3. The validation on criminal justice data, shows potential for broader applications. The method's generalizability across different domains with minimal customization is a significant advantage, and the reduced need for extensive human-annotated training data makes it practical and scalable.

Weaknesses:
1. The paper does not adequately address how to measure the impact of errors introduced by the LLM-based solution on predicted outcomes, nor does it provide uncertainty estimates. This process surely introduces errors, as with any ML-based solution. How do we measure its effect on predicted outcomes, including uncertainty estimates? I suspect more labeled data would be needed to assess this properly (see Egami et al. @ NeurIPS 2023).

2. It is unclear whether the approach should be viewed as a form of dimensionality reduction (based on existing features) or if it extrapolates information not present in the original data. This ambiguity raises concerns about potential bias amplification. For instance, Figure 1 shows deductions made by the model that are not clearly supported by the evidence, suggesting that the method might be amplifying existing biases rather than mitigating them.

3. Connecting to the previous point, the method's ability to learn latent information that is causally predictive of the outcome, as opposed to relying on spurious correlations, remains uncertain. Conducting an out-of-distribution test, where the characteristics of individuals differ from the training data, would be crucial in evaluating the model's generalizability and causal inference capabilities. 

4. The rationale behind not using all available data directly in the LLM for prediction is not well-justified. Directly prompting the LLM with the full data might provide more accurate predictions without the need for dimensionality reduction.

Limitations:
The authors have acknowledged limitations of their work, particularly in addressing the ethical concerns associated with data collection and the need for privacy.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper used large language models to infer latent variables that are important for downstream prediction tasks to augment the existing models. In particular, the author demonstrated the use of the proposal on a criminal justice system use case, in which the LLM-mined-latent features significantly boost the prediction performance.

Overall the paper presents an interesting question and how LLM could help mining for latent features, but I have several questions regarding 1. the generalizability of the proposed method; 2. Appropriate combinations/baseline methods; and 3. The potential ethical implications of this method. I will detail these points in the strength/weakness sections below.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. On the outcome prediction results, using the latent feature seems to have boosted the performance by 7-10%, a big margin.
2. The proposed framework brings a level of formalism to the current crowded LLM for (social) science applications/work, including the text-to-text proposition work.

Weaknesses:
1. Despite the general initial framing in Section 3, it was not very clear how generalizable results from Sections 4-6 are — this includes not only the COT and the prompts used, but more critically the selection of what kind of latent features we are including. 
2. The current work does not seem to go into depth about what kind of latent features are LLM particularly good at constructing and which ones are particularly “bad” (eg subject to the most bias and systematic over-or-under-prediction). I think this is particularly relevant for social science applications where many of the categories and features are more of a “construct” and often qualitative in nature.
3. Have the authors compared the results by using text embeddings of the descriptions as an input feature? It’s interesting that the fine-tuning strategy is necessary for good performance, which seems to suggest that learning the intermediate classification rule is important since direct manipulation of natural language yields less impressive  results.

Limitations:
Mentioned above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TMlGQw7EbC;"REVIEW 
Summary:
This paper proposed a differentiable DAG learning method using the log-likelihood loss and the minimax concave penalty (MCP). The author proved that under such construction, the minimizer of the loss identifies the sparsest graph (i.e. it has the minimal number of edges) which can generate the observational distribution. When faithfulness is further assumed, the identified graph is equivalent to the ground truth graph. The theorems are valid for linear Gaussian DAGs and general nonlinear SEMs where the induced distribution is parametric. The proposed method is validated by extensive experiments involving various dimensions and graph structures.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written and easy to follow. 

The theoretical contribution is solid, demonstrating that a broad class of penalties (SCAD, MCD, quasi-MCD), along with the log-likelihood loss, identify the desired graph structure.

The simulations are well-designed and illustrate the outperformance of the proposed method.

Weaknesses:
The method can be regarded as a direct combination of several works. For example, the log-likelihood loss was also proposed in GOLEM [43]. The MCD penalty was first proposed in [72].

When applied to nonlinear models, the author used $\frac{1}{2n}\sum_{i=1}^d\log\left(||x_i-\hat f_i(X)||^2\right)$ as the log-likelihood loss. This seems only valid for homogeneous Gaussian noise where the variance in the denominator can be canceled out. However, in the experimental setting, the noise variance is heteroscedastic, making the likelihood improper.

Real-world datasets can be added to validate the proposed method.

Code is not available.

Some minor flaws in the presentation:
+ The conclusion section is missing.
+ Section D.3.2, time spent on the simulation. The plots are about SHD rather than about the running time.
+  [72] in the reference should be cited correctly.

Limitations:
The method assumes a known likelihood function, which can be impractical when the noise distribution or the functional structure is unknown.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces new identifiability results (MEC) based on maximum likelihood estimation complemented with sparsity regularization (quasi-MCP) for both a Gaussian linear model and a more general, potentially nonlinear models, under the very standard faithfulness assumption. The paper contains a theoretical analysis showing the score function is invariant to rescaling (in the Gaussian linear case only) and provide experiments showing the approach compares favorably to other baselines (in the Gaussian linear case only).

**Review summary:**
This paper reads very nicely and some of its proposals are new and interesting (scale-invariant result and the usage of the quasi-MCP to get MEC identifiability), but it missed a very relevant work [a] with very similar contributions (which significantly reduce the novelty factor of this manuscript) and a more complete set of experiments (see below). For these reasons, I believe this work is not ready yet for publication. That being said, I think it could get accepted at another conference by
- rewriting the paper in a more transparent way, especially by contrasting with the contributions of [b] (and changing the title); 
- highlighting what are the actual contributions (scale-invariant result + MEC identifiability via quasi-MCP regularizer); and 
- providing convincing experiments with nonlinear models.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- This paper is extremely well written and easy to follow. The notation is always well introduced with kind reminders when less standard notation is used. 
- Overall the paper feels quite pedagogical.
- I believe the theoretical analysis of scale-invariance, although simple, is novel and very interesting, as it addresses problems raised with existing approaches regarding standardization.
- I think some aspect of the theoretical results is interesting, like the proof that the quasi-MCP regularizer can be used to get identifiability of the MEC. However, I believe very similar results have been shown previously in the literature (omitted from the related work) which seriously limit the novelty factor of this work.

Weaknesses:
**Important prior work omitted which seriously limits novelty**

The contribution is motivated by the need to consider more general score functions beyond the mean-squared error used in NOTEARS. The authors missed the work of [a] which proves identifiability for very general likelihood functions, including universal approximators (thanks to normalizing flows). The result of [a] applies to cases where interventions are observed, but covers also the special case without interventions, in which case the result guarantees identifiability of the MEC under the faithfulness assumption. Interestingly, [a] also requires the coefficient of the regularizer to be “small enough”, as is also the case in this manuscript. A key distinction with [a] is that this manuscript is analyzing a differentiable nonconvex sparsity regularizer as opposed to an L0 regularizer, which is not suitable for gradient-descent (in practice, [a] uses gumbel-sigmoid masks and regularizes their probability of being one). Am I missing other key distinctions? Showing that the quasi-MCP regularizer can yield MEC identification is new AFAIK and interesting IMO, but this combined with the simple scale-invariance results makes for a rather limited contribution...

I believe the title reflects the lack of knowledge of the existing literature on continuous structure learning. The title is too general. It sounds like the paper introduces likelihood-based differentiable structure learning, but that’s clearly not the case. [a] is a clear example with very similar theory (although not exactly encompassing this manuscript) as well as [b] and [c] (although without identifiability theory).

**Experiments investigate only the linear case**

The paper is largely motivated by the need to go beyond linear Gaussian score functions, but the experiments fall completely short of that promise, as only the Gaussian linear case is investigated. In contrast, [a] train neural network based architecture, with similar theoretical guarantees. More experiments are needed to confirm that the quasi-MPC regularizer transfers to nonlinear setting. 

**Limitations of the theory:**

Assumption A: Assuming that the cardinality of equivalence of parameters is finite feels like a strong assumption. For instance, if you’d like to parameterize the conditional using neural networks, this assumption wouldn’t be satisfied since most architecture have infinitely many equivalent parameterization (think of ReLU activation where you can rescale the output of a neuron and undo this rescaling in the following layer). The result of [a] does not make this assumption despite their very similar setup. Why is it needed here? Also, can you give a concrete example of function B(\psi) that is Lipschitz.

Assumption B: The same example with ReLU neural networks is a counterexample to this assumption. Indeed, because of this rescaling, the set of parameters yielding the same network is unbounded. Excluding NN feels limiting, especially since existing results can cover this case.

**Unclear limit statements in Theorem 1 to 4**

The usage of the limit symbol “->” is a bit strange. Usually, we say that a_n -> b as n -> \infty, but here the result says a_n = b as n -> \infty. What does the latter even mean? Does that mean there exists a large enough N such that a_n = b for all n > N? I don’t think it’s true anyway. Why not presenting the result directly for the population likelihood (i.e. with \ell instead of \ell_n), as was done in [a]? Skimming the proof very quickly suggests that’s what was proved anyway. Note that this is different from showing consistency of the procedure, i.e. that your estimator converges to the right MEC as the number of samples grows (I suspect the latter will be more difficult to show)

Minor:
- It would be nice to have a small plot of the quasi-MCP regularizer.
- Typo in Lemma 3 in appendix? I think an \mathcal{M} should be replaced by a \mathcal{G}, no?
- Text is absurdly small in Figure 1 e.g.. Please fix this.
- Is the sparsest Markov representation always unique? The phrasing used suggests it is, but it might be worth expliciting.

[a] P. Brouillard, S. Lachapelle, A. Lacoste, S. Lacoste-Julien, and A. Drouin. Differentiable causal discovery from interventional data. In Advances in Neural Information Processing Systems, 2020.

[b] X. Zheng, C. Dan, B. Aragam, P. Ravikumar, and E. Xing. Learning sparse nonparametric dags. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, 2020.

[c] S. Lachapelle, P. Brouillard, T. Deleu, and S. Lacoste-Julien. Gradient-based neural DAG
learning. In Proceedings of the 8th International Conference on Learning Representations,
2020.

Limitations:
This paper lacks a conclusion/discussion, so the limitations of the approach are not addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors analyze a framework of sparsity-regularized maximum likelihood learning under the NOTEARS constraint for score-based causal discovery. Drawing from the sparsest permutations principle (a hybrid method), they show that a sparsity regularized likelihood objective is able to recover an element of the MEC even under structural non-identifiability of the SCM. The (non-identifiable) linear Gaussian setting is analyzed in detail, where it is shown that the MCP and SCAD penalties are able to recover the sparsest graph with appropriate hyperparameter settings. The authors then extend this to general likelihoods under the assumption that the (parameter) non-identifiability class (at the ground truth) is finite and other regularity conditions. For the linear-Gaussian case, the authors also prove scale invariance of the structure obtained from the method, addressing concerns of varsortability that similar methods are susceptible to. Finally, an experimental study is conducted on simulated linear and nonlinear ground truths.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
I advocate for the acceptance of this paper based on two simple dimensions:

- (Significance and Motivation): Structural identifiability is a near-universal, yet entirely convenience-driven assumption when using likelihood-based scores for causal discovery. This work theoretically justifies NOTEARS-type approaches under structural non-identifiability, which broadens the class of applicable likelihood-based scores.

- (Clarity and Execution): I found the paper easy to follow, with precise mathematical notation, careful proofs and comprehensive analysis of the proposed framework.

Weaknesses:
However, I do think the paper falls somewhat short in a few (fixable) areas. Actionable questions are __bolded__, and I may raise my score if these points are clarified. 

- The section on scale invariance seems out of place and somewhat weaker than the rest of the paper. It's clear that the Gaussian likelihood is scale invariant, and structure invariance is not a terribly surprising conclusion of the regularized objective also. __Unless the authors can clarify the contribution of this section, I feel like this is better off stated as a short paragraph or remark.__
- The authors state that the finite parameter equivalence class assumption in the general case is ""relatively mild"", and that it is satisfied by a range of models (""most exponential families""). I'm not convinced that this is the case. First, typically when statistical models with continuous parameter spaces are non-identifiable, the equivalence class is usually infinite. For example, I believe exponential families in canonical parametrization are either full-rank, where the parameter is identifiable (and hence not relevant to the motivation of the paper), or non-identifiable up to entire subspaces of the parameter space (Theorem 1 of [1]), which are infinite. __Could the authors clarify what they mean on l297-298?__
- The authors do not provide any examples, or references, of non-Gaussian log-likelihoods scores that would be useful for causal discovery. The non-linear example in the experiments seems to use a MSE loss which is the same as non-linear NOTEARS [2], and the only improvement shown in the experiments is due to changing the optimization scheme to Adam (and adding the quasi-MCD regularizer?). __Could you provide (practical) examples of likelihoods where the theory in Section 5 hold? I.e., other non-identifiable likelihood scores that are useful for causal discovery?__. 

[1] Notes by Charles Geyer: https://www.stat.umn.edu/geyer/8053/notes/expfam.pdf

[2] ""Learning Sparse Nonparametric DAGs"", Zheng et al., AISTATS 2020.

Limitations:
The authors do not adequately discuss the limitations of the paper in my opinion (in fact, the paper abruptly ends without even a conclusion), and this is also seen in in the checklist where the justification for this section was left blank. 

I think the paper deserves a proper conclusion and discussion of the (potential) limitations of Assumption A1 in Section 5. __I do not think this is a tough ask, so I may lower my score if this remains unaddressed in the rebuttal.__

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
up0qqbdeQu;"REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
paYwtPBpyZ;"REVIEW 
Summary:
The authors proposed FOLDFLOW++, which is built on top of FOLDFLOW [ICLR 2024]. It adds a joint structure and sequence representation and a transformer-based geometric decoder, enabling folding and inpainting applications.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The tasks the authors are attempting to solve seem very interesting and important for drug discovery.

Weaknesses:
Considering that the theoretical novelty of the paper is somewhat limited and its main contribution lies in introducing certain architectures, the experiments conducted for the new tasks (other than unconditional generation) are also somewhat limited. Please note that I am not very familiar with the topic and do not know what potential experiments could be included.

Limitations:
I think the model's performance might depend on various components such as ESM-2 and the structure encoder.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new model FOLDFLOW++ for Conditional Protein Backbone Generation. It incorporates several techniques including sequence model, finetuning strategies, and high-quality synthetic structures to improve its performance on various tasks. The experimental results suggest the method achieves SOTA performance on various protein-related generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and can be used in many real-world scenarios. 

- The proposed method gains sota performance on unconditional generation, motif scaffolding, folding, fine-tuning to improve secondary structure diversity, and equilibrium conformation sampling from molecular dynamics trajectories.

- Considering the sequence embedding and ReFT is reasonable for improving the base model's performance.

Weaknesses:
- This paper has limited technical novelty. The core components are mostly proposed by previous works. 
- Fusing the well-trained sequence model(ESM) into the backbone generation method (FoldFlow) intuitively can improve the structure generation[1]. Therefore, we cannot see the insightful discussion and surprising conclusion from the paper.
- Some baselines concerning MD may be missing: EIGENFOLD[2], STR2STR[3],CONFDIFF[4].

[1] A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design

[2]EigenFold: Generative Protein Structure Prediction with Diffusion Models

[3]Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling

[4]Protein Conformation Generation via Force-Guided SE(3) Diffusion Models

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a protein generative model FoldFlow++ augmented with protein language model embeddings upon FoldFlow. The model is trained with sequence and structure information to learn embedding projections in SE3 space. Experiments on unconditional generation show a favorable performance of FoldFlow++ over SOTA method RFdiffusion. FoldFlow++ has the capability to be aligned to arbitrary awards like secondary structure diversity through reinforce finetuning, as well as the capability to motif scaffolding and conformation sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

The paper is an excellent piece of work implementing protein language model embedding-guided protein flow matching model. The design of the network is reasonable and novel. Training with half-time sequence masking introduces the capability of protein folding and design at the same time. Overall the model is carefully designed and shows wonderful protein generative modeling potential.

**Quality**

The submission is technically sound, with much of the mathematical foundations explained in the previous FoldFlow paper. Various aspects of protein generative models are tested, e.g. unconditional sampling, protein folding, motif scaffolding, conformation sampling.

**Clarity**

The paper is easy to comprehend and figures are well-designed and clear.

**Significance**

This work integrates protein language model into a protein flow matching framework to make its protein modeling and design ability more versatile. It can perform various kinds of tasks in protein design and has a great potential as a foundational model for protein researcher.

Weaknesses:
1. What are the diversity of structures for motif scaffolding task? Please include some visualization of generated structures for motif scaffolding benchmarks and statistical results.
2. Alphafold2 also has conformation sampling ability. Did you benchmark it?

Limitations:
Although FoldFlow++ has various types of protein generation capabilities, performance on some of the tasks like conformation sampling and protein folding is not impressive, though I believe further finetuning on more specific datasets can benefit the model on this.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces FoldFlow++, a sequence-conditioned SE(3)-equivariant flow matching model designed for protein structure generation. FoldFlow++ builds upon previous FoldFlowmodels by incorporating a protein language model to encode sequences, a multi-modal fusion trunk to integrate structure and sequence representations, and a geometric transformer-based decoder. The model is trained on a large dataset of both known proteins and high-quality synthetic structures, demonstrating substantial improvements over previous state-of-the-art models in terms of designability, diversity, and novelty. FoldFlow++ also excels in conditional design tasks, such as designing scaffolds for VHH nanobodies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper includes a detailed ablation study examining architectural components, different flow matching schedules, and more.
- The model surpasses previous state-of-the-art generative models in terms of designability, diversity, and the novelty of generated protein structures.
- The authors propose a large and diverse dataset, including high-quality synthetic structures, which enhances the model's generalizability and robustness.
- The paper explores several meaningful settings, such as Reinforced FineTuning, Motif Scaffolding, and Zero-shot Equilibrium Conformation Sampling.

Weaknesses:
1. The proposed pipeline appears to be a special case of Multi-Flow.
2. The model architecture remains very similar to previous work (e.g., Genie, FrameFlow/FrameDiff), leaving it unclear which specific parts of the algorithm drive the observed improvements.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
s2hA6Bz3LE;"REVIEW 
Summary:
This paper introduces the hyperspherical energy as an objective to prompt the diversity of particles in BNNs. It claims that the hyperspherical energy approach can avoid permutation invariance in traditional diversity metrics.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
I agree that the diversity of particles in BNNs is an important problem. The proposed method introduces hyperspherical energy, which can avoid permutation invariance. This invariance is mainly due to the complex structures of DNNs, which previous Bayesian Inference methods have long ignored.

Weaknesses:
The technical contribution of this paper is not strong enough. Only a new regularization term is added to the standard ensemble framework. The idea of hyperspherical energy comes from previous works, and I do not recognize significant changes from previous works.

Limitations:
1. This paper is restricted to relatively small Bayesian neural networks, and does not discuss the scalability.

2. The proposed method can improve uncertainty estimation, but fails to improve accuracy.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper “Minimizing Hyperspherical Energy for Diverse Deep Ensembling” explores the use of Centered Kernel Alignment (CKA) and Minimization of Hyperspherical Energy (MHE) in Bayesian deep learning to enhance the diversity of ensemble models and parameters generated by hypernetworks. By incorporating these techniques, the authors aim to improve uncertainty quantification in both synthetic and real-world tasks, which is quantified by measuring OOD detection performance and calibration. The key contributions include proposing CKA as an optimization objective and utilizing MHE to address the diminishing gradient issue, leading to more stable training and better performance in uncertainty estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- **Original:** The paper introduces a novel approach by combining CKA and MHE to enhance the diversity of deep learning ensembles and hypernetworks, which is a fresh perspective in Bayesian deep learning.
- **Detailed experiments:** The experimental results are comprehensive and demonstrate significant improvements in uncertainty quantification across various tasks, showing the practical effectiveness of the proposed methods.
- **Well-written:** The paper is well-structured, with clear explanations of the methods and thorough discussions of the results. The figures and tables effectively illustrate the performance improvements.

Weaknesses:
- The comparison of the approach in terms of OOD detection performance is slightly biased due to leveraging of generated OOD samples. Here in order to differentiate the contribution of the method from the incorporation of additional information a further experiments comparing against DDU would be warranted. In particular one could imagine adding a term to the loss which encourages low likelihood of the fitted GMM on OOD samples.
 - The comparisons in Figure 2 paint a slightly overly optimistic picture for the OOD HE approaches. In 2 dimensions the problem becomes quite trivial given negative / OOD samples. The difficulty of generating enough samples to cover the OOD volume becomes exponentially more difficult with increasing dimensionality of the input space and is especially easy when the diversity of the data is limited.
 - (Related to prior point) the datasets used to show the efficacy of the OOD approach are rather simple and small such that sample diversity is limited. Running one experiment on a larger dataset like imagenet would be very beneficial to show that the method can also work in the context of highly diverse input distributions and high dimensionality of the input space.
 - There seems to be a mistake in table 2 row Ensemble + HE, column PE
 - I recommend renaming Fig1 d and e to Cossim evaluation and HE evaluation as using objective gives the impression that the data shown is from models trained using these objectives.
 - As the work points out that applying the method to larger datasets would in principle be possible, this might be a good additional comparison to include. Many comparison approaches will have issues in this case, but comparison to other ensembling based approaches would be feasible.

Limitations:
The work correctly points out that the small size of the datasets is one of the main limitations. Generally, seeing the efficacy of the approach in a larger scale training setup would significantly improve the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed to improve the quantification of particle diversity in deep ensemble with hyperspherical energy (HE) on top of the CKA kernel. They further integrate the HE kernel in particle-based variational inference (ParVI) and generative ensemble with hypernetwork frameworks. The methods are evaluated on both synthetic experiments and small-scale classification datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation of the paper is clear: addressing mode collapse in deep ensemble by using a kernel that is more suitable to measure particle diversity of neural networks (HE kernel).
2. The advantage of HE kernel has been demonstrated in two different ensemble frameworks (parVI & generative ensemble with hypernetwork), and the empiricall performance of the method looks Ok.

Weaknesses:
The datasets considered are a bit outdated and the networks considred seem to be quite small, such that overall the performance is on the lower end (e.g. 85% acc. for CIFAR10, while many BDL methods can easily achieve accuracy higher than 90%). Fuerthermore, recent BDL papers typically consider larger datasets (such as ImageNet) and deeper networks. It is necessary to consider larger datasets and larger models in order to assess the practical effectiveness of the method.

Limitations:
The authors do not address potential negative social impact since the paper is predominantly theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
4FwlejUlg5;"REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Yq2dYPkfRU;"REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
EMV8nIDZJn;"REVIEW 
Summary:
The authors propose to model mixed time series with both continuous variables and discrete variables by constructing latent continuous variables (LCVs) from discrete variables (DVs). Several super-supervised learning constraints are proposed to help improve the effectiveness of LCVs as well as the co-learning of LCVs and CVs with both cross-attention and self-attention modules.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Clarity in paper-writing. Both figures and languages are easy to understand and well-polished. 
2. Originality. The idea is novel, easy to follow, and most importantly, it makes sense. 
3. Quality. The paper is conducted on various datasets for various tasks to support its claim as a general model. 
4. Code is released and sufficient training details are provided.

Weaknesses:
1. The paper lacks evidence to support the effectiveness of the proposed self-supervised objective functions. There is no theoretical or strong intuition behind them. 

1.1 For example, why does Temporal Adjacent Smoothness Constraint matter? I know from experiments it seems necessary, but intuitively and by commonsense, some discrete variables could indeed have sudden changes. In your example of meteorology, rainfall could indeed suddenly stop at a time point because the cloud moves from where the measurement device is. How do you defend your model in case where a DV is indeed subject to sudden changes by its nature? Moreover, for this constraint, do we have to use this specific formulation, because minimizing every two adjacent points could be too strong? How about K-lipschitz continuity as constraint, where k can be determined by the CVs? I would love to see some discussion on why you select this formulation specifically.

1.2 In addition, the adversarial framework that discriminates between CV and LCV seems weak. There is no guarantee that it will work in theory, as GAN-like structures are unstable. Even if the optimization is stable, in what condition would a model think ""I cannot discriminate between CV and LCV""? What specific properties are in CVs that LCV could learn to trick a discriminator? For facial recognition, we can tell if the object in an image is like a real human face or not, based on whether some details are distorted. For time series, how to interpret that? If at some time points, the time series are experiencing abnormal variations, that does not mean it is “fake”. What specific properties are you trying to look into when you design this loss function? If you just want the LCVs to reflect some of the variations in the CVs, you could design some simpler constraints just like your Temporal Adjacent Smoothness Constraint, correct? I would love to see some discussions.

2. Some issues with presentation. Variable z is first used in Eq.2 and then in Eq.4, but it was never explained in text. Usually, the audience is much more interested in the error bars instead of showing both MAE and MSE, as both metrics are extremely similar. Please adjust accordingly. 

3. Some issues with the experimental setting. The reconstruction loss is essential, but the key issue is, is the generated LCVs actually correct? Wouldn't it be wonderful if you could test on some datasets where some DVs are discretized from CVs, and you can try to recover the CVs and plot the results? If the accuracy of actually recovering CVs is not guaranteed, we do not need to have to “recover” a continuous time series, but we can obtain some latent embedding from the DV, which should also work well by intuition. Therefore, this experiment should be added as a demonstration that recovering LCV is possible. It would be a prerequisite to prove it is helpful.

Limitations:
I do not see limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
•	This paper introduces a type of spatial-temporal heterogeneity caused by the gap between continuous variables (CVs) and discrete variables (DVs). 
•	The author introduces latent continuous variables to create a unified continuous numerical space for both CVs and DVs, with the aim to address the heterogeneity caused by the gap between these types of variables.
•	The Latent Continuity Recovery architecture is innovative.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The introduction of latent continuous variables (LCVs) to bridge the gap between DVs and CVs is an innovative approach. The proposed method for latent continuity recovery through adaptive and hierarchical aggregation of multi-scale adjacent context information is a creative combination of existing ideas.

The code is open-sourced, clean, and well-organized, demonstrating a high standard of technical implementation.

The proposed solution addresses a fundamental issue in spatial-temporal modeling, making it highly relevant for various applications in fields such as precipitation, temperature, humidity, etc.

Weaknesses:
1. The presentation for the transformation process for LCVs can be improved for better clarity. A more explicit and detailed connection between the mathematical formulas and the LCV transformation process would strengthen the clarity and comprehensiveness of the paper.  
2. The formulation and application of the regularization term to encourage smoothness across time need to be better explained.
3. The experiment is mostly comprehensive, but the analytical explanation of latent variables can be strengthened. This analysis could be interesting to broad audience.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""Addressing Spatial-Temporal Heterogeneity: General Mixed Time Series Analysis via Latent Continuity Recovery and Alignment"" introduces MiTSformer, a framework designed to address the challenges of mixed time series (MiTS) data, which include both continuous variables (CVs) and discrete variables (DVs). The framework recovers latent continuous variables (LCVs) behind DVs to ensure sufficient and balanced spatial-temporal modeling. MiTSformer employs hierarchical aggregation of temporal context and adversarial learning to align DVs with CVs. The framework is validated on five MiTS analysis tasks, showing state-of-the-art performance across multiple datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novel Approach: The introduction of latent continuous variables (LCVs) to handle the spatial-temporal heterogeneity in mixed time series is innovative and addresses a significant gap in current methodologies.
Comprehensive Framework: MiTSformer is versatile, capable of handling various tasks such as classification, regression, anomaly detection, imputation, and long-term forecasting.
Robust Performance: The framework demonstrates superior performance across a wide range of datasets and tasks, indicating its robustness and effectiveness.
Detailed Analysis: The paper provides thorough empirical evaluations and ablation studies, which validate the effectiveness of the proposed method.

Weaknesses:
Complexity: The framework's complexity may pose implementation challenges for practitioners, potentially limiting its accessibility and usability.
Data Dependency: The performance heavily relies on the quality and diversity of the training data, which may limit its applicability in scenarios with limited data availability.
Scalability: While the method shows promising results, its scalability to very large datasets or real-time applications is not fully demonstrated.
Limited Explanation of Hyperparameters: The paper could benefit from a more detailed explanation of the choice and tuning of hyperparameters, which is crucial for replication and practical application.

Limitations:
The paper presents a compelling and innovative approach to handling mixed time series data. However, the complexity of the method, data dependency, and scalability issues suggest that further development and validation are needed to ensure its broad applicability and practical utility.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The MiTSformer framework includes Latent Continuity Recovery, which recovers latent continuous variables (LCVs) from discrete variables (DVs) using multi-scale temporal context and adversarial guidance, and Spatial-Temporal Attention Blocks, which capture dependencies within and across LCVs and continuous variables (CVs) through self- and cross-attention mechanisms. The paper explores general mixed time series analysis, addressing spatial-temporal heterogeneity. MiTSformer adapts to recover LCVs and capture spatial-temporal dependencies, demonstrating state-of-the-art performance across five tasks on 34 datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces a fresh problem definition by addressing the heterogeneity between continuous variables (CVs) and discrete variables (DVs) in mixed time series analysis.  The MiTSformer framework effectively recovers latent continuous variables (LCVs) from DVs and captures spatial-temporal dependencies, offering a balanced and comprehensive modeling approach. The framework is highly effective in handling various types of variables, providing clear insights into their relationships, which is invaluable for designing robust foundation models for time series.

Weaknesses:
Firstly, the experimental setup relies on converting continuous variables (CVs) to discrete variables (DVs) for experimentation, rather than using datasets with naturally mixed CVs and DVs. This may introduce unnecessary complexity and potential information loss, weakening the paper's motivation. Secondly, the complexity of the framework, which includes the recovery network and adversarial learning components, may pose challenges for practical implementation and scalability. The claim that the framework effectively recovers the inherent continuous nature of DVs and maintains temporal similarity through adversarial learning lacks robust verification and evidence. The improvements attributed to L_Dis are minimal, as shown in Table 4's ablation study, and further analytical reasons should be provided to support these findings.

MiTSformer

Limitations:
One limitation mentioned is that the framework cannot be directly applied to categorical discrete variables. In practice, many time series datasets combine categorical data with time series data, such as event data and product sales demand. This limitation is significant since these types of datasets are common in real-world applications.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
kRwQCAIA7z;"REVIEW 
Summary:
This paper presents a new method for estimating the mean of a subgaussian distribution, such that differential privacy is guaranteed (i.e., the final result does not provide too much identifying information about any individual sample).  For the proposed method, in the case of known covariance,  the sample complexity depends only on the trace of the (1/2) power of the covariance matrix. This means that, if the distribution is highly anisotropic (with the variance mostly concentrated in only a few dimensions) then the sample complexity does not depend explicitly on the full dimensionality of the space *at all*, only on the dimensionality of the lower-dimensional space where ""most"" of the variance lies. This improves on the prior SOTA result, from (Aumüller et al 2023), where the sample complexity explicitly contains a factor of sqrt(d), where d is the full dimensionality of the space. The core algorithm proceeds by first filtering outliers, using a differentially-private filtering mechanism proposed by (Tsfadia et al 2022), and then simply taking the mean of the remaining samples and adding Gaussian noise.

A matching lower bound is also proven, showing that we can't in general do any better asymptotically than O(trace(Σ^(1/2))), up to logarithmic factors.


A variant of the algorithm is proposed for the case of unknown covariance as well, which has sample complexity with an explicit dependence on d^(1/4), which is still an impovement overt the prior SOTA d^(1/2).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem being addressed is important, given the increasing relevance of privacy guarantees in machine learning.
- The improvements from prior work are clearly contextualized. Sufficient background information is provided to understand the major points of the paper.
- To my first estimation, the technique appears sound, although this is not my area of expertise and I am not qualified to rigorously evaluate the technical correctness of the proposed algorithm.

Weaknesses:
- The order of the presentation could be improved. The preliminaries section could be presented earlier in the paper, before the definitions given are referred to.
- No empirical tests are performed. Including a numerical simulation of the algorithm would increase the reader's confidence in the correctness of the theoretical  results, as well as give an idea of the tightness of the bound in practice.

Limitations:
Limitations are addressed adequately in the Future Work section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies mean estimation for multivariate Gaussian distributions. It's well known that this problem under differential privacy suffers from a curse of dimensionality- the sample complexity of estimating the mean (in expected $\ell_2^2$ error) scales with $\sqrt{d}$ where $d$ is the dimension. However, the lower bounds are for isotropic covariance matrices, whereas in the real world, covariance matrices are often far from isotropic. The main intuition is that when covariance matrices are far from isotropic, there is a gap between large and small singular values, and hence the number of 'important dimensions' is relatively small- the object behaves 'more lower-dimensional' than the dimension $d$ indicates.

They consider two settings, the known and unknown covariance cases.

1) In the known covariance case, they show a dimension-independent bound (for approximate DP) that depends instead on the sum of (square roots of) singular values. They argue that this is optimal up to logarithmic factors (and also that no pure DP algorithm can achieve such a dimension-independent bound). 

2) For the unknown covariance case, one approach is estimating the covariance and then applying the known-covariance algorithm, but this might be prohibitive since estimating the covariance is known to require at least $d^{3/2}$ samples asymptotically. The authors instead show that it's possible to achieve a bound with a $d^{1/4}$ dependence instead.

For the known covariance case, the authors use the known FriendlyCore algorithm with a carefully specified predicate to remove outliers (points far from the mean in any coordinate), which allows them to bound the sensitivity and then add dimension-independent noise. The lower bounds are adaptations of the packing and fingerprinting approaches used for pure and approximate DP. In the unknown covariance case, they treat large and small singular values differently. Firstly, they learn the identities and values of the top $k$ singular values up to multiplicative factors and use the known covariance algorithm to estimate the mean restricted to these coordinates. For the other coordinates, they use Holder's inequality to bound the $\ell_2$ norm of the singular values. For small singular values, the variance does not need to be as accurately estimated since these directions are less important. Balancing $k$ to optimize the cumulative error gives the sample complexity bound.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Gaussian mean estimation is a fundamental problem, and this paper suggests a way to deal with the curse of dimensionality that privacy imposes for this problem, by considering non-worst case instances (anisotropic covariances). This is an interesting direction that will likely spawn future work. The paper also does a nice job of suggesting future directions of research in this space.

2) The paper combines known techniques in privacy in clever ways to obtain their upper and lower bounds. The problem of private Gaussian mean estimation has seen lots of prior investigation, and they also do a good job of explaining how a lot of these techniques inherently give dimension-dependent bounds.

Weaknesses:
1) While the results are interesting, they mostly follow from applying known techniques from the privacy literature- the additional technical insight of this paper is rather limited.

2) The unknown covariance case as presented was confusing to me- is there an assumption that the covariance matrix is diagonal; this seems to be used in the author's proofs and approaches? This wasn't clearly described in the paper and could use clarification. I don't see how to extend the techniques of the authors to deal with general covariance matrices.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the problem of DP mean estimation for high-dimensional distributions exhibiting anisotropy, meaning the variances along different directions are highly non-equal. Prior works on this problem were plagued by a ""curse of dimensionality"", requiring sample complexities at least on the order of the square root of the ambient dimension d, even in cases where the non-private setting permits much lower sample sizes. The authors make two main contributions to address this limitation.

Firstly, for the case when the covariance matrix $\Sigma$ is known, they provide a DP algorithm achieving sample complexity independent of the dimension d. Instead, the sample complexity depends solely on $tr(\Sigma^{1/2})$, which can be substantially smaller than d when the distribution is highly anisotropic. The bound matches the optimal non-private sample complexity up to logarithmic factors. Secondly, the authors develop a DP algorithm for the unknown covariance setting that improves upon prior work by reducing the dimension dependence from $1/2$ to $1/4$, while also depending on properties of the diagonal entries of $\Sigma$.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Paper is clear and well written, though it is mathematically heavy. The analysis, supported by proof sketches, appears technically sound.
- The motivations of improved rates with certain covariance structure are intuitive.

Weaknesses:
- The results involving known covariance are straightforward. The benefits comes from (1) application of Tsfadia et al. and (2) rescaled noise adding, which is similar to Aumüller et al.
- There is still a $d^{1/4}$ left.

Limitations:
Limitations are well addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents new differentially private algorithms for estimating the mean of high-dimensional data, addressing the inefficiencies of traditional methods that suffer from the ""curse of dimensionality."" The proposed estimators are tailored for anisotropic subgaussian distributions, where data signals are concentrated in fewer dimensions. These estimators achieve optimal sample complexity that is independent of dimensionality when the covariance is known and improve the sample complexity for unknown covariance cases from $d^{1/2}$ to $d^{1/4}$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studied interesting problems and the writing is clear. The authors give both upper bound and lower bound for the problem.

Weaknesses:
1. I understand the space of the main text is limited but there is no conclusion.
2. There is no experimental design to verify their theoretical findings.

Limitations:
There is no experiment.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the problem of DP mean estimation and the focus is on the high dimensional settings where the distribution is nearly low rank (or tr(Sigma)<<d), and the error metric is l_2. Prior work in this setting still requires sqrt(d) samples to achieve any non-trivial error which is sub-optimal. In the known covariances setting this paper achieves sample complexity n = tr(Sigma)/alpha^2 + tr(Sigma^1/2)/(alpha*eps) which can be significantly smaller than sqrt(d) in the high dimension setting. The algorithm filters out the outlier using the FriendlyCore algorithm [62] and leverages the propose-test-release framework. For the unknown covariance, one will need to first have a rough estimate of the covariance such that appropriate noise can be added to guarantee privacy. However dimension free estimation of the covariance is impossible, so the idea is to estimate the top-K coordinate with the largest variances, and simply adding small and isotropic gaussian noise to the remaining directions. Combining the error incurred in the top and bottom coordinate gives a d^(1/4) dependency in the dimensionality. On the lower bound side, they prove their known covariance result is optimal up to logarithmic factors. It is unclear what the optimal sample complexity for the unknown covariance should be.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
DP mean estimation problem in the high dimensional setting is a fundamental problem in differential privacy, and the paper achieves optimal results for known covariance setting and makes improvement for the unknown covariance meeting. The ideas for unknown covariance seem novel to me. The sample complexity of mean estimation in the unknown covariance setting, posed in this paper, remains an interesting open problem.

Weaknesses:
The techniques for the known covariance setting have been developed previously.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
nRdST1qifJ;"REVIEW 
Summary:
In this paper, the authors introduce a new defense mechanism called Prompt Adversarial Tuning (PAT) designed to protect LLMs from jailbreak attacks. PAT enhances the robustness of these models by attaching a defensive prompt control to user inputs, optimized through a combination of adversarial and benign prompts. This method reduces the success rate of advanced attacks to nearly zero while incurring minimal computational overhead. However, there are some concerns. The main optimization idea is relatively straightforward and bears similarities to the GCG [1] approach. Additionally, the robustness of the experimental results is questionable, particularly due to the use of prefix-based methods to calculate the ASR.

[1] Universal and Transferable Adversarial Attacks on Aligned Language Models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. PAT introduces a new system-level defense mechanism that is interesting and does not alter the LLM itself.
2. PAT demonstrates good transferability across different models, both open-source and closed-source, showcasing its versatility.
3. The method preserves the model's performance on benign tasks while providing robust defense against malicious prompts.

Weaknesses:
1. The optimization of the defense prompt incurs computational overhead, and the optimization process using greedy sampling is time-consuming.
2. I am concerned about the generalization of PAT, specifically whether the optimized prompt can defend against adaptive and real-world attacks.
3. The robustness of the experimental results is questionable, particularly due to the use of prefix-based methods to calculate the ASR. These methods can easily lead to false negative or false positive judgments.
4. It is unnecessary for defenders to discuss their own capabilities. Generally, in most defense settings, defenders have full access to the systems (including models and data), as they are developed by the systems themselves.

Limitations:
1.The defense may be less effective when attackers implement other adaptive strategies (In the wild attack [2], PAP[3], GPTfuzzer[4], PAIR[5] etc.) with knowledge of the defense mechanism.
2. The rapid development of new jailbreak methods means that continuous updates to the defense strategy may be required.

[2] ""Do Anything Now"": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.
[3] Tree of Attacks: Jailbreaking Black-Box LLMs Automatically 
[4] GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.
[5] Jailbreaking Black Box Large Language Models in Twenty Queries

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a prompt adversarial tuning (PAT) to protect large language models against jailbreaking attacks. PAT is a prompt tuning-based defense for jailbreaking attacks where a string and GCG attack string are jointly optimized to have an LLM generate benign outputs. Experiments on AdvBench and MT-Bench show the defense is effective against GCG, AutoDAN, and ICA while maintaining MT-Bench performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The optimization objective of PAT is sound and incorporates the GCG attack as an adversary, which introduces the use of adversarial training techniques to support jailbreaking defenses
* The experimental results show PAT generally outperforms previous defenses
* Subject matter is important and timely

Weaknesses:
* The evaluation only considers three defense baselines (ICD, self-reminder, and perplexity filtering) and three attack baselines (GCG, AutoDAN, and ICA). This does not cover many other attacks and defenses, such as SmoothLLM [1] and RPO [2] (which also has a similar methodology to PAT).
* Closed-source model evaluation is also not comprehensive. Table 2 is missing the defense baselines, and Table 3 is missing both the defense baselines and the ICA attack.
* The adaptive attack setting (4.5) only considers an adaptive GCG attack. This is unconvincing, as the defense string was optimized on GCG, so I wonder if adaptive versions of other attacks will transfer. If there aren't adaptive versions of the other attack baselines, the authors should detail this.

[1] Robey, A., Wong, E., Hassani, H., & Pappas, G.J. (2023). SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. ArXiv, abs/2310.03684.

[2] Zhou, A., Li, B., & Wang, H. (2024). Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. ArXiv, abs/2401.17263.

Limitations:
Authors discuss potential new adaptive attacks that can break the defense

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an in-context defense method against jailbreaking attacks against LLMs. The core idea is to tune the suffix to the system of a LLM with a multi-objective optimization framework: 1/ Benign prompts enhanced with this tuned suffix should activate normal response as if there were no suffix. 2/ Malicious prompts enhanced with the suffix should produce aligned answer to the user-specified benign response. The result shows the defense of the proposed method against GCG, AutoDAN and ICA under the white-box scenario. The empirical evaluation also cover the transferable defense effects over GPT-4.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of using max-min robust optimisation to adversarially tune the system prompt is well defined and explained clearly in this study. As well, adding suffix to the system prompt can provide a transferable defense across different model architectures of LLM.

Weaknesses:
My major concern is this work does not extend comprehensively the coverage over attack and defense baselines in the empirical evaluation.  I would suggest a few more attack and defense methods to enrich the current study: 

**Attack**:

[1] Xie et al, Defending chatgpt against jailbreak attack via self-reminders, Nature Machine Intelligence, 2023. 

[2] Guo et al, COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability, ICML 2024. 

**Defense**: 

[3] Chao et al, Jailbreaking Black Box Large Language Models in Twenty Queries, Workshop on robustness of zero/few-shot learning in foundation models, NeurIPS 2023, https://arxiv.org/abs/2310.08419. 

[4] Xu et al, SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding, ACL 2024, https://arxiv.org/abs/2402.08983. 


As well, the evaluation needs to be extended to more open-sourced models, e.g. Mistral-8B and Llama-3 8B. Furthermore, I would be interested to check the transferability of the proposed defense method across different open-sourced models.

Limitations:
This work still needs to extend its empirical evaluation to cover more attack / defense baselines and include more LLM models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Inspired by prompt tuning and adversarial training, this paper proposed a new jailbreaking defense method named prompt adversarial tuning (PAT) which optimizes a defensive prefix by alternating between updating attack and defense controls with two opposite output targets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper employs adversarial training to defend against jailbreaking attacks, but instead of tuning model parameters, it tunes a defense prefix. Since the model parameters remain unchanged, the impact on utility is likely minimized.


2. The optimized defense prefix exhibits a certain degree of transferability across different LLMs.

Weaknesses:
1. Another paper [1] proposed using an optimized soft system prompt to enhance model safety, which is highly relevant to this study. A corresponding comparison and discussion might be necessary.

2. For chat LLMs, the evaluation of utility should include both instruction-following ability and knowledge-wise capability. The authors only tested instruction-following ability on MT-bench, neglecting the knowledge-wise component. Therefore, additional utility experiments, such as those on MMLU, are necessary.

[1] Zheng, Chujie, et al. ""On prompt-driven safeguarding for large language models."" Forty-first International Conference on Machine Learning. 2024.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on defense against jailbreak attacks. More specifically, it first considers improving model robustness through prompt tuning. The proposed method, Prompt Adversarial Tuning (PAT), aims to design a prefix to input prompts that encourages LLMs to still provide correct responses for benign inputs while responding with messages like ""I am sorry, ..."" to harmful prompts. To achieve this goal, the authors utilize prior attack methods to generate harmful prompts and then optimize the prefix to render these harmful prompts ineffective. Experiments on various datasets against different attack methods show the effectiveness of the proposal.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is effective and easy to implement. Importantly, it maintains benign utility, which is crucial.
2. The authors validated that the proposed method can transfer to different models, demonstrating its generalizability.
3. The method was also tested against adaptive attacks and proved effective.

Weaknesses:
1. The proposed defense relies on obtaining harmful prompts through existing attack methods. This raises doubts about whether the proposed method can be effective against unseen jailbreak attacks. For instance, [b] found jailbreak attacks can be conducted using non-English prompts.  There is not enough discussion on how well the proposed defense method works against unknown attacks

[a]MULTILINGUAL JAILBREAK CHALLENGES IN LARGE LANGUAGE MODELS

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
RSs4o7CSqe;"REVIEW 
Summary:
The paper proposes a controllable conditional image fusion method. This method enables dynamically controllable fusion for each image pairs. The core idea is to empirically construct a conditional bank and dynamically select different control conditions during the diffusion fusion process. The method is suitable for image fusion tasks of different modalities and can also perform controlled fusion given various downstream tasks, such as object detection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- The paper introduces the controllable conditional image fusion algorithm for the first time. This method makes the image fusion process based on the diffusion model controllable through manual feature constraints (empirical).

- This method is suitable for different fusion tasks.

Weaknesses:
- The section Sampling-adaptive Condition Selection (SCS) is mainly derived from ""GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks"", which is cited in the paper, but some of the formulas are not clearly expressed. For example, in Eq 11, Gate(C) has C on the left, but the condition C is not contained on the right. Another example is Line 170, Li(t) = E[wi(t)Li(t)]? Additionally, there are some typos, like Line 170, theta.  
     Therefore, understanding the routing process without reading the GradNorm paper might be difficult.

- There are some issues with the writing and formatting, such as the order of appearance of Tab1 and Tab2, and the captions for the tables do not specify the task for each tab (e.g., Multi-Focus, Multi-Modal, etc.) in the quantitative comparison results. The captions for the images also have this issue and need improvement.

- There are too many empirical conditions, which may require different settings under various circumstances.

Limitations:
- The conditional bank is manually designed, and different experiments likely require different fine-tuning. This limits the potential of the proposed method.
- It is necessary to specify the runtime, as time is usually a key metric for diffusion model-based methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel Controllable Condition Fusion (CCF) framework that utilizes a pre-trained diffusion model to achieve dynamic and adaptive condition selection without requiring specific training for general image fusion tasks. The authors presented a conditional bank conducted by various conditions to fit diverse scenarios. The conditions can be dynamically and adaptively selected according to the diffusion step, allowing CCF to conditionally calibrate the fused images step by step for each individual sample. Experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
(1)The paper is well-structured and the proposed idea is novel. It introduces a conditional controllable fusion (CCF) framework for general image fusion tasks without specific training. The generation and denoising capabilities of DDPM are effectively leveraged to produce high-quality fused images, making its integration with image fusion both ingenious and well-suited.
(2)A new dynamic conditional paradigm was introduced with a conditional bank that regulates image fusion. This allows for the adaptive selection of multiple conditions, enabling control over various image fusion processes.
(3)The paper validates the superior performance of the CCF framework over SOTA methods through extensive experiments on various fusion tasks. This enhances the credibility and applicability of the proposed approach.

Weaknesses:
(1)This paper introduces a controllable condition image fusion model. However, this ‘condition’ is not the common guidance condition, such as sketch, location map, pose image, etc, but evaluation metrics, such as SSIM, Edge Intensity et al. Therefore, suggesting that authors give a clear explanation or definition of the condition in Section Introduction.
(2)An important comparison is missing. From the first impression, the proposed method is to leverage the reconstruction capability of DDPM. However, the article did not demonstrate that the fusion process utilized the reconstruction capability of DDPM, it is important to prove it systematically.
(3)One of the core innovations in this paper is the Conditional Bank, which contains three types of conditions: basic fusion conditions, enhanced fusion conditions, and task-specific fusion conditions. However, there lacks a clear definition of three conditions to identify the difference between these conditions. 
(4)The paper contains some small mistakes in symbol, such as Line 170 theta, the lack of a subscript for eq 6 \epsilon and missing the parentheses. Additionally, there is indistinct use of ""x0|t"" and ""x0"" in Line 134. Overall, the work is very readable, but the problems with the writing should be corrected.

Limitations:
The authors have discussed the limitations of this work. And I don’t think there are any direct negative societal implications. Other limitations and opportunities for improvement are addressed in my responses to previous questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a diffusion-based image fusion method with adaptive fusion conditions. It aims to solve the drawback of existing method, i.e., the application of distinct constraint designs tailored to specific scenes. This method builds a condition bank with basic, enhanced, and task-specific conditions. Then, it employs specific fusion constraints based on these conditions for each individual in practice. The proposed method is tested on various image fusion tasks, including multi-modal, multi-exposure, and multi-focus image fusion.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method can combine the advantages of multiple types of loss functions and take consideration of downstream tasks, thus dynamically adjust the optimization direction during the sampling process. 
2. This method is applicable to multiple image fusion tasks, including visible-infrared, medical, multi-exposure, and multi-focus image fusion.
3. The experiments are rich and the results are competitive.

Weaknesses:
1. The basic and enhanced conditions are some widely used image fusion constraints, and the task-specific condition is based on the feature extracted by a downstream network. Thus, the main contribution lies in the design of the gate of conditions. However, the reason why is the gate defined in this way and the principle behind this definition not fully explained in detail.
2. It focuses on the problem that such data-driven fusion methods are hardly applicable to all scenarios, especially in rapidly changing environments and source images with dynamic differences. However, the experiment fails to show the advantages in these scenarios.
3. This method states that multiple conditions can be considered, but the actual considerations are limited, such as the SSIM-based enhanced condition and detection-based task-specific condition.
4. The article basically describes the process of the method, but the motivation, principle, and details of the method are not clear and explicit enough.

Limitations:
1. The first and second contributions are essentially the same.
2. The conditions discussed in the Appendix are almost different basic conditions. And the results of MSE are a bit strange.
3. It lacks the experiment on challenging and dynamic scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Conditional Controllable Fusion framework called CCF, effectively addressing the issue that existing data-driven fusion methods struggle to adapt to all scenarios. The authors conducted extensive experiments to demonstrate the effectiveness of the CCF. This manuscript is standardized and the writing is fluent.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of conditional controllable image fusion (CCF) is novel.
2. Extensive experiments are conducted to demonstrate the effectiveness of the CCF.
3. This manuscript adheres to a high writing standard, ensuring fluent and easily understandable content.

Weaknesses:
1. How was the Selection frequency map in Figure 1 obtained? Is it based on real data or simulated?
2. It is not clear exactly what iterative refinement of sampling refers to. Why does sampling need to be iteratively refined? The sampling iteration is derived from the diffusion model DDPM and how is it related to the proposed CCF?
3. What are the advantages of using the DDPM for controlled image fusion? In other words, how do diffusion models contribute to the adaptability and controllability of image fusion?
4. Multiple sign ambiguities. Including but not limited to: 1) $c$ denotes both the channels and the given condition in Eq. (5). 2) What does the $f_{\theta}$ refer to? 3) What is the $s_{\theta}$?
5. How are task-specific conditions incorporated into DDPM? Lack of specific implementation details.
6. The theoretical discussion of adaptive customization of conditions for each sample is limited to sections 4.1 and 4.2, lacking specific customization procedures or visual case studies. Consequently, the credibility of this contribution in terms of condition customization is compromised.
7. The authors chose 8 enhanced conditions, i.e., SSIM, Content, Edge, Low-frequency, High-frequency, Spatial Frequency, Edge Intensity, and Standard Deviation enhancements. However, their selection lacks a strong foundation or rationale.

Limitations:
Please refer to ""Weakness"".

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
8bExkmfLCr;"REVIEW 
Summary:
The paper introduces the JOBCD (J-Orthogonal Block Coordinate Descent) algorithm, a novel method designed to tackle optimization problems under J-orthogonality constraints. JOBCD includes two variants: GS-JOBCD (Gauss-Seidel strategy) and VR-J-JOBCD (Jacobi strategy with variance reduction). Theoretical analyses establish the algorithms' complexity and convergence, while extensive experiments show JOBCD's superior performance compared to state-of-the-art methods in various applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The strengths of this paper are listed as follows:

Originality: This paper introduces JOBCD as a novel approach to handling J-orthogonality constraints. It offers GS-JOBCD and VR-J-JOBCD, showcasing flexibility and innovation in optimization strategies.

Quality: This paper provides comprehensive complexity and convergence analyses. Extensive experiments demonstrate superior performance on real-world and synthetic data.

Clarity: The structure is logical.

Significance: This work is relevant to various statistical learning and data science fields.

Weaknesses:
Some proofs for Section 4 are hard to follow.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes two Block Coordinate gradient descent methods(BCD) for solving J-orthogonal constrained problem. One is Gauss-Seidel type, the other one is Jocobi type as well as addressing finite sum problem using variance reduction strategies. Convergence guarantees are proved with KL conditions. Numerical experiments show the advantages of the  proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm decomposes the matrix variable into row block structure, yielding a block coordinate descent algorithm with a small size subproblem. The numerical performance is very impressive.

Weaknesses:
1. This paper is based on the paper "" [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization
under orthogonality constraints. ArXiv, abs/2304.03641, 2023.""
The main difference is the constraint in this paper becomes J-orthogonality constraint. However, the framework follows almost the same as [51]. The authors should highlight the novelty of the algorithm or difficulty in the extension.

2. The authors of the reference [31] may be wrong. Besides, the UMCM algorithm in [31] solves orthogonal constrained problem. Is there any difference in implmenting in solving J-orthogonality problem? The objective value of UMCM is far from the JOBCD method. I'm curious about the reasons.

3. In numerical experiment, how do you select subset from the dataset, see line 309.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two block coordinate descent methods for minimization of a finite-sum subject to the J-Orthogonality constraints — one based on Gauss-Seidel strategy, the other based on variance reduction and Jacobi strategy. The convergence is proved, with a global convergence rate of O(N/\epsilon) and O(\sqrt{N}/\epsilon) respectively, and a local convergence rate that depends on the desingularization in the KL-condition assumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The algorithms proposed are novel and might be useful in practice: the update rules involve solving a small size problem thereby is very simple, and the convergence is proved theoretically under reasonable assumptions.

Weaknesses:
The paper is relatively dense, and I find it a bit hard to keep track of all the terms introduced. For instance, the parameter theta is used in the algorithms but I’m not sure where it is introduced; in Assumption 4.8 KL function is mentioned but it’s not defined…

Limitations:
Yes, the paper discusses the assumptions of the theorems.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a block coordinate descent method for solving optimization problems with J-orthogonality constraints. Several variants of the method are introduced within this framework, and convergence results are established. Extensive numerical results are also presented to demonstrate the efficiency of the proposed methods. However, I have some concerns regarding the novelty of this paper as well as the numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
It appears that optimization with J-orthogonality constraints has not been thoroughly studied in the literature. This paper proposes an efficient method for addressing this problem.

Weaknesses:
1. My major concern is that the novelty of this paper might be insufficient since the row-based approach is very similar to that in [51], even though the two papers tackle different problems.

2.  For the numerical results shown in Table 1, the proposed method fails to return a feasible solution for some instances, such as randn(10-10-5) and w1a (2470-290-145), as well as some other instances in the appendix. This is strange since the paper describes a BCD-type method, which should always return a feasible solution.

3. The information of the reference [31] might be incorrect. 

4. For the GS-JOBCD method, there are two options for choosing $Q$, whereas J-JOBCD only has one option. The authors should provide an explanation for this difference.

5. The presentation could be further improved. Here are a few examples: the formulation of $P_i$  after equation (12) could be simplified by removing the notation $\mathrm{mat}$; it is unclear if the requirement on $\underline{Q}$ in equation (4) is sufficient to guarantee convergence (probably not, since $\underline{Q} = 0$  also satisfies this condition).

Limitations:
At the beginning of the paper, the authors claim that equation (2) can imply 
$\\|\nabla f_i(X) - \nabla_i f(X^+)\\| \leq L_f \\|X - X^+\\|$, which is incorrect. Note that the converse is correct. The other assumptions in Assumptions 4.1 and 4.2 essentially assume the compactness of the iterates.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Y841BRW9rY;"REVIEW 
Summary:
This paper proposed an attack against the RAG-based LLM agents. Specifically, it proposes a constrained trigger optimization to search the trigger so that any queries included can be mapped to a certain compact cluster while keeping the coherence and attack success rate. The experiments show that the proposed method is efficient in real-world RAG-based LLM agents.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The experiments are extensive.
2. Has a good summary of related work.

Weaknesses:
# Confusing Threat Model
The threat model described in the paper is vague. The authors mention that an attacker can poison the RAG database and use a searched trigger to retrieve these poisoned entries for an attack. However, it is unclear how the attacker can add the trigger to the query during inference. In traditional backdoor attacks, the attacker can add the trigger to the input (e.g., placing a small sticker on a stop sign). When the vehicle detects the trigger (the sticker), it makes a wrong decision, which is good and intuitive. In the case of LLM-based agents, the query is initiated by the user (the victim). How can the attacker add the trigger to the user's query? This aspect is not explained, making the threat model less convincing.

# Motivation for the Optimization Method (Coherence, Equation 10)
The motivation behind the optimization method, particularly the coherence aspect (Equation 10), is unclear. The authors claim that coherence is intended to make the trigger stealthier. In traditional backdoor attacks, triggers are often rare words to prevent accidental triggering in natural scenarios. However, the authors aim to optimize a trigger that looks normal, such as “Be safe and make a disciplined upbringing quiet.” This presents two issues:
1. **Natural Triggering**: Such triggers can easily be activated in natural conditions, which is undesirable for the attacker.
2. **Stealthiness**: Why is it necessary to increase stealthiness if the trigger is inserted by the attacker? Unlike traditional backdoor attacks where triggers need to be subtle, in this scenario, the trigger is directly manipulated by the attacker.

# Lack of Optimization for Poisoned Passages
A critical aspect is missing: the optimization of poison passages. The proposed method focuses on optimizing a trigger such that any queries containing the trigger will:
1. Compact with each other.
2. Stay away from other triggers.
3. Be semantically fluent.
4. Mislead the LLM.

However, there is no optimization for the poisoned passages inserted into the knowledge base. The real impact on LLM output comes from the passages retrieved, not the query itself. The authors do not ensure that triggered queries can retrieve poisoned passages or that these passages can influence the LLM. They only optimize the trigger, aiming to cluster triggered queries together. Without ensuring that these queries retrieve poisoned passages, clustering them is pointless.

# Lack of In-depth and Insightful Transferability Explanation
The explanation of transferability is not clear. The authors experimentally show that triggers generated by different retrievers can be transferred and attribute this to similar training data distribution. And in Figure 3, they claim that triggers transfer better among embedders with similar training strategies. This is confusing, and both reasons are unconvincing. To verify this point, the authors need to demonstrate that different training data distributions weaken transferability. Overall, a more in-depth insight is required.

# Lack of ablation study on hyperparameters
I noticed the Table 4 in the appendix shows the setup of hyperparameters. But the how do author set the hyperparameters like $\lambda$ in Equation 4,5,6 is not clear to me.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper titled ""AGENTPOISON: Red-teaming LLM Agents via Memory or Knowledge Base Backdoor Poisoning"" introduces a novel backdoor attack method targeting large language model (LLM) agents. These agents leverage a memory module or a retrieval-augmented generation (RAG) mechanism to retrieve knowledge and past instances to aid in task planning and execution.

The key contributions are as follows:
- AGENTPOISON is the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base with malicious demonstrations.
- The authors have conducted thorough experiments on three types of real-world LLM agents: an autonomous driving agent, a knowledge-intensive QA agent, and a healthcare EHRAgent.
- The paper highlights the potential security threats posed by backdoor attacks on LLM agents, emphasizing the need for developers to mitigate such vulnerabilities.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper initially focuses on the safety issue of LLM agents, which is a popular topics recently.
- The paper is well-written and easy to follow. Good visualizations help reader have a better understanding of methodology and experimental results.
- The trigger optimization process maps the triggered instances into a unique embedding space, enhancing retrieval rates of poisoned instances while maintaining normal performance for benign instructions. The optimization is designed to be highly transferable, stealthy, and coherent in context, ensuring minimal disruption to normal agent functions.
- AGENTPOISON can perform effectively even with a minimal number of poisoned instances and short trigger sequences.
- The experimental results demonstrated AGENTPOISON's superiority over four baseline attacks, achieving higher retrieval success rates and end-to-end attack success rates with less degradation in benign performance.

Weaknesses:
I do not identify any major weaknesses of the paper.

Limitations:
The authors have discussed limitations in the Limitations section of paper.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new setting for red teaming against RAG empowered agents. Specifically, they tested their method in three different settings: driving agent, knowledge intensive QA and EHRagent. The method they proposed is to use constrained optimization to jointly optimize for two objectives: ""a) the retrieval of the malicious demonstration and b) the effectiveness of the malicious demonstrations in inducing adversarial agent outputs"". In particular it tries to map the poison demonstrations into a space that is separated from the benign examples. Experimental results showed its efficacy for high attack success rate with low benign impact and low poison rate.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The problem proposed is novel and was the first paper to tackle this problem
- the paper overall is well motivated
- Well organized, proposes a new red teaming attack specifically for RAG agents
- strong results on ASR

Weaknesses:
- $\eta_\text{tar}$ and $\eta_\text{coh}$ seems hard to pick a intuitive value
- Line 169: not obvious what a), b) and c) are, probably better to outline them explicitly

Limitations:
The authors included them

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents AGENTPOISON, a novel red-teaming approach aimed at exposing vulnerabilities in LLM agents by poisoning their long-term memory or RAG knowledge base. Unlike conventional backdoor attacks, AGENTPOISON does not require additional model training or fine-tuning and ensures high attack success rates with minimal impact on benign performance. The approach optimizes backdoor triggers through a constrained optimization process, ensuring that malicious demonstrations are retrieved with high probability when user instructions contain the optimized triggers. The method is validated through extensive experiments on three types of LLM agents: autonomous driving, knowledge-intensive QA, and healthcare agents, demonstrating its effectiveness and generalizability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.  AGENTPOISON introduces a novel red-teaming method targeting RAG-based LLM agents, addressing a significant gap in existing research. The approach's emphasis on poisoning memory or knowledge bases rather than model parameters is particularly innovative.
2. The paper is well-structured, with thorough explanations of the methodology, optimization process, and experimental setup. The use of constrained optimization to ensure high retrieval rates and adversarial action success is a notable strength.
3. The paper is clear and well-written, with detailed descriptions of the technical aspects and experimental results. Figures and tables effectively illustrate the method's performance and comparative advantages.
4. The approach has significant implications for the safety and trustworthiness of LLM agents, especially in critical applications like healthcare and autonomous driving. By demonstrating high attack success rates with minimal benign performance degradation, AGENTPOISON highlights crucial vulnerabilities that need addressing.

Weaknesses:
1. The paper briefly mentions potential defenses against the proposed attack but does not explore them in depth. Including a more comprehensive discussion on possible mitigation strategies and their effectiveness would strengthen the work.
2.  While the paper demonstrates the transferability of the optimized triggers across different retrievers, further exploration of the method's applicability to a broader range of LLM architectures would be beneficial, rather than just LLaMA3 and ChatGPT.
3. Although the paper uses multiple evaluation metrics, including attack success rates and benign accuracy, additional metrics such as the impact on real-world task performance and user experience could provide a more holistic assessment of the method's implications.

Limitations:
The authors acknowledge the method's reliance on partial access to the victim agent's memory or knowledge base, which may not always be feasible in real-world scenarios. Additionally, while AGENTPOISON is effective in controlled experimental settings, its performance in more dynamic and less predictable environments remains uncertain. The paper could further discuss potential societal impacts, particularly regarding the ethical implications of deploying such attacks and the need for robust defensive measures to protect critical LLM applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel red-teaming approach that targets the vulnerabilities of large language model (LLM) agents by poisoning their long-term memory or retrieval-augmented generation (RAG) knowledge base. The primary contribution is the development of AGENTPOISON, a method to inject backdoor triggers into the knowledge base, which leads to the retrieval of malicious demonstrations when certain triggers are present in user queries. This attack method is tested on three real-world LLM agents, demonstrating high attack success rates with minimal impact on benign performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The introduction of AGENTPOISON, which optimizes backdoor triggers for memory or RAG-based LLM agents without requiring additional model training, represents an advancement in backdoor attack strategies.

2. Extensive experiments validate the effectiveness of AGENTPOISON across three different types of LLM agents, showcasing its high attack success rate (≥ 88%) and minimal impact on benign performance (≤ 1%).

Weaknesses:
As someone not deeply familiar with this field, I found some sections challenging to understand. The authors could provide more context and explanations about the attack mechanisms and their significance to make the paper more accessible to readers from diverse backgrounds.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
n2dvAKKQoM;"REVIEW 
Summary:
This paper focuses on the imputation of missing values in time series. By noticing that different imputation methods might affect the downstream forecasting tasks, this paper proposes the imputation evaluation approach regarding the downstream tasks' performance. 
Then, the authors also developed some methods to improve the efficiency of the evaluation approach and did experiments against some baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposed an evaluation approach to time series imputation methods from the perspective of the impact on the downstream forecasting task. This is a seemingly interesting perspective. 

2. The proposed method has mathematically formulated the intuition of evaluating imputation methods based on the prediction performance, and the authors developed the approximate method to improve the efficiency.

Weaknesses:
1. The evaluation criteria defined in (1) are based on binary comparison and do not look realistic. In real-world applications, the difference in prediction errors between the two methods can be marginal and insignificant, and calculating this binary indicator makes little sense. Meanwhile, in real-world data, the percent of missing values would be very low, and it is doubtful that imputed missing values would lead to significant differences in downstream predictions, especially considering that there are various techniques to regularize models for more robust predictions.  

2. Some mathematical presentation is confusing. For instance, how is Eq.(2) derived?  $f(X_k^v, \theta)$ is irrelevant to $y$, and thus why is there the term of the derivative of  $f(X_k^v, \theta)$ w.r.t. $y$ in Eq.(2)

   According to line 148,  Eq.(3) is for approximating $\frac{\partial f(X_k^v, \theta)}{y_{i, l}}$, while on line 156, it presents the substitute of $\frac{\partial f(X_k^v, \theta)}{y_{i, l}}$ again. This is very confusing. 

3. The experiments have unjustified and confusing setups and results. For instance, the description between lines 259 and 267 is very unclear, e.g., what is the label in a time series prediction task? why specifically replace 40% of data? 
In Fig. 1, what is the percentage w.r.t. on the x-axis?

Limitations:
No potential negative societal impact is found in this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of evaluating time series imputation methods in terms of the performance on downstream tasks. It proposes a fine-grained metric and uses RKHS to efficiently estimate the metric. Experiments demonstrate that the proposed method achieves better estimation than the influence function approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: This paper studies an interesting perspective for the evaluation of imputation methods: performance on downstream tasks. 

S2: This paper borrows an RKHS-based method from the field of model explanation. This may be of independent interest even for a broader research community.

S3: The authors have provided documented code to facilitate reproducibility.

Weaknesses:
W1: The proposed Section 2 & Appendix A are basically Sections 4.1 & 3 in [28], respectively. Thus, the paper should clearly attribute these contributions to [28]. 
- [28] Tsai et al. Sample based explanations via generalized representers. NeurIPS, 2023.

W2: Since this paper focuses on evaluation, it should provide a comprehensive evaluation. However, this paper mainly uses older imputation methods, and a number of latest imputation methods are missing (e.g., [SPIN,PoGeVon]).
- [SPIN] Marisca et al. Learning to reconstruct missing data from spatiotemporal graphs with sparse observations. NeurIPS, 2022.
- [PoGeVon] Wang et al. Networked time series imputation via position-aware graph enhanced variational autoencoders. KDD, 2023. 

Minor issue: Some sentences are unclear and hard to understand. For example, Lines 103-104 wrote ""$X_i^{D\times L_1}$ corresponds to ... $L_1$ temporal intervals,"" but what they actually mean seems to be ""one time range of length $L_1$."" It might be better if the authors further polish their writing to improve clarity.

Limitations:
L1: The proposed method uses RKHS to approximate the function. However, it seems difficult to check in practice whether the approximation is accurate or not. 

L2: The problem setting does not seem to be well motivated. As what people usually care about is just an aggregated metric (like Table 2), it is unclear why it is worthy to evaluate in such a fine granularity (i.e., I(i,l)) using more computation. 

L3: This paper focuses on missing labels but does not consider missing input data. As the authors have dicussed in Section 1, missing input data can have a big impact on forecasting results if the time of missing data is close to the test data.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a strategy that evaluates the effectiveness of various imputation methods used to fill missing values ​​at different timestamps (time series). The effectiveness of each imputation method is evaluated based on the downstream task gain. Subsequently, rather than filling missing values with a single imputation method, each missing value at a specific timestamp is filled with the most accurate imputation method at that timestamp.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Originality: The originality of this work lies in the fact that, unlike previous works that study the impact of missing value in inputs, this study evaluates the impact of missing value in labels (time series). Moreover, it proposes an alternative solution that does not use a single imputation method but several that maximize the gain of the downstream task by providing the most accurate imputed value at each timestamp;

- Quality: The paper quality is good. The document is well-structured;

- Significance: The reviewer believes that the proposed solution could have a significant impact in the community. Indeed, one of the inherent problems with time series data is missing values. The proposed solution allows for an optimal choice of various imputation models or strategies to fill these missing values. Moreover, their solution is time series forecasting task-agnostic.

Weaknesses:
- The experimental section 3.3.1 is unclear and very difficult to follow. Authors must refer to the questions to know the reviewer's concerns;

- The proposed strategy may fail when the labels are multivariate time series (MTS). Indeed, in the case of MTS, many imputation methods/models (e.g. deep learning models) consist of filling missing values based on all observed values, i.e. within and across streams. Consequently, combining these imputation methods/models with others that fill in missing values by considering only the observed values of the corresponding univariate time series (e.g. the empirical mean) can lead to a high variance in the imputed values. This high variance will introduce noise into the model calculation scheme and may affect the gain of the downstream task;

- Using the mean as the sole baseline is not sufficient. Authors should adopt at least two baseline imputation methods to perform robust experiments;


- The limitations of the model are not discussed. The authors simply indicate the directions for future work.

Limitations:
- Unless the reviewer has missed something, it appears that the proposed method is only applicable when the labels are univariate time series. This makes it less generalizable, since in many real-world forecasting applications the objective is to forecast multivariate time series at consecutive discrete timestamps;

- The reviewer would suggest adding the computation time to obtain the estimation gain for each model in Table 2;

- Please correct the typo on line 139, Taylor expansion;

- The reviewer suggests that as a future work, the authors empirically evaluate the gain obtained by combining imputation methods/models that adopt the same or different approaches to filling missing values. For example, it would be interesting to see how the emprirical mean imputation works with GRU-D[1], which also uses the empirical mean to fill missing values. Another example could be to study the gain obtained when imputation models based on graph neural networks and message passing are used exclusively or combined with other imputation models, such as those based on recurrent neural networks.


[1] Z. Che, S. Purushotham, K. Cho, D. Sontag, Y. Liu, Recurrent neural networks for multivariate time series with missing values, Scientific reports 8 (1) (2018) 6085.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a task-oriented time series imputation evaluation approach that assesses the impact of different imputation strategies on downstream forecasting tasks, rather than just the accuracy of the imputed values. The authors introduce a similarity-based method to efficiently estimate the impact of imputed values on downstream task performance, and develop a time series imputation framework that combines the advantages of different imputation strategies. The paper also discusses several axioms that the proposed method satisfies, such as the efficiency, self-explanation, and symmetric zero axioms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposes a task-oriented time series imputation evaluation method, focusing on evaluating the impact of different imputation strategies on downstream prediction tasks, rather than just evaluating the accuracy of imputed values. This approach is aligned with practical application needs.

2. The authors propose a similarity-based method that achieves a balance between performance and computational cost, and can effectively estimate the impact of imputed values on the performance of forecasting tasks.

3. The authors have developed a time series imputation framework that combines the advantages of different imputation strategies, and achieves better performance on downstream forecasting tasks.

Weaknesses:
1. The authors state in the abstract that missing values can be observed in many time series analysis tasks, but the task-oriented imputation evaluation method proposed by the authors is only designed for forecasting tasks. In the problem definition, the label of a time series dataset is similarly defined as a time series, however in the classification task, the label is a discrete value. In addition, for some unsupervised learning time series tasks, such as clustering [1], missing values can affect the performance as well. In conclusion, the authors' method is too limited in its scope of application compared to other imputation methods.

[1] Time series cluster kernel for learning similarities between multivariate time series with missing data. PR 2018.

2. The authors' motivation is to provide imputation values that are more beneficial to downstream forecasting tasks, while ignoring the accuracy of the imputation values themselves. However, for the time series imputation task, performing accurate imputations is also one of the non-negligible purposes [2]. The authors ignored the requirement for imputation accuracy when designing their methodology, and did not show experimentally whether their method would reduce imputation accuracy.

[2] Deep learning for multivariate time series imputation: A survey. Arxiv 2402.04059.

3. The authors' ultimate aim is to make more accurate predictions in the presence of missing values, which is highly relevant to the study of robust time series forecasting [3,4]. However the authors don't mention them in the paper and don't compare them as baselines in their experiments. In addition the authors should discuss the advantages and disadvantages of imputation-based and missing value robust-based forecasting methods.

[3] Weakly Guided Adaptation for Robust Time Series Forecasting. VLDB24.

[4] RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies. ICLR 24.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel strategy to evaluate time series imputation methods based on their impact on downstream tasks, without requiring multiple model retrainings. The proposed method leverages a similarity calculation to estimate the effect of imputed values efficiently, balancing performance and computational cost. Furthermore, the authors introduce a framework that combines various imputation strategies to enhance downstream task performance, demonstrating significant improvements in forecasting accuracy.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Unlike traditional methods that focus solely on data restoration, this paper shifts the evaluation criterion to the performance of imputed data in subsequent forecasting tasks. This innovative perspective addresses a significant gap in the field and offers a more practical assessment of imputation quality.

2. The research is supported by thorough theoretical analysis and experiments on multiple benchmark datasets. The authors provide clear justifications for their approach and present detailed comparisons with existing methods.

Weaknesses:
1. Although the paper introduces a retrain-free method to estimate the impact of missing labels, it does not thoroughly discuss the computational efficiency of this approach compared to traditional methods, particularly for large-scale datasets.

2. The motivation of this paper is not very clear: why considering the impact of missing values as labels in downstream forecasting tasks is a valuable formulation, and why is this the optimal way to combine existing advanced methods

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
AthZ2g6VE2;"REVIEW 
Summary:
This paper proposed LoCoDL, an algorithm than combines communication compression with local training. The authors proved the convergence results under regular assumptions, achieving comparable rate with existing SOTA algorithms. The experimental results also show that LoCoDL behaves best among tested algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieves SOTA for large $n$ and nearly SOTA for small $n$.
3. The algorithm behaves empirically better than ADIANA, an existing theoretically SOTA algorithm.
4. The algorithm is simple.
5. The target problem setting is novel and general.

Weaknesses:
1. Throughout the four experimental settings, the number of nodes, $n$, is comparable to the feature dimension $d$. As the convergence rate of LoCoDL is suboptimal when $n$ is small, I believe it important to compare LoCoDL with ADIANA when $n$ is at least 10$\times$ or 100$\times$ smaller than $d$ to see whether LoCoDL beats ADIANA in these scenarios.
2. The experimental datasets are relatively small. It's recommended to conduct experiments on MNIST or larger datasets.
3. It is not easy to capture the intuition behind each algorithm line. It's recommended to give more detailed explanations on how the algorithm is developed.

Limitations:
As stated in the conclusion part, the algorithm is limited to single-directional, deterministic setting without partial participation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LoCoDL, a new GD-based distributed training algorithm that employs both communication compression (CC) and local training (LT). It achieves double acceleration and a SOTA convergence rate for strongly convex problems.

A crux of the algorithmic improvement is maintaining two local estimates, intuitively enabling efficient LT (similarly to SCAFFOLD) and efficient CC (i.e., compressing values' differences instead of values themselves).

The paper offers a thorough theoretical analysis that proves the main claim and conducts some experiments that show LoCoDL's benefits compared to previous algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is timely and important, and I enjoyed reading it. It appears to set a new bar for distributed communication complexity in the strongly convex case (and with full participation?).

While some works assume similarity between local client functions, this work allows these functions to be arbitrarily different.
Also, interpreting the added term $g$ is intuitive and compelling (viewpoints 1-4). 

The theoretical claims are rigorously proven, and some experiments demonstrate the efficiency of LoCoDL compared to previous techniques.

Weaknesses:
The practical applicability of LoCoDL is unclear. Namely, it does not apply to NNs and possibly is less efficient for partial participation use cases (this part is unclear). Either providing concrete evidence of why this contribution is important for modern practical use cases or slightly rephrasing the paper as a theoretical (and important) contribution would strengthen the claim.   

Strengthening the evaluation section is also advised. The submission would be strengthened if the author provided an experiment other than logistic regression to demonstrate the efficiency of LoCoDL in another task.

Additional points: 

1.	“are smooth,  so their gradients will be called. “ This sentence is unclear.

2.	“is slower than  broadcasting the same message to an arbitrary number of clients.” Are there any real FL systems that employ broadcasting? Or do you mean sending the same message? (The term “broadcast” may be confusing here.)

3.	“In this work, we focus on the  uplink communication complexity, which is the bottleneck in practice.“ The second part of the sentence should be softened or extended with real evidence that this is the case. 

4.	“No other compressor can be used, which notably rules out any type of quantization.” Why is this the case? Why quantization cannot be applied according to the selected pattern? 

5.	“Instead of the cumbersome permutation-based compressor of the latter.” is there a specific challenge in implementing permutation-based compressors? Maybe it's worth specifying specific setups where this is insufficient or cannot be applied.

6.	“Thus, LoCoDL sets new standards in terms of communication efficiency. “ Do you mean: our experiments indicate that...?

7.	What is the communication complexity with partial participation (PP) (i.e., $\rho=1$)? When considering PP, is LoCoDL the current SOTA, or are there better alternatives? 

8. It seems that some elements of LoCoDL have some similarities to DoCoFL [1], which also uses an anchor for the model that allows clients to obtain only a compressed correction to that anchor. Can the authors shed light on this similarity?

[1] Dorfman, Ron, et al. ""DoCoFL: Downlink compression for cross-device federated learning."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The paper does not have a dedicated limitation section. The conclusions section discusses potential future extensions. Outlining the limitations clearly is advised. For example, is the PP use case relevant here?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm (LoCoDL) that leverages two well-known methods of local training. It reduces the communication load in distributed learning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper addresses the interesting problem of distributed learning.

Weaknesses:
1. Generally, compressing the error and feeding it back to the updates is a well-known technique to reduce the variance in distributed learning. The idea of the algorithm is marginal with respect to the previous known algorithms (feeding back the error and aggregating with proper coefficients). 
2. Also, the experiments should include the accuracy versus iteration (or time) to see after how many iterations (or how much time), the performance shown in Figure 1 is achieved. So, there are lots of work to improve the experimental part.

Limitations:
justification, experiments

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
SOxxGATGsl;"REVIEW 
Summary:
The paper investigates a multi-armed bandit problem where the action space is a metric space a stochastic Lipschitz rewards. The authors present algorithms that use a constant amount of memory and achieve a near-optimal regret. This improves on previous results that had heavy memory usage.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good presentation, and the figures in the appendices are helpfulץ The contribution itself is useful in practice.

Weaknesses:
While the result is great, the ideas presented in the paper are modifications of existing methods

Limitations:
I did not find the limitations presented as sufficient and I would like to see more discussion on the downsides of the presented algorithm and future directions of research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Lipschitz bandit problem with a memory constraint. There are two algorithms proposed by the authors. The Memory Bounded Uniform Discretization (MBUD) algorithm uses a fixed discretization over the metric space and implements a strategy which explores first and then commits to an exploitation phase. The second algorithm, called Memory Bounded Adaptive Discretization (MBAD) , swaps arms in and out of the memory while creating a mesh over the metric space adaptively (ala zooming). The authors prove upper bounds which match lower bounds from previous work for Lipschitz bandits without memory constraints while maintaining linear time complexity and constant space complexity. Finally, the authors perform experimental validation of the theoretical results on small 1-dimensional datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel problem formulation in the Lipschitz bandit setting.
2. The authors show upper bounds matching with lower bounds from prior work while maintaining a memory budget on arms.
3. The concepts introduced in the paper are well explained for the most part. I did have a little trouble reading the parts about the crosscut and generating cubes but it might just be me not being familiar with prior work in the area.

Weaknesses:
I am unclear about the novelty and contributions of the paper. The problem formulation (limited memory) is new in the Lipschitz bandit setting but it has been studied in several papers in bandits with finite arms (as the authors point out in the related works). Moreover, the proof techniques used in the paper appear standard - MBAD is based on zooming introduced by kleinberg et al., the clean event analysis is from the recent textbook of Slivkins (and their papers), MBUD is based on an explore first strategy resembling the naive Explore-then-Commit algorithm (which trivially satisfies the O(1) memory constraint). In all, I’m not sure what specific parts of the paper are being claimed as novel vs that from prior work.

The experiments in this paper are very limited - only a 1 dimensional interval with an L1 metric. To show real world applicability, it would be nice to have results in higher dimensions and also on real world datasets (since that was the original motivation).

Minor/Typo:
I think the caption for Fig 1 should clarify what is on the X and Y axes. It is obvious from context but it would be nice to have from a readability perspective.

Limitations:
The Lipschitz constant needs to be known beforehand to apply these algorithms.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two algorithms for Lipschitz bandit problems, with improved time complexity and memory requirements.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the algorithms and proofs are sound, then this is an excellent contribution. Developing these sort of streaming/sketching methods for key bandit problems (such as Lipschitz bandits) is an important area of research, and many people are likely to care about the results of this paper.

Weaknesses:
The paper is sloppy to the extent that it is difficult to understand the authors' algorithm or verify their claims. To be specific, consider the following sentences, all taken from a single two-paragraph subsection (section 2.2):
1. ""Let $\\{\mathcal{X}_1, \dotsc, \mathcal{X}_N\\}[\mathcal{X}_i \subset \mathcal{X}]$ be an cover of the action space $\mathcal{X}$"" --- okay, what is the $\\{\dotsc\\}[\dotsc]$ notation?  
2. ""Let $\epsilon$ denote the maximum diameter of $\mathcal{X}_i$ for all $i \in [N]$."" --- okay, but what is a diameter? Are we in a metric space? This hasn't been specified. 
3. ""Then the arm set $S = \\{ x_i \mid x_i \in \mathcal{X}_i, i \in [N]\\}$ is an $\epsilon$-mesh.""  --- what set is this? Now, I presume that the authors mean to say that they want $S$ to be any set that contains a single element chosen arbitrarily from each of the $\mathcal{X}_i$, but that's not written, instead the authors said its __the__ set, but the right hand side does not specify any unique set. Also, the concept of an $\epsilon$-mesh has not been defined, and when its defined, it needs to be with respect to some metric. And if this description was meant to be the definition of an $\epsilon$-mesh... then that's not clear either (and the definition given by the work the authors state these definitions are from, i.e. Slivkins 2019, is _very_ clear---all the authors needed to do was copy it).
4. ""The covering dimension $d$ of the action space $\mathcal{X}$ is defined as $d=\inf_{\alpha \geq 0}\\{|\mathcal{S}| \leq \epsilon^{-\alpha}, \forall \epsilon > 0\\}$."" But the set $\mathcal{S}$ does not depend on $\epsilon$ (not even implicitly)... (the correct definition, I presume, would be to ask that $\mathcal{S}\_{\epsilon}$ is a minimal $\epsilon$-cover of $\mathcal{X}$ in some metric $D$, and then have that infimum include $\mathcal{S}_\epsilon$ and not $\mathcal{S}$.)
5. ""Define $\mathcal{Y}\_j = \\{x \in \mathcal{X} \colon 2^{-j} \leq \Delta(x) \leq 2^{1-j}, j \in \mathbb{N}\\}$, then the set $\mathcal{Y}_j$ contains all arms whose gap is between $2^{-j}$ and $2^{1-j}$."" --- but $j \in \mathbb{N}$ is within the constructions of the set on the RHS, which could be read as asking that the condition holds for all such $j$, or for some $j$, but it breaks the dependence of the right hand side on the subscript $j$ of $\mathcal{Y}_j$. Of course, the definition shouldn't have the $j \in \mathbb{N}$ inside the $\\{ \dotsc \\}$ on the right hand side of the definition. 
6. ""Consider the $\epsilon$-mesh $\mathcal{S}_j$ for space $\mathcal{Y}_j$."" --- __the__ $\epsilon$-mesh? Also, $\mathcal{S}_j$ hasn't been defined. This should say instead 'fix some $\epsilon > 0$ and let $\mathcal{S}_j$ be an $\epsilon$-mesh of $\mathcal{Y}_j$', or something like that.
7. ""[...] the zooming dimension focuses only on the set $\mathcal{Y}_j$"" --- no, the zooming dimension depends on all the sets $\mathcal{Y}_1, \mathcal{Y}_2, \dotsc$, not only a single one of those sets.

While each individual mistake or ambiguity can be resolved easily enough, verifying the authors claims would require me to rewrite everything myself, and this goes beyond what I'm willing to do (and should do...).  The whole paper is like this, and it's just not acceptable.

I would urge the authors to, in the future, have someone _not intimately familiar with the work_ proof-read the work.

Note, I put down confidence as ""5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."" --- I am indeed very familiar with the related work, but I have not checked the math/other details carefully. It's too much work to read it. I am absolutely certain, however, that this level falls short of any level of clarity that might be expected in published work.

Limitations:
.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers regret minimization for Lipschitz bandits with time horizont $T$ and proposes an algorithm that provably achieves nearly optimal regret while having strictly smaller (by a factor of $T$) time (of order $O(T)$) and memory complexity (of order $O(1)$). This is achieved by considering a tree-like embedding of the state space and pairwise comparison between elements of the tree. A suboptimal method with uniform discretization called MBUD has dependence on the covering dimension of the state space, while MBAD, a method with adaptive discretization, instead has dependence only on the zooming dimension.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Achieving nearly optimal regret bounds in minimax (MBUD) and instance-specific setting (MBAD), while reducing time and memory complexity. 

2) Both proposed algorithms are non-trivial and seem to be novel and interesting on their own.

Weaknesses:
1) It is not simple to parse algorithms in their current form in a short amount of time. Although you give comprehensive descriptions in text, I believe adding illustrations or additional explanations will significantly improve clarity of your algorithms. 

2) I would appreciate a more explicit comparison with previous work - what parts of the algorithms were already reported in the literature?

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
pje1Y71jad;"REVIEW 
Summary:
1. This paper proposes a cost-efficient strategy named ""Coke"" to automatically assign the most promising model for particular questions.
2. Experiments show the effectiveness of their method in Knowledge-based question answering (KBQA).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem definition and mathematical explanation is clear.
2. Experiments on 3 representative domain-specific datasets show the effectiveness of their method to improve performance and cost efficient.

Weaknesses:
1. As for the calls (times), I doubt it is a good metric because in generally users care more about call latency and commercial products always define token numbers to calculate price. (longer calls should have a larger price)
2. Maybe not only LLM cost but also KGMs' cost should be considered
3. 3 datasets are similar in the sense of reasoning, the generalizability is limited.

Limitations:
The authors discuss about the limitation of their work in one section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a method for deciding whether to use a Large Language Model (LLM) or a Knowledge Graph-based Model (KGM) to solve various Knowledge-based QA tasks in an episodic manner, based on historical data. The main goal is to achieve better performance at a lower cost throughout the entire QA process. This work formulates the problem as a multi-armed bandit problem and proposes a solution to this formulation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The problem addressed is interesting. The approach of using multi-armed bandit problem formulation to decide between the efficient use of small models leveraging KG and the effective use of LLMs is novel. There appears to be no prior work addressing this specific problem.
- **Quality:** The authors have appropriately formulated the problem and attempted to solve it using technically sound methods. The experimental results convincingly demonstrate the advantages of the proposed method.
- **Significance:** This work is likely to inspire future studies in system cost optimization especially for LLMs. The experimental results highlight the potential to reduce costs while improving overall performance, which is impressive.

Weaknesses:
- **Clarity:**
    - It is unclear if the cost in the experimental results is solely based on the number of times the LLM is used, with KGM usage considered cost-free. Additionally, the cost associated with using models like RoBERTa for Context-aware expert distinguishing, as discussed in Section 3.3, is not addressed in the experiments. Ignoring these local model costs might not be appropriate, and related discussions should be included in the experiments section.
    - The specifics of the dataset usage are also unclear. The results in Table 1 seem to imply that KGM's arm embedding was updated using fine-tuned train data and then tested directly on test data, but the experimental setup isn't clearly explained.
- **Significance:** There are questions regarding the practical applicability of the proposed method. The QA datasets used in the experiments are multiple-choice QA datasets, making accuracy measurement straightforward and benefiting from extensive research on KG-based models. However, applying this method in real-world applications like chat-bots poses challenges that need to be addressed. Despite this, the research lays a foundation for practical follow-up studies, which is a positive aspect.

Limitations:
The authors already addressed the limitations in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This manuscript presents a novel cost-efficient strategy to leverage LLMs for knowledge-based question answering. It could balance both inferential accuracy and cost saving. Several SOTA methods, inlcuding both traditional KGQA methods and LLMs are combined since KGQA models are small and knowledgeable but less accurate, while LLMs are comprehensive on general questions but rather expensive. Authors design a cluster-based TS technique and a tailored contextual MAB to filter out the experts and constrain the decision with the consideration on cost regrets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
This propsed method has fhe following strengths:

1. Saving costs of invoking LLMs while obtaining higher accuracy is a promising topic with both academic merits and industrial values. It could be somehow inspiring various research communities.

2. The proposed methodology is novel and reasonable. It considers the model selection based on three aspects: the accuracy potential of choosing one model based on historical success, the expertise on particular questions based on the question semantics and the cost regret from the expenditure on historical failure to control the costs.

3. Sufficient theoretical analysis and proofs, e.g., expectation bound of Thompson Sampling and the confidence bound for MAB to correspondingly support the design of automatic selection.

4. Nicely drawn running example is easy for grasping the motivation. It provides a sketched overview of the pipelines, the performance/cost comparison of existing methods and the overlaps among different models. This motivates the combination of KGMs and LLMs with a auto-selection algorithm.

5. Satisfying experimental performance on both accuracy and cost saving performance.

6. The writing is good and clear to follow.

Weaknesses:
1. A pseudo-algorithm should be provided to demonstrate the selection process and how the Thompson sampling and MAB decides in each step.

2. The results are currently from one fixed combination of base models, i.e., HamQA + ChatGPT+ GPT4. It will be more solid to provide the results of different combinations of base models as additional ablation studies. For example, I may like to see the results with GreaseLM + HamQA as the clustered arms for KGMs and ChatGPT+GPT as the ones for LLMs.

3. Missing references for ChatGLM Baichuan and Llama2/3 in the main table.

4. More heuristic baselines should be included, for instance, (1) $E_c$ + random $E_a$; (2) random $e_c$ + random $E_a$ (3) pure random selection without $E_c$, $E_a$ and $R_a$ (4) epsilon greedy-based selection with $E_a$ only.

Limitations:
Yes.

The limitation is acknowledged by the authors, which is from the performance constraint by base models. It should be easy to address by replacing the base models with more advanced ones. While the selection of models requires much prior knowledge of performance, this may be a concern on additional resource consumption.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors proposed a strategy to switch between LLMs and KGMs when performing Question Answering, aiming to optimize both cost and accuracy. The evaluation is performed on three datasets: CommonsenseQA, OpenBookQA, and MedQA.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors address an important and practical problem, which is cost-saving in QnA. The introduction is well-written, and the intuition is nicely presented. The proposed idea to switch between KG-based models and LLMs depending on different questions makes sense and shows some promising results on the test datasets.

Weaknesses:
It lacks implementation details such as how the model is trained, which models are being used as LLMs and KGMs, and the exact formula/implementation of the cost functions.

The proposed solution is based on cluster-level Thompson Sampling, but it lacks an explanation of why it's the best candidate for this setting. Additionally, defining the cost function as the number of calls is less practical; instead, it should be a function of latency and API fees (assuming the use of GPT-4).

The result of cost-saving is promising but not as high as expected. From Figure 1c, it seems that about half of the questions can be answered by KGMs. However, the experimental cost-saving shown in Table 1 is only 10%-20%. This suggests there is still room for improvement in the decision-making model.

There is almost no benefit when using it on MedQA, so one weakness of the solution is that it heavily depends on the performance of the KGMs to save costs

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
I3kIEjoON6;"REVIEW 
Summary:
This paper presents a framework for crystal structure generation, focusing on polymorphs. The framework utilizes matrix representation of crystals and various generative models used in vision tasks, with specially designed similarity metrics and loss function. It is tested on (1) modification of given structures and (2) generation from scratch within the dataset, as well as finding new structures in the Ta–W–B system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work develops domain-specific representation, metric, and loss, so that generative models that have proven useful in image generation can apply to crystals. The topic is timely and important.

Weaknesses:
- In the demonstrated use cases, the generation is conditioned on elements, space group, etc., but not materials properties of interest. These show limited usefulness in materials discovery and design.
- The matrix representation does not take physical constraints into account, e.g., space group determines symmetries in the lattice parameters. Besides, related previous works, e.g., [UniMat](https://openreview.net/forum?id=wm4WlHoXpC), should be discussed.
- The clarity and rigorousness need to be improved (see Questions). The mathematical notations are not unified, e.g., $x$ vs $X$.

Limitations:
Discussed in the Conclusion.
Besides, Sec. 9 contains a GitHub link that could break anonymity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the use of diffusion and flow matching approaches for the generation of crystalline materials. The authors trained UNet models on polymorphs in the AFLOW database (which has a series of DFT-computed properties for these materials) using either simple R3 regression, diffusion/flow matching. The authors then presented inference results on similarity to training structures (Section 3.4, which shows these methods can reproduce training structures to different extent), and showed that a subset of the modified generated crystals (with Ta, W, B) can have a small non-zero formation energy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
*Originality*: The authors attempted to study the problem of crystal structure generation with no invariances/equivariances other than periodic translation invariance. 
*Quality*: The authors attempted to use DFT to validate some inference results.
*Significance*: Crystal structure generation (especially synthesizable ones) is an important problem. It seems that training from uniform noise distribution works better for CFM than training on Gaussian noise, contrary to the established results in the field.

Weaknesses:
*Originality*. The manuscript lacks originality. The diffusion/flow matching techniques are well-established in inorganic crystal structures (e.g. CDVAE cited here, DiffCSP/FlowMM that's not here). Sure, using a network architecture not designed for materials/crystals and using no invariances/equivalences is new, but it deviates from standard practices in the field without sufficient justification. I believe the implementations shown in the paper are a great exercise for practitioners interested in the field, but unfortunately, I do not see it as a NeurIPS paper.

*Quality*. The manuscript is _very_ bare-boned, making a comprehensive technical critique challenging without appearing disproportionately critical. 
- On the ML side, there are numerous large fallacies/mistakes (e.g. no consideration of bonds between atoms at all, Sec. 3.3 there is no description of the PBC loss, the generation does not consider the unit cell, no generation with atom types, and there is no investigation of any experiments observed e.g. why is uniform noise better for CFM, the result in Table 1 appears to evaluate overfitting rather than novel generation, the list can go on). 
- On the chemistry/validation side, there are again numerous problems (why would formation energy be given during the generation process, what functional did you use in DFT, there are no comparisons against existing structures and hence cannot be claimed as novel, etc.) 
- There is no comparison against _any_ known methodologies. 
- The results overall, are very weak both in ML and in chemistry (e.g. Table 3 shows most if not all materials generated have extremely large positive formation energies despite the simple elemental composition; the remaining few negative ones are at the brink of instability, in any case they likely would not be synthesizable).  

*Clarity*. The manuscript suffers from poor presentation, starting with a promotional-style title that lacks scientific descriptiveness. I unfortunately do not understand the novelty of the paper in comparison to existing methods. The paper consistently fails to provide essential explanations across both machine learning and chemical methodologies. 
- On the ML side, there are numerous things poorly presented (e.g. Figure 1 is just periodic translation invariance and in a typical manuscript would be summarized in one sentence). 
- On the chemistry side, things are greatly exaggerated (e.g. computationally making a few materials with negative formation energy can be done by undergraduate students and certainly does not warrant descriptions such as 'This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials)

Limitations:
Partially.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the inverse problem of generating crystal structures based on given properties, thereby avoiding the need for extensive computational resources typically required in traditional methods. The authors utilized the AFLOW materials database, selecting unstable and stable series of structures for two specific tasks: modifying structures to achieve stability and conditional structure generation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors experimented with various generative model approaches and evaluated two tasks in crystal generation. Additionally, they integrated the VASP software for application testing and successfully identified four previously undiscovered stable structures through conditional generation.

Weaknesses:
1. From the perspective of model application, although the authors used the AFLOW database for their study, they did not compare the data range and coverage with other significant databases like the Materials Project. This omission leaves a gap in understanding how the generative models perform across different datasets and whether the results are consistent and generalizable. Comparing the performance of the same generative models on different databases could provide valuable insights into the robustness and applicability of their approach.

2. There is a partial break of anonymity in the GitHub link on Page 9 in this paper.

Limitations:
The authors proposed two major directions: conditional generation and conditional modification. There is room for improvement in both the experimental results and the data used for conditional modification. For example, they could consider recognizing unit cells with translational and rotational transformations and introducing more ways to assess generation results.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with an important application of generative models for science: generation of crystalline structures. However, I have some serious concerns. First, scope. While comparing different methods for the same objective is informative, I am not so sure what is the purpose here. Having so many different methods certainly dilutes the main message in a short paper format like neurips. Second, approach. From what I understand, there are questionable design problems with the technical approach. Third, results. The authors spend most of their space explaining various methods such that there is little room left for explaning the impact of their results, or comparing their results with existing approaches. Finally, presentation. Figure 1 is kind of trivial or at least very simple and I am not sure it is worth a separate figure. The overall typesetting looks not too professional.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The methodology selection is broad and hopefully the audience can benefit from a mini-benchmark of different generative approaches. The overall model architecture is distinctive from what I have seen in the literature.

Weaknesses:
I have a good number of questions on the technical approaches. More specifically, the model takes a specially formulated data structure that does not seem to be obviously invariant or equivariant under permutation, translation, rotation, which is concerning. For example, change the selection or ordering of the unit cell vectors and everything will change in an uncontrolled way.

The conditioning approach seems to be to provide desired properties as inputs to the generative approaches. I am not sure this always make sense. For example, if one desires a certain space group, there is no enforcing compliance with the space group. One can always easily check it. It is perhaps more suitable to enforce space group compliance using a guidance-based conditioning approach.

Limitations:
There is insufficient discussion about the limitations given my concerns shown above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
tRjgapiCpm;"REVIEW 
Summary:
The paper presents a heuristic approach for evaluating the privacy of DP-SGD when only the last model iteration is released. This method contrasts with traditional analyses that consider all intermediate updates, offering a more practical assessment for scenarios where adversaries only access the final model. The proposed heuristic is experimentally shown to provide reliable privacy leakage estimates, making it a valuable tool for pre-audit privacy assessments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Focus on a good and important question.
2. Good explanation and clear paper layout.
3. Propose a new analysis neither from the theoretical nor empirical point of view.

Weaknesses:
I think the proposed method is interesting and new but I still have some questions.

1. I know the linear loss function assumption is common in theoretical analysis but it seems that the proposed method wants to have contributions in the empirical case, so why still make the linear assumption? 

2. While I appreciate the effort to introduce a Heuristic analysis, I remain skeptical about its necessity and effectiveness. The primary benefit of theoretical analysis is its precision and rigor, which often include the flexibility to adjust bounds as needed. If the goal is to find a more relaxed lower bound on privacy risks, this can often be achieved by simply loosening the constraints within the existing theoretical framework. Introducing a separate heuristic analysis seems to complicate matters without providing clear advantages. 

3. I do not think you are using a correct baseline. When you make that only the last iteration model can be seen assumption, it is not fair to use normal DP-SGD analysis. I think it is better to use the theoretical analysis from those hidden state papers you cited. I am curious if you compare your proposed method with those methods, will you still get the same conclusion? 

4. I find Table 1 in the paper somewhat unclear and would appreciate further explanation from the authors regarding its purpose and implications. The table suggests that similar levels of heuristic ε are achieved across varying batch sizes, yet there is a noticeable increase in the standard privacy budget for smaller batches to maintain comparable performance. This observation seems to underscore the well-known impact of batch size rather than demonstrating an advantage of the proposed heuristic method.
Could the authors elaborate on how this data relates to the efficacy of the heuristic analysis?

Limitations:
Please check the weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a heuristic privacy analysis of releasing only the final model iterate of differentailly private gradient descent (DP-SGD). The analysis is based off of the worst-case differential privacy guarantee of DP-SGD with linear losses, under the assumption that the heuristic can be applied to more general loss functions in order to approximate the privacy loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The premise of the paper (a heuristic privacy analysis of releasing only the final model iterate of DP-SGD) is very interesting, and Theorem 1 is a cool result.
* The paper thoroughly assesses the limitations of the heuristic (in Section 4).

Weaknesses:
* I don’t know how useful the heuristic analysis would be in practice — beyond a lightweight sanity check — since ultimately it is just a heuristic and not a rigorous upper or lower bound on the privacy loss.

* The empirical study of the heuristic looks to be very thorough, but sparse on interpretation. I would have appreciated more discussion on the figures, and didn’t really feel like there was a strong take-home message from the paper.

* Algorithm 1 is DP-SGD with a regularizer, but in practice it is somewhat rare to use explicit regularization with DP-SGD. So I’m not sure that the heuristic would be widely applicable to the more common implementation of DP-SGD without regularization.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a heuristic privacy analysis for DP-SGD that focuses on releasing only the last iterate, as opposed to all intermediate iterates. The authors argue that this approach is more realistic and provides sharper privacy guarantees in practical scenarios. The heuristic is based on a linear structure assumption for the model and is validated experimentally through attacks/privacy auditing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written. The paper introduces a new heuristic analysis of DP-SGD for linear loss functions and also critically examines its limitations, and identifies areas for further research.

Weaknesses:
To my understanding, this paper offers a tighter privacy accounting analysis specifically for linear loss functions. However, I find its applicability limited since it cannot be extended to general ML tasks where the loss functions are not linear. Additionally, the fact that the privacy adversary has access to all intermediate iterates of the training process makes DP-SGD overly conservative is quite well-known. The main challenge remains in developing tight privacy accounting analyses for iterative algorithms like SGD.

Limitations:
The limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide exact DP guarantees for cases where only the last iterate of DP-SGD is shared with the malicious clients, and linear models with linear loss functions are used. They propose their DP bound to be used as a heuristic that approximates the true DP guarantees for cases where more complex models are used. They show that for normal DP-SGD training, the predictions of their heuristic fall between the standard DP bound computed under the assumption that all intermediate iterations of DP-SGD are shared with the attacker, which is a strict upper bound of the true DP guarantee when only the last iterate is shared and DP-SGD with full batches and only last iterate sharing. They also compare their method against SoTA DP attacks and show that under most circumstances, their heuristic value for the DP is higher. They suggest that this is the result of the attacks not being good enough at precisely estimating the true DP guarantees. Finally, the authors demonstrate that their heuristic under unrealistic circumstances can underestimate the true DP guarantee but argue this only happens under hand-crafted losses and gradient updates, which do not happen in practical circumstances.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The last iterate setting is important
- The linear function DP bound is exact 
- The linear function DP bound has interesting properties 
- The counter-examples for the DP heuristic themselves seem interesting and probably can be adapted to other settings

Weaknesses:
- I am confused by L234. The authors propose to maximize their heuristic over all $t\leq T$, while beforehand (e.g. in Figure 1/Section 2) they advocated to computing the heuristic for a single $T$. Which one is the exact proposed heuristic by the paper?
- In Figure 2, I am not sure how we adapt existing techniques to the last-iterate-only setting? Can the authors explain in more details?
- Can the authors explain in Figure 1, what network and dataset were used?
- The authors do not provide code. I am not sure about the reason, but I will give them the benefit of the doubt that the reason is indeed related to anonymity 

**Nits:**  
- Eq. 8. I assume you do indexing from i = 1. In that case, $A_{T-i}$ should be $A_{T-i+1}$ instead. If you do 0-based indexing, even more fixes to the equation are needed.
- I believe Eq. 7 should be multiplied by $\eta$ on the right-hand side
- Equation at L442, left-hand side should be $m_T$ not $m_t$
- I believe the last equation at L459 should have $(1-q)^{n-k}$ instead of $(1-q)^{k}$. I also believe $n$ is $T$ in this equation
- The definition of $l(m)$ in L109 is confusing as $m$ is considered input to the function, while in the rest of the paper $m$ is used as a parameter. Consider putting $x$ instead.
- Consider defining the hockey-stick divergence in terms of both its pdf and cdf in the appendix to ease unfamiliar readers. I had to read quite a bit on my own to understand it. 
- Consider adding some information in the appendix as to how to deal with the mixed discrete-continuous probability for $P$. I assume many readers will be unfamiliar. 
- Consider deriving the formulas for $P$ and $Q$ in Section 4.2 in the appendix. They are not obvious. 
- Consider having an appendix section that quickly recaps how [NSTPC21] and [NHSBTJCT23] work. Their operation is critical for understanding Section 3. I ended up reading them to get an idea of what was going on there.

Limitations:
The authors acknowledge the limitations of using the heuristic to compute the DP bounds

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
EiIelh2t7S;"REVIEW 
Summary:
The paper investigates the role of RoPE in long-context LLMs. It highlights that the base of RoPE crucially affects the model's ability to handle long contexts. This paper derives a theoretical (and empirical) lower bound for the base value required to maintain long-context capabilities and validates this through empirical experiments with models like Llama2-7B and Baichuan2-7B, and a 2B model trained from scratch. This work offers insights into the base of RoPE for long-context processing in LLMs, which is inspiring for the development and design of long-context LLMs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The lack of long-context ability is still an under-explored question and a very important one. Thus, the research topic of this paper is both theoretically important and practical.
2. The relationship between the RoPE base and long-context ability is important, and the proposed lower bound is interesting and inspiring.
3. The experiments in this paper are comprehensive and well support the claims made in the paper.
4. The presentation of this paper is overall very good and easy to understand and follow.
5. The claims of this paper are inspiring for the development of long-context LLM.
6. I am also very interested in RoPE-based selection in LLM and like this paper.

Weaknesses:
1. The Desiderata 2. The similar token gets more attention in Section 4 seems intuitively correct but may not be empirically correct. As this desideratum is the fundamental motivation of Theorem 1, a thorough empirical verification is a must-have.
2. The motivation section is not well-written or organized. It confuses me while I read this part. These are just some previous empirical observations and are not deeply discussed. As the motivation part is very important, I would like to see a clear and well-organized motivation.
3. For the Caption of Figure 6, it would be better to show how to derive the value of the dotted line. I would also like to see a detailed derivation here.
4. Will the author plan to release the models (including the fine-tuned Llama2, Baichun2 with varying lengths, and the 2b model series)? This would benefit future work.
5. I wonder about the negativeness and positiveness of the $q^{T}R_{m,\theta}k$ in equations (8) and (9). If the values are negative, say -2 and -1, which one indicates more attention?
6. For section 5.3, I would like to regard this as a conjecture rather than an explanation.
7. Moinries
    - The upper case and lower case of the title are not consistent, such as the title of Section 4.1 and Section 2.2
    - Line 225 “method2” -> “Method 2”

Limitations:
See **Weaknesses**

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
ROPE is wildily employed in popular LLMs which encodes positional information with a rotation matrix. Although RoPE is used to enhance long-context capabilities by adjusting its base parameter to address OOD issues, this paper finds that this may result in only superficial long-context abilities. Authors re-evaluate RoPE's role and introduce a novel property of long-term decay, showing that the base of RoPE limits context length, with an absolute lower bound required for certain capabilities. This work clarifies the theoretical and empirical relationship between context length and the RoPE base, offering insights for future long-context training.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The two provided desideratas are highly logical and well-aligned with language modeling. The assumption made in this paper is quite reasonable and closely aligned with practical scenarios.

- Insightful analysis. The theoretical results presented in this paper are easy to understand. To the best of my knowledge, the final bound for the ROPE base is novel and first introduced in this paper

Weaknesses:
There is no obvious weaknesses in my opinion. I just have a few questions:

- Regarding the ""Desiderata 2 The similar token gets more attention"", recently StreamLLM [1] shows that there exists ""attention sink"" in popular LLMs. Namely most of tokens attend to the first few tokens. This somehow contradicts with the principle that ""the similar token gets more attention"". Could you provide your thoughts on this statement?

- Anthropic's blogs reveal that different heads may have different functionality in in-context learning. How may this interplay with the Rope base? Do you think different heads may have different optimal rope base?

[1] Efficient Streaming Language Models with Attention Sinks
[2] In-context Learning and Induction Heads

Limitations:
See Weaknesses

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the role of Rotary Position Embedding (RoPE) in Large Language Models (LLMs), with a focus on the relationship between RoPE's base and the model's long context ability. 
The study looks into the long-context abilities and limitations of current methods that rely on smaller RoPE bases. With a lower base, LLM models may exhibit superficial long-context abilities, achieving low perplexity but failing to retrieve relevant information in extended contexts.
Theoretically, The paper establishes two desiderata for the attention mechanism in language modeling: 1.Closer tokens receive more attention. 2.Similar tokens receive more attention. It examines these when applying RoPE to LLMs, revealing a long-term decay in attention score and the ability to differentiate similar from random tokens. This leads to a theorem indicating that RoPE’s base sets an absolute lower bound for achieving specific context lengths.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Clarity and Technical Correctness: The paper is clear and technically sound, with theoretical and empirical analyses.

2. Experimental Rigor and Reproducibility: It includes extensive experiments to back up the theoretical findings, along with detailed setup and results.

3. Novel Findings: This paper presents a critical study of whether we should use smaller bases for continuous training, as suggested by previous work. Furthermore, this paper presents a novel perspective on long-term decay, as well as an absolute lower bound on the RoPE base parameter required for specific context lengths. This adds new knowledge to the field and improves our understanding of position embedding in LLMs.

Weaknesses:
Overall, I like this paper. I would like to suggest that the authors strengthen their work by considering the following points:

1. Extensively test the model using benchmarks such as RULER [1].

2. Provide more empirical observations on the relationship between the base of RoPE and model performance on those challenging benchmarks.


--

[1] https://github.com/hsiehjackson/RULER

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Hi Area Chair, I am not qualified for the review of this paper, this is out of my knowledge scope.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
n/a

Weaknesses:
n/a

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
XkMCKoHNCD;"REVIEW 
Summary:
The authors present an analysis of logistic regression on sentence embeddings as a way to predict the speaker of a particular line of dialogue in the Big Bang Theory. Specifically, they fit a PCA model to embeddings obtained from a sentence transformer and then use each PCA dimension as a linear feature. The authors present some qualitative analysis of the most predictive PCA dimensions and some quantitative analysis of the classification accuracy. In addition, they present a brief analysis of the ability of GPT-4 to directly classify lines of dialogue and compare to a limited user study.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The authors have identified an interesting and important debate in the AI community — more tests which help researchers discriminate between mere stochastic parroting and true generalization are certainly needed! In addition, the authors are very thorough in their description of the methods involved and their qualitative analysis of the PCA features is extensive. I also appreciate the thought the authors have given to the limitations of their study and the need for further work.

Weaknesses:
First and foremost, I feel that this paper needs to be much clearer and more focused in its research question. The introduction indicates that the objective of the study is to determine the extent to which the apparent ability of large language models like GPT-4 to generalize to novel tasks is actually attributable to their ability to parrot data from their training. However, a good part of the analysis appears dedicated to the specifics of The Big Bang Theory and the features of its dialogue. Section 3.1, for instance, extensively interrogates the PCA dimensions obtained from the sentence embeddings in a way that feels very specific to the particular dataset. Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add). The connection between these results and the initial framing of LLM evaluation remain, unfortunately, somewhat murky. This is not to say there is no possible link between dialogue speaker prediction and LLM abilities! I encourage the authors to think about this problem more and articulate the specific claim they hope to interrogate.

On that note, and assuming that the main motivation is indeed to study large language models, I feel that the analysis could be strengthened. First, it would be helpful to justify some of the specific decisions made as part of evaluation. For instance, why were the Big Bang Theory and Friends selected over other possible dialogue datasets? Why was dialogue speaker prediction always studied between exactly two characters? Why were these specific characters selected? Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?  When proposing a novel task, it’s important to make the assumptions and decisions that went into the task selection clear.

With regards to human evaluation, I encourage the authors to widen their study. That is to say, a user study which consists of only two participants (both of whom are related to one of the authors) makes it difficult to ascertain the reliability of the results. Indeed, I would suggest a study consisting of a larger number participants (ideally participants who do not have any externally motivating factors like relationships to the authors) so that a more general measure of human ability can be obtained. Further, I think it could actually be preferable for the participants to not have prior experience with the television show. This would make the test more an examination of the ability for participants to generalize their knowledge of personality traits to a novel situation instead of their ability to recall information (which is, ostensibly, closer to the desired research question in LLMs).

Despite these critiques, I hope that the authors continue to refine their research question, justification, and methodology. There are interesting questions to study here!

Limitations:
I feel that the authors have been very up front with the limitations of their work and have situated it in the context of broader impacts.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors are focused on whether or not LLMs can be thought of as stochastic parrots or contain ""Sparks of AGI"". They look into what kind of data is recoverable from internal LLM representations. Specifically, the authors investigate to what extent the task of identifying TV personalities (e.g. Penny vs Sheldon) based on their dialogue lines is solvable using various methods. The authors compare a classifier based on PCA components extracted from existing LLM embeddings, GPT-4 zero shot performance, and human expert judgments. They find that all methods show fairly good performance, with human experts showing best results, followed by GPT-4, followed by the classifier. The authors also present a brief qualitative analysis, interpreting the more prevalent axes of variation in the embeddings identified using PCA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors tackle a very ambitious and important problem. The writing is clear throughout, and the authors provide extensive background for the methods they use.

Weaknesses:
I need to preface this by saying that I hope that my negative review does not discourage the authors from further pursuing the topic. I feel bad for having to reject this paper as it has some good ideas behind it and has an intention of researching a highly important problem. I hope that in next iterations, their work can be improved and expanded. At present, unfortunately, it does not match publication standards. I will try to explain why, and give pointers on how to potentially fix it in the future.

The biggest flaw of the paper is the experiment design. The authors never clearly define what exactly it means to be a ""stochastic parrot"" as opposed to ""general intelligence"". The authors also don't explain how their experiments would help to decide one way or another. So the results we have are impossible to interpret. It would help to go back to the original question and work through the argumentation more clearly. If the internal LLM representations have information about TV personalities, does it make them more or less of a stochastic parrot and why.

Otherwise, the experiments give a very exploratory impression. For example the authors run PCA on sentence embeddings computed on their dataset and interpret the components. But it's unclear why and how this would help to answer the main question the paper attempts to answer.

Additionally, the paper's methods are extremely well-known, but unfortunately, the authors don't refer to relevant literature. The work highly overlaps with the topic of linear and nonlinear probes, as well as with the general theme of transfer learning. In essence, what the authors did can be described as adding a ""classification head"" to a pre-existing LLM. This is a very well-known technique.

If we want to gain new insights into what the models are doing, it is usually more interesting to look into the computations in intermediate layers of the model, rather than the last embedding layer. It is also often desirable to look at causal probes (rather than just a classifier).

Lastly, there are certain writing choices that deviate from common ""conventions"" in academic publishing. For example, oftentimes the authors go into excessive detail on well-known methods (explaining how PCA works and what a covariance matrix is). I highly suggest that the authors look at existing successful papers that use similar methods and copy their approach when it comes to decisions on what to explain in the main text, what to put into the appendix, and what to omit. The general rule of thumb is that newly introduced and important ideas should be at least briefly given in the main text, with extra details given in the appendix. Extremely well-known and established methods such as Principal Component Analysis don't need a full explanation, and a simple reference to the original source is enough.

I really hope that the authors don't get discouraged and try to refine and improve their research in the future. The first starting point would be to more clearly define the problem, and to study in depth the existing literature on linear probes and probing in general, and on investigating what the internal LLM representations contain. One potential starting point is the paper ""Evaluating the World Model Implicit in a Generative Model"", Vafa et al. 2024 and related works.

Limitations:
The authors acknowledge some of the limitations of their study.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper’s main contribution is to apply a logistic regression on the principal components of the LLM embeddings for classifying TV series characters based on their dialog lines. The main finding is the logistic regression approach does worse than GPT-4 in predicting TA characters, but is comparable to human evaluations with two annotators.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The paper focus on an interesting angle of using language model features for predicting the belongings of dialog lines of characters of TV shows.

Weaknesses:
The methodology of using logistic regression over PCA of language model embeddings is not novel, and there's no rigorous quantitative evaluations of the method beyond qualitative examples. The connection of the method and task to the broad discussion around ""spark of AGI"" and ""Stochastic Parrots"" is farfetched.

Limitations:
The paper claims that ""the contribution of the paper is primarily methodological, and their study is limited to a qualitative study of two very specific datasets."" However, the method they adopt is a fundamental ML technique, which lacks novelty.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper aims to prove LLMs work as ""stochastic parrots"" (Bender et al) rather than ""sparks of agi"" (Bubeck et al). To prove this claim, the paper presents an experiment where a task can be solved by training a linear model (logistic regression) on top of PCA of the LLM output. The authors then claim, based on the linear model experiments, that the LLM doesn't exhibit any sparks of agi due to the ability of (nearly) solving the task using linear models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors show a simple linear model trained on the output of an LLM for a given task is good enough to solve it, compared to using a GPT4 model, raising questions on the supposed intelligence often ascribed to the model.

Weaknesses:
While I generally agree that LLM's are closer to ""stochastic parrots"" than ""sparks of agi"", the claim that it can be proved using the proposed PCA experiments is weak to me. 

- Firstly, the embeddings are essentially the output of the LLM in question (all-MiniLM-L6-v2) - I would call it outputs rather than embeddings, as embeddings just indicate input word embeddings to the model, which clearly here isn't the case. 
- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1]).
- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer. That is, a linear probe trained on GPT4 embeddings for a harder task would also likely mimic its own performance. (this is theoretical, as neither the author or anyone other than OpenAI have access to their embeddings)

[1] https://aclanthology.org/D19-1250/

Limitations:
There are no explicit limitation section, however the last paragraph of conclusion discusses it.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
k0qTnbQxzR;"REVIEW 
Summary:
The paper presents CogCoM, a novel approach to training large Vision-Language Models (VLMs) using a mechanism called Chain of Manipulations (CoM). This mechanism enables the model to solve visual problems step-by-step with evidence, inspired by human cognitive processes like marking and zooming into images. CogCoM integrates manipulations such as grounding, zooming, and OCR into the VLM architecture, allowing it to handle various visual problems without external tools. The model is trained using a robust data generation pipeline and evaluated across multiple benchmarks, demonstrating state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Advantages of the Paper

1. **Explainable Reasoning and Manipulation Mechanism**: CogCoM generates intermediate steps with evidence, making the reasoning process transparent and explainable, which is crucial for complex visual tasks. The model incorporates a flexible set of manipulations that can be adapted to various visual problems, improving its versatility and problem-solving capabilities.

2. **Data Generation Pipeline**: The paper introduces an efficient pipeline for generating high-quality training data, which is essential for training VLMs to perform detailed visual reasoning.

3. **Superior Performance**: CogCoM achieves superior results across multiple benchmarks, including detailed visual question answering and visual grounding, showcasing its effectiveness and robustness.

These advantages highlight the paper's contributions to advancing the capabilities of VLMs in solving detailed and complex visual problems through a novel, human-inspired approach.

Weaknesses:
Weakenss in Points

This paper is generally good but I can still spot the following issues.

1. **Design of Figures and Tables**: The figures in the paper are not well-designed. The first and second figures are repetitive in meaning, and the colors in the first figure are too light (consider adding black outlines to the boxes). The font size in the second figure is too small to be legible on smaller screens. Additionally, the captions for Table 2 and Table 3 are too close to the tables, violating the submission guidelines.

2. **Lack of Discussion on Related Work**: The paper lacks a discussion of existing related work. It should consider citing and comparing with at least other agentic LMMs such as LLAVA-Plus[1] to provide a comprehensive comparison and context.

[1] https://arxiv.org/abs/2311.05437

Limitations:
See above Weakness part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Chain of Manipulations (CoM) mechanism for data generation to enhance visual reasoning in VLMs. The authors developed a data generation pipeline, producing 70K high-quality samples, and created the CogCoM model. CogCoM achieves state-of-the-art results across nine benchmarks, demonstrating significant improvements in various visual tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The CoM introduces a new data generation mechanism that enables VLMs to perform step-by-step visual problem solving with supporting evidence.
2.A data generation pipeline is proposed, producing a dataset of 70K high-quality samples.
3.The trained model, CogCoM, achieved SOTA in nine benchmarks.
4.This paper is well-written and easy to understand.

Weaknesses:
1.During data generation, the process relies entirely on GPT-4 for prompting and existing models (GroundingDino, PaddleOCR) for generation. As mentioned in the appendix, inaccuracies in these current visual models can affect the quality of generated data and the model's reasoning capabilities. However, the system lacks validation or filtering mechanisms to enhance data quality.
2.To highlight the specific improvements brought by CoM, it would be helpful to provide results both with and without the incorporation of CoM data. This would clarify the impact of CoM, especially since CogCoM integrates a significant amount of additional data such as MultiInstruct and LLaVAR during the instruction tuning stage as shown in Table 1.
3.The CoM dataset includes 6K high-quality manually annotated math samples, but no test results for math problems are provided. Clarification is needed on whether the purpose of this math data is solely to enhance the model's reasoning capabilities.
4.The paper emphasizes that CogCoM is a model capable of multi-image multi-turn understanding, but no corresponding test results (qualitative or quantitative) are provided.
5.In the model section, some parameters are not specifically explained, such as the maximum turns the model can accept and the predefined threshold.
6.Typos error: Line 288 CogOM->CogCoM

Limitations:
see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Drawing inspiration from human cognition to solve visual problems through localizing, zooming, etc., this paper introduces a new framework called CogCom, which solves visual problems by automatically combining six types of basic manipulations. When facing a visual problem, CogCom can use reasoning to solve each step and employ basic tools to aid in the problem-solving process. To achieve this goal, CogCom constructed a data generation pipeline that leverages GPT4 to build the training data for   CogCom. The CogCom leads to performance gains compared to its baseline CogVLM on several benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The CogVLM makes gains based on CogVLM on several benchmarks. 
2. The pipeline that leverages GPT4 to construct manipulation pipelines for problem-solving is reasonable.

Weaknesses:
1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.
2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.
3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.
4. The differences with some other related works should be discussed [5][6]. 

[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

[3] ViperGPT: Visual Inference via Python Execution for Reasoning

[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs

[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding

[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models

Limitations:
The limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
uhki1rE2NZ;"REVIEW 
Summary:
The authors study the effect of exponential symmetries on the learning dynamics of SGD. They establish the theorem that every exponential symmetry implies the existence of a unique and attractive fixed point in SGD dynamics, which could explain phenomena in machine learning such as matrix factorization and progressive sharpening or flattening.

Soundness:
2: fair

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper studies the dynamics of SGD. The authors prove a novel theorem that relates continuous symmetry to the attractive fixed point in SGD. The theorem is quite general and can have significant potential impacts on the field. The authors apply their theorem to matrix factorization, offering strong analytical and experimental support. And by my knowledge, this is a novel contribution.

Weaknesses:
1.	C is not guaranteed to exist. (Line 70)
2.	Usually, the stability of the learning is determined by the largest eigenvalue of the Hessian instead of the trace as discussed in the paper. The trace may not be an upper bound on the largest eigenvalue as it can contain negative eigenvalues.
3.	I think it’s missing dependence on $\gamma$ in proposition 5.3.
4.	The authors proved that in deep linear networks, the noise will lead to a balanced norm for all intermediate layers. This is an important theorem, but it lacks empirical verification. It is important to verify that the empirics actually match the theorem, which would strengthen the paper by a lot. Such an experiment should not be hard to run.
5.	There is a lack of clarity in how the analyses in Section 5.4 (Equation 24) relate to Figure 4.

Limitations:
The authors have included the limitations. For additional points, please see my comments above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the relationship between the behavior of SGD for solving empirical risk minimization and the exponential symmetry.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The strength of this paper is to study the relationship (Theorem 4.3) between the behavior of SGD for solving empirical risk minimization and the exponential symmetry (Definition 4.1). Moreover, it considers a noise equilibrium (Definition 4.5) and provides explicit solutions of the noise equilibriums for some applications (Section 5).  In addition, it provides some numerical results to support the theoretical analyses presented in this paper.

Weaknesses:
The weaknesses are the following. Please see Questions for details.  
- The mathematical preliminaries are insufficient. 
- The assumptions seem to be strong. 
- The explanations of contributions are insufficient.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper takes important steps toward showing that the noise in SGD pushed the dynamics along symmetry directions toward a unique fixed point. This differs in an important way from GD and gradient flow (GF) where one can show that in the presence of continuous symmetries (called exponential symmetries in the paper) there will be conserved quantities $C$, such as layer imbalance. These conserved quantities ensure that runs starting with different values of $C$ end up on different points along a minimum manifold (with some caveats). 
This paper shows that SGD noise causes drift toward a fixed point on a minimum manifold.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
This is potentially a very significant work. This is one of the few works I know which focuses on the effect of SGD noise on the symmetry directions and derives a concrete dynamics for this. I think the approach is at least partly original. Although there are many related works (see weaknesses), this paper is the only one which derives concrete solutions for the noisy dynamics along the symmetry directions. 
There are some corners that need to be ironed out. But if the results survive, this work could answer many questions about the implicit bias and generalization in SGD.

Weaknesses:
There exist multiple other works addressing the same questions. Two particular ones with very similar claims that the authors may have missed are: 
[1] Chen, Feng, Daniel Kunin, Atsushi Yamamura, and Surya Ganguli. ""Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks."" Advances in Neural Information Processing Systems 37 (2023).
[2] Yang, Ning, Chao Tang, and Yuhai Tu. ""Stochastic gradient descent introduces an effective landscape-dependent regularization favoring flat solutions."" Physical Review Letters 130, no. 23 (2023): 237101.

I ask the authors to clarify the distinction between their work and the above. [1] is also about the effect of SGD noise on the symmetry directions, afaik. 
[2] also derives the effective dynamics of SGD as a Langevin equation and shows a result similar to yours, namely that along symmetric minima, the noise drives the system toward the least sharp minimum.

Limitations:
There are some gaps in the derivations (see questions) which I hope the authors can fill. 
Additionally, the experiments seem to only look at two aspects: alignment of weights in matrix factorization (which is already known under weight decay, afaik), and bias toward less sharp minima (again, known). I would have liked to see a more direct test of the mechanism the paper claims is driving this movement toward flatter minima. At least, the balancing of the weights or some other direct result from the paper would have been desirable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
apI1GltwSx;"REVIEW 
Summary:
This work studies batch-to-global dataset distillation, optimizing the synthetic dataset by matching the statistical information of the synthetic batches to that of the full real dataset. Previous batch-to-global methods lacked diversity because each batch had the same optimization objective, leading to redundant information being learned across different batches. Based on this, the paper proposes an early-late training method. First, the real data is divided into lowest, medium, or highest probability patches based on a pretrained model, and these patches are sampled to initialize the synthetic dataset. During training, within-class samples are divided into smaller sub-batches, which are gradually concatenated for batch-to-global training.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. Previous batch-to-global methods indeed faced the problem of synthetic datasets receiving the same supervision signal, leading to redundant information being learned. This paper attempts to propose a new solution to this issue.

2. Extensive experiments demonstrate the good efficacy of the proposed method.

Weaknesses:
##  The main problem of this work is the writing.

>*It dedicates too much space to introducing previous work.*

The introduction describes previous methods in too much detail, leading to redundancy with the content in the related work section. The related work section also spends too much space summarizing and describing previous methods.

>*The technical part is confused*

The proposed method appears straightforward, but the authors describe the entire process almost entirely in text, lacking mathematical descriptions and definitions, which makes it somewhat difficult to understand. I suggest the authors dedicate more space to explaining the Concatenation Training and Training Procedure, incorporating some formulas to clearly demonstrate how the training is conducted.

>*It is doubtful whether the proposed method can effectively solve the issues present in previous approaches.*

Although the synthetic dataset is further divided within classes and different initializations are used, the supervision signal for each sub-batch seems to still be the same global signal as in other batch-to-global methods. This means that each sub-batch is still optimized in the same direction, potentially resulting in redundant information being learned. I suggest that the authors try to provide a more sound explanation.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent advancements in dataset distillation have led to two main approaches: batch-to-batch and batch-to-global matching. While the former excels in small datasets, the latter, though popular for large datasets, faces a diversity challenge due to independent optimization. Authers propose an EarlyLate training scheme that enhances diversity in batch-to-global matching by partitioning IPC samples into subsets and optimizing them locally. Experiments show significant improvements over previous methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) The technical approach is solid and robust, demonstrating a high level of technical competence.
2) The performance metrics presented are highly competitive, showcasing the method's effectiveness in comparison to existing benchmarks.

Weaknesses:
1) The motivation for the research is unclear, lacking an explicit articulation of the unifying challenges faced by current state-of-the-art works.
2) The resolution of the figures is inadequate, impeding clear interpretation of the results.
3) There is inconsistency in the styling of table borders and captions, with captions for Table 1 and 2 placed in different positions compared to subsequent tables, some above and some below the table.
4) The experimental settings are not uniformly aligned, and efforts should be made to cover all datasets and settings consistently across all experiments to ensure comparability and rigor.

Limitations:
see weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a simple but novel approach to enhance image diversity in dataset distillation. previous methods face challenges in balancing computational efficiency and diversity in synthetic images. The proposed EarlyLate training scheme addresses these issues by partitioning predefined IPC samples into smaller subtasks and using local optimizations for each subset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The EarlyLate training scheme effectively enhances the diversity of synthetic images. This is a very simple but novel approach to increase the diversity of distilled datasets. I believe it can provide some inspiration for future work.
2. The method, or the training scheme reduces the computational load compared to batch-to-global matching methods.
3. The experiments are comprehensive, including performance, cross-architecture generalization, ablation, and application. These experiments verify the method's superiority.

Weaknesses:
1. Compared to previous methods, the work in this paper is incremental.
2. The motivation and the advantages of the ""Selection Criteria"" in the initialization approach are not clear. And I am confused about how to rank, which is presented in Fig 5, could the authors explain it here?
3. There are a lot of hyperparameters involved. How should these hyperparameters be tuned? Are there any principled approaches?
4. I want to know the impact of the initialization method. In the ablation study, only CDA+init is shown. More advanced methods with init and whether EarlyLate uses init are not presented.
5. The performance of other sota methods on MobileNet-v2 is not presented in Table 1. Is the proposed method still better than other sota methods on MobileNet-v2?
6. Training tricks like random crop play a significant role in methods such as SRe2L. I would like to know to what extent the method proposed in this paper relies on such tricks.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an EarlyLate curriculum learner, which distills the easiest samples first and gradually add harder samples. Based on batch-to-global distillation algorithms, the proposed method consistently enhances the distillation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The writing is clear.
- The proposed curriculum learning scheduler seems effective, which is also an interesting point to analyze.
- Good distillation performance.

Weaknesses:
Limited contribution and potential overclaiming: 

1. In section 3, the initialization with real samples is common in DD, the data selection is proposed by RDED, and only the training scheduler is proposed in this paper. I suggest that, at least, add some diversity analysis and comparison of the distilled data.
2. Though the paper is titled with ""diversity-driven"", the method part lacks justification of the relation between ""diversity"" and the proposed scheduler. It seems that only the real initialization contributes to the diversity.

Limitations:
The authors have adequately discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oW6s6zFYj9;"REVIEW 
Summary:
This work belongs to ANN2SNN and proposes a novel coding scheme and neuron model to enhance the efficiency and accuracy of Spiking Neural Networks (SNNs) while reducing energy consumption. The Stepwise Weighted Spike (SWS) coding scheme improves information encoding by stepwise weighting input signals and introducing negative pulses, reducing the number of coding spikes needed. The Ternary Self-Amplifying (TSA) neuron model further enhances accuracy by progressively weighting the input through residual membrane potential adjustments and incorporating negative residuals and thresholds. Introducing silent periods allows the neuron to receive more input information before firing, significantly improving accuracy with minimal latency. Experimental results on datasets like MNIST, CIFAR10, and ImageNet demonstrate that the SWS coding scheme achieves better performance with fewer coding and computing steps, performing well even in very deep SNNs and achieving accuracy comparable to Artificial Neural Networks (ANNs) with the same structure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: This work introduces the Stepwise Weighted Spike (SWS) coding scheme, which is a novel approach in the field of Spiking Neural Networks (SNNs). The proposed method compresses spikes by weighting their significance in each step of neural computation, which enhances the performance and reduces the energy consumption of SNNs. Ternary pulses are a relatively new method in SNN, so the improvement of the ternary SNN encoding method has a relatively high degree of originality.

Quality: The paper provides a comprehensive set of experiments to validate the proposed SWS coding scheme. These experiments demonstrate that the SWS coding scheme significantly reduces operations and latency compared to existing neural coding schemes. The paper outlines the parameters used during training and provides justifications for the chosen experimental settings. 

Clarity: The introduction of the paper effectively motivates the work by discussing the limitations of current SNN coding schemes and proposing SWS as a solution. The methodology is clearly presented, with detailed descriptions of the new coding scheme and the Ternary Self-Amplifying (TSA) neuron model. Important symbols and their meanings are well-explained, contributing to the overall clarity of the paper.

Significance: The paper makes a significant contribution by proposing the SWS coding scheme, which enhances the efficiency and performance of SNNs. This new method addresses critical issues such as high latency and energy consumption in existing coding schemes, making it a valuable addition to the field. By improving the encoding of information in spikes, the SWS scheme has the potential to advance the development of more efficient and lower-power computing systems, thereby providing new options for the choice of coding schemes in SNNs.

Weaknesses:
In the ImageNet experiments in Table 2, SWS and other comparative ANN-SNN methods used different baselines, which is why the '$SNN\  Acc$' results are much higher than those of the comparative methods. However, the ‘$\Delta ACC$’ does not seem to show a significant difference (except for Hybrid training and Spiking ResNet). Using the same network architecture and pre-trained weights would be more credible.

Limitations:
The authors have not explicitly addressed the limitations or potential negative societal impacts of their work. To improve the transparency and completeness of their research, the authors could consider the following constructive suggestions:

1)  Limitations:

Create a dedicated ""Limitations"" section in the paper to discuss any constraints, assumptions, or potential weaknesses of the proposed SWS coding scheme.

Reflect on the robustness of the results to violations of assumptions, such as noiseless settings, model specifications, or dataset dependencies.

Discuss the scope of the claims made in the paper, including the generalizability of the approach across different datasets and scenarios.
Address factors that may influence the performance of the SWS coding scheme, such as computational efficiency and scalability with varying dataset sizes.

Consider possible limitations related to privacy and fairness concerns in the implementation of the SWS coding scheme.

2)  Negative Societal Impact:

Explicitly acknowledge the potential negative societal impacts of the SWS coding scheme, such as privacy risks, fairness considerations, or unintended consequences.

Discuss how the technology could be misused or lead to harmful outcomes, even if not intended by the authors.

Consider mitigation strategies to address any identified negative societal impacts, such as controlled release of models, monitoring mechanisms, or additional safeguards.

Emphasize the importance of ethical considerations and responsible deployment of the SWS coding scheme in real-world applications.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel Stepwise Weighted Spike (SWS) coding scheme designed to improve the efficiency of Spiking Neural Networks (SNNs) by compressing spikes and weighting their significance in each step of neural computation. This method addresses the issues of high delays and energy consumption associated with existing SNN coding schemes, as well as the complexity of neuron models and training techniques. The authors also introduce a Ternary Self-Amplifying (TSA) neuron model, incorporating a silent period to support SWS-based computing. This model is designed to minimize the residual error resulting from the stepwise weighting process. The experimental results provided in the manuscript demonstrate that the proposed SWS coding scheme significantly outperforms existing neural coding schemes, particularly in very deep SNNs. Key improvements include reduced operations and latency, enhanced overall performance, and lower energy consumption.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	Innovative Approach: Introducing the SWS coding scheme and TSA neuron model is innovative.
2.	Performance Improvement: This paper provides experiments showing that the proposed methods outperform existing coding schemes regarding both performance and energy efficiency.

Weaknesses:
1.	Clarity and Detail: Some sections of this paper could benefit from more detailed explanations, particularly in the description of the SWS coding scheme and TSA neuron model. This would help in understanding the underlying mechanisms and their advantages.
2.	Comparative Analysis: While the experimental results are promising, there is no proof from the experimental results that the encoding method proposed is more advantageous.
3.	There are some grammatical errors in the paper. Such as the second paragraph of Section 3.3, ""The neurons only integrates input and performs stepwise weighting"". It is recommended that a uniform representation be used for ""spike"" and ""pulse"".
4.	Symbol design problem, ""t"" in Eq. (3) becomes ""n"" in Eq. (5).
5.	There are many long paragraphs and sentences in the paper, making it difficult for readers to accurately understand the meaning of the paper.
6.	The description of the problem in the third paragraph of Section 1 and the end of Section 2 is not clear, making it difficult for readers to understand the problem that the article really wants to solve.
7.	The description of the encoding method in Eq. (7) is difficult to understand. According to Eq.  (7), the encoded value $A_j$ should have no time step. However, in the experimental part, the method of this paper has 8 time steps.

Limitations:
The paper mentioned that due to the setting of the neuron's silent period, the delay increases. It can be seen from the experiments that the overall latency of the method is lower, which can be regarded as solving this limitation. At the same time, this article does not have potential negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new coding scheme called Stepwise Weighted Spike (SWS) coding scheme for spiking neural networks to enhance the efficiency and reduce the number of operations and thus energy consumption. The SWS coding scheme tackles challenges associated with temporal and rate coding, such as heightened latency and energy usage. It achieves this by compressing spikes and assigning them varying weights at each computational step. Additionally, the paper introduces the Ternary Self-Amplifying (TSA) neuron model, which incorporates a silent phase to mitigate residual errors arising from the weighting procedure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The SWS coding scheme enhances information capacity and reduces the number of spikes, leading to lower energy consumption and higher accuracy as compared to other coding schemes. The effectiveness of this approach is demonstrated using different datasets.

Weaknesses:
1. Which model of a spiking neuron is being employed in equation 3 (line 120)? What is the reset mechanism here after the neuron fires? Are the weights allowed to have negative values? The description of the model is unclear. 

2. The notion of residual error intuitively makes sense but it is confusing. Please define the residual error mathematically (line 139) for better understanding. 

3. Why ANN-(sws)SNN conversion is opted instead of directly training the SWS based SNN?

4. There are some recent works [1,2,3] with TTFS encoding which claims better results in regard to energy-efficiency and low-latency. First, these works need to be cited in the related work section. In my opinion, a detailed comparative analysis with other models and encoding schemes (for instance with [1,2,3]) needs to be carried out. 

[1] Göltz, J., Kriener, L., Baumbach, A. et al. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nat Mach Intell 3, 823–835 (2021).

[2] I. M. Comsa, K. Potempa, L. Versari, T. Fischbacher, A. Gesmundo and J. Alakuijala, ""Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 8529-8533, doi: 10.1109/ICASSP40776.2020.9053856.

[3] Stanojević, Ana et al. “An Exact Mapping From ReLU Networks to Spiking Neural Networks.” Neural networks : the official journal of the International Neural Network Society 168 (2022): 74-88.

Limitations:
There is no potential negative societal impact and and one limitation related to the inclusion of silent period is noted in the main text.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a novel encoding method called Stepwise Weighted Spike (SWS) and a corresponding new neuron model named Ternary Self-Amplifying (TSA) for classification tasks utilizing the ANN2SNN training method. The proposed SWS encoding method assigns weights to the importance of spikes at each time step. The TSA neuron, which employs the SWS encoding method, features a lower threshold and includes a silent period.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Comprehensive method analysis: the authors conduct a thorough analysis of the Stepwise Weighted Spike (SWS) process, proposing a lower threshold and a silent period method to address residual error issues.

2. Superior Performance: the proposed method demonstrates superior performance in the field of ANN2SNN classification tasks.

Weaknesses:
1. Effectiveness of SWS: Various encoding methods, such as rate encoding and Time-to-First-Spike (TTFS) encoding, can be applied to different neurons and models. However, as illustrated in Figure 5, the SWS encoding method alone is ineffective without incorporating a lower threshold and a silent period. It only functions effectively when a neuron employs SWS encoding along with these additional components. Therefore, the paper should emphasize the neuron model rather than the encoding method, as it is not a universally applicable approach.
2. Lack of Experiments: The ablation study shows that the introduction of a silent period is the primary contributor to the improved performance. This raises doubts about the effectiveness of the SWS encoding method itself. Can the authors provide performance metrics for rate encoding combined with a lower threshold and silent period (if applicable) to ensure a fair comparison?

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a new spike coding scheme, which allows them to directly convert quantized ANN to their coding scheme. They demonstrate the effectiveness of their conversion on several pre-trained ANN with minimal loss in performance at the cost of an increase in latency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- strong experimental results
- coding scheme appears to be novel

Weaknesses:
- limited connection to spiking neurons, a more straightforward motivation would be a temporal encoding of quantized ANN

Limitations:
- method only applicable to conversion from pre-trained ANN
- no demonstration of training of a model using this coding scheme.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uNZpvFlsg9;"REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
yKvHJJE9le;"REVIEW 
Summary:
The authors propose a time-varying extension of SAFEOPT to overcome the problems of time-varying rewards under time-varying safety constraints.

Under stationarity conditions, optimality guarantees are provided and the numerical simluation shows a (favorable) comparison to the SAFEOPT.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is very well written and easy to follow.

2. Based on related work, the problems of time-varying rewards under time-varying safety constraints are an open problem in literature, and his paper addresses that.

3. The paper provides formal safety guarantees for their TVSAFEOPT algorithm.

Weaknesses:
1. *Some delineation to related work seems rather vague and requires stronger justification.* An example for TVSBO: the time-variable and temporal aspect of the kernel can just as well be interpreted as context using existing results. Perhaps a table would help here to highlight key aspects. 

2. *Lack of real-world data experiments and comparison to related work.* To support the downsides of existing approaches, an empirical comparison to existing TVSBO approaches mentioned in the related work section would be needed.

3. The *empirical results could be more convincing* by adding a variety of initial safe sets and revised plots. The current plots/results are hard to parse. 

4. It would be beneficial if the *theoretical/technical challenge of extending safety to the time-varying case were more detailed*. This would streamline the presentation and help in assessing the impact of the contribution.

Limitations:
1. Practicality of the safety guarantee: requiring many Lipschitz constants for both for space and time while also requiring an RKHS norm bound.

2. Theoretical and empirical impact: Lack of comparisons to TVSBO approaches makes the implications of the contribution unclear both theoretically and empirically.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a safe Bayesian optimization algorithm TVSAFEOPT with a spatial-temporal kernel and time Lipschitz constants, which improves on SAFEOPT with time-varying reward and safety constraints. The optimality guarantee is proved for the stationary case and the safety guarantee for more general settings. The method is tested on a synthetic problem and gas compressors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The use of a spatio-temporal kernel in Bayesian optimization for time-varying safety constraints is novel.
2. A formal proof of safety and optimality guarantee under certain assumptions.

Weaknesses:
1. More discussion on how to make a tradeoff between optimality and safety is encouraged. 
2. Will this conservatism in safety become too large in high-dimensional problems?
2. The method to choose the proper initial safe set and kernel parameters is unclear.

Limitations:
The societal impact is discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the TVSAFEOPT algorithm, which is based on Gaussian processes with spatio-temporal kernels, designed specifically for optimizing time-varying rewards under time-varying safety constraints. The algorithm provides formal safety guarantees in a general time-varying setting, ensuring safety even when exploring non-stationary safe regions. It robustly subtracts safety margins to prevent unsafe decisions, adapting in real-time to changing environments. Furthermore, they provide optimality guarantees for locally stationary optimization problems, ensuring near-optimal solutions when the optimization problem becomes stationary.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
They provide formal safety guarantees in dynamic environments, ensuring safe decision-making even in non-stationary settings. 

Additionally, the algorithm offers optimality guarantees for stationary optimization problems, enhancing its reliability and performance

Extensive numerical simulations were provided to validate the proposed approach.

Weaknesses:
They extend the Safeopt algorithm from literature. However, it is clear on what are the additional contributions and difference between these two different approaches.

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
PWzB2V2b6R;"REVIEW 
Summary:
This paper explores the problem of open vocabulary video action detection in an online setting, where the action must be detected immediately once it appears in the video stream, vs. the more common offline setting that allows examining the entire video, past and future.

The authors propose a model with two main components: a transformer decoder that cross-attends between recent and past video frames, and an action clustering block that uses slot attention to group related frames and then classify them by action.

The model is trained on combination of three tasks/losses: contrastive image-text loss between the current frame's visual embedding and text embedding (vs. text embeddings from other clips in the batch), multi-label contrastive video-text loss (for text clips described by multiple text labels), and mask loss for identifying which frames come from the background (no action present).

The model demonstrates improved performance over CLIP baselines in the novel open vocabulary online action detection setting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The problem setting introduced in this paper, online-streaming action recognition with an open vocabulary, is very realistic for modeling many practical real-world scenarios, e.g. home security cameras. Further, the model is impressively fast at inference (292 fps), making it quite reasonable to practically apply in this setting.

The paper includes an expansive, robust set of ablation experiments to validate the numerous model design choices.

I appreciate that the paper calls out joint modeling of spatio-temporal information as a limitation and backs this up with an analysis of the model's failure cases.

Weaknesses:
Line 61 says that ""our model successfully learns clusters of similar video frames"", and I see that indirectly based on the fact that there's a clustering module that leads to improved performance. However such a claim would be stronger if supported by evidence or examples of frames being correctly clustering based on action semantics.

The Object-Centric Decoder unit seems to be confusingly-named, since it is employed ""to group frames"" (line 149), rather than to focus on specific objects appearing within a frame.

In Table 1, in the proposed setting, the model is compared only against CLIP variants. Another baseline worth exploring would be video-text models (as opposed to image-text) applied via a sliding window.

There are a few parts of the paper that could be explained more quickly, in my opinion. (See the questions below.)

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses a challenging setting in video understanding: open-vocabulary online action detection. It leverages pre-trained visual language models with a proposed dual-encoder architecture to achieve successful zero-shot detection.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method follows a visual-text dual encoder approach and applies it novelly to zero-shot online temporal action detection, achieving state-of-the-art performance.
2. Detailed ablation studies and analyses are included. This paper provides comprehensive ablations regarding the architecture design, loss design, and efficiency analysis, demonstrating the advantages of the proposed method over previous approaches.
3. Clear writing. The paper is well-organized and easy to follow.

Weaknesses:
1. The choice of VLM. The method uses CLIP as the visual-text encoder. However, as discussed in the limitations section, CLIP is an image-based understanding model and lacks the capability to capture temporal context. I wonder if other VLMs, such as ActionCLIP, could mitigate these drawbacks for the proposed task.
2. The comparison of the proposed method with previous work, such as OadTR, seems to use different visual encoders/features. Can the authors explain the fairness of such a comparison? Besides, the author should also compare the method with more recent methods, since OadTR is not state-of-the-art in recent works.

Limitations:
Limitations have been well discussed by the authors in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces OV-OAD, a novel zero-shot online action detection system leveraging vision-language models for open-world temporal understanding. The authors propose a Transformer-based model with an object-centered decoder unit, trained solely on video-text pairs without manual frame-level annotations. The system is evaluated on THUMOS'14 and TVSeries benchmarks, demonstrating superior performance over existing zero-shot methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovation: The paper introduces a significant advancement in action detection by proposing a zero-shot online detection method that does not rely on manual annotations.
2. Technical Depth: The proposed model incorporates a novel object-centered decoder unit within the Transformer framework, which is a sophisticated approach to frame aggregation.
3. Experimental Validation:  The paper provides extensive experiments on two benchmarks, demonstrating the effectiveness of the proposed method.

Weaknesses:
1. The OadTR model was proposed in 2021, and the authors should consider using more recent OAD models such as MAT, MiniROAD, etc., as comparative benchmarks to enhance the timeliness and competitiveness of their model.
2. The authors have employed a multitude of complex modules for Zero-shot experiments on two datasets, with improvements on the TVSeries dataset being relatively limited compared to THUMOS14. This raises doubts about whether the proposed model is robust enough to serve as a standard baseline for future research.
3. As a pioneering work, the authors have experimented on typical action datasets like THUMOS14 and TVSeries. However, it would be beneficial to also test on atypical human action datasets, such as creating a benchmark for the HDD dataset.
4. In the action clustering block, which serves as the query and which as the key and value between the input Group Embedding and the output tokens? According to Figure 2, it seems that the Group Embedding is the query, and the latter is the key and value, but why is the dimension of the attention weight matrix $A$ $n \times k$ instead of $k \times n$? (line 142)
5. Line 154 mentions that the number of neighboring frames n can be a large value, and Table 4 shows the highest $n$ up to 8. Can the authors present experimental results for higher values of $n$?
6. Could the authors further explain how the model achieves ultra-high inference speed with a larger scale of parameters? For example, the parameter amount is twice that of LSTR, but the inference speed is six times as fast.
7. The paper lacks explanations for some operations, such as the operator ""$\circ$"" in Equation 1. This may lead to misunderstandings, mistaking it for a Hadamard product rather than a composite function.
8. It is recommended that the authors add visible variables in the figures used to illustrate the method (such as Figure 2) and improve the explanation of different parts of the chart to enhance readability.

Limitations:
In summary, the authors have examined the OAD (Online Action Detection) from an interesting perspective and proposed a novel Zero-shot model, which appears to have profound application value compared to traditional OAD methods. However, additional experiments may be necessary to substantiate the claims fully.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
1. The authors have proposed a new method for Online Open Vocabulary action detection by leveraging pretrained vision langugae models. 

2. To that end they introduce 2 main modules, a distant neighboring frame transformer and an object centric Action clustering unit.

3. They train their model with three objectives on filtered versions of the Activity-Net v1.3 and Intern Vid-10M-FLT datasets.

4. They evaluate the model zero shot on the validation splits of Thumos’14 and TVseries datasets and with a base-to-novel formulation on the Thumos’14 dataset. The authors show that in the case of the former their model beats naive CLIP baselines by ~2-7 % on Thumos’14 and ~2% on TVSeries dataset and in the case of the latter beat the OadTR model trained on the seen split of Thumos’14 by ~6-9%

5. The authors also demonstrate superior inference speed compared to OadTR.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors have proposed a new method for Online Open Vocabulary action detection by leveraging pretrained vision langugae models. 

2. The authors introduce 2 main modules, a distant neighboring frame transformer and an object centric Action clustering unit.

3. The authors evaluate the model zero shot on the validation splits of Thumos’14 and TVseries datasets and with a base-to-novel formulation on the Thumos’14 dataset. The authors show that in the case of the former their model beats naive CLIP baselines by ~2-7 % on Thumos’14 and ~2% on TVSeries dataset and in the case of the latter beat the OadTR model trained on the seen split of Thumos’14 by ~6-9%

4. The authors also demonstrate superior inference speed compared to OadTR

Weaknesses:
1. The authors claim in lines 41 and 42 that they do not use any frame-level annotations. However the Actitvity Net dataset contains annottaions for start and end times(therefore frames) for actions. And it seems that for the L_current objective in section 3.3, the text supervision is provided to the current frame. These claims seem contradictory and more clarity about this will be better. 

2. In the zero shot baseline comparison the improvement in case of Thumos-14 seem to be much larger than that of TV-series. Some explanation regarding this huge disparity is neccessary. Zero shot evaluations on more datasets can help indicate the robustness of this method to different data distributions.

3. In the zero shot baseline comparison, there seems to be a very large improvement for Thumos’14 in case of the ANet model compared to the InternVid model. The improvement is 2% for the latter while ~7% for the former.  Could this be attributed to similarity of actions, between thumos’14 and Anet ? Some investigation regarding this could shed light on the previous point. if that is the case then the improvement range disparity for TV-series( ~2%) could be explained.

4. The authors have compared their results for the open vocab evaluation with only the OadTR model. Comparison with more/better Online Action detection models (like GATEhub, MiniROAD, LSTR, Colar) is necessary for a holistic evaluation of the proposed model.

5. The ablations do not contain the different parts of the action clustering unit. Some results without the Object centric decoder are needed to justify its introduction. In table 5, there is no ablation for not using the final transformer encoder.  It should be added to justify its introduction as well.

6. In Figure 2. it is not clear what is being fed from the output of the object centric decoder to the final transformer encoder. It needs to be clearly mentioned in the figure for clarity.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
F8wKoSFSaA;"REVIEW 
Summary:
In this paper, a unified single-loop zeroth-order gradient descent extragradient ascent (ZO-GDEGA) algorithm to solve the nonconvex-concave minimax problem faster and more robustly. The theoretical analysis is provided to guarantee an overall complexity of $O(\epsilon^{-6})$. The experimental results on the data poisoning attack task and the AUC maximization task are shown to validate the practical performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. In this paper, the authors propose a zeroth-order algorithm that achieves lower complexity under the nonconvex-concave condition.
2. The first convergence analysis of stochastic zeroth-order algorithm under the nonconvex-concave condition is provided.
3. For nonconvex strongly-concave problems, the complexity with respect to the condition number is improved.

Weaknesses:
1. In section 2 related work, the complexity of first-order minimax algorithms is not discussed. To my understanding, the error of the zeroth-order gradient can be bounded by parameters $\mu_1$ and $\mu_2$ that are set as small as $O(\epsilon)$. Therefore, the analysis should not change a lot based on the analysis of the first-order counterpart, which probably undermines the novelty and contribution.
2. In ref [18] (Huang et al 2022), variance reduction is used to improve the complexity with respect to $\epsilon$. The contribution of this paper will be further stronger if this part is also considered.
3. The figures in the experiments section is too small and some curves are covered by the legends. I understand the space is limited but maybe some sections could be rearranged to the Appendix.

Limitations:
I did not see any negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They design a new unified ZO gradient descent extragradient ascent (ZO-GDEGA) algorithm, which reduces the overall complexity to find an ε-stationary point of the function ψ for nonconvex-concave (NC-C) problems. ZO-GDEGA is the first ZO algorithm with complexity guarantees to solve stochastic NC-C problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The theoretical section is very thorough and comprehensive.

Weaknesses:
(1)The paper lacks detailed link of code implementation.
(2)While the paper effectively addresses the NC-C problem from a theoretical perspective, it should also provide more experimental details and demonstrate that the NC-C problem exists within the experimental models.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies zeroth-order methods for nonconvex-(strongly)-concave minimax optimization. The achieved rates improve previous results and tolerate much larger choice of the smoothing parameters. The proposed methods also perform well for some empirical tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Minimax optimization is an important problem that has many applications in machine learning and related areas. In many settings, gradients are hard to estimate or impossible to obtain, which motivates the study of zeroth-order methods. Moreover, nonconvex-concave minimax is itself a class of nonconvex nonsmooth optimization, which is considered as challenge problems in the related literature. The paper proposes new algorithms with improved convergence results compared with previous work for this challenge class of the problem.

Weaknesses:
1. I think a discussion on lower bounds can improve the understanding and position of this work. For example, lower bounds on zeroth-order methods justify the dependence on the dimension is inevitable without additional assumptions [1]. This, together with lower bounds for minimax optimization, e.g., [2], provide lower bounds for the considered problem class, which suggest the foundamental limits of this problem and whether the complexity can be further improved.

2. A discussion of the best known upper bounds for first-order methods could also help. As far as I know, the best rate for first-order nonconvex-concave minimax optimization is also $\epsilon^{-6}$ [3]. The authors can do some additional literature review and check whether my statement is correct. As the complexity of zeroth-order methods is usually d times that of first-order methods, this suggests the results in this paper match the state-of-the-arts. For nonconvex-strongly-concave minimax optimization, [3] achieves a rate of $\kappa\epsilon^{-4}$, which suggests that the upper bounds in this paper can possibly be improved.

3. Have the authors considered to use two-point estimators to construct zeroth-order gradient estiimators? In some cases, two-point estimators give better rates [4]. The paper mentions that for the NC-SC case there is no need to use $z_t$ to update $x_{t+1}$ but $y_t$ instead. However, there is $z_t$ involved in the update of $y_t$. Are there any typos in the statement of the algorithm? Also, I suggest to add dimension in the complexity stated in the abstract. Otherwise, this could be misleading.

I did not have time to carefully check every step in the proof. If the results are correct, I think they make enough contributions to the related literature. Therefore, I will keep a low confidence score.

References

[1] Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 2015.

[2] The complexity of nonconvex-strongly-concave minimax optimization. UAI, 2021.

[3] SAPD+: An Accelerated Stochastic Method for Nonconvex-Concave Minimax Problems. NeurIPS, 2022.

[4] An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. JMLR, 2017.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes zeroth-order method called ZO-GDEGA to find a near-stationary point for nonconvex-concave minimax optimization, with complexity guarantee. The proposed method is also extended to stochastic setting, being the first work on ZO method on stochastic NC-C problem. The method has weaker requirement on ZO gradient estimate, thus also has better dependency on condition number in the special case of NC-SC. Numerical results on data poisoning attack and AUC maximization show the proposed method is comparable and usually slightly better than compared baselines.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work designs a unified single-loop ZO method for NC-C minimax, with better complexity and more robust allowance on ZO gradient estimate. The proposed idea of continuous-time dynamics to assist with updates of dual variable and its related analysis is novel. Overall complexity is proposed, with solid and rigorous proof, under weak assumptions such as not requiring lipschitz continuity, more tolerant ZO gradient, which also results in good complexity in the NC-SC special case.

2. The proposed method can be extended to stochastic setting, with first-ever complexity in this case.

Weaknesses:
1. Complexity (deterministic NC-C): The work claims the $O(d\epsilon^{-6})$ complexity of the proposed method is a 'reduced' complexity, however to the best of my knowledge, this is not the best-known complexity of ZO method on NC-C minimax. Even for single-loop methods, the existing method in [43] shown by table 1 has a better $O(d\epsilon^{-4})$ complexity. Intuitively, only a complexity as good as this existing $O(d\epsilon^{-4})$ is near-optimal, because first-order methods on NC-C minimax have $O(\epsilon^{-4})$ complexity. Although assumption-wise, as this work claims, [43] has the extra assumption of 'decreasing regular parameter sequence', and also requires more accurate ZO gradient on the primal variable, but in my opinion, the extra assumption above is not too restrictive, and requiring more accurate ZO gradient would only cost a constant multiple of queries thus would not affect overall complexity. Therefore, the $O(d\epsilon^{-6})$ complexity in this work is not good enough, and is in fact worse than certain existing single-loop ZO methods.

2. Complexity (stochastic NC-C): I acknowledge this is the first-ever complexity of ZO method on stochastic NC-C minimax. However, the 
$O(d_x \epsilon^{-6} + d_y \epsilon^{-8})$ dependence is not near-optimal, since first-order methods on such problem just have $O( \epsilon^{-6})$ complexity. ** Update: typo fixed, from $O(d \epsilon^{-6})$ to $O(\epsilon^{-6})$.

3. Listed numerical results show the proposed method is similar and generally only slightly better than compared baselines. The difference is not significant in both experiments.

4. Overall, both theoretical complexities and numerical results does not surpass existing methods, thus the contribution may not be strong enough for NeurIPS standard.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors establish a unified framework of zeroth-order optimization for nonconvex-concave minimax optimization problems in both deterministic and stochastic settings. This framework is based on the gradient descent-extragradient ascent algorithm. They claim that their algorithms require weaker assumptions on the zeroth-order estimator, while achieve competitive iteration complexity compared with the existing work. Besides, they provide some numerical experiments to verify the effectiveness of the proposed algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Main advantages can refer to the Summary. Besides, this paper has a good organization, which makes the readers reading easily. Numerous experiments including abundant baselines and datasets are provided to illustrate the effectiveness of the proposed algorithm.

Weaknesses:
In the stochastic setting, the authors make the bounded variance assumption on the zeroth-order estimator in Assumption 3. It seems that the assumption makes the proof very similar to first-order method. It is true that the zeroth-order methods are used when computing derivative are not possible, but does this assumption rules out the most challenging technical part in the proof?
This paper has proposed zeroth-order method that is motivated by application scenes when the objective function is not smooth enough to enable one to compute the gradient. But there is no specific examples provided that such method plays a prominent role while the first-order method is not applicable. Since there is a definite trade-off when zeroth-order method are used compared to first-order, the smooth parameter is another factor that should be considered in using zeroth-order method. So I think providing a concrete application that zeroth-order method is inevitable will help convincing the importance of the algorithm in application.

Limitations:
This paper is mostly a theoretical work, no negative societal impact may result in.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
UGwdz3kjht;"REVIEW 
Summary:
This paper makes the key observation that existing dataset distillation methods often introduce misaligned information during both the extraction and embedding stages, which leads to suboptimal performances. In response to this observation, the authors propose a method called Prioritize Alignment in Dataset Distillation (PAD), which aims to filter out misaligned information through two steps: 1). Pruning the target dataset based on sample difficulty according to the compression ratio, and 2) using only deep layers of the agent model during distillation to avoid encoding low-level, redundant information.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper synthesize a universal framework for existing data distillation methods by abstracting those methods into two steps: 1). Information extraction and 2) information embedding. Furthermore, it identifies a common theme of information misalignment in both steps. This observation enhances the understanding of current limitations as well as provides a clear direction for future research. 
2. The method presented in this paper effectively combines known conclusion from two distinct areas of research: data selection and representation learning. By leveraging established principles from both domains, the provide improvements to existing data distillation methods. More importantly, their analysis builds on developing an understanding of data distillation and the underlying mechanism: 1) small datasets require simple data 2) large datasets do not benefit from low-level information/features.

Weaknesses:
1. The proposed method’s filtering of information extraction is supported experiments shown in Figure 2. However, in practice, the method introduces two sets of hyper parameters - initial ratio and data addition epoch. The sensitivity to these hyperparameters (especially AEE shown in Table c) relative to the incremental performance gain presents a challenge, as running AEE can be complex and time-consuming (involves retraining the agent). This sensitivity and the associated tuning complexity could hinder its practical adoption in larger-scale datasets. 
2. The proposed method is adapted from the DATM framework with modifications at enhancing information alignment. However, the performance improves over DATM (shown in Table 1 and in the cross-architecture generalization results in Table 3) are not significant. This marginal improvement raises concerns about the practical value of the proposed changes, as they may not justify the added complexity.

Limitations:
Yes, the authors adequately addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to study the information misalignment problem in dataset distillation.
It proposes two basic pruning strategies: (1) learn the synthetic data with easy real samples first, and gradually change to harder samples, and (2) only match deep layers of the network during trajectory matching. The proposed method could enhance the current method in a wide range of dataset settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written.
- The motivation is reasonable.
- The idea of using a scheduler to dynamically adjust the real sample difficulty is smart.

Weaknesses:
1. The experimental observations to support the two strategies (Information Extraction and Information Embedding) involving Figure 2 and 3, is not sufficient. Experiments on a wider range of datasets and more pruning ratios could rationalize the method. And a comparison of discarding deeper-layer parameters is missing.
2. The wide existence of difficulty-aware dataset distillation could **potentially** weaken the contribution. Some discussion is appreciable:
```
[1] Prune Then Distill: Dataset Distillation with Importance Sampling
[2] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection
[3] (DATM) Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching
[4] On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm
```

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors claim that existing data distillation methods introduce misaligned information, so they propose Prioritize Alignment in Dataset Distillation (PAD). PAD prunes the target dataset and uses only deep layers of the agent model to perform the distillation, achieving state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is somewhat well-written and mostly easy to follow. And the tables/figures are well-demonstrated.

2. The authors analyze the misaligned information from two perspectives and propose method. 

3. PAD achieves improvements on various benchmarks, achieving state-of-the-art performance.

Weaknesses:
The performance gains brought by the method proposed by the authors are subtle and limited, potentially attributable to other explanations. For instance, as mentioned in [1], discarding original data in certain ways, or even randomly, can yield minor performance improvements under different IPC conditions. Or tricks mentioned in [2]. 

The trend changes in Figure 2 are not pronounced, and there are even instances where the trends contradict the explanations. Could additional test ratios or test IPCs be included to validate the findings?

[1] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.
[2] Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality

Limitations:
Due to the limitation of computing resources, the authors only validated their method’s effectiveness on DATM, DM, and DC.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lTUXlmjcva;"REVIEW 
Summary:
This paper proposes the affinity score, which measures the non-linearity of an activation function $\sigma(X)$ given the distribution of $X$.
The affinity score is defined based on how well the 2-Wasserstein distance $W_2(X, Y)$, where $Y=\sigma(X)$, is approximated by $W_2(N_X, N_Y)$, where $N_X$ and $N_Y$ are Gaussian approximations of the distributions of $X$ and $Y$, respectively.
Note that $W_2(N_X, N_Y)$ has a closed-form solution, and it holds that $W_2(X, Y) = W_2(N_X, N_Y)$ if the relation between $X$ and $Y$ is locally affine on the support of the given $X$.
The authors then propose to characterize a DNN model by the set of affinity scores of activation functions in the model under a given input distribution.
Experimental results suggest that the affinity scores are relatively low in transformer-based vision models, meaning that the activation functions are used in a more non-linear region compared to CNN models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The proposed score presents an interesting insight in comparing the series of CNN models and transformer-based models. Experiments suggest that transformer-based models utilize the non-linearity of activation functions more efficiently, leading to the higher prediction performance.

Weaknesses:
* It is empirically shown that the proposed score has a low correlation with existing non-linearity metrics such as R^2, but it is unclear whether the existing metrics are insufficient to analyze the DNN models in the way proposed in this paper. I would like to see how the distribution in Fig. 3(C) changes when other metrics such as R^2 are used instead of the proposed $\rho_{aff}$.
* In my opinion, one would expect the nonlinearity score to behave symmetrically at $x=0$ for activation functions like ReLU, but the proposed affinity score seems to have a lower score at negative $x$, as shown in Fig.2 or Fig.6. Is there any reasonable explanation for such a behavior of the proposed score?

Limitations:
See weakness above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes empirical statistics about different DNN architectures in the hope to shed some light into why some architectures are better than others for some computer vision tasks. To do so, the study leverages common optimal transport results on DNN's internal representations, under some strong assumption about the distribution of those representations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper proposes to consider an interesting and useful question of going to the bottom of why some architectures are better than others as measured by some restricted downstream task.

Weaknesses:
- I do not agree with the following statement `Without non-linear activation functions,
84 most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.` as to me, models such as transformers with linear attention and linear MLP blocks have no actual nonlinearity but are higher order polynomial of the input, i.e., are not linear. Could the authors provide clarifications on that statement or did I misunderstand something?

- I also disagree with the following `Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow
86 network capable of approximating any function, however complex it may be, to arbitrary precision.` since again, Fourier series for example can approximate any function as well. Hence DNN nonlinearities are certainly not the key ingredient to function approximation in general

- Many formal results such as Theorem 3.3 are well known and have been established for years (even decades) but no reference is provided which is misleading to the reader.

- Fig 2. is also misleading since the ""nonlinearity"" of any activation function depends on the range of the inputs. The only case that wouldn't be true is e.g. for ones with constant second derivatives, i.e., a linear activation function.... hence again that statement is highly misleading in presenting ReLU as inherently benefiting form that property compared to others

- the statement `No other metric extracted from the activation functions of the
260 considered networks exhibits a strong consistent correlation with the non-linearity signature.` is again an overstatement as the authors only compare with a few alternatives and theorem is provided to support such a statement

- the statement `We proposed the first sound approach to measure non-linearity of activation functions in neural
270 networks` is also incorrect, see e.g. 
  - https://jmlr.org/papers/v20/18-418.html
  - https://arxiv.org/pdf/2301.09554
  - https://arxiv.org/abs/1810.09274
  all the above works have been published in peer reviewed journals/conferences

Limitations:
In addition to my concerns expressed above, the study does not provide any actionable insights or understanding on the ""why"" of different architectures performing differently beyond the proposed statistical numbers. How could one use the provided analysis to better design model architectures or for model selection? 

Also, the paper does not provide any novel theoretical results. All the major theorems and results are already widely known within the community, yet they are presented as part of the contributions. With that in mind, the paper solely leverages existing OT tools, with some underlying simplifications on the DNN's data distribution, and report computed metrics. Hence the study falls below acceptance level in my opinion and would need a major rewriting + additional novel contributions to be worth acceptance.

The writing style is also filled with unsupported claims and highly misleading statements (see the **Weaknesses** examples).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel method for quantifying the non-linearity of activation functions in neural networks, termed the ""non-linearity signature."" Using an affinity score derived from optimal transport theory, it measures the non-linearity of individual activation functions. It defines the non-linearity signature as a comprehensive set of these scores across all functions in a deep neural network (DNN). The study compares these signatures across a range of popular DNN architectures in computer vision, revealing clear patterns in their evolution over the past decade, notably showing a trend towards decreasing non-linearity until the disruptive impact of vision transformers. It emphasizes the uniqueness of their measure, as it does not strongly correlate with other metrics across different architectures. The approach could potentially be applied to analyze the non-linearity of newer large language models (LLMs) and identify innovative neural architectures that optimize internal non-linear characteristics for enhanced performance, crucial in the era of costly experiments with large-scale model optimizations.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Novelty and importance. The paper introduces a theoretically grounded measure, the affinity score, for quantifying the non-linearity of activation functions using optimal transport theory, providing a robust framework for analysis. This is the first approach to approximately measure the non-linearity of DNNs, which is crucial for understanding their inner workings.
2. Solid theoretical and experimental validation.  The method is grounded in optimal transport theory, providing a rigorous theoretical foundation for the proposed non-linearity signature. This enhances the credibility and robustness of the findings. The experimental results demonstrate the practical utility of the non-linearity signature. It can predict DNN performance and meaningfully identify the family of approaches to which a given DNN belongs, making it a valuable tool for researchers and practitioners.
3. Clear Writing. The structure of the paper is well-organized, with a clear presentation of background knowledge, theoretical properties, experimental evaluations, and conclusions. This clarity aids in understanding the contributions and implications of the research. The paper's figures and tables are comprehensive, providing clear and precise information, and the writing maintains a coherent logical sequence.

Weaknesses:
1. The authors should discuss more activation functions. Currently, only ReLU, Tanh, and Sigmoid are included.  While these are among the most commonly used activation functions in neural networks, many other activation functions have been introduced and shown to be effective in various contexts, like GELU. Including a more comprehensive analysis of a diverse set of activation functions would enhance the robustness and applicability of their proposed method. 
2. There is currently some research on the nonlinearity of deep neural networks that should be compared and discussed.
3.  It would be beneficial to showcase examples from domains beyond computer vision. While the paper focuses on computer vision tasks, it may not address the non-linearity signature's applicability to other domains such as NLP, speech recognition, or reinforcement learning. The findings might be less generalizable if the proposed measure does not perform equally well across diverse types of tasks and data.

Limitations:
Yes, they have discussed the assumption of Theorem 3.3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
