id;text;label
31zVEkOGYU;"REVIEW 
Summary:
The paper discusses the phenomenon of overestimation, the allocation of higher likelihoods to out-of-distribution data points, in deep generative models. It analyses two factors which may cause the overestimation problem specific to VAEs from a reformulation of the ELBO. These two factors are posterior collapse and a difference in entropies between in-distribution and out-of-distribution datasets. The paper proposes, again specific to VAEs, a method called AVOID for alleviating these two factors.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
* The clear definition of overestimation in Eq. (3) following is useful, also for the wider literature.
* The experimental setup is large: Table 3 demonstrates that the number of dataset combinations considered is numerous, in particular in comparison to previous work. 


Weaknesses:
* In 3.2, the authors pose the questions “When is the design of the prior proper/not proper”, but answer these questions by providing an example for each case. While this is useful for illustrative purposes, it does not answer the stated question. The first few examples are furthermore focussing on linear VAEs which are not relevant for common practical uses, which limits the relevance of the theoretical results in this section. 
* The design of the calibration term in ll. 219 is unclear. In my opinion, it is not properly explained, and important choices like SVD are not well motivated. When would SVD likely fail? Why does SVD intuitively capture the difference in entropy between the datasets?  The words “complexity” and “entropy” seem to be used interchangeably, please explain or use consistently.
* The experimental results are difficult to interpret, it is partly not possible to draw meaningful insights from it. It is worth noting that this is common in similar works on alleviating OOD detection in DGMs, the methods are hard to compare due to different experimental setups. However, in this work, important questions I have are: 1) In Table 1, in the unsupervised column, why is AVOID highlighted in bold, even though WAIC  outperforms it sometimes? 2) Where is the performance of a standard VAE without any adaptations listed?  I find this an important benchmark. 3) What is the decision criterion for OOD vs. in-distribution? Is it a threshold on the amended likelihood? If yes, looking at the density plots of Fig. 6 (b), how is it possible that there is still  a lot of overlap between the two datasets in PHP, even though the accuracy according to Table 1 is 99.2%? This seems inconsistent to me. 4) The experimental results report no standard deviations in key tables, such as Table 1 and 2 . DGM based methods are well known to be unstable, hence standard deviations would be useful. However, Table 3 partly alleviates this problem due to the large number of dataset combinations considered. 5) Table 3: I would argue that comparing CIFAR10 and CIFAR100 (and possibly other combinations) seems meaningless: The datasets are overlapping, hence it is unclear what is OOD and what is in-distribution.
* The language is sometimes unclear, in general slightly hard to understand, and could be greatly improved.

In summary, while this work demonstrates a large effort and a clear analytical approach to alleviating the overestimation problem in VAEs, important questions remain unclear. I am open to reconsidering my score upon a response from the authors.


Limitations:
* The overestimation problem is common to many DGM methods, but this work provides a solution for VAEs only. The scope is bigger, and one could argue that it might be more interesting to find the underlying root cause in all DGM methods which suffer from this problem (if there is one). Yet, considering VAEs is a very interesting start.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper first mathematically examines the unsupervised (without training label) OOD detection performance using VAE, decomposing the expected ELBO into two components: (i) entropy $\mathcal{H}(x)$ of a dataset, (ii) KL divergence $D_{KL}(q(z)||p(z))$ between the estimated $z$ and the prior. It's theoretically shown that the entropy of data distribution is defined by itself, thus may not bring benefit to the OOD detection problem (Eq. 8). Then the paper mathematical and empirically analyzes the second component. The paper shows in some simple cases the prior $p(z)$ and the dataset $p(x)$ can not fit well with the VAE model, which results in for some $x$, $p_\theta(x)$ estimated by the trained VAE model has high value when $p(x)$ has low value, which is the overestimation problem for OOD detection. The paper proposes to use post-hoc prior method (estimate the prior from the trained VAE and the ID dataset) to revise the issue of the improper design of prior and add calibration to alleviate the issue of entropy. Empirical results show that the proposed AVOID method constantly improves the OOD detection performance simply based on ELBO (Table 3), and outperforms existing unsupervised non-ensemble OOD detection methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper mathematically examines the unsupervised (without training label) OOD detection performance using VAE, decomposing the expected ELBO into two components: (i) entropy $\mathcal{H}(x)$ of a dataset, (ii) KL divergence $D_{KL}(q(z)||p(z))$ between the estimated $p(z)$ and the prior, which is quite crucial to understand the underline benefit and drawback to use ELBO as OOD score.
2. The demos including Figure 2,3,4 shows the improper of the traditionally chosen prior leading to the mismatch between prior and post-hoc prior, and the high probability of OOD sample in the prior. The observation well inspires the proposed post-hoc prior method.
3. Experiment includes varied OOD detection methods including supervised, auxiliary, and unsupervised (ensemble/non-ensemble), and shows the proposed method beats baselines within a specific category.


Weaknesses:
1. Notation is not consistent such as $p$ and $p_\theta$ in Figure 3.
2. Eq. 8 uses the entropy difference between ID and OOD distributions. Eq. 8 tells us the more diverse the ID distribution, the harder the OOD detection task. I think the OOD here should consider overall OOD distribution instead of an OOD dataset distribution. If not, I can simply define each OOD data point as a distribution which has $\mathcal{H}_{p_o}=1$ or I consider overall OOD data together (overall OOD distribution) which may have a pretty large diversity and very low entropy. Thus the motivation for the second method is not well held. I believe the idea of the second method is good itself, it leverages some extra information to improve the OOD performance.
3. Sec. 3.1 uses 3-layer NN for $q_\phi$ and $p_\theta$. The dataset is synthetic, thus I wonder whether increasing the number of training samples and NN capability would help better estimate $p_\theta(x)$. In other words, the reason that ELBO suffers from overestimation is the number of training samples, NN capability, or something else. Or perhaps the observation from Figure 3 is even when ID is well estimated, the OOD is still not well estimated.


Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies unsupervised OOD detection (i.e., training data contains no labels) using deep generative models. DGMs model the probability distribution of the inputs, and can be an ideal candidate for unsupervised OOD detection. The authors study one specific class of DGMs, namely VAEs. They show that VAEs suffer from overestimation problem ($P(x_{ood}) > P(x_{id})$) due to two main reasons — dataset’s inherent entropy and improper design of prior distribution. The paper then proceeds to theoretically suggest ways to mitigate this issue, and shows experimental results that do so.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The theory of the paper is simple but inspiring, and matches neatly with the designed algorithm.
2. The experiments are well-designed and executed.
3. The ablation studies are well-done.

Weaknesses:
1. Prior work such as [1] that discusses causes of deep generating models’ (specifically, normalizing flows) reason for failure to perform OOD detection was not cited/discussed in the paper. Similarly, [2] is also an important paper for using DGMs for OOD detection that wasn’t cited.
2. The paper is not self-contained and the organization could be improved — for example, one could put the limitations in the main paper instead of in the appendix.
3. Notation of the paper. For example, $p(x) = N(x | 0, \Sigma_x)$ can be more readable as $x \sim N(0, \Sigma_x)$, following more commonly used convention.

[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020

[2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
In the context of VAE, the authors identified two factors that potentially cause VAE to assign higher likelihood to OOD data than ID data. They propose a new scoring mechanism that improves upon VAE's overestimation of the likelihood on OOD samples.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Decomposing the ELBO carefully is interesting. In particular, they give a new prior design targeting the overestimation issue.
- They have a scoring method that improves upon the standard ELBO, which partially validifies their analysis.

Weaknesses:
- the derivation assumes the model distribution can converge exactly to the true one, but this is impractical. 

If it does, there should be no overestimation issue to begin with (for practical datasets that are arguably separable, e.g. SVHN vs CIFAR). Moreover, even if it is possible in theory, the empirical and theoretical observations in [1, 2] will prevent this from happening in practice. 

If it doesn't, the derivation will leave an error gap that is not analyzed. In short, the key reasoning in the above is that real distribution is often supported on low dimensional sets, while model distribution is fully supported.

- the evaluation is a bit outdated on easier benchmarks. To solidifies AVOID's practical impact, evaluation on the harder tasks as in DoSE [3] is necessary.


[1] Dai, Bin, and David Wipf. ""Diagnosing and Enhancing VAE Models."" International Conference on Learning Representations. 2018.

[2] Dai, Bin, Li Kevin Wenliang, and David Wipf. ""On the Value of Infinite Gradients in Variational Autoencoder Models."" Advances in Neural Information Processing Systems. 2021.

[3] Morningstar, Warren, et al. ""Density of states estimation for out of distribution detection."" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.

Limitations:
N/A

Rating:
5

Confidence:
4

";0
HvWfTrjwWa;"REVIEW 
Summary:
This paper proposes a method for estimating the balancing weights in causal effect estimation. The balancing weights are given by the density ratio of the counterfactual distribution and the observed distribution, and the density ratio can be estimated by solving the optimization of the variational expression of the f-divergence between the two distributions. The authors claim that using the $\alpha$-divergence is useful because it addresses the vanishing-gradient problem when we use a neural network for the model. The paper presents some theoretical results.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
- Expressing $\phi$ as $\exp(T)$ in Eq. (14) is interesting since the domain of the optimization variable becomes simpler.
- The discussion about vanishing gradient is interesting, and the suggested way of setting $\alpha$ may be useful to address this issue.

Weaknesses:
- There are several technical parts that might not be precise. See the Questions section below.
- The writing could be improved. The paper rather looks like a collection of definitions of results. Connections between sections are not smooth, and the message of the paper as a whole is not clear.
- The abstract says, ""we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem."" I am not sure what makes the $\alpha$-divergence special in terms of these aspects. Also, the explanation about the vanishing-gradient issue in Section 5.1 is not convincing enough to make me believe that the authors' suggestion really addresses the issue.
- The paper relies on references (including one without open access) for important definitions. The paper could be more self-contained.
- Overall, the paper is mainly about estimating density ratios by the variational expression of the f-divergence, which is not novel.
- There is no experiment in the main part of the paper.

Limitations:
The authors discuss limitations of the work.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper proposes an approach for estimating balancing weights. The ultimate target is to use these weights for the estimation of the casual effects of interventions. The search of balancing weights are formulated as density ratio estimation. To tackle this problem, the authors employ a variational representation of the $f$-divergence. . The density ratio is modeled by a neural network. Specifically, this paper advocates the use of $\alpha$-divergence, and provide various ways to improve the practical performances of the proposed weights.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- This paper successfully makes use of the density ratio learning tools based on $f$ -divergence for causal inference applications.
- This paper summarizes various related existing results on density ratio learning via variational form of $f$ -divergence.
- This paper provides detailed discussion on various practical improvement of the proposed algorithm, which would benefit practitioners.

Weaknesses:
- This paper lacks theoretical results on the causal effect estimation (not the density ratio learning), which is regarded as a key target of this work.
	- It is unclear whether the proposed estimator obtain optimal convergence rates and (semi-parametric) efficiency, when degenerated to standard settings like average treatment estimation, and conditional average treatment estimation.
- This work is not impressive in terms of novelty, as many significant components follow more directly from existing work.
	- The variational form of $f$ -divergence and the corresponding estimation of density ratio has been explored by references [15] and [16].
	- The main theoretical result (Theorem 6.1) seems to be a fairly standard dual result (e.g., Theorem 4.4 of https://arxiv.org/pdf/1003.5457.pdf)
- The theoretical results are not stated clearly. Many results are not stated in rigorous statements with careful listing of assumptions. Some seem to be exclude certain important cases like mixture distributions, as claimed to be a major contribution of this proposed work. (see my question below.)

Limitations:
Section 8 discusses limitations on sample size requirements. The authors claim that the sample size required for controlling the error of $\hat{Q}_{K_0}^{(N)}$ has an exponential dependence on the dimension. Although I have questions regarding this theoretical results (as stated above), I would not be surprised about the curse of dimensionality. However, since the ultimate target is the causal effect, I am more interested in a direct error or sample complexity analysis of the causal effect. The lack of such analysis is a major limitation of this work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper derives a novel method of reweighting covariates for the purpose of causal inference. This is done through learning optimal change of measure weights modeled via neural networks and trained using alpha-divergence measures between the true joint distribution and a mutually independent one. Further tweaks are introduced to help enable this procedure to be more practical in its implementation. 

A brief disclaimer: I am not very familiar with causal inference (to be honest I am not quite sure how I received this paper to review); however, I am quite familiar with variational inference and related techniques. I will focus primarily on the latter during my review, but please keep all of this in mind while reading my comments.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
All of the work presented is very precise and detailed, with novel contributions that I can see with regards to the learning of the weights via the alpha-divergence objective. I found the result of the main algorithm to be particularly concise and intuitive for such an involved derivation (e.g., almost trivial to describe but definitely not to prove). As far as I could tell, all of the decisions made throughout the proposal were well justified and deliberate.

Weaknesses:
The precision presented definitely came at the cost of readability in my opinion. The paper is very notation heavy and introduces many concepts rapidly and with great precision that lead me to having to re-read sections many times over to understand the message. It feels like the paper could benefit from summarizing some of the technical details in the main paper and reproduce the more exact version in the appendix.

Additionally, I understand the work faced space limitations; however, I believe it should definitely have the numerical experiments presented in the main paper. If pressing for pages, I could see including the experiments in the main paper and moving section 7.1 (either partially or completely) to the appendix.

I found no weaknesses in the technical information itself from what I could understand.

Limitations:
The authors very adequately discussed limitations.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This work presents the way to estimate the balance weight, that represents the causal effects of arbitrary mixture of intervention, by using the neural network. Specifically, authors recognize this balance weight as the density ratio between the source and balanced distributions, and estimates this ratio by optimizing the variational representation of  $\alpha$-divergence with $\alpha \in (0,1)$. In this procedure, authors justify why the $\alpha$-divergence should be used in terms of the property of the estimator, such as the sample complexity and unbiasedness, and the vanishing gradient issue for training.  



Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* This work seems to have a solid theoretical explanation for the balance weight estimation through neural network.

Weaknesses:
First of all, I confess that I am not expert in this field, and thus ask for understanding if my feedback does not make sense in this field.

* The proposed has not been compared with other baselines for balance weights.
> Authors claim that the proposed approach is a general method that can estimate balancing weight even when the datasets are generated from arbitrary mixture of discrete and continuous interventions. This means that the proposed method could be comparable with other baselines if either discrete interventions or continuous interventions exists on datasets. I believe that this comparison seems necessary because it can validate whether the proposed method can be regarded as the general method or not by showing that the proposed method is competitive with the existing methods.  However, since the proposed method was validated only for mixed variable interventions, its effectiveness on single intervention (either discrete or continuous) is not clear.

 
*  The advantages of the proposed approach seems less clear as compared to the existing baseline.
> In appendix, table 3 shows that the baseline approach (Entropy Balancing) outperforms the proposed method in most cases. Therefore, I am skeptical about why the proposed approach is meaningful. Does the proposed method have any other advantages over the baseline approach, that are not shared at current draft ?

Limitations:
See above Weaknesses.

Rating:
4

Confidence:
1

";0
l0zLcLGdcL;"REVIEW 
Summary:
This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see.

Weaknesses:
Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6).

Limitations:
The authors have shown some limitations of the proposd method, but more should consider other that the method itself.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.  

Weaknesses:
I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''[17]that''


Limitations:
Yes. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC [24] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.

* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.

* The paper is fairly easy to follow (with the exception of some equations and many typos and grammatical errors, see Weaknesses).

* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show small improvements over some existing methods using the CLIP-pretrained model, this can serve as a new strong baseline for future UniDA work.

Weaknesses:
* The paper claims that existing UniDA works overlook the internal intrinsic structure in the categories. 
    * However, [W1] aims to resolve the same problem. [W1] proposes to learn lower-level visual primitives that are unaffected by the category shift in the higher-level features. And, in their proposed word-prototype-space, different visual primitives can be shared across domains and classes (including unknown classes).
    * There is a significant overlap in the motivation given by this paper and that of [W1]. Consequently, the high-level conceptual novelty of this paper is overclaimed. However, I do believe that these conceptual ideas are interesting as well as important for UniDA.
    * Please discuss the similarities and differences (both in terms of motivation and the actual approach) of this paper w.r.t. [W1].
    * Another paper with similar conceptual ideas is [W2].

* This paper lacks some mathematical rigor.
    * Eq. 1, 2: $\hat{Z}=W\cdot M$ is shown as matrix multiplication (I assume that it is not element-wise multiplication since dimensions of $W$ and $M$ are different), but the expansion of this matrix multiplication contains an arg-max over the elements of $W$. Then, it does not make sense for the overall computation to be a standard matrix multiplication.
    * Eq. 1, 2: the text mentions that $s_i$ is the index of sub-prototypes in the $i^\text{th}$ item but Eq. 2 implies that $s_i$ is a particular dimension found with arg-max. This seems contradictory and is confusing.
    * Eq. 2: Use $\mathop{\arg\max}_{j}$ instead of using `dim=1` since it is a mathematical equation and not the code implementation.
    * Eq. 5: It is unclear which dimension is used for top-$k$
    * Eq. 6: It should be $\max(... , 0)$ instead of just $\max(...)$.

* The requirement of a CLIP-pretrained backbone is very restrictive since the method cannot be extended to other settings (like medical imaging) where the CLIP-pretraining may be suboptimal. While the paper shows comparisons where prior methods use the CLIP-pretrained model, it should also show comparisons when starting from a random initialization as well as the more widely used ImageNet initialization.
    * The paper claims that a CLIP backbone is needed to retrieve sub-prototypes in early iterations. Why not start retrieving sub-prototypes after a few epochs of normal training?

* L135: “eliminates the domain-specific information from the target domain”. This is a very strong claim which does not seem to be backed by evidence. Performing “domain alignment” is not the same as “eliminating” domain-specific information. Further, as we can see from Fig. 3, the sub-prototypes seem to be retaining domain-specific information.

* There are no sensitivity analyses for the several loss-balancing hyperparameters $\lambda_1, \lambda_2, \lambda_3$ (not even in the Supplementary). While the paper claims to have borrowed them from DCC, this approach is vastly different from DCC, and we need to check for sensitivity to these hyperparameters. Further, DCC does not have a reconstruction loss, so it is unclear how that hyperparameter is selected.

* There is no ablation study for the adaptive threshold $\lambda$. It should be compared to various fixed thresholds and the value of the adaptive threshold should also be plotted over the course of training to obtain more insights into its working.

* Other UniDA works, like OVANet [40] and [W1], study the sensitivity of their methods to the degree of openness (i.e. the number of shared/private classes) which changes the difficulty of the UniDA problem. This analysis is missing in this paper. This should be shown for a better understanding of the capabilities of the proposed method.

* Some more related work [W3-W4] on Open-Set DA and UniDA (apart from [W1, W2]) that is not discussed in this paper.

* Minor problems (typos):
    * L53: “adaption” → “adaptation”
    * L59: “shifts” → “shift”
    * L92: use `unknown’ i.e. use a backquote in LaTeX for it to properly render the opened and closed quotes like in L102. 
    * L119: use math-mode for K in top-$K$.
    * L124: “varies” → “vary”
    * L126, 179: add space between text and \cite{...}
    * L134: “differenciates $\hat{Z}$ with” → “differentiates $\hat{Z}$ from”
    * L151: “max” → “maximum”
    * L166: “only the $K$” → “only the top-$K$”
    * L181: “$max$” → “$\max$”
    * L244: “fellow” → “following”

* Minor problems (grammatical errors):
    * L32: “aims” → “aiming”
    * L40: “Since such kind” → “Since this type”
    * L41: “almost happens in all the” → “occurs in almost all of the”
    * L59: “embedding give into” → “embedding is passed to” 
    * L125: “sometimes is” → “is sometimes”

### References

[W1] Kundu et al., “Subsidiary Prototype Alignment for Universal Domain Adaptation”, NeurIPS22

[W2] Liu et al., “PSDC: A Prototype-Based Shared-Dummy Classifier Model for Open-Set Domain Adaptation”, IEEE Transactions on Cybernetics, Dec. 2022

[W3] Chen et al., “Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation”, AAAI22

[W4] Garg et al., “Domain Adaptation under Open Set Label Shift”, NeurIPS22

Limitations:
I appreciate that the paper provides both limitations and broader societal impact discussions in the Supplementary.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score.

Weaknesses:
Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments.

Limitations:
This paper has no limitation sections.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section.

Weaknesses:
W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as [1, 2, ..] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like [3]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
[1] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
[2] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
[3]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022

Limitations:
A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.

Rating:
6

Confidence:
5

";0
jOuPR9IH00;"REVIEW 
Summary:
This paper considers variance-weighted least-squared regression for offline RL with general function approximation. Under a uniform data coverage assumption, they show that the proposed algorithm obtains a sub-optimality bound that scales with the $D^2$-divergence of the offline data set, the positive lower-bounded constant of the uniform data coverage, and the complexity of the function class. Their bound obtains the right order when realized in the linear case. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- clear presentation (though some parts can be improved further -- see Weaknesses)
- the obtained result is new and relevant to the offline RL community 


Weaknesses:
- The main weakness is that the uniform data coverage assumption is very strong. In the linear case, this assumption is equivalent to that the behavior policy is exploratory overall dimensions of the linear feature. A question for the authors is that in such a case, why would we even need pessimism?  Pessimism is used when the data coverage is partial thus we become pessimistic about uncertain actions. But when the coverage is uniform, it can eliminate the need for pessimism and we can simply use greedy algorithms. I understand that without such a uniform data coverage assumption, it seems difficult to get a reliable estimation of the variance of the transition kernel and it would be interesting to get rid of this assumption. But if we could not get rid of it yet, the very least expectation is that we need to explain this assumption further, especially regarding where pessimism is really needed with this assumption. 


- Writing can be improved further. For example, the $D^2$-divergence and the definition of the bonus function (Def 4) can be explained and motivated further. The current presentation of these concepts are not very helpful 

- Some claims might be potentially misleading. It's not comfortable to view the proposed algorithm as computationally efficient even in the oracle sense. Specifically, the construction of the bonus function in Definition 4.1 is far from being computationally efficient since it is essentially a constrained optimization over the version space. That said, it is nowhere more computationally efficient than version-space-based algorithms such as the ""Bellman-consistent pessimism"" of Xie et al. 

- Though the main result is new, it appears expected given the already-developed machinery in Argawal et al. 2022 and Xiong et al. 2022. What are the technical challenges in the current problem that the existing techniques cannot resolve? 

- Some minor: PNLSVI is never introduced before used 

Limitations:
Yes 

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper studies offline RL with non-linear function approximation. The paper is mainly motivated as existing sample complexity guarantees on offline RL algorithms with general function approximation yield suboptimal dependency on the function class complexity, e.g. when the bounds are translated to the linear case. The paper proposes an oracle-efficient algorithm that achieves minimax optimal problem-dependent regret when the bounds are specialized to the linear case. The paper also introduces a new coverage definition.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper appears to be technically sound with some new ideas in the algorithm design and formulation of dataset coverage.
- The approach achieve minimax optimal rate in non-linear function approximation, when bounds are converted to linear.

Weaknesses:
- The main weakness is that the proposed approach either requires uniform coverage or non-linear bonus oracle. The non-linear bonus oracle is a strong requirement and in effect, simply removes the difficulties related to pessimism in offline RL. On the other hand, the uniform coverage assumption is too strong and thus, it is unfair to compare its efficiency to pessimistic offline RL algorithms.
- A clear comparison to prior work is not presented. In particular, there are multiple axes of comparison, such as dependency on $\epsilon$, dependency on function classes, data coverage requirement, type of oracle, computational efficiency/tractability, realizability assumptions, etc. It is difficult to clearly evaluate the results in this paper without such comparisons. For instance, it will be helpful to have a table as well as translating the bounds of the other algorithms into linear case to see in detail. Additionally, there are several pessimistic offline RL algorithms with general function approximation that only require optimization oracles instead of the more difficult bonus oracle, and no comparison with those papers are presented:

Cheng et al. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR

Rashidinejad et al. ""Optimal conservative offline rl with general function approximation via augmented lagrangian."" arXiv preprint arXiv:2211.00716 (2022).

Ozdaglar et al. Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation. arXiv preprint arXiv:2212.13861

Zhu et al. Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a pessimistic nonlinear least-squares value iteration algorithm to tackle the offline reinforcement learning problem. The main motivation of the paper is to propose an algorithm that are both computationally efficient and minimax optimal w.r.t. the complexity of nonlinear function class. The proposed pessimism-based algorithm strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation and is oracle efficient. Also, the proposed algorithm is proven to be optimal w.r.t. the function class complexity, closing the gap originated from the previous work on differentiable function approximation.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
1) The proposed algorithm is proven to be optimal w.r.t. the complexity of nonlinear function class, closing the gap from the previous work on the differentiable function class and generalizes it to the wider nonlinear function class.
 2) The proposed algorithm is computationally efficient if there exist the efficient oracles for both regression minimization and bonus function optimization/searching.

Weaknesses:
1) The paper's presentation needs some work. For example, the terminology definition is not consistent. The D^2 divergence definition in Definition 3.2 is not consistent with the later terminology of D_F in line 239. The language itself needs some work too. For example, lots of places where it needs 'an', but 'a' is used and vice versa. Please define RL before using it in the abstract. There are also some ambiguities in the definitions that needs clarification in the Question section. 
2) The paper's claimed contribution is a bit exaggerated. Although the proposed algorithm does not need the computationally heavy optimization as previous works in planning phase, it transfers the main computation burden to the Oracle to find the satisfied bonus function, which seems to be a very time-consuming task. It also applies to the claim of being the first statistically optimal algorithm for nonlinear offline RL. Being able to get optimal result in the reduced linear function class does not necessarily mean it's optimal in the broader nonlinear class.
3) Although the considered class is the nonlinear one and general than the previously considered linear or differentiable class, the techniques used in the analysis are nothing new in my opinion, except re-defining the metrics in the nonlinear function class and connect the results together along with additional assumptions.
Overall, I think the paper is well motivated, but given the presentation and the insignificant contribution, it's not ready to be published. 

Limitations:
N/A

Rating:
3

Confidence:
3

";0
vBx0yNQmik;"REVIEW 
Summary:
This paper introduces an approach on Federated learning using dataset distillation techniques

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using dataset distillation for FL is interesting
2. The solution is reasonable 
3. The experimental results show the effectiveness of the proposed approach 

Weaknesses:
1. In a few equations, the details is not provided. For instance $L_CE$ in 3, $Dist$ in 5. The paper should be self-contained 
2. The technical contribution is low
3. In the experimental results, Table 1, can you highligh both first and second place? In MNIST-M, the winner should be VHL/R, 85.7> FedLGD 85.2. 

Limitations:
The limitation on using virtual data should be discussed. Is there any drawbacks on using fake data instead of real data?

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a method called FedLGD that utilizes distilled virtual data on both clients and the server to train FL models. To address the synchronization issue and class imbalance, the authors use iterative distribution matching to distill the same amount of local virtual data on the clients for local model training, thereby improving the efficiency and scalability of FL. The authors also reveal that training on local virtual data exacerbates the heterogeneity issue in FL. To address this problem, they use federated gradient matching to distill global data on the server and add a regularization term to the local loss function to promote the similarity between local and global features. They evaluate the proposed FedLGD method on benchmark and real datasets and show that FedLGD outperforms existing heterogeneous FL algorithms.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors use visualization to reveal the limitation of local data distillation in federated virtual learning, which makes the motivation of the proposed method clear.
2. The proposed method preserves local data privacy by leveraging averaged local gradients to distil global virtual data.
3. The experiment results validate performance improvement and privacy protection.


Weaknesses:
1. The initialization for data distillation requires each client to calculate the data statistics and the server to aggregate these statistics, which still raises privacy concerns since the statistics contain some private information. How about using random initialization or other strategies? The authors need to justify it. 
2. Compared with VHL that uses untrained StyleGAN without further updates, the proposed FedLGD method needs to update the global virtual data iteratively.  Therefore, it is not surprising that FedLGD outperforms VHL. If the StyleGAN can be updated the same number of times as FedLGD, does FedLGD still outperform it? This requires justification or experimental validation.
3. The structure of the proposed method is not clear enough, which makes it difficult to follow. The authors first present the overall pipeline and then describe each component. However, the connection between the components and the overall pipeline is not clear. This requires significant revision.
4. The presentation quality is not satisfactory. There are too many typos and grammatical errors. Some notations are unclear, e.g., $i$ represents both the data index and client index; the subscript $t$ disappears in many places; $\tau$ is a set but denoted as a scalar in the caption of Figure 2.


Limitations:
1. As pointed out by the authors, data distillation incurs additional communication and computation cost. Further investigation is required to enhance the efficiency.
2. The proposed method performs well but lacks of theoretical analysis to support its performance improvement.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a method to address data heterogeneity from the perspective of dataset distillation, named FedLGD. Specifically, the proposed iterative distribution matching and federated gradient matching strategies are used to iteratively update the local balanced data and the global shared virtual data, and the global virtual regularization is applied to coordinate the data domain drift between clients effectively. This method can effectively solve the problem of client data imbalance and domain drift in heterogeneous data scenarios. Extensive experimental results show the effectiveness of the proposed method. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The problem of data heterogeneity studied in this paper is important for applying federated learning in real-world scenarios.
2. The idea of this paper to solve the problem of data heterogeneity in federated learning through the dataset distillation method is novel.
3. The authors perform various experiments to analyze the proposed method


Weaknesses:
1. Some symbols are written differently. The authors should unify these symbols. In section 3.1, local virtual data is written as $\widetilde{D}_{I}$, but in section 3.2 and 3.3 is written as $\widetilde^{D}{c}$, in Figure 2 is $\widetilde{D}_{t}^{c_{I}}$. In Eq. (3), $L_ {Con} $is about \ widetilde ^ {D} {g} and \ widetilde {D} ^ {c} function. But they do not appear in Eq. (4). The authors should give more details about Eq. (3) and (4). 
2. other approaches to address heterogeneity are personalized federated learning, e.g. FedAMP[1] Ditto[2] KT-pFL[3], etc. However, it is not mentioned in related work, and there is no comparison in experimental methods.
3. The legend and curve in Figure 3a do not match. In Table 1, ResNet18 generally performs worse than CNN. The authors mention that overfitting may be happening (at line 275). The authors should increase the dataset size or use a smaller model to make the results more convincing.
4. According to the results in Figure 4, the visualization results of FedLGD do not define the boundaries of each class well. Although FIG. 4 can prove that FedLGD solves the data drift of both clients, the degree of clustering of each class looks reduced. The authors should analyze it further. 


[1] Huang, Yutao, et al. ""Personalized cross-silo federated learning on non-iid data."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.
[2] Li, Tian, et al. ""Ditto: Fair and robust federated learning through personalization."" International Conference on Machine Learning. PMLR, 2021.
[3] Zhang, Jie, et al. ""Parameterized knowledge transfer for personalized federated learning."" Advances in Neural Information Processing Systems 34 (2021): 10092-10104.


Limitations:
The authors discuss the limitations in section 5.

Rating:
6

Confidence:
3

REVIEW 
Summary:
To solve the challenges of synchronization, efficiency, and privacy, this paper presents a local-global distillation mechanism for FL (FedLGD). In FedLGD, an iterative distribution matching scheme is proposed to distill global virtual data to alleviate the heterogeneous problem. Experiments have shown superiority of FedLGD compared with existing FL methods. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The whole pipeline of FedLGD is well depicted in Figure 2. Each component involved in the pipeline is carefully designed.

2. It is an interesting idea to solve the existing FL challenges from the virtual learning perspective. This can inspire future studies in this direction. 

3. Experimental results look solid with sufficient implementation details. 


Weaknesses:
1. It seems that only feature heterogeneity is considered in this work. How the proposed method performs on different heterogeneous cases should be discussed. 

2. The definition of small distilled dataset is not very clear, which can affect the readers’ understanding towards the motivation and detailed technical parts. 

3. Privacy concern. Since there are image-level data transferred between the server and clients, it is better to further discuss the potential privacy-preserving risks. 


Limitations:
Yes. The authors have addressed the limitations. 

Rating:
4

Confidence:
4

";0
MJJQRUFzeX;"REVIEW 
Summary:
The authors propose a new way of solving minimization problems that are not necessarily convex.
The main element of their approach is that they use some helper functions which give auxiliary information about the optimization problem at hand.
That enables them to give improved optimization algorithms.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality:
The main approach is original.
That is, combining stochastic and variance-induced second-order algorithms is a novel direction.

Quality:
Writing and results are of good quality; see questions.

Clarity:
Clear writing; see questions.

Significance:
Work is significant, as optimization is a central task in computer science.

Weaknesses:
Not significant weaknesses found;
see questions.

Limitations:
Yes.

Rating:
8

Confidence:
3

REVIEW 
Summary:
In this paper, the authors introduce a meta-algorithm for second-order methods which they refer to as the ``helper framework''. Their helper framework unifies stochastic and variance-reduced versions of Newton's method, and the framework could also be viewed as auxiliary tasks when applied to first-order methods. The paper uses the helper framework to establish various convergence rates for objectives with Lipschitz Hessian and helper functions with two different similarity conditions (bounded similarity and Lipschitz similarity). Their algorithm (Algorithm 1) applies this framework to analyze cubic Newton. Different settings of Algorithm 1 (e.g. using different helper functions) replicate stochastic Newton, variance-reduced Newton, stochastic and variance-reduced Newton, lazy Hessian Newton, etc. They apply the helper framework to gradient-dominated objectives. They derive convergence rates for stochastic cubic Newton algorithms with variance reduction in the gradient-dominated case, which they claim to be the first to establish.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Introduces a new framework to unify analysis of different variants of Newton's method. 
2. Instead of assuming convexity or strong-convexity, extends framework to gradient-dominated functions in general.
3. Through this framework, the authors claim to achieve improved complexity guarantees.
4. Well-written paper with clear notation and good recap of related work.

Weaknesses:
1. Unclear why we need a new framework to analyze the different variants of Newton's method.
2. Unclear if the meta-algorithm presented is implementable.
3. The improved complexity guarantees claimed in the paper are unclear.
4. The additional variables in the helper framework may make it harder to understand basic second-order methods.

The reason why I only rated a 2 for Contribution of this paper are mainly due to the four points above. I may have misread or overlooked some details. Please see the questions below that address the perceived weaknesses in more detail.

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper provides a unified analysis for the Cubic regularized Newton Methods with variance-reduced estimators to solve stochastic nonconvex optimization problems. The results rely on the helper framework which generalizes the use of stochastic estimators for first-order and second-order oracles. The analysis recovers best-known rates and improves in terms of Hessian computation via Lazy Hessian update. Convergence is considered in general nonconvex and gradient-dominated (PL condition) problems.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The strengths of the paper lie within the unified convergence analysis that can recover best-known rates. This can be done via using the helper function which is a clever way to generalize the estimation of stochastic first-order amd second-order (possibly higher...) oracles.

Although Algorithm 1 is similar to the original Cubic Newton method but slightly difference in the gradient/Hessian estimators and the update of snapshot points.

All claims are backed by proof. I have gone through the proofs and I find no problems.

Weaknesses:
The paper lists 2 options to update the snapshot but there is lack of discussion on why these options are chosen (not others).

Other than that, I agree with the weaknesses pointed out by the authors, especially on the choice of the helper function. As helper function is a new concept, the paper does not provide much details on how to find a concrete choice of helper function given an application. Also note that details on the choice of helper function are not needed in the results due to the similarity assumption.

Limitations:
I believe the authors have clearly pointed out the limitations of this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies stochastic Cubic Newton methods for solving possibly nonconvex minimization problems. The authors propose a flexible “helper framework” that unifies stochastic and variance-reduced second-order algorithms, offering global complexity guarantees.
This framework allows arbitrary batch sizes, noisy gradient and Hessian estimates, and includes variance reduction and lazy Hessian updates. The work achieves state-of-the-art complexities for stochastic and variance-reduced Cubic Newton methods without artificial logarithms. It also introduces a new efficient lazy stochastic second-order method for large dimension problems.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper proposes a new lazy stochastic second-order method.

Weaknesses:
- The paper lacks a discussion of variance-reduced methods in the literature, neglecting 
numerous related papers on the subject. 
- The experiments are only in Appendix and weak with a “toy” dataset. 
- The loss function is just similar to “convex” although using the non-convex regularizer. 
- Many assumptions have been used in the paper. That could restrict the applications of the 
problems. 
- It is not clear about the results that could use for machine learning applications.

Limitations:
NA

Rating:
3

Confidence:
4

";0
b4Tr8NWTDt;"REVIEW 
Summary:
The paper addresses the problem of multi-agent RL by using learned world models over multiple potential opponent policies. The technique uses a Dyna-style algorithm to train the core policy with a combination of experiences generated through a world model and experiences playing against opponents. One evaluation demonstrates the world models benefits from training on data from multiple distinct policies. A second evaluation compares ways to use experiences generated from a world model to train a policy, showing pretraining on purely generated experiences is effective to warm-start a policy. An ablation study compares the proposed Dyna-PSRO model to vanilla PSRO on three MARL games, showing improvements over the PSRO model.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
## originality

Modest. The paper extends existing lines of work on MARL and world models, specifically studying the question of the policy diversity for training the world model.

## quality

Modest. The core results (figure 5) show clear improvements over PSRO. This is limited to a small number of games and the games themselves are relatively simple game domains.

The paper does a good job of breaking down specific claims to isolated experiments.

## clarity

Low. It was difficult to interpret many of the figures (questions and suggestions below). Generally the results of each experiment were hard to understand and would benefit from a single clear statement of the core outcomes in each section.

## significance

Low. The core audience of this work is researchers in MARL and particularly those considering world models as a solution.

Weaknesses:
The experimental results are promising, but would benefit from expansion. There are a few experiments that would help:
1. More games from MeltingPot. I hate asking simply for ""more"", but in this case it would help to show how well the agents perform on a wide variety of tasks. The results would help clarify where DynaPSRO benefits and may reveal limitations or areas for improvement. The wider set of results would give others confidence in the generality of the improvements gained by planning against diverse other agents.
1. More complex games. Consider more complex environments from PettingZoo (https://pettingzoo.farama.org/) or SMAC (https://github.com/oxwhirl/smac) that would highlight the potential of these algorithms in more compelx scenarios. This would help address the point that world models can become unstable and the value of strategic diversity in scenarios that support a much wider array of behaviors.
1. Scaling experiments. For example, when do prediction improvements level off when adding more policies? The experiments only examine adding 2 policies, which is a sparse sample of the space of strategies for most games.

The evaluations would benefit from other baselines to compare. What other algorithms could be used aside PSRO?

The full evaluation (last experiment) would benefit from a set of ablation studies. This could easily replace some of the planning experiments as the ablations would examine similar capabilities. I ask for ablations as these will be more convincing that the parts of Dyna being used add benefit over PSRO.


Limitations:
Yes. The work is focused on integrating world models into game playing agents and recognizes the preliminary nature of this work along with the potential risks introduced by using a simulation (world model) for decision making in real world applications.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to PSRO algorithms, where a world model of the environment is learned concurrently to the iterative PSRO strategy expansion. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors are right that the problem of having to re-learn policies from scratch is a large problem in the PSRO literature. Therefore, the idea to co-learn a world model alongside the expansion of the empirical game, in so taking advantage of the diversity of experience created by agents with slightly different best-response targets is an interesting approach to this problem. To the best of my knowledge this is also a novel solution to this problem.  
- I really like the presentation of the paper, and in general I think the authors do a very good job in terms of analysing the different moving parts of the framework in a reasonable manner.


Weaknesses:
I have a few concerns with the paper, however none of these necessarily game-changing in my evaluation of the work.

- I think the greatest misgiving I have with this work is that the related work seems to miss quite a large collection of PSRO papers that probably deserve mentioning. PSRO-style algorithms is a fairly small research area and I am surprised that the authors fail to make mention to many variants. In particular, as there is a section on strategic diversity itself in the paper, it seems odd that the authors have failed to comment on the line of works on diversity-based PSRO frameworks. For example, [1], [2], [3], [4], [5] are all diversity PSRO approaches. It also fails to place itself in the literature involving PSRO algorithms that attempt to speed up convergence times such as [6], [7].

- Furthermore, I was additionally surprised at the lack of comparison to NeuPL [8] which is another population-based framework attempting to similarly deal with the best ways to transfer information between agents in the population. 

- I do not necessarily believe that the authors need to benchmark against all of the approaches that I have listed. I do however believe the paper still needs work in terms of placing itself within the current literature on PSRO and other population-based frameworks.

- Based on the above, my score is set at a borderline accept. However, I am willing to revise this upwards upon seeing a better framing of this work in the current literature.

REFERENCES  
[1] Policy Space Diversity for Non-Transitive Games - Yao et al. 2023  
[2] Open-ended learning in symmetric zero-sum games - Balduzzi et al. 2019  
[3] Modelling behavioural diversity for learning in open-ended games - Perez-Nieves et al. 2021  
[4] Towards unifying behavioural and response diversity for open-ended learning in zero-sum games - Liu et al. 2021  
[5] A unified diversity measure for multi agent reinforcement learning - Liu et al. 2022  
[6] Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games - McAleer et al. 2020  
[7] Neural auto-curricula in two-player zero-sum games - Feng et al. 2021  
[8] NeuPL: Neural Population Learning - Liu et al. 2022  



Limitations:
I think the authors actively engage with the limitations of the work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors consider learning world models for deep reinforcement learning in combination with the construction of empirical games through PSRO. They first show that world models benefit from training on a diverse set of strategy profiles as can be generated through PSRO meta-game solvers. They then empirically show that PSRO best responses can enjoy sample efficiency benefits by training with simulated world model experience. Finally, they present Dyna-PSRO, in which PSRO best responses make use of a world model trained on all available experiences collected thus far in a run of the PSRO algorithm. Dyna-PSRO provides lower-regret solutions with higher sample efficiency than PSRO without a world model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well written and presented, and the experiments are well designed. 
- World models are seeing increased use in the RL community, and PSRO is one of the more practical and general methods currently available for finding approximate game solutions. This paper provides insights on how to properly combine the two and make improvements to PSRO's sample efficiency, which is one of its largest issues.
- The proposed Dyna-PSRO method is sound.
- While many implementation details are not present in the main paper, the appendix describes these details thoroughly.

Weaknesses:
It would have been nice to see how current high-performing world model methods such as Dreamer, which employs latent state spaces [1,2] might perform with the same approach. It's not immediately clear if experiments like in section 3.3.2 would have had the same outcome.

[1] Hafner, Danijar, et al. ""Dream to Control: Learning Behaviors by Latent Imagination."" International Conference on Learning Representations. 2019.

[2] Hafner, Danijar, et al. ""Mastering diverse domains through world models."" arXiv preprint arXiv:2301.04104 (2023).

Limitations:
All limitations have been adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper describes combining two things: training a world model of a game, and doing Policy Space Response Oracles (PSRO) on the game.

Doing PSRO involves getting a lot of episodes from the game (episodes are used to train the RL best-responses, and also to estimate the payoffs of the empirical game). The novel algorithm in this work (Dyna-PSRO) can be thought of as a modification of PSRO where those episodes are **also** used to train a world model (which is essentially a learned simulator of the game engine). Then, the world model is used to improve the training process of the best-response policies. 

Through experimental results, the authors show that this improved training process (based on Dyna) can cause the best-response learners to learn a stronger policy than the normal method when using the same amount of interactions with the real game environment. It does this by training the policy using trajectories from the world model (in addition to the usual trajectories from the real game environment), and by equipping the agents with one-step lookahead planning during training.

This paper showcases experiments on the Dyna-PSRO algorithm in three games, and Dyna-PSRO outperforms PSRO in all three, as measured by an approximation of NashConv.

The paper also shows experiments to measure the quality of the learned world model, to test the hypothesis that Dyna-PSRO results in a good world model.

I think the paper has some flaws* in its current form, but the core work of the paper is good, the charts are beautiful, and the results are strong.

---

*Edit: many of the abovementioned flaws were addressed during the rebuttal/discussion period.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Overall, the paper is well-executed.
    - It is well-written and polished.
    - There has clearly been a lot of time and effort put into the engineering and writing of this paper. There are 4 sets of thorough experiments in the main paper, and more in the appendix.
    - The figures are extremely readable.
- The results concerning the performance of the best-response policies are impressive.
- The effort will surely be helpful to future researchers: the research directions of (1) improving the efficiency of PSRO response calculations and (2) training better world models should continue to flourish, and the work presented in this paper contributes to both.
    - The research direction seems natural, especially in the direction of using world models to improve the performance of PSRO.


Weaknesses:
I think the paper could be better in explaining or hypothesizing the ""why"" for a lot of things, even if just qualitatively.

I think the paper states some conclusions too strongly.

I think some things are not explained well enough and are confusing to the reader (at least, to me):

- SumRegret metric:
    - It's really not clear from the main paper how SumRegret works (even though it is explained in the appendix). This could be clarified by defining the terms ""method"" and ""combined game"".
    - Also, I would feel a lot better if I saw results measured by an alternative metric, where the deviation set is the set of **all** policies. The $max_{\pi_i \in \bar{\Pi}_i}$ could then be approximated by just training one more response policy (as if doing one last epoch of PSRO). This seems like it would be a more accurate approximation of the Nash Conv. Is there any reason to use the metric in the paper instead of this?
- Empirical Game Solution not described
    - Since the settings here are general-sum, it's probably important to specify what solution concept is used for the meta-strategies in the main paper (even though it is included in the appendix).
- Experiments in Section 3.1 Strategic Diversity
    - Looking purely at Figure 2, the conclusion ""Overall, these results support the claim that strategic diversity enhances the training of world models"" does not ring true to me. For example, there are three world models which perform better on the metric (accuracy) used in Figure 2 than the most diverse one, for Observations.
    - Even if I look in the appendix at E.1, there doesn't seem to be significant evidence to support the conclusion: multiple world models have similar recall scores than the most diverse one, and the one trained without the random policy seems to have better scores.
    - I would be interested in seeing the cross-entropy loss instead of (or in addition to) the accuracy.
- The discussion of the Decision-Time Planning results (3.2.2) seems incomplete:
    - ""**The main outcome of these experiments is the observation that multi-faceted planning is unlikely to harm a response calculation,** and has a potentially large benefit when applied effectively. These results support the claim that world models offer the potential to improve response calculation through decision-time planning."" (emphasis mine)
    - However, Figure 4 does show that decision-time planning causes the response to be *worse*: The solid blue line (top) has no decision time planning, and the dashed gray line (second from the top) has decision-time planning, and performs worse.
    - It would be nice if there was some discussion about this, perhaps an intuitive/qualitative reason why this is.
- Dyna-PSRO results need more details (Figure 5):
    - For each experiment, how many policies (iterations of PSRO) were there?
    - Does each policy train for a fixed number of steps, or until some measure of convergence is reached?
- Was ""policy"" vs. ""strategy"" ever defined like this before? In my opinion, we shouldn't define these terms like this, because they are usually considered synonymous. The terms I'm familiar with are ""policy"" or ""strategy"" for the former, and ""meta-policy"" or ""meta-strategy"" or ""meta-strategy distribution"" for the latter. Just my opinion!
- I was very confused by the definition of World Model while reading the paper.
    - Even after reading it through entirely, I was under the impression that each player had their own world model, and that it implicitly modeled the actions of the opponent.
    - If one misses the bold notation of the definition of agent world model from line 137 to 141, it's easy to think that this is the case, especially since the phrasing is that ""the agent learns and uses a ... world model"" (instead of, say, ""the agents learn and use a ... world model).
    - On one hand, the formal definition given for an ""agent world model"" is technically accurate, and I am just dumb. On the other hand, I suspect many of us are dumb, and will be similarly confused upon reading the paper. (Also, I'm not **that** dumb: it's really hard to tell that the O and A are bolded!) (Also, even if some of us are not dumb, we are likely lazy and will gloss over the explanation that boldface means joint.) This is all to say that I would suggest explicitly stating that the world model takes as input an observation and action from **each** player, and returns an observation and reward to each player. And that the world model does NOT model the actions of any player.
- The bolding is nice, but it would be less confusing to **also** say ""strategy profile"" or ""joint strategy"" anytime this is meant instead of just ""strategy"" and something bold, as it's very easy to miss or forget what something bold means (plus, it seems incorrect to call a strategy profile a strategy). Also, maybe emphasize that sentence that explains what boldface means, so that readers don't miss it?
    - For example in line 774 and 775 of the appendix:
        - ""This is typically not tractable, but instead draws are taken from a dataset generated from play of a behavioral strategy **σ**. And the performance of the world model is measured under a target strategy **σ∗**""
        - should be ""strategy profile"" and ""target strategy profile""
    - and same in Line 159 and 160 of the main paper, and throughout section 3.1


Limitations:
Limitations addressed

Rating:
6

Confidence:
4

";0
wjqT8OBm0y;"REVIEW 
Summary:
In this paper, the authors formally define five anomalies for an
explainability score and prove that for every n >= 4, there exist
Boolean classifiers defined over n features that exhibit one or more
of these anomalies for the SHAP score. In this way, the authors
provide evidence of the inadequacy of Shapley values for
explainability.

The aforementioned anomalies are defined by considering the concept of
abductive explanation. More precisely, given a binary classification
model M : {0,1}^n -> {0,1} and a tuple v in {0,1}^n, a subset X of {1,
..., n} is said to be a weak abductive explanation of (M,v) if for
every y in {0,1}^n such that y[i] = v[i] for every i in X, it holds
that M(y) = M(v). In other words, the values of v for the features in
X are enough to obtain the same result as M(v), so they are enough to
explain the output of M for v. Moreover, a subset X of {1, ..., n} is
said to be an abductive explanation of (M,v) if X is a weak abductive
explanation of (M,v), and there is no weak abductive explanation X' of
(M,v) such that X' is a proper subset of X. In other words, X is an
abductive explanation for (M,v) if X is a minimal weak abductive
explanation for (M,v). Then a feature i is said to be relevant for
(M,v) if there exists an abductive explanation X of (M,v) such that i
belongs to X, and otherwise i is said to be irrelevant for (M,v). With
this notion of irrelevance, the anomaly I5 for the SHAP score is
defined as the existence of a feature i such that i is irrelevant for
(M,v), but the absolute value of the SHAP score of i is greater than
the absolute value of the SHAP score of every other feature. Thus,
this can be considered as an anomaly of the SHAP score, as i is an
irrelevant feature that is considered more relevant according to the
SHAP score that all the other features (some of which are
relevant). The other four anomalies considered in the paper (I1, I2,
I3, I4) are defined in a similar fashion.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The five notions of anomaly studied in the paper clearly represent
anomalies for explainability scores. These notions are properly
formalized in the paper.

2. The paper provides valuable insights into the SHAP score,
specifically providing a formal framework to assess its adequacy as an
explainability score.

3. The paper provides one of the first formal results of the
inadequacy of Shapley values for explainability.

4. The paper is well written.

Weaknesses:
1. The results of the paper show that a tiny proportion of the Boolean
classifiers defined over n features exhibit some of the anomalies I1,
I2, I3, I4 or I5. For example, the paper proves that at least
2^{2^{n-1} - n - 3} Boolean classifiers exhibits anomaly I1, which is
a tiny proportion of the 2^{2^n} possible Boolean classifiers defined
over n features. Hence, it could be the case that the vast majority of
Boolean classifiers do not exhibit the anomalies studied in the paper.

2. In practice Boolean classifiers are given in some specific
formalism, such as decision trees or binary decision diagrams. The
authors do not provide any results about the formalisms that are
suitable to express the Boolean functions exhibiting anomalies. For
example, is it possible to express the Boolean functions in the proofs
of Propositions 3, 4, 5 and 6 as decision trees of polynomial size in
the number n of features? If this is not possible, can these functions
be expressed as FBDDs (or d-DNNFs) of polynomial size in the number n
of features?

Limitations:
The following are the main limitations of this work (see Weaknesses), 
which are not addressed in the paper. 

- The results of the paper show that a tiny proportion of the Boolean
classifiers defined over n features exhibits some of the anomalies I1,
I2, I3, I4 or I5. 

- The authors do not provide any results about the practical
formalisms (such as decision trees) that are suitable to express the
Boolean functions exhibiting anomalies. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper reviews previous work on ideas of feature importance and hi-lighted inconsistencies with Shaley values. It defines ideas of importance and irrelevance of features in a Boolean ML model. These definitions are based on the idea of a minimal set of inputs needed to freeze an model output. necessary inputs are in every minimal coalition that can freeze the output, relevant inputs are in at least one minimal coalition, and irrelevant inputs are in no coalitions. They then go on to show that, among other issues, there exist Boolean models and certain inputs where irrelevant inputs are given large Shapley values, while relevant inputs are given a Shapley value of zero. Thus, the logic goes, Shapley values do not track importance.

The paper's original contributions are to prove that model/input pairs with issues exist/can be found for models of any input size. Previously, only small models were exhibited to have these issues, but it was unknown if larger models also had these issues. They also give lower bounds on the number of models that have these issues.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Generally clear and straightforward exposition.
- Good background and presentation of previous results.
- Results are easy to understand.
- idea of necessary, relevant, and irrelevant is intuitive.



Weaknesses:
- Paper is based on a comparison of apples to oranges, without an in-depth analysis of the issue. It is possible that the whole paper is based on a misunderstanding. Further analysis is needed.
- Some grammatical issues.
- Contributions are not very significant.

Limitations:
The author has not discussed the limitations of the claim that Shapley values are refuted. This statement seems not entirely supported. See questions.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper demonstrates / constructs functions with features whose Shapley values (i.e., attributive importance in a prediction) is misaligned with their true relevance.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- Addresses a theoretical gap in our understanding of Shapley values.

Weaknesses:
- I find the problem being investigated to be mostly a mathematical curiosity that so happened to be open and has now been addressed. 

Limitations:
n/a

Rating:
3

Confidence:
3

REVIEW 
Summary:
Based on definitions of feature necessity, relevancy, and irrelevancy from previous work,as well as systematic issues with Shapley values for explainability on boolean classifiers (e.g. non-zero Shapley values assigned to irrelevant features, zero Shapley values assigned to relevant features, among others) identified in previous work, the authors offer proof for their existence in functions with an arbitrary number of variables. They conclude that the existence of such systematic issues is cause for concern in using Shapley values for explainability, as misleading information about feature importance can induce errors in human decision making.
 

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- Originality: The work offers proof for the existence of issues with Shapley value explanations on boolean functions with an arbitrary number of variables that were previously only studied empirically.
- Quality and Clarity: The theoretical framework, preliminaries, and proofs are described in a very concise manner. Despite the theoretical nature of the paper, the authors are able to concisely state to the reader what is described in each formula (e.g. lines 125-127: Thus, given an instance (v, c), a (weak) AXp is a subset of features which, if fixed to the values dictated by v, then the prediction is guaranteed to be c, independently of the values assigned to the other features). Similarly, the main idea for each proof is described in a very intuitive manner, increasing readability of the paper significantly.
- Significance: The present work proves systematic issues exhibited by Shapley value explanations on boolean functions. Shapley values are one of the most popular solutions, as they are based on clearly defined axioms, i.e. properties deemed desirable for explanations. For boolean functions, the present work shows that these axioms (which Shapley values do fulfill) may be lacking for treating irrelevant and relevant features as would be expected.

Weaknesses:
I am a bit concerned with the novelty, as the present work only provides proof for observations about unexpected behavior of Shapley value explanations for boolean functions that were already observed empirically in previous work (however, the authors also state themselves that these issues have been identified empirically in previous work). To raise concern about e.g. I1, it would be sufficient to simply identify a case where irrelevant values are assigned nonzero Shapley values. 

I also believe the title promises a bit more than is provided by the paper. The proofs and resulting claims are restricted to boolean functions, however, it would be interesting to see how and if the described issues occur in continuous settings, e.g., when explaining DNNs.

Limitations:
restriction to boolean functions, as described in ""Weaknesses"" section. I think a paragraph of how the described proofs and observations may impact Shapley value explanations in more real-world settings would go a long way here, as well as suggestions on how to mitigate the proven issues.

Rating:
6

Confidence:
4

";0
iF4gWu6QjU;"REVIEW 
Summary:
The paper deals with variational inference (VI) of the posterior distribution when using a diffusion based generative model as the prior distribution. The paper proposes using a lower bound on the log probability of the prior distribution (instead of calculating it through the  ODE in standard diffusion based generative models as in [1]) for the optimization of the Kullback-Leibler (KL) divergence between in the VI framework. This results in a more efficient training procedure for the VI than using the true log probability as well as a more efficient memory footprint. 

The authors then validate their method using an MRI dataset as well as the most common CelebA and Cifar datasets.

[1] Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William T318
Freeman. Score-based diffusion models as principled priors for inverse imaging. arXiv preprint319
arXiv:2304.11751, 2023

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* The paper proposes a solution to estimating the KL that is indeed way faster than the previous known methods and this allows for **effective gains in training time and memory footprints**. They provide some visual evidence that those gains do not alter considerably the quality of the variational approximation of the posterior distribution.

* The presentation of the problem is good and they motivate the need for an efficient algorithm with a real word application (Accelerated MRI).

* The explanation of the method is clear and reproducible. I expect practitioners would be able to clearly implement the algorithm with the description given in the main part of the text.

Weaknesses:
* The paper for me gives a slightly overstated presentation of the variational inference as a sampling from a true Bayesian posterior. The Bayesian posterior is clearly defined once one states that the prior is the diffusion based generative model. Therefore, when doing Variational Inference, we are only approximating (to an unknown degree) this posterior distribution, so I no way the samples generated by the outcome distribution of the VI procedure are not an approximation of the Bayesian problem.  Of course, VI is still a useful approach, but I would say that I agree that the proposed algorithm is closer to the ""true Bayesian inference"" (line 42) than other such as DPS [2] or SMC-DIFF [3].

* The numerical evaluation and comparison with other algorithms is insufficient.
 1) **Evaluating the distance to the true posterior**: The numeric compare only visually the posterior with the posterior obtained from [1]. When comparing with [2], the paper compare SSIM and PSNR as well as visual assessment of the reconstructions. As stated in line 258 of the paper, for ill posed inverse problems comparison to the ""true image"" can not be considered an adequate metric. I'd suggest considering an example where the posterior is analytically available (for example, when considering a Gaussian likelihood with a diffusion model over a mixture of Gaussians). In such case, several metrics can be used to compare the different methods ([1], [2]) such as the sliced wasserstein or even the KL.

2) **Complexity**: When comparing with other methods such as [2], we should keep in mind that the proposed method needs an optimization problem for each measurement $y$ (minimization of the KL). This is not the case for some of the ""posterior diffusion samplers"" such as [2] and [3]. Therefore, the actual computing time needed once we receive a measurement $y$ is smaller for [2] and [3] quite considerably.

I'd be inclined to augment my grade if the quantification of the error to the true posterior is better understood, specially in comparison with [1] and [2]. 



[2] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, & Jong Chul Ye (2023). Diffusion Posterior Sampling for General Noisy Inverse Problems. In The Eleventh International Conference on Learning Representations .

[3] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, & Tommi S. Jaakkola (2023). Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem. In The Eleventh International Conference on Learning Representations .



Limitations:
NA

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper focuses on solving inverse problems using diffusion based probabilistic models. The approach considered consists in minimizing the KL divergence between a variational posterior and the true posterior of the diffusion model. Computing this KL involves approximating the log probability of the diffusion model's marginal (which is assumed to approximate the true data generating distribution). A previous paper [1] used the very expensive ODE approach to approximate this log probability, making the whole method prohibitively expensive and inefficient when factoring in the computational cost. The present paper suggests instead minimizing an upper bound on the KL divergence, using a lower bound on the log probability of interest that was derived in [2]. 

[1] *Feng, B.T., Smith, J., Rubinstein, M., Chang, H., Bouman, K.L. and Freeman, W.T., 2023. Score-Based diffusion models as principled priors for inverse imaging. arXiv preprint arXiv:2304.11751.* 

[2] *Song, Yang, Conor Durkan, Iain Murray, and Stefano Ermon. ""Maximum likelihood training of score-based diffusion models."" Advances in Neural Information Processing Systems 34 (2021): 1415-1428.*

 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The proposed method is more efficient than that proposed in [1]. It results in a drastic reduction of computational time and memory cost while maintaining having very similar performance. 

Weaknesses:
- I believe that the KL approach is sound and reliable since the resulting variational approximation will likely not sample outside the support of the posterior. It is however extremely costly in terms of computational time and memory cost. It takes 9 hours to obtain a variational approximation over a **single image** of dimension 2^16. On the other hand, classical posterior sampling methods such as DPS [3] take less than one minute for larger images. One might then argue that such methods do not target the exact posterior and it is true! However, the approach in the present paper is also not guaranteed to sample the correct posterior; the forward KL suffers from mode collapse and posteriors over high dimensional images are highly multimodal. The only advantage of this approach is that it will likely not ""hallucinate"" but I do not think that it is in anyway competitive with other existing methods when one factors in the computational cost. 

- The authors should have at least illustrated their method on simple toy examples where the posterior is available and multimodal, so that we can see if it indeed recovers the posterior completely. 

[3] Chung, Hyungjin, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. ""Diffusion posterior sampling for general noisy inverse problems."" ICLR (2023)

Limitations:
see above. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
Authors propose a non-amortized variational inference approach to solve large-scale Bayesian inference problems where the prior is based on an cheap-to-evaluate approximation to a pretrained diffusion model.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:

**Originality.** This paper hits the nail on the head when it comes to large-scale Bayesian inference in the context of inverse problems, especially when diffusion models are used as priors.

**Quality and clarity.** The paper is well-written and easy to follow. The authors have done a great job in explaining the proposed approach and the experiments are well-designed to demonstrate the effectiveness of the approach.

**Significance.** This approach allows to leverage the power of diffusion models in approximating complex distributions in solving large-scale Bayesian inference problems. This is a significant contribution to the field and I am interested to apply this approach to another inverse problem domain.


Weaknesses:
* My primary concern revolves around the justification for selecting diffusion models as prior distributions over alternative generative models. It would greatly enhance the paper to thoroughly examine the advantages and disadvantages of diffusion models compared to other generative models, specifically within the framework of large-scale Bayesian inference. It would be ideal to include a comparison with amortized normalizing flows and/or injective flows. This raises the question: why not initially employ a normalizing flow to learn the the prior or full posterior distribution (amortized VI)?


Limitations:
* It would be beneficial to provide additional comments on the limitations that arise when dealing with ""out-of-distribution"" data (unknown being out of diffusion model distribution).

Rating:
9

Confidence:
5

";0
zR6V9fPRBn;"REVIEW 
Summary:
This paper presents a Shapley value based cohort discovery, by constructing ""Negative Sample Shapley Field"" that possesses isotropy property. By doing so, negative samples can be effectively clustered and separated with respect to the Shapley values. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
I think this paper points out many important problems in healthcare research. Specifically, 
1: how to deal with pos/neg imbalance and how to make better use of vast negative samples?
2: how to identify negative samples that are more useful for the current research problem? 

The use of latent variable models to deal with misssingness in EHR dataset is a promising approach too. 


Weaknesses:
1: Correct me if I'm wrong, but I think Eq.2 is ill-defined. For any metric M, let's say accuracy, then $s_i = s_j$ as long as both negative samples have the same predicted labels by a predictor F, right?
2: Many parts need justifications. For example, a) why is the defined metric in Eq.2 means high contribution to prediction task? b) why does Eq.6 help with isotropy? c) How is k-th DAE different from a normal encoder with k layers? 
3: I don't see why DBSCAN + AE is proposed as a contribution when you can simply use VAE. 
4: I doublt the logic between line 248 and line 249. The defined Shapley value in Eq.2 is 0 does not mean these patients are healthy. Note that you are defining M=AUROC, therefore M has a very stable value when you have sufficient samples to draw a smooth ROC curve when calculating $M(D^+ \cup A)$. As a result, $s_i = E[ M(D^+ \cup A \cup d_i) - M(D^+ \cup A)]$ is usually zero.


Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
The paper addresses the cohort discovery problem for supervised learning in the machine learning for healthcare domain. Positive examples of the cohort are easy to identify while it is not as straightforward to determine which negative examples should be admitted into a cohort.  To deal with this problem, the paper calculates the data Shapley value of the negative samples in the dataset. Then, the paper carried out representation learning using a stacked denoising autoencoder to mitigate the nonuniform changes of Shapley value in the original feature space. Finally, the paper carried out clustering in the learned representation space to identify important negative examples to create the cohort. The paper evaluated the proposed method on a clinical dataset to demonstrate the utility of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The paper deals with the cohort discovery problem, which is an important problem in the machine learning for healthcare domain.

* The proposed method is straightforward and makes sense to me for the most part.

* In terms of empirical evaluation, the paper provides a detailed explanation of the cohorts discovered from a clinical perspective, although I won't be able to judge whether such findings make precise, clinical sense given that I do not have a medical background. I also appreciate the authors carried out additional experiments to study the effectiveness of each component of the proposed method.

Weaknesses:

* Because the proposed method is straightforward and directly takes advantage of existing methods, I am not quite sure whether the paper has enough technical novelty from a machine-learning perspective.

* Regarding experiments, while I think the authors dive deep into providing an analysis of the outcome of the proposed method from a clinical perspective, there are no alternative methods compared to the proposed method to understand the performance of the proposed method. It would also be interesting to see the proposed method applied to more than just one dataset as discussed in the paper. Finally, it should also be noticed that identifying relevant negative examples is not a problem that is exclusive to the healthcare domain. Many application domains will be interested in the proposed method to identify relevant negative examples for binary classification problems. As such, the authors may also consider applying their methods beyond the medical domain down the road.

* Clarity of the paper can be improved. Some key concepts are not well explained. For example, what is the role of data Shapley value? It appears to be the contribution of a data point to the learned classifier. The authors do not seem to elaborate on this concept enough in the paper. What's the intuition behind it? Why it makes sense to use Shapley value to measure contribution?  I also don't think the authors explain well the phenomenon of ""the non-uniform distribution of negative samples with similar data Shapley values"". Further intuition on this point will help to better motivate the need for representation learning.



Limitations:
The paper mentions some limitations related perspectives in the conclusion section.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper describes a method to understand the set of unlabelled / negative samples in a healthcare data-set. In this setting, one typically has a set of patients with a particular label, such as indicidence of a particular disease, and a large set of unlabelled samples. Training a classifier involves selecting some subset of the unlabelled samples as the set of negative samples for training. This set is often quite heterogeneous, so methods that enable better understanding of the structure in the data and selection of negative samples can be informative. 

The key contributions are as follows:
1. Definition of the Negative Shapley Value Field, which associates the Shapley Value for the the prediction task of interest with each negative sample
2. Illustration of a representation learning method which discovers a low-dimensional representation for the negative samples in which samples with similar Shapley Values are close to one another
3. A method for cohort discovery based on clustering in the low-dimensional space and interpretive analysis to demonstrate the clinical coherence and relevance of the discovered cohorts
4. Demonstration of improved predictive performance when selecting samples based on the negative Shapley value
5. Demonstration that predictive performance is maintained when the low-dimensional representations are used

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem of understanding and selective negative samples for cohorts in healthcare data is an important one and methods that can be used by practitioners in this domain will be valuable. 

The authors provide a demonstration of the utility in improved classifier performance when limiting to the set of negative samples with Shapley value > 0. This is suggested to be due to those samples which have negative Shapley value corresponding to patients who are likely to present with AKI in the future, but do not yet have this information in their medical record. This is a nice result and tackles a common problem in biomedical data science. 

Mapping the unlabelled samples into a low-dimensional space where samples with similar representations are expected to have similar Shapley values is shown to enable the discovery of distinct and interpretable cohorts of samples with similar features and similar Shapley values. This result could provide a useful tool for practitioners to select or filter the set of negative samples when building classifiers on EHR data

Weaknesses:
*** These weaknesses have been addressed in the author response ***

The ""Effectiveness of the Negative Sample Shapley Field"", described in lines 316-322, is illustrated by showing that filtering the set of unlabelled samples to exclude those which had a negative Shapley value improves the performance of the trained classifier on held-out data reminded me of co-training [1] or positive-unlabelled learning [2]. It would have been interesting to see this approach benchmarked against other methods for developing classifiers based on positive and unlabelled data, where we expect a number of the unlabelled samples to be positive rather than negative samples

The ""Effectiveness of Cohort Discovery"", described on lines 332-345 is a nice result but it is not clear from the experiments to what degree the isotropy constraint enabled this. This could be demonstrated by an experiment in which the same SDAE model is applied to the data  without the isotropy constraint.

[1] Blum, A., Mitchell, T. Combining labeled and unlabeled data with co-training. COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann, 1998, p. 92-100.
[2] Bekker, J., Davis, J. Learning from positive and unlabeled data: a survey. Mach Learn 109, 719–760 (2020). https://doi.org/10.1007/s10994-020-05877-5

Limitations:
*** These concerns have been addressed in the author response ***

This is an interesting paper with some results which could be useful in biomedical data science, but which would be made more convincing by more thorough benchmarking and comparison with other approaches to achieve each of their key results.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In healthcare analytics, cohort constructions is one of the key steps that drives the analysis. For most problems, where the outcome of interest is a disease, the problem has asymmetrical formalism - while patients with disease are defined using string criterion and are homogenous w.r.t problem the negative set can be diverse and can have important information that is under-analyzed. The authors present a Shapley value driven approach to analyze the negative set in terms of their contribution to the predictive power of the models. Furthermore, these mappings are transformed and clustered to identify potentially clinical important patients. They have presented results and commentary from clinicians on identified clusters.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The authors raise an interesting hypothesis about under-analysis of the negative samples that can drive the community to develop standard methods to handle such problems 
- It is commendable that the authors validated their findings and presented commentaries from clinicians on the identified patterns. Such efforts are increasingly important to ascertain the clinical validity of proposed AI methods
- Overall, the intuition behind the method is novel and somewhat defensible. The authors have also made an effort to formalize many aspects of their approach 
- The authors have also made an effort to validate the components of the method individually (see more on this below)

Weaknesses:
- The primary weakness of the paper is a lack of comparison against baseline methods that necessitates the complexity of the proposed methods. There is also a lack of studying the correctness of the proposed cohort discovery method. It may be beneficial for the authors to support their claim on a synthetic datasets and/or provide comparisons of discovered cohorts using other standard methods such as contrastive PCA. 
- Continuing from the above, the computational complexity of the proposed approach hasn't been acknowledged in a satisfactory manner. While Monte Carlo methods have been proposed to calculate the values, the true complexity in evaluating over the entire negative set and the subsequent calculations imposed the isotropy constraints hasn't been analyzed clearly. 

Edit: The authors have responded by providing additional baseline comparisons that alleviates some of the concerns. I have updated my review to reflect the same

Limitations:
N/A

Rating:
5

Confidence:
5

";0
Eu4Kkefq7p;"REVIEW 
Summary:
The paper proposes a new system for learning a joint embedding space for text, images, and 3D shapes (point clouds). It starts from pre-trained CLIP embeddings for text and images, and learns one more embedding function for 3D shapes in a self-supervised manner, using a contrastive loss. Several additional improvements are proposed, including combining several datasets, cleaning some of those including via automated re-labelling of Objavrese, and optimisation of the 3D embedding neural network architecture. Because of these improvements, the new embedding performs significantly better than prior works on strand tasks such as ""zero-shot"" classification.



Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The paper contributes sound incremental work on learning multi-modal embedding functions for text, images and 3D shapes. Various engineering dimensions are considered (model scaling, data diversity, scaling and quality, etc) and some insights are provided on how each of these can be improved, at least compared to current baselines. I expect these findings to provide useful guidelines for future work in this area.

* The paper does contain some interesting suggestions on how to improve datasets such as Objaverse to train multi-modal embeddings. The scheme where captions are assessed via GPT-4 and then replaced using BLIP if needed is useful.



Weaknesses:
* The paper is mostly about good engineering, but there isn't a lot of very deep technical innovations, or qualitatively surprising findings. The main finding, in fact, is that this is an area where scaling is still very limited (mostly due to the lack of suitable training data), and that, thus, scaling is where most of the low-hanging fruits can be had. This is a good but not very surprising message.

* The model is trained on synthetic datasets and looses much of its edge when applied to real data, compared to prior models like ULIP that are otherwise suboptimal when tested on synthetic data (e.g., Figure 5, right panel).

* Likewise, there isn't a very clear dominance of the two proposed backbones, SparseConv and PontBert: their performance swings significantly depending on the testing data in Table 2.



Limitations:
The paper does address some limitations, particularly in the discussion and conclusions where they are implicitly recast as ""future work"".

There is no discussion of ethics, although this does not seem to be an issue for this paper (notoriously, though, dataset like ShapeNet had issues with copyright, and I don't believe ObjaVerse is necessarily immune from such controversies either).


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of multi-modal learning of text, image and shapes. Shapes are represented by pointclouds and the learning is driven by the standard contrastive loss. The main technical difference of this work to prior ones lies in the scale of data and corresponding training strategies. The authors train their model on a much larger scale of data which pools Objaverse, ShapeNet, 3D-Future and ABO. To handle the noise in the text (mainly from the less-curated Objaverse), the authors use several large pretrained models to clean up the text and make it better aligned with the shapes. Hard-mining is also leveraged to make the model train better under class imbalance. With these techniques which make the model scales better with data size, the proposed model achieves the new SOTA performance across several relevant tasks and datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The model in this paper achieves significantly better results than previous SOTA in zero-shot shape classification. It also demonstrates great qualitative performance in shape retrieval, shape interpolation, shape captioning and has good potential in other cross-modal applications as well. Such model will be practically very useful.
2. The benchmark and the findings in the paper about different models' scaling performance are valuable information to the community and future research along this direction.
3. The paper is well-written and easy to follow.

Weaknesses:
1. The evaluation of the zero-shot shape classification on Objaverse-LVIS might not actually be zero-shot. Based on the description in L235, even the ""Ensembled (no LVIS)"" can include LVIS categories, because seemingly the author only excludes the exact evaluation samples from the training set. Ideally, all shapes from the evaluation *categories* should be removed from the training set. This makes the real zero-shot generalization performance of the model potentially lower than reported and the overall comparison on this test set less informative. Besides, ModelNet40 and ScanObjectNN can also have overlapping categories with the training set. (If the authors think category overlapping does not violate their definition of zero-shot, it should be clearly stated so / ""zero-shot"" should be clearly defined at the beginning.)

2. The linear probe results show the proposed model performs similarly to ULIP-retrained on ModelNet and ScanObjectNN. The authors try to explain it with in-category sample bias (what does this mean?) and domain gap dominance, without providing any evidence for these hypotheses. My concern here is that, given that OpenShape performs much better than ULIP in shape classification (requires both shape and language rep) but not linear probe (only requires shape rep), the actual underlying reason could be that the shape representation is of similar quality when tested on out-of-domain, and the real difference between these models lies in the text representation. I believe it would be quite beneficial to provide more analyses on this result, as this is central to the ""shape representation learning"" story of the paper. For example, such analyses could be visualizing and comparing the latent structure of the shape representation; or simply performing shape-latent based NN query and evaluate the distance between queried shapes; or measuring the shape latent's alignment with image latent, to name a few. 

3. The limitation of the model is not discussed at all. This is a very important aspect for readers to thoroughly understand the contribution of this paper.

Limitations:
The limitations and failure cases are not discussed and should be included.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a joint learning framework for multi-modal representations among text, image, and point clouds. Specifically, the authors fix the pretrained CLIP language and image encoder, and align the point cloud representation by the proposed multi-modal representation alignment technique. Then, to enlarge the pretraining dataset, the authors introduce text filtering and enrichment technique to annotate point clouds without descriptions. The proposed OpenShape is pretrained on an large-scale ensembled dataset, and the experimental results demonstrate that OpenShape has promising zero-shot classification ability and shape retrieval ability. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed method fully leverages the large-scale point cloud datasets to align the point cloud backbone with frozen pretrained CLIP encoders, thus obtaining open-world point could recognition and shape retrieval ability. 

2. The proposed text filtering and enrichment technique is effective to preprocess large-scale point cloud datasets. 

3. The overall writing is polished.

4. The ablation study is extensive and critical.

Weaknesses:
My major concern is the fairness in the experiments (i.e., table 2). As the authors stated in section 3.1, the chosen vision-language encoder is OpenCLIP ViT-G-14, which is an extremely large backbone and obtains much better feature representation ability. Therefore, the authors should list the point cloud backbone and pretrained vision-languge encoders concretely in the table, and make fair comparisons if possible. 

Limitations:
The authors do not state the limitation of this method. Instead, the authors present the future direction of the proposed OpenShape, i.e., part-level information and synthetic-to-realistic domain gap. These directions can be seen as the limitation of this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work learns a multi-modal representation among text, images, and point clouds based on a scaling 3D dataset. The results demonstrate remarkable performance in point cloud zero-shot classification, retrieval, and captioning tasks. In order to construct this dataset, the authors combined four commonly used 3D datasets while filtering and enhancing text prompts. Additionally, they introduced an offline hard negative mining strategy to improve the efficiency of the training process.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper collects 0.8M text-image-point cloud triplets from four popular 3D datasets and does text cleaning and enrichment to get high-quality prompts for each object.
2. This work proposes a hard negative mining strategy to improve joint representation training efficiency and performance.
3. The trained representation shows surprising performance on zero/few-shot classification.

Weaknesses:
1. The major problem is an insufficient novelty compared with ULIP. The pertaining method, triplets construction and downstream tasks are very similar to ULIP.  The primary distinguishing point of this work is the text prompt cleaning and enrichment flowchart, as well as the offline hard negative mining strategy employed. However, the latter strategy lacks an ablation study to provide supporting evidence.

2. The motivation behind this work is not clearly articulated. While the aim is to develop a scaling-up 3D representation, the purpose of this representation is unclear. Unlike ULIP, which focuses on enhancing current 3D backbones through developing a new pipeline, this work attempts to scale up text-to-3D pairs through text augmentation that is still limited by the insufficient number of 3D objects. 

3. There is a lack of exploration into the downstream applications of pre-trained representation to counterparts in 2D. This work is more suitable for submission to the dataset and benchmark track.


Limitations:
No needed

Rating:
6

Confidence:
5

REVIEW 
Summary:
OpenShape explores scaling-up strategy for learning joint representations of texts, image, and 3D point clouds. It proposes to construct larger-scale 3D datasets, filter and enrich paired texts for pre-training, which achieves SOTA results on zero-shot 3D classification and retrieval benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The zero-shot performance of OpenShape is impressive, surpassing existing SOTA methods with large margins. This demonstrates the significance of the method.

2. The retrieval visualizations are interesting and clearly illustrate the embedding ability of OpenShape.

3. The authors re-train exiting methods under new settings, which is good for a fair comparison.

Weaknesses:
1. OpenShape utilizes GPT-4 for textual-level processing, which however is expensive to access. How about the performance using a more affordable GPT-3 (like PointCLIP V2) ?

2. I'm curious about how OpenShape can be incorporated with non-parametric 3D network Point-NN (or its parametric derivative Point-PN) for zero-shot learning?

Starting from Non-Parametric Networks for 3D Point Cloud Analysis, CVPR 2023

Limitations:
Yes

Rating:
6

Confidence:
5

";1
vcNjibzV3P;"REVIEW 
Summary:
The authors provided theoretical analyses and proof that the 3-WL algorithm and the Euclidean version of the 2-WL algorithm can distinguish any complete Euclidean graph pairs. The authors then demonstrated that the algorithm can be approximated with GNNs and ran the proposed model on synthetic data to show that it was indeed able to ""separate"" the hard graph pairs.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The authors provided rigid mathematical formulation and proof. The problem was well-defined and formulated in the paper.
2. The organization of the paper is clear. The authors first discussed the problem, followed by theoretical analyses and proof. Model architectures and experiments are then provided to support the theoretical claims.
3. The theoretical results are general and can be potentially applied to a wide range of models.

Weaknesses:
1. The experiments are inadequate. The authors only tested their model and the baselines on small-scale synthetic datasets. The dataset used in the paper only contains small molecular graph pairs whereas, in practical 3D point cloud scenarios, there can be easily thousands of points. Furthermore, at such a scale, many traditional GNN-based networks including MACE and TFN are also ""separating"" according to the results. The authors may test their model and the baselines on larger practical datasets.

2. The potential benefit of a model being ""separating"" was also not experimented. Eventually, we want models to output some meaningful values in the classification or regression task. The author may experiment with such tasks to demonstrate the capability of graph isomorphism tests indeed help with representation learning.

3. The potential application of the algorithm is greatly limited by the assumption that the graph is complete. The authors also mentioned in the future work section that the proposed model scales as $O(n^4)$ with respect to the number of nodes, which is prohibitively large even for small point clouds.

4. The writing of the manuscript needs to be improved in some places. All references were not in parenthesis, making it hard to read the manuscript (e.g. line 143 when quoting the WL test). The quotation marks are not paired. The sections are also a bit strange. I would suggest making the related work and future work a separate section and the current Sec 3 into a subsection (as it is also a theoretical analysis).

Limitations:
The authors have mentioned the limitation of this work in the manuscript and no further negative societal impact is expected.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper analyzes neural networks for point clouds toward modeling of geometric phenomena. It considers the application of message passing networks/GNNs to Euclidean graphs, whereby a variation of the well-studied k-WL test is adapted to point clouds by using a complete graph on the point cloud and making use of Euclidean pairwise distances. To this effect, the authors propose the k-EWL test and show that: (1) For k=1, two iterations of message passing are sufficient to separate most point clouds in any dimension, (2) A single iteration is sufficient for all point 3D point clouds when k=3. Furthermore, additional differential architectures are proposed and demonstrated to have similar separation power as k-EWL tests.

I think the paper has some promise and is generally well-written. But it can be strengthened by better motivating the problem and providing more detailed experimentation, ideally on chemistry/molecular datasets (given that this was cited as a motivation/application in the intro). I think the paper needs some more work, but addressing some of these points would make me open to raising my score.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
(1.) The paper is generally easy to follow and well-written. I understood the definitions and theorem statements without any problem.
(2.) GNNs + point clouds seems like an underexplored area, and the authors make progress in this area (motivation nonwithstanding).

Weaknesses:
(1.) I think there is some motivation lacking for why one wishes to separate point clouds via GNNs, or why it is desirable to construct variants of k-WL. The paper hints at the importance of this for chemical applications, but there isn't much discussion about this beyond the intro.
(2.) The formulation of EWL doesn't seem novel; it is simply a standard MPNN on a complete graph with use of distance as an edge feature in the update rule (in the framework of Gilmer et. al 2017). SEWL seems more interesting, but the motivation is a bit lacking.
(3.) The experiments could be stronger. While the authors provide experiments on synthetic datasets of point clouds demonstrating effectiveness of the proposed architectures, the paper could benefit from some experiments on real-world chemistry tasks, as chemical applications, biological molecular datasets, etc. were cited as a motivation in the intro.

Limitations:
Some limitations are highlighted in the Future Work section at the end of the paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the theoretical completeness of neural networks for Euclidean/3D point clouds, from the perspective of whether they can distinguish all non-isomorphic point clouds. 

Key theoretical contributions include showing that variations of the k-WL graph isomorphism test are complete for 3D point clouds, and that distance-based 1-WL tests are complete for *almost all* point clouds (measure theoretic perspective). 

The work also demonstrates that a GNN can be designed with the proposed completeness guarantees, and sanity checks the theoretical results on synthetic counterexamples from previous studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This work shows that adaptations of the k-WL hierarchy of graph isomorphism tests can be 'complete' on 3D point clouds. I believe this is a **novel** theoretical contributions for neural networks on point clouds in Euclidean space.

- I believe the findings are **significant**, as neural networks on Euclidean graphs and point clouds are an emerging area of interest from both theoretical and applied perspectives.

- The paper is **well written** and **clear** in terms of presentation:
    - The Introduction does a good job highlighting the research gap.
    - The coverage of related work in Section 1.1 is useful.
    - Section 2 makes a good bridge from WL to the Euclidean setting.

- I went through the proofs, which are correct to the best of my understanding.

Weaknesses:
- It seems challenging to translate this paper's ideas into practice as the model's parameters depend on the number of points $n$ taken as input. This probably makes it very difficult to build a trainable model **while retaining** theoretical guarantees.
    - The authors are upfront about this when discussing limitations.

- Beyond sanity-checking the theoretical ideas on the counterexample from Pozdnyakov-Ceriotti, 2022, the synthetic experiment does not seem to provide any further insights into practical instantiations of the ideas in this paper, or about this class of models more broadly.



Limitations:
The authors have adequately addressed the limitations but not discussed any potential negative social impact.

Beyond what the authors mention regarding practical instantiation of their models, one major theoretical limitation is that the framework is restricted to complete geometric graphs, and the construction of complete/universal models for the general sparse graph setting remains an open question. This my be worth reiterating. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper seeks to theoretically demonstrate the complete determination of point clouds, up to permutation and rigid motion. The authors formulate a Euclidean variant of the 2-WL test, effectively illustrating the separation capacity of the Euclidean Graph Neural Network on highly symmetrical point clouds.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The paper delivers a theoretical exploration of point cloud completeness.
2. It discusses the separation capability of the Euclidean Graph Neural Network in high-dimensional representations.

Weaknesses:
1. In appendix Line 564, what does $(\star)$ stand for? 
2. Does the proposed method risk confounding reflection equivariance?

Limitations:
NA

Rating:
5

Confidence:
2

";0
M03sZkmJXN;"REVIEW 
Summary:
The paper addresses the challenge of integrating protein sequence and structure information to improve protein representations. The authors propose CoupleNet, a network that utilizes graph convolutions to model the relationships between protein sequences and structures. The approach involves constructing two types of graphs to model sequential features and structural geometries and performing convolutions on nodes and edges simultaneously. The proposed approach outperforms state-of-the-art methods on various protein-related tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) Comprehensive Protein Features: CoupleNet incorporates multiple levels of features in proteins, including residue identities, positions, and geometric representations. By considering both primary (sequence) and tertiary (structure) information, the model captures different aspects of proteins, leading to more informative and rich representations. The authors have demonstrated good work in engineering the features of their model. By carefully considering both primary (sequence) and tertiary (structure) information, the model has succeeded in capturing diverse aspects of proteins, leading to more informative and robust representations. 

2) Experimental Results: The paper presents experimental results on a range of protein-related tasks, including protein fold classification, function prediction, and domain prediction. The results demonstrate that CoupleNet outperforms state-of-the-art methods by large margins. The comprehensive evaluation and superior performance validate the effectiveness of the proposed model.

Weaknesses:
1) Limited Novelty: While the proposed approach in CoupleNet is interesting and effective, it does not offer a significant departure from existing methods. Similar approaches, such as GearNet, have previously explored this concept by integrating the radius and sequence information as different edge types within a single graph. Therefore, the idea of constructing two separate graphs in CoupleNet, while slightly different in implementation, does not present a significant departure from existing methods.

2) A weakness of the paper is the lack of a detailed explanation regarding why the proposed model, CoupleNet, performs better than the state-of-the-art methods. While the experimental results demonstrate superior performance, the authors do not provide a thorough analysis or insights into the specific aspects of the model architecture or design choices that contribute to its improved performance. Without a clear explanation of the underlying factors that make the model more effective, it becomes challenging for readers to fully understand and interpret the advantages of CoupleNet over existing approaches.

3) Lack of Code Release: One notable weakness of the paper is the absence of code release. The lack of code availability hinders the reproducibility and transparency of the research.


[1] Zhang, Zuobai, et al. ""Protein representation learning by geometric structure pretraining.

Limitations:
yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a representation learning framework using GNN for protein datasets. A key contribution is combining sequence and structure information using the proposed GNN. The learned representations achieve better performance on downstream tasks of protein fold classification and function prediction.


The sequence+structure methods have been previously investigated in several works, as included in the baseline. I don’t see the motivation behind using a “sequence-structure graph” that differentiates the novelty of CoupleNet. I suggest updating the abstract and introduction by mentioning the limitations of existing “ sequence+structure” approaches and then specifying how CoupleNet addresses that. Could discuss the drawbacks of existing “feature fusion” methods.

I think the primary area of this paper should be applications. The construction of a joint sequence-structure graph is specific to modelling proteins, and the rest of the GNN operations are standard. The structure graph is based on Ingraham et al., and the sequence graph is based on trRosetta et al.. The message-passing scheme is based on ComENet. I don’t see any methodological novelty.
Moreover, empirical results are reported without proper discussion on application to protein problems. The empirical results of baselines are reported based on the article [15]. Without error bars or cross-validation (commonly done for fold prediction and function prediction), it is hard to make any conclusions. At this point, I lean towards rejection.

########
Post Rebuttal
########

I have read the author's rebuttal. Overall, the authors did a great job in responding to the comments, and I have accordingly adjusted my score, recommending accepting the paper.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Learning protein representations that combine sequence and structure information is relevant to the community. Using GNNs is a good demonstration of DNNs' applications to biological problems like protein function prediction.

A new joint sequence-structure 3D graph that can model complex interdependencies for learning representations that achieve good improvement over existing approaches.

Weaknesses:
The introduction doesn't provide sufficient motivation behind the proposed method. It is unclear what the paper means by ""...methods cannot deeply integrate the information…."" What is ""deep integration""? The proposed method itself uses a type of message-passing mechanism, so what is the issue with message-passing cited in the article [8]? 

The paper should first identify the issues in existing ""feature combining"" methods. Then motivate how the proposed CoupleNet addresses that. For instance, could take an example of a specific protein where existing methods perform poorly and then discuss the complex interdependence between structure and sequence for that particular example that needs appropriate modelling for improving the performance of downstream tasks such as fold prediction. 

I would suggest adding a paragraph that discusses the importance of sequence-level information and another on structure-level information for tasks such as fold classification or function prediction. Discuss the importance of local/non-local residue contact information. Then establish how encoding for such a multitude of information can be helpful for downstream applications. I would suggest taking a look at non-homologous proteins. Could discuss the availability of large-scale high-resolution structure data and how deep GNNs can leverage that.


Line 94-99: No clear explanation of the benefits of complete message passing. Why is it essential to consider ""global"" completeness? What is the downside of ""local"" completeness? Why not cite SphereNet [a] here? Which notion of ""completeness"" does this paper build on?
It is implicit from the equation, but that simply is restating from the ComENet paper. The paper should be well explained, avoiding room for any such ambiguity.

This paper needs to explain better the need for global ""completeness"", which can be done by discussing the importance of conformers like structure. Discuss the equivalence of 3D graphs under SE(3) transformations for protein structures and why that's important to be considered for representation learning.

In my understanding, the classes in the fold classification task tend to be unbalanced. I suggest authors report per class accuracy.

[a] Liu, Yi, et al. ""Spherical message passing for 3d graph networks."" arXiv preprint arXiv:2102.05013 (2021).

There is no comparison of time-space complexity.

Limitations:
Although a sentence is added on limitation, ”. A limitation is that the detailed inter-relationships between sequence and structures remain to be explored and uncovered”. This sentence is just vague and provides no meaningful information. The paper consistently discusses “deeply co-model sequence and structures together”. So what could not be “deeply modelled” and needs more exploration? 

The sequence and structure of information have been combined in several existing works. The main issue is that the paper doesn’t establish clear motivation for combining the two using a combined graph. Moreover, the need to consider “completeness.”

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors proposed CoupleNet to co-model the protein sequences and structures. CoupleNet separately builds a sequence-based graph and a radius graph for message passing. It achieved state-of-the-art performance on several datasets compared with recent baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed CoupleNet utilized much biological knowledge to incorporate useful geometric information like backbone torsion angles and inter-residual torsion angles. This information may help the model better capture the structural information.

2. The experiments demonstrated the SOTA performance over a wide range of baselines. Ablation studies demonstrated the effectiveness of ""coupling"" the sequence and structural information.

3. The protein figures clearly and concisely defined the geometric features used in the paper.

Weaknesses:
1. The construction of two separate graphs (one for the sequence and one for the structure) is not very innovative. In fact, GearNet (already cited by the authors) already used this formulation and the description of the two graph construction in this paper is very similar to GearNet.

2. Though the backbone structure can be completely determined by the descriptor, the residue information is lost in the proposed model. Failure to capture this information may affect the performance on downstream tasks.

3. There is some confusion regarding the graph construction that can be clarified. See the following questions.

Limitations:
Limitations and potential negative societal impacts were properly addressed in the manuscript.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a network called CoupleNet that combines protein sequence and structure information to generate informative protein representations. The network utilizes multiple levels of features, including residue identities and positions for sequences, as well as geometric representations for tertiary structures. It constructs two types of graphs to model sequential features and structural geometries, and performs convolution on nodes and edges simultaneously to obtain superior embeddings. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods on various protein-related tasks. The paper highlights the significance of complete structural representations in learning protein embeddings and suggests further exploration of the inter-relationships between sequence and structures.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The paper is clearly written and easy to follow. Related work is thoroughly discussed, situating the context and contributions of the proposed method relative to prior studies.

* The experimental results show substantial gains over previous state-of-the-art baselines on benchmark datasets for protein structure prediction. The proposed method achieves these results with a relatively small training dataset, whereas some baselines utilize much larger resources such as the AlphaFold database. This indicates the model may be more data-efficient and able to elicit more from limited information.

Weaknesses:
* The relationships between protein sequence and structure, especially as they relate to function, are not deeply explored or discussed. For tasks like protein function prediction that aim to determine the utility or effects of a protein, understanding the connection between its sequence, structure, and biological role is critical.

* The proposed method achieves promising results for these functional tasks through an end-to-end modeling approach, but additional analysis interpreting what the model has learned about sequence-structure-function relationships would strengthen scientific validity. Has the model captured complex, nuanced relationships, or is performance driven more by statistical associations in the training data? Discussion of these relationships and how the model may be representing them would address concerns about the depth of knowledge actually obtained.

* For full reproducibility and scientific validity, additional details on the experimental setup, hyperparameter selections, and sensitivity analyses are needed. e.g. The variance or confidence intervals of reported results should be provided to determine their reliability and sensitivity to stochastic effects. Point estimates alone do not indicate the variability across trials or uncertainty in conclusions.The effects of different random seeds on performance should be analyzed to confirm results do not depend highly on a single seed selection. Sensitivity to initialization is an important consideration, especially for complex neural networks. Hyperparameter choices require further explanation and analysis of the effects of varying key values such as layer sizes, attention heads, learning rates, loss trade-offs, etc. The initial values selected may bias conclusions if performance is highly sensitive to these hyperparameters. Exploring this sensitivity would reinforce the results do not depend entirely on the specific choices made.

Limitations:
N/A

Rating:
6

Confidence:
3

";0
9MwidIH4ea;"REVIEW 
Summary:
The authors propose a novel dual-guided spatial-channel-temporal attention mechanism to audio-visual problems, which leverages pre-trained audio and visual encoders. And they show the improvement in various audio-visual tasks such as event localization, parsing, segmentation, and question answering.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This work applied proposed methods and evaluated with diverse audio-visual downstream tasks, including localization, parsing, segmentation and question answering. This helps to showcase the generalization of proposed approach.

Weaknesses:
- The use of notations in Section 3.3 is very complicated and difficult to follow. Consider utilize Figure 2 (4) for illustration and walk through each step along with the modules in the figure to make it easier for the readers to follow.
- For the results presented in Table 2 and 3, and discussed in Section 4.3, only the better performance is highlighted without providing potential explanation and/or hypothesis why proposed system performed worse in some scenarios. For example, the performance on event-level as in Table 2, and the performance with MS3 of segmentation in Table 3. Consider adding some discussion for these cases.
- Some abbreviations are referred to without any information provided, such as LLP in line 210, and CMBS in line 218. Consider adding one-liner explanation to help the reader.
- Minor comments: Section 3.2 in equation (1), the first superscript ""l"" does not follow the same style as others. 

Limitations:
- No potential social or ethical implications.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a parameter-efficient approach, DG-SCT. DG-SCT can adapt pre-trained audio and visual models on downstream audio-visual tasks without updating pre-trained encoders (i.e., keep pre-trained encoders frozen.)

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
$+$ DG-SCT can achieve state-of-the-art results on several downstream audio-visual tasks.
$+$ DG-SCT is able to learn channel-wise, spatial, and temporal information to incorporate with pre-trained audio and visual encoders.

Weaknesses:
$-$ Although DG-SCT leverages different types of cross-modal attention (i.e., channel-wise, spatial, and temporal), the design is not clear.
For example, DG-SCT leverages RNN for modeling cross-modal temporal information and learnable weights for cross-modal spatial information. Such a technique can be implemented with divided (space-time) attention after channel-wise attention.

$-$ The effectiveness of the proposed temporal modeling is not clear. The baselines for the implementation of DG-SCT in AVE/AVVP/AVQA have already cross-modal temporal modeling modules.  

$-$ The number of trainable parameters is not reported. For example, the baselines in AVVP and AVE usually use pre-extracted audio and visual features, which contribute to a lower number of trainable parameters.  

$-$ Lack of efficiency comparison (e.g., FLOPs). Combining several attention mechanisms will lead to huge computational costs. It would be great to include these metrics.


$-$ some experimental settings and results are not clear (See. Questions)

Limitations:
Yes, DG-SCT uses more number of trainable parameters

Rating:
4

Confidence:
5

REVIEW 
Summary:
This work proposes a new mechanism to utilize audio-visual features as novel prompts to extract task-specific features from large-scale models. This work introduces an attention mechanism named Dual-Guided Spatial-Channel-Temporal (DG-SCT), which utilizes audio and visual modalities to guide the feature extraction of their respective counterpart modalities across spatial, channel, and temporal dimensions. The proposed method is evaluated on a series of tasks including Audio-visual event localization, Audio-visual video parsing, Audio-visual segmentation, and Audio-visual question answering. Moreover, it proposes a new benchmark to perform Audio-visual few-shot/zero-shot tasks on AVE and LLP datasets.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This is a very interesting work. The use of prompting is primarily focused on language and later vision, notably this work performs audio-visual prompting, which seems quite innovative.
-  Moreover, unlike previous works that offer unidirectional prompts, the proposed approach introduces bidirectional prompts, where both visual and audio modalities can mutually guide each other in the feature extraction process.
- The proposed approach is evaluated on several benchmarks and compared fairly with prior works showing the effectiveness of the proposed method.
- It's a nicely written paper and easy to follow.

Weaknesses:
Please see #Questions for more open-ended discussions. 



Limitations:
Please see #Questions.
My questions are mostly open-ended. I will look forward to the discussion with the authors and the arguments of the other reviewers. 


Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper mainly proposes a new attention mechanism named Dual-Guided Spatial-Channel-Temporal (DG-SCT), which utilizes audio and visual modalities to guide the feature extraction of their respective counterpart modalities across spatial, channel, and temporal dimensions. Experiments on 4 tasks shows the advantage of the proposed method. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall it is a nice paper. 

- The presentation of this paper is very clear. 

- Experiments are extensive and appear solid.

- From a high level, I think a better audio-visual attention mechanism of this type would benefit a series of downstream tasks. Prompting is a trend to make audio-visual systems smarter. 



Weaknesses:
With the above said, I am not sure if the claim on Page 1 lines 25-27 is valid for all audio-visual tasks. ""However, when perceiving the roaring sound of an engine, the visual region depicting a ""car"" should receive more attention than the region of ""trees"". Simultaneously, when observing the car, it is crucial to concentrate on the audio segments of the engine sound."" it is true for tasks about audio-visual correspondence like retrieval/localization/segmentation, etc. But in other tasks like audio-visual joint classification, we do want to leverage the information that uniquely appears in a single modality to make predictions. This is because if we only use mutual information, then a single modality is enough, what we are looking for is information not appear in one modality but can be found in the other one. The proposed method seems to attend to the mutual information. I am wondering if it would negatively impact the performance of joint classification tasks.

- minor: Page 2, line 54, it should be HTS-AT, not HT-SAT. 

Limitations:
There is one sentence limitation ""We consume a few more parameters than LAVisH."" I don't think this weakens the proposed method.

Rating:
6

Confidence:
4

";1
b2WpR0Fymj;"REVIEW 
Summary:
This manuscript proves universal approximation results for MAM neurons. MAM neurons are essentially ReLU neurons that operate on the sum of the maximum and the minimum of the weighted inputs, plus a bias. Previous work claims these neurons are useful for reducing the memory footprint of deep neural networks. Thus, in short, the paper proves how any real-valued continuous function defined on a compact set can be approximated to any arbitrary degree of precision by a network consisting primarily of MAM neurons (for instance in the sup norm).

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The paper is relatively well-written and easy to understand. Universal approximation properties are important, and proving them is a key step when new activation functions are introduced. 

Weaknesses:
There are two problems with this contribution. First, there are many universal approximation results in the literature already, and thus this contribution is perceived as incremental. Second, and more importantly, an even better result can easily be proved by adapting well-known techniques for providing simple proofs of universal approximation properties (the authors do not seem to be aware of the existence of such techniques).One only needs to slightly tweak the proof that is used for the case of ReLU neurons.   In particular, it is easy to show that any continuous function from [0,1] to R, can be approximated by a neural network with a single linear output unit and two hidden layers of MAM neurons to any degree of precision \epsilon. To see this, note that f is *uniformly* continuous over [0,1]. Thus the [0,1] interval can be subdivided into small intervals of size \alpha, such that within any such interval f is contained ""within a sleeve of thickness \epsilon"". Now connect the input to all the MAM neurons in the first hidden layer with identical weights equal to 0.5. As a result, the max + min portion of the input of each MAM neuron in the first hidden layer is equal to the input x. Now select a sequence of (arithmetically) increasing biases so that:  
1) the first hidden neuron is turned on if x in the first interval of size \alpha and all the other hidden neurons produce a zero;
2) the first and second neurons are turned on if x is in the second interval of size \alpha and all other hidden neurons produce a zero;
etc. In other words, essentially code the value of x by the number of neurons that are turned on in the hidden layer. It is then easy to see how to design the following hidden layer and connect it to the single linear output neuron to obtain the desired approximation. 

Limitations:
The main limitation of the universal approximation results is that the hidden layers can be arbitrary large (depending on the size of \alpha in the proof sketched above). And thus in general the constructive proofs of these results are not practical. This is a well known limitation and the authors, to their credit, do mention it.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper demonstrates that the network can still maintain the universal approximation property after substituting the classical MAC hidden neurons of neural networks with the MAM neurons, which only rely on the maximum and minimum elements of the summation, allowing for more aggressive pruning. Specifically, the authors consider a network with two hidden MAM layers and two kinds of output layers. They show that the networks can achieve universal approximation capabilities under different norms for target functions with varying smoothness. The constructive proof of the first case utilizes a similar idea to the partition of unity, and the second one decomposes the whole domain and deals with the local behavior of the subnetworks involving the second hidden layer.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The authors provide a universal approximation result for a recently proposed hidden neuron for neural networks with two different output designs. The constructive proof introduces localized hyper-rectangles, which may inspire other step-function-like construction in some approximation problems. It’s also helpful to provide some intuitive illustrations of the construction.

Weaknesses:
While the result is somewhat interesting, it fails to provide more compelling evidence for the newly proposed MAM neurons. In the introduction, MAM neurons’ main advantage is that they can allow more aggressive pruning. But the theorems and the proof process seem to have no investigation about these properties. And there is no further discussion about the academic and practical potentials of MAM neurons, weakening its significance and attractiveness as well as this work.

In Section 5, the authors claim that the theorem has no constraints on the layer width, which contradicts the conventional universal approximation properties. However, the proof introduces a parameter n that needs to be chosen sufficiently large (See line 158). Interestingly, this parameter seems to be related to the width or scale of the neural network according to its definition in line 141. Hence, there appears to be a potential contradiction in this context.

There are several evident traces of incompleteness throughout the manuscript, such as lines 141-145, equation after line 170 and line 216, and the unfinished Conclusions section in lines 229-230, indicating that this paper was hastily written and has not undergone thorough revisions.

The inadequate mathematical formatting in the manuscript has resulted in difficulties in comprehending the proof. Some mathematical notations used in this paper contradict commonly used notations in classical theories. The LaTeX formatting in the manuscript is not standardized.

Limitations:
The authors do discuss the limitations in Section 5. However, due to the issue with the parameter n (See Weaknesses), I think the discussion is insufficient. Besides, they mention that the theoretical results wouldn’t directly lead to an efficient approximation, so I wonder if there are any numerical experiments conducted based on the setting in this paper. Besides, how large is the gap between the theoretical setting and practical applications since this paper has many assumptions and hypotheses?

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents two universal approximation theorems for deep neural networks associated with a so-called Multiply-And-Max/min (MAM) activation function defined with the maximum and minimum of the input components and a bias constant. One is for uniform approximation and the other for approximation in the Sobolev space W^{1, p} with p\ge 1. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The presented universal approximation theorems are interesting and should have implications to show the power of pooling layers in deep neural networks. Studying simultaneous approximation in terms of the norm in the Sobolev space W^{1, p} should be able to explain the efficiency of some deep learning algorithms. These ideas are novel in my opinion. 

Weaknesses:
 Though the approximation theorems are novel to me, the paper has a few weak points and should be improved:

1. To demonstrate some theoretical advantages of the MAM activation function. This might be done with the pooling layers in deep neural networks. 

2. To present rates of approximation. Quantitative estimates for approximation of functions in various function spaces are crucial in the approximation error estimate for generalization analysis of deep learning algorithms. 

3. To give rigorous statements and proofs. In Lemma 1, the sentence ""Let z be any ... layer."" should be removed because the output z is constructed by (5) and is not an arbitrary output function. In its proof, ""We assume ..."" and ""the output ... only one of the inputs"" should be revised: the neurons are constructed by (7), not by assumption. In Lemma 4, P is not a constant. It is a quantity depending on \ell of the form constant + o(1/\ell).  

4. To give fair credits to the existing literature. For example, the construction of trapezoid functions has a long history in the study of deep neural networks and can be found in the papers of Shaham-Cloninger-Coifman (2018), Chui-Lin-Zhang-Zhou (2020), and some others. 



Limitations:
Better theoretical results would improve the quality of the paper. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper studies the universal approximation properties of ReLU networks using the Multiply-And-Max/min (MAM) neurons. Literature on the universal approximation properties of ReLU networks using the Multiply-and-ACcumulate (MAC) neurons is vast. However, the study on MAM neurons seems lacking. Hence, two theorems taking a step in characterizing the universal approximation properties for MAM neurons are proved in this paper. The first theorem states that a two-hidden-layer ReLU network using MAM neurons in the first two layers and the normalized linear combination in the last layer can approximate any continuous function on a unit hypercube arbitrarily well in terms of the infinity norm. The second theorem is similar to the first one, stating that a two-hidden-layer ReLU network using MAM neurons in the first two layers and the linear combination in the last layer can approximate any twice continuously differentiable function on a unit hypercube arbitrarily well in terms of the Sobolev norm. The proofs of these two theorems are constructive and the authors also acknowledge that their results do not imply efficient approximation.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The novelty is clear. The novel contribution of this paper is apparently the theoretical guarantees on the universal approximation properties of ReLU networks using MAM neurons. Although I have not carefully validated the proof, the explanations and statements given in the paper seem to be sufficiently convincing. Overall, this is a well-written paper. The presentation is concise, clear, and easy to follow. I enjoy reading the paper.

Weaknesses:
1. The requirement of the target function being twice continuously differentiable seems a bit limited. It would be great if the authors could relax this assumption or clarify why this assumption is necessary.
2. The result in Theorem 2 relies on the L^p Sobolev norm. Would it be possible to extend the result to the infinity norm? The paper would be more convincing and clearer if the authors can justify why the normalized linear combination and the linear combination use different norms. The connection between Theorem 1 and 2 seems missing.
3. Given the observation that using MAM or the mixed MAM/MAC neurons gives better pruning performance than the MAC neurons in practice, the paper would be more convincing if the authors can provide some insights into the constructive approximation of these different schemes.

Limitations:
The authors clearly state the limitations of their work in Section 5. Specifically, they point out that their results do not imply efficient implementation. I’m glad to see they make it very clear. They also state that the efficiency of approximation will be their future focus. I think this paper has laid a good foundation for their future work.

Rating:
6

Confidence:
4

";0
Ce0dDt9tUT;"REVIEW 
Summary:
This paper investigates the influence of adding three types of noise to the images or the latent representation of the images on the performance of CNN and ViT. The authors theoretically and empirically show that the family of random noise can be divided to positive noise and harmful noise regarding their impacts on the deep learning models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well-motivated and inspiring because it shows that the noise, which is normally perceived as harmful to the DNNs, can actually be good to the performance of DNNs in many ways. 
2. The authors have done extensive theoretical analysis to support their claims with great novelty. 
3. The presentation of this paper is clear and well-written.

Weaknesses:
1. Although the authors propose a novel idea of positive noise and harmful noise, the current content of this paper is not enough to support their claims. 
2. My major concern is that there are some fundamental errors in the theoretical analysis (see the questions for details), which makes me question whether the theorems really hold. 
3. The experiments are also not enough to support the theorems.

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper aims to improve the accuracy of image classification models by injecting noise into the latent representations during training. Please be aware that I do not understand key aspects of the proposed method, so my summary and interpretation below may be wrong.

The paper argues that injecting noise into the latent representations of neural networks can reduce or improve performance, depending on the kind of noise. The paper analyzes different kinds of noise using an abstract information-theoretical framework of task complexity that I do not understand. Three kinds of noise are analyzed:

Additive noise (e.g. Gaussian): This is determined to be always detrimental according to the theoretical framework.
Multiplicative noise (e.g. salt-and-pepper noise): This is also determined to be always detrimental.
Linear transform noise: As far as I understand, this noise consist of randomly permuting rows of the feature matrix my multiplying with an elementary matrix Q. Depending on the choice of Q, this type of noise can improve model performance.

The paper goes on to show that across many model architectures and classification tasks, injecting linear transform noise leads to huge gains in performance. Here, I must be missing important aspects of the method, since the gains are so large that they are basically impossible. Perhaps these are theoretical upper performance bounds when the noise is optimized for a given test set? I am not sure.


Soundness:
1

Presentation:
2

Contribution:
3

Strengths:
The performance improvements presented in the paper would be spectacular if true, but I have serious doubts about their validity.

Weaknesses:
1. The theoretical parts of the paper are hard to follow. For example, equation 4 introduces the concept of “task entropy”, but no mathematical definition for a “task” is given, and no explanation what “task entropy” is in the context of image classification. Another example is the matrix Q. This is described as an “elementary matrix”, which, according to Wikipedia, is a matrix that differs from the identity matrix by a single elementary row operation. The paper must be using the term in a different sense, since the elements of Q can have values between 0 and 1 in the paper (e.g. Equation 19). This needs to be explained. In general, more intuition and relation to image classification should be given in the theoretical part.

2. The experimental aspects of the paper are also insufficiently described. When and where exactly is the noise injected during training? How much? Does the noise depend in some way on the data (this is suggested by statements in L246, which say that the effectiveness of the noise depends on the dataset size)? Is the validation/test set used to choose the noise?

3. The accuracy improvements shown in the paper appear unrealistic and need to be discussed more. Table 1 reports 91.37% top-1 accuracy on ImageNet, Table 4 even reports 95.12%, both for ViT-B models. The value of 91.37% is already far above the best existing models (https://paperswithcode.com/sota/image-classification-on-imagenet), which are much larger than ViT-B. The value of 95.12% is very likely impossible without accessing the test data, given that ImageNet has significant label noise, which means that even humans or a perfect classification model could not reach 100% on ImageNet (see https://arxiv.org/abs/2006.07159). Therefore, I suspect that the presented results either represent some sort of upper performance bound when optimizing the injected noise directly for the test data (in which case it would be wrong to compare these numbers to models that have not seen the test data), or there are some other issues with the evaluation.

Limitations:
The paper does not discuss limitations.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper challenges the conventional view that noise is usually detrimental in deep learning and proposes the concept of positive noise.  It explores how particular types of noise can, under specific circumstances, enhance performance, from both theoretical view and experimental view. It illustrates that injecting positive noise can facilitate learning and achieve promising improvements (sometimes even SOTA) on both image classification and domain adaptation tasks for both CNNs and ViTs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. It proposes the concept of ""positive noise"", which is contradictory to the conventional view that noise is usually harmful to performance.
2. It provides solid theoretical analysis and comprehensive experiments to support its claims
3. Injecting positive noise to hidden layers as proposed by this work is applicable to any deep learning architectures. It holds significant potential for exploring the application of positive noise in diverse deep learning tasks beyond the scope of image classification and domain adaptation tasks investigated in this study.

Weaknesses:
1. The paper focuses on exploring three common noises: Gaussian noise, linear transform noise, and salt-and-pepper noise. Whether it can be generalized to other noises remains to be explored.

Limitations:
The types of noise and the experimented tasks are limited to demonstrate its generalizability. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper challenges the view that noise is always harmful in computer vision tasks and deep learning architectures. The authors demonstrate that specific types of noise, termed positive noise (PN), can actually boost the performance of deep architectures under certain conditions. The paper provides theoretical proof and empirical evidence of the benefits of positive noise, showcasing significant performance gains in large image datasets like ImageNet. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper provides both theoretical analysis and empirical evidence to support the presence and effectiveness of positive noise.
- The empirical experiments conducted on large image datasets, including ImageNet, demonstrate significant performance gains by proactively injecting positive noise. 
- The signal processing perspective on noise for neural networks is interesting. 

Weaknesses:
- The main weakness is lack of novelty. Adding noise to neural networks and machine learning models in general has been proposed under various occasions as an effective way to improve the performance [1-5]. As such, I think the contribution of this paper is limited. 

- The authors of the paper should conduct a more careful set of experiments to compare against some of the previously proposed noise-based regularization techniques against their proposed method.

- A more careful literature search should be done to address the difference between this work and prior works on noise-based regularizations. 

[1] Noh, Hyeonwoo, et al. ""Regularizing deep neural networks by noise: Its interpretation and optimization."" Advances in neural information processing systems 30 (2017).

[2] You, Zhonghui, et al. ""Adversarial noise layer: Regularize neural network by adding noise."" 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019.

[3] Bishop, Chris M. ""Training with noise is equivalent to Tikhonov regularization."" Neural computation 7.1 (1995): 108-116.

[4] Li, Yinan, and Fang Liu. ""Whiteout: Gaussian adaptive noise regularization in deep neural networks."" arXiv preprint arXiv:1612.01490 (2016).

[5] Li, Bai, et al. ""Certified adversarial robustness with additive noise."" Advances in neural information processing systems 32 (2019).

Limitations:
The authors might want to address limitations of the proposed method further. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies noise injection into popular image classification models (ResNets and ViTs) from a theoretical as well as experimental standpoints. The analyses are carried out for various noise types (Gaussian, salt-and-pepper or linear transform) as well as different points of application (at the input images or at the intermediate activations). Experimental results demonstrate a strong improvement over baseline and competing methods with the linear transform noise.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
* Strong experimental results
* Analysis of a broad range of design choices (noise types, noise injection points, vision architectures)
* In-depth theoretical analysis of the effect of noise injection that aligns well with experimental results.

Weaknesses:
**Major**

* I have to say that I found the paper to be difficult to digest. The main findings are somewhat masked by the breadth of analysis. For example, a considerable amount of time is spent on image-level noise injection or on salt-and-pepper noise, which end up not yielding significant improvements. I found the simultaneous consideration of both - image-level as latent-level noise - throughout the paper to be confusing. Similarly, the space allocated to salt-and-pepper and Gaussian noise could be repurposed to provide additional details on the linear transform noise.
* At a high-level noise injection described in the paper is a form of augmentation and/or regularisation. Yet the connection to these techniques isn't explored in the paper. In this context, the statement on lines 1-3 about noise being viewed as a harmful perturbation isn't fully fair, as we do view augmentation as helpful and often necessary.
* Details of how linear noise application are missing. For example, is it applied across the entire activation tensor? Or is it perhaps applied per spatial position / token?
* The authors mention that models that are overfitting are not expected to benefit from noise injection. Could they please elaborate on this? When viewing noise injection as augmentation, it seems that noise injection would help battling overfitting.
* I did not follow the derivations in the paper closely, but it appears that the authors assume that label and input images follow the standard normal distribution (lines 150-151). How realistic is that? If the theoretical analysis conclusions are based on this assumption, can we expect them to be valid for real-world scenarios?
* There seems to be some confusion about the structure of the linear transform matrix Q. In line 162 it is set to be an elementary matrix (i.e. different from an identity matrix by a single elementary operation), where as in equation 14 it is allowed to be from a broader class of matrices.
* Beyond the connection of the proposed methods to data augmentation techniques, it would be very interesting to understand whether the proposed noise injection scheme and SOTA augmentation methods (e.g. RandAugment) are complementary.
* Noise injection in the last layer only should be justified further, also experimentally.



**Minor**
* Unclear text: line 83 ""[...] contribute to less mean [...]"", Tables 1 and 2 ""[...] in the bracket [...]"", line 206 ""[...] where $p$ is a probability generated by a random seed [...]"", line 236 ""[...] because of trendy replacing Equation [...]""
* Unclear what's meant by ""severe problems"" in lines 124-125, or by ""brute force attacks"" in line 128
* Figure 1 talks about injecting noise at a random layer in the network, yet experimentally only input image and last layer were considered.
* line 147 ""[...] Gaussian noise is independent and stochastic"" - Gaussian noise does not have to be independent (i.e. can have a non-diagonal covariance matrix)
* In line 152, is the covariance taken between variables with a different number of dimensions?
* In line 156 ""[...] we define the function [...]"" - unclear for what purpose.
* Column names in Table 5 are ineligible

Limitations:
Not addressed by the authors.

Rating:
4

Confidence:
2

";0
lRxpVfDMzz;"REVIEW 
Summary:
This paper presents several data augmentation methods for training prompts that include both frozen text and learnable soft tokens so that they are still effective on out-of-domain examples.

Soundness:
3

Presentation:
1

Contribution:
3

Strengths:
The keyword extraction method to create text-prompts that are informative to the current example as a method for reducing how much information the model needs to fit in the soft prompt to perform well on the current task is a good ideas that makes a lot of sense.

Care was taken in creation of the dataset including things like removal of overlapping prompts/keywords from training and testing.

Weaknesses:
The proposed X-Prompt approach of combining soft and text prompts is not novel, Gu et al., 2021 https://arxiv.org/abs/2109.04332 and Wei et al., 2022 https://arxiv.org/abs/2109.01652 both touch on how the combination of text and soft prompts can result in differences in performance. Thus this paper would be much stronger if it was framed as the first deep dive into the interaction between text and soft prompts with the novelty coming from the data-augmentation that enables more robust OOD performance. These paper should be mentioned in the related work. The template augmentation approach is similar to the multiple prompts using in papers like Flan (see above) and T0 (https://arxiv.org/abs/2110.08207).

The prose makes assertions about the performance of soft prompts in the OOD setting is poor without citations. Several papers (https://arxiv.org/abs/2111.06719, https://arxiv.org/abs/2110.07904, https://arxiv.org/abs/2208.05577) have confirmed that it is hard to use soft-prompts in out of domain settings and should be cited.

The prose redefines in-distribution (ID) multiple times (not a weakness, just feedback that doesn't fit anywhere else)

Differences in performance seem rather small making it difficult to trust the results without some way to capture variance. For example, In the OOD accuracy in Table 6, the difference between Prompt tuning and X-Prompt is 0.6. With the test split being 5% of 52,541 this means X-Prompt only gets 16 extra example correct. Given the variance in prompt tuning from Lester et al., (2021), it seems like this could be within noise.

The performance of X-Prompt in Table 7 isn't very convincing, it is the strongest in ""overall"" but that seems to be an artifact of the other methods only being good in one category while X-Prompt is ok in both. It doesn't seems like it actually the best option.

Rather than framing issues using in-distribution vs out-of-distribution (which seems overly broad) it seems like this paper would be stronger if it was framed to be about task-signaling. It seems that prompts do poorly on a new task because the prompt was the only way to signal to the model to do a new task. Therefore when applied in a new setting it still signals for the model to do that original task. This can be seen as a type of overfitting, but the task-signaling framing makes it clearer why X-Prompt is a good idea.

Limitations:
The authors do a good job highlighting the computational requirements for their methods.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposed X-Prompt which instructs an LLM with not only NL but also an extensible vocabulary of imaginary words. Besides, context-augmented learning (CAL) is introduced to learn imaginary words for general usability, enabling them to work properly in OOD (unseen) prompts.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. A concise idea to combine the merits of NL and soft prompts.

2. The paper demonstrated both descriptive capabilities and OOD robustness of X-Prompts.

3. X-Prompt achieves a good balance of BLEU and accuracy in zero-shot style transfer.

4. The paper is well-written and easy to understand.


Weaknesses:
1. Prefix-tuning[1] method needs to be compared in Experiments.

2. As shown in Table 6, X-Prompt has no significant advantage over Prompt-tuning.

3. The generation results of zero-shot style transfer lack human evaluations.

4. Writing content issues.
  (1) Please try to use published sources rather than Arxiv sources in citations.

[1] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021.


Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a solution called eXtensible Prompt (X-Prompt), which enables instructing a Language Model (LM) using imaginary words. These words serve the purpose of providing instructions to the LM that are difficult to articulate using natural language. In order to prevent overfitting of the LM and facilitate its generalization to out-of-distribution examples, the authors propose two strategies: Template Augmentation and Context-Augmented Learning. Through a series of experiments, the authors evaluate the ability of X-Prompt to generate suitable content in the style of a specific individual, as well as its capacity for zero-shot style transfer generation. The results, both quantitative and qualitative, demonstrate the efficacy of X-Prompt and the utility of context-augmented learning.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1.  The paper introduces X-Prompt as an extension of the soft prompt method, offering notable advantages over previous approaches. X-Prompt exhibits enhanced flexibility by enabling its application to out-of-distribution examples, thereby providing significant adaptability. Its novelty lies in its minimal learning cost, in contrast to the substantial efforts required for LLM pretraining.
2.  The training methodology, referred to as context-augmented learning, harnesses the capabilities of LLMs to generate novel contexts. This cost-effective approach facilitates the generation of additional training examples and can be potentially applied to various prompt learning experiments, showcasing its versatility.
3.  The paper is commendably well-written, featuring informative diagrams that enhance comprehension of the proposed method. Moreover, the inclusion of numerous example prompts and the corresponding generated content greatly enhances the readability of the paper.
4.  The experiments conducted are comprehensive, incorporating quantitative metrics such as perplexity and next-token accuracy, as well as qualitative assessments involving human annotators. This multifaceted evaluation methodology ensures a thorough evaluation of the proposed approach.

Weaknesses:
1.  Lack of novelty: The paper is an extension to the soft prompt solution. Except for that this paper uses LLMs while most of the previous focuses on models like BERT, the paper improves the training strategy by introducing the method of context-augmented learning. The contribution alone might not be significant enough for a paper at NeurIPS.
2.  Limited Experiment Scope: Although labeled as an ""extensible prompt,"" the paper primarily focuses on style transfer generation tasks. It would be valuable for the authors to broaden the experimental scope to encompass additional tasks, akin to the approach taken in the prefix-tuning paper (Li and Liang, 2021). This expansion would further showcase the versatility and potential of the proposed method.
3.  Model Performance Concerns: While the experiments understandably revolve around adhering to specific styles, there may be implications for content faithfulness with the use of X-Prompt. Table 7 indicates that the content score of X-Prompt falls noticeably below that of natural language, albeit demonstrating significant improvement over the soft prompt method. The evaluation of content faithfulness in other experiments, such as open-ended generation, remains absent. Lower perplexity may stem from the chosen style rather than from generating appropriate content. More evidence is needed to evaluate the effectiveness of the imaginary tokens.

Limitations:
The paper should address a potential limitation concerning the experiments on open-ended generation. The successful generation of content imitating the style of a specific individual raises concerns regarding the potential for generating deceptive or fake statements. If the generated content closely resembles the authentic statements of the person, it could be challenging to distinguish between the two. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes X-prompt: a technique that learns an imaginary token to represent a concept that is hard to describe in natural language. Compared to soft prompt tuning, X-prompt is designed to be OOD robust with template and content augmentation, in which the X-token is trained with various prompt templates and examples of different topic keyword to prevent overfitting. The author quantitatively and qualitatively demonstrated the advantage of X-prompt over prompt tuning on styled text generation and style transfer.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Compared to soft prompt tuning, X-prompt is OOD robust as shown intuitively on Table 2. The secret source is context augmented learning (CAL) which involves augmented templates and content (topic keywords). CAL is logically reasonable and feasible, which is very effective generalising X-prompt to OOD as shown in Table 6
2. Table 11 shows that imaginary tokens of X-prompt trained for a specific task (styled generation for training), can be reused to support other tasks via different natural language instruction (style transfer for inference). This shows imaginary tokens learned from X-prompt can interact with natural language tokens. As such, X-prompt can be potentially utilised for compositional usage.

Weaknesses:
1. Missing baseline: for table 6, the baseline of using NL instruction is missing. As X-prompt's core objective is to learn the concept which is hard to describe in NL, it is desirable to compare with baselines which use NL descriptions. For example, a baseline which puts some example tweets of a specific user as in-context prompt, can be compared. In the same spirit, for qualitative evaluation (Table 7&8), there should be one more NL baseline which prompts the LLM as ""Criticise the C++ language in Donald Trump's style"".
2.  The qualitative evaluation (Table 7 & 8) raises concerns as the styles chosen are from well-known characters (Trump, Satya, Sheldon). This is problematic as the LLM already learned about their styles during pretraining, and such knowledge might be well associated with their names already. For example, the following is the result I get from ChatGPT with query ""Praise the C++ language in Donald Trump's style"": ""The C++ language, folks, let me tell you, it's tremendous. Absolutely tremendous. It's a beautiful, beautiful language."". 
3. Following point 2, I think the evaluation well-verified the advantage of X-prompt over soft prompt tuning. However, there lacks robust and detailed comparison with baselines which use descriptions in natural language for styled generation experiment. 

I'm willing to reconsider score if the above three concerns are addressed.

Limitations:
The author has adequately discussed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes eXtensible Prompt (X-Prompt), a new way to prompt large language models beyond natural language. With an extensible vocabulary of imaginary words, X-Prompt allows for more descriptive prompts and is designed to be out-of-distribution robust. The paper also proposes context-augmented learning (CAL) to learn imaginary words for general usability. Experiments with OPT-6B reveal some effectiveness of the method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and easy to understand.
2. The idea of the X-Prompt is interesting. The use of imaginary words in prompts is an innovative idea that allows for more descriptive prompts and is designed to be out-of-distribution robust.

Weaknesses:
1. The idea is interesting but the method is not novel. The training way of X-Prompt is similar to continuous prompt learning. And why only use 1 token to learn the imaginary word?
2. The paper lacks a sufficient number of baselines for comparison and utilizes a limited set of datasets. The “Prompt tuning”(maybe) and X-prompt method are fine-tuned, which is unfair to compare with “No prompt” or “32-shot”.
3. In Table 11, there is a difference in the input format of “Prompt tuning” in the train stage and inference stage. I don’t think [SOFT] can be used directly in the NL.

Limitations:
Yes, the authors discuss the computing resource limitations of the paper.

Rating:
6

Confidence:
3

";1
DvRTU1whxF;"REVIEW 
Summary:
The authors make two claimed contributions:
1. They propose an architecture for an encoding model. This architecture consists of three key components.
* A frozen DINO trained ViT-B backbone + learnable convolutional layers on top of ViT feature maps
* A differentiable spatial sampling layer (implemented via pytorch grid_sample, similar to [1]), where the 2D coordinates are predicted from 3D voxel coordinates
* A differentiable softmax based layer selector
2. They propose an ""All-for-One"" training recipe. Which incorporates the following:
* ""dark knowledge distillation"" (typically referred to as knowledge distillation or network distillation in most other machine learning works), where they use ROI specific networks to train larger networks
* They propose a new parcellation across brains which they call ""veROIs"", which is extracted via k-means clustering of voxel weights.

To validate this method, the authors visualize the learned spatial sampling grids and how the preferred layer varies across voxels. The authors further perform an ablation study of the encoder, and perform image retrieval using their network.

The authors provide an illustration of how the sampling grid evolves during training in the supplementary.

[1] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is novel in the combination of techniques, which uses online-learned sampling grids with soft backbone layer assignment to predict the brain activations, this design is reasonably biologically motivated.

The authors proposal of clustering based ROI assignment is also novel in the context of decoding (image retrieval).

The paper provides definitions and dimensions for most variables where appropriate, and the figures are illustrative.

My view is that the proposed decoder architecture is moderately novel in the context of encoding models. There is insufficient information (lack of detail) to judge the all-for-one training. 

Weaknesses:
The paper is interesting, however my key concern lies in the lack of details for the all-for-one training scheme, and the lack of evaluation in comparing against simple linear-regression + backbones (ResNet, CLIP ViT [1])
* On the clarity of the paper
  * The high level clarity of the paper regarding the all-for-one training is poor and could use significant improvement. 
  * It is not clear how exactly the veROIs are used. Is the encoding model ultimately at the voxel-level? If so, are the veROIs used for the network distillation and image retrieval steps? Do you share the backbone and just use independent linear weights for the voxels?
  * It is not clear which subjects you use for Stage 1/2/3. Do you train with one subject's ROI? Or do you train on all subjects' single ROI (using stage 1 as an example). If so, do you take veROI-1 for example, then train a model on all subjects from all modalities for veROI-1?
  * It is not clear how you get the all-ROI model to derive the veROIs. Is this model trained on all subjects and all modalities? Do you train just the last linear weights? Or you use the DINO features? Do you train the conv weights?
  * It is not clear how you train across subjects and datasets where the number of voxels are not the same. This is a central claim in your paper.

* On motivation
   * Currently the justification for repeated distillation is weak. Your experiments show that distillation does help from a performance standpoint, however your motivation differs from the typical use of distillation (which is to accelerate inference using a student model). Could you better motivate this? 

* On prior work
  * For the topic of brain encoding models that utilize differentiable spatial sampling, I recommend citing [2,3] which are similarly brain motivated, as well as [4] which is one of the more significant papers that uses differentiable sampling.

* On the soundness of the baseline
  * For the ""FrozenRM"" encoder, you mention that every voxel is mapped to the center. Since you use DINO based on ViT, there is an extra classifier token (usually the first token). Typically when using ViT based architectures for spatially invariant tasks, this is the embedding you use. I recommend modifying the FrozenRM baseline, or adding an additional baseline when you use this token
  * The same criticism applies to the GlobalPool token, I recommend adding/replacing a baseline where the global pooled representation is replaced with the classifier token

* On the lack of baselines
  * Currently the authors perform ablation studies, but do not perform qualitative or quantitive comparisons against other works [1]. The paper would be strengthened by adding comparisons to linear-regression based single subject voxel-wise encoding models based on different architectures (global pooled ResNet features , VGG, Gabor features, GIST features) trained on ImageNet, or different objectives (CLIP/OpenCLIP/EVA-CLIP, DINO, any of the SSL/Masking work) adapted to a single subject. I don't expect their multi-subject network to necessarily perform better (nor would it be a negative if they perform worse), but some evaluations are still necessary. 

* Minor
  * Figure 4 was quite confusing to me. In the retinagrid case, the colors indicate spatial extent. However in the retinamap, the colors indicate layer selection. I would ask that you provide additional clarity here.
  * The notation in Table 3 is quite confusing, you do not specify what veROIsX is. It is implied that it corresponds to what stage of network distillation you use. Please clarify this. 
  * Line 199 ""regulirazation"", minor misspelling. 

On balance, the paper is interesting from an architectural standpoint. I would be happy to take another look at the paper if the authors can clarify their training scheme and add additional baselines. 

[1]  Conwell, Colin, et al. ""What can 5.17 billion regression fits tell us about artificial models of the human visual system?."" SVRHM 2021 Workshop@ NeurIPS. 2021.

[2] Mahner, Florian, et al. ""Learning Cortical Magnification with Brain-Optimized Convolutional Neural Networks."" Conference on Cognitive Computational Neuroscience. 2022.

[3] Jun, Na Young, Greg Field, and John Pearson. ""Efficient coding, channel capacity, and the emergence of retinal mosaics."" Advances in neural information processing systems 35 (2022): 32311-32324.

[4] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Limitations:
The authors adequately address the limitations of their model.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper develops an all-for-one training model to address the challenge of one big-model problem by converting it into multiple small models, in which the small models aggregate the knowledge while preserving the distinction between the different functional regions. With the proposed method, biological knowledge of the brain, particularly retinotopy, is used to introduce inductive bias into a 3D brain-to-image mapping that ensures a) each neuron knows which regions and semantic levels to gather information, and b) no neurons are left out. Overall, it is an interesting paper, however, there are several concerns about the machine learning novelty, validating the empirical studies, and the clear presentation of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Please refer to the question section


Weaknesses:
Please refer to the question section


Limitations:
Please refer to the question section


Rating:
4

Confidence:
4

REVIEW 
Summary:
The manuscript addresses the challenge of generating brain encoding models (specifically for visual stimuli) - which seeks to predict brain responses at the voxel level to visual stimuli. A challenge facing brain encoding is heterogeneity in data modality, individual variability, and functional differences across brain region. Existing models often address the problem of heterogeneity by fitting separate models for different brain regions. However, the authors seek to to fit a single model that encompasses the entire visual brain by leveraging a dark knowledge distillation method in which each ROI distills the dark knowledge present in the other ROIs.  The authors evaluate their method on a variety of functional imaging datasets spanning fMRI, MEG, and EEG.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Originality: The authors developed a novel training pipeline utilizing dark knowledge distillation in order to allow ROIs to collaborate during training. The authors also develop a new method for incorporating retinotopy, a biologically realistic feature, into their encoding model.

Quality: The model seems to work across different data modalities (e.g. EEG, MEG, and fMRI), making it broadly applicable.

Clarity: The manuscript provides several ablation studies that investigate the limitations of the approach and which features of the architecture are important for improvement.

Significance: The study introduces a new approach for creating visual brain encoding models from functional imaging data. Their approach could be employed by other groups studying non-visual stimuli as well.

Weaknesses:
Unless I am mistaken, the authors do not evaluate their model against the state of the art in  computational speed, making it hard to evaluate whether their model represents a significant improvement, as the improvement in correlation appears to be modest. Furthermore, the message of the paper is a little unclear - what is the main breakthrough the authors are trying to present: the all-for-one training, retinotopy, or ability to work with multiple data modalities. Evaluation against state of the art should occur for each of these topics. Finally, the model is trained on data across fMRI, MEG, and EEG modalities, but the held out data consists only of fMRI data, making evaluation of model generalizability difficult.

Limitations:
Limitations are addressed except for points raised earlier about comparison to state of the art.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The authors tackle the brain encoding task, which predicts brain voxel-level responses to image stimuli. The authors aim to train a comprehensive brain encoding model using the vast amount of public data from diverse imaging modalities and numerous participants. The proposed method, the All-for-One training recipe, divides the one-big-model to multiple small models and aggregates the knowledge together in inference time. An intriguing technique the authors propose is to use retinotopy to introduce inductive bias to learn the mapping.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The figure quality (especially aesthetics) is not often seen in NeurIPS submissions. Take a look at Figure 2, 4, 5 and 6. Those figures are on par with Nature family submissions. 
2. Decent ablation studies.

Weaknesses:
1. The design choice is not the most straightforward to process. I have some trouble understanding the logic behind the three-staged design of the proposed All-for-One recipe — specifically, why is it necessary to have these 3 stages?
2. RetinaGrid and RetinaMap seems a bit far-fetched. By far, my impression is that the authors are simply trying to draw an analogy from image formation process in the retina and provide a fancy visualization. The authors are welcome to defend with explanations.
3. Lack of other alternative baselines.

Limitations:
Nothing that I am aware of.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on the task of brain encoding model, which aims to predict brain voxel-wise responses to stimulus images. First, the paper proposes the All-for-One (AFO) training recipe, which enhances interactions among multiple ROI models to handle the large diversity within the data. Second, the paper introduces the RetinaMapper to learn a 3D brain-to-image mapping and the LayerSelector to selectively merge features from multiple layers. Finally, the paper trains the model on a very large scale of data and demonstrates the effectiveness of the model through extensive qualitative and quantitative analysis.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The overall motivation is clear, and the techniques used are straightforward and reasonable. For example, enforcing cross-model interaction/distillation is a reasonable way to enhance each model, and selectively merging information from multiple layers with different receptive field sizes is sensible.

2. The experiments are extensive, and adequate qualitative and quantitative analysis is provided. For example, Table 2 demonstrates the effectiveness of TopyNeck, and both Figure 4 and Figure 5 show the effectiveness of the LayerSelector.

3. The work pre-trains the model in large-scale data, which is impressive.

Weaknesses:
1. need to provide more details and insights into specific techniques
2. missing more explanations/references
3. some writing issues

Limitations:
The authors have addressed the limitations

Rating:
6

Confidence:
2

";0
dnEFueMZ43;"REVIEW 
Summary:
This work presents Q-switch Mixture of Policies (QMP) that identifies shareable behaviors and incorporates shareable behaviors. The authors propose to utilize each task’s learned Q-function to evaluate shareable behaviors, and incorporate helpful behaviors from other tasks to aid the exploration of the current task. Experiments on manipulation and navigation tasks are done to validate the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1, The paper is written well and the MTRL framework has the potential to generalize to kinds of RL tasks because of its simplicity. The idea of Q-switch is straightforward, but seems to work in the experiments.

2, The analysis is comprehensive, and validates the effectiveness of the behaviors identifying and incorporating. Moreover, behavior sharing seems to be a good complement to parameter sharing. By combining them, the sample-efficiency would get improved further.


Weaknesses:
Though compared with many benchmarks, the experiment environments are sort of over-simplified. I would suggest testing the framework in the meta-world environment with more tasks, like insert peg, pick&place, to make the results more convincing and reliable.

Limitations:
As discussed above in the weaknesses, it's still a good paper to accept.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies sharing behaviors between tasks in multi-task reinforcement learning. In the proposed method, each task maintains an independent policy network. During online exploration, it selects the action maximizing the task's Q function among actions proposed by all the tasks' policies. Experimental results show the method improves multi-task performance in three continuous control environments. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well written. The motivation of behavior sharing for MTRL is clear.
2. In experiments, figures and results are clearly presented.


Weaknesses:
1. The paper makes a strong assumption that tasks are only different in reward functions. Many complementary methods, like parameter sharing, tackle a wide problem setting where the transition functions and state spaces can be diverse.
2. In baseline methods: the Fully-Shared-Behaviors baseline, a policy without any task information as input for multi-task RL, is weird. A fully-shared baseline with task identifier input makes more sense.
Two ablation methods seems unnecessary, since the proposed method is simple enough. 
3. In experimental results, the proposed method does not outperform baselines very significantly.

Limitations:
Authors discussed some of the limitations and they should be addressed in future work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduced a new exploration mechanism for MTRL. They suggested training a different policy for each task and “sharing the behaviors” between them. In order to do so, each policy, in a certain state, chooses the most suitable action using its own Q function. The author evaluates several MTRL benchmarks and shows increased sample efficiency and final performances over behavior-sharing baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
$\underline{\text{Clarity:}}$

1. Overall, the paper is coherent and easy to follow
2. The introduction is well-written and the motivation for the work is clear

$\underline{\text{Significance:}}$

1. The method seems quite general and might be useful in many cases

2. To the best of my knowledge the idea of using the Q function as a metric for policy selection is novel

Weaknesses:
My main issue is with the technical soundness of the paper. Throughout the paper, the level of the formulation was relatively low, and I spotted some inaccuracies in notation and claims. In general, I understood the motivation for the Q-switch, but there lacks some theoretical analysis or empirical study to support it. I think this method might be suited for some set of tasks, but probably have a limitation, due to the generalization capabilities of the Q function, that was not discussed in the paper. Here are some more specific examples regarding the technical quality of the paper:

$\underline{\text{In the problem formulation section:}}$

1. The MDP components are not defined. Are the state and action spaces continuous or discrete? Should state that $\gamma \in \mathbb{R}$

2. In line 104 you state that $T$ is a number of tasks in the task set and in line 109 you denote the i’th task in the set as $T_i$. This is a confusing abuse of notation.

3. In line 107 - “We parameterize the multi-task solution as…” what does the solution for a multi-task mean? 

4. In line 109+110 - the objective is not formulated. “the tasks are uniformly sampled during training” - what does that mean? From which distribution the tasks are being sampled? Does the sample accure at the beginning of the training phase once?

5. In line 112 - what is $\pi_i^*$? It is not defined. 

$\underline{\text{In section 4.3:}}$

1. line 173 - “over all the task policies $\pi_j$” -> “over all the task policies $\left[\pi_j\right]_{j=1}^T$” 

2. In line 173 - how does $\pi_i^*$ defined? I believe it is an abuse of notation from section 3.

$\underline{\text{Related work and baselines:}}$

1. Limited coverage of skill learning and intrinsic reward literature. I don’t think the statement in line 101 (“.. assume that the optimal behaviors of different tasks do not conflict with each other”) is true for many skill-learning methods

2. Although this work approach is quite different than intrinsic reward/skill learning, I believe that a standard state visitation bonus should be competitive (or at least a good baseline) for the evaluation benchmarks

$\underline{\text{Experiments:}}$
1. In section 6.2, the chosen baselines (QMP-uniform and QMP-domain knowledge) are too simplistic, please provide a more solid baseline, e.g. the one you suggested in line 178 (a probabilistic mixture). 

2. In Figure 6(c) you show that the best performances were achieved when incorporating both parameter sharing and behavior sharing, and showed that using only parameter sharing beats using only behavior sharing. This raises the question of what would happen if we used different kinds of exploration mechanisms together with your method (e.g. intrinsic exploration). Overall, I feel that this evaluation is limited, both in the variation in testing environments and exploration combinations.

Limitations:
Although the authors raised a valid point in the Limitation section of the paper, I believe other limitations of the method exist and aren’t addressed (please see the weaknesses section for further details).

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes Q-switch Mixture (QMP) for identifying shareable behaviors over tasks and incorporating them to guide exploration. QMP identifies shareable behaviors from other tasks and incorporates them to make exploration efficient. The proposed framework is tested on three different multi-agent tasks and compared with other methods. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper introduces the problem of selective behavior sharing for improving exploration in multi-task reinforcement learning requiring different optimal behaviors. The proposed method consists of a Q-switch for identifying shareable behaviors and is used to guide an exploration scheme incorporating a mixture of policies. The Q-function of each task is used to assess the quality of other task policies’ behaviors when applied to the task. The Q-switch acts as a filter to evaluate the potential relevance of explorative behaviors from other tasks.

Weaknesses:
1. The proposed method aims to simultaneously learn multiple tasks. Do they share the same observation space and action space? If it is true, the contribution of the work is limited. If not, the author should consider how to measure the similarity of two tasks. If the two tasks are quite different, it is hard to transfer samples from one task to the other.
2. For incorporating shareable behaviors, the number of training samples from other tasks may be much less than the number of training samples generated for the current task. It would be hard to learn from training samples from other tasks.
3. The scenarios used in experiments are simple tasks. It would be better to see the performance of the proposed method in complex  problems.


Limitations:
The author has discussed the limitations of the proposed method.

Rating:
4

Confidence:
2

";0
au9VfbABDO;"REVIEW 
Summary:
The paper proposes a new algorithm for behavior cloning (BC) where the BC learning objective is modified with a diffusion modeling loss that models the joint state-action distribution of the expert data. The paper demonstrates the benefits of modeling both the conditional probability and joint probability of the expert distribution. The results of the proposed method have been compared with baselines modeling both conditional probability and joint probability distributions and have been supported with appropriate ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and easy to understand. 
- The problem has been well motivated. The authors claim that while solely modeling the conditional probability p(a|s) of the expert data struggle to generalize to unseen states during training, solely modeling the joint probability can improve generalization to unseen states at the cost of high inference time. Further, though generative models provide encouraging results in modeling stochastic and multimodal behaviors, they struggle with manifold overfitting (shown in Fig. 4(c)). Accordingly, the paper proposes a loss function that combines modeling the conditional and joint probabilities of the expert data while using a state-of-the-art generative model (diffusion model) to benefit from their superior behavior modeling capabilities (as shown in [1] which has also been cited in the paper).
- An interesting feature of the proposed method is that since it only uses a diffusion model to guide the learning of conditional probability, it is able to circumvent the issue of high inference time associated with the noising and denoising procedure in diffusion models (mentioned in Sec. 5.3).
- The primary results have been compared with baselines modeling both conditional probability and joint probability distributions. Table 1 shows that DBC outperforms BC (conditional probability), Implicit BC (joint probability), and Diffusion policy (a state-of-the-art generative model) on all tasks except the Walker. However, since DBC performs comparably to the best performing baseline (BC) (the authors provide some justification for this in Sec. 5.3 Locomotion).
- The paper provides interesting insights on modeling high dimensional action spaces, inference efficiency, generalization to unseen goals, and manifold overfitting. Additional ablation studies to justify design choices such as the choice of the generative model, loss coefficient value, and effect of the normalization term have also been provided.

[1] Chi, Cheng, et al. ""Diffusion policy: Visuomotor policy learning via action diffusion."" arXiv preprint arXiv:2303.04137 (2023).

Weaknesses:
- The limitations of the proposed approach can only be found in the limitations. Though I believe that is fine given space limitations, it would be nice to mention it briefly in the main paper for completeness.
- Since the paper focuses on imitation learning from expert data, it would be interesting to also add *Action Chunking with Transformers* (ACT) [1] as a baseline. ACT also uses a generative model (a conditional variational autoencoder) as a policy and given the reported results, it would be interesting to see how DBC compares with ACT. I understand that the paper does compare with a VAE. However, ACT with its transformer encoder and decoder would be an interesting baseline to compare with.
- I am curious about how the performance of the proposed method scales with the dataset size. Specifically, since diffusion models seem to be “data hungry” (based on results from the computer vision and NLP community), it would be interesting how this method scales with dataset size. Since this method solely uses a diffusion model to guide the learning of policy, I believe a study of differences in the performance of DBC and the baselines with different amounts of training data might give interesting insights.

[1] Zhao, Tony Z., et al. ""Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware."" arXiv preprint arXiv:2304.13705 (2023).

Limitations:
Yes. However, the limitations and societal impact of the work can only be found in the appendix. It would be nice to either have a brief mention or a reference to the appendix section in the main paper for completeness.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The submission proposes an imitation learning method optimizing a loss that is a weighted sum of a behavioral cloning loss and a loss based on a diffusion model. The diffusion model loss penalizes the policy for generating actions that are unlikely under the diffusion model, which is pre-trained to maximize the likelihood of joint state-action pairs sampled from the data.


Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
Originality:

I am not aware of any previous work combining a behavioral cloning loss with a diffusion model loss.

Quality:

I think there is one interesting ""key trick"" that seems to be required for the method to work, which is using the diffusion-model-loss incurred by the expert demonstration as a baseline. I can see how if the diffusion model is wrong somewhere, it could inappropriately penalize the policy for doing something reasonable. Only penalizing the loss in excess of the expert's loss mitigates the effect of errors in the diffusion model.

The paper's strongest point is probably the experimental results in section 5.3. A reasonably diverse set of tasks were evaluated, and the proposed method performs significantly better than the baselines in most cases. I also appreciated that the experiments were targeted towards testing specific hypotheses—e.g., the results in section 5.5, which validate the hypotheses that the BC loss is necessary, and that the diffusion model is the best choice of EBM for this purpose.

Significance:

The relative simplicity of the approach, along with positive experimental results in non-trivial tasks, could encourage more research into combining BC with EBM-type losses.



Weaknesses:
Originality:

Although the specific combination of BC and DDPMs is novel, there is some precedent for combining BC losses with other losses—for example [A], which combines a BC loss with the GAIL loss.

[A] Jena et al. Augmenting GAIL with BC for sample efficient imitation learning. https://arxiv.org/pdf/2001.07798.pdf

Clarity:

Some key aspects of the submission's presentation could be improved. In particular, the mathematical notation is lacking in key places. Section 4.2.2 is missing notation to describe where we are taking various expectations, and with respect to which distributions. For example, should eq. 5 read $E_{(s,a) \sim D, a \sim \pi(s)} \max(..., 0)$ or $\max (E_{(s,a) \sim D, a \sim \pi(s)} \dots, 0)$? Are any expectations taken with respect to $s$ sampled from the DM? I believe the answer is no, but I had to check the algorithm description in the appendix to be sure. The answers to these questions should be obvious from the math, but this is unfortunately not the case.

Another point of confusion is in section 4.3, which mentions ""jointly optimizing the proposed diffusion model loss."" I generally interpret this to mean simultaneously optimizing the parameters of all the models, but this is not the case. I again had to check the algorithm description in the appendix to verify that the method is strictly a two-phase method, where the DM is pretrained and fixed before optimizing eq. 6.

Quality:

One of the paper's main weaknesses is confusion as to the motivation for the method and why the method seems to work. Some of the explanations offered by the paper include:
1. Diffusion models / EBMs generalize better because they model the joint probability p(s,a) as opposed to the conditional probability p(a|s) (lines 52-54).
2. Combining BC with an EBM helps because EBMs alone suffer from ""manifold overfitting."" (lines 127-128)

Both of these explanations seem a bit tenuous. As for the first statement (EBMs generalize better because they model the joint probability), it is simultaneously vague, debatable, and it is unclear how that property would benefit the method, even if it were true. It is stated (line 113) that ""These methods demonstrate superior generalization performance in diverse domains,"" but this statement seems too vague to be useful. Even assuming that modeling the joint probability produces better generalization in some sense, how does that translate to better results in the proposed method? No plausible mechanism is given for this. 

For example, I could imagine that if one were to sample joint state-action configurations from a good model, and then train behavioral cloning using those state-action pairs, then that could conceivably perform better than training BC on raw data, because sampling from generated states may serve as a form of data augmentation. However, this is not what the method does, according to the algorithm description in the appendix—states are sampled strictly from the data distribution.

As for the statement about manifold overfitting, it is true that this is theoretically an issue with EBMs—however, there is a trivial solution for this that works fairly well, which is to perturb the training examples with a small amount of random noise to break any manifold constraints in the data. So, it is not necessary to add a BC loss to EBMs in order to solve the manifold overfitting problem.

That said, I do believe it is plausible that combining the BC and DM losses helps, but not for any of the reasons above. A more plausible explanation for why the method works is that the loss optimizes (ignoring eq. 4)  an approximation to ""forward"" KL divergence plus ""reverse"" KL divergence—since the DM loss is a lower bound on the log-likelihood of the ""data,"" where the ""data"" in this case consists of samples from the model. The forward KL divergence encourages mode coverage, at the expense of occasionally producing bad samples, whereas the reverse KL has the opposite properties—it produces ""good"" samples, at the expense of losing modes. Therefore, these losses have complementary characteristics that benefit each other when combined. However, one has to be careful not to inadvertently penalize good samples in the reverse KL term, which is probably why adding the human baseline in equation 5, helps.

Notably, imitation learning based on reverse KL divergence and more general f-divergences has been previously explored, e.g., in [B]. This would make one of the methods suggested in [B] a fair comparison in experiments.

I noted a few other issues with the experiments. As stated earlier, I am suspicious of the idea that it is essential that the DM model the joint probability as opposed to the conditional probability—it seems plausible to me that the method would work just as well if the DM were trained as a conditional model (since the method doesn't use states sampled from the DM), so it would be interesting to test this experimentally. 

As also mentioned previously, I believe the manifold problem can be trivially solved by adding a small amount of random noise to the training examples. Doing so may significantly affect the results in Table 2—i.e., adding noise would probably boost the performance of the ""without BC"" column. Similar comments apply to the ""manifold overfitting"" results in section 5.4. I think this is an important experiment to run, because it may weaken one of the core claims, which is that adding BC to EBMs helps because it helps address the manifold overfitting problem.

[B] Ke, Liyiming, et al. ""Imitation learning as f-divergence minimization."" Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14. Springer International Publishing, 2021.


Limitations:
I didn't see much discussion in the way of limitations. What about the efficiency and stability of training?

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a method to augment a behavior cloning (BC) agent with additional diffusion loss. The goal is to leverage the conditional probability learned by the BC loss and the joint probability learned by the diffusion loss. The diffusion loss is calculated using the prediction error of a pre-trained diffusion model on both the agent's and expert's state-action pairs. Experimental results demonstrate that this method outperforms baseline methods, achieving improved performance.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Effective Experimental Results: The paper provides experimental results that showcase the superior performance of DBC compared to several baselines, including BC, Implicit BC, and Diffusion Policy.
2. Leveraging the Advantage of Offline Learning: The proposed method operates purely in an offline setting, eliminating the need for interaction with the environment. This reduction in training complexity is an advantage.

Weaknesses:
1. Sensitivity to the $\lambda$ Parameter: The paper introduces a $\lambda$ parameter for BC loss and diffusion model loss balancing, but the analysis is limited to the maze environment. It is crucial to investigate and analyze the performance of the algorithm across various environments when the $\lambda$ parameter is modified.
2. Lack of Theoretical Analysis: The paper lacks a theoretical analysis of the proposed diffusion model loss. Providing a theoretical foundation for the diffusion loss would enhance the paper's scientific rigor.


Limitations:
1. Limited Task Scope: The experimental evaluation is confined to specific tasks, raising questions about DBC's performance in other types of tasks or more complex environments.
2. Offline Learning Restriction: As an offline imitation learning algorithm, DBC lacks the capability to acquire knowledge through online interaction with the environment.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents a method for guiding behavior cloning via state-action joint distribution learning. They train a diffusion model to maximize the log-likelihood of state and action pairs in conjunction with an imitation learning model that learns to mimic expert actions given state observations. They combine the two objectives to obtain a policy that predicts actions that fit the expert joint probability captured by a diffusion model. The authors demonstrate experiments on four continuous control domains, and show ablations to demonstrate the effects of their design choices.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
1) The paper is well-structured and easy to follow. The experimental setup is clearly described with relevant details. The diffusion model for learning a joint distribution over state-action pairs is novel and is presented concisely.
2) The considered tasks for this method are challenging and results on 4 out of 5 continuous control tasks show improvements over the considered baselines.


Weaknesses:
1) Intro: The authors state that implicit behavioral cloning (IBC) learns a joint distribution of state and action p(s,a). This is incorrect. IBC learns the joint “energy” E(s,a) but the learned distributions are still conditionals p(a|s). This is evident from the fact that IBC is trained to maximize the log-likelihood of actions in the dataset, and minimize those of sampled negative actions, given state inputs, and never trained to generate states or minimize the energy of negatively-sampled states. This makes a major claim of this paper incorrect.
2) Section 3.1: Talking about reinforcement learning as a preliminary seems very absurd and misleading for a subsection on imitation learning, and also when the paper has nothing to do with learning from rewards.
3) The core approach of this paper does not seem technically sound to me. Learning two distributions over the same random variables (state and action) seems to bring inconsistency to the probabilistic model where at least one of the two distributions is bound to be approximate.


Limitations:
The authors have addressed the limitations of their method in the supplementary submission.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents a novel approach in the field of imitation learning. The authors address the challenge of learning from expert demonstrations without access to reward signals from the environment. They propose a framework called Diffusion Model-Augmented Behavioral Cloning (DBC) that combines the benefits of modeling both the conditional and joint probability of the expert distribution. The authors demonstrate the effectiveness of DBC through extensive experiments in various continuous control tasks, including navigation, robot arm manipulation, dexterous manipulation, and locomotion. However, there are certain strengths, weaknesses, limitations, and questions that need to be addressed regarding this work.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1) Extensive experiments: The article presents a wide range of experiments conducted on diverse tasks, including navigation, robot arm manipulation, dexterous manipulation, and locomotion. This comprehensive evaluation demonstrates the effectiveness of DBC in various scenarios.
2) Novel approach: The proposed DBC framework offers a promising approach to imitation learning. By combining behavioral cloning with a diffusion model, the authors achieve more stable training compared to methods that combine behavioral cloning with GANs, such as GAIL.
3) Clear comparison: The authors compare DBC with existing baselines, providing a clear understanding of its advantages over other methods in terms of performance and generalization.

Weaknesses:
1) Coefficient selection: The combination of the behavioral cloning loss and diffusion model loss in DBC relies on a simple addition of coefficients. However, the sensitivity of these coefficients to different environments should be further investigated. A more elegant approach and thorough ablation experiments on diverse tasks are needed to validate the coefficient selection process. Currently, this paper only provide ablation on this coefficient in a single environment (Maze).
2) Lack of comprehensive comparison: Although DBC is compared with baselines, the article does not provide a comprehensive comparison with other state-of-the-art methods in the field of imitation learning. It would be beneficial to evaluate DBC against a wider range of approaches, including GAIL and other recent advancements, to gain a more comprehensive understanding of its relative performance. This would provide a clearer perspective on the strengths and weaknesses of DBC in comparison to alternative methods.

Limitations:
While the article highlights improved generalization performance, there is limited analysis or discussion on the factors contributing to this improvement. A deeper analysis of the generalization capabilities and limitations of DBC would enhance the understanding of its strengths and weaknesses.


Rating:
6

Confidence:
3

";0
R7lDPUgpaA;"REVIEW 
Summary:
The paper presents a modification to existing optimisation-based margin computation technique in neural networks to be used as an empirical complexity measure. The measure is evaluated on the Predicting Generalization in Deep Learning (PGDL) 2020 competition dataset (now all public) and shows better performance that other margin-based as well as the competition-winning method.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
The introduced constraint to the margin definition is straight forward - restrict the walk towards the margin point from x to be within the top-m PCA components subspace of the training data.

The proposed modification seems to be effective according to the PGDL evaluation.


Weaknesses:
The new margin definition is not motivated theoretically. It seems like something sensible to do, but I am not convinced it gives us all that much insight beyond improving PGDL score. I suspect the approach might also fail on some datasets (as evident from the performance of the proposed method on Task 5 of Table 2 (though to authors credit, the do not shy away from talking about it in the Limitations section)). It doesn't seem like a rigorous derivation of a new principle, but a bit of a hack to improve an existing approach. I presume the point of the competition was to spur new ideas and new understanding in this domain. The proposed modification seem like a minor change, and other than improving the PGDL score, I don't find any new insight in terms of learning theory. As such, this seems like an exercise in optimisation for PGDL score, and I am not sure how the proposed PCA trick relates to gaining any new understanding about generalisation principles of neural networks.

Limitations:
Yes, the authors speak about the limitations of their approach.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose a novel notion of constrained input margin to measure the generalization performance of neural networks. Compared to conventional input margin defined on $\mathbb{R}^n$, the proposed constrained input margin is constrained on the data manifold and estimated via g Principal Component Analysis (PCA). Extensive study has shown the decent correaltion between the constrained input margin and generalization gap of neural networks. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well organized and the writing is fairly clear;

2. A novel measurement of constrained margin is proposed by contraining the margin in the data manifold;

3. The empirical results show the superior correlation of constrained margin with generalziation gap of neural networks in the taks of Predicting Generalization in Deep Learning compared to other metric;

4. This work is insightful to understand the margin in the input space by taking the data manifold into consideration.

Weaknesses:
1. There is lack of a theoretical analysis on the contrained margins, as a theoretical analysis will make the metric more convincing to connect to model generalization;

2. As the adversarial training can reduce the conventional input margins and hurt the generalization, experiments on adversarial training are required to show the effectivness of constrained margins under different training scenarios. I will consider to increase my score if the results on adsersarial training are provided.

Overall, the manuscript is well organized and self-consistent, but more theoretical and empirical evidence are needed to verify the effectiveness of the proposed constrained margins.

Limitations:
See weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper deals with the relationship between input margins and generalization. Considering the decision boundary may be not inherently useful in classification, the authors develop a constrained margin based on directions extracted from input data. Principal Component Analysis (PCA) is used to identify the subspaces that are thought to contain truly useful features. In this way, the margin measurement is limited to the input space, making the method more robust without selecting specific layers as in hidden margins. A DeepFool-based algorithm is proposed to calculate the margin efficiently. Experiments show that the proposed method has a good performance on the prediction of generalization, and achieves higher score than other complexity measures for most tasks. The authors carry out further experiments to show that high utility directions are more predictive of a model’s performance that low utility directions, which aligns with the initial intuition.

####################### After Rebuttal ##############################################
I've read the authors' responses. However, I still believe that the novelty of this paper is below the average standard of NeurIPS. I'll keep my score in this period.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow.
2. It is interesting to measure generalization from the view of input margins, which is firstly demonstrated as the authors claim.

Weaknesses:
1. The approximation in Equation (3) is rough. It may be useful on small datasets and easy tasks, but can be massively computational for PCA and not indicative on more complicated datasets and occasions. The authors are expected to give examples or discuss under this setting.
2. Though the authors provide an interesting prospective, the contribution is limited as the approximation and algorithm, such as Taylor approximation and DeepFool algorithm, are actually well-known methods.
3. The claimed connection between input margins and generalization only has empirical evidence. As we all know that traditional definition of margin enjoys very strong theoretical support. Hence, I would like to see more theoretical evidence to support this conjecture.
4. The selection of the number of principal components (by the Kneedle algorithm) requires the ground-truth ranking. Thus, it remains to be discussed whether it is suitable to compare this method with others.

Limitations:
NAN

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper empirically studies input margins as a predictive measure for the generalization of neural networks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper has a very interesting finding: the distance of the decision boundary of a neural network to the training samples is predictive of its generalization performance when such a distance is calculated with respect to a small number of principal components of the training set. This suggests new interesting directions for theoretical and empirical research to explain neural network generalization.
 

Weaknesses:
The measure presented in the paper is not predictive of generalization in a simple case of linearly separable data and a single neuron: w*x.

Consider the dataset in R^{d+1} drawn from ( 1 , 10*N(0,I_d) ) for positive labels and  ( -1 , 10*N(0,I_d) ) for negative labels. 
In this case, you cannot reach the decision boundary for w=(1,0,...,0), even taking all components except the last one (corresponding to the first coordinate in the training set). So the margin should be infinity. 

On the other hand, consider the dataset in R^{d+1} drawn from ( 1 , 0.1*N(0,I_d) ) for positive labels and  ( -1 , 0.1*N(0,I_d) ) for negative labels. In this case, for w=(1,0,...,0), taking the first component (or the first few) would yield a margin of 1. 

Yet in both cases, w=(1,0,...,0) generalizes perfectly for both distributions.

One might argue that we should normalize each coordinate in advance. In that case, we can take the same datasets and randomly rotate them, and the problem will still hold even after normalizing each coordinate.  

Also, see some remarks below in terms of presentation.

Limitations:
The authors adequately addressed the limitations.

Rating:
7

Confidence:
4

";0
7qfkImn0dL;"REVIEW 
Summary:
The authors introduce a novel approach SynTO to address a challenging setting - few-shot black-box optimization. Specifically, SynTO adopts a pretraining-adaptation pipeline. SynTO can be pretrained using synthetic functions and then adapt to downstream tasks via an in-context learning manner. Comprehensive experiments demonstrate the effectiveness of the proposed method in multiple settings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The proposed few-shot BBO is more real-world applicable and generalizable to multiple optimization tasks. Generally speaking, the paper is technically solid.

2.	It is interesting to pretrain models with synthetic data from families of other functions. 

3.	The paper writing is clear, and the presentation is satisfying.


Weaknesses:
1.	SynTO assumes the access to large amounts of unlabeled data, which may cause unfair comparisons as the other methods are trained only with a few labeled data.
2.	In Table 1, albeit with the extra unlabeled data, SynTO seems to perform inferior in some settings.
3.	It would be better to give the efficiency of SynTO compared with previous methods, since it further introduces an additional pretraining step.


Limitations:
Yes. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper tackles black-box-optimization from few-shot examples, by pretraining a transformer model on synthetic proxy tasks using in-context learning, and evaluating with the same procedure but with real data. The synthetic tasks are generated by using 1) real unlabeled data and 2) a synthetic generative process that generates different tasks (i.e. input/targets pairs $x_i$, $y_i$) given the unalbeled data (inputs) for the task of interest. The procedure studied to generate the synthetic tasks is Gaussian processes with a radial basis function kernel. During training, the model receives D input/target value pairs generated by the process, and is tasked to predict the the distribution of the input given a target value ($p(x_y|y_i)$) and the context points using a VAE. At inference time, the model receives the few-shot examples context points, and is tasked to predict the distribution for the current task.
The approach results in competitive performance against existing methods, which generally outperforms existing methods. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The method proposed is simple and sound, and incorporates recent progress from transformer-based modelling, such as in-context learning which allows to efficiently adapt the model during inference time without backpropagation.

- Despite its simplicity on how to train the system and use it during inference, it achieves competitive performance on a wide range of benchmarks.

- The experimental section is thorough, giving lots of insights about the method. For example, it gives insights on how the training with GP performs when the (synthetic) tasks during inference are clearly out-of-distribution (Section 4.1), as well as a thorough analysis of alternative design choices such as random vs sorted selection for context/target points, which are shown to be suboptimal.

- The paper proposes pretraining on fully synthetic data, which is easy to construct and has less ethical concerns than pretraining on real data.

Weaknesses:

The main point of the paper is pretarining using synthetically generated data, and more inisights on why this works at all would make for a stronger paper. My main concerns for the paper are regarding this issue:
-  For example, regarding the following sentence: 
L50-51: ""Our key insight is, if a model learns to perform few-shot learning on a diverse and challenging set of functions, it should be able to adapt quickly to any objective function at test time.""  Generating tasks that are ""diverse and challenging"" from the unlabelled data does not seem enough for the method to perform well, and the concepts of ""diversity and challenging"" are not well defined in the paper nor in references, neither theoretically nor experimentally. For example, one could generate a totally random process without any type of correlations, which would be both ""challenging and diverse"" and the method would likely not perform well during inference. Making this notions clearer and relating it to the choice of Gaussian processes, either theoretically or through experimental comparisons with pretraining with other types of processes would make the paper more complete.

- More insights on why the selected random process (Gaussian processes) is good, and analysis with other alternatives would make for a stronger paper. For example, although the functions used for synthetic evaluation in 4.1 are much more simple, it would be good to use them as pretraining at least to illustrate this point. 
- If the only requirement is the data being ""diverse and challenging"", can real data from similar domains (or other domains that can be adapted to match the desired data statistics) be used for pretraining? Why training with a fully synthetic process is an advantage?
- It would be good to perform an analysis of how the statistics of the pretraining data compares to the statistics during inference (e.g. precision/recall as in [1] or similar metrics), to see if the inference task are statistically different or not when compared to the pretraining tasks. This or similar would illustrate if the the diversity of the pretraining data covers all modes of the inference tasks (high recall) and/or whether the diversity is too high compared to the distribution of data for the inference task (low precision). 


[1] Assessing Generative Models via Precision and Recall. Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, Sylvain Gelly

Limitations:
Yes, the authors adequately addressed the limitations

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper presents a method for tackling few-shot black-box optimization problems in which the model queries a few hundred data points from the black-box function. The proposed method utilizes synthetic pretraining, where a family of synthetic functions is employed to generate data for in-context learning of a transformer-based model. After pretraining, the model adapts to downstream function using few-shot data.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper proposes and addresses the problem of few-shot black-box optimization, a problem with many potential real-world applications.
- The proposed inverse modeling approach is robust to the quality of synthetic data and allows for gradient-free optimization during testing.
- As Gaussian Processes (GPs) are used to generate synthetic functions, there are no extra costs associated with data generation.
- Experimental results show performance comparable to previous state-of-the-art methods, while also demonstrating increased robustness to the quality of few-shot data.

Weaknesses:
- There's no guarantee that real-world downstream functions follow a Gaussian Process with a Radial Basis Function (RBF) kernel. In other words, performance might be sensitive to the similarity between downstream functions and those generated by a GP with an RBF kernel. While the authors mention the universal approximation property, it isn't certain that the pretraining stage covers a sufficient range of functions over a sufficient number of training iterations in practice. For instance, the hyperparameters of the RBF kernel used in the experiments are bounded, and the range was heuristically chosen.

Limitations:
The authors have addressed the limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper investigated the problem of few-shot black-box optimization, and presented Synthetically pre-trained Transformer for Optimization  (SynTO). By combining synthetic pretraining with in-context learning to enable few-shot generalization, SynTO demonstrate its superior performance on Design-Bench. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Synthetically pre-trained Transformer for Optimization  (SynTO). 
+ Combining synthetic pretraining with in-context learning to enable few-shot generalization.
+ Demonstrating superior performance on Design-Bench. 
+ The paper is well written. 

Weaknesses:
- Can the problem in this paper be directly solved by few shot learning methods? If so, Some experiments may be required to compare the proposed method with the existing few shot learning methods. Else some discussion may be required to explain this issue. 
- It seems that SynTO shares similar structure with BONET. Thus, some discussion is required to explain the performance gain of SynTO w.r.t. BONET.
- Ablation studies are required to clarify which components of SynTO explain the performance superiority.   



Limitations:
Yes

Rating:
6

Confidence:
2

";1
XT9mL5vxX2;"REVIEW 
Summary:
The proposed method introduces a novel approach to deepfake detection by integrating natural language and image information. Moreover, it attains state-of-the-art performance on several contemporary deepfake datasets and can generate explanatory sentences that justify the authenticity or falsity of the input image, which is crucial in the field of deepfake detection.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
See Questions Section in detail.

Weaknesses:
See Questions Section in detail.

Limitations:
The limitations are well illustrated in the manuscript.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a multitask learning framework for video deepfake detection. The idea is to rely on a joint embedding architecture and define a set of coarse-to-fine face forgery detection tasks with corresponding textual descriptions for fake face images (binary level, global-attribute level and local-attribute level). This helps to obtain understandable explanations and hence a more interpretable forensic detector. CLIP is used to implement the joint embedding architecture, while ViT-B/32 is adopted as the visual encoder and GPT-2 as the text encoder. Experiments are carried out on several publicly available datasets and show better performance in terms of generalization compared with SOTA methods.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- It is very relevant to design a deepfake detector that is able to generalize to different types of manipulations since often current deepfake methods perform poorly on forgeries not seen during training.
- It is also very valuable to design a detector that can provide explanations about the manipulations. 
- The idea to encode the ground-truth labels via language prompts is interesting and not explored yet in the context of deepfakes.


Weaknesses:
- The technical description of the method based on multitask learning (Section 3.3) is very generic and not related at all with the problem of deepfakes. In addition, the technical contribution seems to come from already published work: the joint embedding formulation is inspired by minimizing an energy-based model as in [39] and the losses for multitask Learning are inspired by [73]. 

- The section on Multitask Language Prompts (3.2) is more related to the specific application, but it is not justified why it is important to consider a coarse-to-fine approach and above all it is not clear how the ground-truth labels via language prompts have been generated. It is said 'Face attribute manipulations associated with other textual prompts are already included in FF++.', but this is new to me. FF++ is only labelled using four different manipulations but does not include the global-attribute level and local-attribute level as described in Section 3.2. This is absolutely not clear. 

- In this same section there is a reference to a face attribute called 'physical consistency', which is explained in the supplemental material. However, Section 1 of the appendix is very confusing and I was not able to understand it clearly. 

- The ablation study is confused. There are several variants that perform well as the proposal in Table 1 and this is puzzling.

- Comparison with SOTA methods should be enlarged including also other methods, such as [66] and [19].

- The experiments that show that the explanations provided by the detector are correct are too limited. They have been shown only on FF++ (Section 4.5 and Appendix). What is more interesting is the ability to generalize to other datasets different from the training one and these are not present in the paper. This is very limiting and does not help to show the relevance of the proposal as stated in the Introduction.

- The paper needs a major re-writing. The presentation is poor and hence not adequate for NeurIPS.

Limitations:
Authors have presented the limitations of their method.

Rating:
4

Confidence:
4

REVIEW 
Summary:

The paper appears to be about a method for detecting manipulated facial images, specifically deepfakes. The authors have used a model that employs a joint embedding architecture, using ViT-B/32 as the visual encoder and GPT-2 as the text encoder. The model is trained using AdamW with a decoupled weight decay of 1 × 10−3 and an initial learning rate set to 1 × 10−7, which changes following a cosine annealing schedule. The authors compare their method with several state-of-the-art (SOTA) methods, including Face X-ray, PCL, MADD, LipForensics, RECCE, SBI, ICT, SLADD, and OST. The results show that their proposed method outperforms all the recent SOTA.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1.	This paper is well written and easy to follow and will be of interest to researchers from the community of multitask learning in deepfake.

2.	Finding the semantic dependencies among tasks using texture prompts is clear and places the previous work very well in context of this framework.

3.	Finally, the authors provide experimental results to demonstrate the effectiveness of the proposed objective function and algorithms.


Weaknesses:
1.	The majority of the contributions in this study are essentially modifications of existing work. Additionally, the significance of the main contribution appears to involve identifying similarities among previous work and proposing a comprehensive generalization that encompasses a significant portion of the existing research. While this contribution may enhance understanding, it seems to be primarily pedagogical in nature rather than being a novel research finding.

2.	The complexity of the proposed method seems high (impractical). How effectively does it handle large datasets? Is it possible to use it in conjunction with sparse variational inference approaches?

3.	It would be great if the authors could extend the proposed algorithm to adapt to other types of loss functions ( from eq.3 – eq.7) such as exp-concave and strongly convex functions.



Limitations:
As described in the manuscript, the proposed method may perform unsatisfactorily when encountering fake face images generated by diffusion-model-based methods. 

Also, see the weaknesses above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work proposes an automated multitask learning framework for face forgery detection from a joint embedding perspective. The central idea is to utilize the multi-modality of visual and textural features to enhance blending-based face forgery detection with the global and local semantic face attributes. Experiments demonstrate the effectiveness of this proposed framework.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The new paradigm of multitask learning strategy from a joint embedding perspective is introduced into the face forgery detection field. The work trains two encoders to jointly embed visual face images and textual descriptions in the shared feature space. Thus, one can guide the forgery detection that is mainly based on visual content with textural descriptions. This work successfully explored the feasibility of using multi-modality data with a multi-task learning framework. Extensive results on the ablation studies verified the effectiveness of the proposed framework.

Weaknesses:
The majority of technical components of this work are borrowed from existing works, e.g., multitask learning, embedding space representation (latent space), textural space and etc. The technical contributions that inspire the following research are quite limited.

Limitations:
The authors clearly stated the limitation of this work. The proposed method cannot be applied to totally generated AI-generated images such as GAN-generated or diffusion-based model generated.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a joint embedding approach for multitask learning in face forgery detection. The method defines a set of coarse-to-fine face forgery detection tasks based on face attributes at different semantic levels, and describes the ground truth for each task via a textual template. CLIP is used to implement the joint embedding architecture, and multi-level fidelity losses are used for multitask learning. The proposed method outperforms state-of-the-art detectors in terms of generalization ability.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper proposes a joint-embedding-based multitask learning method for face forgery detection. It could probably be the first work to apply the language prompts on the task of face forgery detection.
2. This paper defines a set of coarse-to-fine face forgery detection tasks based on face attributes at different semantic levels to facilitate the multitask learning.
3. The proposed method achieves better performance than the SOTA schemes in terms of generalization ability.

Weaknesses:
1. The authors apply the existing technologies including CLIP and fidelity loss for joint-embedding-based multitask learning. The technical contribution is rather limited. 
2. It lacks of explanation of why the authors use CLIP for joint learning. For the same token, it also lacks of analysis regarding the use of fidelity loss for multi-task learning. 
3. The works for comparing the robustness do not include the SOTA schemes. 



Limitations:
Please refer to weaknesses. 

Rating:
5

Confidence:
5

";0
3S9Oiu6gMf;"REVIEW 
Summary:
The paper describes an approach for learning bounded in-degree polytrees that a family of Bayesian networks. More precisely, given the skeleton of the polytree $P$ from which the samples are from, their algorithm learns a $d$-polytree whose distribution is likely to be close to $P$ (with respect to KL divergence) using mutual information tests. Importantly, the algorithm runs in polynomial time for a fixed $d$, whereas the exact learning problem is known to be NP-hard for $d > 1$.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
To my knowledge, the theoretical results are novel and show that even though the optimal $d$-polytrees are hard to learn exactly, it can be done approximatively.

Weaknesses:
My main concern is in the relevance of the article to AI community, i.e., it has nice theoretical results, but their practicality remains unclear to me (see Questions section of this review). I would be happy to increase my score if the authors can offer convincing arguments for this.

I also recommend carefully proofreading the paper to improve its presentation. To mention some of the minor issues:

- 139: ""We denote $\pi(v)$ to denote""
- 143: The definition of deg-l v-structure should probably include the lack of edges between $u_i$ and $u_j$? Of course, that holds implicitly for forests.
- 143: ""We say that -- is said to be""
- 153: Meek [1995] -> [Meek, 1995]
- 186: has -> have

Limitations:
See Questions.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the number of samples to learn a particular class of distributions: bounded-degree polytrees (Bayesian networks whose skeleton is a forest). Recent work has shown that tree-structured Bayesian networks (1-polytrees) are learnable with finite samples; this work makes progress on the natural generalization to polytrees, showing a positive result when the skeleton is given. The work also provides some conditions under which the skeleton is learnable, and a lower bound for the number of samples required.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Learning a distribution approximately from finite samples is one of the most fundamental tasks in learning theory. This study of the finite-sample learnability of polytrees is a very natural step for building our understanding of this problem, particularly in the context of the recent work showing learnability for tree-structured models.

The main result of Theorem 1 (finite-sample learnability of degree-bounded polytrees given the skeleton) is quite fundamental. The algorithm and proof are generally quite natural, and furthermore they help demonstrate the clean manner in which the mutual information tester machinery of [Bhattacharyya et al., 2021] can be leveraged for such results. While accompanying results in Section 4 (Skeleton assumption) and Section 5 (Lower bounds) are less surprising, their presence adds more completeness to the general picture.

The paper is generally well-written.

Weaknesses:
More motivation for studying polytrees might be appreciated by the general NeurIPS community. Regardless, Bayesian networks are well-motivated and polytrees are a natural continuation of the aforementioned recent work.

The assumption of being given the skeleton is perhaps the most unsatisfying aspect of these results. For context, my understanding is that when learning tree-structured models (as is the focus of the main prior work of [Bhattacharyya et al., 2021]), the entire task is determining the skeleton, as any rooting of the tree is equivalent. In this sense, it is somewhat disappointing that the entire task of the main prior work needs to be given to the polytree learning algorithm. It would be nice to know whether this assumption is inherently required or just an artifact of the current algorithm.

Limitations:
The limitations are addressed fairly in the paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces an efficient learning algorithm for bounded degree polytrees and establishes finite-sample guarantees. Explicit sample complexity and polynomial time complexity are provided. An information-theoretic lower bound is provided, which shows that the sample complexity of the algorithm is nearly tight.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides a novel algorithm for learning d-polytrees with general d, extending a previous algorithm for d=1. The theoretical analysis shows that the algorithm is nearly tight in terms of sample complexity. The results do not require distributional assumptions such as strong faithfulness. The ideas and results are clearly presented in the paper.

Weaknesses:
The recovery of the true skeleton relies on Assumption 11. It would be nice if some comments on this assumption could be given (e.g. whether it is expected to be tight)

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper gives an efficient PAC-learning algorithm for learning graphical models called ""bounded polytrees"". These are distributions where 1) the undirected skeleton of the graph is a forest and 2) the in-degree of every node is bounded by some constant $d$. This extends a recent result [1] for directed trees, which is corresponding to the case $d=1$.

In contrast to [1], the paper gives a learning algorithm assuming that the skeleton is given. To achieve that, the estimator of conditional mutual information from [1] is extensively used. This estimator is used in a sequence of clever greedy-like checks in order to orient as many edges as possible. After orienting the remaining edges, it is shown that the resulting distribution must have small KL divergence to the true distribution.

A sufficient condition is also given, under which the skeleton can be learned for certain distributions by the Chow-Liu algorithm (so it does not have to be given to the algorithm). Finally, a lower bound on sample complexity is proved, roughly matching the upper bound of the algorithm in the case of binary alphabet.

[1] Bhattacharyya, Gayen, Price, Vinodchandran, ""Near-optimal learning of tree-structured distributions by Chow-Liu"", STOC 2021.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The studied problem of efficient learning of graphical models is important and interesting.

* The paper considers distributions with a tree skeleton and arbitrary orientation of edges as opposed to just directed trees. This is a natural and long-studied class of distributions.

* Even given the estimator from [1], the algorithm and proofs are interesting and not trivial.

* Section 3 gives a good outline of the algorithm and its correctness proof and the figures were helpful to me.

Weaknesses:
* The algorithm requires the skeleton as input, which I think is a significant limitation. It is not clear how useful is the sufficient condition proposed by the authors in order to remove this limitation.

* The writing could be clearer. Especially the steps which I assume are more standard/obvious to the authors felt rushed. In my opinion, a few places could be rewritten in order to be clearer and more self-contained.

Limitations:
see above

Rating:
6

Confidence:
2

";0
9buR1UFCDh;"REVIEW 
Summary:
This work proposes an extension to DQN aimed at improving projection steps in Q value updates. There are two main contributions of the paper:

- The paper intuitively explains the Q-value learning characteristics of DQN variants caused by a mismatch between the optimal Bellman operator and the set of representable Q functions.
- The authors propose the iterated DQN (iDQN) method, which keeps track of a collection of K online Q-functions. When updating these Q networks, the previous Q function is used as the target network.

Experiment results show that iDQN outperforms a collection of DQN variants on the standard Atari benchmark. Further ablation studies explore the effect of K and sampling strategies for iDQN and conclude that bigger K and uniform sampling of Q networks are in general preferable.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The figures explaining the projection characteristics of existing DQN variants are very intuitive and provide excellent motivation for the proposed method.
- The iDQN method, the newly introduced hyperparameters, and the experiment settings are communicated clearly and transparently.
- The results on Atari are convincing.
- The ablation studies on the choice of K and sampling method are insightful.

Weaknesses:
- The figures used to explain the projection behaviors of DQN variants are created for illustration, but not from actual experiments.
- As discussed by the authors, iDQN doesn't outperform more recent DQN variants which employ other tricks to improve performance.

Limitations:
The authors discussed how iDQN is not able to outperform more recent DQN variants using other tricks. It would also be nice if the authors can discuss the training stability of iDQN.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, a new variant of DQN algorithm, iDQN, is proposed by replacing the classical Bellman iteration with several consecutive Bellman iterations and using multiple Q networks.
Intuitively, this new Bellman operator propogates reward sigals more efficiently thus speeds up learning, with the cost of more computation and memory.
As the number of consecutive Bellman iterations increases, it is shown that the learning performance of iDQN in Atari games is also increased.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
As far as I know, the proposed method is novel. The new algorithm, together with several baselines, are tested in 54 Atari games.
Many illustrative pictures are included to help understand the new Bellman operator.
The paper is generally well-written.

Weaknesses:
1. It will make this work much better if a theoretical analysis of the proposed Bellman operator is provided, such as convergence speed, the affect of the number of Q networks, etc.
2. The performance of iDQN is only slightly better than baselines, such as DQN(Adam). A summarized result (e.g. the first figure in [DQN Zoo](https://github.com/deepmind/dqn_zoo)) can make the comparison in Atari games much clearer.
3. Missing related works about ensemble methods + DQN, e.g. Averaged DQN and Maxmin DQN.
4. Misssing baselines: DQN + n-step return. Both iDQN and this baseline try to accelerate the propogations of reward signals. Furthermore, although it is mentioned that ""We do not consider other variants of DQN to be relevant baselines to compare with."", more explanations are necessary.
5. It is claimed that ""Our approach introduces two new hyperparameters, rolling step frequency and target parameters update frequency, that need to be tuned. However, we provide a thorough understanding of their effects to mitigate this drawback. "". However, I don't find the understanding thorough enough.

Limitations:
The limitations of iDQN are that it takes more memory and computation than DQN. Jax is used to parallelize the computation, making the training time increase acceptable.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on learning the projection of the empirical Bellman operator's iteration on the space of function approximator (neural model). This being done through increasing the number of gradient steps using multiple heads with a certain form of update. While retaining the same total number of gradient steps and samples compared to common approaches, the proposed method seems to provide better results (at the cost of retaining multiple heads and more computation).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea is interesting, novel and practical. The paper also **experimentally** shows noticeable improvement over various baselines.

Weaknesses:
- The presented method is quite simple and could have been presented much more efficiently with simple math and direct explanation rather than lengthy discussions over multiple figures. 

- The choice of $K$ seems to have a significant impact on the behaviour, which also varies depending on the domain. Suggestion: some formal analysis (e.g., the algorithm's variance) could be useful to provide better insight about what to expect from larger $K$ in terms of other characteristics such as the transition kernel.

Limitations:
While the authors mentioned at the end of Introduction that ""We conclude the paper by discussing the limits of iDQN ..."", they apparently forgot to do so! No discussion of limitations is provided.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents Iterated Deep Q-Network (iDQN), a new DQN-based algorithm that incorporates multiple Bellman iterations into the training loss. The paper highlights the limitations of traditional RL methods that only consider a single Bellman iteration and proposes iDQN as a solution to improve learning. The algorithm leverages the online network of DQN to build targets for successive online networks, taking into account future Bellman iterations. The paper evaluates iDQN against relevant baselines on 54 Atari 2600 games and demonstrates its benefits in terms of approximation error and performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed iDQN algorithm introduces a novel approach to incorporate multiple Bellman iterations into the training loss, addressing the limitations of traditional RL methods.
2. The paper provides a well-structured review of relevant literature, discussing the behavior of various Q-learning algorithms in the space of Q-functions. This analysis helps in understanding the motivation behind iDQN and its relationship with other methods.
3) The empirical evaluation on selected Atari games demonstrates the superiority of iDQN over its closest baselines, DQN and Random Ensemble Mixture. This provides empirical evidence of the effectiveness of the proposed approach.

Weaknesses:
1. It would be helpful if the paper included more comparisons with widely-known baselines in the field. While the paper compares iDQN to DQN and Random Ensemble Mixture, it would be valuable to see how iDQN performs against other popular RL algorithms like R2D2.
2. Some parts of the paper could be further clarified to improve the reader's understanding. For example, the explanation of the loss function and the graphical representations of DQN variants could be made more concise and intuitive.

Limitations:
It would be helpful if the paper included more comparisons with widely-known baselines in the field. This paper has no negative social impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers the problem of how to get accurate approximations of optimal Q-functions. The paper introduces a new algorithm called iterated DQN (iDQN). iDQN incorporates multiple consecutive Bellman iterations into the training process, which aims to allow for better approximation of optimal action-value functions. It uses the online network of DQN to build a target for a second online network, and so on, for considering future Bellman iterations. The authors conducted several experiments based on Atari games by comaping iDQN with baseline methods, including DQN and Random Ensemble Mixture. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Significance and Originality: The topic that the paper studies - how to learn the Bellman iterations efficiently, is an important topic in reinforcement learning. The authors propose a simple yet effective method, called iterated DQN, to improve the learning efficiency. Specifically, iterated DQN uses a second online Q-network for learning the second Bellman iteration simultaneously, where the target for the second online Q-network is created from a second target network according to the first online network. In this way, the loss can include K-1 more terms compared to the original loss used in DQN. The way for using such kind of ensemble seems novel, which allows for improved efficiency.

- Clarity: The paper is well-written and easy to follow, with very clear illustrations for the update for DQN, REM, and iterated DQN as in Figures 1-4.

Weaknesses:
- Quality: The paper presents a simple yet effective idea, but it could be further strengthened particularly in theoretical and empirical analysis. First, the authors could provide a theoretical guarantee for iterated DQN by examining its convergence speed, in addition to the intuitive explanation given in Section 4.1. This would lend credibility to their claims. Second, the empirical validation raises concerns, as iterated DQN's performance is only marginally better than that of previous baseline methods such as DQN (Adam), C51, and REM. This modest improvement does not strongly support the paper's assertions. Lastly, it would be beneficial for the authors to include a memory comparison, as employing more Q-networks may lead to increased memory costs, which is an important consideration for practical applications.

Limitations:
The authors have discussed some of the limitations by considering other value-based methods.

Rating:
4

Confidence:
3

";0
a4kspTMV9M;"REVIEW 
Summary:
The authors propose an implementation of Vacher et. al. (2021) based on a Semi-Smooth Newton (SSN) scheme. They reformulate their optimization problem as a root finding problem (Proposition 3.1) to which they apply the SSN scheme. They provide convergence guarantees (Theorem 3.3) that gives a $O(1/\sqrt{T})$ convergence rate where $T$ is the number of iterations and provide an efficient way to reduce the cost per iterations (l.184 - l.224). Then they provide numerical experiments to validate that their method is faster than the one proposed in Vacher et. al. (2021).

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Trying to get a scalable version of kernel based OT is a very legit topic as current implementations are slow and impossible to run on real data sets. Indeed, recall that using an Interior-Point-Method, kernel based OT was solved with a precision $\epsilon$ in $O(n^{3.5} \log(n/\epsilon))$ time where $O(n^3)$ comes from the cost per iteration and $O(\sqrt{n}\log(n/\epsilon))$ is the number of iterations. In this paper, the main contribution is to get rid of the dependency of $n$ in the number of iterations which is indeed a desirable feature. From my understanding, the authors can solve kernel based OT with precision $\epsilon$ in $O(1/\epsilon^2)$ iterations.

Weaknesses:
I believe that the authors oversell the work. As can be deduced from my comment above, the proposed method requires $O(1/\epsilon^2)$ iterations for a precision $\epsilon$ while previous work requires $O(\sqrt{n}\log(n/\epsilon))$ iterations. When a high precision a sought after $\epsilon \to 0$, the proposed algorithm is indeed less efficient. The authors should have explicitly mentioned that.
Furthermore, nothing precise is said on the cost per-iteration which is a crucial component of the practical efficiency. We can vaguely guess that it is $O(n^3)$ but it is stated nowhere.

The overall writing is confusing, the whole part on the computational efficiency should be clearly stated in a theorem or a proposition.


Limitations:
The authors do not compare with enough precision their algorithm with the existing one, both in theory and on practice.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The authors focus on the problem of approximating OT numerically.
They focus on one approximated version of OT which leverages a Sum of Squares approximation to stratify both statistical guarantees and computational amenability.
While the first proposal to solve this SoS approximation relied on interior point methods, the authors focus on a semi-smooth Newton method.
It consists in considering KKT optimality as some equation $R(w)=0$ and solve this equation using Newton updates.
They derive the algorithm in this specific OT setting, and prove convergence guarantees and rates of their methods.
They show experiments on synthetic data to see the approximation impact, and compare with interior point methods.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
This recent OT formulation satisfying statistical guarantees and being computationally amenable is an interesting quantity to estimate.
The proposal of the authors to propose another algorithm to estimate it and scale it to larger measures would increase the interest of this formulation to practitioners.


Weaknesses:
*The introduction is not precise enough*
- Line 25, the rate $O(n^{-1/2d})$ is actually worse than the original rate. I think the authors meant a rate $O(n^{-2/d})$.
- This is a secondary remark, but Line (31,32) another approach which attempts to regularize OT and ease computation is to consider mini batches of input data. I mention the work [FZFGC] and references therein if the authors wish to complement their introduction review.
- The citation [44] in your paper is irrelevant. It focuses on estimating the OT Monge map when it exists, which is not the problem of estimating the cost, which you consider. Also, saying ‘a specific […] estimator’ is a super vague formulation which should be made precise.
- Do the authors have references or precise rates to defend the assertion line 45-47 that « interior-point method is well known to be ineffective […] as the sample size increases » ? Similarly, do the authors have references that semi-smooth Newton method have better convergencce/scaling guarantees ? 
- I do not understand the sentence « While there is an ongoing debate in the OT literature on the merits of computing the plug-in OT estimators v.s. kernel-based OT estimators […] ». Which debates it is ? On which aspect does it especially focus ? This sentence is too vague to be insightful.
- I do not understand the sentence Line 129 « kernel-based OT estimators are better when the sample size is small and the dimension is high ». Does that mean that the fewer samples we have, the better the approximation ?

*The semi-smooth Newton method is not clear to understand* 
- Line 76, I think the authors should have introduced background knowledge on Semi-Smooth Newton methods instead of postponing them in the appendix. Furthermore, what is described by the authors is a review of previous contributions on this method, but no mathematical formulas are detailed. I would have put this part in the main body for related work, especially [33] which is exactly the same method as you, but for unregularized OT, and which you do not mention as related work. Lastly, to provide a self-contained and pedagogical description, I would have ideally wanted a brief description of SSN with a general framework, so that your work is an instantiation of this formulation.
- I think Definition 2.1 is not extremely useful as it is the definition of optimality in a minimization program, and you can remove it.
- Something that is not clear for me is whether some matrices are symmetric or not. First the set $S^n_+$ usually represent symmetric, positive matrice, but I see no symmetry in Line 152. The projection over $S^n_+$ of Equation (3.1) is true if Z is symmetric (or X in you context), but I see nowhere that X is assumed to be symmetric (or proved to be symmetric through iterations).  Line 192, you mention a Schur Complement trick to make the Jacobian symmetric, but when the matrix is asymmetric, there is no reason that the Schur complement is symmetric. All in all, the derivation of the method seems unclear and ill-posed. Could you please clarify on this ?
- Could you please define a quadratic rate of convergence using an equation ?

*Some experimental improvements to suggest*
You reproduce the experiments from [59], which is good to establish a comparison. However I think it could be made much clearer with some modifications.
- In Figure 2, I would be interested in seeing the point wise difference between $c - \hat{u} - \hat{v}$ and $c - u_*-v_*$. It would emphasize where the approximation is best performed using this estimator. Reproducing the same experiment using interior point method would be insightful.
- I don’t understand how time is estimated in Figure 3. Do you report the time to do a given number of iterations ? Is it the time to reach a given level of accuracy ? Without this I cannot make sure the comparison is fair.
- I think that reproducing Figure 6 from [59] would be insightful. My main question is that you focus on time and approximation error, but I would like to see the statistical error estimation as the number of samples grow. Reproducing this Figure (and comparing with interior point) would illustrate that your computational approach maintains the favorable statistical properties of this OT estimator.

[FZFGC] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., & Courty, N. (2019). Learning with minibatch Wasserstein: asymptotic and gradient properties


Limitations:
The authors adressed the societal impact of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on investigating kernel-based optimal transport estimation. The approach involves reformulating the problem as a nonsmooth equation model and utilizing the semismooth Newton method to solve it. The study demonstrates that the associated residual mapping exhibits **strong semismooth** properties, ensuring the applicability of the semismooth Newton method. Additionally, it is verified that the subproblem within the semismooth Newton method is well-defined, as it is equivalent to solving an invertible symmetric linear system. Finally, the proposed algorithm is supported by both theoretical guarantees, including global and local rates, and numerical experiments that highlight its superiority.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The algorithm is highly practical and can be easily implemented. The paper provides clear instructions on solving the subproblem and updating the parameters, making it accessible for real-world applications.
2. The theoretical investigation is rigorous and well-founded. The authors define a suitable residual function and present both global and local convergence rates of the proposed semismooth Newton algorithm. 
3. The numerical experiments provide compelling evidence of the algorithm's efficiency compared to existing methods. The results showcase the superior performance and computational advantages of the proposed approach, reinforcing its practical relevance and effectiveness.

Weaknesses:
1. The global convergence rate of the proposed algorithm is dependent on an auxiliary sequence of iterates, which adds extra computational complexity to the algorithm. It would be helpful to provide further clarification in line 238 regarding whether the condition $$w_{k+1}=v_{k+1}$$ always holds. If so, the proposed algorithm will reduced to extragradient method. 

2. To show the power of semismooth Newton steps, the proposed algorithm should be compared with the pure extragradient method in numerical experiments.



Limitations:
See weakness and questions for further details. 

Rating:
7

Confidence:
4

";0
8SDsff42Lj;"REVIEW 
Summary:
This paper studies continual learning in NLP by leveraging global prototypes. The authors attribute the catastrophic forgetting to the disruptive updates caused by the misalignment between the knowledge learned from observed tasks and the knowledge required for future tasks. To tackle this problem, the authors propose NeiAttn which derives global prototypes and learns proper relationship between the prototypes and data representations for each task. Experiments show that models learning data representations well related to global prototypes can induce less catastrophic forgetting and NeiAttn outperforms baselines in both task-incremental and class-incremental learning setting.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper and proposed method are well-motivated. The paper reveals a misalignment between the knowledge learned from observed tasks and the knowledge required for future tasks. This is a core issue in continual learning, especially in the continual learning with pre-trained models. The learning objective introduced in Line 154-156 is clear.
2. The paper considers the property of pre-trained language models (discussed in Line 175-178). Personally, I think this perspective is important as pre-training is very common in building machine learning system but previous literatures in continual learning seldom consider the property of pre-trained models.
3. The experimental results show the effectiveness of NeiAttn.

Weaknesses:
1. The writing of the proposed method is not clear. Based on my understanding, the core of the proposed method lies in Equation 9, though there are many related contents in Section 3 and Section 4. It would be better if there is an overview of your proposed method given by pseudo code or illustration; or, it may be clearer to introduce your method in a top-down way.
2. NeiAttn outperforms Prompt Tuning marginally according to Table 1 and Figure 4. Based on the motivation and desiderata introduced by this work, I'm wondering if you could apply the training objective in Equation 3, 4 to the Promt Tuning framework. Will it give you better results and simpler method than the current NeiAttn?
3. In Appendix F, the authors give the ablation results of number of neighbor attention layers. The results show that fewer neighbor attention layers give better CL results. If all layers use Neighbor Attention, the results are very bad. Then what's the benefit of using Neighbor Attention for continual learning?

Limitations:
The paper mentions some limitations in Section 7.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper focuses on continual learning in NLP and introduces a regularization-based method to tackle the problem. The main contributions of this work include global alignment, highlighting general-purpose knowledge across tasks, and neighbor attention, which offers a novel parameter-efficient tuning approach. Experimental results on both task-incremental and class-incremental learning scenarios demonstrate the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written with smooth flow. It addresses two popular continual learning (CL) settings effectively. The proposed intuition aligns well with LLM and makes logical sense.






Weaknesses:
1. The underlying idea is built upon the assumption that LLM contains general-purpose knowledge and that learning tasks should not deviate too far from it. This idea shares similarities with other regularization methods and may suffer from similar drawbacks, such as potential negative impact on new task performance and insufficiency of soft regularization. From this perspective, it is not clear how this paper addresses the problem in a way that other regularization methods cannot.

2. Table 1 lacks inclusion of several CL NLP baselines, (there is an extensive survey in [1]). Only MBPA, published in 2018, is listed for NLP. It is suggested to consider adding more CL NLP baselines, such as [2] (which you have cited) and other latest work mentioned in the survey.

[1]: Continual Learning of Natural Language Processing Tasks: A Survey. https://arxiv.org/abs/2211.12701  
[2]: Achieving forgetting prevention and knowledge 443 transfer in continual learning, NeurIPS 2021


Limitations:
See weakness 1

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors address the problem of catastrophic forgetting in continual learning and propose to connect observed and unknown tasks by means of task-specific data representations which can be seen as general-purpose representations useful for a wider range of tasks. To this end, they introduce the notion of global prototypes which can be pre-learned and reflect data semantics. Based on these they learn more task specific representations using an objective that trades off two losses (classification loss and prototype loss) . In experimental verifications of their ideas, they consider NLP tasks and find that catastrophic forgetting can successfully be reduced and their neighbor attention model achieves better performance than previous baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The broad ideas of this paper are easy to follow. The overall idea for how to mitigate catastrophic forgetting appears simple, sound, and solid. The idea of considering transformers with neighbor attention, too, is simple yet compelling. Experimental evaluations appear to be rigorous and comprehensive; results reveal favorable characteristics of the proposed framework.

Weaknesses:
At points there are concerns regarding technical details. Certain statements appear to be handwavy.  Sentences such as „In practice, we add neighbor attention to less than half of the transformer layers and leave the last layer untouched for guidance.” or „In continual learning, the optimal layer selections for different tasks may vary.“ could need more elaboration. Throughout, several hyperparameters are introduced (K, \alpha, \beta, …) which apparently have then been set in a heuristic manner. The contribution would be stronger if a (mathematical) reason for these choices had been given. 

Limitations:
The authors openly address limitations of their proposed approach (increased memory requirements and additional need for hyper-parameter tuning). There are no concerns w.r.t. to negative societal impact of this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a continual learning method for both task and class incremental learnings by incorporating global prototypes. These global prototypes are derived from a pre-trained masked language model and are used to make connections with task specific prototypes. By maintaining these connections, the proposed method prevents task knowledge from forgetting. The authors additionally introduce a trainable module called AttnNei for multi-head attention. The proposed method is evaluated using several existing CL methods and the performance is compared with different adaptation models on BERT-base.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The overall approach of using global knowledge and task-specific knowledge to prevent forgetting is interesting.
2. The proposed method is applicable to existing CL methods.

Weaknesses:
1. I found it difficult to comprehend how and why Eq.2 establishes a connection between global knowledge and task-specific knowledge, leading to the prevention of forgetting.
2. The experiment results do not present good advantage of the proposed method over existing methods due to the following reasons: i) The baselines are too old to know how much improvement it would make when it’s applied to more recent methods. ii) The experiment only demonstrate improvements for CL methods whose original models do not leverage adapters or prompts. However, there re CL methods that already incorporate prompts or adapters [1, 2]. To provide a broader perspective, the authors should compare their method with these approaches. iii) According to Fig 3 and Tab 1, PT2 appears to be comparable in performance, while requiring fewer resources as indicated in Tab 2

3. This method is only applicable to NLP tasks, given that the global prototypes are obtained from language models. Most continual learning methods are designed to be general and not limited to specific task types such as vision or text. Therefore, a discussion on how to apply this method to non-language tasks should be included.

Misc.
Typos: Rationle in line 251. preservingholistic in line 81.

[1] Wang et al. Learning to Prompt for Continual Learning. CVPR, 2022

[2] Kim et al. A multi-head model for continual learning via out-of-distribution replay. CoLLAs, 2022

Limitations:
Refer to Weaknesses and Questions

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper addresses the continual learning problem by leveraging a concept called global prototypes, which are invariant features that are not altered during task-specific continual learning. The training is thus augmented with an additional alignment loss of the data features to the prototypes. The paper then motivates that adapter-like parameter efficient fine-tuning (PeFT) are prototype aligned. Experimental-wise, the paper conducted studies on different PeFT fine-tuning methods for continual learning and showed that their proposed light-adaption NeiAttn and an existing PeFT method PT2 is significantly better than others.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The motivation of the paper is clear and the experiments conducted are extensive. The overall presentation is clear. Figures are nice and illustrative.

Weaknesses:
1. The paper is dense and while the authors have tried to illustrate it, I still find some parts less convincing. Mostly, I feel like there is simple argument to summarize Section 3: continual learning should not deviate from previous task learned parameters much. This seems to justify all the later experiments and analysis. The current Section 3 draws an abstract component of global prototype, but only realizes it as fixed model parameters, which I feel is redundant. Maybe I missed a point in the paper though, happy to be corrected.
2. There is a lack of measurement on how an adapter module is closer to the original model (closer to the prototype), the paper arrived at a conclusion that PT2 and their NeiAttn are better, but there doesn't seem to be a strong reason behind.

Limitations:
some limitations were discussed about the requirement of hyperparameters for their proposed NeiAttn.

Rating:
5

Confidence:
2

";0
IaoovD6nDx;"REVIEW 
Summary:
In this paper, the authors introduce a method called State-Wise Action Refined to explore the causal relationship between actions and task rewards in reinforcement learning. They address the issue of action space redundancy and propose interventions on the primal action space as a means to discover causality. The authors present two practical algorithms, TD-SWAR and Dyn-SWAR, which respectively identify task-related actions during temporal difference learning and uncover important actions through dynamic model prediction. These approaches not only provide insights into the decision-making process of RL agents but also enhance learning efficiency in tasks with redundant actions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:


The authors proposed a method called State-Wise Action Refined to explore the causal relationship between actions and task rewards in reinforcement learning, which is significant and interesting.

Weaknesses:
The technical description of the proposed approach is complicated, and the main concern lies in the fact that the main technical contribution is not clear.

The paper contains some grammar mistakes and lacks sentence flow, making it difficult to understand the main contribution of the paper. For example, lines 34-36.

The proposed two practical algorithms, TD-SWAR and Dyn-SWAR, do not have sufficient experimental analysis support.

Limitations:
The technical description of the proposed approach is intricate, and the primary concern is the lack of clarity regarding its main technical contribution. The paper contains grammar mistakes and lacks sentence flow, making it difficult to understand the central contribution. For instance, lines 34-36 are particularly confusing. Additionally, the practical algorithms proposed, TD-SWAR and Dyn-SWAR, suffer from insufficient experimental analysis support,

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a strategy for action space reduction, based on causality-inspired modeling. It employs a method of action space selection, using collected data to identify the specific dimensions of actions that are meaningful to the task, as well as to improve the efficiency of exploration in reinforcement learning. The empirical evaluation confirms the effectiveness of the proposed module. Overall, the method is straightforward and can be embedded into most existing model-free and model-based RL frameworks. However, there are some details of the method and specific applications of causality, along with theoretical guarantees, that need clarification from the authors. Therefore, in the initial review, I give it a borderline score. If the authors provide the corresponding discussion and explanations, I will increase the score in subsequent reviews.






Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
[**About Motivation and General Idea**] Even though there have been related studies, research on action reduction is very meaningful for Reinforcement Learning (RL), as it can improve sample efficiency and potentially even learning efficiency. The proposed approach of integrating causality to identify which actions have a causal relationship with the task reward provides an interpretable action reduction method.

[**About presentation**] The overall presentation is clear and easy to follow. The pipeline figure (Fig. 1) helps the readers to understand the steps. 

[**About experiments**] Although there are similar baseline methods for comparison, overall, the experimental results validate the effectiveness of the method proposed in the paper, and the experimental design is fairly comprehensive. I will provide some additional suggestions regarding the experiments in a later section.

Weaknesses:
Note: Most of the weaknesses listed below are more like questions, discussion points, or suggestions, rather than outright flaws. Any clarifications provided by the authors would be very welcome and appreciated. Additionally, I don't have any personal interest in the related papers I suggested for reference. Given the limited time for rebuttal, it is not necessary to fully supplement experiments for comparison with these papers. Some clarifications and discussions on these methods would be greatly appreciated.

[**Regarding Intervention**] The authors mentioned that the causal discovery here is based on intervention, but there is a distinction between action and intervention. The authors could consider providing a clearer definition of intervention in this context, how interventions are conducted, and whether the target of each intervention is known, etc.

[**Regarding the Causality**] Although the entire pipeline is well designed and its effectiveness can be verified empirically, more explanation is needed on how to connect it with causality. From the method, it's not clear how to use causal properties for modeling or how to directly learn the causal relationships among states, actions, and rewards. The authors could consider providing some clarifications, either regarding theoretical guarantees (on the causal level) or presenting learned causal graphs, to check whether they empirically match the causal relationships between actions and state rewards in the environment. Here are some references: identifiability proof with action involved system: [1-2], and causal RL with learned causal graphs [3-4]. 



[**Aout related works**]

Regarding the discovery of causal RL and how to use causal structure to simplify the action space in RL, there are some related works. The authors could try to discuss them in the main text, possibly adding a subsection to introduce these works.

[**About Experiments**]

- For the experimental section, is it possible to visualize the learned sparse graph structure and compare it with the physical structure in real scenarios to see if they are similar?

- The current comparison does not involve related causal RL methods. It would be beneficial to discuss these, especially the one presented in [5], which uses a learned causal structure to find the minimal space—a very similar approach.

[**About Presentation**]

- In Figure 2, the graphical model is presented, but the title lacks some explanations, such as which dimensions have a causal relationship with the dynamics and reward, and what the blue and pink colors represent.

-  On line 195, does ""figure 5"" refer to ""figure 3""? I noticed that figure 5 is in the appendix, and its content is similar to that of figure 3.

- In my opinion, it would be beneficial to first introduce the overall framework of action reduction and causal discovery, then provide specifics about its implementation within Reinforcement Learning, in conjunction with both model-free and model-based RL. This approach could also simplify algorithms 1 and 2, merging them into one algorithmic pipeline, detailing the implementations under different RL frameworks.

**References**

[1] Lachapelle, Sébastien, et al. ""Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA."" Conference on Causal Learning and Reasoning. PMLR, 2022.

[2] Yao, Weiran, Guangyi Chen, and Kun Zhang. ""Temporally disentangled representation learning."" Advances in Neural Information Processing Systems 35 (2022): 26492-26503.

[3] Ding, Wenhao, et al. ""Generalizing goal-conditioned reinforcement learning with variational causal reasoning."" Advances in Neural Information Processing Systems 35 (2022): 26532-26548.

[4] Wang, Zizhao, et al. ""Causal dynamics learning for task-independent state abstraction."" arXiv preprint arXiv:2206.13452 (2022).

[5] Huang, Biwei, et al. ""Action-sufficient state representation learning for control with structural constraints."" International Conference on Machine Learning. PMLR, 2022.






Limitations:
The authors briefly describe some limitations in the paper. I believe the main limitation might lie in how to theoretically link with the causality claimed in the article. From an empirical perspective, the work is relatively complete. Future work could consider making the entire framework scalable to more complex scenarios, such as MARL, etc.






Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes an intervention-based method for understanding causality in Reinforcement Learning, aiming to enhance learning efficiency in tasks with redundant actions. The authors present two novel algorithms, Temporal Difference State-Wise Action Refined (TD-SWAR) and Dynamic State-Wise Action Refined (Dyn-SWAR). TD-SWAR identifies task-related actions during temporal difference learning, while Dyn-SWAR uncovers important actions through dynamic model prediction. The paper expands upon the recent advancements in instance-wise feature selection technology (INVASE), demonstrating how these improvements allow for more effective selection of task-related actions. Experiments underscore the effectiveness of causality-aware RL agents in action-redundant settings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper tackles the underexplored topic of causality-awareness in RL, utilizing interventions on the action space to ascertain task-related actions. cal underpinnings.
2. The authors have made a clear effort to explain their work, providing intuitive examples such as the in-hand manipulation tasks and human learning analogies. The logical flow of the paper seems coherent.
3. The paper tackles a relevant issue in RL - action space redundancy and the importance of understanding causal relationships for learning efficiency. The findings have the potential to influence future RL research, particularly on exploration strategies and action space selection.

Weaknesses:
1. The action space of experiments environments are all very small. Given the known scaling limitation of causal discovery, it's important to discuss the capability of the proposed model as most of the real world RL environments are complex.
2. The paper could benefit from more comprehensive evaluation. They only compared with TD3 while there are many more advanced RL algorithms that have better performance on the experimented environments. Therefore it's not convincing that pruning action space is beneficial even in the simple environment.
3. The connection between the proposed methods and real-world application scenarios seems somewhat abstract. The authors could better explain how their proposed methods can help solve practical problems.
4. It's better to provide code to enhance reproducibility.
5. Formatting issue in line 6.

Limitations:
The authors didn't address limitations nor broader societal impact, but I didn't see any ethical concerns.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper shows a Q-learning-based RL algorithm that jointly performs feature selection to detect action variables that are not relevant to dynamics or rewards. The feature variable selection algorithm adopts INVASE, and the experiment confirms the idea of five artificially created extended environments.

----

During the rebuttal phase, we found that there are several major technical flaws around the causality. 
Reflecting on such issues, I adjusted the score to the criteria.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The strength of the proposed approach is a simple extension of Q-learning-like algorithms that jointly perform action variable selection that improves the overall learning performance when there is a large number of redundant actions.

* originality: The idea of combining feature selection with INVASE during TD learning is novel
* correctness: The proposed idea looks correct
* clarity: The algorithm was presented in detail
* significance: This method may work well when there are large number of redundant actions

Weaknesses:
Although the title and Figure 2 mention “causal” and SCM, this paper is not relevant to causal discovery, or SCM is not relevant in the context. It can be better said as a kind of “feature selection.”
I am not sure how realistic it is to assume the nuisance actions and inflate action space to 100 by adding redundant actions in the experiment. 

* Originality: The idea is original, and it could be improved if it considers settings common in causal discovery
* correctness: I believe the overall framework is correct, and I don't see a particular weakness
* clarity: This paper is not relevant to causal/SCM, which confused me by giving the impression that it performs causal discovery while TD-learning.
* significance: The experiment setting is unrealistic, and it limits assessing the significance

Limitations:
I think this paper is not relevant for discussing limitations.

Rating:
2

Confidence:
4

";0
SlXKgBPMPn;"REVIEW 
Summary:
This work proposes another auto-regressive-based graph generative model similar to GRAN. The authors propose a hierarchical generation scheme to un-coarse a graph level by level. In each non-leaf level, the abstract graph is weighted both in nodes and edges. A node represents a community, and its weight represents how many edges should be inside the community. An edge is the ""connection"" between two communities, and its weight represents how many edges should exist between the two communities. The weights of each community are generated through a stick-breaking process. And the number of communities is automatically decided by it. The structure within the community is generated using an AR model. Then the edges between communities are generated using GNN.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The assumption makes sense, and the model decomposition is quite convincing.
2. This method can indeed improve generation efficiency by only auto-regressively generating the diagonal blocks of the adjacency matrix and using GNN (which has O(M) runtime) to predict the off-block entries.
3. The method is simple and straightforward.

Weaknesses:
see questions below

Limitations:
1. one possible limitation is the edge independency of the model.
2. The model has made a strong assumption that the graph should have a community structure, while the experiment datasets are relatively small and may not have such a structure.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes HiGen a hierarchical generative graph model. The model consists of a clustering process (Louvain), followed by a GNN model (GraphGPS) to estimate probabilities. The generative process is separated by communities and bipartite sub-graphs.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper seems original. The proposition of a new model is always an important contribution. Even though the new model is the combination of a clustering process and GNN. The combination of both ideas is interesting. 

The theoretical quality of the demonstrations is good. Most of them seem fine and no errors were observed during the revision. 

Parts of the papers are quite clear. Figure 1 really helps to understand the main idea of the paper. However, there is room for improvement.

The significance of the paper is high, it seems that this new model is able to reproduce the mean of the distribution quite correctly in comparison to other state-of-the-art methods, as it is shown in the results.

Weaknesses:
The state of the art can be improved. The paper mentions ""there exists no data-driven generative models specifically designed for generic graphs that can effectively incorporate hierarchical structure."". Neville et al. focused on this type of work, generating several papers related to hierarchical graph models (doi.org/10.1145/3161885, doi.org/10.1145/2939672.2939808, doi.org/10.1007/s10618-018-0566-x). 

Parts of the paper are closed related to mKPGM (doi.org/10.1145/3161885). In both cases there is a hierarchical structure, both have the idea of a super-node at the higher level, and the sampling process is also based on a multinomial distribution (doi.org/10.1007/s10618-018-0566-x). Please take a look at the sampling process proposed, because it has similarities to the proposition of this paper, and the authors claimed to sample a network with billions of edges in less than two minutes.

The paper must state its main contribution. In the beginning, it seems to be the model, but after reading the paper, it seems to be the sampling process. Unfortunately, both of them have different issues.

If the main contribution is the model, then the paper should improve the modeling of the main network and be fairly compared in the experiment section against other baselines (not just the mean of the distribution). The main models consider $\ell$ hierarchies, but just two are applied. It is also not clear how the final probabilities are obtained. 

If the main contribution is the sampling process, there are some issues too. The time complexity of the generative model claims to be O(n_c \log n), but this is not demonstrated. The results of the paper are focused on the modeling of networks, not the sampling process. For example, there are no empirical results about the time complexity, and the largest networks have some thousand nodes, rather than millions.

I understand that the papers follow the experimental setup and evaluation metrics of Liao et al. However, this methodology must be stated in the main paper, otherwise, the experiments of the main paper are not reproducible. 

The results of Table 1 are difficult to read because of the lack of explanations. There are no details on the separation of the data in the main paper. I understand that this is explained in the supplementary material (80% for training and 20% for testing), but it must be considered in the main paper too. Moreover, I do not know if the values are the average over the 20% of the testing graphs or, if you just considered it as a single distribution. In the first case, please add the standard deviation, to see if the difference at statistically significant. 

Section 5 claims: ""The results demonstrate that HiGen effectively captures graph statistics"". Considering that, generally speaking, MMD estimates the distance between the means of two distributions, I suggest you change it to ""The results demonstrate that HiGen effectively captures the mean of the graph statistics"". Given the use of MMD, you can not determine if the other part of the distribution are correctly estimated. 

The conclusions state that HiGen ""enables scaling up graph generative models to large and complex graphs"" but this is not demonstrated.

Limitations:
No, the authors did not consider the limitations of the proposed model. For suggestions, please check weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduces an innovative hierarchical method for graph generation, which employs multiple levels of graph coarsening. This approach begins with the first level, representing the most coarse graph, and progressively expands nodes and edges to form new communities and connections between the newly created nodes. At each level, nodes serve as communities for the subsequent level, and the edge weights, including both inter-community edges and self-loops, dictate the total number of edges within each community in the final graph. Consideration of independence among the generation processes of inter and intra-community edges, conditioned on the graph and edge weights from previous levels enables parallel execution of the steps, resulting in acceleration of the generation process.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper effectively utilizes hierarchical clustering to enhance the graph generation process, capitalizing on the benefits of this technique.
2. By introducing parallelization in generating distinct clusters at each level, the paper successfully minimizes the number of sequential steps required.
3. The experimental results presented in the paper demonstrate improvements across multiple datasets.
4. Paper for the most part is well-written and easy to follow.

Weaknesses:
1. In lines 35-36 paper mentions that this work is the first hierarchical method for generic graphs. I believe [1] is also a hierarchal method for graph generation. I understand that methods are significantly different but still it would be more accurate to highlight the unique aspects of the proposed method and consider including a comparison between the two approaches.
2. The time complexity analysis provided in the paper focuses solely on the sequential steps, neglecting to consider the computational requirements. It would be valuable to compare the overall computational workload, particularly since the proposed method utilizes the GraphGPS approach, which has a time complexity of $O(n^2)$, in contrast to conventional GNN methods with a complexity of $O(n+m)$. Including such a comparison would provide a more comprehensive analysis.
3. The paper lacks a study examining the distribution of community sizes during the generation process across different datasets. Addressing this limitation by investigating and reporting the distribution of community sizes would enhance the understanding of the method's behavior and its adaptability to various datasets.
4. The paper uses a more advanced GNN compared to methods like GRAN, raising the question of how much of the observed progress is solely due to the change in the GNN architecture. Conducting an ablation study specifically focused on the GNN architecture used would provide valuable insights into its individual contribution to the overall performance of the method.
5. The evaluation metrics commonly employed for graph generative models have their limitations, as discussed in [1] and [2]. It is important to consider these limitations in the evaluation process. The paper mentions the use of random GNNs as an alternative evaluation method, but this approach is only used in the appendix for a few experiments. I would suggest using this in the main body and comparing all models using this metric. [Additionally/Optionally, there are two more recent approaches, one based on contrastive training and another one based on Ricci curvatures that could be incorporated for evaluation purposes.]

[1] Shirzad, H., Hajimirsadeghi, H., Abdi, A. H., & Mori, G. (2022, May). TD-gen: Graph generation using tree decomposition. In International Conference on Artificial Intelligence and Statistics (pp. 5518-5537). PMLR.

[2] O'Bray, Leslie, et al. ""Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions."" arXiv preprint arXiv:2106.01098 (2021).

Limitations:
Considering the assumed independence among the clusters and the cross-edges connecting them, it is evident that there exist certain graph distributions which the model may struggle to learn.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a graph generative model that is analogously structured as the inverse process of graph pooling, where the model first split a single node into a metagraph. This metagraph is further partitioned by utilizing a multinomial scheme, which allows for the division of nodes and edges into intra-community and inter-community connections. The proposed model's performance is evaluated on several benchmarks using various metrics, demonstrating state-of-the-art results.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
1. The approach of initially generating a graph's skeleton and subsequently refining its details is a novel and intuitively logical motivation for the proposed model.

2. The proposed methods have demonstrated state-of-the-art performance on some widely adopted benchmarks.

Weaknesses:
1. Some important technical aspects in the paper may require additional clarification or more detailed elaboration. Here are the major concerns regarding specific aspects:

    (1) Can the authors please provide more information on the loss function utilized in the model?

    (2) How is the weight on level 0 determined during model inference?

    (3) On line 188, the node embedding matrix $\mathbf{h}_{\hat{C}}$ is referenced without being defined. Could the authors please explain how this matrix is generated from the node and edge embeddings of prior levels?

    (4) Could the authors please elaborate on how the graph neural network (GNN) is utilized throughout the entire process?

2. The utilization of notations in the paper has resulted in a significant amount of confusion. There are two main issues that need to be addressed:

    (1) Inconsistent notations caused by reusing the same symbols: One notable example is the letter ""t"" used at lines 187-188, which has multiple interpretations. In $\hat{C}_{i,t}^l$, ""t"" represents the ""t-th"" step in the stick-breaking process. In $h{(t, s)}$, ""t"" denotes the node that is associated with community ""i"". Moreover, when referring to the node matrix size as ""$t \times d_h$"", it indicates the total number of nodes in community ""i"". These varying interpretations of the same notation can lead to confusion and should be clearly distinguished or explained consistently throughout the paper.

    (2) Notations used without being defined: An example is the ""r"" symbol in Figure 1 (c). Although it is assumed to represent the acronym for ""remaining (edges),"" its precise definition is not explicitly provided in the paper. To enhance clarity, it would be beneficial to define such notations explicitly or provide a glossary of symbols and their corresponding definitions.

3. In the paper, the specific method for determining the number of mutually exclusive events (i.e., the edges split from the same parent node) when modeling the partition weights using a multinomial distribution is not explicitly mentioned. This aspect requires further clarification or explanation. The paper should provide details on how the number of events is determined, whether it is considered a fixed parameter based on the model's architecture or if it is treated as a latent variable to be inferred during the training process.

Limitations:
1. Please refer to questions 2 & 3.

2. The model is built upon the assumption that the graph contains underlying communities. While this assumption can aid in generating higher-quality graphs with evident community structures, it may come at the expense of the generation quality for graphs where the community structures are less apparent. It would be intriguing to explore how the quality of generated graphs varies with changes in graph modularity or other community metrics.

Rating:
5

Confidence:
3

";0
XF923QPCGw;"REVIEW 
Summary:
The paper focuses on two fundamental problems in regularized Graphon Mean-Field Games (GMFGs). The first problem is to establish the existence of a Nash Equilibrium (NE) of any $\lambda$-regularized GMFG (for $\lambda \geq 0$). The second problem is to propose provably efficient algorithms to learn the NE in weakly monotone GMFGs. Regarding the first problem,  this paper used weaker conditions than previous works analyzing unregularized GMFGs ($\lambda = 0$) or $\lambda$-regularized MFGs, which are special cases of $\lambda$-regularized GMFGs. To address the second problem, the paper proposes a discrete-time algorithm and derives its convergence rate solely under weakly monotone conditions. Furthermore, the paper develops and analyzes the action-value function estimation procedure during the online learning process, which is absent from algorithms for monotone GMFGs. The efficiency of the designed algorithm is corroborated by empirical evaluations.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. It's quite fascinating to uncover the link between MFG and GMFG as explored in the paper. Notably, the use of lambda-regularized MFG in Proof 1 to find the NE of regularized GMFG provides an intuitive understanding.

2. PMD for the general function approximation is impressive. The decision to employ policy mirror descent adds an interesting dimension to the methodology.

The paper appears to be well-rounded and articulately composed. It has a clear presentation of its findings. 

Weaknesses:
The paper does have quite a few notations, but it's understandable considering the complexity of GMFG. I've got a few questions, which might also point out some areas in the paper that could be improved. I've put these questions and potential weaknesses together in the question section for easy reference. 

Limitations:
The authors adequately addressed the limitation and potential negative societal impact on their work.



Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper analyzes policy mirror descent for solving regularized GMFG. The results provide new guarantees for learning GMFG without stringent oracle assumptions, and unlike some past works, it does not restrict the results to continuous time analysis. Furthermore, the paper provides an analysis of the case of function approximation. Experimental results are also presented for certain GMFG.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides convincing theoretical guarantees for the proposed algorithm. Table 1 is in general convincing of the theoretical contributions of the work. The analysed setting is novel and removes theoretical oracle assumptions in past work. Furthermore, the analysis is in discrete time (i.e., purely algorithmic) and not in continuous time dynamics.

The theoretical results are very clearly presented, and the assumptions are explicit. There is no ambiguity, and the proofs seem correct (although I might have missed details).

Furthermore, the incorporation of a function class in the analysis as opposed to oracle access is new in MFG/GMFG to the best of my knowledge.

Weaknesses:
While the provided result for offline RL-based approximation of value functions is interesting theoretically, it might be prohibitive in practice as the results stand.

Table 1 seems to indicate a large variety of assumptions employed in literature. It is not directly clear how the setting compares to other alternative settings, for instance, if related, a comparison of weak monotonicity with other definitions of monotonicity as well as contraction. This makes it difficult to compare the theory.

Experimental results are restricted to toy problems, however, it is possible that no alternative benchmarks exist for GMFG.

Limitations:
Potential additional limitations to be discussed were mentioned in the weaknesses and limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Intuitively, a ""graphon mean field game"" (GMFG) describes the large-$N$ limit of a game with $N$ players, where the payoff of a player $i$ depends on a weighted average of the states of other players $j\in [N]$. The graphon aspect comes from the fact that players have ""identities"" given by numbers $U_i\in [0,1]$, and the weights in the averages are of the form $W(U_i,U_j)/N$, where $W$ is a suitable function (if $W$ is constant, we have a regular mean field game MFG). GMFGs are potentially useful in multiagent reinforcement learning whenever agent interactions are not too strong. 

The present paper does not deal with the finite-$N$ problem, but rather with its continuous limit. One important point is that it considers regularized versions of GMFGs, with an added penalization term. The main results are as follows.

* Theorem 1 is a new result on the existence of equilibria for (potentially regularized) GMFGs. The main attraction of this result, in comparison with previous work, is that it only makes continuity assumptions on the reward function and transition probabilities (the function $W$ is still assumed Lipschitz). Moreover, the regularization had not been considered previously. Theorem 1 is obtained via a careful reduction to a non-graphon Mean Field Game, for which the authors also prove a new existence result (Theorem 2). 

* The paper then considers algorithms for approximating Nash equilibria for GMFGs that satisfy a monotonicity condition. Theorem 3 obtains a result under the existence of an ""action function oracle"". When that is not available, one can resort to function approximation: this leads to Theorem 4, which works under additional assumptions (and require the regularization). 

A small set of experiments suggests that the authors' method performs well in practice, and also that regularization is important to achieve good performance. 

   

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is original and significant. It is also quite clear. The following two points stand out. 

* The existence results work under weak conditions because of the clever use of ""soft"" techniques. 
* The algorithms work under relatively natural conditions.  

Weaknesses:
* It is not clear to me if the Lipschitz assumption on $W$ is really needed. 
* The proof of Theorem 4 is not particularly surprising. (I hesitate to call this a ""weakness"", but it is true that this part of the analysis is not too surprising.)
* In certain settings the user may be interested in the unregularized GMFG; however, Theorem 4 requires nonzero regularization. The paper does not provide bounds on how close a (near-)NE for the regularized case is to being a NE for the unregularized game.  



Limitations:
None were discussed, but I don't think there was any need to do so. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies regularized Graphon Mean-Field Games (GMFGs). They make two theoretical improvements over previous works on this topic:
* They prove existence of Nash equilibrium under weaker assumptions (e.g., weaker requirement on the continuity of the game) than previous works.
* For the special case of monotone regularized GMFGs, they give an mirror-descent algorithm that learns the unique Nash equilibrium. Compared to previous works, the novelty here is that the algorithm works for regularized games in discrete time (as opposed to unregularized games and continuous time).

In terms of techniques, their first result follows the proof plan of Cui and Koeppl, [2021] that reduces the problem to proving existence of Nash equilibrium for a subclass of GMFGs called MFGs. Their main technical contribution is proving an equilibrium existence result (Theorem 2) for MFGs under weaker assumptions using a different approach than previous works. Their second result essentially adapts the algorithm from Perolat et al. [2021] to their setting.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
This paper builds upon previous works and makes reasonable improvements. Most interestingly, the condition of their equilibrium existence result (Theorem 2) seems to be significantly weaker than previous works. The paper is well-written. They did a great job introducing the problem and the results and explaining the techniques and the difference from previous works.

Weaknesses:

As someone who is not closely following this line of work, it is hard for me to gauge the significance of the new equilibrium existence result, i.e., whether the weakened assumption is significantly more applicable than the assumptions made in previous works. The paper briefly mentions the assumptions in previous works are ``overly restrictive for real-world applications'' but did not provide any concrete example.

The algorithm for learning Nash equilibrium in their setting seems to be rather straightforward adaptation (with more careful analysis) from previous work of Perolat et al. [2021].

Limitations:
n/a

Rating:
6

Confidence:
3

";1
mWMJN0vbDF;"REVIEW 
Summary:
The paper proposes a novel framework for sign language translation (SLT), which can integrate multiple SLT subtasks. The work is motivated by a series of experimental analysis, e.g., converging speed of different subtasks and relationship between SLT and sign language recognition (SLR) performance. Besides, two constraints are proposed to improve the faithfulness of the model and ease the model training. The method achieves SOTA performance on two widely adopted SLT benchmarks using only keypoint inputs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper figures out two important problems: the lack of faithfulness in current SLT models and the inconsistent trend between SLR and SLT, which can inspire future works in this field.
2. The method is well motivated by a series of in-depth analysis in Figure 3.
3. The code-switching operation is interesting and novel, which can inspire future works on cross-modality modeling for SLT.
4. SOTA performance are achieved with a lightweight model using only keypoint inputs.


Weaknesses:
My major concerns come from method details and experiments.

Method:

1. The MLP and classifier in Figure 2 should be shared or not? In VAC [9], two different classifiers are appended to the visual and contextual module, while SMKD [40] uses a shared classifier, and the paper follows the design of SMKD. Intuitively, different classifiers should be used to project two features from different spaces into a common space. More discussion is needed for the discrepancy.
2. Gloss embeddings and mixup also appeared in a recent paper in the field of SLR [R1]. In [R1], the gloss embeddings are extracted by FastText, and the mixup is also achieved between visual and gloss embeddings. Some discussion or comparison should be added.
3. What is the motivation to fulfill code switch between the visual and gloss embeddings? Is it possible to use it between the contextual and gloss embeddings?

Experiments:

4. In Table 5, the sentence-wise code-switching does not consistently outperform the token-wise counterpart. The authors may explain why the sentence-wise one performs better when not using annealing and consistency.
5. As stated in line 299, logits are a closer representation of glosses. Also, [12] uses logits as the input for the translation module for CSL-Daily. Thus, it is not rigorous to conclude that adopting logits will degrade the performance since the ablation study is conducted on Phoenix14T.
6. The paper focuses on improving the faithfulness of SLT. But there are not objective metrics mentioned to measure the faithfulness. 

[R1] Natural Language-Assisted Sign Language Recognition, CVPR 2023.

Limitations:
The authors have discussed the limitations and societal impact adequately.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper mainly discusses the challenges of improving faithfulness in sign language translation and proposes solutions. The researchers leverage rich monolingual data and adopt back-translation to generate synthetic parallel data, explore the potential of denoising auto-encoder, and propose the MonoSLT framework to improve the accuracy of sign language translation. They also emphasize the importance of alignment and consistency constraints to align visual and linguistic embeddings and improve faithfulness. This paper has important reference value for improving faithfulness in sign language translation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Proposing a new unified framework, MonoSLT, which integrates subtasks of sign language translation into a single framework, allowing these subtasks to share acquired knowledge.

2. Proposing two constraints: alignment constraint and consistency constraint, which help improve the faithfulness of translation.

3. Experimental results show that the MonoSLT framework is competitive in improving the faithfulness of sign language translation and can increase the utilization of visual signals, especially when sign language vocabulary is imprecise.

Weaknesses:
The paper does not explicitly address the handling of non-manual signals and sign language morphological changes, which are crucial factors influencing the faithfulness of sign language translation. 

The experimental settings in the paper do not provide detailed explanations for many hyperparameters.

Limitations:
The proposed method lacks proper metrics to quantitatively evaluate the faithfulness of Sign Language Translation models and continues to use BLEU and ROUGE for evaluation. 

The paper's analysis and experimental results regarding faithfulness are not clearly defined.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper discusses the issue of faithfulness in sign language translation (SLT), which refers to whether the SLT model captures the correct visual signals. It is found that imprecise glosses and limited corpora can hinder faithfulness in SLT. To address this, the paper proposes MonoSLT, which integrates SLT subtasks into a single framework that can share knowledge among subtasks. Two kinds of constraints are proposed to improve faithfulness: the alignment constraint and the consistency constraint. Experimental results show that MonoSLT is competitive against previous SLT methods and can increase the utilization of visual signals, especially when glosses are imprecise.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method proposed in this paper outperforms multiple baseline methods, which is a promising contribution to sign language translation. 

Weaknesses:
1. There is no comparison with [12,15] on the bug-free dataset, which is a concern. Although I understand that reproducing [15] would require additional effort, since your code is based on MMTLB, it would be reasonable to verify the effectiveness of MMTLAB on the bug-free data.

2. The analysis of faithfulness and hallucination in the paper is not in-depth enough. There is no metric (either manual or automatic) to quantify faithfulness and hallucinations, and the improvement in BLEU is not sufficient to indicate that the faithfulness issue has been effectively addressed. The few cases presented in the paper are not enough to support the conclusions.

3. The analysis in section 3.2 is not thorough enough, and the conclusions are somewhat forced. For example, the statement that overfitting is caused by faithfulness is not well-supported, and the conclusion that there is no obvious negative correlation between SLT and SLR in Figure 3(c) is due to hallucination lacks data support and quantitative analysis. The few examples presented in section 4 are not sufficient to demonstrate the issue of hallucination.

Limitations:
see weakness

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work is dedicated to enabling the SLT model to capture correct visual signals (faithfulness in SLT). In order to improve faithfulness in SLT, the author integrates SLT subtasks into a single framework named MonoSLT, and based on this, proposes alignment constraints and consistency constraints. The former is used for aligning the visual and linguistic embeddings. The latter is used for integrating the advantages of subtasks. To demonstrate the effectiveness of the proposed method, the authors conduct experiments on two public datasets.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
[1 - complete layout and detailed description]. The article has a relatively complete overall layout and a detailed work description.

[2 – method novelty]. The author Introduced the code-switching phenomenon in the Alignment Constraint, mimicking the phenomenon of language alternation in conversations between multilinguals, and mixed visual embedding and gloss embedding as an input to the Translation Module.

[3 – the rationality of alignment]. Implicitly align visual and linguistic embeddings through shared translation modules and synthetic code-switching corpora. Better utilization of the characteristics of different subtasks.

[4 – method performance]. On the Phoenix14T dataset, the author's method only uses skeleton sequences as input, which improves performance (+2.2 BLUE-4) compared to the best method using RGB video as input.


Weaknesses:
[1 – Writing quality]. In section 3.2, some analysis is confusing, and the conclusion seems to be the author's subjective thoughts. And in the title of table 2, ‘the inconsistent punctation bug’ is confusing.

[2 - method performance on CLS-Daily]. On the CLS-Daily dataset, MonoSLT performs poorly, lagging behind several sota models.

[3 - Model evaluation issues]. The paper also mentions that although it alleviates the problem of faithfulness in SLT, there are no suitable metrics to measure it. The author still uses BLEU and ROUGE for evaluation


Limitations:
1.Find or create proper metrics to quantitatively evaluate the faithfulness of SLT models.

2.You said that the CLS-Daily dataset provides more precise gloss annotations, which leads to other models with lower SLR performance being able to achieve better SLT results This also leads to your model not performing as well as some models on the CLS-Daily dataset. As the sign language dataset becomes larger and more accurate, your model may not be as good as other models. I think this is worth considering.


Rating:
5

Confidence:
5

REVIEW 
Summary:
This work mainly studies the faithfulness issue in SLT (i.e., whether the SLT model captures correct visual signals). The study identifies imprecise glosses and limited corpora as the main factors contributing to limited faithfulness. In order to mitigate this issue, this work proposes a framework called MonoSLT, which leverages the shared monotonically aligned nature among SLT subtasks. This framework incorporates alignment and consistency constraints. Experiments demonstrates the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper is well-written and well-organized.

This work performs in-depth analysis on the previous works and the association among SLT-relevant tasks.

The overall performance is promising and shows notable performance gain over the baseline.

Weaknesses:
The main focus of this work is on the concept of faithfulness in spoken language translation (SLT). However, a notable limitation of the study is the absence of quantitative metrics to evaluate faithfulness. While the authors acknowledge this limitation in the paper's discussion of limitations, it remains a drawback. It would be beneficial for the authors to provide further clarification on this issue, perhaps by suggesting potential quantitative metrics that could be used to assess faithfulness in future research.

It is suggested that the proposed framework be compared with VAC, as they share similar components such as consistency loss and visual module constraints.

Regarding the discrepancy in length between the embeddings produced by the visual GCN module and the gloss module, it is essential to understand how the code-switching module handles this challenge. The authors should provide clarification or explanation on how the code-switching module addresses this issue.

Limitations:
It is better to design a suitable metric to evaluate faithfulness in SLT.

Rating:
5

Confidence:
5

";0
Eewh7sl0Xj;"REVIEW 
Summary:
The present paper offers a Toeplitz matrix architecture which can handle sequence modeling. The architecture comes in two flavors. The first flavor is a fast version that is most useful for bi-directional tasks. It speeds up previous Toeplitz networks by using an interpolation and low rank approximation scheme in its setup. The second is a Fourier based model that appears to offer advantages in causal tasks. Benefits are shown in terms of the speed of training and some marginal benefits in performance.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The main strength of this paper is a noticeable speed up in an alternative architecture to transformers. I am a fan of papers that look at ways to speed up sequence modeling. The Curren paper presents a nice idea. More specifically:
- The approximations to the TNN appear to be effective in experiments and result in a faster network
- Approximations do not seem to deteriorate performance and may offer some added performance boosts
- The changes to the architecture are grounded in some theory 


Weaknesses:

**Disclaimer**: I am not an NLP expert and am more focused on the theory side. I have significant concerns about the theory in this paper, but feel the experiments and techniques are solid enough to potentially overcome that issue. Furthermore, none of the theorems are crucial to the crux of the paper, and if any are wrong, they can be removed. For this reason, I placed a borderline accept rating for now, but I believe this will need to be confirmed by people who are closer to the experimental side of the literature and can assess the experiments in a more rigorous fashion. 

\
Broader comments:
- Most of my larger concerns revolve around the theory and proofs in this project listed below.
- Reading through many times, I could not understand what was gained in the “causal training” proposal in section 3.1. First, it seems the parameters are changed to live completely in the Fourier regime. This change was made to obtain “an alternate causal speedup"", but I don’t see where that speedup arises. Following the steps, the main change seems to take the algorithm to do an FFT on the $n$-dimensional space resulting in runtime of $O(n \log n)$ which is worse than before. Also, changing the algorithm to work in Fourier space introduces a different implicit bias that I’m not sure is desired. For example, the $\lambda$ parameter controlling the decay is not controlled here. I also have many questions about this approach which I’ve left below.
- Experiments in table 1 don’t appear to offer much improvement especially considering the added parameters. I would also ask the authors to include citations to the models or results compared to in this table so it is easier to see what is being compared to. 

\
Theory comments:
- Theorem about ReLU MLPs being $d$-piecewise linear has assumptions missing or is just wrong. If the authors are implying that any ReLU network from $\mathbb{R} \to \mathbb{R}^d$ has $d$ pieces or contiguous linear regions, this is clearly wrong. MLPs are universal approximators so this is clearly false. If the authors are saying that this only holds for an MLP with a single hidden layer of width at most $d$ then this may be correct. But I don’t see this assumption made anywhere.
- Theorem 2 has a few confusing elements from my end. First, it is hard to parse. There are many variables and factors like condition number that it is hard to know the scaling of. Second, the bound doesn’t appear to be all that good. The error grows at least linear in $n$ and depends on other factors like singular values or the nystrom error that may also be badly bounded. I suppose the authors would argue that it is exponentially small in the degree of interpolation $N$, but this degree would have to grow at least logarithmically in $n$ to counteract the $n$ factor. This would result in a runtime essentially equivalent to just doing FFTs on the whole space. Third, the error in interpolation is an unusual thing to even bound in my opinion. The weights are updated with this interpolation taken into account. In other words, the algorithm learns a weight matrix with parameters contained in this interpolation. 
- Definition 2 and 3 present the discrete time Fourier transform, but in practice only the DFT of the matrix form is ever used. The resulting statements, regardless of their correctness, do not seem to apply to the setting in practice. Unless I am missing something.
- Related to the above, I cannot see why Theorem 3 and 4 are correct. Similar to my previous statements, MLPs are universal approximations so they can output any possible function. How can that statement hold true? 

\
Small:
- Simply having a % label on the y axis of Fig 1B is confusing. Percentage relative to what? Also, what does 20% speed-up mean; i.e. that it ran in 20% less time? Simply having this number on the first page can be confusing without the context added.
- Fig 1A and 1B also appear to be different size fonts.
- Line 150: I think the dense-case runtime is only a factor of $r$ worse so $O(nr  + r \log r)$ and not $O(nr^2 + r \log r)$ unless I’m missing something.
- For someone outside of the NLP community, section 3.1 needed more motivation and formality. Some details about what “causal masking”, “causal kernel”, and the sequential nature of the data would be helpful.
- To follow easier, it would be good to define what the role of N is in section 4.1 (i.e., number of interpolating points)

\
Formatting:
- Hyperref links seem to be broken
- Line 189: sentence is a run-on and hard to follow

\
Finally, to add ideas not for criticism, but instead for completing the paper and offering new ideas, there is a wealth of literature on related techniques that could be useful here, or at the very least cited. For example, there are sparse Fourier transforms that can offer speed-ups beyond the $O(n \log n)$ that the paper aims to improve on, e.g., [1]. Since matrices are low-rank and/or sparse, this could be a more direct way to get the speed-ups desired. Second, there are a number of papers on optimizing structured matrices like Toeplitz matrices, e.g. [2]. In the sequence modeling specifically, there have been a lot of papers on unitary networks for example [3], one paper which actually uses low rank approximations in its implementation [4]. Third, low rank approximations have also been used to speed up other architectures like conv-nets, [5-6]


\
**References:** \
[1] Hassanieh, Haitham, et al. ""Simple and practical algorithm for sparse Fourier transform."" Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2012.\
[2] Kochurov, Max, Rasul Karimov, and Serge Kozlukov. ""Geoopt: Riemannian optimization in pytorch."" arXiv preprint arXiv:2005.02819 (2020).\
[3] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. ""Unitary evolution recurrent neural networks."" International conference on machine learning. PMLR, 2016.\
[4] Kiani, Bobak, et al. ""projUNN: efficient method for training deep networks with unitary matrices."" arXiv preprint arXiv:2203.05483 (2022).\
[5] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\
[6] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.



Limitations:
There is a very brief discussion of limitations in the conclusion, though I feel this could be expanded. I would also appreciate some context for this work in relation to other works in NLP and how it fits into the broader landscape of NLP architectures. For someone like me not in the community, this would be useful to better understand its limitations from a practical perspective.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors of the paper propose two modifications of a recently published alternative to attention mechanism, Toeplitz Neural Operator (TNO), which constitutes the most important part of Toeplitz Neural Networks. The application of TNO is the multiplication of the input sequence by a Toeplitz matrix. Parameters of this Toeplitz matrix are given by a lightweight feed-forward network, called Relative Position Encoder (RPE). The first proposed modification, called SKI-TNN, represents a learned Toeplitz matrix as a sum of a sparse and low-rank matrix. Thus, its multiplication by a vector has the complexity of O(nr^2 + r log r) instead of O(n log n), where r is the rank of a second summand. However, this modification can speed up only the task of bidirectional modeling. In order to speed up the causal modeling (such as autoregressive language modeling), authors view the TNO as an application of kernel to a vector. Then, they train the RPE to model the real part of the Fourier transform of this kernel. The imaginary part is then computed via the Hilbert transform of the real part. This modification does not change the asymptotic complexity, but achieves empirical speed up.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The article explores an important topic of speeding up the token mixing part of a general sequence modeling pipeline. Nowadays, this topic is highly relevant because of its applications in the field of NLP. The article builds off of a very recent paper [1].
2. The article creatively combines together a large body of previous work. It uses the ideas of TNN, SKI, FFT, Hilbert transform, Nyström approximation, fast causal masking, etc. 
3. The experimental results show that the proposed modifications do indeed speed up the original TNN.
4. Overall, the presentation style is mathematically strict and to the point. The formulae in section 3.2.1 and in Appendix are sufficiently well-explained. Both modifications proposed in the paper are succinctly defined in Algorithm 1 and Algorithm 2. This helps the reader significantly to understand the main ideas.
5. In section 3.2.1, the authors specified not only the theoretical complexity of their modification, but also the practical limitations they meet when implementing it, and specified the practical complexity as well as theoretical.
6. The theory on the smoothness in Fourier Domain is supported with experimental visualizations
[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023.


Weaknesses:
1. The main claim of the article is the speedup achieved by the proposed modifications. The only results supporting this claim are Fig. 1 and some percentages in the text (in section 5.1). Fig. 1 shows the performance on specific tasks from the LRA benchmark. Firstly, it is not clear for which task the baseline (TNN)  is evaluated. Secondly, the choice of the tasks shown on the graph is questionable. The hardest task from the LRA benchmark, Pathfinder-X, is not shown. As for the speedups mentioned in the text: it would be better to put them all into a separate table. Moreover, it would be interesting to see a speed comparison in the form of a table similar to Table 5 from the TNN article.
2. In section 4.2, theoretical results on the choice of activations are presented. Several possible improvement ideas. The ablation study with experimental results for different activation types would be of interest. Moreover, the graphs showing the decay rate for randomly initialized networks might be improved. It would be better to leave only the lowest and highest lines and show the average line in between. Also, if you compare the rate of convergence to some baseline rate (e.g. exponential), plotting it would be appropriate. In addition, it seems to be not quite fair to compare the decay rates of trained and untrained networks.
3. While the overall presentation style is to the point, as mentioned earlier, it would help the reader if the abstract, introduction and related work were more general. Both abstract and introduction may be hard to read for an unprepared reader, as they contain too much mathematical details and not enough motivation. Moreover, some paragraphs of the introduction repeat the abstract almost word for word, while rephrasing would help the reader to understand the ideas more deeply. The related work section should describe either the ideas of mentioned papers or their connection to your paper more clearly.
4. The section 3.2.2 is a bit obscure. “Inverse time warp” is not a common term, and it is not described in the section.


Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents several techniques to speed up Toeplitz neural networks (TNNs). In particular, TNNs use convolution of length n (sequence length of the input) and so scales as O(n log n), and TNNs have many calls to the MLP that generate relative positional encoding (RPE) and decay bias. To reduce the time of convolution, for bi-directional modeling the paper proposes to approximate the Toeplitz matrix as a sum of a short convolution and a low-rank matrix, which results in O(n + r log r) complexity where r is the rank of the approximation. For uni-directional model (e.g. auto-regressive modeling), the paper proposes to parameterize the convolution directly in frequency domain and uses the Hilbert transform to obtain the imaginary part from the real part of the filter to ensure causality. The approximation error is then analyzed. Validation on language model (Wikitext-103) and long-range benchmark (LRA) show that the approximation lead to some speedup (10-15%) and the quality stays around the same.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The idea of using asymmetric Nystrom to approximate the Toeplitz matrix is quite clever. This allows a decomposition into a sparse and a low-rank component, which leads to asymptotically faster algorithm in the case of bi-directional modeling.

2. While uni-directional modeling prevents the Nystrom technique due to causal masking, parameterizing the filters directly in the frequency domain is able to overcome this challenge. While this is not asymptotically faster, it avoids one inverse FFT per layer and leads to some speedup.

Weaknesses:
1. Unclear what problem the paper is trying to address, and how it is motivated.
The intro starts out with Toeplitz neural networks, and the paper aims to make it faster. However, it's not clear why we want to make these faster, and what we would enable if we make these faster. Are they being used in very large-scale tasks? Are they being scaled to very long sequences?
While the technical contributions are solid, it's not clear to me why the paper chose to tackle this problem.

2. Unclear what the technical challenges are. 
- The paper mention that they want to avoid O(n log n) computation. But in practice O(n log n) isn't very slow, especially on GPUs. FFTs are pretty much bounded by memory bandwidth, and they take only 2-3 times as long as any pointwise operation. So if the goal is to speed up TNNs, then it makes more sense to have an efficient implementation, rather that using algorithms that faster asymptotically (O(n + r log r)) but is slower than a hardware-friendly algorithm (line 150, where using matmul with O(n r^2 + r log r) is faster). 
- The paper mentioned ""many calls to the RPE"". Why is this a problem? Showing a profile of how much each operation is taking will be much more convincing. That would motivate the approaches in the paper much better.
Without knowing how long each operations in TNNs are taking, how do we know that we're solving the right problem?

3. Lack of detailed speed benchmark. Given the goal is to speed up TNNs, I would have expected one of the main results to be speed benchmarks, across different sequence lengths, on different devices, to show the tradeoff. In the main paper, speed is only reported in Figure 1b, which is end-to-end speed for a particular sequence length (512).
How do we know that we're close to the maximum speed on these devices (GPU)? Or are we still far from optimal? When we speed up convolution and RPE, what is the remaining bottlenecks.
Having these would make the paper stronger.

At sequence length 512, an optimized implementation of attention (e.g. FlashAttention) is likely faster than FFT and the method in this paper. This is my impression as the Hyena paper [1] reports that TNNs are not faster than FlashAttention until sequence length > 4k.

[1] Hyena Hierarchy: Towards Larger Convolutional Language Models. Poli et al. 2023.

Limitations:
Not necessary.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes to reduce the computational complexity of Toeplitz neural networks. TNNs are a new form of network for sequence modeling that reduces space complexity of the attention matrix to allow for longer sequences.TNN model consists of a stack of Gated Toeplitz Units (GTU) that includes TNO (Toeplitz Neural Operator) that does token mixing with relative positioning. Then, GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO). 
The paper addresses the TNNs efficiency limitations: 1) super-linear computational complexity 2) many calls to the RPE: for each layer, one call per relative position. Thus, the paper proposes to reduce both the complexity of sequence modeling and of the relative positional encoder. The work proposes solutions by means of both the Structured Kernel Interpolation (SKI) [2] and working with frequency domains.
It does so through:
–	Approximating Toeplitz matrix using low-rank approximation and replacing the RPE MLP with linear interpolation and using Structured Kernel Interpolation.
(for O(n) complexity, that is use linear interpolation over a small set of inducing points to avoid the MLP entirely   -using an inverse time warp to handle extrapolation to time points not observed during training)
–	Causal training, SKI does not bring benefits, so instead they eliminate explicit decay bias by working in the frequency domain, using Hilbert transform (to force causality) and also use some smoothness
–	 For the bidirectional case, they eliminate the FFT applied to the kernels.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The work includes a number of solutions to improve TNN speed-up (addressing RPE MLP, the FFT, and the decay bias).

RPE is a neural network to obtain relative position embedding to obtain entries in Toeplitz matrices. These entries could be evaluated with stationary non-SPD kernel which is a good idea.

So first decomposing Toeplitz matrix and then using interpolation for MLP is a comprehensive pipeline.

The theory part makes the arguments more sound, and the explanations in supplementary materials are fairly abundant.

The experiments on LRA show good predictive performance on long range data and on wikitext some speed-ups.


Weaknesses:
The major paper of the paper talks about the SKI to accelerate the TNNs but in experiments SKI is only shown in the LRA experiment and does worse than both TNN and FD-TNN

As mentioned in the paper, doing sparse-dense multiplication in practice can be slower than dense-dense matrix multiplication (but that is only part of the potential speed-up)


Limitations:
Yes.

Rating:
5

Confidence:
3

";0
3b5e2AFs7f;"REVIEW 
Summary:
The paper proposes and studies a notion of feature attribution in which features are scored for a given instance according to the proportion of minimal explanations for that instance in which they participate. Although an exact computation of this scores can be computationally unfeasible, the paper exploits the minimal-hitting-set duality between abductive and contrastive explanations to approximate them efficiently. Finally, they study their approach empirically over several datasets.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
- The paper is really well written and easy to follow; presentation is excellent
- The paper does a good job at showcasing the importance of the problem and providing references to issues that are present in SHAP 
- The proposed metric is simple and seems promising
- The experiments have good results and are presented nicely
- The sections on limitations and conclusions provide a nice and helpful starting point for further discussion
- References seem appropriate and detailed

Weaknesses:
- The paper does not provide proofs in the supplementary material for its propositions. While proposition 1 is self-explanatory, proposition 2 is not and should be accompanied with a proof.

- Despite mentioning the weighted variant (Definition 2),  the paper doesn't seem to do anything with it; there are no theoretical nor practical results about it as far as I can see, and it is only discussed in the appendix. I understand that due to the page limit not everything can fit in the paper, but including the definition of WFFA without doing anything with it in the paper seems like a poor choice to me.

- Even though the paper is about formal explainability, and the scores themselves are formally defined, the approximation algorithm doesn't seem to have any formal guarantee, and thus it is unclear to me what the interpretation of the results should be. It seems to me that further theoretical studies are required; how do we know that their approach can't fall into cases where it gives answers that are arbitrarily far from the ground truth? The starting point of the paper is about how methods such as SHAP have pitfalls when certain conditions are met, and yet it is not clear at all that this approach is exempt of similar (or even worse) potential problems.

- The experimental data is, unless I am missing something, a bit strange; LIME, SHAP and their FFA approximation compute different things, both in theory and practice, and thus I don't understand at all what the meaning of their comparison is; LIME and SHAP are not approximations to the FFA score, and thus it doesn't feel sound or fair to use them as such. Their idea of taking absolute values and normalizing might make sense, but this is far from obvious and limits how convincing their results are.

Limitations:
Yes; there doesn't seem to be much to address and the authors discuss appropriate points in section 6.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach to XAI called Formal Feature Attribution. The authors address the limitations of existing model-agnostic methods and formal XAI approaches by proposing FFA as a solution for feature attribution.
The FFA method leverages formal explanation enumeration to define feature attribution as the proportion of explanations in which a specific feature occurs. The paper highlights the challenges in computing exact FFA but presents an efficient approximation technique using a dual trait of such explanations.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper exhibits a high level of clarity and coherence in its writing style, making it easily understandable for readers. The claims and contributions are clearly stated, enabling the reader to grasp the main objectives of the research.

The paper makes a noteworthy contribution to the field of Explainable AI by introducing a novel approach. This fills a gap in the existing XAI landscape, where formal foundations are relatively loose, and offers a promising solution for feature attribution.

The authors demonstrate extensive work in various aspects of the research. They have invested effort in both the theoretical aspects, establishing formal foundations for FFA, as well as in the practical aspects, conducting experiments to validate the proposed approach. This comprehensive approach enhances the credibility and reliability of the findings.

Weaknesses:
Axiomatic Analysis of FFA: While Lime and Shap have established a set of axioms for explanations, it remains unclear which set of axioms the FFA explanations adhere to. It would be beneficial to explore and define the axioms underlying FFA explanations or engage in theoretical debates surrounding them. For example, I think that duplicate features increase the importance of other features that exists in a common AX'p. Using a specific example, think of a scenario where two correlated features, f1 and f2, are considered. In this case, a decision tree is constructed where f1 appears at a certain node while f2 is absent. Now, create a separate decision tree that includes the condition for either f1 or f2 to appear at that node, resulting in identical predictions due to their correlation. However, despite the consistent predictions, I'm pretty sure that the two models provide different explanations for the features, questioning whether the ratio of feature occurrence in formal explanations increases equally in the numerator and denominator. 

Approximation Guarantees for FFA: Since FFA is an NP problem, it necessitates further consideration regarding the guarantees provided by approximation techniques. Investigating the quality and limitations of these approximations would strengthen the practical utility of FFA.

The absence of code implementation limits the reproducibility and practical adoption of the proposed FFA approach.

Addressing these aspects would further enhance the theoretical and practical implications of FFA within the field of Explainable AI.

Limitations:
Distinguishing Out-of-Distribution and Manifold Sampling: The paper does not explicitly discuss the distinctions between out-of-distribution sampling and manifold sampling, despite their potential significance in this context. Exploring the differences and implications of these sampling techniques would contribute to a more comprehensive understanding of FFA.

Addressing Local Explanations: The paper briefly touches on the issue of local explanations, but does not delve into their meanings, differences, or implications. Future research could focus on formulating FFA specifically for global explanations, shedding light on the disparities between local and global explanations within the context of FFA.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose formal feature attributions, a novel type of local feature attribution method for
explaining the predictions of black box models.  Their approach builds on the notion of abductive explanations (AXp's), a type of minimal sufficient subsets.  One issue with AXps is that there are a potentially exponential number of them, most of which fall outside of the data distribution.  The authors propose - essentially - to summarize the set of AXps via averaging into a per-feature relevance score akin to those provided by LIME and SHAP.  An inverse proposeity weighted variant is also introduced.  The authors then show that typically computing FFAs is computationally intractable
and propose an approximation algorithm for quickly estimating approximate FFAs.  This approximation is shown to work well on two MNIST-like data sets.

**Post-rebuttal update**: increased score, see the discussion.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ Very clearly written, a pleasure to read.
+ Ideas are clearly presented, with a couple of exceptions (see questions below).
+ Related work is well done.
+ FFAs are rooted on a simple and clear concept.
+ Algorithm is sensible.
+ Good empirical performance on a relatively varied set of data sets (for boosted DTs only).
+ Some essential limitations are clearly discussed, but see below.

Weaknesses:
- The authors seem to assume FFAs are the ""real feature attribution"", but provide no real motivation for this (see Q1, this is the big one).
- Unclear reasons why OOD sampling is an issue and how FFAs deal with it (Q2).
- Missing discussion of information loss due to averaging over AXp's (Q3).
- Experiments consider boosted DTs only (relatively minor).
- Missing evaluation of approximate FFA algorithm on non-MNIST data (minor).

Limitations:
The paper has an explicit limitations section which is quite well done.  I have outlined a couple of other possible limitations in my questions.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a new approach called formal feature attribution (FFA), inspired by successful FXAI methods, to compute feature attribution scores. FFA is defined as the proportion of explanations where a feature occurs. Experiments try to demonstrate the effectiveness of FFA compared to SHAP and LIME under several public datasets. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors proposed a new perspective on providing feature attributions. They did point out some limitations of existing popular methods.
2. The proposed FFA is straightforward and with clear motivations. 


Weaknesses:
1. Important questions regarding the reasonableness of the proposed method are unanswered. For instance, the three advantage properties of FFA claimed in this paper are not well reasoned. 
2. The experiments provided in this work are hard to support FFA is better than other existing XAI scoring methods. 
3. Several definitions and annotations throughout the text lack proper illustration or clarification, which can hinder the reader's understanding. Without clear explanations, it becomes challenging to grasp the intended meanings and implications of these terms and annotations.


Limitations:
1. The proposed FFA only focuses on the classification task, which makes it limited to be applied in several other common tasks, such as the regression task. 
2. The paper lacks clear illustrations of experiment settings and does not compare with relevant advancements, leading to unconvincing results for verifying the proposed hypothesis. More detailed descriptions of the experiment settings and comparative analyses with related advancements are necessary to strengthen the study's validity.
3. Several annotations and definitions are not clearly illustrated. For example, the “right arrow” in Equation 1 and the definition of “formal explanation” is not well-illustrated in this work. This makes the work hard to follow. 


Rating:
5

Confidence:
4

";0
m2getD1hpk;"REVIEW 
Summary:
The authors present a transformer model for forecasting and anomaly detection. The model performs both forecasting and reconstruction in the frequency domain with a fraction of parameters compared to state-of-the-art transformers. The model demonstrates impressive performance on anomaly detection and forecasting tasks.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This is an interesting work and novel to the best of my knowledge. 
2. I strongly believe that the lightweight nature of the model is an asset. 
3. The model demonstrates near state-of-the-art performance. 
4. The paper is well-written. 

Weaknesses:
Major: Most of these are not deal breakers, but the following would make the evaluation of the model stronger:
1. Baselines for forecasting : The authors compare with only transformer based models. I would encourage the authors to compare with advanced non-transformer based models such as N-HiTS [1] and N-BEATS [2]. 
2. Handling of multi-variate time-series data (see Q1)
3. Baselines for anomaly detection: I would again encourage the authors to compare with state-of-the-art time-series anomaly detection e.g., DGHL [3]. 
4. Evaluation for anomaly detection detection: I would encourage the authors to use standard evaluation metrics for anomaly detection like adjusted best F1 [3, 4], Average Precision [4, 5], and Volume Under Surface (VUS) [6]. The datasets used also have known flaws [7]. 

Minor:
1. ""To avoid information leakage, We choose"" --> ""To avoid information leakage, we choose""

References:

1. Challu, Cristian, et al. ""NHITS: Neural Hierarchical Interpolation for Time Series Forecasting."" Proceedings of the AAAI Conference on Artifi
2. Oreshkin, Boris N., et al. ""N-BEATS: Neural basis expansion analysis for interpretable time series forecasting."" arXiv preprint arXiv:1905.10437 (2019).
3. Challu, Cristian I., et al. ""Deep generative model with hierarchical latent factors for time series anomaly detection."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
4. Goswami, Mononito, et al. ""Unsupervised model selection for time-series anomaly detection."" arXiv preprint arXiv:2210.01078 (2022).
5. Schmidl, Sebastian, Phillip Wenig, and Thorsten Papenbrock. ""Anomaly detection in time series: a comprehensive evaluation."" Proceedings of the VLDB Endowment 15.9 (2022): 1779-1797.
6. Paparrizos, John, et al. ""Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection."" Proceedings of the VLDB Endowment 15.11 (2022): 2774-2787.
7. Wu, Renjie, and Eamonn Keogh. ""Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress."" IEEE Transactions on Knowledge and Data Engineering (2021).

Limitations:
The authors discuss some limitations of your work. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors introduce FITS, a lightweight model for time series analysis. Unlike existing models that process raw time-domain data directly, FITS operates on the principle of manipulating time series through interpolation in the complex frequency domain. By discarding high-frequency components that have a negligible impact on the time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks. Additionally, FITS has a remarkably compact size, consisting of only approximately 10k parameters. This lightweight model is mainly constituted by a simple Complex-valued Linear Layer functioning on frequency domain.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- About the presentation: Straight-forward to follow, simple and direct sentences.
- About the contribution: The proposed FITS framework is very light-weighted, since its learning parameters mainly coming from 1 dense layer. This contribution make its great application for actual real-world scenarios, applying on edge devices. The natural idea about *Low Pass Filter* is not new, but is used with good reasoning.
- About the experiment: Authors illustrate the effectivenesses of FITS with two main time series tasks: Forecasting and detecting anomaly. While these experiments are not extensive, it effectively support the contributions the authors claim: Comparative performance and much more lightweight compared to existing SOTAs.

Weaknesses:
- About the presentation: Figure 1 has some too dim colors and hard to see; Figure 2 is also quite small, the font size should be increased.
- About the contribution:
    - Details about input or output dimensions are not discussed (only discuss about the temporal axis). It is not clear whether the framework can be applied to multivariate series? How to choose cutoff frequency in case of multivariate series, when the harmonics are likely to be different over different variables?
    - While the different components constituting FITS are used with clear intentions, these techniques or algorithms (rFFT, RIN, low-pass filter, …) are not new and even well established.
    - The experiments for both time series and anomaly detection tasks suggest the framework has a great variance when evaluating on different datasets. In general, FITS cannot achieve SOTA performance in many scenarios, which may be unsuitable for performance-critical applications.

Limitations:
Authors recognize following weaknesses:
- FITS struggles with binary-valued time series and time series with missing data.
- Binary-valued time series are better suited for time-domain modeling due to their compact raw data format.
- For time series with missing data, a two-step approach is suggested: apply simple time-domain imputation techniques before utilizing FITS for analysis.

The authors should consider amend the weaknesses mentioned above if possible or make some modifications to the manuscripts to rebuttal the comments.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper builds a model for time series learning in the frequency domain. The key idea is to discard high-frequency components to reduce the model size. This leads to a simple model under 10k parameters, 50x smaller than DLinear. Experimental results suggest the model can achieve comparable performance to the state-of-the-art methods on long-term forecasting tasks and several anomaly detection tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Transforming time series into a frequency domain and then training a model is an interesting idea.
2. The model is extremely small, even 50X smaller than DLinear, which is already very small.
3. The experiments on long-term forecasting are strong.

Weaknesses:
1. It is unclear whether the approach has practical value. DLinear could be already sufficiently small to fit in a normal edge device. Thus, it is not persuading to have an even smaller model.
2. Training efficiency is not reported.
3. The experiments on anomaly detection are not strong. Firstly, all the baselines are neural networks. However, traditional methods like OCSVM and IForest can have strong performance [1]. Secondly, the existing anomaly detection datasets could be flawed. Thus, I am particularly interested to know how the proposed method performs in the synthetic data provided in [1]. The pattern-wise outliers in [1] are synthesized by modifying the sinusoidal waves. Thus, the proposed method seems to well align with the design of this dataset. I am curious how it performs on this dataset.

[1] Revisiting Time Series Outlier Detection: Definitions and Benchmarks

Limitations:
Yes

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposed a novel method on time-series forecasting and anomaly detection. The solution relies on the frequency-domain feature of the given time series and proposes to use a linear model to interpolate the data in the frequency domain. After that, the model utilizes the inverse FFT operation to transform the frequency domain data into time domain. The interpolated time series is longer than the original one. Thus, the forecasting has been conducted based on the augmented time series sequence.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea is novel. The interpolation of the time series in the frequency domain provides some insights for this community.

2. The model efficiency is quite amazing, only 10K parameters have provided the comparable even better performance for the time-series prediction task.

Weaknesses:
1. The experiments are not promising. What's the number of the random runs and did you control the random seeds? I suggest to report the mean and std values of the results.

2. As this is the new architecture of the time-series model, more analysis should be conducted such as more prediction tasks, more ablation studies for the key components of the method, etc.

3. The reason behind the effectiveness of the proposed method remains unclear to me. 

Limitations:
See those in weakness part. 

Moreover, the augmented time series is not appropriate for the forecasting tasks. Forecasting is the extension for one given time series, however, simply interpolating for augmenting the original time series is not intuitively correct, because it has modified the original data piece rather than prediction upon the given information (in time domain).
And the experimental results show that the performance is not comparable to the baseline methods such as PatchTST. I suggest the authors to further improve the method and figure out the reason of its effectiveness.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes an impressive compact model, named FITS, for time series tasks, including forecasting and anomaly detection. FITS achieves manipulation to time series through interpolation in the frequency domain. The whole framework is quite simple and has remarkably few parameters. FITS achieves competitive performance to SOTA baselines on both forecasting and anomaly detection with about 50 times fewer parameters. With such impressive performance, the proposed model would have a certain impact on the community.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of implementing time series forecasting and anomaly detection through interpolation in frequency domain is interesting and technically sound.
2. Detailed designs, including LPF and utilization of RevIN, are well motivated and described, making the whole framework reasonable and easy to follow.
3. The proposed model achieves impressive performance on both forecasting and anomaly detection with a remarkably compact size.

Weaknesses:
1. Lack of time consumption analysis. Due to rFFT and irFFT in the model, time efficiency is the main concern. 
2. The coverage of related works is barely satisfactory. Adding some preliminary about manipulation in frequency domain to the manuscript would be helpful to understand the model.
3. Typos. For example, duplicated citations in line 383 and line 385.

Limitations:
Not applicable.

Rating:
6

Confidence:
4

";0
IL7F4soYyg;"REVIEW 
Summary:
This paper introduces a new pre-training strategy that takes cells as tokens and tissues as sentences. It also encodes cell-cell relations by leveraging the spatial information of cells acquired from spatially-resolved transcriptomic data. The proposed method achieves state-of-the-art performance in various downstream tasks.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- It's a good insight and more biologically meaningful to take cells as tokens and tissues as sentences instead of using genes as tokens and cells as sentences, as there's no sequential relationship among the order of genes.
- It's also an interesting idea to utilize SRT data to encode spatial information as the input to the transformer.
- Good reproducibility: the authors have provided code. 
- The paper is overall well-written and conveys the idea clearly.

Weaknesses:
- Though the model is pretrained, full finetuning is required when deploying to the downstream tasks. This is somehow in contrast to the idea of training a pre-trained model that can be easily adapted to downstream tasks by only tuning the task-specific layer. The drawback is 1) it requires a lot of computing resources and data, and 2) we don't know if the performance (i.e. representation learning ability) is obtained from the pretraining or just fine-tuning. 
- Following the above problem, the author should compare the performance w/ and w/o pretraining, and also the performance of fine-tuning the task-specific layer, to ablate whether the pretraining helps.
- The authors evaluated tasks like denoising, imputation, and perturbation prediction, but why not the most common and important task like cell-type classification? I think it's the most straightforward way to benchmark the performance of the proposed method.


Limitations:
Overall I find the idea interesting, but the descriptions of the method are not very clear and the experimental evaluations are not strong enough to support the claim.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed a pre-trained model based on single-cell data motivated by the special characteristics (i.e., bag of genes structure, cell-cell relation, noisy) in cell-data, which viewed cells as tokens and tissues as sentences. This is different from the most existing pre-trained models treat genes as tokens and cells as sentences. The paper designed a new training paradigm based on transformer while introducing Gaussian mixture prior to handle data limitation.

Experiments over some representative down-stream tasks are conducted by fine-tuning the pretrained models to demonstrate the effectiveness of the proposed training paradigm.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.pre-trained model over cell-data which considers the cell-to-cell relation and bag-of-genes structure and introduces Gaussian mixture prior distributions to handle noisy and limited data.

2.Extensive experiments were conducted over different down-stream tasks (including both cell-level and gene-level tasks) to demonstrate the effectiveness of the proposed pre-trained models.

Weaknesses:
The training procedure is inspired from masked language model, however, as stated in this paper, cell/tissues are different from text sequence, the technical details of pre-trained data process are not clear enough. This is very important due to that it is helpful to a) understand the characteristics of cell-data and b) repeat the experiments and utilize more related data to further enhance the performance of pretrained models.

Please refer to questions for details.




Limitations:
The setting of pre-training should be clear stated.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a scRNA-seq model that takes into account correlations between gene expression values in nearby cells in a tissue. The proposed model represents cells as tokens with a very simple embedding model. The tissue representation is modelled using self-attention between cells. The model is trained with a probabilistic objective where the transformer outputs represent the latents with a mixture distribution and then an MLP decoder is used to decode the expression values for genes in the cell from the latent and a batch encoding.

The model is pretrained using 9M cells and 2M spatially resolved transcriptomics cells. The model is tested on 3 tasks scRNA denoising, spatial transcriptomic imputation and perturbation prediction. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* Understanding cell-cell communication is a very interesting open problem.
* Task 2 zero-shot performance is on par with finetuning which seems promising. 


Weaknesses:
* I have doubts about the scRNA denoising results and how meaningful the comparison to single cell imputation methods. (see question 1 below).

Limitations:
none

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper presents a study on the problem of enhancing cell representation learning through pretraining. Inspired by the recent success of pretrained language models, the researchers explore the use of Transformer architecture to achieve improved cell representations.

In contrast to previous approaches that only consider modeling a single cell, the authors propose a novel model called CellPLM that takes into account a sequence of cells, enabling the capturing of inter-cell relationships more effectively.

To incorporate spatial relationships into the model, 2D position embeddings are introduced to enhance the basic Transformer architecture. Additionally, a mixture of Gaussian distribution is employed to model the latent embedding space, enabling the capturing of cell group information and mitigating batch effects.

The proposed CellPLM model is first pretrained using a combination of scRNA-seq cells and SRT cells. Subsequently, three downstream tasks are conducted to evaluate the effectiveness of the pretrained model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The representations and foundation models of genes and cells proposed in this study have the potential to be highly interesting and useful in advancing AI applications in the field of science (AI for Science).

The model architecture and adaptations made from the vanilla Transformer model are sensible and allow for efficient modeling of cells. In particular, the joint modeling of cell groups convincingly addresses the issue of data sparsity at the gene level.

The empirical design chosen to validate the pretrained CellPLM using three different use cases is reasonable and provides strong evidence of its effectiveness. Whether in a zero-shot or fine-tuned setting, the proposed CellPLM consistently performs well. Additionally, the results of ablation experiments further demonstrate the significance of each component designed in the model.

Weaknesses:
One weakness of this paper is that most of the datasets and tasks considered in the experiments are focused on single-cell analysis. Although the paper introduces a novel aspect by jointly modeling cell groups and sequences, it would be valuable to observe how the proposed model performs on tasks that involve multi-cell information.

Additionally, it is commendable that the author provides both the mean and standard deviation of the runs. However, to further support the significance of the improvements, it would be more informative to conduct statistical significance tests on the small downstream datasets, in order to validate whether the observed improvements are statistically significant.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The CellPLM paper presents a novel pre-trained model that encodes cell-cell relationships in spatially-resolved transcriptomic data. The authors demonstrate that their model outperforms existing state-of-the-art methods in various downstream tasks, including cell type classification, cell-cell interaction prediction, and spatial gene expression imputation. 

The paper is well-written and provides a clear motivation for the need to develop a pre-trained model that can capture cell-cell relationships in spatially-resolved transcriptomic data. The authors also provide a detailed description of the architecture and training procedure of their model, which is helpful for readers who are interested in implementing the model in their own research.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. In terms of originality, the CellPLM paper presents a new approach for encoding cell-cell relationships in spatially-resolved transcriptomic data. The authors leverage a pre-trained transformer framework that encodes inter-cell relations and adopts a reasonable prior distribution. 
2. In terms of clarity, the CellPLM paper is well-organized and easy to follow. The authors provide a clear motivation for the need to develop a pre-trained model that can capture cell-cell relationships in spatially-resolved transcriptomic data and provide a detailed description of the model's architecture and training procedure. The authors also provide a clear evaluation of the model's performance on various downstream tasks, which is helpful for readers who are interested in implementing the model in their research.

Weaknesses:
1. It is unclear to me why to use a Gaussian Mixture Latent Space to capture the information of distinct functional groups of cells. First, I do not understand the distinct functional groups of cells, which have not been explained clearly. Moreover, it seems that you can directly decode the masked cells like BERT without complicated latent space estimation. From Figure 4, the ablation study show that removing this part almost does not hurt the performance. 
2. Not the weakness (just suggestion), please use a clearer figure or vectorgraph (e.g., Figure 2, 4).

Limitations:
As mentioned in the discussion, the work does not compare with other pre-trained models, which may be very important.

Rating:
6

Confidence:
3

";0
OAOt75zsdP;"REVIEW 
Summary:
The paper demonstrates a root cause of the over-pessimism issue of existing distributionally robust optimization (DRO) methods: excessive focus on noisy samples. To mitigate this issue, the authors proposed a novel DRO method called Geometrically-Calibrated DRO (GCDRO). They introduce the free energy implications of their method (Section 3.1) and approximate optimization method (Section 3.2). Finally, they empirically validate their method for both simulation data (Section 4.1) and real-world data (Section 4.2).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- While this paper is a theoretical work (for the ML community), I believe that the problems addressed in this paper can be applied in practical situations. Specifically, existing debiasing methods [1, 2] focus on minor group samples with loss/gradient information. Similar to existing DRO methods (as the paper shows), these debiasing methods will focus excessively on noisy samples. In this case, GCDRO (or a more computationally scalable approximation version) can be applied to mitigate this issue.
    - [1] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" *Advances in Neural Information Processing Systems* 33 (2020): 20673-20684.
    - [2] Ahn, Sumyeong, Seongyoon Kim, and Se-young Yun. ""Mitigating Dataset Bias by Using Per-sample Gradient."" *arXiv preprint arXiv:2205.15704* (2022).
- The free energy perspective of DRO using duality seems novel and interesting.

Weaknesses:
- The organization of this paper is somewhat puzzling and hard, although it deals with theoretical topics that are difficult for non-experts to understand. For example, it would be recommended to highlight the key difference between DRO and ERM in Section 2. As ERM does not assume graph structure G_N, readers unfamiliar with DRO would expect a detailed explanation/usage of the graph in L74.
- It would be recommended to add a confidence interval for the results in Figure 2. Also, a vector image format provides more clear results as it does not blur when zooming in.

Limitations:
The authors did not discuss the limitations of their method.

Rating:
8

Confidence:
3

REVIEW 
Summary:
In this work, the authors propose a novel approach called Geometry-Calibrated Distributionally Robust Optimization (GCDRO) to address the over-pessimism issue in traditional Distributionally Robust Optimization (DRO) methods. DRO aims to optimize worst-case risk within an uncertainty set to mitigate the effects of distributional shifts in machine learning algorithms. However, DRO often leads to low-confidence predictions, poor parameter estimations, and limited generalization.

The authors analyze a possible cause of over-pessimism in DRO, which is the excessive focus on noisy samples. To mitigate the impact of noise, they incorporate data geometry into calibration terms in DRO, resulting in GCDRO specifically designed for regression tasks. They demonstrate that their risk objective aligns with the concept of Helmholtz free energy in statistical physics, and this free-energy-based risk can be extended to standard DRO methods.

To optimize the GCDRO objective, the authors leverage gradient flow in the Wasserstein space and develop an approximate minimax optimization algorithm. This algorithm guarantees a bounded error ratio and standard convergence rate. The authors further explain how their approach alleviates the effects of noisy samples in the optimization process.

To validate the effectiveness of GCDRO, the authors conduct comprehensive experiments. These experiments demonstrate that GCDRO outperforms conventional DRO methods in terms of prediction accuracy, parameter estimation, and generalization performance.

Overall, the proposed GCDRO approach addresses the over-pessimism issue in DRO by incorporating data geometry into the calibration terms. The authors provide a theoretical analysis and empirical evidence to support the superiority of GCDRO over conventional DRO methods, showing its potential for improving the robustness and performance of machine learning algorithms in the face of distributional shifts and noisy samples.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Novel Approach: The paper introduces a novel approach called Geometry-Calibrated Distributionally Robust Optimization (GCDRO) to address the over-pessimism issue in traditional Distributionally Robust Optimization (DRO) methods. This new approach incorporates data geometry into calibration terms, offering a unique perspective on mitigating the impact of noisy samples and improving the performance of machine learning algorithms.

Theoretical Analysis: The paper provides a theoretical analysis of the over-pessimism issue in DRO and establishes a connection between the risk objective in GCDRO and the Helmholtz free energy in statistical physics. This theoretical analysis deepens the understanding of the problem and the proposed solution, providing a solid foundation for the proposed method.

Optimization Algorithm: The paper leverages gradient flow in Wasserstein space to develop an approximate minimax optimization algorithm for GCDRO. This algorithm guarantees a bounded error ratio and standard convergence rate, providing a reliable and efficient optimization framework for GCDRO.

Empirical Evaluation: The paper includes comprehensive experiments to evaluate the performance of GCDRO compared to conventional DRO methods. The experimental results demonstrate the superiority of GCDRO in terms of prediction accuracy, parameter estimation, and generalization performance. The empirical evaluation strengthens the claims made in the paper and highlights the practical benefits of the proposed approach.

Practical Significance: The paper addresses an important problem in machine learning—dealing with distributional shifts and noisy samples—and offers a practical solution that can improve the robustness and generalization of machine learning algorithms. The proposed GCDRO method has the potential to be applied in real-world scenarios where distributional shifts are prevalent, making it highly relevant and valuable.

Weaknesses:
Limited Comparison: The paper compares GCDRO only with conventional DRO methods, without exploring a broader range of state-of-the-art approaches or alternative methods for addressing the over-pessimism issue in DRO. Including a more diverse set of baselines would provide a more comprehensive evaluation and better contextualize the performance of GCDRO.

Lack of Real-world Applications: The paper focuses on theoretical analysis and empirical evaluations using synthetic or benchmark datasets. However, the absence of real-world applications or case studies limits the understanding of how GCDRO would perform in practical scenarios. Extending the evaluation to real-world datasets and applications would enhance the applicability and relevance of the proposed method.



Limitations:
See weaknesses

Rating:
7

Confidence:
1

REVIEW 
Summary:
This work investigates the impact of noisy samples on Distributionally Robust Optimization (DRO) algorithms. Using a simple model and empirically, they show how DRO variants tend to give to much importance to those samples, which make them overly pessimistic in finding the worst case distribution shift. To solve this problem they propose a new method called Geometrically Calibrated DRO (GCDRO), which adds two new calibration terms. One of this term is looking at the relationship between samples (provided by a graph $G_N$) and penalizes shifted distributions $\mathbf{q}$ which assign high probability mass to connected samples when those have very different prediction losses.The other entropy term favors shifted distributions $\mathbf{q}$ with larger entropy, penalizing $\mathbf{q}$ which overly focus on a subset of samples. The authors provide an algorithm to approximately solve the inner maximization loop which they use to apply their algorithms to several benchmarks in which GCDRO is outperforming prior methods. Moreover, the authors draw a parallel between their method and Helmholtz free energy.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: While the two added calibration terms are not entirely novel on their own, I find some novelty in combining them and (i) demonstrating a parallel with Helmholtz free energy, as well as (ii) showing how those can tackle the specific problem of noisy samples in DRO.

Clarity: Albeit dense, the paper is clear enough. I would maybe recommend clarifying early on how to get the graph $G_N$.

Quality & significance: This work is well motivated and I can see it being impactful. Especially, the link between DRO methods and statistical physics could motivate interesting future directions.  

Weaknesses:
The method requires a graph $G_N$, which might not be easy to get. There should be a paragraph on how this graph was obtained for each dataset in the experiments section. In the supplementary, it is mentioned that $k$-NN graphs are used and the authors show that the results do not depends so much on $k$. While $k$-NN graphs might work on simple regression datasets, I doubt they would be efficient on more complex ones. For instance, DORO has been applied to CelebA, I'm not sure a $k$-NN graph would work there. Some more complex regression datasets can be found as part of WILDS [1]. I understand the goal of the paper is not manifold learning, yet better understanding the limitations associated with requiring $G_N$ seems important to grasp how useful the proposed method can be in practice.   

Would proposition 3.3 still hold considering the error obtained from the inner maximization? 

References:

[1] WILDS: A Benchmark of in-the-Wild Distribution Shifts (https://www-cs-faculty.stanford.edu/people/jure/pubs/wilds-icml21.pdf)

Limitations:
The limitations of the method are somewhat discussed in the supplementary. I like that the authors did experiments with several $k$-NN graphs, but I believe this analysis can be more convincing by testing the method on more complex regression datasets. 


Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel approach to tackle the issue of overconservativeness in conventional DRO models. The proposed method incorporates data geometry properties into the design of the objective function and ambiguity set. The authors leverage the discrete geometric Wasserstein distance, initially presented in Chow et al. (2017), as a probability metric to construct the ambiguity set. Additionally, they enhance the model's performance by introducing the graph total variation quantity to the objective function. This modification effectively diminishes the influence of detrimental data points, thus mitigating their impact on the overall optimization process. 
The effectiveness of the proposed approach is extensively validated through experiments conducted on synthetic and real datasets. The results demonstrate its superiority over conventional DRO models, exhibiting improved accuracy and robustness.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
I acknowledge the significance of addressing the challenge posed by noisy examples in machine learning, and I commend the motivation behind the design presented in this paper. The proposed method is intriguing, and the introduction of new calibration terms appears to be a reasonable approach. The experimental results showcase promise, particularly on synthetic datasets, and also demonstrate competitiveness on real datasets.

Weaknesses:
1.The Discrete Geometric Wasserstein Distance restricts the worst-case distribution to have the same support as the training dataset. If this is the case, then the method you propose is fundamentally distinct from Wasserstein DRO. I am interested in understanding the specific scenarios in which Wasserstein DRO outperforms divergence-based methods. As far as I comprehend, GCDRO aims to enhance phi-divergence DRO methods by incorporating graph information. 
2. I kindly request verification of the assumption made in Proposition 3.3, which states that F(\theta) is L-smooth. It appears that the convergence results were obtained based on this assumption alone.

Limitations:
NA

Rating:
5

Confidence:
5

";0
xdtBFMAPD2;"REVIEW 
Summary:
The submission proposes an approach to improve model monitoring by rather evaluating changes in explanations instead of input features. The authors provide synthetic examples to justify their method and compare it empirically to existing strategies on tabular datasets. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
 - The paper addresses an important topic as effective model monitoring based on unlabeled data only is a relevant problem.
 - Although rather, simplistic the synthetic examples help to get a rough idea about the potential benefits of explanation monitoring.
 - The authors provide code as well as tutorials on how to apply their method to ensure reproducibility.


Weaknesses:
 - The theoretical analysis is extremely limited such that the overall assumptions under which the proposed method can be expected to yield actual benefits are too vague. Also, the basic notations section seems a bit inflated. 
 - I think the novelty is limited as well. Monitoring feature attributions instead of input data is not new and is already offered by popular ML service providers. See for instance here the functionality implemented by Google (https://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai). I would have also liked to see such an alternative approach to use explanations for monitoring somewhere included in the experiments.
 - The conducted numerical experiments are not sufficient to demonstrate the benefits of the proposed approach. If only considering tabular data I think including more than 3 actual datasets and 4 prediction tasks is necessary to be convincing. This is especially true for methods where rigorous theoretical analysis is challenging. See also the question below for further suggestions. 
 - The evaluation section is hard to follow, and lacks formulation of insights derived from the experimental results, e.g., it is unclear what benefits can be derived from the feature importance in Figure 4. Given the lack of baselines and justification for those explanations, it is also not clear if they represent useful insights into the effect of a distribution shift on the model’s behavior.  Also, Table 1 comes out of nowhere and is not described sufficiently.
 - Given the limited theoretical and empirical investigation the submission does in my opinion not make a significant contribution to the field.


Limitations:
I appreciate the discussion at the end of the paper that hints at some relevant limitations. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new concept called ""explanation shift"" for detecting shifts in data distributions with the changes of the attribution distributions on machine learning models. The authors argue that current methods for detecting shifts have limitations in identifying changes in model behavior. Explanation shift provides more sensitive and explainable indicators for these changes. The paper also compares the proposed method against other methods for detecting distribution shifts in both synthetic and real datasets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ Leveraging the changes of explanations as a manner of detecting the distribution shift is a novel idea.
+ The authors provide a compreshensive analysis to show the connections between explanation shift and various distribution shifts, which could be helpful for readers to understand how to use explanation shift to detect distribution shift

Weaknesses:
+ The overall presentation is not clear and many key terminologies and notations are not well explained or defined. For example, in Equation (3), what is $x^*$? What is the formal definition of $S(f_{\theta},x)$? What are the definitions of ""sensitivity"" and ""accountability"" which are used as evaluation metrics in Experiments? The lack of clear presentations of these terms makes me extremely hard to understand the key information in this paper
+ Although the authors proposed a new concept called ""explanation shift"", the technical contribution is still very limited. First of all, the method proposed for detecting the explanation shift (i.e., Section 3) is very simple. But the authors failed to justify why this is an effective method from the theoretical perspective by comparing it against other methods. 
+  Some key empirical studies are missing. In Section 5.3, the authors evaluate their methods on some real datasets to detect novel group distribution shift and geopolitical and temporal shift. However, the authors did not perform the same experiments by using baseline methods. Thus it is unclear whether those baseline methods can discover the same types of shifts or not. If yes, then what are the benefits of the proposed method? If not, why are those baseline methods unable to find out those shifts?

Limitations:
Not applicable.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper uses explanation shift as a way to detect different types of distribution shift between the training set and unseen (test) data sets. The method is based on measuring the changes between the explanation provided by an explanation approach such as Shapley values, for the two data sets for a trained model. As such, the two data sets could be statistically similar but appear different from the model’s perspective. Overall, the proposed approach is novel and interesting but the paper needs to be improved. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
To the best of my knowledge, this is a novel approach that uses explanation to detection distribution shift. The proposed method is clear and the method seems to be effective in practice.

Weaknesses:
Section 4.1 provides examples where the proposed method works but simple distribution shift evaluation fails. But this does not provide any guarantee whether in general the proposed model is better or not. The same is true for Section 4.3. Section 4.2. provides a disposition but as mentioned by the authors, the prediction shift implies explanation shift, but the opposite is not true. Thus, no conclusion can be ae when there is an explanation shift.  

Even though the authors compared their proposed model with the baselines on the synthetic data set in Section 5.1, they have not done it using any real data sets. The real data set is mainly used to study the sensitivity of the model on the parameters. 

There is lack of consistency in notation used in the paper that makes it more difficult to follow. Notation changes from one section to another, and in some extreme cases from one example to another. Here are some instances:
1-	Val function is defined differently in Equation 1 and 2. 
2-	Equation 3 is not clear and not explained either. What is the expected value is defined on? If it is X, why the notation differs from Equation 2?
3-	There is a sign used in Example 4.2 which is not defined. 


The paper benefits from a round of proof-reading.
Line 143: out approach –> our approach
Line 182: a hard tasks --> a hard task (the sentence that includes this is also not clear and needs explanation)
Line 279: AppendixE.1 --> Appendix E.1


Limitations:
No.

Rating:
5

Confidence:
3

REVIEW 
Summary:
Detecting shifts in data distribution between training and deployment is critical for ensuring models function as intended and operate in their domain of applicability. However, detecting such shifts is challenging. In this paper, the authors propose an approach based on techniques from the explainability literature. They define the concept of explanation shift and introduce an Explanation Shift Detector. They validate their approach on a synthetic data and 4 tabular datasets, demonstrating improved performance over a range of baselines. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well written, the introduction well motivated, and the formalism both precise and easy for the reader to follow. I found the method interesting, and the analysis of explanation shift detailed and informative. I think this work is a meaningful contribution to the literature.  


Weaknesses:
The experiments were only conducted on several, relatively simple, tabular datasets. Demonstrating the method for another modality would strengthen the paper.

Please see Questions below. 


Limitations:
Yes

Rating:
7

Confidence:
3

";0
M1dTz6QmuM;"REVIEW 
Summary:
This paper focuses on developing theories for the learning dynamics of policy gradient reinforcement learning (RL) algorithms with a particular focus on high-dimensional latent feature space. As an early work along this direction, the authors study a binary-action environment setup for simplicity. The authors develop ODE-based learning dynamic equations that generalizes across diverse protocols, including different policy horizons, the existance of failure penalties, and the choices of dense / sparse rewards. The authors further develop theories for optimal learning rate, optimal horizon scheduling, and learnability with respect to these hyperparameters. The authors finally conduct an experiment on vision-based Procgen Bossfight environment and demonstrate that under more general settings, similar phenomena arise as their theoretical model developed under simpler setups.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall the paper is well-structured and well-written. Under the binary-action environment setup, the ODE dynamics model developed by the authors is shown to accurately describe diverse common learning setups. There are also extensive experiments and plots that illustrate the difference of learning dynamics under different environment and optimization parameters, which provide much insights for the readers.

Weaknesses:
In the main paper, the latent feature space dimension $D$ is fixed to 900 except Procgen. It would be helpful if authors provide more analysis on the influence of latent dimension $D$ on the learning dynamics. Empirically, for which $D$ is author's proposed ODE-based learning dynamics equation still accurate?

For Fig. 6b, Plotting environments of different episode lengths by comparing their ""Number of episodes"" seems misleading. Authors claim that agents learn slower for environments with shorter episode lengths, but this is not accurate. If one compares the total time step of learning (num episodes * episode length), agents acturally learn faster on these shorter horizon environments.


Limitations:
Limitations need to be explicitly addressed in the conclusion section, including (1) the simplicity of the problem setup studied by the paper (binary action scenarios); and (2) the paper's focus on shallow, one-layer neural network that takes high dimensional feature as input. For more general applications, neural networks typically consist of many layers, activations, and normalizations stacked on top of each other, so empirical analysis in these scenarios will be particularly helpful.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a model for solving high-dimensional problems in reinforcement learning (RL) referred to as the RL perceptron. The model is used as a framework for analyzing generalization dynamics of simple neural networks for RL tasks. More precisely, the model employs a student teacher design in which the student takes a sequence of choices and the correct choices are given via the teacher. However, the student does not have access to the correct choice at every step but rather only receives a signal at the end of each episode. As such, the model is studied as a sequential version of the perceptron algorithm. The work first derives a set of differential equations that capture the learning dynamics of the model. The dynamics are analyzed via the overlap of the weight vectors of the student and teacher respectively. The manuscript studies multiple reward settings: a vanilla setting that constitutes sparse rewards, a setting with penalty at every step and a setting where the agent is given small sub-rewards after a certain amount of time. Each of the proposed settings provides insights into how different reward functions lead to different solutions for optimal parameters when solving the system. The derivation of optimal hyperparameters demonstrates that annealing the learning rate and building a curriculum of episodes are crucial for optimal convergence. Then, it is shown that there exist phase transitions under different learning rates that can lead to convergence to sub-optimal minima and that there is a speed-accuracy tradeoff when varying reward functions. Lastly an experimental section provides insides in the practical properties of the algorithm that closely follow the analytical results on the speed-accuracy tradeoff.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.) First, I would like to say that I enjoyed reading this paper. I think it is well written and well structured with a clear line of reasoning throughout the manuscript. The figures are illustrative of the analysis and the captions are sufficiently descriptive to understand the plots quickly.

2.) The idea of employing a perceptron-like algorithm in order to understand the policy gradient system dynamics is novel to the best of my knowledge and the generated insights are interesting. I think this is a nice contribution as it offers a way to analyze high-dimensional RL systems in a different way than the commonly employed linear MDPs.

3.) The manuscript is technically sound and the analysis provides a good understanding of the proposed algorithm and its inner workings. I did check the math in the appendix for crude errors and was unable to find any but I did not try to understand all the math in detail.

4.) The manuscript provides a way to think about RL systems that is not common and I think it is likely going to be useful in understanding some parts of the systems that may have been hard to understand previously. As such, I think it is a decent contribution that can likely be built upon by others in the sub-field of policy gradient methods.


Weaknesses:
a.) The connection to policy gradients was not immediately clear to me. It might make sense to move equation 9 in place of equation 1 and highlight the connection between the perceptron update rule and the REINFORCE algorithm in a brief sentence. I think it would be good to highlight that the update rule uses an approximation that is only accurate early in training.

b.) The limitations of the model are addressed rather sparsely. I think the work would benefit from having a clearer picture of the weaknesses of the approach which would enable researchers to use it and improve upon it in the future.

c.)  The model seems to be unable to solve the benchmark problem fully. However, it is hard to tell whether that is just a limitation of the model or whether the task is hard the way it is designed with the changes in the manuscript. Having a baseline performance line in the plot or giving a brief sentence of what the expected performance of a commonly used RL algorithm on the benchmark is would be very useful to determine the capabilities of the method.

d.) I think one key thing that is missing from the paper and would make it a very strong contribution is to show the relationship between the proposed model and common deep RL methods. It would have been nice to have a direct comparison from the proposed method to a neural network approach using standard REINFORCE-like updates to see if there is a correlation produced from insights of the RL-perceptron with the behavior of the regular deep neural network. I do understand that space is limited though.

----

Overall, I think changes that could improve the manuscript would establish the model's connections to other research that people have done in the area. This could, for instance, include baseline performances, transfer of insights to other methods or any theoretical results that put the work into reference with commonly knows results.

Minor clarity suggestions:
* Line 250, there is a broken off sentence in there that should be removed.
* Line 562 equation reference is missing.


Limitations:
I do not believe there is any negative societal impact that needs to be addressed regarding this work. The limitations are addressed rather sparsely. As stated before, I believe that the manuscript would benefit from more structured limitations sections. One limitation that I see is that the model currently requires actions to be discrete and (possibly ?) binary. While for a first version of the model this is absolutely fine, this might be something that researchers can work on in the future. Another limitation of the model is that it seems to not be able to fully solve the suggested benchmark problems. Again, I don't believe this to be an issue for the manuscript as the goal is not to provide a state-of-the-art model but rather to make progress towards understanding the learning dynamics of deep RL systems. Yet, in the future a goal should be to have models that can be described analytically that also achieve comparably high performance on realistic tasks.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors develop a set of differential equations that describe the learning dynamics in high-dimensional settings, allowing for a quantitative analysis of learning behaviors. This framework enables the computation of optimal hyper-parameter schedules and the visualization of phase diagrams for learnability. It also serves as a starting point for exploring RL scenarios closer to real-world situations, including those with conditional next states. The RL perceptron can be used to investigate various training practices, such as curricula, and advanced algorithms like actor-critic methods. The authors aim to gain analytical insights from the differential equations, particularly regarding how initialization and learning rate affect an agent's learning process. Overall, their research highlights the complex interplay between task, reward, architecture, and algorithm in modern RL systems.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The paper tackles an important problem of understanding high dimensional RL policies. 
* The method section seems theoritically sound.

Weaknesses:
Experiments are not convincing. For a paper investigating such an important problem, the paper should have shown results on multiple environments.

Limitations:
A considerable limitation of the paper is the lack of a comprehensive evaluation of the method. The paper provides only results on one experiment. I would suggest the authors conduct more thourough experiments in order to make the applicability of method more clear.

Rating:
6

Confidence:
1

REVIEW 
Summary:
The work proposes a theoretical framework to study the average case learning behavior of deep RL policy gradient methods. The framework is based on ODEs that can describe the typical learning dynamics of PG RL agents. The framework is used in various settings to describe learning behaviors in these and a final experiment on training a policy-gradient agent in a ProcGen game is added to bridge the theory-practice gap. This final experiment verifies that a speed-accuracy trade-off exists in practice, similar as predicted by the theoretical framework.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
* The work addresses an important aspect in deep RL research as most theoretical guarantees for RL are not well connected to the practical side of deep RL.
* The work sets out to provide a theoretical framework from which to study deep RL methods.
* It shows how such the theoretical framework can be used to understand the influence of
  * delayed rewards and reward penalties
  * learning rate schedules and episode lengths
  * reward stringency

Weaknesses:
* The work was very difficult to follow for me. Due to the structure of the paper, many aspects seem to ""fall out of thin air"".
* Without Appendix A it seems impossible to begin to understand Section 2 since more assumptions about the student teacher environment are given.
* It often feels like the work requires extensive prior knowledge to be understandable.
* Wording is often confusing. For example, in the beginning when talking about the reward for the RL-Perceptron case Fig 1 has a description about rewards that seems permissible for very dense rewards, whereas lines 48-50 talk only about extremely sparse rewards.

Overall the paper seems very interesting and full of great ideas but due to a somewhat convoluted presentation and missing details that seem to be pushed to the appendix it falls short of clearly communicating these ideas.

I might have missed something obvious, but to me it seems that the paper would first need fairly substantial rewriting to be easier to parse before it can be accepted.

Limitations:
The authors do not explicitly list limitations of their framework 

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper discusses the application and theoretical understanding of Reinforcement Learning (RL) algorithms in high-dimensional settings. The authors propose a high-dimensional model of RL that can capture a variety of learning protocols and derive its dynamics as a set of closed-form ordinary differential equations.

The authors introduce the RL perceptron, a model for high-dimensional, sequential policy learning. In this model, a student network learns from a teacher network in a sequential decision-making task. The student does not observe the correct choice for each input; instead, it receives a reward that depends on whether earlier decisions are correct.

The authors derive an asymptotically exact set of ODEs that describe the typical learning dynamics of policy gradient RL agents. They use these ODEs to characterize learning behavior in a diverse range of scenarios, including exploring several sparse delayed reward schemes, deriving optimal learning rate schedules and episode length curricula, identifying ranges of learning rates for which learning is 'easy,' and 'hybrid-hard,' and identifying a speed-accuracy trade-off driven by reward stringency.

They also demonstrate that a similar speed-accuracy trade-off exists in simulations of high-dimensional policy learning from pixels using the procgen environment ""Bossfight"" and Atari ""Pong"". The authors aim to close the gap between theory and practice in high-dimensional RL.
The paper also discusses the sample complexity in RL, statistical learning theory for RL, and dynamics of learning, providing a comprehensive overview of the current state of RL theory and practice.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This is a robust theoretical paper that enhances our comprehension of high-dimensional RL policy learning. The paper's claims are supported by high-level experimental evidence. A significant advantage of this paper is its use of the challenging procgen ""Bossfight"" environment, which is much closer to the real use cases compared to the preceding works. Additionally, the ""Pong"" game was analyzed in the supplementary materials to demonstrate the speed-accuracy tradeoff. Another strong aspect of the paper, and a significant advantage, is its focus on analyzing the average-case scenarios rather than the worst-case ones. The authors have also released the code to reproduce the results, further strengthening the paper's credibility.

Weaknesses:
To the best of my knowledge, the paper does not exhibit any significant weaknesses.

Limitations:
Limitations were addressed reasonably well. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces a new problem definition and class of solution methods for decision making. 


The motivation put forward by the authors is that the current pool of solution methods do not have theoretical results that capture the neural functional classes used by practical applications in RL, leaving a gap between theory and practice. 

The authors claim that the model of learning by reinforcement that they propose has sufficient flexibility to capture the same class of problems as the classic RL model, and that their problem definition, despite its generality, is solvable in closed form for higher dimensionalities (and even infinite), as opposed to the classic RL model, presumably formalised as POMDPs. In addition, they claim their model (unclear whether the authors mean the solution method or the problem, see detailed comments) behaves similarly w.r.t. hyperparameters, for which they provide optimal schedules and hypersensitivity plots. 

Moreover, the authors claim their proposed class of problems and solution methods exhibits in practice similar behaviour as predicted by theory, and closes a gap between theory and practice, providing as evidence empirical illustrations in a particular environment, specially designed, called “Bossfight”, which they engineer using a platform for a procedurally generated environments. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
$
\textbf{Originality}$

The authors propose a new problem definition and class of solution methods for RL, particularly policy optimisation. They describe their model as a perceptron, which they call “the RL perceptron”, and analyse the learning dynamics of a particular solution method for action space of 2 discrete actions. They then define different problem instances via heuristics for various feedback signals that such a solution method can receive. The authors propose a new perspective on RL inspired by statistical mechanics and dynamical system theory.

$\textbf{Quality}$

The particular math and statistical expressions the authors derive was not checked in detail beyond  the first few simple update equations, which seem correct. The appendix was not verified for correctness. 
Plots also appear to be consistent with the experimental claims provided in text.


$\textbf{Clarity}$

The paper is at times clear, with some mysterious redefinitions, naming, and confusions (see in the next sections). Although the flow is cursive, the authors do not use the general breakdown of the paper and miss important sections like a “Background” section, which is placed in the introduction. It does not bother much, but clearly marking such section can help the reader understand better what is novel and what is known. 


$\textbf{Significance}$

The main motivation of the paper is very important. Theoretical guarantees for solution methods with feasible at-scale practical implementations is highly desirable, since this informs us that they are generalizable to all problem instances, not just the particular settings in which they were tested in and shown good empirical results. Generalisation and theoretical guarantees in RL, particularly in policy-based methods with neural policy classes, which are the most successful algorithms used in practice, is a very important area of research. 



Weaknesses:
$\textbf{Motivation}$

While the motivation of the paper is very important, i.e. insufficient theoretical guarantees for policy-gradient methods with neural function classes, PG have been shown to have global convergence beyond tabular fn classes which the authors claim to be the issue.  For log-linear policy classes see: Yuan  et al - “Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies” - and references within. For neural policy classes see “Neural Policy Gradient Methods: Global Optimality and Rates of Convergence” -- Wang et al and references within. Both of these show global convergence, the latter for a two-layer NN, including finite-sample guarantees/convergence rates and properties of actor-critic methods, with estimated critics, required for convergence. 

$\textbf{Relation to RL and placement in context}$

The second main weakness of this paper is the lack of placement of their work in relation to the standard RL model used by the community through the formalism of MDPs or POMDPs, with accompanying class of solution methods. The paper would be significantly strengthened if they authors can clearly state what the limitations are with the previous class of problems described in RL via a reward signal and transition dynamics and in addition for POMDPs, how current definitions of high dimensional observation spaces proposed are better captured with their problem definition. It is unclear how the models proposed by this paper generalize beyond the log-linear policy class and action spaces of dimension 2, described within. The authors claim lack of theoretical work, yet previous work in RL has also analyzed distribution shift and generalization error, but with a different definition than the authors propose. It is unclear to me what the prior limitation in definition were and how this is a better way of capturing such quantities. Additional details in this respect would significantly strengthen the paper..

$\textbf{Purpose/Goal}$

Furthermore, it is rather confusing the purpose of the paper, i.e. it seems the authors compare problems against each other, for the same solution method, instead of comparing solution methods that are general enough to work on every problem instance. It is also confusing the setting and problem definition. It appears that the problem is not learning by trial-and-error, and that the problem instances proposed need heuristic descriptions of reward signals, horizon sizes and task termination. It is unclear the setting in which we are in, whether that is undiscounted finite-horizon, or continuing learning (infinite horizon, average reward). The authors reference terms from RL related to this but never actually formalize the problem.

$\textbf{Empirical study/experimental illustration}$

Lastly, the experimental section is performed on a certain game designed in particular way, which is an illustration rather than a practical algorithmic implementation, akin to the solution methods employed in empirical RL. 

More details/questions in the next sections about these points.

I am happy to adjust my score if I have not correctly understood the paper, and the authors provide more details on how their work can be placed in the context of RL, which would help me understand the significance and impact of the result and analysis provided.

Limitations:
$\textbf{Lack of clear description of the limitations}$

The authors do not provide a clear description of the limitation of their model. It is unclear what exactly is “solvable” in closed form, is it any problem of any dimensionality with any kind of reward signal, including non Markovian? How general is the problem that is “solvable”? Does it capture all problems a POMDP would capture, and these problems are solvable? Any kind of additional information in this respect would be useful for the reader. It is also unclear what is “the average-case”. A short definition would be very useful.


Rating:
5

Confidence:
3

";0
hsZTLwE6N1;"REVIEW 
Summary:
In this paper, the authors propose an enhanced version of DensEMANN, which efficiently grows and trains small DenseNet architectures. They employ a macro-algorithm to expand new layers and utilize a micro-algorithm to construct new convolution operations. Through iterative layer growth, this method generates novel architectures within a few GPU hours.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Detailed experimental settings are provided in this paper.

Weaknesses:
- The motivation behind this research requires additional clarification.
- As shown in Table 2, GO methods consumes the leaset GPU days and achieves the best performance. Therefore, it raises the question of why not directly utilize the GO methods.
- The experimental comparison with the original DensEMANN is missing.
- Experiments are only conducted on small datasets. Can DensEMANN be applied on large datasets, for example ImageNet-1k?
- Too many hyper-parameters are introduced in this method, which brings difficulties for applying this method on other tasks.

Limitations:
Please see the Weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors study an algorithm for neural architecture search (NAS) called DensEMANN, which uses a progressive adaptation of a DenseNet architecture during training to find an efficient neural network for the target task.

Soundness:
3

Presentation:
4

Contribution:
1

Strengths:
The paper is very well written and the authors do a good job of explaining how the DensEMANN algorithm works.

Weaknesses:
I’m not sure that the paper currently has enough substance for publication. The authors’ spend the first 5 pages on introduction and description of the previously published DensEMANN algorithm. The changes made to this algorithm are described in section 3.2 in only 20 lines of text and appear to be primarily changes to the various hyperparameters of the existing algorithm.

The primary contribution of the paper is comparison of DensEMANN with other NAS methods on CIFAR-10 in section 4.3. The results are certainly interesting, but I think the authors should focus on quality per unit time rather than quality and parameter counts plotted in Figure 2. Plotting quality against the execution times in table 2 would make it much easier to compare DenseMANN to existing methods in efficiency, which is the primary property of interest, I think.

That being said, I’m not sure empirical comparison of an existing method with state-of-the-art methods is enough novelty to merit publication at NeurIPS. I’d encourage the authors to continue to develop their exploration. For example, clearly establishing a new state-of-the-art in efficiency for NAS. Or, taking the models from DensEMANN and studying their efficiency for deployment.

Limitations:
I did not identify potential negative societal impact of this work.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper presents  a new version of DensEMANN, an algorithm for generating small and competitive DenseNet architectures with optimal weight values. The authors aim to approach state-of-the-art performance for well-known benchmarks, or at least the state-of-the-art Pareto front between performance and model size. They achieve this by introducing a new version of the algorithm that uses a combination of layer pruning and weight optimization techniques. The authors evaluate DensEMANN on three popular image classification benchmarks (CIFAR-10, Fashion-MNIST, and SVHN) and show that it outperforms or matches the state-of-the-art methods in terms of accuracy and model size. The contributions of the paper are a new algorithm for generating small and competitive DenseNet architectures, a combination of layer pruning and weight optimization techniques, and state-of-the-art results on popular image classification benchmarks.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The pruning and recovery stages which are the contributions of this paper are very well motivated and clearly explained. The way the pruning and recovery stages are designed is novel and using it in this context is rather unique.
- The authors have very aptly identified how DensEMANN fits in and very well introduced incremental approaches and NAS architectures.

Weaknesses:
- The difference in this paper with the original DenseEMANN is clearly communicated by the authors however all the the points mentioned except 2 (d):

> The pruning and recovery stages have been heavily modified to avoid long recovery
stages and their effects. We indeed observed that the kCS of settled filters is not
constant but actually decreases very slowly over time. If the recovery stage is too long,
this causes a very harsh pruning after which the accuracy cannot be recovered.

are just based on observation or not introduced in the paper or are very straightforward changes, I would suggest to consider only 2 (d) as a contribution of this paper.
-There are other aspects of this model which are very well framed and novel however it is important to note that these parts of the DensEMANN architecture arer not introduced in this paper but the original DensEMANN paper which reduces the novelty of this work by a huge margin.
- The paper does not provide a clear explanation of how the parameter limit of 500k was chosen for the experiments. Does going above these number of parameters make DensEMANN very computationally intensive or is unable to grow the network sensibly especially considering that 500k parameters in modern comparison are very few parameters especially for vision tasks?


Limitations:
- With the current set of evaluation the authors do it is very hard to determine if DensEMANN like techniques can be applied for larger and more modern models

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a new version of the existing DensEMANN, which grows small DenseNet architectures and trains them on target data. It claims that this version can quickly and efficiently search for small and competitive DenseNet architectures. The proposed approach has been evaluated on a number of benchmarks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The idea of automatically generate efficient architectures from a reference makes sense, and can be of great interests in many application scenarios. 
- The proposed approach grows the architecture at both macro and micro levels, which seems to be a valid strategy. 
- The proposed approach has been evaluated on various benchmarks, showing comparable performance with the state of the art. 

Weaknesses:
- The delta compared to the original algorithm seems to be not very significant. 
- The claim on being able to generate efficient densenet architectures is only supported by the number of parameters. For densenet like models the number of parameters might not be a good indicator for efficiency, due to many skip connections. Thus it seems not a very fair comparison.
- It is not clear how the proposed approach performs on larger datasets such as ImageNet. 

Limitations:
N/A

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper proposed a new and improved algorithm to grow small DenseNet architecture from scratch while simultaneously training them from target data.


Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
1. The paper is very clear and readable.
2. The evaluation is comprehensive and detailed, demonstrating the effectiveness.

Weaknesses:
1. The novelty is limited. The algorithm is backboned on a well-known algorithm, and the change to it is limited.
2. The scope of this algorithm is limited too. 

Limitations:
Can this algorithm is adapted to other application fields?

Rating:
4

Confidence:
2

";0
B4TAPfHa7g;"REVIEW 
Summary:
The paper introduces Constrained Proximal Policy Optimization (CPPO) for Constrained Reinforcement Learning (CRL). The CPPO method is designed to overcome the limitations of existing methods by offering a first-order feasible region method that doesn't require dual variables or second-order optimization, and it is an incremental extension of the  CVPO algorithm (Constrained Variational Policy Optimization for Safe Reinforcement Learning). The improvement seems incremental, improving the computational efficiency of CVPO. The method is evaluated in different environment,  comparable or even superior performance to other baseline methods.  . However, the paper does not provide  a direct comparison between CPPO and CVPO.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The authors propose a new first-order method for constrained RL. The method seems to be designed to be simple, and overcome  limitations of existing methods such as CPO and CVPO (e.g., the authors do not require the usage of a dual variable).

- The proposed method demonstrates comparable or even superior performance compared to other baseline methods. 

Weaknesses:
- Clarity: the paper could benefit from improvements in clarity and readability. The presentation of their ideas is somewhat dense and could be difficult for readers to follow. A concise description of the algorithm is missing.

- Novelty and results: The method builds directly upon CVPO. The authors need to clearly highlight this fact when introducing their method, and clearly explain what are the differences. As of now, the changes seem incremental, and therefore the paper seems to lack in novelty. Furthermore, the paper does not provide  a direct comparison between CPPO and CVPO, especially in terms of computational complexity

- No theoretical results are provided, therefore I'd have expected more extensive numerical results.

- Typos and notation: there are several typos and errors in notation throughout the paper.

Limitations:
Authors briefly discuss limitations and broader impact.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper introduces the constrained version of PPO that is devised for solving constrained MDP problems in the discounted reward setting with discounted cost constraints.
 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper presents the constrained PPO. It also gives some analytical results and presents a heuristic algorithm that is seen to perform well numerically. 

Weaknesses:
1. Inadequate literature survey. The authors should broaden the scope of their survey - in fact constrained actor critic was proposed in the paper: ""V.S.Borkar, An actor-critic algorithm for constrained Markov decision processes, Systems and Control Letters, 54(3):207-213, 2005'', for the full state case and ""S.Bhatnagar, An actor–critic algorithm with function approximation for discounted cost constrained Markov decision processes, Systems and Control Letters, 59(12): 760-766, 2010, in the function approximation setting.

2. The authors point to the fact that they have not provided a convergence analysis for their algorithm even though other works in the literature such as the ones listed above do possess an asymptotic analysis of convergence.

3. The results given in the text carry imprecise statements, for instance, Prop. 3.1 says ""if there are a sufficient number of sampled v, then E[v]=1 and E[vlog v] <= var(v-1). This has to be made precise. Does it  mean that in the limit that the number of samples goes to infinity, the statement is valid? Then, if so, the question will be what will be the form of the statement in terms of the number of samples N.

4. The recovery update problem in Sec 3.2 is not clear - how it has been arrived at? 

5. Subsequently a heuristic procedure has been proposed and the authors say that an optimal solution to (5) is provably obtained, But if it is provably obtained, how is the procedure a heuristic procedure? 

6. After (4), it is said that (4) can be directly solved through existing convex optimization techniques. This is not clear since one needs A(s,a), A_c(s,a) etc. to be known in order to solve it directly. But that is not known and needs to be estimated. So not clear what the authors mean?
****
7. On further reading of the paper and supplementary material post the response of the authors, I feel the technical results are imprecise and flawed. For instance, there is no way to verify Assumption 3.5. Moreover, the statement of Proposition 3.1 seems to suggest that it depends on a ""sufficient number of sampled v"" but gives a bound on the true expectation. Why should the true expectation depend on the number of samples of v? Also, when you say ""sufficient number of sampled v"", what does it mean? How many v is a sufficient number?


Limitations:
The major limitation is in terms of a lack of credible analysis. Even the theoretical results presented are not precise, so nothing much can be said about the algorithm. In addition there are several typos and grammatical errors throughout the paper.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel first-order feasible method, CPPO, for efficient constrained reinforcement learning. The proposed approach integrates the Expectation-Maximization (EM) framework to solve the policy optimization problem by treating the CRL as the probabilistic inference. In the E-step, CPPO calculates the optimal policy distribution within the feasible region. In the M-step, CCPO conducts a first-order update for policy optimization. The authors also propose an iterative heuristic algorithm from a geometric perspective to efficiently solve the E-step and a recovery update strategy to improve constraint satisfaction performance. They evaluate their algorithm in several benchmark environments. The reported results show comparable performance over other baselines in complex environments.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The proposed algorithm converts the CRL problem into a convex optimization problem with a clear geometric interpretation which mitigates the impact of approximation errors and strengthens the capability of the proposed method to satisfy constraints.

(2) Since the proposed method does not require second-order optimization techniques or the use of the primal-dual framework, the policy optimization process is largely simplified.


Weaknesses:
(1) I suggest further polishing and improving the mathematical formulation and notations to make them more rigorous. For example, the objectives and constraints in (5) (6) are represented by the dot product of two vectors. In my understanding, it should be easier to understand with a transpose on the top of the first vector. 

(2) The policy update strategy looks overly conservative. Figure 1 shows that only in case 3, the policy can be updated toward seeking a higher reward. Does this strategy result in over-conservativeness? I am also wondering in cases 1 and 3 why the solution is not at the feasible boundary (that is to find the feasible distribution to maximize A).

(3) It is very impressive that CPPO can work well in the AntCircle and Push tasks. However, the experiment lacks sufficient baselines and environments for comparison. For example, the recovery update in Section 3.2.2 is similar to the idea in Yang et. al [1, 2], where an additional projection step is introduced to recover the safe policy, so I am wondering how does the author compare CPPO against these methods, both in theory and empirically. In addition, the results only present a subset of tasks in SafetyGym, so I wonder how the algorithm performs in other tasks.

[1] Yang, Tsung-Yen, et al. ""Projection-based constrained policy optimization."" arXiv preprint arXiv:2010.03152 (2020).

[2] Yang, Tsung-Yen, et al. ""Accelerating safe reinforcement learning with constraint-mismatched baseline policies."" International Conference on Machine Learning. PMLR, 2021.

Limitations:
The authors adequately addressed the limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel CPPO method to solve the contained RL (CRL) problem. Specifically, CPPO leverages the probabilistic inference and converts the CRL problem formulation based on the probabilistic ratio, resulting in the first-order optimization solution. CPPO also develops the recovery update method to safely optimize the policy when there are inaccurate cost evaluations and infeasible solutions. 
The resulting EM-based framework of CPPO shows its effectiveness across various safety gym scenarios compared to baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. CPPO solves the constrained RL based on first-order and does not use dual variables and second-order optimization, resulting in a simpler, intuitive (i.e., geometric perspective), and computationally efficient method. 
2. The recovery update method is developed thanks to CPPO's first-order optimization/geometric perspectives and shows its effectiveness (as shown in Figure 4).


Weaknesses:
1. As stated in Section 3.2.1, CPPO builds on CVPO and has two main differences (using advantage instead of q and using the probability ratio instead of directly calculating q). While  Some readers may understand these as a limited novelty. Possibly adding comparisons against CVPO in the evaluation section can convey the importance of these differences better. 
2. While I agree that CPPO reduces computational complexity, there are no empirical results in the evaluation section. Possibly, adding computation time results in the evaluation section can help. 
3. I understand the benefits of converting the CRL problem into first-order optimization, but it is unclear what potential limitations/disadvantages the first-order optimization may have. Could there be an approximation error compared to second-order optimization? Related to this concern, could the authors clarify further why Assumption 3.5 is a fair assumption?


Limitations:
The authors have adequately addressed the limitations.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies constrained reinforcement learning problems. It proposes a new EM-type algorithm and designs a heuristic version for practical use. The authors also conduct some numerical experiments to validate the performance of the algorithm.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is first-order and thus computationally efficient in practice.
2. The numerical results look convincing. 

Weaknesses:
1. The algorithm looks very similar to CVPO and only has some small modifications.
2. This paper does not have convincing theoretical analysis. For example, the authors claim that the heuristic algorithm in Section 3.3 will stop in just a few iterations (remark 3.6) but there are not any theoretical proofs. It would be better if the paper can provide some theoretical study, even only for simple cases.  

Limitations:
None.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper focuses on constrained reinforcement learning and proposes a method called Constrained Proximal Policy Optimization (CPPO). Experimental results demonstrate the improved performance in terms of episodic return and episodic cost.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ The proposed CPPO achieves improved balance between return and cost empirically.

Weaknesses:
- The novelty and contribution of this work is not clear. The difference between the proposed CPPO and CVPO does not seem to be very significant. Adopting the advantage value instead of Q-value is a natural extension. Besides, it is unclear how much sample complexity can be reduced by replacing $q$ by $v$. After all, $v$ should still satisfy the expectation constraint.
- The paper does not have any theoretical characterization of the proposed CPPO algorithm.
 

Limitations:
 The paper mentions that CPPO method is an on-policy constrained RL, which suffers from lower sampling efficiency compared to other off-policy algorithm.

Rating:
4

Confidence:
3

";0
3HlULdiKFM;"REVIEW 
Summary:
This paper focuses on the task of Compositional Video Retrieval (CoVR) in which given a video and a text which modifies the video aims to rank and retrieve the modified video. In this work, a new large-scale dataset for pre-training (named WebVid-CoVR) is developed with generated modifications from a Large Language Model (LLM). Additionally, a smaller dataset for evaluation is manually annotated. Alongside this, a method is proposed that follows HH-NCE to learn the video compositions. Experiments show that the proposed method works well in both the supervised and zero-shot cases, and even translates well to Compositional Image Retrieval (CoIR) datasets CIRR and FashionIQ.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
The created dataset(s) looks like it will be a useful addition to vision-language pre-training and especially for the Composed Video Retrieval task which currently doesn't have a dataset to train on.

The methodology of creating the dataset is well-done and all the steps make sense, especially for the manually annotated portion of the test set.

The paper is generally well-written and easy to follow.

Weaknesses:
Line 137: Is the Top K sampling referring to the tokens within a modification text? If not it's not clear what this means as only a single modification is generated as specified on Line 138.

Regarding the rule-based ablation in Table 6, could the generated rules be paraphrased by a LLM? In this case, the MTG-LLM wouldn't necessarily need to be fine-tuned and a standard LLM could be used instead.

Line 164: The paper mentions that WebVid-CoVR is noisy which is why a subset is manually annotated for the test set. What is the main source of noise within the main dataset? Is this from the generated modifications? The captions not matching the videos or something else? Has a human study shown how much of the dataset can be considered noisy from looking at a small number of samples?

\alpha and \tau are learnable parameters? Normally, \tau at least, is a set hyperparameter. How these are learnt aren't mentioned within the text.

Has any analysis been performed into the types of modifications within the dataset? I.e. noun/adjective/verb changes and others? It is mentioned within the limitations that certain modifications may not have been generated and it would be good to see what coverage there could be - even if this is from a human annotated subset.



Limitations:
There is a short section on limitations which mentions the scope of modifications within the dataset, I think this could be improved by performing some analysis on the generated prompts.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper addresses the challenge of composed video retrieval, which involves querying a video and a modification text to find videos that exhibit similar visual characteristics with the desired modification. The main challenge is the lack of data for composed video retrieval. To overcome this, the paper proposes mining paired videos with similar captions from a large database and generating the corresponding modification text using a large language model. The paper explains the BLIP-based video and text encoder and the training process for the model using the collected data. Experimental results demonstrate that the model trained on the compiled dataset can generalize to both zero-shot and fine-tuning settings.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper introduces a novel task of video retrieval and establishes a benchmark for future research in this area.
- Overall, the paper is well-written, clear, and easy to follow.

Weaknesses:
- Examples in Figure 3 suggest that most samples do not require handling dynamic content, implying that there may not be a significant difference between the proposed CoVR task and existing CoIR tasks.
- The MTG-LLM method requires manually created data for fine-tuning, which can be resource-intensive.
- The training method is standard and not particularly innovative, although it is reasonable for the task.
- The training data is limited to modifications that can be represented with single-word differences, potentially excluding other types of modifications. This point is mentioned as a limitation in the paper. By providing concrete examples of scenarios not addressed by this work, readers will understand the challenge clearly.

Limitations:
The paper mentions two limitations.
- The data creation pipeline may not adequately capture some visible changes due to its design.
- The generation of modification text may not be optimal as the text generation depends only on input captions.

I appreciate the authors for pointing out the meaningful limitations and suggesting ideas for future research.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a scalable approach to automatically generate composed visual retrieval training data. Specifically, based on the WebVid2M dataset, the authors generates a WebVid-CoVR training dataset with 1.6M CoVR triplets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The data augmentation strategy is scalable.

Weaknesses:
The overhead for the dataset augmentation should be detailed.

Limitations:
The overhead for the dataset augmentation should be detailed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper automatically constructs a new dataset called WebVid-CoVR by applying a scalable automatic dataset creation procedure that generates triplets from video-caption pairs to a large-scale WebVid2M collection, resulting in 1.6M triplets. Moreover, this paper introduces a new benchmark for composed video retrieval(CoVR) and contribute a manually annotated evaluation set, along with baseline results. The results demonstrate that training a CoVR model on WebVid-CoVR transfers well to CoIR with competitive performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The automatic triplet generation pipeline is carefully designed with many phases.
- Provides strong baseline results on CoVR and shows transferability to CoIR.

Weaknesses:
- The automatic triplet generation pipeline seems to rely only on caption similarity, as well as MTG-LLM, which may introduce noise and ignore visual similarity. Visual similarity between videos should also be taken into account.
- More dataset analysis, especially about the visual part, should be provided to gain more understanding of the characteristics of WebVid-CoVR. Besides, the human check may be necessary.
- Only one model (CoVR-BLIP) is evaluated on the proposed CoVR task. More baselines like frozen/finetuned multimodal transformers could be added.

Limitations:
Please refer to Weaknesses.

Rating:
5

Confidence:
4

";0
3s6aE1LeiR;"REVIEW 
Summary:
This paper proposes a 0-shot NAS method. The key point is that there are latent factors that can influence the architecture search procedure, making the validation accuracy of one-shot NAS unreliable. The method adopts Gaussian intervention to the data and evaluates each operation's performance to reduce the bias brought by validation dataset sampling. Some experiment on CIFAR-10, NAS-Bench-201, and ImageNet show that the method can efficiently search architectures.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well written and easy to follow.
2. The paper considers the latent factors that may influence the NAS validation, which has been hardly considered in existing works.
3. The proposed method uses causal inference techniques to solve the NAS problem.

Weaknesses:
1. Some definitions and connotations are unclear in the paper. See Questions for more details.
2. The proposed algorithm is too straightforward. It seems directly use intervention technique in NAS procedure, but do not consider any characteristics of NAS problems, which makes the contribution of the method limited.
3. In the experiment part, the authors claim the efficiency of the method, but lots of 0-shot NAS methods has a very high efficiency. The authors do not compare with those 0-shot NAS methods in this part.

Limitations:
N/A

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper formulates zero-shot NAS as a causal-representation-learning. Further, it uses the high-level interventional data from one-shot NAS to facilitate zero-shot NAS to refine the imperfectness. Extensive experiments achieved comparable performance results on multiple benchmarks.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1) This paper proposes to use the high-level interventional data to facilitate zero-shot NAS to address the imperfect-information issue.

2) This paper provides theoretical support for the proposed approach.

Weaknesses:
1) The novelty is incremental compared to baseline Zen-NAS approach, and also directly applies the Shapley value which is also leveraged in Shapley-NAS. 

2) Although the search cost is low, the achieved performance improvement is not significant.

Limitations:
The novelty is limited, and the achieved performance improvement is limited.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents a causal definition of zero-shot NAS and facilitate this with interventional one-shot knowledge data. The paper theoretically demonstrates the validation information of either a neuron or a neuron 60 ensemble obeys a Gaussian distribution given a Gaussian input. It then uses high level interventional data from one-shot NAS to solve the imperfectness of zero-shot NAS. The zero-cost NAS method is studied on DARTS space and NAS Bench 201 with very low search cost while maintaining comparable test accuracies. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and novel. Studying NAS from a causality perspective is original and interesting. 

Weaknesses:
- The main weakness in my opinion is the weak evaluation of the approach. [NAS-Bench Suite Zero](https://arxiv.org/pdf/2210.03230.pdf) provides easy access and evaluation on about 13 proxies and 28 tasks. Since the performance of a given proxy can vary widely depending upon the search space, task and datasets I am not convinced of the effectiveness of the proxy based on only the results on NATS-Bench and DARTS spaces. Furthermore how would one apply this method to transformer spaces (eg [AutoFormer](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_AutoFormer_Searching_Transformers_for_Visual_Recognition_ICCV_2021_paper.pdf) , [HAT](https://arxiv.org/abs/2005.14187)) and MobileNets(eg: [OFA](https://arxiv.org/pdf/1908.09791.pdf), which prove queriable validation accuracy based on a surrogate. Can this method be applied in these spaces? Since modern one-shot NAS methods are applied on transformer and mobilenet spaces too, it is important to design a proxy that does indeed generalize well. I encourage the authors to evaluate their method on these search spaces too.
- The results in table 1 and table 2 show a drop in accuracy ( though the num params is lower and search cost is low). Hence I am not very convinced of the effectiveness of the method in finding effective architectures.
- Since reproducibility is quite important in NAS, I think it is very important for the authors to release their code (I couldn't find the code attached). 
- Correlation coefficient of the ranking with the true ranking is not studied


Limitations:
I encourage the authors to discuss the limitations of the proposed method more extensively for eg: assumptions which are architecture type specific? search space generality? Any assumptions which may not hold in practice?

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes a causal zero-shot neural architecture search (NAS).  The NAS problem is decomposed into two components: ensemble selection and neuron selection. By employing the Gaussian intervention to approximate validation accuracy, the authors adapt the perturbation-based approach from DARTS+PT to search for architectures. The performance of their proposed Causal-Znas is evaluated on both NAS-Bench-201 and DARTS search spaces.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- It is reasonable to consider the one-shot interventional information for zero-shot NAS.

Weaknesses:
- The overall contribution of this research appears to be limited. The combination of zero-shot NAS with DARTS+PT is not considered novel and does not provide significant insights.
- There are some omissions of recent related works in Section 2.2, such as GradSign[1] and ZiCo[2].
- The experimental comparison provided is insufficient. The results of Zen-NAS are missing in the comparison, and it would be beneficial to include a comparison with GradSign and ZiCo. Additionally, recent works often conduct experiments on other tasks such as NLP and ASR, which could provide further insights.

[1] Zhihao Zhang and Zhihao Jia. Gradsign: Model performance inference with theoretical insights. In ICLR, 2022.

[2] Guihong Li, Yuedong Yang, Kartikeya Bhardwaj, and Radu Marculescu. Zico: Zero-shot nas via inverse coefficient of variation on gradients. In ICLR, 2023. 

Limitations:
None

Rating:
3

Confidence:
4

";0
7w4RGjzd81;"REVIEW 
Summary:
This study explores watermarking large language models without reducing output quality. It introduces ""unbiased watermarking"" which avoids trade-offs in prior work. Two novel techniques - $\delta$-reweight and $\gamma$-reweight - are proposed along with an improved likelihood ratio test for detection. Risks of large language models and how unbiased watermarking enables responsible AI are discussed.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Introduces unbiased watermarking that maintains output quality. Prior work showed a trade-off, but an unbiased watermark avoids this. 
 
2. Proposes novel $\delta$-reweight and $\gamma$-reweight techniques that preserve output quality in experiments.  

3. Develops an improved detection method with a proven upper error bound, improving detection reliability. 

4. Concrete demonstration of the efficacy of watermarking techniques in maintaining the utility of LLMs for downstream tasks.


Weaknesses:
1. The paper lacks a thorough examination of the efficacy of the watermark detection method. It would strengthen the findings to provide more details on factors like detection accuracy, robustness to interference, and computational efficiency of the detection approach.  

2. The experiments only test the watermarking techniques on BART language models, without evaluating other popular LLMs like GPT models.

3. Conducting additional experiments using LLMs for other natural language tasks, beyond text summarization and machine translation studied in the paper, would provide a wider test case set and bolster the claims regarding the output quality preservation of the watermarking techniques.  

4. The paper does not discuss the resilience of the proposed watermarking methods against potential adversarial attacks or interference attempts. 


Limitations:
See the weakness section.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a modification of the watermark of Kirchenbauer et al. that ensures each next token prediction is marginally indistinguishable from a regular sample from the language model (whereas Kirchenbauer et al. bias some tokens over others). The main idea is to use inverse transform sampling to sample the text token (the paper also proposes a ""soft"" version of inverse transform sampling, i.e., \gamma-reweight), where the inputs to the sampler are a function of the context.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a neat, simple modification of the watermark of Kirchenbauer et al. that addresses a salient problem (i.e., the watermark of Kirchenbauer et al. does not preserve the original text distribution). The paper validates the proposed watermark with both theory and experiments.

Weaknesses:
The empirical validation of the watermarked proposed in the paper is somewhat lacking. For example, how robust is the watermark to paraphrasing compared to the watermark of Kirchenbauer et al.? Given Kirchenbauer et al. evaluate robustness to various kinds of paraphrasing attacks, it seems reasonable to expect the paper to do the same. It's also not clear what the purpose of Table 1 and Figure 3 is if the main claim is that the watermark *provably* does not bias the text distribution (i.e., shouldn't all the metrics stay the same?). Also, Section 4.1 seems needlessly abstract, since ultimately both watermarks are variations of inverse transform sampling (it feels unintuitive to think \delta-reweighting as a ""reweighting"" of a distribution, since it is essentially deterministic).

Limitations:
The paper does not discuss limitations (if any) of the proposed methods in meaningful detail. The Conclusion section would benefit from more detailed/concrete discussion and takeaways.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper discuss about the important problem of how to watermark the outputs from language models while keeping the model not impacted by watermarking. A perfect watermark scheme should be undetectable without prior information and should have no harm on the utilities of LLMs. This paper proposes some desired properties of watermark schemes such as **n-shot-undetectable** and **downstream-invariant**. The papers also gives the proof that there exists perfect watermark schemes. And guided by the proposed concepts, two reweighing watermark scheme are proposed, as well as corresponding methods for verification. The experiment results show the proposed methods have minor impact on the generated text quality. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. It's super important to have a theoretical framework to guide researchers to design better watermark schemes, and this paper is one of the pioneers in this direction. 
2. Considering those automatic metrics, the results shown in the paper, the proposed two methods look good.

Weaknesses:
Major concerns: 
1: The quality of the watermarked texts are only evaluated by automatic metrics. 
2: The results only comes from BART ( along with examples from OPT ).  

Minor concerns:
1: The word 'unbiased' is a little misleading.
2: It's a little hard to understand some paragraphs in this paper. 

Limitations:
Please read last section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel framework for embedding watermarks into Large Language Models (LLMs) without compromising their output quality. The proposed watermark is designed to be undetectable by LLM users. 

A general framework is put forth for incorporating this unbiased watermark into LLMs. This is achieved using two innovative and practical watermarking techniques: $\delta$-rewrite and $\gamma$-reweight.

Experiments conducted on summarization and machine translation tasks demonstrate that these watermarking techniques do not degrade the LLM's output quality, thereby substantiating the effectiveness and practicality of the proposed framework.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This paper tackles a critical issue in the Large Language Models (LLMs) field, which is the misuse of LLMs. The authors propose an unbiased watermark and a novel framework for its implementation. The experimental results clearly prove that this watermarking technique works effectively.

The paper's layout is good and easy to follow. It uses clear formulas and theorems that help explain both the problem and the proposed solution. Overall, this is a solid piece of work that contributes significantly to the field.

Weaknesses:
This paper does not provide a comparison with any existing watermark baselines. Such comparisons would be beneficial to demonstrate the relative performance of non-unbiased watermark techniques.

Only two types of downstream tasks have been evaluated in the study, which limits the generalizability of the findings. It would enhance the robustness of the results if a broader range of tasks, perhaps using a comprehensive benchmark, were tested.

Additionally, the evaluation of model output quality relies solely on automatic metrics. However, these metrics alone may not be sufficient to provide a comprehensive assessment of output quality. Including more diverse and possibly human-centric evaluation measures or LLM auto evaluator could strengthen the evaluation process.

Limitations:
N/A

Rating:
6

Confidence:
2

";0
j2EaW49Rk7;"REVIEW 
Summary:
In this paper, the authors propose a gradient-boosting algorithmic framework for rule ensemble learning, emphasizing the interpretability of produced rule set. Various gradient-boosting algorithms are reviewed in the rule-learning context, and the authors argue that a specific boosting algorithm, called fully corrective orthogonal gradient boosting (FCOGB), is particularly suited for rule boosting. The intuition is that existing additive rule-boosting procedures operate in a strictly greedy fashion - the weight of each added rule is fixed in later iterations. In contrast, FCOGB allows the weights of preceding rules to be adjusted in each later iteration, which may help to reduce the number of required rules (to reach a certain accuracy) and thus the cognitive complexity of the final rule set. Based on FCOGB, the authors derive the stepwise boosting objective function for single rule search, which is similar to existing gradient boosting objectives but with a different regularization term. The overall algorithm looks like the conjugate gradient method - in each iteration, the rule aligning best with the gradient in the orthogonal complement of the subspace spanned by previous rules is added. The authors demonstrate the effectiveness of FCOGB through experimental comparison with existing rule-boosting algorithms on classification, regression, and Poisson regression tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Applying FCOGB to rule learning, to the best of my knowledge, is a novel idea, and the authors provide a comprehensible justification for this choice. Figure 2 is helpful in understanding the difference between FCOGB and existing rule-boosting algorithms.
- How to search the optimal rule in each iteration is especially considered, which is a key step in rule boosting. The authors propose a strategy that exploits the nice structure in the boosting objective function to speed up the bound calculation in branch-and-bound search of optimal rules.
- The proposed algorithm is evaluated on a wide range of datasets and tasks. The authors provide a detailed analysis of the results. Figure 1 clearly shows that FCOGB can achieve a better accuracy-risk trade-off than existing rule-boosting algorithms.


Weaknesses:
- The presentation of the paper can be improved. For example:
  + In the ""Rule Boosting"" section, the ""Gradient boosting"" subsection mixes the description of general gradient boosting and the more specific rule boosting. This makes it hard to understand these objectives for readers who are not familiar with the rule-boosting literature. For example, obj_gb(q) = |g^T q|/||q|| is nonstandard in the general gradient boosting literature. It would be better to separate the general gradient boosting and rule boosting parts.
  + The ""Single rule optimization"" subsection assumes too much prior knowledge about the rule learning literature. I would suggest the authors merge this subsection with the ""4.3 Efficient Implementation"" subsection to make the paper more fluent and self-contained.
  + The authors should provide more details about the proposed algorithm, especially the BnB/beam search of a single rule.
- Lack of comparison with rule induction algorithms based on column generation, e.g., [30] and [b]. In the column generation approach, the weights of all added rules are also adjusted in each iteration when solving the restricted master problem, which is similar to FCOGB. I am interested in how FCOGB compares with this approach experimentally.
- The presentation of the prefix optimization problem is misleading. The authors claim that ""This function can be efficiently computed for many objective functions by pre-sorting the data in time O(n log n)"" in Section 3, but is this true for the objective function obj_{ogb}(q)?  The authors should clarify this point. I cannot immediately see how the optimal solution to (2) under this objective function is contained in the prefix of the data sorted by some (what?) criterion. If this is true, the authors should provide a proof or a reference to support this claim.
- There is a mistake in Lines 233-234.
- Missing references:
  + [a] Jonathan Eckstein, Noam Goldberg. An Improved Branch-and-Bound Method for Maximum Monomial Agreement. INFORMS Journal on Computing, 2012.
  + [b] Jonathan Eckstein, Ai Kagawa, Noam Goldberg. REPR: Rule-Enhanced Penalized Regression. INFORMS Journal on Optimization, 2019.
  + [c] Fan Yang, et al. Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach. NeurIPS 2021.

Limitations:
The limitations of this work are not explicitly discussed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach to gradient boosting of decision rules for interpretable machine learning models. By incorporating a
weight correction step and orthogonal projections, the method maximizes predictive gain per rule.
Their experimental evaluation on various classification, regression, and Poisson regression tasks confirms that the resulting rule learner
enhances the trade-off between comprehensibility and accuracy in the fitted ensemble. Moreover, it maintains a comparable computational
cost to previous branch-and-bound rule learners.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Originality: The paper introduces the first rule boosting algorithm that consistently optimizes the accuracy/complexity trade-off of produced rule sets. This represents a novel contribution to the field.
2. Quality: The research exhibits high quality as it adopts the fully corrective boosting approach, which entails re-optimizing all rule consequents in each boosting round. The study's rigorous algorithm development provides a strong foundation for the research, ensuring
the reliability and robustness of the findings.
3. Clarity: The paper explains the new objective function for selecting individual rule bodies, the corresponding efficient algorithm for cutpoint
search along with some other algorithm details. The clear explanations contribute to the overall clarity of the research.
4. Significance: The research demonstrates significant improvements over previous boosting variants in terms of the risk/complexity tradeoff.
The better risk reduction per rule and the affinity to select simpler rules contribute to the overall significance of the findings. Additionally,
the comparable computational cost to previous approaches adds to the practical relevance of the research.

Weaknesses:
In terms of the compared established methods, SIRUS [1] is the most recent work included in the analysis, published in 2021. However, it is
worth noting that some more recent publications, such as [2,3], are not included in the experiment section.
One limitation of the paper's presentation is the heavy reliance on text and equations, with less emphasis on the use of figures and intuitive
example case studies. This approach may hinder the reader's ability to grasp complex concepts and visualize the practical applications of
the proposed methods. Incorporating more visual aids, such as figures and illustrative examples, could enhance the clarity and accessibility
of the research.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Limitations:
A limitation of the study is that while it includes the most recent work, SIRUS [1], which was published in 2021, it does not incorporate some
more recent publications like [2,3] in the experiment section. This omission limits the comprehensiveness of the analysis and may overlook
potential advancements or alternative approaches introduced in these newer works.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents a new algorithm for learning rule ensembles and claims that these are interpretable, but does not present any support.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The proposed method is reasonable, but, in the context of other work in this area, not ground-shaking. The experimental evaluation is done well, but does not touch on interpretability.

Weaknesses:
There is no evidence that the learned rule sets are interpretable.
Rule complexity has not much to do with cognitive complexity.
The efficiency of algorithm is over-stated in the paper.

Limitations:
This is another paper that claims that rule ensembles are interpretable. No evidence is presented to that end, the claim is just derived from the fact that rules are, by themselves, interpretable. However, for example, random forests are well-known to be not interpretable, and they are also just a rule ensemble. In addition, the situation is even worse here, because in a random forest at least each individual rule is interpretable, and may be viewed as an explanation for all the examples it covers. In an additive boosting setting, this property also does not hold, because each rule corrects and refines predictions of previous rules, so rules can no longer be interpreted in isolation, but only in the context of all previous rules. Even a single example can not be easily explained by a gradient-boosted rule set, because one would have to understand the interaction of multiple rules.

The authors use the term ""cognitive complexity"" for something that is essentially the size of a rule set. Again, this is a complete misnomer, as the cognitive effort to parse a rule set does not only depend on the size of the theory. As explained above, there might be dependencies between rules, or rules may be considered in isolation (the latter having a much lower cognitive complexity). There are also factors such as the familiarity with the used concepts. For example, the cognitive effort required to read a page of text in your mother tongue is much lower than the cognitive effort required to read a page in a language that you are just learning, even though both, the content, as well as the syntactic length (essentially the author's measure of cognitive complexity) is the same.

It is a pity that they authors make such unfounded claims about intepretability, where they could simply present their work as an attempt to learn a simpler rule ensemble. As such, the work is reasonable, but also not great break-through. What they essentially propose (following previous work) is to re-optimize all weights once a new rule is added, and build an efficient algorithm around that idea. It gains a little in performance, as can be expected, but it is not great break-through.

The small advantage seems to be bought with an increase in computation time, which the authors interpret as ""in the same order of magnitude"" except for one case, where it is by a factor 26 slower. Actually, it seems to be the case that in most of the datasets, the algorithm is at least a factor of 2 smaller, sometimes worse. 

Minor comments:

Some of the numbers in Table 1 are obviously wrong (e.g., testing risks of 109.5 or 4.115 for XGB).





Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces Fully-Corrective Orthogonal Gradient Boosting (FCOGB), a novel algorithm aimed at facilitating interpretable rule learning. The study contends that existing rule learning algorithms often yield complex models that pose challenges for interpretation. FCOGB addresses this concern by generating simpler and more easily understandable models.
FCOGB is an extension of the widely employed gradient boosting algorithm, utilized for constructing predictive models. It employs a branch-and-bound search algorithm to identify the optimal set of rules that minimize prediction errors.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed method is supported by theoretical justifications and intuitive explanations using figures. Additionally, the paper proposes algorithms with computational complexity analysis to efficiently implement the method, demonstrating practical applicability.

Weaknesses:
Despite an increase in the required training time (takes several times longer computation), the generalization performance does not improve. If this weakness is addressed, I believe it would become a very strong paper.

(Minor comments)
- Despite Figure 2 being referenced on page 5, the figure is actually inserted on page 3.
- The scatters plot in Figure 3 are difficult to interpret due to overlapping points. Please set alpha (transparency).

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a framework of fully corrective orthogonal boosting. The main algorithmic difference here is the objective for each next weak model. It is the cosine between the gradient (which is orthogonal to previous weak learners by construction) and the part of the new model that is orthogonal to previous models. Authors motivate their work by the need of interpretable models, so they restrict themselves to the case of rules as weak learners. Also, they use a variant of b&b algorithm for optimal weak learner search instead of commonly used greedy construction in depth. 

Authors claim that this the paper proposes an algorithm for constructing shorter and more interpretable rules for Gradient Boosting of Decision Rules model. Experiments show that described method outperforms standard implementations of GB in case of using models of low complexity.

Although theoretical part is sound, practical questions are not thoroughly addressed or answered.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The main part of the paper is well-written, the terms, designations and ideas are clear. The proposed method is sound, reasonable and well described. The idea of orthogonal rule search in conjunction with fully-corrective goosting looks good. The theoretical part is described very well, the main formulations are correct, and the obtained contributions look important and are novel to the best of my knowledge. The proposed algorithm is justified and has the potential to compete with SOTA in the outlined formulation that refer to ""cognitive complexity"".

The main part of the paper is well-written. The terms, designations, and ideas are clear. 

The only point I did not buy is the Poisson loss defined in line 109, in my opinion, incorrectly (or unclear), because, formally, from that definition, its minimum is at $f(x_i)=0$ independently on $y_i$.

Weaknesses:
I have the following concerns about the research direction itself. Claimed advantage of ensembles of rules over ensembles of trees is their human interpretability. However, I cannot agree that the decisions a rule ensemble makes can be treated as interpretable. Particularly, I argue that in the domains where interpretation is important summation of even two terms is usually not interpretable for humans. Most critical decisions in such domains like medicine and justice, partly science and risk management are usually based on several binary factors, not a sum of dozens of rules. Where ensembles of rules are really used in practice?

Second, I am disappointed that the term ""cognitive complexity"" was left without any background. I would expect references to some papers using this metric or explicit statement that this way to estimate models' complexity is originally proposed in the current paper. Futhermore, I would expect some consideration of actual research in psychology domain that address the problem of cognitive complexity of calculations.

For example, we can see in ""Human knowledge models: Learning applied knowledge from the data."" Plos one, 2022, by E. Dudyrev et al., that a human decision is usually based on:
-	Boolean operators: OR, AND, NOT, and thresholded Boolean SUM (arithmetic sum of
Boolean variables, compared to an integer threshold)
-	At most four (Boolean) variables, where each variable is used at most once\

These ideas are rather far from the concept of sums of dozens of rules

See also:

Lemonidis C., “Mental Computation and Estimation: Implications for mathematics education research, teaching and learning”, 2015,

Marois R et al, ""Capacity limits of information processing in the brain,"" Trends in cognitive sciences, vol. 9, no. 6, pp. 296–305, 2005

Nys J. et al, ""Complex Mental Arithmetic: The Contribution of the Number Sense,"" Canadian journal of experimental psychology, vol. 64, no. 3, pp. 215–220, 2010.

At last, but not least, the experimental part spoils the impression of the work and requires improvements:

- First of all, I see no hyperparameter tuning step description (e.g. regularization terms for XGB, number of boosting rounds, length of decision rules) in the section on experiments. Are there any hyperparameters which may have a significant impact on the performance of FCOGB? Where they left ""defaulted"" or were they were tuned by a separate step of an algorithm?

- In the beginning of Section 5, it is mentioned that you use only 5 runs for each dataset with < 50 cognitive complexity (CC) limit. But then I see averaging over complexities between 1 and 50 in the description. What does it mean? I suppose that CC may alter in different runs but it is limited to 50, is it true? Or did authors perform exhaustive search of all possible CCs and averaged over them? If the first is true, I have a doubt that different model may have had different mean CC values, so that it is not quite fair comparison results. Is the second is true, then it is unclear why such an averaging can prove something

- It would be interesting to see the dynamics of quality with respect to increasing CC. In particular, some graphs that plots quality vs CC to see which algorithm uses the CC limit more effectively.

- It may be useful to provide comparison with other variants of fully-corrective boosting implementations since the quality gain may origin from described by this scheme only

- Time limitations should be discussed more in terms of time per CC point and pareto curves (time to achieve the desired quality)

- How should we interpret relatively low quality for regression problems?

- This paper addresses interpretability of trained decision rules, so it would be profitable to demonstrate a difference in the simplicity of interpretation for FCOGB rules and, e.g., XGB rules

Limitations:
I do not see any particular limitation of the proposed work

Rating:
7

Confidence:
4

";0
gSyjaunurQ;"REVIEW 
Summary:
- This study proposes a new framework to combine neural coding concepts of information transmission and probability density modeling.
- This framework is based on an even code principle where the output response density strives to be even, given some arbitrary input density.
- The authors show that this coding principle produces sensible bases for low-dimensional inputs, and orientation-tuned filters for natural image patches.
- While conceptually straightforward, it is unclear to me whether this study provides unique insight into sensory coding in neural populations.

UPDATE: Sep 1, 2023. I have read the rebuttal, and maintain my score (see details below).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The study is clearly presented.
- The concepts of max/min entropy on the output and input densities are conceptually simple to follow.
- The numerical experiments are reasonable.

Weaknesses:
- This paper begins with what seems like a false dichotomy of information transmission vs. sensory probability density modeling. Indeed, from a pragmatic point of view, how can one guarantee optimized information transmission without having a good density model of the signal to be transmitted? There exists literature in this area (see questions section), and the motivation/framing of this present study is concerning.
- There is a bit of a conceptual leap from 1 or 2 pixels to full image patches, with additional complexity and machinery introduced. The described rationale seems reasonable enough, but it is unclear whether the two-pixel orthogonal case can provide adequate intuition for the multi-dim case. Would a 2D non-orthogonal example be illuminating at all?
- Unclear to me whether these results, which rely on binary coding provides theoretical insight for real neural coding. Spikes are inherently binary, yes, but typically spike counts/rate are what is considered the informative variable in neural coding.

Limitations:
There was no discussion of limitations. Unclear to me what the drawbacks are of this approach compared to existing literature.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper presents a method for the representation of elementary natural images, based on the observation that classical studies in computational neuroscience focus mainly on methods to improve code efficiency, but that this could be complemented by a study of probability density modeling between neighboring pixels to improve image representation. This work consists in studying a coding principle based on a probabilistic representation and its formalization in a form of variational optimization. The paper presents the elementary method for a single pixel, then extends it to two pixels, and applies it to  small images extracted from natural images. This method is enhanced by a heuristic that allows  to formulate a cost function and thus derive an optimization algorithm. The results allow  to numerically validate this principle by deducing output statistics, as well as the emergence of local contour detectors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
A major strength of this paper is that it derives the image representation algorithm from fundamental principles of machine learning, particularly probabilistic representations. In this way, it rigorously defines the problem of establishing dependencies between the luminance values of neighboring pixels.


Weaknesses:
The first limitation of this paper is that it applies to very elementary signals, i.e. a pixel, a pair of pixels, or small images of dots. As the initial aim of the paper is to understand the computational functioning of the biological networks that underlie the efficiency of vision, this approach is extremely caricatural, and dismisses many fundamental aspects, such as the largely parallel processing of large images, the use of large neural networks, or the ability to process multimodal images, in color or in motion, or more generally hierarchical processing that can be forward, but also modulated by feedback signals. Finally, the results that have been obtained, for example for the detection of local elementary contours, are difficult to interpret quantitatively and seem very preliminary.

Limitations:
Finally, these questions about the paper reveal the main limitations of this work.

In particular, the introduction to the paper presents at length principles that seem very general, such as Shannon entropy, and the rest of the paper does not sufficiently highlight the novelties that are brought forward. This brings to light a main limitation of the paper, which is the fact that the propositions that are put forward are very ambitious, but the results are applied to very limited situations.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper explores the relationship between the information theory approach and the probabilistic generative model approach in the context of understanding neural coding. The author suggests that maximizing the information-carrying capacity of output channels and modeling the input probability distribution can be pursued as independent dual objectives. To investigate this hypothesis, the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. The resulting codes obtained for the images exhibit similarities to edge detectors and orientation-selective neurons in V1, akin to many efficient coding models developed over the past two decades.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The presentation is reasonably clear.  It is rather interesting that the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. 

Weaknesses:
While the idea that both information transmission and probabilistic modeling of the images should be taken into consideration simultaneously might be new,  and is sufficient to learn edge detectors and orientation-selective neurons, the author has not established it is a necessary condition. In fact, literature in the last thirty years (from Law and Cooper's to Olshausen and Field and many others)  that such codes can be learned based on either one of the criteria. 

It is surprising that the V1 neural codes were assumed to be sparse binary codes. What is the evidence?  The distribution of output values as shown in Figure 2a has not been observed biologically.  This brings the Even Code hypothesis into serious question. 


Limitations:
Societal impact not discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors studies simple of neural encoding. The question is whether two
distinct goals, accurate transmission of information and learning the
distribution of environmental stimuli can be achieved simultaneously. The
authors argue that yes, it can, using the key assumption of a uniformly
partitioned input space. The coding principle of the authors is finally applied
to image patches, where it yields edge-like features.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The author studies an important question, namely simple coding schemes that
  reproduce filters that resemble those of deep convolutional networks or parts
  of the visual system.
- The author develops intuitions in simple toy models before moving to
  applications on real images.
- The filters shown in Fig. 3 bear a striking resemblance to the filters of a
  trained VGG model (although I have some questions on the methodology, see
  below)



Weaknesses:
I found the article confusing to read in a few places. For example, early on,
the author states that ""maximising the rate of transmission"" is equivalent to
maximising the entropy of the output distribution $H_Q$. I would think that what
you transmission of information requires maximising the mutual information $I(X;
Q)$ between the distribution over inputs and outputs. (around eqs 1 + 2; note
that the notation is rather confusing here, using lower-case $p$ for the
distribution over input stimuli $x$, and capital $Q$ for the distribution over
output states $y$). Why are you maximising simply the entropy of the
distribution over outputs?

Similarly, in the section on the even code principle, I'm confused by the
question of how the IPU models the input distribution. The way I read Sec. 2,
the IPU is considered a function of the stimuli $y=f(x)$ -- in that sense, it
doesn't model the input distribution, we cannot sample from it. It can give a
more or less faithful representation of $x$, as measured for example by mutual
information if the mapping is probabilistic, 

As you then move on to learn two pixel distributions, I'm confused about your
use of MLPs. MLPs are powerful neural networks, but you seem to use them to
""learn"" to partition the input space into equal partitions - is this not
possible by just writing down a simpler model?

Given my trouble understanding the first few sections, I cannot competently
comment on the experimental results - while the filters obtained by the authors
do bear a striking resemblance to the filters of a VGG network, I don't really
understand how the author obtained them. Some additional clarifications would
therefore be more than welcome.


Limitations:
See above

Rating:
3

Confidence:
3

";0
9NzC3PjpAt;"REVIEW 
Summary:
The authors present a new reinforcement learning objective, dubbed Reinforcement Learning from Human Gain (RLHG), that explicitly incorporates an understanding of human performance with an intervention into the objective function. They show that training with this added component improves outcomes in a MOBA, both on the overall objective of winning and on subobjective related to satisfaction.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
They present an important research problem (ML systems working as collaborators) and make a good attempt at overcoming it.
They did experiments with human participants.
The domain they trained models in is non-trivial and showing results shows good ability


Weaknesses:
I don't believe the main claim. The authors say that by explicitly adding their more complex models of human wants/behaviours they can get better performance, but they don't compare against a model that attempts to optimize for those things directly. A perfect RL agent that has correct values for the different objectives should be able to learn the optimal policy without using their proposed more complicated methods. They only test against a model that is trained to win, and a model that is trained on short term value optimization. How does the method compare to another model trained with similar data and objectives? The lack of comparison makes this feel like they shows that PPO and deep learning work, not that there new methods are better.

Limitations:
The authors do not explain why their decomposition of the loss/training loop is _theoretically_ better than any other, they only provided some _empirical_ evidence to suggest it.
The authors only discuss the RL literature in their framing/discussion. In other domains, such as recomender systems, the issue of over optimizing for a single metric to the detriment of other objectives is a major area of research. Consider looking over some data science papers from KDD as a starting point

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method called Reinforcement Learning from Human Gain (RLHG) to effectively enhance human goal-achievement abilities in collaborative tasks with known human goals. The paper evaluates the RLHG agent in the widely popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting experiments in both simulated environments and real-world human-agent tests.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The problem setting considered by the paper is tightly connected to some real-world problems (e.g., assistive agents in MOBA games). 
Experiments are performed in a real-world application (Honor of King).

Weaknesses:
It is difficult to evaluate the major contribution of the paper (the two-step training process) because
1) the major contribution of the paper, as the authors claimed in the paper, is orthogonal to many of the complications in the paper (e.g., multiple agents, multiple goals, partial observability). These complications are not contributions and they make it hard to understand the contribution of the paper clearly. Maybe the authors added them because their experiments are in Honor of Kings?
2) probably because the paper focuses too much on these complications, the paper fails to explain why RLHG is a good idea and provides clear evidence. For example, why do the authors propose estimating primitive human performance rather than primitive human+agent performance? What is making estimating primitive human performance helpful? Can this idea be used in environments without humans?
3) the writing of the paper is unsatisfying. I was completely lost when reading the paper. Please see Questions for my questions about the paper.
4) in experiments, the new method achieved a worse winning rate compared with the baseline method. I can understand this performance drop given that there are improvements regarding other metrics. What should I learn from this indeterminate result?

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose Reinforcement Learning from Human Gain, an RL algorithm that explicitly optimizes for enhancing human abilities in cooperative human-AI settings. Given a predefined set of human goals, the main approach first learns a value network to estimate the primitive human performance at achieving said goals. Then, a secondary Gain network is trained to estimate the enhancement the human return receives under interactions with the cooperative agent. The cooperative policy is trained with a combination of a traditional agent value network and the proposed human gain-based value network. The authors test the RLHG framework in a cooperative game and find that across experiments with real humans, the RLHG agent is preferred over an agent without the human gain objective, despite having a lower overall win rate.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem setting is interesting and relevant, and the proposed solution of optimizing for human gain is intuitive — it makes sense that a cooperative agent should account for improvements in human behaviours, rather than having the agent directly optimize for its own reward or what it perceives human rewards to be (which may reduce human enjoyment and overall autonomy in a task, as noted by the authors).
- The experiments use a complex multiagent task and test both human models and real human participants. The results show a significant improvement in human preference for the RLHG agent across the predefined goals as well as various subjective metrics.

Weaknesses:
- The method seems sensitive to the choice of partner $\pi$ while collecting the primitive human episodes. If the initial partner is already very good, will its success be attributed to the human? And vice versa -- if the partner is very bad, would that lead to a false representation of the base human skill? The paper would be strengthened with additional studies on how sensitive the method is to different pretrained policies.
- The method relies on knowing the set of human goals beforehand. It would be interesting to have additional analyses of how the method is affected in the case where some of the goals are missing or misspecified, which is more representative of a real-world scenario.

Limitations:
The authors adequately discuss the limitations of this work. Additional discussion on societal impacts would be helpful, as these agents are trained to interact with people directly.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on the fine-tuning of a pre-trained agent to assist and enhance the performance of a given human model in achieving specific goals. The authors assume access to a human model and a pre-trained agent. The authors propose a two-step approach.

1. The human model's initial performance is determined by training a value network to estimate its effectiveness in goal attainment using episodes generated through joint execution with the agent.

2. The is trained agent to learn effective behaviors for enhancing the human model's performance using a gain network that estimates the improvement in human return when compared to the initial performance.

The algorithm is evaluated on a Multi-player Online Battle Arena (MOBA) game.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The idea of developing algorithms to assist humans to solve tasks is interesting and of great practical interest, and this paper makes headway in this direction. The authors conduct extensive experiments which includes evaluating using real players to test their algorithm in a game.

Weaknesses:

1. The algorithmic contribution in this paper appears to be relatively modest, as it mainly builds upon the existing Proximal Policy Optimization (PPO) approach. The authors introduce a gain function, which essentially computes an advantage by comparing it to another state-dependent baseline ( $V_\phi(s)$)
 
2. The assumption of having a human model is justifiable; however, the strong reliance on assuming knowledge of human goals, in my opinion, limits the direct applicability of this research (as acknowledged in the limitations section).

3. There is a lot of notational ambiguity in the paper (Section 2.2), which makes reading a little hard. For example, 
 
3a. Advantage function generally depends on state/observation and action. In this setting, the Advantage is independent of both. 

3b. Is V value of a state or the infinite horizon discounted reward? It is unclear as its used in both contexts. 

3c. G is used as return-to-go, which should be a state dependent function. 

4. I do not understand the exact need for the gain network. What would happen if in line 12 of the algorithm, you drop the - Gain(s) part? This essentially means that you are computing advantage with respect to the human primitive baseline. 

5. The authors have invested significant effort and computational resources in conducting their experiments, making it extremely challenging to reproduce or recreate such experiments due to the demanding compute requirements. Although this does not diminish the value of their work, it would be beneficial if the authors could incorporate simpler environments into their experimental setup. This addition would aid in evaluating the algorithm's performance and further validate its quality.

Limitations:
The authors discuss limitations and future work. 

Rating:
6

Confidence:
3

";0
9HJyRsgU13;"REVIEW 
Summary:
This proposes a novel surrogate for Bayesian optimization (BO), kernelized tensor factorization (BKTF). The authors claim that BKTF is able to model more complex functions (nonstationary, nonseparable) compared to additive and product kernel Gaussian processes. For inference, they leverage Gibbs sampling to do full Bayesian inference. They compare against BO with the regular Gaussian process surrogate and tree-structured Parzen estimators.

Soundness:
1

Presentation:
2

Contribution:
3

Strengths:
* The papers proposes a novel surrogate model, BKTF, for BO. I believe this is new, and as long as the authors can demonstrate the utility of BKTF, this will be a valuable contribution to the BO community.

Weaknesses:
* The proposed strategy uses Gibbs sampling, which is well known scale poorly with the number of parameters and correlations. Thus, I am concerned that this method's performance will fair poorly at even moderately higher dimensions and number of observations than those considered in this paper.
* On a similar note, unless I'm not mistaken, the method requires to infer the latent functions (or bases) $g_d^D$. This means inference needs to be performed at every BO steps. This contrast to GPs, where, even if one decides to do fully Bayesian inference, he does not to run MCMC at every step. Thus, the method comes with a reduction in flexibility. If the authors believe that their method can work with less expensive inference strategies, say, VI or MAP, then this should be demonstrated and evaluated.
* The paper claims that the experiments are ""extensive"" (line 73), but unfortunately, I find that the experiments conducted in this paper cannot be considered to be extensive in today's standards. See for example [1,2], which I would consider extensive. Furthermore, at this small scale / low budget applications, noise can very easily swamp the effects. Therefore, I would expect a lot more runs. Moreover, the hyperparameter tuning experiments in Section 5.2 are not reflective of real-world use cases. So these are gain inadequate to evaluate the real world performance of BKTF.
* Furthermore, the baselines are not enough. The research space for alternatives to BO surrogates has certainly been active, but here only the tree-structured Parzen estimator is considered. In fact, the paper mentions that BKTF here corresponds to a two-layer deep GP. Then, they should compare against deep GPs for an apple-to-apple comparison. The computational costs/scalability of DGPs would probably be comparable so this would be a more appropriate comparison.

Limitations:
Yes, in Section 6. However, I think the limitations I've discussed above could also be included.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Bayesian optimisation most commonly uses Gaussian Processes with the Squared exponential or Matern kernel as the surrogate model. The authors propose a new type of surrogate model, ""Bayesian Kernelized Tensor Factorization"" which introduces some advantages and disadvantages over Gaussian Processes. There seems to be prior work investigating these models for surrogate modding in general, and this paper is a followup applying these models within Bayesian optimisation in particular.

The papers introduces the model which models the data $\{x_i, y_i}$ from a black box function $y=f(x)$ as a sum of functions where each function is a product of 1 dimensional GPs, e.g. in 2D, leteach $g()$ be a 1D GP then 
$$
\hat{f}(x_1, x_2) = \sum_i g^i_1(x_1)g^i_2(x_2)
$$
which is a continuous analogue of how a matrix can be represented by it's SVD or eigen decomposition. This concept generalizes for multiple input dimensions (e.g. $g^i_1(x_1)g^i_2(x_2)g^i_3(x_3)g^i_4(x_4).....$) and the authors discretize the search space into a grid hence the implementation uses tensors.
 and it positive properties,
- to be able to model function with separability (where variables do not interact like in additive kernels) and
- non-stationarity.

In my interpretation, the thesis of the paper is that these properties are significant disdvantes and using a model that has these properties enables a performance improvement.

The disadvantage of the proposed model is that inference is no longer in closed form (a product of Gaussian random variables is not another Gaussian) hence an MCMC method is proposed to sample function values at points across the input space. While one could use a random discretization, (e.g. a latin hypercube, or a cluster around the current best point)  given the product structure of the surrogate model, there appear to be implementation benefits using tensor and matrix Kronecker products if the discretization is a fixed grid, discretize each dimension and build a a Cartesian product of each dimension to have a full grid.

A range of synthetic and hyperparameter tuning benchmarks show the new model performing favourably with standard GP using SE-ARD kernel.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- in theory, I really like the idea of the model, in particular, that any matrix can be decomposed by SVD, hints that any function can be decomposed into a sum and product over functions of each dimension, i.e. the proposed model is, in theory, a universal approximator? Although intuitively, both BKTF and SE-ARD can model any smooth non-stationary surface, discontinuities and kinks are not modellable.

- the inclusion of grid based GP methods is nice to see, and shows how much the grid decays performance compared to using the full unrestricted continuous space

Weaknesses:
# Technical
- the proposed Cartesian discretization, $S_D$, scales exponentially with input dimension, and presumably contains _a lot_ of useless points in empty parts of the search space would a random discretization (LHC or Gaussian around current best $x$) be so much worse? Given a random set of points $X_D$, it is trivial to compute a the joint prior density $P[f(X_D)]$ density and the likelihood is just Gaussian $P[y_i|f(x_i)]$, sampling function values can be done with any off-the-shelf MCMC method.

- I believe at least an additive GP should be a baseline. If non-stationary and non-separability are the main advantages of the BKTF model, presumably an additive GP with 2 kernels per dimension (matching CP rank=2 for BKTF) is an obvious baseline that has separability, such a baseline is exactly equation (7) but with sum-sum instead of sum-product. From this perspective, BKTF is simply an additive GP (that can only model separable variables) with a product over dimensions instead of a sum and this one change introduces a lot of engineering overhead (MCMC inference vs closed form inference) but also introduces more modelling power (separability can be modelled), given a high enough CP rank (CP rank =1 is just a product of 1D funs and is not separable).


- the related work consists of two paragraphs, the first is discusses prior work on  BKTF (and feels a bit repetitive), the second focuses on stochastic process models. I feel the novelty of this paper is in using another surrogate model inside BO methods, and given the large body of BO work, there have been many works acknowledginbg the limitations of SE-ARD and proposing alternative models that are not cited or empirically compared to
  - [Bayesian Neural networks](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+neural+networks&btnG=): 
    - Bayesian optimization with robust Bayesian neural networks, NeurIPS 2016
    - Scalable bayesian optimization using deep neural networks, ICML 2015
    - Multi-fidelity Bayesian optimization via deep neural networks: NeurIPS 2020
  - [Deep GPs](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+deep+gaussian+&btnG=)
    - Bayesian optimization using deep Gaussian processes, Arxiv

# Presentation

(in my personal subjective view) some changes to presentation would have made the paper far more accessible to me
- can ""CANDECOMP/PARAFAC"" be simply described as a tensor generalization of SVD to make it easier for readers?
- L119: ""we construct a D-dimensinoal Cartesian product Space"", can we just say ""grid"" like the authors do for the rest of the paper?
- L81: kronecker product is introduced and never used again in the main paper
- Section 3.1 would be much easier for me to understand if Eq (7) and (8) are introduced first, then next Section 3.3 (model inference) describes the grid and Equation (6) and MCMC details and the justification for the grid.
- Section 3.2 is nice to mention but for me distracts from the main paper hence would be much better suited to the appendix.
- L192: given a mean and uncertainty, this seems to be standard UCB, why is ""Bayesian-UCB"" defined?

Limitations:
- as above mentioned comment, discretizations in higher dimensions are generally considered bad practice, in particular, ungioded/naive grid discretizations that include many dead points.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents a new surrogate model called Bayesian Kernelized Tensor Factorization (BKTF) for Bayesian Optimization (BO).The BKTF model approximates the solid in the D-dimensional space using a fully Bayesian low-rank tensor CP decomposition. It uses Gaussian process (GP) priors on the latent basis functions for each dimension to capture local consistency and smoothness. This formulation allows sharing of information not only among neighboring samples but also across dimensions. The paper proposes using Markov chain Monte Carlo (MCMC) to efficiently approximate the posterior distribution. ). The paper demonstrates the effectiveness of BKTF through numerical experiments on standard BO test functions and machine learning hyperparameter tuning problems. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. One of the significant strengths of the paper is the novel and reasonable solution of incorporating the idea of tensor decomposition into Bayesian Optimization (BO). This approach allows for a more efficient and effective representation of the D-dimensional Cartesian product space, enhancing the performance of BO. The adoption of tensor decomposition represents a significant advancement in the field and demonstrates the authors' innovative thinking.

2. The usage of two-layer GPs is impressive. This approach is clever as it allows for the sharing of information among neighboring samples and across dimensions, enhancing the model's ability to capture local consistency and smoothness.  



Weaknesses:
1. The cost of several cascaded full GPs may be high, especially for cases with large nodes (refer to ""coordinate"" in paper) at some dimension. More discussions are encouraged on the scalability analysis or the possible solutions, such as sparse GP,  to reduce the cost. 
 
2. As the tensor rank R  is always a crucial hyperparameter for tensor decomposition. I'm curious about how the rank setting could influence the BO. It will be great if the authors could give some comments or results on why R=2 is sufficient for the model setting.   

Limitations:
See Weakness

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents a surrogate based on tensor decompositions for approximating complex functions, allowing for Bayesian-style maximization.
Numerical experiments show the (slight) superiority of this model over classical Bayesian approaches. However, a limitation of this approach is the small dimensionality of the target functions and the need to use a discrete grid.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

- The proposed algorithm uses a very small budget to find the maximum of complex functions (gradient-free, multimodal).

- A good potential for expanding and improving the proposed algorithm.

- This article uses a tensor approach for machine learning problems

- The presented new algorithm, in my opinion, has quite a lot of possibilities for improvement, and the article itself is complete.

Weaknesses:
- A small number of numerical examples.

- Final accuracy in Fig. 3b better, but very close to the accuracy of the other methods with which the comparison is made.

- No comparison with non-Bayesian methods of finding the maximum.

Limitations:
Small dimension of functions for which the maximum is searched for. This is due to the fact that AF has to be found by unrolling the tensor from CP to the full format.
Thus, one of the main advantages of the CP tensor format, related to overcoming the curse of dimensionality, is not used.




Rating:
7

Confidence:
4

";0
DS4rKySlYC;"REVIEW 
Summary:
The paper establishes a formal link between a (pre-trained) self-attention layer and a causal graph underlying a sequence of symbols. Specifically, it shows that self-attention learns to represent pairwise associations for which there exists a linear-Gaussian Structural Causal Model that has the exact same pair-wise associations between its nodes. The paper then proceeds to derive an algorithm for recovering the causal structure of the SCM solely from the weights of a pre-trained attention layer

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The Attention-Based Causal-Discovery (ABCD) method, which is developed for learning an equivalence class of the causal graph that underlies a given input sequence is reasonable. It relies on partial correlarion CI-testing, which is sound.
Then the paper proceeds to apply ABCD to reasoning about predictions by generating causal explanations from pre-trained self-attention models, such as BERT (8). Specifically, the algorithm produces explanations through selecting a subset of the input symbols that are claimed to be solely responsible for BERT prediction. ABCD is shown empirically to be much stronger than the common approach of using the attention matrix to learn about input-output relations for providing explanations. 

Weaknesses:
The Attention-Based Causal-Discovery (ABCD) method relies on an unsupervised training assumption. This restricts applicability to only models of BERT type, you cannot deal e.g. with decoder-only models. 

In addition, it is extremely troubling that the authors do not perform any comparative evaluation against standard methods in the field. They limit their examination to standard self-attention and a variant of it. Even worse, they have avoided use of any standard benchmark in the field. 

Eventually, it is not clear how useable the generated causal explanations are, and how easy it is to exploit them in an application setting.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
The ABCD paper starts off by motivating the need for understanding the complex dependencies that underly current Transformer architectures, pointing to causality as the missing puzzle piece. Following brief introductions of both SCMs and Self-attention, the authors then provide covariance matrix derivations for both linear Gaussian SCM and attention such that they are able to equate them eventually. By introducing another assumption on how the attention was trained, they propose a theorem that provides a clear connection between causal discovery and attention in that one can make CI tests based on the attention matrix essentially. Finally, the authors propose a complete pipeline for going from e.g. a pre-trained Transformer model to causal explanations based on their ABCD approach computed from the last attention layer. As put by the authors, this can be considered as a ""one-shot"" causal discovery paradigm.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. A unique perspective and valuable contribution to the causal discovery literature in that it expands the horizon on what kind of connections can be established between existing models to foster new understanding of said models. The derivation of the covariances and then equating them is both simple and effective, and then Thm.1 seems like a nice result for understanding the final correspondence between attention and CI-testing for constraint-based causal discovery.

2. Presenting a complete pipeline is a great bonus (since it is a difficult task) and shows an immediate practical approach to the previously presented theory. The explanation approach is convincing and practically applicable since the theory justifies the use of the attention matrix.

Weaknesses:
1. While the causality assumptions Markov and faithfulness can be considered standard, as soon as moving to the realm of data that nowadays Transformers are confronted with we run the risk of violating them. Furthermore, the assumption of a linear Gaussian SCM is quite restrictive and should be highlighted more, especially under consideration of Transformers. Finally, Assumption 1 right before Theorem 1 is only vaguely comprehensible IMHO. I'd wish for the authors to be more specific about what property is required for the deepest attention layer such that the equating to CB-CD works. This could be done through formalization of that assumption or if not possible, then through an example.

2. Thm.1 is incomplete writing-wise i.e., sentence cuts off at end of page 4. Starting from p.4 the paper generally lend the impression of being not as carefully written as the pages before. Please consider a careful readthrough post-submission.

3. Missing discussion of explanation methods based on causal graphs. Two very different approaches include Causal Shapely Values by Heskes et al. 2020 and Causal Explanation of SCM by Zečević et al. 2021. The first is a rather classical approach, where numerical attributions are being computed and in Sec.5 they present an approach for causal chain graphs (a special case like your Linear-Gaussian case), and the second discusses a traversal algorithm for generating immediate textual representations based on more general SCM (although only showcased for a linear SCM setting).

Limitations:
No concerns here.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this work, authors, interpret self attention in transformers as linear gaussian SCM over output embeddings. 
SCM over input embeddings is same as SCM over output embeddings, since deepest attention layer is trained to predict input symbol.
Existing conditional independence based testing algorithm is applied on the weights of the deepest attention layer of the pretrained transformer in order to estimate causal structure. 
Furthermore, the authors demonstrate the applicability of the proposed approach in deducing causal explanations for attention based recommendations.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Authors proposed novel way of viewing self attention in transformers as weight matrix of linear gaussian SCM
Pretrained model is used to obtain causal structure, which would be of great use to gain causal perspectives for existing model
Estimating causal structure from the attention layer and using it for causal explanantions is promising.

Weaknesses:
Theorem1, in main paper has few words missing, however, it's available in supplementart material

Limitations:
1) The accuracy of the proposed method as explained by the authors is dependent on the prediction of the transformers and the conditional independence testing accuracy
2) Authors assumed learnt graph will be faithful, since exogeneous variables V under central limit theorem will have identity covariance, however in practice the faithfullness assumption will be violated, this will act as one more reason of inaccuracy

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper proposes Attention-Based Causal Discovery (ABCD), a causal interpretation for the attention matrix of transformers (e.g., LLMs like BERT). The authors suggest CLEANN, an algorithmic approach that first finds a causal structure of the input symbols (e.g., tokens (words) of a sentence as input to BERT). This causal structure is unique to each input (sentence). This can then be used to get causal explanation for different taks (in their experiments classification or recommendation) based on the representations of the input sentences (e.g., the word ""but"" and ""bad"" in the input sentence caused the negative sentiment classification).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Thank you for your submission. I enjoyed reading your paper. 

Clarity & Presentation: I found the paper to be clearly structured and written. On clarity and presentation, I would like to positively highlight the introduction to each (sub) section gave an orientation that helped to clearly guide the reader, what the next chapter is about and how it fits in the bigger picture. The figures (e.g., Fig2a, Fig 4) helped a lot to understand the method/pipeline.

Novelty: I found the causal interpretation and its application to causal explanation of predictions apealing and novel. Finding a causal explanation to downs-stream tasks of LLMs for example, can be very helpful. However, I am not an expert in the field of LLMs and I do not feel qualified to fully assess the novelty and hence the contribution of the work in the general field.

Evaluation: I found the experimental section clearly structured and interesting to read.

Weaknesses:
I have a few comments for improvements. 

* Section 5, Table 1: What do the bold numbers indicate, what the +- (i.e., over how many runs did you do it, were these runs of CLEANN or different data samples)? This would be helpful to comment in the caption. Am I right in understanding tha CLEANN does yield lower average explanations, but the numbers suggest that it is not a significant? Over how many runs did you do it?  

* Section 5, Figure 2 (b, c): I find it hard to understand and interpret Figure 2(b,c) despite your explanation in ln. 255ff. Also, the labels are very small and hard to read. 

* Section 5, Figure 3: I am missing an explanation/interpretation of Figure 3 in the text. Did I overlook it? 

* I am missing a related work section. 

Minor: 
* Section 1: While I enjoyed reading the empirical evaluation, I found the causal explaining application could be stronger highlighted in the introduction. 

* Section 2.1, line 67: While I appreciate the introduction to SCMs, I was a bit confused about the Causal Markov and Faithful properties at the end of the section. They seem very disconnected of the rest of the text. It would help, if there would be a short explanation, why they are stated / needed for the paper. 

* Section 2.2 is very brief. In the light of Section 3 (ln. 100 ff.), where the self-attention mechanisms is further detailed, it makes sense. However, it may be helpful to just point to that at the end of the section 2.2. 

* Section 5: Caption of Table 1 - by review length, you mean the length of the sentence, right? It would have helped me  to point that out. 

* Section 5: I find it weird that Fig. 6 is discussed before Fig. 5.  A natural way would be to discuss them the other way around.

* I got a bit confused by the difference between ABCD and CLEANN. Could you kindly clarify?

Typos:
* ln 222 ""(1) In addition, it"" ->  ""(1) It"" 
* ln 262 ""influences on the "" -> ""influences the""



Limitations:
Section 6 Discussion:  ln. 305 mentions ""under certain assumptions"", but these assumptions are not re-stated here. It would be helpful to have them restated. In this regard, I am missing a discussion of the limitations regarding the (causal) assumptions. 

I am also wondering, how robust your causal explanations are? I believe, when providing causal explanations to individuals, it is important to discuss any limitations regarding the sensitivity to assumptions and to comment the potential negative societal impact, if assumptions are not met.


Rating:
5

Confidence:
3

";1
sXD4idbnBw;"REVIEW 
Summary:
This paper propose a differentially-private local stochastic gradient descent both for centralized and distributed settings. The authors argue that the proposed method has less number of clipping and in turn produce less clipping bias compared to its counterpart DP-SGD which do not involve local steps. They also show that DP-LSGD converges sublinearly to a ball of the optimum, which is claimed to be faster than that of DP-SGD, and exhibit a better utility-privacy tradeoff.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The authors characterize the convergence performance by regarding clipping noises as biased noises and assuming that the incremental norm of local update be bounded, which is otherwise not easy to deal with.
- The authors prove that the proposed DP-LSGD converges faster than that DP-SGD and empirically show that it also has a better utility-privacy trade-off.
- The paper is technically sound and well organized.


Weaknesses:
- Assumption 4.1 and 4.2 seem restrictive to the reviewer. In particular, Assumption 4.1 seems not practical in the sense that one can not ensure the boundedess of the incremental norm of $\nabla w$ without knowing in advance the basic convergence of the algorithm (note that the algorithm may diverge, making $\nabla w$ unbounded); Assumption 4.2 is assumed to be hold for any value of w instead of the optimum w*, which is more common to be adopted.

- Theorem 4.1 and 4.2: the authors claim that DP-LSGD enjoys faster convergence to a neighborhood of the global optimum/ saddle point than DP-SGD; however, local sample-level differential privacy is guaranteed for DP-SGD, but not for DP-LSGD. For a fair comparison, each client for DP-LSGD should clip the calculated gradient and add the DP-noise at each SGD step in local update to satisfy the local sample-level differential privacy, which will inevitably degrade the convergence performance of the algorithm. In that case, what are the advantages of DP-LSGD compared to DP-SGD and does DP-LSGD still produce less clipping bias than DP-SGD?

- The selection of many parameters such as B, \eta and c lack of intuition; also, the experiment does not corroborate the theoretical result very well; for instance, the clipping bias captured by $\mathcal{B}$ is independent of $K$ (c.f., Theorem 4.1) and thus can not reduced with increasing value of $K$, which is inconsistent with the claim that DP-LSGD Produces Less Bias.


Limitations:
please refer to weaknesses and questions.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper studies DP-LSGD and compares its performance with DP-SGD. The paper first provide the convergence result of FedAvg under the bounded variance assumption (Theorem 3.1 and 3.2) and provide the convergence analysis of DP-LSGD-GC under the bounded gradient assumption 4.1 and similarity assumption 4.2. for both convex and non-convex cases. The results imply that using multiple local updates, DP-LSGD converges faster to a neighborhood of the stationary point. Through numerical experiments, the paper demonstrates that DP-LSGD converges faster than DP-SGD with the same privacy budget and ""communication"" iterations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: this paper provides a novel analysis of DP-LSGD, which considers both clipping and DP-noise. The resulting convergence rate improves upon the existing FedGD algorithm. 

Significance: This paper provides a DP algorithm (DP-LSGD) that outperforms DP-SGD with faster theoretical convergence to a neighborhood of stationary points.

Clarity: This paper provides a clear statement of the theorems, assumptions, and adequate numerical justification for the assumption used in the proof.


Weaknesses:
Assumption 4.1: This assumption assumes that the clipping error is bounded, which might be too strong. In Fig 1 (a), it seems like $\Phi$ is increasing as $t$ increases. Such an assumption simplifies the analysis of gradient clipping; Thus, it weakens the significance of the paper a bit.

Comparison with FedAvg: In general FedAvg considers local SGD updates, while the analyzed DP-LSGD algorithm considers local GD updates. Therefore, such a comparison is unfair. [R1] also provides the convergence rate of FedAvg, which matches the rate in this paper. Therefore, the convergence part without clipping is hard to be said to be an improvement.

It is unclear whether the numerical comparison is fair or not. It is hard to decide if the reported result for DP-SGD matches the SOTA results (e.g. in [R2]). The authors should report how the hyper-parameters are chosen and whether they are optimal for the algorithm.

The theoretical result suggested that $c = \Theta(\eta)$ and at the same time $B = O(c)$. However, it is unclear if these two results can be satisfied at the same time. The authors should also conduct numerical justification on different choices of $\eta$ and $c$ and report the corresponding $\Phi$.


[R1] Glasgow, M. R., Yuan, H., & Ma, T. (2022, May). Sharp bounds for federated averaging (local SGD) and continuous perspective. In International Conference on Artificial Intelligence and Statistics (pp. 9050-9090). PMLR.
[R2] De, S., Berrada, L., Hayes, J., Smith, S. L., & Balle, B. (2022). Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650.

Limitations:
The authors have addressed the empirical efficiency limitation of the proposed algorithm, which I believe is the largest limitation.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposed a unified analysis of the convergence of (DP)-Local SGD, which covers (DP) parallel SGD as a special case with K=1, for both convex and non-convex optimization. Under this unified analysis, one can identify error effects due to non-iid objectives, clipping, and DP noises and the convergence rate to the error neighborhood.

  

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1) The introduction section provides a brief yet insightful summary of related subjects including ""relation between local SGD and parallel SGD"",  ""sensitivity and privacy analysis methodology of DP"", ""effect and convergence limitation of clipping"" etc.

(2) The unified framework that covers both local SGD and parallel SGD is attractive and makes sense at a high level.

Weaknesses:
(1) The problem setting of this paper assumes each sampled worker is running gradient descent instead of stochastic gradient descent as the full gradient is available in equation (1) though later an additive stochastic noise is added after the aggregation of local updates in equation (2) and (3).   This problem setting is much simpler than the standard setting of local SGD as less divergence across involved local workers is introduced.  The comparison between the analysis from this paper, e.g., Thm 3.2, and the state-of-the-art is no longer fair.  (All the state-of-the-art compared in this paper considered stochastic optimization.  And it is well known that GD for deterministic non-convex has O(1/T) convergence while SGD for stochastic non-convex only has O(1/\sqrt{T}) convergence.) 


(2) The assessment in line 313 that DP-LSGD converges faster with O(1/T) than DP-SGD corresponding to K=1 with O(1/\sqrt{T}) is unfair as the convergence rate from Thm 4.2 requires K=\Theta(T) such that the overall computation/iteration is TK = O(T^2) to attain an error decay like 1/T.  This is effectively the same O(1/\sqrt{S}) convergence where S is the number of computation steps/iterations. 


(3) The new assumption in Assumption 4.1 that involves \Phi in Definition 4.1 does not seem much different from a bounded 2nd-order moment assumption as \Phi_i measures how much the norm of update is larger than the clipping threshold.  (Given that a bounded 2nd-order movement further implies a first-order moment by the inequality E[||X||] \leq \sqrt{E[||X||^2]}, I don't see how this new assumption can relax the widely used bounded gradient assumption in the literature.) Could the author discuss whether the new assumption strengthens or relaxes the standard assumptions? 




Limitations:
I do not see any potential negative societal impact of this paper.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper focuses on Differentially-Private Local SGD (DP-LSGD), and studies its advantages over the foundational technique of DP-SGD. In particular, the authors show why DP-LSGD provides higher clipping efficiency and less clipping bias compared to DP-SGD. The authors start by showing a convergence analysis on the released iterates of LSGD under perturbations and a bounded variance assumption on the stochastic gradients. Next, they generalize the results to DP-LSGD, and show that DP-LSGD has a faster convergence rate near an optimum point compared to DP-SGD. Lastly, they show that DP-LSGD behaves as an efficient variance reduction of local update, and enables more efficient clipping compared to DP-SGD.


Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The authors focus on the important problem of improving the privacy-utility trade-offs for DP Learning.


Weaknesses:
1. The paper contains many theoretical results (Sections 3-5), and I have not been able to verify the correctness of any of the proofs in the Appendix, but after reading the paper it is not even clear to me whether the proofs use any techniques/ideas that are novel (and might be of independent interest), or use methods from prior works to obtain novel results for (DP-)LSGD.
2. The empirical evaluation is very limited, focusing only on image-classification settings (CIFAR10 and SVHN datasets). Given that the focus of the paper is on (DP-)LSGD which is a building block of (DP) Federated Learning, it might be useful to have experiments on FL benchmark datasets, e.g., StackOverflow, EMNIST, etc.?


Limitations:
Yes.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This submission studies the Differentially-Private Local Stochastic Gradient Descent (DP-LSGD), and shows that  DP-LSGD with multiple local iterations can produce more concentrated local updates and   less clipping bias compared to DP-SGD, assuming that the stochastic gradient is of bounded variance.  The main contribution of this submission is to show that DP-LSGD has a faster convergence rate  compared to DP-SGDThe authors also add the experiments to  show that
DP-LSGD produces a better  utility-privacy tradeoff  than DP-SGD.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This submission develops the connections between the clipping bias and the second moment of local updates, which is something new in the research of differentially private optimization.
2. This submission shows that DP-LSGD can converge faster compared to regular DP-SGD.
3. The experimental results ( the comparison between DP-SGD and DP-LSGD) in this submission look convincing, and this paper is well-written.


Weaknesses:
1.The implementation inefficiency (local update in parallel at a cost of large memory) is a minor issue here.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors provide a unified analysis of the clipping bias and the utility loss in privacy-preserving gradient methods for centralized and distributed setups. The conclusion shows that LSGD behaves as an efficient variance reduction of local update, where multiple local GDs with a small learning rate cancel out substantial sampling noise and enable more efficient clipping compared to DP-SGD.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The authors build the connections between the clipping bias and the second moment of local updates. This initializes a new direction to systematically instruct private learning by connecting the research of variance reduction in distributed optimization.

2. The authors conduct analysis on both convex and non-convex ERM problems with a fairly mild assumption of the bounded stochastic gradient variance.

Weaknesses:
1. I understand this is a theoretical paper, but the authors should claim the experimental setup more clearly. It is unclear whether the setting is IID or non-IID. The authors should illustrate the consistency of the empirical support under both IID and non-IID settings, which are the most concern to the FL community.

Limitations:
The authors discuss the limitations of their work in the last section.

Rating:
5

Confidence:
1

";0
VvnfMeC3gQ;"REVIEW 
Summary:
In this paper, the authors propose a new backbone RevColv2, which is suitable to the MIM pretraining and could learn disentangled representations during the pretraining. The strong experiment results show its effectiveness.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
Please refer to Questions

Weaknesses:
Please refer to Questions

Limitations:
Please refer to Questions

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposed novel architecture to explore disentangled representations with masked image modeling. Different from previous MAE-like methods, this paper design a unified network and do not drop the decoder in downstream task. This paper showed that the disentangled representation is learned in different network levels. Experiments are done on ImageNet, MS-COCO and ADE20k for base and large model sizes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of combining MIM with disentangled representation learning is novel.
2. The idea of keeping the entire autoencoder tackles the problem of inconsistent representation between pre-training and fine-tuning.
3. The performance in downstream vision tasks is competitive.

Weaknesses:
1. Figure 2 is misleading, the pre-training target on ImageNet-1k is single MIM or combined MIM with image labels?
2. The network parameters in Table 1 and Table 2 is not consistent. E.g. for base-size, 101M in Table 1 and 88M in Table 2.
3. As described in line 204, the initialized weight for semantic segmentation is ImageNet-1k classification fine-tuned model and not MIM pre-trained model. It is not fair to compare it with a bunch of MIM pre-trained models.
4. The DropPath op seems to be conflict to the idea of disentangle representation in each level.
5. In the supplementary materials, the linear probing experiments is based on the bottom-up columns which is conflict to the idea of keeping the entire autoencoder.
6. This paper had several typos and grammar mistakes. E.g.
line 1: per-training -> pre-training
line 13: performances -> performance
line 15: intermediately -> intermediate
line 66: per-trained -> pre-trained
line 211: fine-turning -> fine-tuning 
7. There are some factual errors in this paper. E.g. in Table 2, the target of ConvNext-B is label not pixel and the target of BEiT-L is DALL-E not pixel.

Limitations:
Yes.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes to keep the entire auto-encoder architecture during both pre-training and fine-tuning based on RevCoI. It contains bottom-up columns and top-down columns, and the information is reversibly propagated and gradually disentangled between them. Better results are achieved on ImageNet-1k and downstream tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Good motivation to keep the same structure for both pretraining and finetuning. 
2. Better results are achieved compared with RevCol.

Weaknesses:
1. This paper is a little bit hard to follow, and I do not think the figures help a lot for understanding this paper. Maybe better visualizations/figures are needed. 
2. Tiny and small size models are also suggested to exploit. Or the authors needs strong arguments why they did not do so. 
3. Some papers achieves better results than this method, but they did not present them and compare with them.

Limitations:
None

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a revised version of RevCol, referred to as RevColV2, which is applicable for MAE training. RevColV2 consists of an encoder-decoder framework. The encoder is the same as RevCol, while the decoder uses reversed column connections. The paper also uses a unified fine-tuning framework utilizing a decoder for downstream tasks. RevColV2 demonstrates impressive performance on diverse vision tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- RevColV2 decoder presents an interesting design with plenty of novelty. In particular, reversed column connection between enc-dec is an innovative architectural approach for MAE training.
- RevColV2 achieves meaningful performance improvements on diverse tasks.
- Multi-column architecture is different from the mainstream of transformer architecture, which enhances the novelty of RevColV2

Weaknesses:
- Basic component of the architecture is the same as RevCol. RevColV2 is an improved version, not a new architecture. Although it is interesting, the contribution of V2 paper is limited.
- The paper's contribution is similar to ConvNeXt V2, enhancing existing architecture with MAE training and minor architecture revision. I think shedding light on existing architecture can be a contribution. But, similarity with ConvNeXt V2 paper might decrease the impact of this paper.
- There are no reports for the throughput or latency. FLOPs numbers for detection and segmentation are omitted. I think throughput and FLOPs are necessary for architecture research in 2023. I strongly recommend authors to report those numbers.

Limitations:
.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces ""RevColv2,"" an advancement over the RevCol model, enabling compatibility with MIM training. The authors propose the new architecture comprising a bottom-up reversible column encoder and a top-down decoder, facilitating MIM compatibility while preserving disentangled low-level and semantic information throughout the network. The authors conduct experiments on ImageNet, detection, and segmentation tasks and the results show it achieve strong results on ImageNet and ADE20K. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
•	The idea of RevColv2 to make RevCol compatible with MIM and the design of top-down column decoder is intuitive. It’s nice to see some downstream tasks could benefit from both pre-trained encoder and decoder.

•	The Illustration of key motivation of maintaining disentangled low-level and semantic information is clear and further verified by analysis in Figure 3.


Weaknesses:
* The results on ImageNet-1K are strong; however, there is a lack of speed comparison with other methods. It would be valuable to assess the runtime speed for both pre-training and fine-tuning stages, particularly considering the impact of the reversible column network design on speed.
* One benefit for reversible networks is memory-saving (at the cost of some speed). It would be beneficial to discuss whether this holds true for RevColv2. Exploring the trade-off between memory usage and speed for RevColv2 will add valuable insights to the paper.
* In figure 2, the sequence length is different for the same level in the encoder and decoder. It seems unclear about the strategy used to handle this dimension change when connecting the encoder to the decoder.
* For dense prediction tasks, RevColv2 utilizes both encoder and decoder pre-trained weights. To verify the effectiveness of this approach, one missing ablation is to compare to a variant that employs the same encoder and decoder during downstream fine-tuning but only utilizes the encoder's pre-trained weights while initializing the decoder weights randomly.
* While the COCO detection results for the base model are strong, the performance of RevColv2-L appears to lag behind ViTDet-L using Mask R-CNN (54.0 vs. 55.6, citing the results from the ViTDet paper). Additionally, no results for RevColv2-L with Cascade Mask R-CNN are reported. It would be insightful to discussions the scaling results for RevColv2 on detection and provide some intuition on the potential reasons behind the observed performance differences.


Limitations:
No limitations have been discussed. Suggestions on potential limitation that worth to discuss could be about the speed issue and further scaling the model.

Rating:
5

Confidence:
4

";1
axRMkinASf;"REVIEW 
Summary:
This paper focuses on the channel simulation problem, which finds applications in stochastic lossy compression. In contrast to the importance sampling approach A* coding (Flamich et.al. 2022) for channel simulation with Poisson processes, this work adopts a rejection sampling method. They demonstrate that the standard sampling approach may yield suboptimal code lengths. As an alternative, the authors propose a novel greedy rejection sampling algorithm called the Greedy Rejection Sampling (GPRS) algorithm. This algorithm composites the density ratio $r = \frac{dQ}{dP}$ with an invertible function referred to as the ""stretch"" function $\sigma$ that utilizes the temporal structure of the Poisson process without changing the target distribution.  Theoretical findings establish the correctness and optimality of the proposed GPRS algorithm in terms of code length. Furthermore, the expected runtime of GPRS matches the runtime of standard rejection sampling.

In addition to the vanilla GPRS, the authors introduce parallel and Branch-and-bound variants to further enhance the runtime of the algorithm. Specifically, the Branch-and-bound GPRS demonstrates a provable runtime improvement over A* coding when the density ratio exhibits unimodal characteristics. To validate the theoretical results, the authors conduct several toy experiments.






Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Mathematical derivations look rigorous. 
- This paper is well-written and easy to follow.
- Theoretical results seem to be compelling. The authors successfully demonstrate, through rigorous analysis, that the GPRS algorithm outperforms both standard rejection sampling and A* coding in terms of either runtime or code length in some cases.

Weaknesses:
- Some baselines and closely related works are not compared theoretically or empirically (See the first two bullets in Questions).

Limitations:
N/A

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper proposes a new relative entropy coding algorithm for compression without quantization. Compression without quantization is an exciting line of research that tries to eliminate training-test mismatches in learned compression by avoiding discrete representations altogether. Consequently, one can losslessly compress data in a continuous latent variable model, such as a VAE, without quantizing the latent representation. The downside of existing algorithms for relative entropy coding is that their runtimes are intolerably slow. The paper makes a big step towards more runtime efficiency by designing new efficient rejection sampling approaches for transmitting samples from a variational posterior, especially in cases where the likelihood ratio between prior and posterior is uni-modal. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
+ Relative entropy coding is a somewhat under-appreciated but up-and-coming field of research at the intersection between information theory and machine learning. There are only a few papers in this domain. (For example, recent advances made compression in diffusion models possible [Theis et al., 2022].) 
+ The paper significantly improves runtime efficiency for these algorithms, paving the way toward its scalable deployment. Compared to runtime complexities exponential in the Renyi divergence, it achieves a runtime *linear* in the KL divergence between the target and proposal distributions. Notably, the involved algorithm is conceptually much simpler (and in practice faster) than relative entropy coding with A* coding, which achieves the same asymptotic scaling. 
+ Beyond its base version, the paper also proposes two computationally much more efficient extensions: parallel GPRS and branch&bound GPRS. Both approaches rely on non-trivial mathematical insights, e.g., additivity of a Poisson process. 
+ Moreover, in one-dimensional channel simulation problems where the density ratio is unimodal, the algorithm displays the theoretically-optimal runtime (unlike existing approaches such as A* sampling)
+ The paper shows a nice combination of solid empirical results and theory. For example, most claims on the runtime complexities and theoretical guarantees (finite runtime) are proven rigorously. 

Weaknesses:
See questions below. There are no immediate weaknesses evident, but some clarifying questions need to be addressed. 

Limitations:
Limitations in terms of scaling and speed are sufficiently addressed. Societal impacts are a less important topic for general-purpose data transmission protocols. 

Rating:
9

Confidence:
5

REVIEW 
Summary:
The paper addresses the problem of representing a target distribution using the least possible number of bits. The authors refined the idea of encoding a sample from the target distribution as the first sample from the proposal distribution that passes a rejection sampling condition. The refined approach is shown to achieve runtime optimality.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The idea of using a Poisson process to encode a target distribution is powerful. The strategy is well known but further investigations and attempts to make it more feasible could have an impact in various domains.

Weaknesses:
The authors' technical contribution. could have been outlined more precisely. It is hard to understand why the new strategy is expected to be better than the A* algorithm or naive rejection sampling. The authors should have explained why ""a criterion that does depend on the time variable"" is better.

In the introduction, the authors say that the distribution is assumed to be unimodal and 1 dimensional. This is a strong assumption and it is unclear why the proposed method requires it. Similar assumptions are made in Flemich 22. The authors may have specified whether these are also required in Maddison 16.

The authors should have found a way of testing the algorithm on real-world data.

The text could be more curated and structured.



Limitations:
The authors do not discuss the limitation of the proposed approach. It is unclear what assumptions are required for the proofs. The theoretical runtime of related methods is not reported.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper investigates the problem of one-shot channel simulation, which can be used as lossy compression without involving quantization.  
A new rejection sampling algorithm called greedy Poisson rejection sampling (GPRS) is proposed. Then,  a parallelized and a branch-and-bound variant is proposed. Those algorithms are analyzed regrading both correctness and runtime.  
Toy experiments on one-dimensional problems show that GPRS compares
favourably against A* coding, the current state-of-the-art channel simulation protocol.   

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The proposed GPRS is properly positioned against previous and concurrent works, and contains sufficient novelty.    
The proposed method has much better runtime complexity compared with previous SOTA A* coding.  
The manuscript is well written and enough background information is provided for general readers.  

Weaknesses:
Though this manuscript is a theoretical contribution, it would be better to discuss more about promising application scenarios and current gaps regarding both performance and efficiency, which can encourage more future works in this field and facilitate more applications of similar techniques.  

Limitations:
see above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a new algorithm for lossy compression, using ideas from Poisson rejection sampling.

Given a sample $y \sim P_y$, Alice wants to communicate the smallest number of bits possible such that Bob can simulate $x \sim P_{x | y}$, when Alice and Bob have access to the distribution $P_{x, y}$ (and shared randomness to allow simulation).

The proposed algorithm achieves an almost optimal codelength by rejection sampling -- by introducing a temporal random variable t, Alice can communicate the hitting time $t$ for which $(t, X_t)$ lies under an appropriately defined function $\phi$. Variations of this algorithm are suggested: one variation is to parallelize using $L$ servers to reduce the runtime by a factor of $L$, and another binary search variant which gives an exponential improvement when the Radon-Nikodym derivative of $dP_{x|y} / dP_{x}$ is bounded.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper has several nice ideas from information theory, and seems to be the first that can provably achieve runtimes predicted by Li and El Gamal.

- The algorithm is built on A* coding, but is more robust, in the sense that while the runtime of A* coding depends on the Renyi entropy between $P_{x|y}$ and $P{x}$, the runtime of the proposed algorithm is proportional to the KL divergence between them, which may be much smaller and finite even when the Renyi entropy is infinite.


Weaknesses:
- The rejection sampler requires access to likelihoods of the conditional distribution and marginal, and hence it's not clear how much this can be generalized.

- The inverse function in Eqn (2) is solved numerically. How would this be computed for distributions with harder CDFs? 

- All experiments are toy experiments on Gaussian, Binomial variables etc.

- The experiments are for scalar random variables, and extending to higher dimensions would be extremely non-trivial. The authors state that extending this to multivariate Gaussians is an open problem.

- The paper tries to establish connections with areas like generative modelling by arguing that source compression is useful in VAEs/ latent diffusion models, but this is probably not correct. The works of Theis et al for example use generative models for compression, not the other way around.

Minor:
- The name GPRS for a communication protocol is probably a poor choice given that it's an existing standard.
- $\mathbb{V}$ in Theorem 3.1 is undefined.



Limitations:
There needs to be more discussion about the limitations of this work, especially if the authors are going to mention connections between the proposed algorithm and compression of neural networks. The connections to generative modelling are extremely weak.

Rating:
6

Confidence:
3

";1
IvEWhB1P90;"REVIEW 
Summary:
This paper claims to identify a flaw in the formulation of probabilistically robust learning.  To correct this supposed flaw, the authors propose a regularization scheme based on a notion of probabilistic perimeter.  This leads to a set of theoretical results that generalize the losses used in probabilistic robustness, identify conditions under which solutions exist, and initiate a theory of the limiting cases of these generalizations.  Experiments on MNIST and CIFAR-10 are also provided, showing that algorithmically, this new approach performs almost exactly the same as the previously proposed CVaR approach.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**New generalizations of probabilistic robustness.**  This paper rigorously analyzes the framework of probabilistic robustness, and provides several avenues for future research.  The perimeter-based perspective gives a new interpretation; it feels almost, but not necessarily the same as saying that this paper is to the original PRL paper as TRADES is to vanilla adversarial training (e.g., PGD).  I think that mathematically, this paper presents a novel and deep set of results which may be of interest to others in the community.  Furthermore, the idea of introducing the probabilistic perimeter is relatively clean, in the sense that it seems natural to consider the ProbR(A) function as a decomposition of the probabilistic risk and the clean risk.

**Existence of solutions.**  The existence proofs are interesting and general here.  In particular, I found the insight that the CVaR problem can be directly related to a generalized perimeter problem in Theorem 6 to be surprising.  Although it was previously known that solutions existed in special cases, this result is much more general.

**Writing.**  In general, I found the writing in this paper to be relatively strong.  I would tend to soften some of the language, such as referring to various results as ""astonishing"" and calling various results ""extremely important,"" but at the end of the day, these are stylistic choices that can be made at the discretion of the authors.

Weaknesses:
### Contributions

I'll start by evaluating the extent to which this paper lives up to the contributions it claims to make in Section 1.3.  In particular, I am going to focus on the first and last contributions; the middle two contributions are solid in my opinion.

**Contribution 1.**  The first claimed contribution is as follows:

> ""We address the geometric limitation of the model by Robey et al. [2022] by introducing a family of perimeter regularizations.""

Throughout the paper, the authors make several related claims regarding the validity of the approach outlined in [Robey et al. 2022], e.g.,

> ""We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this.""

In order to validate this contribution, it is essential that the authors 

1. Clearly articulate which property PRL is missing; 
2. Explain why missing this property constitute a flaw; and
3. Show that their perimeter-based method addresses this flaw.  

In this respect, I'm not sure whether any of these three points are satisfied.  I'll go through each of these points, and then hopefully we can have a discussion about this during the rebuttal phase.  To this end, let's start by looking into the claims of this PRL paper.  Based on my reading, the goal of that work is to do the following (as quoted from the introduction of [Robey et al., 2022]):

> ""The fundamental drawbacks of these learning paradigms motivate the need for a new robust learning framework that (i) avoids the conservatism of adversarial robustness without incurring the brittleness of ERM, (ii) provides an interpretable way to balance nominal performance and robustness, and (iii) admits an efficient and effective algorithm. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average-case approach of ERM and the robust, yet conservative worst-case approach of adversarial training. By enforcing robustness to most rather than to all perturbations, we show theoretically and empirically that probabilistic robustness meets the desiderata in (i)–(iii)""

In short, the goal seems to be to design a learning framework that enforces robustness to most of the perturbations in a ball around each data point.  Now, returning to the paper under review, the supposed flaw is described as follows:

> ""We highlight that [the PRL model] does not constitute a non-negative functional of A. Hence the loss function in (6) is not a regularized version of the standard risk (4) and in fact can be strictly smaller. This observation reveals a subtle flaw in the approach of Robey et al. [2022]: Points which lie in thin or spike-like regions of A penetrating the other class and that are more likely to have the label zero than the label one (meaning they lie in the set $[\rho_0 > \rho_1]$) yield negative contributions in (7) and are hence favored.  Such a scenario is visualized on the left side of Figure 1. From an adversarial perspective this means that points which are already misclassified are attacked nevertheless, which can lead to the bizarre situation that the adversary helps the learner by putting these points in the correct class with high probability, thereby reducing both adversarial robustness and clean accuracy.""

Below I'll list some of my thoughts about this, but first, it seems important to agree on what constitutes a ""flaw.""  It seems reasonable to define a flaw to be **an inherent limitation or weakness that affects the validity, reliability, or applicability** of a particular method.  Hopefully the authors agree with this definition, because I'm going to base my discussion of this on this definition (and if not, we can hash it out in the rebuttal).  Here are my thoughts vis-a-vis this definition.

* *Non-negative functional.* Unless I missed something, it's not clear to me whether [Robey et al., 2022] claimed that their framework constituted a ""non-negative functional of A.""  Or, zooming out, they did not claim that it was essential to their paradigm to ensure that the clean data was classified correctly.  While they do measure the clean accuracy in their experiments, their paradigm does not seem to be designed to explicitly minimize the standard risk or any regularized form of the standard risk.  On the contrary, the aim seems to be to enforce robustness to ""most"" of the perturbations in a ball around each data point, which more or less seems to be accomplished in the PRL paper.  Therefore, the motivation based on the non-negative functional perspective underlying the supposed flaw seems to be in doubt.  Or in other words, can a feature of paradigm constitute a flaw if that paradigm never claimed that feature as a contribution?

* *Spike-like regions.* The next issue is that the authors do not define what they mean by ""spike-like"" regions.  This confusion is amplified by Figure 1.  In this figure, why is PRL being applied to a non-robust classifier?  And what do the green and yellow regions correspond to?  Are the regions $\epsilon$-expansions of the decision boundary?  If this is the case, then why are these regions not a uniform distance away from the decision boundary?  And why are some points in red on the wrong side of the decision boundary?  Without *clearly* identifying the problem, it's difficult for the reader to accept that a flaw has truly been found.

* *Label probabilities.* I don't understand how the idea that the data points which are ""most likely to have the label zero than the label one"" yield negative contributions to the standard risk.  Can the authors explain this further, or can you illustrate this set $\{x : \rho_1(x) > \rho_0(x)$ in Figure 1?  And then how is this impacted by the perturbations added to the instance? The picture that I (perhaps incorrectly) have in mind is Figure 1 from the randomized smoothing paper by Cohen et al. (https://arxiv.org/pdf/1902.02918.pdf), although I'm having trouble mapping back onto the proposed approach.

* *Adversarial perspective.* The next point worth discussing is the ""adversarial perspective"" mentioned above.  Based on my understanding of PRL, there is no adversary.  The learner simply receives a distribution over perturbations and attempts to be robust against most of the perturbations sampled from that distribution.  And so from this perspective, it's unclear how the adversary factors in.  Thus, I don't understand the so-called ""bizarre situation"" that the authors claim to identify, or why it would serve to reduce both clean and adversarial accuracy.  Could the authors elaborate here?

To summarize, it's unclear to me what the supposed flaw is and why this flaw constitutes an inherent limitation of the method.  And given this, the contribution involving introducing a new family of ""perimeter regularization"" is unclear, because it's not clear what flaw or problem this solution is addressing.  More generally, I think it's worthy being wary of claiming to have found a flaw in existing work unless that flaw can be clearly identified.  While scientific research should certainly scrutinize past work, it seems important to charitably acknowledge what that past work claims -- and dually, what it does not claim -- to accomplish.  And in this case, my understanding of the argument laid out by the authors is that it is somewhat parallel to the ideas laid out in PRL, in that the supposed flaw is not something that the PRL framework was ever intended to do.  However, it could be the case that I have misunderstood either the author's argument or the original PRL paper, and therefore a discussion of this would be welcome.

**Contribution 4.**  The final contribution is as follows:

> ""We extend our models to encompass general loss functions and hypothesis classes. Our numerical experiments demonstrate that our geometric correction can enhance the adversarial robustness of probabilistically robust classifiers without compromising clean accuracy.""

I believe that this is inaccurate.  Based on Table 1, it seems that around half of the time, the original PRL method actually achieves better adversarial robustness than the proposed geometric method.  Moreover, the clean accuracy of models trained with the original PRL method seem to be either better or within a single percentage point of the accuracies reported for the geometric variant.  Therefore, it seems at best inconclusive as to whether this approach actually results in better performance.

---

### **Notation**

**Nested subscripts.**  The notation in this paper is at times difficult to parse.  I find it particularly hard to parse expressions like (9) and (10).  I think that it would be easier to read if the authors used expectation notation, and avoided the up and down carets for and/or.  For example, the first term in (10) could be written as

$$
\mathbb{E} [ x\in A \quad\text{and}\quad \text{Pr}_{x'\sim \mathbb{P}_x}[x'\in A] > p ]
$$

and one could perhaps add $x\sim\rho_0$ as an underscore (LaTeX in openreview isn't rendering this underscore correctly).  This would be much clearer, and it would prevent the use of three levels of underscores, e.g., $1_{\mathbb{P}_{x'\sim p_x}}$ in (10).

**Definition of the probabilistic perimeter.**  I think that the definition of the probabilistic perimeter could be clearer.  Overall, I'm not sure what is gained by reducing the problem to the setting of binary classification and considering sets $A$.  (8) defines ProbPer in this special case, but to me, it seems clearer to define it as follows:

$$
\text{ProbPer}(f) = \text{Pr}_{x,y} [  f(x)\neq y \quad\text{and}\quad \text{Pr}[f(x+\delta) = y] > p ]
$$

where $f$ is a classifier and the inner probability is taken WRT $\mathfrak{p}_x$.  This avoids the somewhat confusing $\rho_0$ notation and naturally extends to multi-class classification settings.

### **Related work**

**Other interpolation methods.**  It would be worth citing other recent relevant work on robust learning between the average and worst case.  Two notable papers, which are compared to in the PRL paper, are 

> Rice, Leslie, et al. ""Robustness between the worst and average case."" Advances in Neural Information Processing Systems 34 (2021): 27840-27851.
> Li, Tian, et al. ""Tilted empirical risk minimization."" arXiv preprint arXiv:2007.01162 (2020).

One could also think of the TRADES paper as interpolating between adversarial training and ERM through the trade-off parameter $\beta$, so it would be worth comparing to this paper as well.

**Engagement with PRL.**  As discussed above, it would be worth softening the claims regarding the extent to which PRL has fundamental flaws.  In addition, the authors make several other claims in the same spirit which don't necessarily agree with past work.  For instance,

> ""To the best of our knowledge, existence of solutions for (6) or even (3) has not been proved so far."" 

Having read the PRL paper, this seems to be untrue.  In [Robey et al., 2022], the authors derive closed form expressions for PRL in linear regression and mixture-of-Gaussians classifications.  This latter scenario is particularly relevant given that the authors restrict their attention for much of the paper to binary classification.  Therefore, I think it may make sense for the authors to revise this claim.

### **Experiments**

**Introducing upper bounds on the 0-1 loss.** I'm not sure I understand the value in introducing the upper bound $\Psi$ in the paragraph starting on line 145.  The authors do not conduct any experiments (as far as I can tell) using this formulation.  So the question remains: Is there any practical/empirical value in using such a surrogate?

### **Theory**

**What constitutes a theorem?**  It's worth broaching what can at times be a sensitive topic: What constitutes a theorem?  I would argue that Theorem 1 and 3 are closer to being remarks than they are to being theorems, although I could see counterarguments.  The fact is that these results hold essentially by construction, whereas as results such as Theorem 2 are quite non-trivial.

**What is weak-*?**  It would be helpful if the authors could offer some of the preliminaries necessary to understand their results.  For instance, I do not have really any understanding of what weak-* means (even after reading definition 1) and how it differences form other notions of convergence (e.g., in the L^p sense). Moreover, I expect that most of the people reading this paper will be in the same boat.  This not an issue per se, but it's worth considering that you may lose a good chunk of your audience by Section 2.3 due to the inaccessibility of some of this math.  One way to ameliorate this would be to offer an intuitive explanation of what weak-* convergence means and to point to references where one could learn about what weak-* means, or to define weak-* in the appendix somewhere.  If it only appeared in the appendix, I would probably not have noticed it, but since it features relatively prominently in Theorem 4 and Example 1, it's definitely worth discussing and adding some intuition before jumping into examples.  Another way of helping the reader to understand the classes of hypotheses that mean the weak-* closed condition would be to give an example or two that do *not* satisfy this property.

**Why are the properties and asymptotics of PropPer interesting?**  The contribution of the Section 2.4 is unclear to me.  The fact that under certain conditions, $\text{ProbPer}_\Psi$ is submodular doesn't seem to have a punchline, i.e., it's unclear why this property is important and/or why one would want this result to hold.  Perhaps the authors could elaborate here on why submodularity is desirable, or what properties it engenders on the task of optimizing ProbPer?

A similar criticism applies to Proposition 1.  The authors say that this result speaks to the fact that for adversarial training and probabilistic robustness, the perimeter dominates the regularization.  Again, I'm left wondering, why is this notable?  Does this lead to a better understanding of how to optimize ProbR(A)?  I can see why one might want it to be the case that PRL converges to the Bayes classifier as $\epsilon\to zero$, but again, it's not clear to me why such a result would be ""extremely important.""  One way to make this more clear would be to elaborate more on the implications listed in the paragraph starting on line 269.

### **Miscellaneous**

Here are a few other points:

* I think that ""Machine Learning"" doesn't need to be capitalized on page 1, nor does ""Probabilistically Robust Learning.""
* There is a heavy reliance of $p$ and similar looking characters here.  We have $p$ as the risk probability, $\mathfrak{p}_x$ as the distribution over perturbed instances, $\mathbb{P}$ as the probability function, and $\rho$ as a measure over a particular class.  It would improve readability to use characters that look more different from one another.
* What is meant by an *admissible* set in line 126?  It doesn't seem relevant to statistical notions of admissibility unless I'm missing something.
* The comma after neighbors on line 127 should be removed. 
* On line 174, it should be *an* $\mathcal{A}$-measurable function.

### **Overall evaluation**

Overall, I thought that there were some interesting insights in this paper.  The formulation is relatively clean and there are several interesting new insights about probabilistic robustness.  The writing is generally of high quality and some of the new results are surprising and generalize past results.

However, there are also numerous drawbacks.  I can't help but feel that this paper -- as it is currently written -- will have a limited impact.  Many may read it and come to the conclusion that this paper does a lot of interesting math and generalizes notions of probabilistic robustness.  But at the same time, these readers may wonder: Why did we do all of this?  Or, in other words, do these generalizations offer any insights into how one could achieve better probabilistic robustness in practice.  And unfortunately, I don't see how one could answer this question in the affirmative.  The experiments do not demonstrate the benefit of the vanilla geometric or the generalized variants (e.g., soft classifiers, smoothed losses $\Psi$, etc.) of probabilistic robustness.  This is all to say that I think that this would be a much stronger paper if there was a resounding demonstration that it's actually worth generalizing probabilistic robustness, and/or whether or not the supposed flaw actually holds back the original framework.

This is related to my feeling that there isn't an inherent flaw in the original framework.  I was not convinced by the arguments given by the authors (as explained above) and I think that it would be worth rethinking the claims to having identified this flaw, especially given that the authors are in a sense focusing on a feature that was never claimed to be a component of probabilistic robustness (based on my limited understanding).  This doesn't necessarily dilute the contribution; it would simply serve to engage with the past work in a different way.  

Overall, I think that this paper is borderline, and as written I would tend to recommend that it not be accepted on the basis that all of the mathematical insights given in the main text do not lead to any notable improvements on the algorithmic end.  If the authors can demonstrate that these insights do lead to better algorithms, and if we can work through some of the topics discussed above, I would consider raising my score.

Limitations:
I don't see any major limitations.

Rating:
3

Confidence:
5

REVIEW 
Summary:
Congratulations to authors for a timely and interesting paper. The paper advances the work of Robey et al. [2022] that established computational less expensive version of robust learning, dubbed Probabilistic Robust Learning (PRT). In particular, random perturbations and gradient based attacks had been used before and aside, while the work of [Robey 2022] proposed entirely probabilistic RT, which enjoys relatively lower computational costs.

The paper casts PRT as regularised Empirical Risk Minimisation. It further 1.) identifies a regularizer - 2.) improves it by introducing new objective $ProbR$ and 3.) relax it convex $\Psi$ instead of indicator function used in ProbR Further, the Section 2.3. 5 4.) Extends it to a soft classifiers (relevant to soft-max classification used in NNs). 
Section 3 deals with general loss functions and upper bounds probabilistically robust objective by $CVaR$ (conditional Value at Risk) that is convex if loss is convex in the first argument (e.g. most of common losses used in NNs) and easier to optimise. Theorem 6 then establishes CVaR as a special case of $ProbR_{\Psi}$ and its corollary establishes the existence of the solution (novel result).

Numerical experiments demonstrate that hereby proposed version of PRT works on par with original version [Robey 2022] supported by pseudo code algorithm provided in Appendix to help adaptation.


Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
+ Clarity: Accessible presentation of the problem, solution and use. Yet high standard of rigour is present in both main pair and appendices.
+ Theoretical framework with potential: Besides correcting the PRT and establishing existence guarantees (novel results) the paper develops clear mathematical toolkit for solving similar problems in adversarial robustness in rigour.
On top the Appendix A.6 PAC learnability for Lipschitz continuous Ψ - suggests future research and improvement of the statements of the paper.

+ Applicability/Impact: Appendix B1 contains the pseudo code for practical usability together with drawing the connections to CVaR as a surrogate objective.



Weaknesses:
- (minor) Numerical experiments presented are minimalistic yet arguably sufficient since theoretical guarantees have been provided. Intriguing results are mentioned but not furhterexplored or commented: “Note that the original or the geometric version of PRL should not be expected to match the adversarial robustness of classifiers trained with PGD attacks [Madry et al., 2017] or other worst-case optimization techniques. Instead, they shine with superior clean accuracies and easier training while maintaining probabilistic and a certain degree of adversarial robustness, as also observed by Robey et al. [2022].”

Limitations:
Limitations have been commented on in a reasonable extend in Conclusions.

Rating:
7

Confidence:
1

REVIEW 
Summary:
The probabilistically robust learning (PRL) framework aims to trade-off clean accuracy and adversarial robustness in model training by introducing a loss that penalizes the suceptibility towards adversarially attacks in the perimeter of the data points. The authors of the paper under review note that the loss of this framework has some (possibly) unintended property: In binary classification, for misclassified data samples an adversarial attacker would attack the sample ‘correctly’, i.e. assign it the ground truth label instead of the wrongly learned one. For the PRL loss such a setting would decrease the loss, incentivising such behaviour. On noting this flaw, the authors propose an new loss which simply applies the PRL loss only to correctly classified points, and the standard ERM loss to the ones that are not (yet) correctly classified. The authors then go on to prove for various settings the existence of solutions to these minimization problems and finally show that empirically, their approach leads to a competitive clean accuracy while increasing the adversarial robustness for small datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This work notes the question: What should one do with misclassified points in the context of adversarial robustness? Since they do not refer to previous work on this angle to adversarial robustness, I assume that this is a novel question. It seems like a very natural and relevant discussion point.
2. The existence results are novel and seem to be following a line of work which has previously explored the topic. Adding this new type seems relevant.
3. The writing is easy to follow and clear, as the authors successfully explain the differences between the different losses.


Weaknesses:
1. It is unclear and not discussed how detrimental considering misclassified examples in the PRL loss is in practice, possibly also depending on the quality and clean accuracy of the learned function. It would be especially interesting to discuss this as the large/small p results for MNIST and CIFAR seem to have opposite trends (improving/decreasing the adv score compared to “original” for large p).
2.  I believe the empirical results would be stronger, if datasets with a larger number of difficult datapoints were selected. The high accuracy in MNIST leaves only few datapoints to be misclassified and it is known that these may be ambigous even for human interpretation.
3. Weakness of the reviewer: I find myself unable to access correctness and relevance of the existence results.

Limitations:
I was mostly interested in the abstract due to the geometrical interpretation of decision boundaries and its relation to robustness. 
Since I am more interested in the application side, for me personally the most intersting points of the paper are the observation that an adversary might correct a model and the definition and practical evaluation of the novel and improved loss.
I acknowledge that the relevance for the authors might weight more on the proof techniques, but I find myself not being able to assess their value and novelty with full justice.
Since the empirical evaluation and discussion of the idea of corrective adversaries within its context seem to be lacking, as described in Weaknesses, I assign the paper a score of 4. 
However, I am willing to increase my score in line with the other reviewers if they are confident and convinced about the evaluation of the existing results.

*** Update after rebuttal: Edited Soundness score 1->3, updated overall score 4->5

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper takes a fresh and geometric view on the method—Probabilistically Robust Learning (PRL) [Robey et al., 2022], an approach which interpolates between the robustness offered by adversarial training and the higher clean accuracy and faster training times of Empirical Risk Minimization. The paper proposes a geometric framework for understanding PRL, which identifying a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. The paper proves existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced perimeters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and organized, and is innovative and original. The structure of the paper is clear and rigorous.

Weaknesses:
Experiments are not sufficient, only minist and cifar-10 datasets.

Limitations:
It's better to evaluate the theoretical conclusions on larger datasets,for example ImageNet.

Rating:
7

Confidence:
4

";0
rzu41O7us0;"REVIEW 
Summary:
The paper presents a new approach to identify task-specific building blocks of neuronal activity from fMRI data by using supervised matrix factorisation. The identified patterns generalise from the train to a test set and match expectations on the brain activity for the different tasks from the neuroscience literature. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and addresses an important problem in the analysis of fMRI data in a novel way that achieves impressive performance. 

Weaknesses:
While the paper criticises that existing methods cannot be applied to large, diverse datasets, the paper lacks a study of the computational efficiency of the proposed approach and a comparison with existing methods. 

Limitations:
Limitations of the approach are not openly discussed. 

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper addresses the challenge of identifying elementary functional neuronal networks and their combinations in the context of complex tasks, using task-specific functional MRI (fMRI) data. The central problem it tackles is the deconvolution of task-specific aggregate neuronal networks into elementary networks. These elementary networks can then be used for functional characterization and mapped to underlying physiological regions of the brain. Due to the high-dimensionality, small sample size, acquisition variability, and noise inherent in this task, the authors propose a deconvolution method based on supervised non-negative matrix factorization (SupNMF). The results demonstrate that SupNMF can uncover cognitive ""building blocks"" of task connectomes that are physiologically interpretable, predict tasks with high accuracy, and outperform other supervised factoring techniques in both prediction accuracy and interpretability. Overall, the proposed framework offers valuable insights into the physiological foundations of brain function and individual performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper presents a valuable effort to implement a supervised decomposition method in a novel way, showing the potential for this approach in a complex context, such as the analysis of neuronal networks.
2. The authors provide fascinating results indicating that each task has unique markers within these learnable networks. This insight could contribute significantly to understanding how tasks are represented and processed within the brain. Observing that some networks are shared across tasks also provides a meaningful direction for future research.
3. The alignment of the findings with existing physiological research is a good sense for future study.

Weaknesses:
1. Reproducibility: The study could be enhanced by applying the proposed method to other datasets or by resampling the existing dataset. This would help to assess the generalizability of the method and the robustness of the findings, which is currently a limitation of the work.
2. Baseline Comparison: It would be beneficial if the authors had compared the proposed SupNMF method with other supervised decomposition methods, such as Partial Least Squares regression. This lack of comparison limits the understanding of how their proposed method stands in relation to existing methodologies regarding performance and effectiveness.
3. Parameter Study: The authors need to provide an in-depth analysis or sensitivity study concerning the weight parameter \lambda. As this parameter likely plays a significant role in balancing different loss terms, this omission constitutes a substantial weakness, potentially leaving readers unclear about the effectiveness of supervision signals.

Limitations:
Yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a decomposition method for task-functional connectivity. It proposes canonical task connectomes which derives sub-structure of functional brain connectivity which group connectomes which identify elementary components of the overall connection. The authors use supervised non-negative matrix factorization to factor connectome matrices and show that the derived features are suitable to predict tasks for functional MRI and robust dimension reduction of the original representation. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written. 
- It construct a clear optimization problem whose results are easily interpretable. 
- It demonstrates a solid dimension reduction for connectome data. 

Weaknesses:
- Motivation for including supervision in the connectome decomposition is very weak. 
- There are some missing details on variable descriptions, e.g., $d$ in line 96 is missing, $\hat y$ in eq (2) is missing (although can be inferred that it is a prediction for a class), and derivation of $X$ from $C$ should be better explained. 
- Experiment is performed only on one study. It can be excused if the dataset is rare, but there are so many publicly available fMRI data. 
- Lack of baselines. It is missing the most fundamental baseline, i.e., LDA. Moreover, just typing in ""supervised dimension reduction"" in google scholar yields various literature but this paper demonstrates only SupSVD as a supervised baseline. 



Limitations:
The paper does not discuss any limitation of its own. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This contribution presents a novel method to find a functional basis for a database of task fMRI acquired from different subjects. The functional basis, dubbed canonical task connectomes, is shard across large cohorts; can be composed into task-specific networks; and is predictive of task efficacy.

The authors produce this functional basis through supervised and non-supervised NMF and SVD. For this they propose an objective function (in equation 3) which is compatible with these methodologies.

To implement this approach the authors use the HCP100 database which has 6 cognitive tasks. To show that the obtained basis is task-specific the authors use UMAP plots of different resulting decompositions showing good separability of tasks in the embedded UMAP space. To show that their basis is generalizable across cohorts they use 80/20 splits of the HCP 100 database and use the basis to inform a classifier that predicts the task performed by an unseen subject given the fMRI acquisition. To claim physiological and anatomical grounding for their basis, the authors compare qualitatively the basis against known anatomical traits and the involvement of different basis components as important features for each task.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This contribution is a high-quality application of known methods to the significant problem of understanding how functional connectivity in the human brain (as measured  by fMRI) is centric to cognitive tasks.

The manuscript presents a well-justified formulation of the problem as a deconvolution case and solves it through different approaches, supervised and non-supervised. This formulation and resolution are original and well presented. Even if the methodological contribution is not at the center of this manuscript, the application of known techniques is well-justified and evaluated.

The evaluation of the results are a good balance of qualitative evaluation (e.g. with UMAP embeddings), and quantitative (e.g. with the clustering approaches) in the case of task-specificity of the connectomes. The generalisation experiment using a downstream classification task is also well conceived. Finally the relation with anatomy and physiology is well organised.

In all, this contribution presents a very good application of known methods to an important problem in neuroimaging. So it's a paper that will have impact in one area, the neuroimaging one.

Weaknesses:
I find two weaknesses which are related to claims of cohort generalisability. In short, with the availability of public datasets of fMRI, it's hard to justify a cohort generalisation claim while staying in one 100-subject database. Specifically when the used 100-subject set is a subsample of a 1,200 total database. In light of this, second weakness I find is the lack of an analysis of the stability of the functional basis across datasets, including a study on dataset size.

Limitations:
The authors have not explicitly mentioned the limitations in the manuscript.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents a novel framework for fMRI analysis that aims to deconvolve complex neuronal networks into task-specific elementary networks called ""canonical task connectomes."" The proposed method utilizes supervised matrix factorization to identify these task-specific networks and demonstrates their interpretability and generalizability. The study showcases experimental results on the Human Connectome Project dataset, highlighting the ability of the framework to capture the natural task-specific structure in neuroimages. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The paper presents a new problem formulation and introduces the SupNMF method, which is a novel approach to identifying task-specific networks. The authors demonstrate the usefulness of the proposed framework in identifying canonical task connectomes that have a strong physiological basis and can be mapped to regions of the brain to identify physiological underpinnings of tasks.
- the authors present the problem formulation and the proposed method in a clear and concise manner.
- The proposed interpretable framework has the potential to advance understanding of complex cognitive processes and to identify biomarkers for predicting tasks. 
- The authors also provide a comprehensive discussion of relevant methods and materials.

Weaknesses:
- While the authors present comprehensive experimental results, they could provide more details on the performance of the proposed framework in comparison to other state-of-the-art methods. Additionally, the authors could provide more details on the interpretability of the identified canonical task connectomes and how they relate to existing literature in neurosciences. 
- While the authors briefly mention the potential applications of the framework in understanding shared and unique functional networks across different pathologies and how task-specific networks can get dysregulated due to the onset and progression of diseases, a more in-depth discussion of these applications and their potential impact on the field would be helpful. 
- The authors did not explain much on why the “unrelated set” of subjects in the Human Connectome Project is selected. Also, more datasets are expected to be included to demonstrate the generalizability of the proposed method. 
- The authors could provide more details on how they determined the optimal number of latent connectomes and how this choice impacts the results. One potential concern of the proposed framework is that it relies on the assumption that the observed connectome matrix can be represented as a linear combination of a small number of latent matrices. While this assumption may hold for some datasets, it may not be applicable to all fMRI datasets, especially those with high levels of noise or variability. Additionally, the choice of the number of latent connectomes (i.e., the dimensionality of latent space) is critical and may impact the performance of the proposed framework. 
Another potential weakness of the proposed framework is that it requires task-label vectors for each connectome in the dataset. While the authors provide details on how they obtained the task-label vectors for the HCP dataset, it may not be feasible to obtain such labels for all fMRI datasets. Additionally, the choice of the task-label vectors may impact the performance of the proposed framework, and the authors could provide more details on how they selected the task-label vectors and how this choice impacts the results.

Limitations:
While the proposed framework has the potential to advance our understanding of complex cognitive processes and to identify biomarkers for predicting tasks, it is important to consider the potential ethical implications of this research. For example, the use of fMRI data for predicting cognitive states or identifying biomarkers could raise concerns about privacy, informed consent, and potential misuse of the data. 

Rating:
4

Confidence:
4

";0
Y8Jfbqx0bA;"REVIEW 
Summary:
The authors consider the use of synthetic data $X^{sync} $ created from a model $p(X^{sync}|Z, I_S)$ on a further Bayesian analysis where the analyst has access to $p(Q|X^{sync})$ and to $p(X^{sync}| Z)$, where $Q$ is the parameter. 
 the paper is based on equality (4) 
$ p(Q|Z) = \int p(Q|Z.X^*)p(X^*|Z)dx^*$ where $X^*$ essentially represents $X^{sync}$ and $Z$ is either the real data or a differentially private version of the data and the idea is to replace 
$\int p(Q|Z.X^*)p(X^*|Z)dx^*$ by $p_n(Q) = \int p(Q|X^*)p(X^*|Z)dx^*$ which is fully accessible by the Bayesian and then show that the latter is close to the former. 

To do that the authors assume that 
$p(Q|Z,X^*_n)$ and $p(Q|X^*_n)$ are both close in total variation to the same seq of distribution say $D_n$ in probability when $X^*_n $ follows $p(X^*_n|Z,Q_0)$ and that given $Q$ $X^*$ and $Z$ are independent. 

Then the authors treat 2 toy examples a Gaussian and a logistic regression example and run some simulations to illustrate.

Inthe above presentation I am not mentioning the fact that the Bayesian models can be different from the generating model, which is treated but quickly pushed by assuming that this is not a problem.

Soundness:
1

Presentation:
1

Contribution:
3

Strengths:
The paper is well motivated and it is an important problem. If the results are correct then the paper is relevant and interesting. 

Weaknesses:
I am not sure the results are correct. From the presentation I don't understand the author's eq (4) or rather their comment which says that 
$p(Q|X^*, Z) $ is different from $p(Q|Z)$. The reason is that the authors do not explain what is the generating model for $X^*$ (In their paper the authors sometimes use $X^{sync}$ and sometimes $X^*$ as if they were the same, so I gather that they represent the same thing , but in the DAG of fig 1, they are not the same at all. 

The $X^* = X^{sync}$ is distributed from $p(X^*|Z)$, which does not depend on $Q$. Hence unless the authors clarify this point then their result are not valid. 

The authors consider two toy examples but in neither of them do they check tht the theoretical setup considered before is valid. For instance in the Gaussian example the generating model for the synthetic data is 
$ X^* \sim p( \cdot | X) = \int p(\cdot | \mu) \pi(\mu| X) d\mu$, i.e. the posterior predictive density. 
The relation (4) writes as $\pi(\mu|X) = \int p(\mu | X, x^*)p(x^*|X)dx^*$ but the model $p(\mu | X, x^*)$ is not defined. The authors seem to consider that  $x^* , X|\mu$ are iid but this is not possible because $\mu$ is unknown and it does not correspond to their Gaussian example. 

There are a number of other results which seem dubious to me. See below. 

Limitations:
The authors are conscious of some of the limitations of their method.


Rating:
3

Confidence:
2

REVIEW 
Summary:
The paper works on performing consistent Bayesian inference from synthetic data under DP. The authors propose a solution that involves mixing posterior samples from multiple large synthetic datasets, proving that this technique converges to the posterior of downstream analysis under specific conditions. This was established through experimentation involving non-private Gaussian mean estimation and DP logistic regression. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper offers a unique and engaging exploration of Bayesian Inference in the context of Synthetic Data, providing a fresh perspective in a field predominantly characterized by frequentist analysis.

Weaknesses:
See questions.

Limitations:
While the study explores the concept of Synthetic Data, its impetus is not distinctly articulated, leading to ambiguity regarding the problem the authors aim to address. The theoretical contribution appears to be weak.

Rating:
3

Confidence:
3

REVIEW 
Summary:
Inspired by Bayesian approaches for performing multiple imputation of missing data, this paper investigates the applicability of similar strategies for the analysis of synthetic data. Namely, the paper proposes inferring the downstream posterior of a Bayesian analysis by: generating multiple synthetic datasets; inferring the analysis posterior for each synthetic dataset; and mixing the posteriors together. (Interestingly, the paper finds that, contrary to the missing data imputation context, in the synthetic data case this strategy requires the synthetic datasets to be larger than the original dataset.)  

The paper provides theory showing that under the regularity conditions of the Bernstein-von Mises theorem (augmented by the additional conditions presented in Lemma 3.3), and assuming the congenial conditions in Definition 3.1, then the proposed strategy will approximate the data provider posterior distribution as the number of synthetic datasets and the synthetic dataset sizes increase. (The paper also proves a convergence rate result under stronger assumptions.)

The method is evaluated using two simple examples: (i) non-private univariate Gaussian mean estimation (when the variance is assumed to be known); and (ii) differentially private logistic regression.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This is an interesting paper. It addresses an important topic with an approach that appears to be novel and sound. 

Weaknesses:
One limitation of the proposed approach appears to be its reliance on the congeniality assumption (which we should not expect to hold in general). While the paper uses a simple example to illustrate that the method was still able to recover the data provider’s posterior when congeniality was violated, the paper needs to provide more extensive evidence of the robustness of the proposed approach w.r.t. violations of this assumption (as, in practice, it seems that the usefulness of the proposed approach for data analysis will depend on how robust the method is to violations of congeniality).

More specifically, the paper shows that for the toy problem of Gaussian mean estimation (with known variance) the mixture of posteriors converges to the data provider’s posterior even when the analyst’s variance is different from the data provider’s variance (right panel of Figure 2). However, for this example we have that the posterior distribution for the mean is already Gaussian in the finite sample setting to begin with. Providing additional examples where the posterior distribution of the quantity of interest is not Gaussian in the finite sample setting, but where the mixture of posteriors approximates the data provider’s posterior when congeniality is violated would provide more convincing illustrative examples. Perhaps, one simple example is the problem of Gaussian variance (or precision) estimation with known means. In this case, the paper could assess the robustness w.r.t. congeniality violations by choosing different means for the data provider and data analyst.  The paper should provide additional examples along these lines.

The paper might also want to include some discussion about some practically important settings where the Bernstein-von Mises theorem does not hold, and where the proposed approach might not be applicable (e.g., for models where the number of parameters increases with the sample size).

Other minor suggestions:

Line 69: change “which makes method their more” to “which makes their method more”

Line 302: change “To recover the analyst’s posterior …” to “To recover the data provider’s posterior …”


Limitations:
Yes, the paper addresses well the limitations of the proposed method.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper studies Bayesian inference based on synthetic datasets generated in a DP and non-DP setting. 

The paper suggests a specific sampling approach for downstream Bayesian inference using synthetic DP and non-DP dataset. It contributes theoretical results on the convergence of the inferenced posterior (from synthetic dataset) to the true posterior showing that (under certain assumptions) it converges as the the of number of synthetic datasets and the size of the datasets increases. Additionally, a convergence rate is derived 
Experimental results are provided for non-DP Bayesian mean inference and DP Bayesian logistic regression showing that the inference approach works, along with examples of the effect of parameters influencing the convergence (including number of observations, samples and level of congeniality)


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Very timely and interesting topic; I enjoyed learning about the specific Bayesian+DP setting.
- The theory is mostly well presented in the main paper (see suggestion/questions below). The narrative is relatively easy to follow.
- The theory appears sound. I have not found any obvious issues; however I would need to rely on other reviewers (and perhaps later the community as a whole) to validate the many proofs in the supplementary.


Weaknesses:
The following are question and comments; not necessarily weaknesses per se:

  -  The balance between theory and experiments is generally very good for my taste, but I feel the experimental part (in the main paper) let the theory part down a bit. The initial experiments focus on intuition and basic insights which I string support; however once the basics have been presented it would had been helpful with an experiment which covers many more scenarios proving summaries of the performance (using TV, coverage, means/modes and variance as metrics), along the most relevant dimensions such as number of observations, level of 
  -  ... it would also have been interesting with a more realistic example (high dimensional) using a more complicated model to better motivate the paper. Have the authors validated the results on such an example?  
  -  Figure 4 (right): It is not clear to me why the non-DP posterior does not mange to center its mode closer to the true parameter, is this an effect of the prior being centered on the true parameter combined with relatively few observations (the prior is not specified in the main text as far as I can tell?) - or other things?
  -  Figure 1: I am slightly confused by the graphical model, probably because the nature and role of $\theta$ is never really explained in detail. I hope the authors can clarify this (perhaps along with a detailed explanation the generative model in general)? For completeness, I would suggest including $I_a$ and $I_s$ in the figure as well. 

Overall, I am generally positive about the paper but the experimental parts misses an opportunity to convince me. I will opt for a borderline score until I get a chance to see the other reviews and the authors' response.



Limitations:
Included above.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This work is sloving a interesting task, which infers the downstream analysis posterior using synthetic data. The work proved that the Bernstein-von Mises theroy applies, the method can converage to the ture posterio as the number of synthetic datasets. The experimental settings are under two examples, i.e. non-private univariate Gaussian
56 mean estimation and differentially private Bayesian logistic regression.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1, The work is trying to solve an interesting task, which is infering the downstream analysis posterior using synthetic data.

2, The paper is well-writen and presented. 

3, The code is provided. So it will be helpful for the following work.

Weaknesses:
1. Since synthetic data is generated by models which are trained using real data. So why synthetic data can improve the consisten bayesian inference is not clear. I think the paper needs more discussion about differences bewteen the real data and synthetic data.

2, The synthetic data is a big topic. In the work, for me, it is not clear which synthetic data methods are used and how the synthetic data method is trained using real data.

3, The applications are missing. Is it possible to extend the proposed method or therory to some kind of real application.

Limitations:
See weaknesses.

Rating:
5

Confidence:
1

";0
ARJG1kr8A7;"REVIEW 
Summary:
Authors propose a way to prompt GPT-3 to exhibit behavior simulating execution of iterative programs. Authors propose the following prompt constructs: providing structured examples of program execution; using fragments of execution; not using self-attention on some parts of the generated text. Authors compare the results to baselines and show significant improvements

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- Authors introduce a method that enables GPT-3 to mimic the execution of iterative programs. They achieve this by supplying the model with intermediate steps and outcomes.
This is somewhat novel and could be useful for using LLMs to solve problems that require iterative processing. 
- The use of path fragments may prove beneficial in situations where the context size is insufficient for comprehensive examples.
- The strategy of confining self-attention to particular segments of the output might be advantageous when the context size needs to be considered
- The examples provided in the paper are well written

Weaknesses:
- Authors' approach requires the manual construction of prompts for each problem at hand. It is not automated and not scalable. This limits the usefulness of the approach in practice.
- Authors compare their approach to simple baselines. There should be a comparison to at least chain-of-thought reasoning. 
- Paper is hard to read and accept as a standalone without appendices. Authors refer to the content in the appendices too much.
- The significance of the work is low. It is known that LLMs can produce iterative output. Although the authors have enhanced the quality of such outputs via structured prompting, structured prompting is not entirely novel. Same can be said about using fragments in prompt/context.

EDIT: I have raised my evaluation of the paper from 3 to 4. Authors have promised to address the issues I and other reviewers have raised. However, in my opinion, such changes would require a major rewrite of the paper. I am not confident if these changes can be done well for the publication.

I have read the author’s rebuttal and further discussion with authors based on my questions and feedback. 
Authors' rebuttal addressed some of my concerns by more in depth discussion of IRSA relationship to chain-of-thought reasoning and possible automated generation of IRSA prompts.

Limitations:
It would be good if authors explored the limitations of what can be achieved by their approach. At some point the LLM ""execution"" (simulation really) of the program should fail. The points at which the simulation would fail likely depend on the input/output/context size. It may also depend on the algorithm semantic and/or algorithmic complexity. (Those are different complexities and may affect the breaking point differently). These questions could be explored and would be useful to know.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Making LLMs follow procedural rules precisely, as done when executing a program, is been a challenging task. In this paper, the authors introduce iterations by regimenting self-attention (IRSA), a prompting technique to make large-language models (LLMs) *execute* a hand coded programs (on novel inputs) precisely. The authors propose three techniques for IRSA: 1) by prompting the LLM with a step-by-step example showing many state-transitions in details, 2) By prompting with fragments of the state-to-state transitions only (along with the latest state for which it predicts the transition), and finally 3)skipping attention on intermediate state-to-state transitions. LLMs are shown to be significantly more successful at tasks such as sorting arrays, finding longest sub-sequence in a string, or simpler logical puzzles etc. when prompted with IRSA.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
### Originality

Recently, many new proposals for prompting LLMs, including scratchpad, and Chain-of-Though prompting have been proposed. However, most approaches focus on making LLMs reason and solve problems/puzzles. Instead, in this paper, the focus is on making LLMs *follow instructions accurately* (which then can be used for solving certain puzzle). This is a novel and original direction.

### Quality
The paper is well presented, including the figures, and the tables. Additionally, the experiments have been conducted on many tasks to sho

### Clarity
The paper is very clearly written and well presented. Specifically, I found the prompt examples very useful in understanding the paper's ideas.

### significance
Getting LLMs to precisely execute procedural rules is of intereset to the research community at large. LLMs inability to successfully tackle procedural problems (such as multiplication) of complexity beyond the training set complexity has raised questions regarding its ability tackle compositional problems. This paper provide an important perspective and countering result that will further enrich this discussion.


Weaknesses:
The two main drawbacks of the paper are motivation and experiments. 

### Motivation
The paper does not sufficiently motivate the problem statement. Why should we care about making LLMs execute programs - perform iterative behavior? When the process is deterministic and easy to programmatically describe, why would we prefer LLMs over a deterministic typical program (one can even use programs which explicitly *show* transition rules applied as well)? The authors mention education or software engineering vaguely, but there is no concrete motivation in these use-cases (when would a LLM be more suitable for this task over a REPL-like loop with python/cpp?).


### Experiments

1) The authors do not evaluate on significantly larger sequences sizes. Since fragmented prompting seems to allow arbitrarily large sequence of state-transitions, I believe authors can indeed use IRSA on larger sequence problems. The trend between success rate and sequence length would be insightful.
2) I strongly appreciate the authors for showing the negative result in Figure A.1 (Appendix section A.3.2). This shows that despite using IRSA, the model may end up performing wrong state-transitions based on its correlation to patterns in recent history. If multiple previous state transitions contain sequences where the statement ""2 < x = True"" appears, then when asked ""2 < 1 ="" the LLM has higher likelihood of filling True than False. This seems to directly negate the claim of this paper that we can make LLMs execute programs precisely. It clearly seems to be affected by the prompt history which can make them act in unreliable ways.

Limitations:
Yes the authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposed several novel prompting methods that could trigger GPT-3 to perform iterative behavior for executing algorithms with loops. The main technique the paper presented, IRSA, is to use highly structured prompts that contains information about unrolled execution trace, program states, and a detailed explanation of the motivation of a specific action, thus bringing strict attention controls to LLMs and help them reason about the procedures to get the solution. 

Based on the proposed IRSA method, the authors also introduced two alternative prompting methods: Fragmented prompting and Skip attention. The fragmented prompting technique strips out some of the iterations in the full execution trace, thus enabling the prompt to contain more diverse scenarios under a limited prompt length. The skip attention technique explicitly marks program state information with a special token and puts emphasis on the original prompt that serves as a demonstration to the LLM and the last execution state where the LLM could continue its execution from, thus also bringing LLM server side and client side optimization opportunities. Meanwhile, the authors also briefly discussed the automatic generation of the proposed prompts using  LLMs. 

The proposed prompting methods were evaluated on various tasks whose solutions involved loops. It is shown by the evaluations that the proposed methods could achieve state-of-the-art results on multiple tasks.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:

1. The paper showed great originality and insights in that the authors identified the failing reason of the LLM on tasks involving iterations, considered related Turing machine concepts, and designed several novel prompting methods incorporating highly structured information about unrolled execution trace, program states, and a detailed explanation of the motivation of a specific action, bringing strict attention controls to LLMs and help them reasoning about the procedures to get the solution. 

2. The proposed prompting methods showed state-of-the-art results on multiple loop-involving tasks.

3. The proposed Fragmented prompting method can be a promising technique that could encode diverse scenarios while keeping the prompt relatively short. This can be helpful for working with LLM APIs.

4. The proposed Skip attention prompting method can also be promising in that it emphasizes the concept of program state in the context of using LLMs as general Turing machines. This technique could also help reduce prompt length, and with adequate supporting modifications and implementations on LLMs, this can be a solid base for future works.

Weaknesses:
The overall presentation of the work can be relatively hard to comprehend for the readers. Especially for the presentation of the proposed Skip attention, it could be better if the authors provided a figure that briefly demonstrates the idea of the server and client-side implementations of the method.

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper explores the use of regimented self-attention (IRSA) to prompt GPT-3 to perform iterative behaviors necessary for executing programs involving loops. The authors investigate three approaches to trigger the execution and description of iterations. The results suggest that IRSA leads to larger accuracy gains than using the more powerful GPT-4 for dynamic program execution. The authors highlight the potential applications of IRSA in education and discuss the implications for evaluating large language models (LLMs). While LLMs have limitations in complex reasoning tasks, prompt design plays a crucial role in their performance.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ Interesting problem and approach
+ Providing examples of prompts

Weaknesses:
- Presentation
- Concern about reliability
- Concern about ""Turning machine"" claims


Limitations:
* The authors make huge claims, but do not discuss the limitations of their work.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work introduces Iterations by Regimenting Self-Attention (IRSA) which is a set of LLM prompting techniques for producing repetitive, algorithm-like behavior that can be useful for a range of tasks, such as carrying out a sorting algorithm or solving a logic puzzle. There are 3 techniques discussed: 1) ""Basic IRSA"" which is a chain of thought prompt that looks a bit like an execution trace of some natural-language-like pseudocode - theres a lot of repetitive structure, the current state is verbosely repeated after each step, and changes to the state are explicitly describe before the happen. 2) ""Fragments"" which is the idea that instead of prompting with a full trace of an algorithm you can just prompt with random unordered individual steps of the algorithm to prepare the model for executing a random step. 3) ""Skip Attention"" where only the most recently produced state is attended to (plus the original, fragment-based prompt), since changes to the state should be independent of the history of states – this cuts down on computation and helps the LLM not get confused by patterns in its recent output.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Skip attention and fragments (which pair well together) are great ideas, and are original as far as I know – other reviewers can correct me if I'm wrong. In many algorithms (and in fact, in the execution of interpreted code in general) only the current state matters as opposed to the history of how the state has changed. Only showing the most recent state to the LLM makes a lot of sense. It saves on computation cost and makes long running algorithms feasible, since there's no need to attend over the whole history of generations (which would become a huge problem as an algorithm runs for dozens or hundreds of steps). As the authors point out, this Markovian setup of not looking at the history of states also means that the LLM won't get confused by patterns in its recent history.
  - The ""fragments"" approach is a clever way to get the LLM used to this idea of seeing somewhat random states and needing to do a single algorithmic step for each one.
  - Skip attention makes so much sense, I'm surprised past work like ""Show Your Work"" (Nye et al 2021) didn't take an approach like this, since I imagine it would work fine with executing interpreted Python programs (where the state is the set of local variables/values along with the current line number in the program, and the LLM just has to output a next set of local variables and next line number).
- More generally, getting LLMs to do things that look more like rigid computation can be difficult and I think that this is a paper with a pretty good evaluation of a particular approach to this problem, and would be useful for the NeurIPS community to see.
- The evaluation is reasonable and shows unsurprisingly that the skip attention method can work great and generalize to very long sequences (eg bubble sort with 25 steps).

Weaknesses:
- The descriptions of what IRSA is were quite difficult for me to understand. The first line of section 2, the section describing IRSA, is ""Prompt 1, as well as the prompts 2, A.4, A.5, and A.6 in the Appendix, illustrate the basic IRSA."" (line 66). Written as is this feels a bit overwhelming as it suggests that I need to look at 5 different prompts (including 3 in the appendix) and try to look for the common features among them to figure out the method. Also, in reality many of these references (2, A.4, A.5, A.6) are actually going to show up later on in places where they're discussed so it's okay if I don't look in detail at them now, but since I haven't been told that I feel some need to dig them all up before continuing.
    - A flow for section 2 that would be much more understandable to me (and I believe others) would be the following. This is just one suggested way of doing it and I think there are many valid ways that would be widely understandable (you dont need to do the below), but the current flow is difficult to understand:
        - Give a brief but concise description of the key feature of IRSA (similar to lines 71-73 right now) so we're primed with looking for that *before* we're told about any prompts to look at. I might even suggest that instead of putting the CoT comparison at the end (lines 78-82), it might flow nicer to actually frame it *in terms of CoT / as an extension building on CoT* since that is a closely related framework many readers know about. In general after reading the paper I actually still find I have trouble precisely articulating what makes something count as ""basic IRSA"", so presenting it from the start in terms of its relation to CoT might be helpful.
        - Tell us to look at Prompt 1, and briefly walk us through what we're looking at / why this is IRSA. 
        - Mention that the precise keywords/format of Prompt 1 (""EXECUTION"", ""Prep"", ""EndPrep"" ""Iteration"", the indentations, ""State:"") are not important (IRSA is not a set of specific keywords to use) and point to Prompt 2 as an example of something that looks different on the surface level but is still IRSA.
        - At this point, you might parenthetically refer to the 3 appendix prompts as additional examples used in the evaluation that the reader can look to if they want more.
    - Again, to be totally clear, I'm not prescribing this format, I just find the current flow difficult to understand so I took a stab at restructuring it, but there are many other ways of doing so that would also flow well.
- I'd like to see some discussion of how this relates to the paper ""Show Your Work: Scratchpads for Intermediate Computation with Language Models"" (Nye et al 2021) which is currently just referenced by the paper in the list of CoT related works without specific discussion. In that work, the authors showed that while LLMs are bad at directly predicting the output of a Python function called on certain inputs, they could instead have the LLM repeatedly output the current state (what the variables are set to) plus the next line of the program to run. That was essentially a form of CoT with extra structure. IRSA seems somewhere in between the strict state/instruction format of Show Your Work and the free flowing reasoning of more general CoT. I think an explicit comparison to that paper (and/or any other paper that does some form of rigid CoT) is important, so it's clear how this work should be viewed in relation to others that have structured CoT.

- I'm generally coming out of this paper still somewhat unsure what precisely ""basic IRSA"" is (i.e., without fragmenting or skip attention), and I felt I could only gesture towards some of its important features when writing the Summary section above.
    - It feels related to CoT and I'd like to understand it in terms of that. One section comparing CoT to IRSA says ""a significant distinction lies in the number of reasoning steps, which is limited and fixed in *usual* CoT applications"" (emphasis is mine) (lines 78-82) but this is not always true (e.g. in the Show Your Work paper above – so does that make Show Your Work and instance of IRSA and this present paper is proposing a general framework encompassing that?).

- (minor weakness) Section 2.4 sounds interesting but is largely confined to the appendix. It doesn't really flow with the rest of the story and as far as I can tell isn't used in the evaluation alter. But I'm a bit torn because it is actually quite cool and maybe it doesn't hurt to have as just an aside (though maybe with a slightly more clear verbose explanation). I don't terribly hold this one against the paper, it just feels a little out of place. 

**Overall** I think that, though it is difficult to follow the flow of this paper in places so it took me quite a while to understand, and I'm still not totally clear on what makes something ""basic IRSA"" or how it relates to prior work like Show Your Work, I think that in particular given the contributions of skip attention and fragments this would still be valuable work for the NeurIPS community to see. The positives outweigh the negatives in my view, but with revisions around the points mentioned above I would be more supportive.


Limitations:
The authors address limitations adequately 

Rating:
6

Confidence:
3

";0
01GQK1gwe3;"REVIEW 
Summary:
In this paper the authors explore whether better optimization solutions can be found by jointly optimizing several inverse problems together. These inverse problems share a connection as they all can be formulated through a differentiable function $F(\xi_i | x_i)$. The authors implement the joint optimization through a set of NN parameters $\theta$ which connect all the different inverse problem variables $\xi_i = \hat{\xi}_i (\theta)$.

Upon reading the authors' rebuttal to the questions I posed, I am inclined to adjust my score accordingly.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The main problem that the paper is trying to solve is interesting and relevant. As the authors mention in lines 101-105 ""generic optimizers often fail to find the global optimum due to local optima, flat regions, or chaotic regions"".
* Also the setting expressed in section 3 is not typical which opens up many potential future work leveraging this setting.
* The experimental results are interesting in how they all improve the solutions by increasing $n$ which does provide evidence of cross-talk between the problems.

Weaknesses:
* The experiments generate inverse problems synthetically. I would like to see a real-life example of a problem that follows the settings exposed in section 3. I understand that several of the equations have practical applications but in this case I'm referring to a real-life problem that has a naturally occurring (not sampled from a known distribution) set of inverse problems that can be connected through a function $F$.
* The method is not applicable to many problems. I'm not familiar with any ML application that has similar optimization problems that can be pooled together. Put differently, it is unclear to me how restrictive is the setting in section 3 of having a function $F(\xi_i | x_i)$ that expresses all the inverse problems.

Limitations:
* The limitations that I see are encapsulated on the questions that I raised above.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper develops a novel approach to gradient-based non-convex optimization. The proposed methodology begins with the reparameterization of the parameter space utilizing neural networks, followed by the application of classical techniques such as BFGS, or alternative Neural Network surrogate models for the forward function, to accomplish the optimization task.

The efficacy of these methods is verified through four distinctive experiments, which include applications to the Kuramoto-Sivashinsky (K-S) equation and the Incompressible Navier-Stokes equation. The results indicate that the reparameterized optimizer delivers enhanced convergence overall.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper presents a compelling concept of reparametrizing the parameter space of the optimization problem using a neural network and accomplishing optimization via a two-step process that employs the trained/optimized neural network as a preconditioner. However, it remains unclear to the me as to why such a reparameterization is likely to benefit the non-convex optimization procedure. Despite this, the experimental results appear to indicate an enhancement, as evidenced by the four case studies investigated by the author.

Weaknesses:
My primary concern regarding this paper pertains to its lack of rigor. The authors do not clearly define the inverse problem that they are attempting to solve, nor do they provide cogent proofs or insights explaining why the introduced reparameterization would aid the optimization process. It is quite plausible that the limited experimental studies offered in this paper lack generalizability, and it's conceivable that there are counterexamples where optimizers, without the incorporation of reparameterization, achieve superior convergence.

Limitations:
While the authors recognize that they have not extend the method for constrained optimization problems, I believe there is an inherent limitation in the approach, as it lacks a mechanistic understanding of why such a method would be effective. This comprehension is fundamental for a method to be broadly applicable and reliable.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The manuscript presents a method to reparameterize and solve multiple inverse problems jointly using neural networks. The manuscript tests the proposed method on multiple inverse problems (including some chaotic problems) and compares against Neural Adjoint and BFGS baselines to show measurable performance improvements.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The method is simple, and the authors haven't tuned architecture for problems, which would allow their usage as drop-in replacements.
* Comparison against baselines shows the method provides noticeable improvements.

Weaknesses:
* The main downside of these methods is the added training cost (which the authors have mentioned in the limitations section)
    * I would recommend adding the training wall clock times + solving times in a table to give potential users of this method a proper estimate.
* Adding benchmarks for the same problems used in the Neural Adjoint paper would strengthen the paper.

Limitations:
All limitations are clearly stated.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper discusses a novel approach to finding model parameters from data, a crucial task in science. Traditional iterative optimization algorithms like BFGS can accurately solve simple inverse problems, but their reliance on local information can limit their effectiveness in complex situations with local minima, chaos, or zero-gradient regions.

To overcome these issues, the study proposes the idea of jointly optimizing multiple examples. The authors use neural networks to reparameterize the solution space and utilize the training procedure as an alternative to classical optimization. This method is as versatile as traditional optimizers and does not require additional information about the inverse problems, making it compatible with existing general-purpose optimization libraries.

The paper evaluates the effectiveness of this novel approach by comparing it to traditional optimization on a variety of complex inverse problems involving physical systems, such as the incompressible Navier-Stokes equations. The findings show significant improvements in the accuracy of the solutions obtained, suggesting that this method could be a powerful tool for tackling complex inverse problems.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This paper points out a potential new use case of neural networks and deep learning for optimization instead of existing learning to optimize (L2O) methods, that is to use neural networks as part of classic optimization, trying to learn unknown common structures among problem instances of interest. A major difference is that generalization to unseen instances does not matter.

2. The paper is written in crystal clarity with information in every details about the methods, the experiments and the results. The authors discuss about the results of different methods for each setting. Limitations and outlook to future work are also faithfully discussed.

3. Improvements without refinements look impressive on all settings. Improvements after refinements still look great on the first three settings.

Weaknesses:
1. For the 4th setting, Incompressible Navier-Stokes, the proposed reparameterization method gives much higher mean losses than BFGS despite the fact that the majority of problems actually improve over BFGS. Could the authors elaborate more on the potential reasons specific to this experiment setting?

2. Since the mean losses could change with different dataset sizes because of the varying instance difficulty, would it be a better presentation of results with relative error or relative loss? (also a relative improvement could be better for results like Figure 4 in the Appendix.

3. 3~6 times more computational cost for the first three settings and up to 22 times for the fluids could be too high for the benefits achieved. This could be subjective but I hope the authors could provide some discussions or justification.

4. Would the ""similarity"" requirement be too strict to make the proposed method practically useful? For example, in the wave packet localization setting, the parameters A and $\sigma$ are fixed. Is there a practical application scenario that corresponds to this setting?

Limitations:
High computation cost and potential lack of practical applicability.

Rating:
6

Confidence:
3

";0
bA3iR0jQO9;"REVIEW 
Summary:
The paper proposes a general subgame curriculum learning framework to accelerate MARL training for zero-sum games. It adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. The author derives a subgame selection metric that approximates the squared distance to NE values and further adopt a particle-based state sampler for subgame generation.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper presents a general framework to accelerate NE learning in zero-sum games by training over a curriculum of subgames. The author develops an automatic curriculum learning algorithm, i.e., Subgame Automatic Curriculum Learning, which can adopt any MARL algorithm as its backbone and preserve the overall convergence property. The paper is well written. A motivating example is also described to illustrate the main idea, which is beneficial for readers to understand the work.


Weaknesses:
1. As we can see, the convergent speed is accelerated by the proposed method. However, there is no evidence that the final performance is improved. 
2. The core part of this work is to compute the weight of the state. It would be better to compare with other metrics.



Limitations:
The author could make a discussion about the application of the proposed method on three or more players. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel subgame curriculum learning framework for accelerating multi-agent reinforcement learning (MARL) in zero-sum Markov games. The framework uses an adaptive initial state distribution to induce subgames of varying difficulty for agents to learn, and leverages a sampling metric that approximates the squared distance to Nash equilibrium (NE) values to prioritize subgames with fast value change and high uncertainty. The paper instantiates the framework with a particle-based state sampler and integrates it with any MARL algorithm, resulting in the Subgame Automatic Curriculum Learning (SACL) algorithm. The paper evaluates SACL in three zero-sum environments and show that it converges faster and achieves lower exploitability than existing methods.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:

- The paper addresses an important and challenging problem of reducing the computational cost of solving complex zero-sum games with MARL, which has many potential applications and implications.
- The paper provides a illustrative motivating example to justify the effectiveness of the subgame curriculum learning framework, which leverages ideas from goal-conditioned RL and prioritized experience replay.
- The paper is able to provide practical methods to approximately solve the hard parts of the theoretical work. For example, the paper introduces a novel subgame sampling metric that approximates the squared distance to NE values with a bias term and a variance term. The paper also adopts a particle-based state sampler that is compatible with most MARL algorithms.
- The paper conducts extensive experiments on three different zero-sum environments and demonstrates that SACL can converge to NE policies with substantially fewer samples. The ablation study part is able to demonstrate how well the approximation methods in SACL work comparing to other alternatives.

Weaknesses:

- The paper assumes that the environment can be reset to any desired state to generate induced subgames, which may not be feasible or realistic in some settings.
- In Section 5.1, it is not clear how the approximate exploitability is computed using MARL methods for best response training. What are the exact algorithms used for this purpose? How many samples are used for each best response training? How reliable are these estimates? Some details on these aspects would help evaluate the results more fairly.
- In Section 5.2, it would be helpful to report some quantitative results on how different choices of state buffer size, hyperparameters alpha, subgame sample probability, or other hyperparameters affect SACL's performance or convergence.
- It would also be helpful to have more details on how to implement FPS for buffer update and how to measure the distance between states.

Limitations:

- The paper requires access to full state information and reset function for each environment.
- The paper relies on approximate exploitability as a proxy for measuring closeness to NE policies. However, approximate exploitability may not reflect the true performance gap between different policies due to sample variance or approximation errors.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a subgame curriculum learning framework to accelerate multi-agent reinforcement learning (MARL) training for zero-sum games.  The framework adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. The paper derives a subgame selection metric that approximates the squared distance to Nash equilibrium (NE) values and further adopts a particle-based state sampler for subgame generation.  Experiments in the particle-world environment, Google Research Football environment, and hide-and-seek show that SACL produces much stronger policies than baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper provides a detailed analysis of the proposed approach and justifies it through experiments and analysis of a simple iterated Rock-Paper-Scissors game.
- The paper provides a clear and concise explanation of the proposed approach and its implementation.
- SACL is shown to produce much stronger policies than baselines in experiments conducted in the particle-world environment and Google Research Football environment.

Weaknesses:
- There is a gap between the motivated iterated Rock-Paper-Scissors game and the experiments conducted in the particle-world environment and Google Research Football. Since state and action space is discrete and finite in RPS, while these spaces are continuous in MPE and GRF. Also, The state is not communicative in RPS while it is communicative in MPE or GRF. In one rollout, two states are communicative if and only if each is reachable from the other with nonzero probability in a finite number of steps. So it is ok to use samples in RPS since it is a trade of time and space. While the state in MPE or GRF is something like position or speed, the advantage shown in the motivated scenario is hard to extend to more realistic environments.
- The link between eq.9 and eq.10 is weak, which should be the key to the theoretical contribution of the paper. I have several concerns about eq.10.
	- The estimated value function at different checkpoints represents different policies' values. Do the authors consider the missing $\pi$ inside the value function?
	- Does the asymptotic convergence of the estimated value function of a few sampled states mean the policies converge to NE? The explanation in Appendix is not theoretical and less convincing by giving examples.
- The subgame sampler is also essential, why the farthest point should be sampled first?
- Go is a good baseline for SACL. Since it is a fair game. To my understanding, SACL can be seen as an alternative to MCTS. One can start from the arbitrary state of a Go board, rollout, and then train the policy and value function. From this point of view, only having one rollout in Alg.2 might cause a large variance in the value function. 

Limitations:
The paper does not explicitly mention any limitations of the proposed method. The paper has no potential negative societal impact.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents an algorithm (SACL) for accelerating MARL training in zero-sum Markov games based on the subgame curriculum learning framework. A sampling metric based on approximated squared distance to NE and a particle-based sampler are proposed to sample states for subgame generation. Experiment results show the superiority of SACL in producing stronger policies and boosting training efficiency.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow. The general idea of the algorithm is well illustrated via a toy example: the iterated RPS games. 
2. The experiment results show both the efficiency and effectivity of SACL. 


Weaknesses:
* I am not sure the game considered in this paper is perfect-information or imperfect-information. From the Rock-Paper-Scissor example and the baseline methods considered (NeuRD and PSRO), I am aussming the case of imperfect-information.  If it is the case of imperfect-information, the optimal policy for a subgame depends on the distribution of hidden information. Yet, I don't see any component of SACL that deals with this distribution.  For instance, the DeepStack (a poker AI) algorithm uses the agent's range and opponent counterfactual values when resolving a subgame in poker. 


* Also, the choice of the metric and oracle sampler appears ad-hoc to me.
  1. Why can we  use Equ 10 to approximate Equ 9? Is this estimator unbiased or with low variance?
  2. In Equ 10, is the algorithm stable for different choices of \alpha? It would be good to see some discussion or ablation study on it. 


Limitations:
Please see weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a framework for learning Nash equilibria in zero-sum Markov games based on subgame curriculum learning. Novel sampling metrics for subgames generation are proposed. The proposed SACL algorithm is able to achieve equal performance at lower  sample complexity compared with self-play algorithms.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Proposed a novel framework for learning NE in zero-sum games. This can be combined with different RL algorithms to produce learning mechanisms with lower sample complexity.
- The presentation of the paper is clear.

Weaknesses:
- While the baseline methods require an exponential complexity, the proposed SACL costs approximately half, which is a limited practical improvement especially considering real-world applications as the authors suggested.

Limitations:
A more thorough discussion of the limitations and future steps could be beneficial.

Rating:
5

Confidence:
2

";0
UNOeQGHNaN;"REVIEW 
Summary:
This paper empirically shows previous supervised adversarial training methods have two shortcomings: (1) The features of the natural examples and those from other classes are not distinguishable and (2) the features of the natural and adversarial examples are not aligned. To mitigate these two issues, the authors propose a regularization to push away the features from other classes and freeze the natural examples and a reverse attention mechanism that increases the weight of the target class. The empirical results seem to validate the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and can be easily followed. 

1. The proposed method is well-motivated. The empirical study on the feature spaces is interesting and inspirable. 

2. The authors conducted the experiments on comprehensive datasets to support their claim.


Weaknesses:
1. Regarding the experiments, the authors only provide the results on ResNet18 and PreActResNet18, which is limited. I suggest the authors provide the results on the WideResNet and make a comparison with the current state-of-the-art performance listed in the RobustBench (https://robustbench.github.io/) to validate the effectiveness of the proposed method.

2. The authors do not provide the error bar to validate the significance of the results.

3. It is weird the accuracies under PGD, FGSM, C&W are higher than the natural accuracy achieved by ANCRA shown in Table 1. It would be better to provide some explanations for these abnormal results. It seems the defence has the issue of the obfuscation gradients. I suggest that the authors report the results under the adaptive attacks that use the auxiliary probability $p$ and even different $p$ during the testing phase and AutoAttacks. 


Limitations:
Though the authors claim they have limitations, I did not find the discussion of the limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper focuses on robust feature learning by combining two approaches: Adversarial Contrastive Learning and Robust Feature Selection. Specifically, it defines two characteristics for features: exclusion and alignment. The authors aim to enforce exclusion through Asymmetric Negative Contrast (ANC), which ideally should separate different classes. On the other hand, they aim to achieve alignment through Reverse Attention (RA), which should enhance the model's robustness.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The proposed method sounds interesting, especially the Reverse Attention approach. However, I have some concerns and feedback that will be provided in the later section of the review.

Weaknesses:
Writing Style:

Overall, the quality of the writing style could be significantly improved. Reading the paper is not smooth, and understanding it requires reading back and forth several times. For instance:

1. Generally, the flow of the paper is not engaging, which requires the reader to go back and forth in order to understand it. And, there are some sentences that are vague and difficult to understand.
3. Caption: Overall, the caption writing is not good; it contains long sentences that are difficult to follow. Labels for each plot have not been provided. Specifically, in Figure 1, regarding the distance between natural examples and OEs, it would be preferable to show distances between each pair instead of comparing class 0 with others, as class 0 may share common features with some other classes. It would be more informative to have the following plots: 0-1, 0-2, ..., 0-9.
4. Typos: ""perturbation -> perturbations"" in line 28 | ""which does harm to robust classification -> which harms robust classification"" in line 35 | ""... negative pair (PP) -> (NP)"" in line 64 | ""import feature -> important feature ..."" in line 118 | ""a example -> an example"" in line 147 | ... 

Content:
1. There are certain terms that are vague. For instance, what does ""well-trained DNN"" mean in line 20?
2. The definition of an Adversarial Example is not entirely correct. The perturbation itself may be visible, but when it is added to a natural image, both the adversarial example and the original image may not be easily distinguishable by human eyes. Furthermore, it is important to note that it is not the adversarial example itself to which perturbations are added. Rather, the perturbations are added to natural examples so that they cannot be correctly classified.
3. Description about Figure 1 in line 45-47: It is not a Gaussian distribution; it is a bell-shaped plot but not a Gaussian distribution. The main condition for a Gaussian distribution is that the area under it should be 1. And indeed, those numbers show a significant distance between classes. A cosine similarity below 0.5 should be large enough to indicate a difference between the representations of one class and another. Again, indeed, a cosine similarity between 0.9 and 0.99 indicates that representations of NEs and AEs are very close to each other! And comparing AEs plots with the OEs plots, we see that they have a very close representation to their original examples that OEs. Furthermore, if the representation of NEs and OEs is not significantly distant, models should confuse them, leading to lower clean accuracy. However, that is not the case.
4. The definition of OEs is not clear from the beginning of the paper. The reader should read the whole paper to figure out what they are, especially since different strategies are considered to select or generate them. 
6. Similarly, the term 'partial parameters' is vague, and the reader doesn't understand what it refers to until the later sections of the paper, which confuses the reader.


Approach and Methodology:
1. IT seems the main weakness is that ANC does not make significant contributions to the clean accuracy and robustness of the model, as indicated by the results in Table 3 of the ablation studies. Specifically, a model with only RA performs just as well as ANCRA. On the other hand, considering this, a large portion of the paper is dedicated to introducing and explaining ANC. However, the most important component, which is RA, is not studied well enough and lacks supporting experiments and theoretical insights.
2. The training process of a model with ANC is not clear enough. It would have been better to depict it through a diagram. Furthermore, the last paragraph in the introduction section is abstract and confusing, making it difficult for readers to understand the exact contributions.


Experiments:
1. The proposed method in the main table (Table 1) is evaluated without considering ""p"" in attack which creates a false sense of security. However, in Table 2, attacks with ""p"" demonstrate a significant decrease in the model's performance.
2. Even though ""Error Bars"" is marked in the checklist, I don't see standard deviation values in the reported tables.
3. Similarly, although ""Reproducibility"" is marked in the checklist, the code necessary to reproduce the results is not available.
4. I conducted an experiment with TRADES, and the accuracy I obtained differs from what is presented in Table 1. The clean accuracy is 80.92%, and the PGD accuracy is 50.10%.
4. The results of the Tiny-ImageNet experiments are inconsistent with those of other datasets, and the performance of the model does not seem promising. Additionally, they have not been compared with baselines, which is essential.

 

Limitations:
1. It appears that the approach does not work properly with large datasets like ImageNet.

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the authors address a notions of exclusion and alignment in representaion learning for robust adversarial training (AT). They propose a generic framework for AT that includes asymmetric negative contrast and reverse attention in order to obtain robust representation. In addition, they propose to weight feature by parameters of the linear classifier as the reverse attention, to obtain class-aware feature and pull close the feature of the same class. The authors show empirical evaluations on three benchmark datasets, showing the improved robustness under AT and as well as giving increased performance in comparison to state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality. The proposed  AT framework that concentrates on robust representation with the guidance of the two characteristics like exclusion and alignment appears to be novel. Generating and using negative samples by targeted attack to assist learning also appears novel. 

Quality. The quality is good. The paper is nicely motivated. The idea seems reasonable regarding the use two characteristics for robust representation: (1) exclusion: by pushing away the feature representation of natural examples from the feature representations for examples coming from other classes; and (2) alignment: by pulling close to each other the feature representation of the natural examples and the feature representation for the corresponding adversarial examples. The approach can be used in a plug-and-play manner for a number of defence methods, while the empirical validation shows advantages under the setting of white-box and adaptive attacks. While on table Table 2, we see consistent improvement for the used attacks with p, while we do not have results for comparison when we do not use p. 

Clarity. The paper organisation, the presentation and the writhing is good. I find the paper easy to follow on most parts. It might be beneficial some parts of the paper to be explained more easily so that the main idea behind the paper to be more obvious. Fig 2. might be improved to more concretely pin point the cases of class confusion issue. In Sections 3.2.1, the section within the lines 162-169, a bit difficult to rad on the first pass regarding the cases covered by the class confusion issue. I would suggest an additional small figure to illustrate intuitively and more obviously the cases. In Sections 3.2.1, the section within the lines 170-172 is not clear (not sure when the some sentences end and begin). Section 3.2.2 could benefit further polishing. Maybe a table or small figure describing what is negative samples or pairs (i.e., OEs), natural negative examples/pairs, adversarial negative examples/pairs, positive examples/pairs, adversarial negative examples/pairs, etc. could be helpful. The text in the lines 234-257 could also benefit sharpening, since some explanations are not that smoot and straightforward at least to my understanding. The results on Table 4 are interesting, but maybe the table could be reorganised and more clearly shown so that we can have a better grasp of what is the actual advantage. 

Significance. Adversarial training is important topic for privacy and security applications. This paper proposes an approach towards more reliable defence of adversarial attacks. The approach also seems to provide improvements on top of other approaches like the TRADES approach. Generating and using negative samples by targeted attack to assist learning seems valuable. Adversarial training with reverse attention on the feature level seems interesting.

Weaknesses:
Weaknesses
- In the empirical evaluation, some tables could be made more obvious (please see the comment on clarity for Table 4).
- The black box attacks not considered and comparison for black box attacks not done.

 

Limitations:
Limitations

- By generating negative samples by targeted attack, we use prior on about the possible attack, so possibly, the defence of the approach is biased towards the generated negative samples by the used targeted attack (or similar attacks).
- In the case of black box attacks when we do not have a prior about the attack, I'm not sure how much the proposed approach could help.  

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents two characteristics of robust features, exclusion and alignment, and proposes a novel adversarial training method with asymmetric negative contrast and reverse attention. For exclusion, it introduces asymmetric negative contrast loss and generates adversarial negative examples by targeted attacks to push out examples of other classes in the feature space, and for alignment, it introduces reverse attention that weights features based on the parameters of the linear classifier to obtain class-aware features. Experimental results on three datasets show that the proposed method significantly improves the robustness of the models.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1. This paper introduces two characteristics that robust features should have: exclusion and alignment, and proposes a new AT framework that enables models to learn robust features effectively. It can be used in a plug-and-play manner and works well with existing AT methods.
2. Specifically, asymmetric negative contrast loss for exclusion of robust features, a technique to create hard negative samples through targeted attacks, and reverse attention using class information for alignment are proposed, which induce the characteristics of robust features. To my knowledge, these techniques are novel in AT.
3. The proposed method records the SoTA performance in experiments on CIFAR-10, -100, and Tiny-ImageNet. The performance improvement shown in Table 1 is impressive.
4. Overall, the paper is well written to help the readers understand the presented concepts. In detail, figures 1 and 3 show the statistical differences between NEs and OEs and NEs and AEs, making it easier for the reader to understand the difference in the distribution of features and the effectiveness of the proposed AT.  Figure 2 also helps to illustrate the problem of class confusion.


Weaknesses:
1. The practical implementation details for Reverse Attention in Section 3.3 are unclear. Looking at line 228 of the paper, it says that the proposed method uses p and p' together to train the model, but it is ambiguous in what way the proposed method uses them together (e.g., does it use p+p' or alternate between p and p'). Releasing the source code in the future will answer a lot of questions, but it would be nice if it could also be clarified in the paper.
2. The experimental results for ""adaptive attack"" in Section 4.3 raise the question of whether the experimental results in Section 4.2 are an unfair comparison. Therefore, it is necessary to clarify what the role of p is and whether this is the role of the key in the gray-box setting.
3. Table 1 in the supplementary material, the results on Tiny-ImageNet, does not have baselines, making it difficult to see performance gains.
4. Making hard negative examples via targeted attacks requires additional computation, but according to Table 4, the resulting performance gain is small. This paper also needs a clear and specific description of the negative sample generation (such as whether PGD-N was used).  In the supplementary material, it is described as if they used the AEs that are predicted as the classes of NEs in the current batch as OEs, which is difficult to apply in cases with a large number of classes like CIFAR-100 unless the batch size is large enough.


Limitations:
In this paper, possible limitations are not discussed separately.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work aims to improve the adversarial training (AT) techniques from the perspective of learning robust representation representations. Specifically, the authors highlight two characteristics of having robust features. Exclusion: the similarity of features of samples of one class should be very less from the features of samples of other classes, so that model can differentiate between features of different classes for better classification.  The second attribute is Alignment: the gap between features of adversaries and clean samples of same class must be very small, which would increase model's robustness against perturbed samples.

To effectively satisfy these conditions, this work proposes two techniques, (1) to enforce exclusion, a asymmetric negative contrast loss is proposed which minimizes the clean sample similarity with negative samples of other classes, crafted by the adversarial attack. (2) to satisfy alignment: reverse attention strategy is proposed which align together the features of training examples which belong to the same class.

The proposed method is compatible with existing adversarial training techniques. Extensive experiments are conducted where the proposed method improves the natural accuracy as well as the robust accuracy when combined with existing AT algorithms.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Strengths:
1) The idea of improving adversarial robustness of model by explicitly learning robust representations seems interesting. Although the traditional AT and contrastive learning based AT implicitly learns robust features, this method attempts to achieve the same in more explicit manner.

2) The proposed techniques of utilizing asymmetric negative contrast loss and reverse attention to achieve exclusion and alignment during adversarial training are intuitive and are properly justified in the manuscript. 

3) The method provides impressive results as compared to previous state-of-the-art approaches. 



Weaknesses:
Weaknesses:

1) There are concerns regarding the proposed reverse attention strategy. During the testing, the true labels are not known and reverse attention uses the predicted label h(x) to calculate z'. In case the model provides wrong predicted class label, the corresponding z' will be also then multiplied with wrong classifier vector. This will further lead to degraded performance. The authors have not tried to address this scenario. 
2) From the works of [1] and [34], how is the proposed reverse attention different? Unfortunately the authors have not provided any comparisons or contrast.
3) In the ablation studies provided in Table 3, combining ANC and RT marginally improves results as compared to individual ANC and RT results. It looks like there is some sort of competition between the both proposed techniques.
4) Similarly, the use of negative samples via proposed targeted attack in Table 4 shows marginal improvements overall.
5) The paper is very difficult to understand, especially for the readers who are new to the technique of adversarial training. The presentation can be significantly improved. 

Minor weaknesses:
typo at line 281 scenaios -> scenarios 

Limitations:
The authors have not discussed any limitations of their work. 

Rating:
5

Confidence:
3

";0
9RUblEXVVD;"REVIEW 
Summary:
The authors process a training method to efficiently and automatically find optimal subnetworks without the need to configure the search space or a pre-specified supernetwork. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- A clever way to build the search space without manual intervention using zero-invariant group partition
- Avoiding the need to choose the supernetwork beforehand
- Automatically finding the optimal subnetwork
- Good results to show the validity of the method

Weaknesses:
- While OTOv3 is able to outperform prior art on smaller datasets like FashionMNIST, CIFAR, SVHN, it doesn't beat them in a larger dataset like ImageNet (It's still competitive). Examples: P-DARTS, AmoebaNet-C, PC-DARTS - similar FLOPS/PARAMs but better than OTOv3
- In practice, accuracy (or relevant metric) is one of the improvement metrics to optimize against. Therefore, would it not be better to have a suite of subnetworks that trade-off accuracy/flops vs just being given one automatically?

Limitations:
- Limitations of the approach have not been adequately discussed


Rating:
6

Confidence:
3

REVIEW 
Summary:
OTOv3 is an automated system that trains general super-networks without pretraining or fine-tuning, constructs search spaces automatically, and produces high-performing sub-networks. Experimental results show competitive or superior performance compared to state-of-the-art methods across different benchmark datasets and architectures.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Strengths are shown below:

1. OTOv3 with codebase sounds like a systematic work, which may have some actual impacts.

2. Exquisite drawing is appealing.

Weaknesses:
However, some weaknesses make me think that I am not fully agreeing with this article now.

1. Although I have read the comparison in the appendix, I find that the work lacks novelty when compared to OTOv2. It appears that some key modules in OTOv3 are simply improved versions of those in OTOv2. For example, the comparison between GeZIGs and ZIG, Dependency Graph Construction in OTOv2 and H2SPG versus DHSPG. As a result, OTOv3 does not seem to introduce any essential innovations but rather represents a slight technological improvement over OTOv2.

2. Some of the comparisons made in the experiments are unfair. In Table 2, the search cost is not fairly assessed since the author's value was obtained using a new GPU (A100), while others were conducted using an older version GPU. Additionally, the authors fail to consider the training time of the super-Net when comparing it with Training-free methods. In Table 3, some of the baselines used are outdated, not the most recent ones. The authors mainly compare papers published before 2020, with only one paper from 2022. A more appropriate reference would be OTOv2.

3. The methods sections are poorly organized. Even after reading the Method section multiple times, I am still confused about how this method actually works. The authors often introduce concepts without providing a sufficient explanation in the appropriate positions, resulting in explanations being scattered throughout the text, which is highly confusing. It seems as though the authors assume the readers are already familiar with these concepts. For instance, ""$x^*_{H2SPG}$"" is first mentioned in the introduction but not adequately explained.

4. The writing itself needs further improvement. For example, the sentence in line 39 ends abruptly, and it is unclear whether the subsequent sentences in line 40 should be read as a continuation. In the introduction, the authors use the phrase ""perhaps the first"" to describe their method. If the authors are confident that it is indeed the ""first,"" they should remove the word ""perhaps"" or refrain from using ""the first"" altogether.

Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes OTOv3, an approach to train general supernets and discover promising subnetworks. It claims to be able to automatically generate the search space, and construct subnetworks based on hierarchical half-space projected gradient. The proposed approach has been evaluated on a number of datasets, showing comparable performance to the state of the art. 

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
+ The proposed approach has been evaluated on a number of popular datasets. 
+ The objectives make sense, and the overall idea of automatically building a search space and then search for strong performing models is interesting. 

Weaknesses:
- It seems not clear how this 3rd version of OTO advances the previous versions. It seems the delta in this paper is quite minor. 
- There are many places throughout the paper, are either not clear or perhaps not correct. For instance, the very first sentence in the abstract:`Existing neural architecture search (NAS) methods typically rely on pre-specified super deep neural networks (super-networks)...` I think in NAS context this can be very misleading. Also the dependency graph construction seems to be quite standard in graph analysis. 
- Some of the experimental results reported are not very clear. How the proposed approach could be 4-5x faster than training-free NAS, if compared in a fair way? Does the 0.1 GPU day search cost include everything you need to do to get the final architecture? 
- Missing comparison with existing work such as TE-NAS, ZiCo etc. 

Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper ""OTOv3: Towards Automatic Sub-Network Search Within General Super Deep Neural Networks"" presents a new automated system called Only-Train-Once (OTOv3) for Neural Architecture Search (NAS). Unlike existing NAS methods that often depend on pre-specified super deep neural networks with handcrafted search spaces, OTOv3 can train general super-networks and generate high-performing sub-networks in a one-shot manner without pre-training and fine-tuning. The authors outline three main contributions of OTOv3: automatic search space construction for general super-networks; a Hierarchical Half-Space Projected Gradient (H2SPG) for ensuring network validity during optimization; and automatic sub-network construction based on the super-network and the H2SPG solution. The effectiveness of OTOv3 is demonstrated on a variety of super-networks and benchmark datasets, with the computed sub-networks achieving competitive or superior performance.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. The proposed method is backed by substantial theoretical considerations and empirical validation, demonstrating the quality of the research.
2. The paper is well-structured and clearly written, making the proposed method and its benefits understandable.
3. OTOv3 can be applied to a wide range of super-networks and has shown competitive or superior performance on several benchmark datasets, indicating its potential impact in the field of NAS.

Weaknesses:
1. The term search space and supernet are not well defined in the paper. To my understanding, supernet is a kind of representation of search space. In the paper, the author says that ""OTOv3 automatically generates a search space given a general super-network"", which is confusing.
2. The authors have not discussed the potential limitations of OTOv3, such as its possible limitations in search space, or potential for overfitting. These factors could impact its practical applicability.

Limitations:
See weakness.

Rating:
7

Confidence:
4

";0
qoiOpVrIEa;"REVIEW 
Summary:
The paper is an empirical study of different dimensionality reduction (DR) methods for brain activity data. 
Authors compare various approaches to brain representation using the MRI data from Human Connectome Project. 
In the manuscript, ""brain representations"" are called ""dimensionality reduction"" (DR) since they present brain MRI data in a compact way.
By DR, the author mean ways of presenting brain activity: selecting brain segmentation (parcellations), measuring activity inside these zones, calculating cross-correlation etc. The definition is different from the one used in ML/AI community, where by DR we mean algorithms like t-SNE, UMAP, etc. 
Authors use topological data analysis (persistent homology, topological bootstrap, prevalence score) to evaluate the quality of DR.
Lots of computational resources were (80, 000 CPU hours over the course of a month) were spent to such an evaluation.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
Neuroimaging is an active field of research. The paper is technically correct in my opinion. Some recent tools of TDA (like prevalence, topological bootstrap) are applied.



Weaknesses:
First of all, I'm not an expert in neuroscience, so I can evaluate only the dimensionality reduction/topology part.

1. In the manuscript, ""brain representations"" are called ""dimensionality reduction"" (DR) since they present brain MRI data in a compact way. By DR, the authors mean ways of presenting brain activity: selecting brain segmentation (parcellations), measuring activity inside these zones, calculating cross-correlation etc. The definition is different from the one used in ML/AI community, where by DR we mean algorithms like t-SNE, UMAP, etc. 
2. No contribution for ML/AI/DL. Typical conclusion from the study: ""As expected, we also saw that feature number was a more important driver of persistence structure than the underlying rank of the decomposition"". The conclusion gives insights about types of brain features, not DR algorithms from ML.
3. I didn't understand some parts of the paper with neuroscience jargon (cortical parcels, grayordinates, subject space). Captions on Fig. 2 are not explained, what does mean ""Shaefer600 pNMs Psim-ztrans"", etc?
4. Some references are missing and some relevant methods are not evaluated.

Overall, the paper seems to fit more traditional neuroscience community than NeurIPS community.
But I can be mistaken.

Limitations:
Authors adequately addressed the limitations.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper investigates shared geometric structure across different (very broadly speaking) dimension reduction algorithms for functional brain connectivity. The authors examine different connectivity representations through persistent homology via topological statistical and bootstrap. 

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
- The paper is written clear language. 
- The paper proposes novel metrics to evaluate graph structures. 

Weaknesses:
- This paper is quite ambiguously written and not self-contained although the languages are clear. 
- I believe the presentation can be far improved by explaining background better, restructuring paragraphs and better description of mathematical notations. 
- For example, topological sampling is not well explained and readers would have to rely on other papers. Also, sections do not flow smoothly as mathematical notations are either not consistent or variables in equations do not connect. 
- There is no baseline experiments and the result from the analysis cannot be properly validated. 
 

Limitations:
There is a section describing the limitation. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors study shared geometric structure across different dimensionality reduction (DR) algorithms applied to neuroimaging data (fMRI data from the Human Connectome Project). In particular, they compare different DR algorithms (which they call ""brain representations"") by applying them to the same data sample and comparing the resulting Vietoris-Rips complex in each low rank data embedding using a modified topological bootstrap and cluster on the resulting estimated topologies.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The authors do introduce framework for the comparison of DR methods that work with any data or dissimilarity measure amenable to Vietoris-Rips filtration and apply their method to real neuroscience data. 

Weaknesses:
The authors seem to have put in a good amount of work. However the paper currently lacks motivation or clear significance. Instead the paper reads like a number of different parcellations and DR methods were all applied to a public dataset after which a number of somewhat justified, somewhat arbitrary sequential analysis decisions were made to arrive at a clustering to determine similarity. Good background is given on each step, but it is not clear where the novelty lies here, or why these steps are the right ones. It is unclear exactly what deeply useful conclusions can be drawn from this analysis.

The writing and definitions could be much more clear throughout. Some are used before they are defined well, some terms with precise meaning seem misused (e.g., induced topology seem misused; ""induced on...data""), some terms are highly redundant and misleading (e.g., ""brain representations"").

The github link is not really anonymized (there is another project at the same github link clearly from the ""Personomics Lab""). 

Limitations:
The authors have a limitations section.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes an approach of comparison of different standard dimensionality reduction technics (DRT) of fMRI, using topological data analysis tools. In this case, these DRT computes lower dimensional representations of the Human Connectome Project dataset.
Then, for this specific dataset, the authors associate to each DRT (and for each feature type of each DRT) divergence matrices computed using correlations of these representations.
Finally, these matrices, seen here as distance matrices, are compared to each other, by 
 1. Computing persistence diagrams associated to the Vietoris-Rips complex of these matrices,
 2. Reweighting these diagrams by *prevalence scores*,
 3. Computing wasserstein distances between these diagrams,
 4. Cluster the DRTs using this distance matrix.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
Looking at the fundamental differences between different dimension reduction techniques is a very interesting topic: on one hand, this can help increasing the performance of these methods by cleverly taking all of them into account; and on the other hand, this allows to have an insight on what these algorithms are retrieving from the original data. This also motivates to ignore the individual statistical performances of these algorithms. Furthermore, looking at the topology, and the *prevalence* of topological structures of the output of such method, for such a geometric dataset (brain representations), seems to be a very interesting and promising idea.

Weaknesses:
Some techniques used to tackle this problem are not natural or are not motivated enough; either intuitively or theoretically, especially the prevalence-weighted wasserstein distance. In particular, I have the following comments:

 - The bijections between persistence diagrams usually add the points of the diagonal, with an infinite mass. In particular, 
    - how is defined the prevalence on the diagonal?
    - If the diagonal has no mass here, there are also a few problems
        - A bijection may not exist (not the same cardinal)
        - This distance is not symmetric, as points of $d_2$ that are not matched to points of $d_1$ are not taken into account.
 - Multiplying points in the diagram by a real correspond to do an **homothety with center $(0,0)$**, which raises a few questions
    - To my knowledge, 0 has no particular role in a diagram, so what is the motivation for using it?
    - For a same prevalence $\alpha$, the rescaling depends at which scale the topological structure appears. And the same goes for the angle direction in which the points are moved. Is this behavior wanted, and why? 

I think that taking into account the prevalence is an interesting idea, but I'm not convinced that this is the best way to do it.

Moreover, there are small typos:
 - Section 2.1
   - Feature types details could be improved, i.e., mathematical definitions.
 - Section 3.2.1
   - Mention who is $k$
   - The Vietoris Rips filtration is not a graph, but a clique complex (it has simplices of arbitrary dimensions)
   - This section could be made bigger, for non-TDA practitioners. 
 - Section 2.3.2
   - Clarity could be improved here,
   - line 196 ""multiple data element to [represent] the same homology [class]""
   - line 205 : intervals are never defined

Limitations:
Limitations have been addressed.

Rating:
3

Confidence:
3

REVIEW 
Summary:
A very interesting and rigorous exercise of comparison of embeddings for the purpose of evaluating manifold learning is presented. The chosen framework is root in trendy topological concepts such as persistent homology for the purposes of analyzing the data topology mixed with geometry-based measures of similarity across representations, and coupled with stochastic (topological) bootstrap to study variations over co-embeddings.
Contributions are explicitly mentioned in lns 113-117 and certainly delivered.
Best of my lot for this year.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
+ The idea is exceptional IMHO. I’ve known of (and used myself) some other frameworks trying to establish and understand similarities or dissimilarities of projections, but they were all much more naïve. This one offers a clearly more sophisticated approach that yields a much richer picture without substantially sacrificing interpretability of results, with the “only” price to pay of a very large computational cost.
+ Extremely well explained despite the very complex concepts involved.
+ The observation on lns 290-1 that cycles with low persistence may carry meaningful structure is something that I have thought myself occasionally but couldn’t put my finger on it nor how to articulate it nor know how to reveal it. This is a very nice confirmation of that intuition.


Weaknesses:
Not many that I can see to be honest…
+ The study is only conducted in experimental data from the human connectome project but it is never validated in synthetic known manifolds with and without added noise. This means that some of the observations in the last part of the draft are (most likely correct but) difficult to verify. For instance, the implication that the proposed framework distinguishes more feature types and numbers than representation types.


Limitations:
+ Lns 92-106 literature review is perhaps missing some of the most primitive approaches; e.g. distance distortions plots, (graph/manifold) isomorphisms, … this is possibly intentional as the persistence element there is only implicit rather than explicit as in the case of the works reviewed. But if it wasn’t, well, it is perhaps convenient to at least mention some early efforts.
+ Perhaps the number of compared embeddings (brain representations) is not as large as one would like for this type of exercise but the authors clearly state why they stay on low numbers (computational cost) and promise larger comparisons in the future. Looking forward to those!


Rating:
8

Confidence:
4

";0
R4ivHjNi8V;"REVIEW 
Summary:
This paper addresses the challenges associated with the tensor nuclear norm, which is a proxy for the low-rank property of tensors. Existing transformations for the tensor nuclear norm lack adaptability and theoretical guarantees due to fixed or non-invertible nonlinear approaches. This paper introduces a practical and data-adaptive framework and provides an exact recoverable theoretical guarantee. Experiments are included to support the theoretical guarantees.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper contains solid theoretical results for a very challenging problem. It provides efficient algorithms for recovery that are explained well and clearly. The experiments include both real and synthetic data.

Weaknesses:
The paper is dense to read in some places, like in the slurry of definitions at the end of Section 2. This is likely due to space limitations, but reduces the readability of the paper. The main results include some pretty strict assumptions like incoherence and uniformly distributed support sets. Although some of these are also used in other results, it could be discussed further what kinds of tensors have these properties in practice and how sensitive the methods are to these.

Limitations:
The authors discuss interesting future work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a new approach for learning a data-adaptive and learnable column-orthogonal matrix (COM) transform for tensor nuclear norm (TNN) minimization. The authors show that the proposed transform can capture the low-rank structure of tensors more effectively than existing fixed or nonlinear transforms. They also provide theoretical guarantees for the exact recovery of tensors under the COM transform. The proposed method is applied to tensor completion and tensor robust principal component analysis tasks and achieves better performance and efficiency than several state-of-the-art methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors build up an adaptive tensor nuclear norm, and develop tensor completion robust tenor PCA model. 
2. To guarantee the theoretical correctness of the proposed models, the authors analyze two theorems to ensure the exact recovery capability.

Weaknesses:
1. The proof architecture is regular and simple and has been used in [1,2], thus the theoretical novelty of this paper is rather limited.
2. The adaptive TNN model has been already proposed in many related works, but the authors missed some important related references, see the linear invertible TNN-based RPCA [1,2], the nonlinear transform TNN [3], and the subspace denoising strategy NGmeet [4]. Thus, a related works section should be considered to give detailed differences between these methods and the proposed ATNN.
3. In the experimental section, the authors only compare the computation-cost methods and neglect the related adaptive TNN methods.

[1] Lu C. Transforms based tensor robust PCA: Corrupted low-rank tensors recovery via convex optimization[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 1145-1152.

[2] Lu C, Peng X, Wei Y. Low-rank tensor completion with a new tensor nuclear norm induced by invertible linear transforms[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 5996-6004.

[3] Yisi Luo, Xile Zhao, Deyu Meng, and Taixiang Jiang, ‘‘HLRTF: Hierarchical Low-Rank Tensor Factorization for Inverse Problems in Multi-Dimensional Imaging,’’ IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[4] He W, Yao Q, Li C, et al. Non-local meets global: An integrated paradigm for hyperspectral denoising[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 6868-6877.

Limitations:
Please see the above Weakness section.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces a new method of learning a transformation that can make the tensor nuclear norm better capture the low-rank structure of tensor data. The method is fast, data-adaptive, and has a theoretical guarantee of being reversible. The paper shows that the proposed method works well in experiments.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper presents a clear and reasonable idea that is well written and easy to understand. 

2. The paper proposes an adaptive transformation matrix, which has a better expressiveness than methods with fixed transformations. 

3. The paper proves that ATNN has exact recovery capability under certain conditions. 

4. The paper conducts extensive experiments to validate the theoretical results and demonstrate the effectiveness of ATNN.


Weaknesses:
1. The performance of ATNN is limited by the linear transformation. As shown in Table 3, S2NTNN achieves higher accuracy and faster speed than ATNN. Since the theoretical analysis of deep neural networks is challenging and remains an open problem, the theory of S2NTNN is harder to establish.

2. ATNN has an advantage over TCTV in terms of computational efficiency for color video completion task, but it cannot surpass TCTV in terms of overall model performance.

3. Typos: Eq.(2) $A^{(k)}\rightarrow A^{i}$


Limitations:
See Weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a novel Adaptive TNN (ATNN) for tubal rank (t-SVD)-based low-rank tensor recovery and apply it to Tensor Robust PCA and Tensor Complete problems. Theoretical recovery guarantees have been established which is technically sound. However, tubal rank is known as an analog to matrix rank. Compared to CP rank and Tucker rank, the theory of tubal rank is relatively easy to work with. That said, the reported theoretical results are solid but no surprise. In my option, the proposed ATNN is an analog to matrix sketch, although fast, it is not equivalent to the standard t-SVD.I also think 'learnable' is a misleading term in this paper.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Solid analysis reslut.

Weaknesses:
1. In my option, the proposed ATNN is an analog to matrix sketch, although fast, it is not equivalent to the standard t-SVD.Just like matrix sketch is not equivalent to the stardand SVD. I don't see this discussed in the paper.

2. When a matrix algorithm is learnable, something like 
      Deep convolutional robust PCA with application to ultrasound imaging, ICASSP 2019.
      Learned Robust PCA: A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection, NIPS 2021.
come to my mind. To me, 'learnable' is a bit misleading in the paper. Anyhow, the learnable feature of the ATNN is neither discussed enough nor numerically demonstrated through the paper.

3. The reported numerical results in Tables 3-5, the proposed approach doesn't show sufficient emperical advantage.

4. The notation is really messed up in some part of the paper. For example, in (12), \mathcal{E] should be \mathcal{S} (or the other way around). In (13), \mathcal{S} should be \mathcal{E}.

Limitations:
No negative societal impact was found.

Rating:
4

Confidence:
3

";0
2MRz5bSnan;"REVIEW 
Summary:
The authors proposed the permutation decision trees method, which uses Effort-To-Compress as the impurity measure to model the order dependencies of data instances, and extended the proposed permutation decision tree to a variant of random forest. They also did some experiments to compare the performance of the proposed methods with random forests. 

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The proposed structural impurity can actually capture the order dependencies of data instances, as shown in the examples in Table 1. 

Weaknesses:
The paper exhibits several weaknesses, which are outlined below:

1. Insufficient clarity regarding the chosen setting: The authors' intended focus appears to be on time series data; however, the task discussed pertains to multi-class classification, which is an i.i.d. setting. The authors are recommended to formalize the problem setup.
2. Inconsistent use of notation: The paper demonstrates inconsistencies in notation usage. For instance, the features presented in Table 3 are denoted as $f_{1}, f_{2}$, whereas in Figures 3-7, they are represented as $x_{0}, x_{1}$.
3. Unfair experimental setup and insignificant results: The experiment setup lacks fairness, and the obtained results do not exhibit statistical significance. In the only out-performing dataset, the random forest model employed only one tree, while the proposed permutation decision forest utilized five trees, indicating an apparent unfairness in the comparison. Furthermore, the hyperparameters' n_estimators vary across different datasets, which is deemed unreasonable. 

Limitations:
The authors adequately addressed the limitations in Section 4.

Rating:
2

Confidence:
4

REVIEW 
Summary:
In traditional decision tree algorithms such as CART and C4.5, impurity measures are used to split internal nodes. The paper proposes a decision tree induction method by using effort-to-compress (ETC) measure, which can capture order dependencies in the data. With ETC’s ability to capture order dependencies, permuting the data can result in different trees, thereby constructing a forest without the need for bagging. This proposed decision tree induction method can be used for datasets with temporal order. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper uses ETC as a new impurity measure for constructing decision trees. Since ETC is sensitive to the order of data points, the tree built using this measure may be well-suited for temporal datasets. And it also provides a different way for constructing diverse trees and thereby getting a forest. Overall, the paper is clearly written and easy to follow. 

Weaknesses:
- What about the bias and variance in the permutation decision forest? Random forest uses bagging and random feature selection to make trees in the forest uncorrelated, thereby reducing variance. But trees in the permutation forest are not uncorrelated. Using the ensemble of these correlated trees may not reduce variance. 

- In the toy example, some leaf paths are shown in different trees. I am wondering if there will be a significant number of duplicated leaf paths within the permutation forest. 

- Experiments only show the comparison between random forest and permutation tree forest in terms of F1-score. How about other evaluation metrics, e.g. misclassification loss? The results don’t show that the proposed method outperforms random forest. And there is no comparison between the performance of single trees, such as CART vs. ETC tree. 


Limitations:
The authors have addressed the limitations of the paper and propose future directions. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper ""Permutation Decision Trees using Structural Impurity"" introduces a novel split criterion for the training of decision trees that also takes the order of labels inside the training data into account. This way, to obtain a forest, one only needs to shuffle the data before training individual trees. Moreover, the novel split criterion supposedly works better for data that includes (temporal) dependencies, although der are no experiments to support this claim. 

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
- I think the idea of tackling non-iid data with a novel split criterion is nice, and Effort-To-Compress as an impurity measure seems like a good choice

Weaknesses:
- The experimental evaluation is very weak. The authors compare their method on 6 small real-world datasets and one artificial dataset and compare it only against Random Forests. Moreover, their method seems to be worse compared to RF. Hyperparameters are also incomparable, as the RF uses smaller trees than their method although it is well-known that RF benefits more from larger trees. In addition, the number of estimators changes for every experiment. There is no clear experimental protocol, and the authors do not use random repetitions and/or cross-validation but resort to a single train/test split. The experimental evaluation is hence borderline useless and can only be seen as a first test-experiment.  
- The paper contains limited valuable information. While the Effort-To-Compress (ETC) measure seems to be of central interest here, the authors do not present a formal mathematical explanation of it. They mention the NSRPS algorithm to compute ETC, but also do not explain it mathematically, and only offer a single example. A thorough mathematical explanation and the typical explanations of the notation (Model function f(x), samples X, labels Y, etc.) is missing entirely. 
- The authors deal with the case in which the order of samples is important. This is completely against the typical IID assumption we have in Machine Learning. Unfortunately, the authors neither discuss this (certainly interstring) difference in more detail nor do they really present any real-world example of this. 
- A dedicated Related Work section is missing, although there is plenty of space left in the paper. The authors decided to waste roughly two pages by printing different DTs, which does not add any new information to the paper. This space would have been used better to highlight related work or pinpoint the novelty of this work in more detail. 
- Eq. (1) and Tab. 4 do not fit the page width

Limitations:
The authors acknowledge that their method is worse compared to the state of the art and intend to perform more testing. As this paper presents a novel method I don't see any immediate negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper proposes a novel in Decision Tree literature splitting criteria based on Effort To Compress (ETC) gain. Use of this criteria is justified by a desire to work with data that doesn't conform to i.i.d. assumption about the generating distribution. There is an experiment on a synthetic data that shows that different decision trees are generated when different orderings of the data are used for training. A permutation voting forest is introduced, that allows using random permutations of full data to obtain multiple different decision trees for use in the final ensemble of trees. There is an evaluation of Permutation Voting Forest against regular random forests on multiple real world datasets that however show slightly lower results when using proposed method. 

Soundness:
2

Presentation:
2

Contribution:
4

Strengths:
- The paper opens a novel line of research about using Decision Trees for modeling data, that comes in a sequence and does not follow i.i.d. assumption. 
- There is a novel application of ETC measure as a splitting criteria in decision trees.
- A generalization of the proposed model: Permutation Decision Forest is introduced, that uses a novel idea of shuffling the data in the context of a splitting criteria that generates different trees for different permutations of the training data. 


Weaknesses:
- It is noted that usage of ETC allows to get rid of i.i.d. assumption. But this claim needs more thorough theoretical analysis. If we want to keep sequential structure of the data, the sequence still gets destroyed upon split: split does not split examples in a consecutive way; some examples may go to the left split, then some to the right, then again to the left part of the split and so on. So, new left and right sequences after the split will have completely different properties. Considering an example from introduction, where ETC is used: musical compositions. Splitting the musical composition according to some feature, like presence of some range of frequencies at a given moment, will result in an unpleasant music on both sides of the split, because instead of hearing half of the musical composition, we will hear a ""fractal"" - small parts of original sequence with small gaps inbetween, that got assigned to left or right parts of the split
- Related to the previous point: at the testing phase there is no ""memory"" in the model, and the model still predicts elements by looking at them one by one. So, shuffling the testing set will result in exactly same predictions. Can we say that the problem of non-i.i.d. distribution is solved, if the behavior on the testing set is equal to the behaviour of the i.i.d. models?
- Testing of regular Decision Trees with proposed splitting criteria on real data is needed (only the forests were tested on real data, but proposed forests work differently due to the proposed shuffling of the input data, so regular trees must be evaluated separately as well). It would be nice to both test on regular datasets (that are not sequentially ordered, like the datasets from section 3.2), and also to find at least some example real datasets where ordering is important, and where proposed model (regular permutation decision tree) is both practically and theoretically better than the baseline decision tree models.
- In section 3.2 a more thorough experimental design would be more convincing. (a) If we compare proposed model to the baseline, why hyperparameters are different for same dataset? If hyperparameters tuning was done, that should be thoroughly described. (b) Experiments should be run several times on different train-test splits and mean scores and standard deviations should be reported to allow fair comparison in the presense of noise.


Limitations:
Limitations are well described in the paper, which is good. Since there were identified significant limitations in terms of accuracy of the proposed permutation decision forests (that may also affect proposed single permutation decision trees), it would be more convincing to include additional experiments that will clarify the extent of such limitations right away without deferring them to the future work. 


Rating:
4

Confidence:
5

";0
a648X9AoL4;"REVIEW 
Summary:
This paper presents an approach (called Tree-of-Thought, or ToT) for boosting the problem-solving abilities of LLMs by means of backtracking in solution space. The proposed ToT architecture augments the LLM with four modules, and is broadly framed to include multiple potential implementations of those modules, including neural networks (for the prompter and controller) that could be trained by means of policy gradients. One implementation of ToT is evaluated on Sudoku, where it demonstrates significant improvement over an LLM without the ToT augmentations.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
The motivation is highly topical, as improving the reasoning power of LLMs is a challenge of great research interest and economic value. Most of the paper’s presentation is clear, and the experimental results seem fairly sound, as far as they go.

Weaknesses:
The work is limited in four ways.

1 - While the paper lays out a fairly sophisticated and general architecture, only one narrow implementation of ToT is actually tested. Specifically, the current version does not include the policy-gradient options laid out in Algorithm 1 and equations 1-4. Instead, the tested implementation uses the much simpler rule-based controller and LLM-based prompter that have no additional weights to train. This makes it impossible at present to draw conclusions regarding the effectiveness of more advanced realizations of the architecture. 

2 - The evaluations, limited as they are to Sudoku, do not demonstrate the generality of the approach, despite the assertion that ""in principle it can handle many other mathematical and logical reasoning tasks''. For comparison, consider the recent work of Yao et al., 2023, *Tree of Thoughts: Deliberate Problem Solving with Large Language Models*, which reports evaluations on three separate problem types:  Game of 24, Creative Writing, and 5x5 Crosswords.

3 - Since humans play Sudoku in graphical rather than language-based forms, there is little reason to expect text-only LLMs to perform particularly well on Sudoku at all. And for any algorithmic puzzle (like Sudoku) for which solutions can be explicitly verified, it is unsurprising that explicit checking for valid solutions would boost the performance of an LLM alone. The more central question is how much LLMs contribute to solving such puzzles, but this question is not addressed. The *baselines* tested (zero-shot, one-shot, and few-shot) are actually just *ablations* of ToT, all of them using LLMs.

4 - This paper discusses none of the prior work on Sudoku, such as *SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver* by Wang et al., 2019, which provided a Sudoku benchmark dataset that was used by Ahmed et al., 2013, in *Semantic Strengthening of Neuro-Symbolic Learning*. The *Neural Logic Machine* of Dong et al., 2019, has also been applied to Sudoku (https://github.com/ashutosh1919/neuro-symbolic-sudoku-solver), achieving a 94% success rate on 5x5 puzzles, which is significantly better than the ToT results reported in this paper. 

Because of these limitations, the work’s contributions do not seem significant at this point. Nevertheless, the general architecture might prove to be significant once it is fully implemented and tested on a broader set of benchmarks for which baseline results from the literature are available for comparison. 

Limitations:
Regarding the first limitation described in the Weaknesses section of this review, the paper states ""We expect a neural network based ToT controller will perform better, which we will verify in the future extension of this work."" However, the crucial point is that policy-gradient training is not evaluated at all, and this limitation is never stated with sufficient clarity. The other three limitations are not adequately addressed. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper present tree-of-thought (ToT) as a way of using LLMs to solve problems. ToT involves search + backtracking in a tree-like structure. The work demonstrates the success of this method in simplified sudoku tasks.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The idea is interesting, and the Sudoku tasks are a reasonable regime for evaluating it.

Weaknesses:
While the method is interesting, the evaluation is minimal. Furthermore, while the sudoku tasks are reasonable as a single setting for evaluation, as the only setting they are quite a narrow domain. The paper also seems to be missing some valuable ablations/baselines. In more detail:

1) The ToT method appears to contain many details which are not individually tested with ablations. Without testing these, it is unclear which aspect is relevant, and what we should learn from the work. 
    - For example, the paper emphasizes the benefit of the tree-structured generation, which allows the model to backtrack in its reasoning. However, if I understand correctly, none of the baseline experimental conditions include *any* form of generating multiple answers, or using the checker to verify the answer is correct. For example, rather than a tree search, the model could just run 10 complete rollouts (productions of the correct answer), and then use the checker to identify the correct one (if any). Or, the model could use the checker to decide at each step whether to accept a generation, but without backtracking further. Would one of these methods perform as well? If so, then the key feature is not the tree structure per se, but merely the possibility of generating multiple completions and then checking. In order to understand the contributions of the method, it would be necessary to see these ablations. 
    - Likewise, the ToT method appears to benefit from a prompter policy which can decide on the examples for the prompt (if I understand correctly), while the other methods don't; perhaps a prompter is all that's needed.
    - More fundamentally, if I understand correctly (although the paper is not entirely clear on this point), the ToT method is the only one that involves training; there are many ways the one/few-shot prompts could be ""trained"", such as selecting the examples in the prompt. Ideally, the methods would be evaluated in a setting where the baselines can also benefit from training. 

2) The evaluation is quite minimal, in both scope and depth.
    - The method is only tested on a single domain (simplified sudoku, without the box constraints); sudoku is a well-known puzzle, and so is likely to be well-covered in the training corpus. While that does not mean the present results are invalid, it would be useful to see a demonstration of the idea in other, less commmon domains (even other NP ones, such as Traveling salesman, are likely much rarer in the training corpus), or ideally fully novel ones.
    - What evaluation there is only involves a handful of puzzles (10 per condition). The differences between ToT and FS in each condition would not achieve statistical significance by an exact binomial test. While the consistent benefit of ToT across conditions makes it more plausible that there is an overall positive effect, it would be useful to run a larger number of puzzles per condition in order to more accurately assess the magnitude of that effect.

3) The paper could do a more thorough job of situating the present work within the existing literature with similar motivations, e.g. https://arxiv.org/abs/2208.14271 or https://proceedings.neurips.cc/paper/2021/hash/d3e2e8f631bd9336ed25b8162aef8782-Abstract.html

4) The paper presents itself as though it evaluates on Sudoku, but in fact it evaluates on a simplified variant (no boxes). The present abstract, for example, seems somewhat misleading in that it never makes clear that the algorithm is tested on a smaller, simpler version of the task.

5) It would be nice to see comparisons with other language models (especially smaller ones, and ones with an open training process) to understand how general the results are.



Limitations:
Yes, limitations are discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper presents a novel algorithm ToT (tree-of-thought) based on: 
- LLM (GPT 3.5 in this case)
- checker module (which verifies solutions and partial solutions)
- memory module
- ToT controller that guides the search (it can be a neural network or a set of rules)
- prompter agent (in this paper this is a policy network that selects the best in-context examples for a given tree node

The toT algorithm is tested on a challenging Sudoku task for 3 different board sizes.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The approach is novel. It mixes general LLM with two neural networks, trained together. The introduction Section is good: it identifies two main limitations for using LLMs in complex problem-solving. Sudoku is a complex task that requires backtracking and a search, which makes it interesting in the context of ToT. The training of policies is an interesting idea that could be of use in other LLM-based algorithms. 

Weaknesses:
The biggest weakness of this paper is the small number of experiments, which also are conducted on a single task (sudoku). In the text, many different versions of ToT are discussed, however, experiments are done only for a single setup and a single task. ToT was not tested on any other task, thus we cannot know if it really generalizes at all.

There are far too few experimental results and data. What is missing:
- How many nodes ToT needs on average to solve a given task?
- How many steps baseline needs to solve a given task?
- There are only 3 versions of the Sudoku: for n=3, 4, and 5. (this would be ok if there was more tasks). What happens for n >= 6? If ToT is still the best then it would be great for the method. If not, we will clearly know where is the limit for ToT. If the method is too slow for n>=6 it is important to know.
- What is the price (or number of tokens needed) on average for a single ToT run?
- What was the number of Sudoku puzzles used for evaluation? It is not clearly stated in the text, however, I guess it was 10 boards for each n=3,4,5. If I am wrong please correct me. If it was 10 boards then the results cannot be trusted at all. In such a case for the 0.4 success rate, the 90% confidence interval is (0.15 - 0.7), which tells nothing about the real results. Results on 10 testing boards have no scientific meaning and this is the main reason for such a low score I gave. If I am wrong (and the number is higher I will be happy to improve the score).
-There are no error bars in Figure 2.


Authors claim that: ""If the ToT framework can solve instances of the generalized Sudoku [...] in principle it can handle many other mathematical and logical reasoning tasks."". The claim that ToT should be able to handle other complex problems is based on the idea that complex tasks require a similar way of thinking  More advanced mathematical problems like automated theorem proving have their own sources of complexity (e.g. choosing the appropriate lemmas to consider or how to represent the state in a compact form, which fits to the transformer). I know that authors do not claim that for sure ToT works in such tasks, but after reading the paper it seems that the significance of the paper is built upon a promise that ToT can be easily adapted to more serious problems. Since there are no experiments to support this claim I think that the significance of this paper is limited. 

There are no experiments concerning other variants of ToT: for example with neural network checker or rule-based policy. 

Notation in Algorithm 1 is hard to understand. I had trouble understanding which \pi stands for ToT policy and which for prompt policy. Please consider more natural notation like \pi_{tot} \pi_{prompt} or similar. 

Algorithm 2. The meaning of (nil) is not introduced in the algorithm, it is only later in the text. 

I think that the version of the text review should use specific LaTeX options: for example line numbers. It is hard for me to refer to concrete lines without them.

It makes no sense for me to describe the procedure of training ToT policy if it was not used in the experiments.

Limitations:
In the paper, there is no separate section for limitations (some ar ementioned in Section 5). Many missing limitations I already described in the Weaknesses Section of this review.




Rating:
2

Confidence:
4

REVIEW 
Summary:
The paper introduces the Tree-of-Thought (ToT) framework, a novel approach to enhance the problem-solving capabilities of large language models (LLMs). The ToT technique mimics the human mind's trial-and-error thought process, allowing LLMs to explore the solution space of complex reasoning tasks and backtrack when necessary. The paper presents an implementation of a ToT-based Sudoku puzzle solver and evaluates its effectiveness on a suite of Sudoku puzzle benchmarks. The experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. 

The contributions of the paper include introducing a new approach to enhance the problem-solving capabilities of LLMs, presenting an implementation of a ToT-based Sudoku puzzle solver, and demonstrating the effectiveness of the ToT framework on a suite of Sudoku puzzle benchmarks.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
1. The motivation for moving from linear reasoning, like Chain-of-thought, to a tree-like searching/reasoning is strong and well recognized. Considering the fundamental limitation of autoregressive generation of GPT-like LLMs, we do need more advanced reasoning/search algorithms for better decoding.
2. The proposed method is reasonable and technically sound. The checker module echos the recent findings of self-evaluation of LLMs, and the memory module also is useful in agent-based modeling. 
3. The empirical results on the Sudoku pizzles show the effectiveness of the proposed method, esp. when the puzzle becomes harder, the performance of the proposed method is still good.

Weaknesses:
1. One of the biggest issues of this paper is the mismatch between the described method and the actual one used in the experiments. The paper spends lots of space talking about how the ToT controller and prompter agent can be modeled by a policy network and trained via multi-agent RL. But it never tried such formulation and training in the experiments and only presented them as some kind of future work. Without valid evidence, empirically or theoretically, the method section is largely questionable. 
2. Another issue is the novelty of this work probably is not as big as the paper claims. The formulation of multi-agent RL for controller and agent probably is overcomplicated, and I encourage the authors to think of reformulating them all as LLMs to make things easier. Also, there are many missing related works [1, 2, 3, 4] that have a similar tree search/reasoning formulation with more operational and rigorous experiments.
3. The experiment scope is limited. The proposed method is only demonstrated in one task with the simple formulation of the controller and agent (discussed above). This isn't ok for NeurIPS papers, and we need to better figure out why and how the proposed method can or cannot be applied to more general tasks. 

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes a tree-of-thought (ToT) framework to improve complex reasoning and problem solving capabilities of auto-regressive language models. Specifically, motivated by how humans process thoughts with trail and error, ToT maintains a memory module, and employs a ToT controller to decide when to proceed on a thought of nodes in a tree, and when to backtrack to a previous parent node depending on a tracker. Evaluated on Sudoku 3x3, 4x4, and 5x5 puzzles, results suggest that ToT is effective compared to few-shot baselines.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. This paper presents an interesting tree of thought method to enable back-tracking in auto-regressive language models. This solves one of the key limitations of LLMs. Similar to humans and inspired by system 2 reasoning, the proposed ToT structure, especially how we can employ a checker to dynamically modify and utilize memory, makes a great contribution to the field, and can inspire future work on how LLMs can be prompted, and even pre-trained.


Weaknesses:
1. Although the high-level idea of tree-of-thought is promising, with corresponding ToT controller, agent, and memory, the paper is only evaluated on one Sudoku task, especially when the details of evaluation (e.g., number of games evaluated, and computational cost and prompts used compared to the baselines) are not specified. This makes the evaluation results less convincing. Moreover, despite that the method sounds generalizable, there is no strong evidence on how each module in the framework should be actually implemented to be effective and demonstrated (apart from simply mentioning some future work in the end). 
2. It is not clear how each module in the ToT framework should work in details. For example, the memory module seems to be compelling where the LLM can retrieve previous configuration when backtracking, there is no explicit demonstration of how the memory is maintained, and how backtracking would work. Furthermore, the ToT controller is rule-based in the experiment, but Section 3.2 and 3.3 mostly explains how the ToT controller should be trained similar to a policy network. This makes it very confusing to judge the proposed method. I would suggest the authors to add more detailed illustrations using specific examples in the paper revision.

Limitations:
The authors only briefly mentioned the limitations in terms of implantation, but not the limitations of the method overall, such as computational cost.

Rating:
4

Confidence:
4

";0
Cu3T82cegI;"REVIEW 
Summary:
Even though there have been a lot of studies on learning in noisy settings, they are often disparate, and the authors aim to present a unified view of a diverse class of corruptions and their implications on learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper offers an interesting direction of looking at existing learning formulations in a noisy setting. It attempts to unify a more extensive class in a single umbrella and then analyzes the different settings through the unified framework.
- They propose a new taxonomy of corruption in classification sitting and attempt to link the existing works to the taxonomy.
- The authors also derive a loss correction formula, which would help infer the consequences of different types of corruption on learning.

Weaknesses:
I encourage the authors to discuss the practical implications of the paper's theory, which would strengthen the submission.
- It would also be helpful to understand how much insight their work provides compared to the corresponding works for each problem. For instance, what advantages does unifying the noises & studying them offer as opposed to studying them in separation?
- One of the issues is how realistic the assumption is that we know the process, which adds to the corruption. Does the Markov kernel capture all the classes of often encountered corruptions (or what assumptions does it impose on each class of noises)?
- In practice, the models often pick up spurious correlations in the data. Does that also fit into the taxonomy considered? How does the bias problem (regarding the fairness of predictors) fit into the framework (even if that can be considered a noise-generating process)?

Limitations:
The authors do talk about limitations, but it isn't complete. I encourage the authors to refer to the questions & address them in the list of limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper presents a theoretical framework for analyzing corruptions in machine learning using Markov kernels. Within this framework, the authors establish a taxonomy of various types of corruptions and provide data processing equalities for each category. Additionally, the paper derives corrected loss functions for different types of corruptions within their theoretical framework.

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
The authors categorize corruptions in a rigorous and systematic manner. This classification and the theoretical view from Markov kernels can potentially help future research future research in understanding and studying different types of corruptions. The derived corrected loss can potentially be used to improve performance of machine learning algorithms in the presence of corruptions.

Weaknesses:
Overall, I find it hard to appreciate the contribution of the paper. It lacks clear intuition and practical implications, making it difficult to understand its usefulness. Theorems are presented without concrete insights, and it is unclear how the results can guide real-world practice.
1. In Section 4, the paper discusses the ""consequences"" of corruption in supervised learning. However, there is a lack of concrete interpretations of these consequences. How do Theorems 3, 4, 5 translate to real-world machine learning scenarios? What can we deduce about the nature of these corruptions and their implications? 
2. The paper introduces a corrected loss function, but it is not well-explained how it operates or why it is effective. What is the underlying intuition behind this correction and how does it improve the performance under label noise?
3. It would be beneficial if the authors could provide concrete examples to illustrate how the corrected loss function can be implemented. For instance, what would be the formula of the loss in the context of cross-entropy loss under label noise? Right now it is even unclear whether these losses are feasible and whether they require any information that is not observable.
4. It would also be valuable to include experiments that demonstrate the effectiveness of the derived corrected loss function.

Limitations:
The authors have adequately discussed some limitations in section 6. For other limitations, see Weaknesses.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This study addresses the prevalent issue of data corruption in machine learning and the lack of a unified understanding of how different corruption models interrelate. The authors analyze corruption models at the distribution level using a comprehensive framework based on Markov kernels, providing a fresh perspective on the issue. They draw attention to joint and dependent corruptions on both labels and attributes, which existing research often overlooks. By examining the effects of these corruptions on supervised learning through Bayes Risk changes, the authors gain insights into the implications of complex corruptions. The proposed framework also has applications in corruption-corrected learning, which the authors explore in this paper. However, the paper does not have any empirical contributions, and has several theoretical discussions that need further clarifications. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper delves into the crucial problem of data corruption in machine learning, offering a comprehensive framework based on Markov kernels to analyze corruption models at the distribution level.

2. It paves a way for future quantitative comparisons by delivering a foundational understanding of more complex corruptions' consequences.

3. The authors' taxonomy of corruption is comprehensive and organized hierarchically through the notion of dependence, helping to relate existing corruption models.



Weaknesses:
1. the paper does not involve any empirical studies, which is very rare for this venue, although there are precedents. Most theoretical papers still choose to validate their finding with some small-scale experiments, the authors does not seem to clarify a particular reasons that they do not need any empirical evidence to support their findings. 

    - This seems a particular issue when the authors explicitly suggest that the theoretical framework can help answer ""what can we do to ensure unbiased learning from biased data"" (line 228), and mentions two previous works [7] and [34], where [34] has a decent amount of empirical works. 

2. on the theoretical end, there are several questions need further clarifications, potentially paint a significant issue of rigor of the results. (see below questions)

Limitations:
no explicit discussions of limitations, but there are discussions about future directions, which might be interpreted as the limitation discussion. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper presents a framework for systematically analyzing and categorizing corruption models in machine learning. The authors propose a new taxonomy of corruption based on its dependence on the feature and label space, rather than relying on invariance assumptions. They utilize the concept of Markov kernels to provide an exhaustive framework that includes all possible pairwise stochastic corruptions. The authors also derive corruption-corrected loss functions within the framework.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- A novel unification of existing data corruption models: The proposed theoretical model based on the Markov kernel is capable of modeling various existing data corruption and distribution shifts. This offers a new perspective to understand how different data dynamics connect to each other.

- Insights on how to correct the errors: The authors also derive loss correction formulas for a few instances of the unified data corruption model. These results give an abstract way to study data corruption-aware learning.

Weaknesses:
- Unaccessible to a wider community: One major concern with this paper is whether it is accessible to the NeurIPS community. This paper seems to be a pure theory paper: it is symbol-heavy, theory-oriented, and without any empirical support. The theory community (e.g., COLT) might appreciate it more, and I am a bit worried about the NeurIPS community. See my questions in the next section for more details.

- Unclear concrete applications of the proposed theoretical models: It is unclear how useful the unification is. Do its insights give statistically or computationally more efficient algorithms for corruption-aware learning? Or does it implies one corruption instance subsume another and thus focusing on it is enough? Or does it reveal new corruption instances not studied before but important in practice? A good paper, in my opinion, should give useful advice to its readers.

Limitations:
Yes

Rating:
6

Confidence:
3

";0
c5Inzw6giM;"REVIEW 
Summary:
The paper employs a few heuristic methods to accelerate logistic regression training over encrypted data. The heuristics considered include: a new loss function called squared likelihood error (SLE) along with a polynomial approximation of sigmoid function, a faster gradient-descent method based on quadratic gradient, and a matrix encoding method called volley revolver.  

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The only strength of the paper is that it attempts to solve a really challenging problem of learning over encrypted data and it provides the code apriori through an anonymous GitHub link.

Weaknesses:
1) First and foremost, the title of the paper is misleading. The paper never deals with CNN training even in the limited context of transfer learning. It is true that most practical ML applications start with a pre-trained model and finetunes the parameters of this model. However, transfer learning implies that the whole model is finetuned apart from learning the application-specific last fully connected (FC) layer. What this paper attempts to do is just learn the last FC layer, which is nothing but multiclass logistic regression (MLR) training. Therefore, the title of the paper should not claim anything about CNN training.

2) Numerous attempts have been made over the last five years attempting to achieve MLR training on encrypted data, which have not been acknowledged in this paper and compared against. For example, see the works starting from:

[A] Crawford et al., ""Doing Real Work with FHE: The Case of Logistic Regression"", 2018
[B] Han et al., ""Logistic regression on homomorphic encrypted data at scale"", AAAI 2019
[C] Bergamaschi et al., ""Homomorphic Training of 30,000 Logistic Regression Models"", 2019

3) This current paper appears to be very similar to the rejected NeurIPS 2022 submission entitled ""Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant"". While the NeurIPS 2022 submission focused on only the quadratic gradient component, the current paper also introduces the SLE loss. However, it is not clear how this SLE loss function is better. Moreover, what is the expression for the gradient of $ln L_2$ and where is it used in Algorithm 1?

4) The so-called volley revolver does not constitute any novel ""matrix-encoding"" method. Such packing tricks are regularly used in the context of efficient SIMD operations in FHE.

5) Overall, none of the three claimed contributions (namely, quadratic gradient, SLE loss, and volley revolver) appear to be original or significant enough to make an overall impact.

6) Finally, though the paper claims that the goal is to make logistic regression training practical, not a single experimental result has been shown to prove this point. Running 2 iterations with 128 MNIST images takes approximately 21 minutes and the last line claims that real experiments would take ""weeks, if not months"". There are other reported works in the literature, which showed more realistic results.

[D] Nandakumar et al., ""Towards Deep Neural Network Training on Encrypted Data"", CVPR-W 2019
[E] Lou et al., ""Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data"", NeurIPS 2020

Limitations:
All the limitations have not been presented and addressed. There appears to be no potential negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper presents a method for CNN transfer learning implemented in homomorphic encryption to protect privacy.


Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
I'm not aware of the method being implemented in HE before.


Weaknesses:
I cannot judge the machine learning aspects, but I don't see a strong novelty on the cryptographic side. The paper claims that some prior work is overly complex without going into details.

I find it concerning that the work relies relatively heavily on non-peer reviewed references by a single author (5 out of 19).

Line 150 says ""well-studied by several works"" without giving any reference.


Minor issues:
- l6: ::
- l15: .;
- l50: pervacy-persevering (privacy-preserving?)
- l51: diffuclt
- l71: seveal
- l154: After many attempts (unscholarly language)


Limitations:
n/a

Rating:
3

Confidence:
2

REVIEW 
Summary:
In this paper, the authors proposed a CNN training technique on the homomorphic encryption domain based on transfer learning. A gradient variant called Quadratic Gradient on homomorphic encryption was proposed. And a sigmoid function-based Softmax approximation was proposed. In addition, a new loss function for squared likelihood error was proposed, and a matrix-encoding method called Volly Revolver was also proposed. Finally, they released the code they implemented.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Properly implementing functions for training on a homomorphic encryption domain is challenging. It is worth evaluating for implementing this and also disclosing their source code.

Weaknesses:
This paper performed a simulation on the MNIST dataset for performance evaluation. This seems too simple a dataset, even considering the homomorphic encryption environment. Although they claim that it is to be the first implementation of transfer learning-based CNN training on the homomorphic encryption domain, a similar study was recently published first. Of course, this paper takes a different approach.

[*] https://openreview.net/forum?id=jJXuL3hQvt

This paper is considered incomplete in several respects. The main reason is that the proposed scheme's threat model needs to be clarified. At first, ""the proposed architecture"" is not clear. There needs to be a description of the proposed architecture. They should explain the exact part where homomorphic encryption was actually carried out in the transfer learning process and what benefits can be gained from doing so.


Limitations:
Not exactly.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper combines several existing techniques to achieve privacy-preserving CNN training. These techniques include transfer learning,  Quadratic Gradient,  mathematical transformation, and matrix-encoding method Volley Revolver. 

This writing is more of a technical document rather than a research paper with insights.

Soundness:
4

Presentation:
1

Contribution:
3

Strengths:
1)For the first time, they apply homomorphic encryption to neural network training.
2)They demonstrate the feasibility of homomorphic CNN training.
3)They propose pervacy-perserving friendly Squared Likelihood Error (SLE) for CNN training.
4)Experimentally, their algorithm has a state-of-the-art performance in convergence speed.

Weaknesses:
1)The introduction of related works is pretty simple, which makes it difficult to evaluate the contributes of the paper.
2)The quality of writing/presentation is very weak and unreadable.

Limitations:
See Questions and weakness.

Rating:
5

Confidence:
3

";0
wv79UiY5U7;"REVIEW 
Summary:
This paper focuses on data curation for image captioning. This paper shows that mismatched image-caption pairs do harm to the captioning model. To address this problem, generative models are used. In detail, the BLIP model is used to generate captions based on images, and the Stable Diffusion model is used to create images based on captions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The data curation is an important and effective topic, which could benefit many tasks including visual synthesis, image captioning, language and visual representation, etc.
2. This paper discovers the weak point of captioning datasets, especially for the Flicker30K.
3. It is interesting to use the BLIP model and the Stable Diffusion model to create data for training.

Weaknesses:
1. There are many methods to augment text dates, e.g., adding or editing some words, using synonyms, and changing sentence structure. I think these methods are also worth evaluating.
2. From Table 2, we can see that the BLIP's performance is not significantly affected by the methods proposed in this paper (i.e., Remove, ReplaceCap, and ReplaceImg). For example, the CIDEr of COCO only slightly raises from 132.0 to 133.1.
3. Figure 5 shows that the proposed methods might make the performance worse, especially in the COCO dataset. So I am concerned about the generalization of the proposed methods.

Limitations:
None.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper studies data curation strategies for training image captioning models. Firstly, it identifies the “difficult samples” based on the captioning loss dynamically at the end of each epoch. Subsequently, it introduces three data curation strategies to modify the difficult samples: (1) removal of an image-text pair, (2) replacement of the caption and (3) replacement of the image using text-to-image generative models. The main technical innovation is the third strategy, which is carefully designed in terms of prompt engineering and fine-tuning on the image captioning datasets.

The empirical studies show that the proposed data curation strategies can enhance the performance of the baseline BLIP captioning model. The authors also conduct analysis on the data curation ratio, dynamic versus static curation strategy and the errors of images generated by the stable diffusion model.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The idea of employing text-to-image generative models to curate training data for image captioning is novel and well-motivated.

Weaknesses:
### Effectiveness of the proposal
* According to Table 2, the performance of the third data curation strategy, which is the main technical innovation of this work, is not advantageous compared to the heuristic removal and caption replacement strategies.
* According to Figure 5, all three proposed strategies are sensitive to data curation ratio. Consequently, training the captioning model multiple times is necessary to achieve satisfactory performance, which is less efficient compared to the baseline BLIP model.

### Design of the method
* Identifying the samples to modify based on training loss is questionable. A higher loss does not necessarily imply that the sample is harmful to training. Although Section 5.2 has shown that more errors are identified in images of higher loss, the experimental setup has two issues: (1) The loss is computed over the generated images rather than the real images in the original dataset. (2) The errors are categorized as targeting image generation, rather than image captioning. In other words, an image that possesses imperfect visual quality but aligns well with the caption may not necessarily be considered a noisy training sample for image captioning.

### Missing reference
* An idea similar to the “round-trip captioning evaluation” is already proposed by [1], which generates a caption from the synthesized image and measures the similarity between input text and predicted caption.

### Clarity
* In line 243, it is unclear whether the “model loss” refers to captioning loss or image generation loss.

[1] Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis.


Limitations:
The authors have discussed the limitations of this work from three aspects: (1) Lack of adaptation of the proposal to the pre-training stage. (2) Reliance on pre-trained image understanding and text-to-video generative models. (3) Increase in training time due to the usage of text-to-image generative model. Moreover, a significant limitation of this study is the absence of evidence demonstrating the superior effectiveness of the generated images compared to heuristic data curation strategies.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a data curation model for image captioning. If the loss of a particular image caption pair is high, then either remove the image-caption pair from the training set or replace the caption with a more similar caption or they generate a new image for the difficult caption. The authors demonstrate these strategies help to improve the performance of BLIP caption generation model.


Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The idea is interesting.

Paper shows some positive gains on COCO and FLickr30K.


Weaknesses:
The details of the method are not clear. How to select a replacement caption?

Why one should pick only the high-loss image-caption pairs? Loss may be high due to many other reasons.

Compared to other data augmentation methods in the literature that is also discussed in the related work, what is the novelty?

Why this is a significant finding? I am not sure if this is a significant finding. 

The method is also evaluated using a single model. 

Obtained results are not state-of-the-art.

It is not clear whether such a mechanism will contribute to any state-of-the-art methods in captioning.

Limitations:
Limitations and societal impact are discussed in the paper.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper focuses on improving image captioning by improving the quality of the existing dataset. To this end, this paper proposes three data curation methods: the removal of an image–caption sample; replacing a caption with another caption; and replacing images using a text-to-image generation model. Experimental results demonstrate that models trained with the proposed methods consistently outperform baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.The proposed method is well-motivated, that is to improve the quality of the existing dataset. This paper explores the problem of making better use of existing datasets, which is a very interesting research direction.

2.The authors conduct extensive experiments over these two datasets, where the models trained with the proposed methods outperform baseline methods consistently.

3.The paper is well-written and easy to follow.

Weaknesses:
1.From my understanding, it is risky to judge the quality of the sample based on the loss value. A sample with a large loss value may be a hard sample or a mislabeled sample, so it is risky to judge the sample quality only based on the loss value.

2.In addition to the performance of the model on the test set, the generalization ability of the model is also important. It is not clear whether the proposed method reduces the gap between the training set and the test set or improves the quality of the training set.

3.Lack of necessary theoretical analysis.

Limitations:
yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose an iterative training approach to improve image captioning models.
This approach _refreshes_ the training dataset every epoch with _higher quality_ image-text pairs (authors call it ""data curation"").
Dataset samples with very high training loss are updated -- the real image is replaced with one generated by the Stable Diffusion model.
The authors compare their approach with two baselines: one which removes high-loss samples, and one where the image is replaced by another from the training dataset itself.
Experiments are performed with the BLIP model and two captioning datasets -- COCO and Flickr30K.
Authors also perform an accompanying human study to provide directions for future work.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
This paper has numerous technical strengths:

- The proposed method is conceptually simple and easy to implement.
- The strategy of updating the training dataset with ""better"" samples is very general: it is agnostic to the model architecture and the multi-modal task at hand.
- The writing and presentation quality of the paper is excellent. It contains adequate implementation details to make this work reproducible.
- The experimental setup and ablation study is very meticulous. Tables of results contain experiments that begin with a BLIP baseline, and subsequent rows introduce one change at a time.
- The authors have conducted a human study with sensibly defined failure categories to understand how failure modes of Stable Diffusion can impact captioning performance.


Weaknesses:
Like its technical strengths, this paper also has some shortcomings.
Below I list a few salient concerns with the paper.
I look forward to hearing the authors' response, and I am happy to update my final assessment.

1. **Results do not match with the presented story:**
The main results (`Table 2`) indicate that all considered dataset curation approaches are beneficial over a BLIP baseline that doesn't train on curated data.
However, the main pitch of this paper is to use generative models like Stable Diffusion to replace images (last row),
which in fact performs marginally better or even worse than other curation techniques.
The biggest improvements are generally yielded by ""Remove"" strategy.
I recommend the authors rethink the positioning of motivation and frame it as an exploratory study --
it seems obvious to use generative models for iterative training/distillation and some works already do it for other applications,
but for this task, a practitioner is better off by simply filtering noisy samples altogether.

2. **Captioning metrics appear saturated, maybe overkill for COCO/Flickr:**
The captioning metrics on COCO and Flickr are already saturated,
e.g. decimal improvements are less meaningful for COCO in the range of 130+ CIDEr and 20+ SPICE score.
Since BLIP is already rained with large amounts of data and diverse tasks,
the proposed approach may be an overkill for the tasks considered in this paper.
I suggest the authors rethink other applications where the benefits of this strategy are more prominently observed (see Weakness 5 below).

3. **What if the caption is noisy and can't generate meaningful images?**
An image-text pair may be unaligned if the caption is uninformative,
as frequently encountered in larger web datasets like
[Conceptual Captions](https://arxiv.org/abs/2102.08981),
[YFCC](https://arxiv.org/abs/1503.01817),
[RedCaps](https://arxiv.org/abs/2111.11431), etc.
For instance, captions coming from alt-text may not have any semantic content whatsoever
(e.g. see Figure 2 in [ALIGN paper](https://arxiv.org/abs/2102.05918)) to generate meaningful images.
The proposed approach forces the generative model to create an arbitrary image and ends up adding noise to the training data.
Some selective mechanisms to replace either image or caption may be needed to scale this approach to general image captioning beyond COCO and Flickr30K.

4. **Related work needs more coverage:**
The main focus of this paper is image captioning, hence a broad coverage of prior works on image captioning is necessary.
However, this section only cites a handful of very recent modeling papers.
I suggest the authors begin the discussion with some early image captioning papers like:

  - (Vinyals et al, CVPR 2015) Show and tell: A neural image caption generator
  - (Karpathy and Li, CVPR 2015) Deep visual-semantic alignments for generating image descriptions
  - (Donahue et al, CVPR 2015) Long-term recurrent convolutional networks for visual recognition and description

5. **[Related to 1, 2] Have the authors considered applications others than image captioning?**
What if this curation strategy is used to train general visual representations?
I suggest a CLIP-style contrastive model and/or BLIP/VirTex-style generative model.
The contribution can be strengthened by broadening the scope to various downstream tasks.


Limitations:
The limitations section should be updated if any of the above-mentioned open questions are not within the scope of this paper.

Rating:
3

Confidence:
5

";0
hoyL1Ypjoo;"REVIEW 
Summary:
This paper proposes a new black-box optimization framework, called WireMask-BBO, for macro placement, which is an important problem in the electronic design automation (EDA) community. By using different black-box optimization algorithms, The experiments show it can achieve improvements (shorter half-perimeter wirelength (HPWL)) over previous methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The general framework WireMask-BBO proposed by this paper somewhat provides new angle for macro placement problem. The paper is well-written and it is easy to follow.

Weaknesses:
The scalability of the proposed method is doubtful because Bayesian optimization and evolutionary algorithm may not be suitable for large-scale problems.
The experiments are insufficient. The experiments don’t compare with the state-of-the-art macro placement method. The experiments don’t compare the runtime of the proposed method with other methods.

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a framework using BBO for macro placement in VLSI designs. Any placement solution for a set of macros can be optimized using the wire masks (presented in a prior work using RL) where the optimization goal is to minimize the HPWL of the output. In addition to random inputs, the framework can also be used for further enhancement of any existing solutions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The work tackles a critical problem in VLSI designs. 

The idea of casting the placement problem to a BBO is interesting and novel.

This framework can be used to further improve any existing solutions for the macros. This can be used as another step in PnR with reasonable runtime.

Overall, the paper explains the problem, the existing solutions, and the proposed work clearly.

Weaknesses:
The idea of casting the placement problem to a BBO is interesting and novel. However, as the authors admit in Section 1 that this work does not develop any new BBO algorithm. To the reviewer, the experimental results are not extensive and convincing; this is described in the limitation section. 

The paper also states in the conclusion that it can only place macros but not standard cells without explanation, which limits the application of the proposed work to the actual VLSI problem. On the other hand, this framework can be used to further improve any existing solutions for the macros. This can be used as another step in PnR with reasonable runtime.

Limitations:
The computational time for the proposed work seems to be high. Therefore, in the experimental section, the paper does not include the complex benchmark that has thousands of macros to be placed. This contradicts with the claim the paper that this approach is scalable.

The evaluation of the proposed method only uses 5 random seeds. It would be more convincing if the paper includes more experimental data.

The proposed framework cannot place the standard cells in the design. Therefore, the comparison between this work and the existing methods seems to be unfair.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a new placement method that is based on the black-box framework. The framework leverages the wire mask-guided information and can achieve significant placement results compared with the state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The novel method is based on black-box optimization, which has not been implemented on the placement task before. Although many RL methods have been proposed, they are not very efficient enough. Black-box optimization might be a viable direction. 

2. The wire mask as the guide for generating phenotype representation is also very novel. It can quickly render a suitable solution based on the initial representation, improving efficiency. 

3. The experiments are very comprehensive and solid, showing that the performance of the proposed method can consistently outperform existing methods.

4. The paper writing is well-written and easy to understand.

5. The code is open-source, improving reproducibility.


Weaknesses:
1. The full placement cannot surpass the DREAMPlace, which means the proposed method can only work well in macro placement.

2. The reasons for the improvement in the congestion metric are not clear. The method does not consider the congestion metric in its search process.

3. The recent work [1] based on Maskplace should also be discussed in the related work part.

[1] Lai Y, Liu J, Tang Z, et al. Chipformer: Transferable chip placement via offline decision transformer.


Limitations:
Yes.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel black-box optimization (BBO) framework, namely WireMask-BBO, for macro placement in chip design. Specifically, it devises a post-processing technique that legalizes any searched placement solution while optimizing the half-perimeter wirelength (HPWL). The post-processing technique allows us to perform BBO algorithms to search for solutions with better HPWL. Experiments demonstrate that WireMask-BBO outperforms previous state-of-the-art (SOTA) methods, achieving better HPWL performance in less time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	This paper explores BBO methods for macro placement, which may provide a new insight for the research community.
2.	The proposed post-processing technique is simple yet effective. It also has a good versatility because it can be combined with other placement methods and BBO methods.

Weaknesses:
1.	The propose of the post-processing is purely heuristic. The motivation is unclear and intuitive explanation for the advances is insufficient.
2.	The proposed method only targets on optimizing HPWL, without explicitly considering other important metrics like routing wirelength or congestion. It does not consider cells and routing either. Moreover, the framework can be hardly transferred to tasks with those metrics under consideration, which limits its real application in EDA.
3.	Because introducing BBO-based methods is one of the core contributions of this paper, the authors may want to illustrate the implementation of BBO algorithms for macro placement more detailly.
4.	In Algorithm 1, the macros are ordered decreasingly according to areas, while in Figure 3, the smaller macro-A is considered first.
5.	A recent related work [1] is missing.
[1] Lai Y, Liu J, Tang Z, et al. Chipformer: Transferable chip placement via offline decision transformer. ICML 2023.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
77Nq1KjmLl;"REVIEW 
Summary:
In this paper, the authors proposed contrastively and predictively strategies for pretraining GNNs based on graph fragmentation. Using principle subgraph extraction, the authors pretrain two separate encoders for molecular and fragment graphs, capturing structural information at different resolutions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-organized and easy to follow.
2. The authors conduct comprehensive comparisons with baselines to show their advantages.
3. The fragment-level contrastive pretraining framework is novel, which captures both granular patterns and higher-order connectivity.
4. The t-SNE visualization in Figure 3 clearly shows the effectiveness of the authors' design.

Weaknesses:
1. The technical contribution is limited. For example, the principle subgraph extraction module is borrowed from [19].
2. The authors do not clearly state how to choose hyperparameter alpha.
3. In Table 3, the effects of the vocabulary size are only explored on three datasets.
4. The pertaining time is not reported in the paper.

Limitations:
The limitation is not clearly discussed. Please provide the discussion in the rebuttal period. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper presents a novel approach to pretrain Graph Neural Networks (GNNs) at the fragment level for property prediction on molecular graphs. By utilizing a compact vocabulary of prevalent fragments and introducing fragment-based contrastive and predictive pretraining tasks, the authors overcome the limitations of node-level and graph-level pretraining. Two different GNNs are pretrained to capture structural information at multiple resolutions, and the fragment information is utilized during finetuning. The proposed models show improved performances on both common molecular and long-range biological benchmarks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is easy to follow.
- The idea that using motif for pretraining is novel and reasonable.



Weaknesses:
- Empirical performance is not strong enough. Especially in Table 2, it's hard to distinguish the absolute gain over the baselines. The authors are encouraged to report the average score over all tasks in molecular property prediction.
- The authors are also encouraged to compare with stronger baselines. For example, the authors can also compare JOAO V2 in addition to JOAO.
- Missing ablations: the authors add many components and loss functions in the system. It would be interesting know how each contribute to the performance.

Limitations:
Yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
Based on the belief that learning with fragments can help capture structural information at multiple resolutions, this paper proposes a fragment-based strategy for pretraining and fine-tuning. 

First, the paper extracts fragments by an existing heuristic algorithm called Principle Subgraph Mining algorithm to obtain fragments from a large molecular dataset (i.e.,  ChEMBL database). Then, the paper uses two separate GNNs (one for molecules and one for fragments) to learn the node embeddings. In particular, the node embeddings obtained by molecular-based GNN are pooled into fragment embeddings. 
The model is trained with respect to three tasks: a contrastive task between fragment embeddings obtained by molecular-based GNN and  fragment-based GNN, a fragment existence prediction task, and a fragment graph structure prediction task. 

The experiments are done on 8 binary graph classification tasks from MoleculeNet and 2 graph prediction tasks on large peptide molecules from the Long-range Graph Benchmark.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ The proposed method is easy to follow and conduct. 
+ The results on Long-range Graph Benchmark are particularly good. 
+ Figure 1 clearly shows the method.
+ Codes are provided. Appendix further provides more details.

Weaknesses:
The idea of using molecular fragment can be interesting. The proposed method shows some effectiveness, although how it obtains can be less illuminating. 

Many design choices need to be further described. Please reply to my questions below.

In addition, some writing issues exist. Sentences cannot start with ""[reference]"".

Limitations:
No potential negative societal impact of their work as far as I know.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose a novel method for generating representations for molecule graphs where two GNNs are contrastively learned. Using this new represntations, the authors achieve good results compared to a variety of baseline methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper and method are presented clearly.

The results are strong and the method is interesting + well explained 

Weaknesses:
I found the presentation of Figure 3 a bit confusing

Limitations:
N/A

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper proposes a contrastively and predictively strategy for pretraining GNNs based on graph fragmentation. Specifically, it leverages a frequency-based method for extracting molecule fragments, and performs fragment-based contrastive and predictive tasks to jointly pretrain two GNNs on a molecule graph and a fragment graph. It also enforces the consistency between fragment embeddings and atom embeddings for multi-solution structural information.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	The paper is easy to follow.
2.	The paper investigates an interesting fragmentation strategy for pretraining tasks.
3.	The proposed method enforces the consistency between fragment embeddings and atom embeddings for multi-solution structural information, which is a promising trick. Experiments demonstrate the effectiveness of this method.

Weaknesses:
1.	The technical novelty is limited, because it is a combination of existing methods. While the performance improvement is not very remarkable.
2.	The authors may want to conduct ablation studies on the effect of molecule fragmentation strategy and the pretraining strategy, respectively.
3.	Table 3 shows that the performances worsen on some downstream benchmarks as the vocabulary size grows larger. The authors may want to investigate a smaller vocabulary size. When it reaches 1, the method is the same as without fragments.
4.	A related work [1]---which leverages a similar frequency-based motif extractor and uses contrastive learning for generative training---is missing.

[1] Zijie Geng Z, Shufang Xie, Yingce Xia, et al. De Novo Molecular Generation via Connection-aware Motif Mining. ICLR 2023.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
ddKCg3OhGw;"REVIEW 
Summary:
The paper investigates the functional equivalence class for reducible neural network parameters and its connectivity properties. The authors focus on single-hidden-layer hyperbolic tangent networks, but the findings can be generalized to other feed-forward network components.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides a comprehensive understanding of the functional equivalence class for reducible neural network parameters. This can be key to understanding the structure of the parameter space and the loss landscape on which deep learning takes place.

The authors describe a complex union of manifolds, displaying rich qualitative structure. This includes a central discrete array of reduced-form parameters, connected by a network of piecewise linear paths, and various manifolds branching away from this central network.

The paper establishes that with a majority of blank units, the diameter of this parameter network becomes a small constant number of linear segments. This can be beneficial in understanding the trade-offs between shortest path length and rank for different unit permutations.

The paper also discusses the relevance of their findings to modern architectures and deep learning, suggesting that understanding reducible functional equivalence classes may be key to understanding these topics.


Weaknesses:
The paper is highly theoretical and may not provide immediate practical applications for those working with neural networks. The exact relevance of reducible parameters to these topics remains to be clarified.


The paper focuses on single-hidden-layer hyperbolic tangent networks, which may limit the applicability of the findings to more complex architectures.

Limitations:
NA

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper introduces an algorithm to calculate the canonical equivalent parameter for a tanh feed-forward network.
Further, the full equivalent class of each parameter is given by a union of subsets constructed out of (inverse-)reductions and permutations of the canonical parameter.
Finally, the paper shows that each functional equivalence class is path-connected and that the diameter of a functional class with rank at most the half of the number of units in the hidden layer is 7.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The canonicalisation algorithm is novel for feed-forward networks with a tanh transfer function. Furthermore, this is the first result that characterizes the diameter of some of the equivalence classes of these networks.
The paper fully proves each theorem and includes useful visualizations for the path structure of a functional equivalence class.
The paper is overall very clearly written, with only a few possible minor typos.
The results presented here could be of importance to loss minimization problems.

Weaknesses:
The scope of applications of the algorithm and the theorems seems to be rather narrow or at least is not properly motivated to be broad.

The paper has very limited evaluation of the proposed algorithm and further uses of the results.

It is for example unclear in which practical application Algorithm 4.1 could be used, because of the real valued parameters being usually all different in a trained network, see Questions.



Limitations:
They are adequately discussed.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper focuses on the study of functional equivalence classes in neural networks, specifically in fully-connected, single-layer networks with hyperbolic tangent activation. It  presents a Canonicalisation algorithm for reducible networks to determine canonical representative parameters for different functional equivalence classes. Finally, it explores the concept of connectivity between these classes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper gives a well-motivation exploration of functional equivalence classes in neural networks. It provides a well-written definition of these classes, introducing an analysis of connectivity between them. The article references foundational works on simple neural networks and incorporates recent literature in the field of deep learning, enhancing the credibility and relevance of the study.

Weaknesses:
The weaknesses of this article stem from the focus on a very theoretical network type with implausible parameters. While a useful theoretical case, further discussion on the applicability of this analysis to modern architectures would benefit the article.

Specifically, a drawback of the article is its narrow focus on a specific architecture type, namely single-layer networks with tanh activation, a single input and single output. While it does discuss expanding the input and output space, it fails to address how the findings extend to more complex multi-layer networks or different activation functions commonly used in modern architectures. By limiting the scope of investigation, the article has limited application to contemporary neural network design and learning.

The proposal of the Canonicalisation algorithm also presents limitations. The algorithm relies on exact equivalence between weights or to 0, which is rarely achievable in practice, especially for weights trained through backpropagation. This raises questions about the adaptability of the Canonicalisation algorithm to networks trained through backpropagation, and whether it can provide meaningful results. The article would greatly benefit from addressing these concerns and exploring potential adaptations for networks trained through backpropagation.

It appears to me that the Canonicalisation algorithm is a unit pruning / neuron removal algorithm. While Kuditipudi et al. is cited, the article would benefit from considering and discussing other neuron removal methods, specifically the following which focus on individual neurons in fully-connected layers:

+ Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In Proceedings of the 37th International Conference on Machine Learning, pp. 10820–10830. PMLR, 2020.
+ Xiaocong Du, Zheng Li, Yufei Ma, and Yu Cao. Efficient network construction through structural plasticity. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 9(3):453–464, 2019.
+ Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks. In Advances in Neural Information Processing Systems, volume 33, 2020.

Additionally, GradMax is a method which adds new neurons, similarly to Fukumizu and Amari, which was cited. The new neurons do not impact the function of the network upon addition by setting fan-in weights to 0. This case doesn't seem covered by remarks 5.3-5.5.
Evci, Utku, et al. ""GradMax: Growing Neural Networks using Gradient Information."" International Conference on Learning Representations. 2021.

Limitations:
The paper does not adequately address the limitations induced by the narrow focus on a highly theoretical architecture. The sections on ""towards modern architectures"" and ""functional equivalence and deep learning"" offer insight into the potential application of the analysis to contemporary deep neural networks, but are not comprehensive.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper deals with functional equivalence problems in neural networks, i.e., the characterization
of all neural networks that lead to the same given output function. This is a problem that has been
studied since the early 1990s, including work by the fields medalist Charles Fefferman. 
The present paper considers single-layer networks with tanh-nonlinearity and puts forward
a completely new perspective by paying attention to reducible parameters. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper tackles a problem that has been studied in various guises for over 3 decades, and
puts forward a completely new vantage point, by considering reducible parameters. This leads
to rich insights into the functional equivalence problem. In addition, the paper exhibits a strong algorithmic component,
specifically by providing an algorithmic characterization of redundancies and connecting the underlying
theory to the beautiful concept of piecewise-linear path connected sets.


Weaknesses:
could not find any

Limitations:
none

Rating:
10

Confidence:
5

REVIEW 
Summary:
The paper addresses fully connected feed-forward neural networks (NNs) with a single hidden layer and the hyperbolic tangent activation function. A parameter is thus a vector of weights and biases. It considers reducible parameters, which means that a NN with strictly less neurons can implement the same function. The paper provides a ""canonicalise"" procedure taking a parameter as input and yielding a canonical parameter that implements the same function. Also, if two parameters implement the same function then the output of the procedure is the same for them.

Based on this procedure, the paper characterizes the set of parameters that implement the same function as a given parameter. It shows that this set is piecewise linear path-connected, and if the set is defined with respect to a ``sparse'' parameter, then the number of linear pieces of the diameter of the set is bounded by 7.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written. The figures are very clear and help understanding.

The theoretical topic is well motivated and well connected to the literature. 

The theoretical results are interesting, and so are the proofs in my opinion.

Weaknesses:
The main weakness is that the paper is restricted to a single hidden layer. 
Also, the hyperbolic tangent activation function is studied, while ReLU would have been more relevant, in my opinion.

For these two weaknesses, it seems they cannot really be fixed in the frame of this paper, as considering multiple hidden layers and/or ReLU would probably change the nature of the results and the proofs.

Limitations:
The authors adequately discuss the main limitation (a single hidden layer) in the discussion.
I do not see potential negative societal impact.

Rating:
5

Confidence:
3

";1
805CW5w2CY;"REVIEW 
Summary:
This paper introduces Trajectory-Aware Imitation Learning from Observations (TAILO) for offline imitation learning. TAILO tackles the problem of learning from incomplete trajectories, where other state-of-the-art (SOTA) methods fail. Specifically, TAILO proposes a simple yet effective solution. It first learns to recover the task reward function by distinguishing the expert demonstrations and the sub-optimal data. In addition, TAILO treats the non-expert data as an unlabeled dataset, and relabelles the reward of sub-optimal transitions to provide more training data for the policy. Next, TAILO performs policy learning by weighted behavior cloning with accumulative return along a successful trajectory segment. In the experiments, TAILO successfully outperforms a series of baselines on offline imitation learning with incomplete trajectories.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper studies an interesting problem with potential real-world applications.
- The empirical performance looks promising.


Weaknesses:
There are several main weaknesses of the paper.

Firstly, the presentation of the paper still has room for improvement. Specifically, I would suggest the authors double check the literature and be careful when making statements. There are several important but unfortunately erroneous statements in the paper. For example, the very first sentence in the abstract:

> Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available.

This statement is inaccurate as by its definition, offline imitation learning discusses the case where a policy is learned with only off-policy data, excluding the on-policy environment interactions. Consider the standard behavior cloning, for example. The use of non-expert transitions is just one of the solutions to assist policy learning, as in ValueDICE [1], DemoDICE [2], and etc.
Also, in line 113, the authors state that there are three main steps for DICE methods:

> DICE methods consist of three parts: reward generation, optimization of the 114 value function V(s), and weighted behavior cloning.

This is inaccurate as well. For example, ValueDICE [1] directly learns the value function, rather than the rewards.
Besides the presentation issues, the contribution of the paper is relatively minor. The use of cumulative return / advantage function is standard in reinforcement learning literature. Mathematically, only by maximizing the cumulative return would give a correct policy improvement step. If we consider the specific form of weighted behavior cloning, or more precisely, advantage weighted regression, we can still find many existing works with similar structure [3, 4, 5]. 

There are some potential issues with the mathematical correctness and experiment details, which I will reserve for the questions.

References:

[1] Kostrikov, I., Nachum, O., & Tompson, J. (2020). Imitation learning via off-policy distribution matching. In International Conference on Learning Representations.

[2] Kim, G. H., Seo, S., Lee, J., Jeon, W., Hwang, H., Yang, H., & Kim, K. E. (2022, October). Demodice: Offline imitation learning with supplementary imperfect demonstrations. In International Conference on Learning Representations.

[3] Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Riedmiller, M. (2018). Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920.

[4] Peng, X. B., Kumar, A., Zhang, G., & Levine, S. (2019). Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177.

[5] Wang, Z., Novikov, A., Zolna, K., Merel, J. S., Springenberg, J. T., Reed, S. E., ... & de Freitas, N. (2020). Critic regularized regression. Advances in Neural Information Processing Systems, 33, 7768-7778.


Limitations:
As discussed above, this paper has certain limitations regarding its presentation, novelty, the mathematical correctness, and its novelty. I do appreciate the authors’ efforts on the extensive experiments and detailed appendix. However, I would think the paper is not ready for publication yet and major revision is needed for it to be published in another venue.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper provides an offline imitation learning method upon a specific problem setting where task-specific expert states (observations) are restrictively available and task-agnostic non-expert state-actions are supplementarily available. 

In this problem setting, the authors follow the line of works using DICE-based imitation learning methods, and present TAILO, Trajectory-Aware Imitation Learning from Observations. Specifically, TAILO employs the 2-step positive-unlabeled learning scheme to have the state discriminator, and then it uses rewards calculated by the discriminator as the weight for weighted behavior cloning. 

This procedure in TAILO hinges on the assumption that in the pool of task-agnostic data, there exist trajectories or long segments that are closely aligned to the optimal expert trajectories for the specific target task. 

The authors focus on different use cases of offline imitation learning on incomplete trajectories and show the benefits of TAILO in such context for those cases through experiments. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper contributes to the area of offline imitation learning, particularly in scenarios where offline data conditions exhibit variability due to incomplete trajectories. The robustness of the proposed method, Trajectory-Aware Imitation Learning from Observations (TAILO), is thoroughly demonstrated in different data conditions through the experiments detailed in Section 4.

These experimental conditions encompass scenarios with incomplete expert trajectories, incomplete task-agnostic trajectories, observations, and examples. The tests conducted appear to be comprehensive and the obtained results align well with the authors' focus, thereby solidifying the consistency and potential effectiveness of TAILO.


Weaknesses:
TAILO, as outlined in the paper, comprises two primary techniques, each of which is described in Section 3.2 and 3.3. One shortcoming of this work, however, is the absence of ablation studies that evaluate these proposed techniques separately.

To clarify the effectiveness of the Positive-Unlabeled (PU) learning method, alternative PU algorithms could be implemented and compared with the proposed technique. It would also be instructive to remove some losses as detailed in Equations (4) and (5) and assess the performance impact.

In a similar vein, various weighting strategies could be trialed in the context of behavior cloning, including cases where no weight is applied at all. Carrying out these ablation studies could provide a clearer understanding of the individual effectiveness and contributions of each method within the TAILO framework.
 
Minor errors:
- The legend of Figure 5 could be relocated without overlap.


Limitations:
The limitation is described in the conclusion section. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies offline imitation from observations, assuming a small amount of task-specific expert states and task-agnostic non-expert state-action pairs are available. The method is to learn a discriminator to identify expert states in the task-agnostic dataset and then apply weighted behavior cloning to imitate states. Empirical results show that the proposed method TAILO outperforms state-of-the-art methods (the family of DICE), particular for datasets with incomplete trajectories.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The idea behind this approach is intuitive, well-motivated, and conceptually simple.
2) Empirically, TAILO performs well in learning from datasets with incomplete trajectories, significantly better than state-of-the-art approaches.
3) The empirical evaluation is extensive. Considering different ways and hyper-parameters to modify datasets, the authors show good performance across all these datasets. It is impressive that the same set of hyper-parameters for TAILO performs well in all these experiments.


Weaknesses:
1) Compared with DICE methods (and the previous state-of-the-art), the proposed method lacks a theoretical foundation. The objective functions in equations 4,5,6 look mathematically complex, but we do not have a theory to support them.
2) The ablation study is missing. Thus, the importance of each component in TAILO is unclear. See the section of Questions for more details.
3) In experiments, the datasets are mostly manipulated, different from the original dataset used in previous work. This seems not standard for benchmarking and comparing approaches.

Limitations:
This work is based on the assumption that there exist expert trajectories/segments for the task of interest in the task-agnostic dataset. I'm concerned about the reliability or generalizability of this assumption. In real-world problems, when collecting demonstration dataset, there seems a small chance that this assumption holds.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose TAILO, Trajectory-Aware Imitation Learning from Observations, a method to solve MDPs from offline data in the form of task-specific expert states and task-agnostic state-action trajectories. The method addresses the instabilities of algorithms like DICE, takes the context of a trajectory into account even when it may be incomplete, and demonstrate across many Mujoco environments that TAILO outperforms baselines especially with incomplete trajectories. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
+ [Clarity] The paper is well-written, organized, and easy to follow. 
+ [Clarity] The authors include relevant background about DICE and document baselines clearly.
+ [Originality] The authors propose a relatively simple solution, which is novel to the best of my knowledge.
+ [Quality] Experiments on several Mujoco tasks demonstrate compelling performance. The authors additionally report results on the Franka kitchen environment.

Weaknesses:
-

Limitations:
-

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a simple weighted BC algorithm for offline RL with missing data. The authors first study that DICE family of algorithms suffer from missing data due to inaccurate value estimation or sparsity of observations which causes undesired monotonicity. The authors propose training a reward model from PU data and use MC value estimation with BC to train a policy. Experimental results on offline RL benchmarks show that this method is more robust to missing data and outperforms previous DICE family of methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper discusses limitations of DICE family of models in a practical way. The proposed method is also practical in a real setting. Results show robustness and overall improvement in missing/noisy data regime.

Weaknesses:
- The paper studies DICE family of algorithms with noisy data but the proposed method is mainly independent of these findings. It is a new objective without any value function or duality, which feels like it is not focusing on circumventing issues with DICE.

- Model based methods are missing from the comparison.

-  The proposed method is similar to [1] in which the authors use exponential of advantage with BC for batch RL setting. While they don’t study missing data setting, I believe the objective is more general and can be applied to yours as well. A detailed comparison would be helpful.

- Which one is more crucial: learning a better reward using PU data or using discounted future rewards for the value estimate? What is the performance if you used one-level PU learning without safe examples or two-level PU learning with 1-step reward?

- Eq (6) doesn’t handle missing data explicitly. What happens if some $i+k,k>0$ is missing from the trajectory? Do you just ignore that from the summation?

- I would have expected that with more noise, DICE methods perform worse. But in Figure-2, SMODICE-KL performs well Walker2d_1/3 and worse with Halfcheetah_1/5 while for others it is the opposite. Could you please explain why?





[1] Exponentially Weighted Imitation Learning for Batched Historical Data. Qing Wang, Jiechao Xiong, Lei Han, peng sun, Han Liu, Tong Zhang.

Limitations:
The authors study some limitations but a discussion on whether progress on D4RL reflects progress on real life would be helpful.

Rating:
5

Confidence:
4

";1
FTh5Rd3urw;"REVIEW 
Summary:
This paper proposes and analyzes a mechanism for incentivizing data contributions to federated learning systems in a context where contributors consider both monetary incentives for data contributions and the ability to experience the benefits of personalized systems.

The paper provides background on incentivized federated learning, describing the problem setting and proposed mechanisms, and theoretically analyzes a particular subset of this problem category to understand impacts of how underlying data that’s being modeled is distributed. Then, the authors perform experiments with classification datasets, with the assumption that classification datasets that might be under the control of a single entity in a classical setting are distributed between 100 clients and all data points belong to one of two clusters.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper has a number of strengths:

The paper begins with a very helpful overview of work in federated learning. The paper will be of interest beyond the federated learning-focused sub-community. The current draft provides strong motivation by explaining the large gap in incentivized federated learning. It seems with further development this approach could be impactful in practice.

The paper combines theoretical analysis of a new mechanism (which has the key insight of accounting for both monetary incentives and personalization-as-an-incentive) with learning experiments. This combination can help convince readers the proposed approach is useful. While there is some room to provide more details about the intended ""data contributor scenario"" and ""data user scenario"" (see below), overall I think the methods lean more towards being an overall strength vs. a weakness.

Presentation of results was strong overall.


Weaknesses:

While readers will appreciate the combination of theoretical analysis and experiment, it was not entirely clear exactly how the specific research questions being answered in each section were chosen (i.e., the “intuitions” R1-R3 seem useful but were not well prefaced). The experiments could be better motivated (more below in ""Questions"").

The paper is pretty abstracted away from any specific platform or use case where data contributors and data users interface via a particular FL architecture.

There may be an opportunity to strengthen the paper by specifying which ML use cases / contexts map well to the needed assumptions. In particular, while the paper relies on the existence of prior work on FL to motivate the need for FL, it’s not clear how many platforms running actual incentive systems (e.g. data markets, markets for crowdwork, etc.) are also supporting FL.

In general, I worry this paper may undersell its contribution by not providing enough details about what an on-the-ground implementation of the system would look like. Of course, this same criticism can be applied to a large body of similar work (including the RW here) and so it's not existential. Given the specific intended contribution here of proposing a new way to think about incentives in PFL, emphasizing plausibility may be especially valuable.


Limitations:
There are opportunities for the paper to be a bit more upfront about the limitations of focusing on incentive systems without discussing any specific populations of data contributors or specific use cases (beyond computer vision evaluation). While this paper seems to be part of a longer conversation between research papers that is primarily mathematical and leans heavily on assumptions about rational agents with programmatic approaches to computing utility, given that a key goal of having practical advantages mentioned in Related Work, addressing some practical use cases could make the work more convincing. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposed PI-FL, an incentive-aware federated learning method that produces clustered personalization of client models. This work uniquely considers the client contribution assessment in the multi-objective setting of clustered personalization. The clients are also given the freedom to choose the cluster based on their incentives. Therefore, this paper could be valuable in exploring the topic of incentivization in the very specialized area of clustered personalization. Rather extensive empirical experiments were also carried out by the authors to demonstrate the practicability and effectiveness of PI-FL.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. Recognition that personalization and incentivization in FL should be considered as interrelated challenges. In fact, incentivization is easier to be achieved when client models are allowed to be personalized.
2. In addition to the proposed method, the paper provides good theoretical analyses of it through toy examples that are easy to understand. The analysis also offers good and interpretable insights.
3. The types of empirical experiments carried out are rather extensive and demonstrate the effectiveness of the method. However, the comparison to baselines and the dataset choices can be further improved to be more comprehensive (sometimes omitting certain settings for certain methods).

Weaknesses:
1. This paper might need more concise and precise writing in Section 3 Proposed Methodology. The description of the methodology is long-winded and many design choices are not backed by theoretical justifications.
2. The theoretical analysis focuses on the gain from collaboration but neglects the theoretical aspects of the mechanism that incentivizes participation and honest data contribution.

Limitations:
1. It might be necessary to discuss the incentive and fairness more clearly. For example, we can see from Figure 2 that clients have varying PMA, which indicates different levels of benefits were gained by different clients in the collaboration. How should this phenomenon be viewed under the lens of fairness? Also, the opt-outs in a very coarse binary indicator and might not be a good way to see whether clients are really incentivized.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a novel clustering-based pFL combined with a token-based incentive mechanism to address incentive provision in pFL. Specifically, the proposed method clusters clients based on their cluster preference (the distribution similarity), which maximizes their contribution to the clusters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed approach of incentive-driven clustering based pFL is novel.

2. Theoretical analysis of the proposed method is provided.

3. Experimental results on multiple datasets demonstrate promising outcomes.

Weaknesses:

1. Some notations are not explained:


	What's $\zeta^{*}_k$ in Eqn.(4)? Only $\zeta_k$ is explained in line 177 of page 5

	What's $\theta_i$ in line 211 of page 6? Is it the same as the $\theta$ in Eqn.(6)?


2. Some parts of the method is not explained well


	What's the difference between reimbursement and payment? 

	What does the term of ""token"" actually means in pFL?

	What's the bidding strategy of clients?

3. Experiment:
	
	In pFL, the data are often heterogeneous in label shifting or feature shifting. However, the datasets used in the paper seems only related to dataset size imbalance, which are not the representative datasets used in pFL research. 

	Hope to see the experimental results on CIFAR10 data with Dirichlet splitting (referring the settings in [1]).

	[1] Marfoq, Othmane, et al. ""Federated multi-task learning under a mixture of distributions."" Advances in Neural Information Processing Systems 34 (2021): 15434-15447.

Minor:

	Typos: In figure 2, the x-axis is the number of clients which supposed to be a positive int type, but in the figure, it ranges from 0 to 1. 

Limitations:
NA

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper presents PI-FL, a new cluster-based personalized Federation Learning (pFL) approach. The new idea is the first to propose to consider motivation and personalization as interrelated challenges and address them through incentive mechanisms that promote personalized learning. PI-FL let clients provide incentive-driven preferences for joining clusters based on their own data distribution, and this client-centric clustering approach ensures accurate clustering and improved performance. This approach results in improved personalized model appeal (PMA) and reduced opt-outs, which in turn improves the accuracy of the clustering model. Theoretical analysis initially shows the effectiveness of the motivating algorithm. Experiments on several datasets show that the algorithm can obtain higher accuracy than the baseline.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper proposes to consider motivation and personalization as interrelated challenges and to address them through incentives that promote personalized learning. This is a good and reasonably innovative point that is informative for subsequent research on personalized federal learning.
2. The ability of PI-FL, a customer-centric clustering approach, to access the original customer data ensures accurate clustering and improved performance even in the case of dynamic distribution shifts in the customer's local data or incorrect clustering decisions by the customer, which improves the robustness of federation learning.
3. This paper is basically written very clearly. The three main modules of PI-FL: the profiler, the token manager, and the scheduler are very cleverly designed and their functions and working principles are well described.
4. The theoretical analysis is carried out in the paper to prove the effectiveness of the incentive algorithm, and the theoretical part is relatively complete.
5. The experimental comparison of test accuracy in multiple partitions shows that PI-FL can maintain good performance in all partitions. The comparison experiments of the two metrics, PMA and opt-outs, show that PI-FL can not only reduce opt-outs but also improve PMA in all data heterogeneity conditions.

Weaknesses:
1. The paper seems to lack the analysis about the convergence of PI-FL and the comparison with other pFL algorithms about the convergence speed.

~~2. In order to show the effectiveness of PI-FL method, the comparison between PI-FL and other clustering-based pFL algorithms about computation and communication cost should be added, and there should be more comparisons between several clustering-based pFL algorithms instead of only comparing with FedSoft.~~

3. All experiments are conducted in a simple CNN model, and the empirical study would be stronger if the authors could add some linguistic tasks, such as BERT pre-training/ fine-tuning experiments
4. Figure 1, as the overall design of PI-FL, is not drawn in enough detail, e.g., the icons representing customers are not labeled as customers. the fold lines in Figure 4 are dense and affect the perception, e.g., it is difficult to see clearly the fold lines representing 10:90 (NI).
5. The N_p parameter of the scheduler in PI-FL is the number of customers selected based on performance, and N_r is the number of customers selected randomly. N_p and N_r  parameters are very important for the training results of PI-FL, and further analysis should be done on how to select the appropriate N_p and N_r  parameters.

Limitations:
N/A

Rating:
4

Confidence:
4

";0
yloVae273c;"REVIEW 
Summary:
This paper studies offline reinforcement learning (RL) with linear function approximation and partial data coverage. The authors propose a primal-dual optimization method based on the linear programming (LP) formulation of RL. They prove a $O(\epsilon^{-4})$ sample complexity in both discounted setting and average-reward setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The algorithm proposed in this paper only requires near-minimal dataset coverage assumption, which is important in offline RL.
2.	The paper also considers average-reward offline RL, which is often neglected by literature.
3.	I like the table for comparison to previous work, which makes the presentation more clear (although I think there is some missing important literature, which I will mention in the weakness section).
4.	The proposed algorithm is both computationally and sample efficient.

Weaknesses:
1.	The first concern is the ‘linear function approximation’ setting, which is restricted. Actually, the main motivation that this paper studies function approximation beyond tabular settings is large state (or action) spaces in practice. However, in real settings, the linear function approximation assumption hardly ever holds. Even in Table 1, many algorithms in previous work apply to general function approximation, which further makes the setting studied in this paper restricted.
2.	Algorithm 1 in this paper achieves a $O(\epsilon^{-4})$ sample complexity. This is in terms of expectation (as shown in Theorem 3.2) instead of high probability. The previous results that the authors are comparing to are high probability bound (e.g., [1,2]), so it would be more comparable if the authors could also show a $O(\epsilon^{-4})$ sample complexity bound under high probability. Also, since the previous work studies general function approximation while this paper studies only linear function approximation, it is hard to say that a $O(\epsilon^{-4})$ sample complexity bound in linear function approximation setting is better than a $O(\epsilon^{-5})$ bound in general function approximation setting. Moreover, [3] achieves the near-optimal sample complexity $O(\epsilon^{-2})$ with near-identical settings of [1,2]. (So I disagree with the statement that ‘It is very important to notice that no practical algorithm for this setting so far, including ours, can match the minimax optimal sample complexity rate of $O(\epsilon^{-2})$’. Therefore, a $O(\epsilon^{-4})$ in linear function approximation is not that attractive compared to previous work.
3.	The authors use an LP formulation of offline RL. I think it would be better to compare to other work using LP formulation, e.g. [4,5], where [4] is computational and sample efficient under partial data coverage and general function approximation, and [5] achieves near-optimal sample complexity under similar settings.
4.	The authors compare the computational complexity. However, it is not that direct to compare an $O(n)$ complexity in linear settings to a $O(n^{7/5})$ complexity in general settings. If the authors really want to demonstrate that their algorithm has better computational complexity, it would be better to do some simulations in the same environment (even in some toy examples).
5.	Another advantage that the authors claim is that their algorithm could be adapted to average-reward setting. However, neither did the authors emphasize and explain the importance and challenges of average-reward settings, nor discuss why (or whether) previous work could not be adapted to average-reward settings. I suggest the authors discuss this a bit more.

**References**

[1] Xie, T., Cheng, C. A., Jiang, N., Mineiro, P., & Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34, 6683-6694.

[2] Cheng, C. A., Xie, T., Jiang, N., & Agarwal, A. (2022, June). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR.

[3] Zhu, H., Rashidinejad, P., & Jiao, J. (2023). Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

[4] Zhan, W., Huang, B., Huang, A., Jiang, N., & Lee, J. (2022, June). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory (pp. 2730-2775). PMLR.

[5] Rashidinejad, P., Zhu, H., Yang, K., Russell, S., & Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studied offline RL in linear MDP setting, where the transition and reward have low-rank structures and the feature $\phi$ is known. The authors formulated the problem in a primal-dual way and proposed a gradient-based algorithm. They provided convergence guarantees, which only requires coverage over optimal policy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper writing is clear and easy to follow.

The discussion and comparison with previous works is very detailed.

The algorithm is computationally efficient. The algorithm design has some interesting points, especially the reparameterization design to avoid knowlegde of $\Lambda^{-1}$ and updates for variables $v$ and $u$.

The coverage assumption seems weaker than previous literatures.

Weaknesses:
I didn't see too much technical novelty in the method and proof.

The setting is linear MDP, which is kind of restrictive.

Convergance rate is kind of far away from optimal.

Limitations:
N.A.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed an primal-dual framework for offline reinforcement learning in linear MDP Contrary to the more common case of finite horizon, they considered the case of infinite horizon with discounted reward. They reduced the problem of offline reinforcement learning to a problem about solving the saddle-point of a Lagrange form. They designed an algorithm which uses stochastic gradient-based optimization to solve the saddle point. They provide a sample complexity of O(\eps^-4) for both cases of discounted MDP and averaged-reward MDP, and their algorithm is also computational efficient. 

To summarize, the formulation of offline RL into a linear programming problem is very interesting. The proof seems very solid, and I like the comparison for the concentrability constant in the last discussion section. The comparison for the constant C is thorough and very good. 

However, I still have some questions about some details in the main text.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The formulation of offline reinforcement learning to a linear programming problem is very good.
2. The algorithm is clearly motivated by solving the saddle points of a Lagrange form. The algorithm itself is simple and computationally efficient, with a guaranteed sample complexity for both discounted MDP and averaged-reward MDP.
3. They proposed a new concentrability constant C and compare it to other constants appearing in other literatures about offline RL. I think the understanding of the relationship of these concentrability constant is basically correct and very clearly expressed.
4. The proof seems very solid and the result in averaged-reward case is new.

Weaknesses:
1. I have some question about your comparison to previous results. Your main references are Cheng et al and Xie et al. 

1.1 For Xie et al, the Theorem 3.2 in https://proceedings.neurips.cc/paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf implies that their sample complexity is O(1/\eps^2) when applied in linear function approximation. This result is based on assumption3 in their paper. This assumption naturally holds in your paper since you consider linear MDP and they consider the case of 'linear function approximation' (for their difference, see point 2). So it is natural for you to compare your sample complexity to this result, not the O(1/\eps^5) one. [notice that, their algorithm in section 3 is computationally inefficient]

1.2 In Theorem 4.1 in Xie's paper, their sample complexity is O(1/\eps^5) when applied on general function approximation, and O(1/\eps^3) when reduced to linear function approximation case (see paragraph 'Dependence oon T'). Again, their assumption for linear function approximation holds in your case. **This algorithm, however, is computationally efficient.** So you should also compare with this alg with  O(1/\eps^3) sample complexity.

1.3 In Cheng's paper, in theorem 5, their sample complexity seems to be O(1/\eps^3), not O(1/\eps^5). I wonder how you derive their sample complexity in Table one.

1.4 I am not sure how you get the O(n^{7/5}) computational complexity for Xie's paper. Could you derive it in more detail?

2. I think in many places in your paper, you confuse the two terms: linear MDP and linear function approximation. Your case is called linear MDP instead of linear function approximation, so I suggest you changing the wrong terms. For reference, both Xie's paper and Cheng's paper consider the 'linear function approximation' case.

Limitations:
/

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies offline reinforcement learning with linear function approximation. They propose a primal-dual algorithm, formulating linear RL into a minimax problem and solving it with gradient descent-ascent. Sample complexity analysis is provided for infinite-horizon discounted and average-reward MDPs, where the rate is $O(\frac{1}{\epsilon^4})$ for both settings.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is primal-dual and thus easy to implement in practice.
2. The paper provides rigid theoretical analysis.

Weaknesses:
1.  The newly defined coverage ratio $C_{\phi,c}$ is a little strange when $c\neq \frac{1}{2}$. For example, when we choose $c=1$ and thus we don't need the knowledge of $\Lambda$, the coverage ratio $C_{\phi,1}=\sum_{x,a}(\frac{\mu^*(x,a)}{\mu_B(x,a)})^2$. Then when $\mu^*=\mu_B$, $C_{\phi,1}$ will become $|X||A|$. However, in the literature, when the behavior policy is the same as the optimal policy, the coverage is typically 1. The authors claim that we can estimate the $\Lambda$ via the offline dataset so that we can choose $c=\frac{1}{2}$, but do not provide any theoretical analysis about this point. I will be more convinced if the authors can give more rigid proofs for this method.
2. The sample complexity is worse than the typical rate $\frac{1}{\epsilon^2}$.

Limitations:
None.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of offline reinforcement learning (RL) for linear Markov Decision Processes (MDPs) under the infinite-horizon discounted and average-reward settings. The authors propose a primal-dual optimization method based on the linear programming formulation of RL, which allows for efficient learning of near-optimal policies from a fixed dataset of transitions under partial coverage. The proposed algorithms improve the sample complexity compared to previous methods from $O(\epsilon^{-5})$ to $O(\epsilon^{-4})$ under the discounted setting and provide the first line of result in the average-reward setting with realizable linear function approximation and partial coverage.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 The proposed algorithm improves existing algorithms in both statistical efficiency and computational efficiency under the discounted reward setting with the linear function approximation (we note the baseline may handle problems beyond the linear MDPs).

2 The algorithms presented in this paper do not explicitly leverage the principle of pessimism, but focus on the linear programming formulation of MDP, and rely on a new reparametrization trick extended from the tabular case. The technique itself seems to be novel to me.

3 The algorithms present the first line of work for the offline average-reward MDP.  

4 The paper is easy to follow, with a thorough comparison with existing work that clearly positions the results in the literature.

Weaknesses:
1 I am confused about the requirement of $\Lambda$ to be invertible (line 140) as this seems to be very closely related to the uniform coverage condition where we assume that the smallest eigenvalue of $\Lambda$ is lower bounded from zero. I am wondering what is the key difference between them. Can you elaborate on this with some intuitions or examples?

2 The authors discuss the relationship between the coverage condition considered in this paper and that of [1] and show that the coverage condition is a low-variance version of the standard feature coverage ratio if $c=1/2$. However, in this case, the algorithm explicitly uses $\Lambda$, while the PEVI proposed in [1] does not. In contrast, $c=1$ leads to a worse bound but we do not need the knowledge of $\Lambda$. Could you provide a more detailed characterization or example to illustrate the difference between these two cases?


typo: line 328, $\epsilon^2 \to \epsilon^{-2}$

[1] is pessimism provably efficient for offline rl

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors investigate offline RL in linear MDPs and introduce a novel LP-based method. They assert that their proposed approach achieves the lowest sample complexity of $O(1/\epsilon^4)$ among computationally efficient algorithms. In comparison, existing computationally efficient algorithms can achieve $O(1/\epsilon^5)$. Additionally, the author's theory can be extended to the average reward setting.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* To the best of the author’s knowledge, in offline linear MDPs, the result in the average-reward setting is novel. 
* The LP formulation in linear MDPs is worthwhile to investigate 


Weaknesses:
* I am uncertain about whether it is appropriate to claim that existing offline RL algorithms in linear MDPs achieve $O(1/\epsilon^5)$. It appears that [38] may have better sample complexity. In Table 1 of the manuscript, the author mentions that [38] cannot handle the discounted setting. However, extending from the finite-horizon to the discounted infinite-horizon setting is relatively straightforward. Hence, this comparison may not be entirely fair. If [38] indeed has better sample complexity, it significantly impacts the author's contribution. Thus, I currently rate the paper with a score of 4.

* I am not entirely certain about the significance of the extension to the average reward case.

* Presently, I cannot determine whether the reason [9] and [36] cannot handle the average reward case is due to the algorithms or their analysis. If this limitation arises from their analysis, their algorithm has the potential to be superior as it can handle more general MDPs.

Limitations:
They discussed. 

Rating:
4

Confidence:
2

";0
oAHjj0of5z;"REVIEW 
Summary:
This paper proposes a new probability theory named indeterminate probability theory and a new model name Indeterminate Probability Neural Network (IPNN). For the new probability theory, it is an extension of the classical probability theory, with which some intractable probability problems become tractable (analytical solution). For the new model IPNN, it can perform unsupervised clustering while doing classification and make very large classifications with very small neural networks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper is well organized. The intuition of the new concept of indeterminate probability is clearly demonstrated from a simple example. The theory of indeterminate probability theory is well formulated and all assumptions are clearly numbered.

2. To the best of my knowledge, the indeterminate probability theory is proposed for the first time.

Weaknesses:
1. The proposed new probability theory and the IPNN model are interesting contributions of this paper, but not the CIPNN. As listed in the second contribution in the introduction part, the authors claim CIPNN as the contribution of this paper. However, CIPNN is the main contribution of the authors' other papers. Thus, the authors should either delete this contribution and introduce CIPNN in other places (such as related work) or contain CIPNN in this paper (then the ICCV paper should be withdrawn).

2. There are many grammar mistakes in the use of articles. For example, 'as example' on line 46 should be 'as an example', and 'an unique category' on line 257 should be 'a unique category'.

3. On lines 110 to 112, the authors mention that the event in classical probability theory can only be happened or not happened, while for IPNN, we can have the probability of an event state. However, the authors do not consider (or demonstrate) the properties of the probability in IPNN. In classical probability theory, the happening of the event is an unbiased estimate of the probability, which means that we can approximate the true probability through plenty of experiments. But what about the probability in IPNN? What is the quality of the probability output by IPNN? Typically, the softmax output of a neural network can only be treated as a probability distribution (since the sum of all coordinates is 1) but does not indicate the true probability of one class.

4. On line 120, the authors mention that $\alpha_{ij}^j (k) \in \{ 0 , 1 \}$ in classical probability. But as defined in Eq. (2), $\alpha_{ij}^j (k)$ is a conditional probability. Then it can be any decimal between 0 and 1 if there is no more constraint. Can authors provide more detailed explanations for the claim on this line?

5. As mentioned in the abstract, IPNN 'is capable of making very large classification with very small neural network'. And the idea is using binary encoding ('the binary vector is the model inputs from 000000000000 to 111111111111, which are labeled from 0 to 4095') as mentioned in appendix D.1, which is a basic concept in information theory. But in practical implementations, one-hot encoding is preferred since it does not introduce a redundant distance between different labels. For example, the binary code 001 is closer to 000 than to 111. If binary encodings can successfully reduce the network size, this would be a great contribution to this paper. Can authors provide a more detailed introduction about output encodings (especially the comparison between binary encodings and one-hot encodings in history) and more discussions about the key tricks to making binary encodings work in IPNN?

6. As mentioned in the abstract, the indeterminate probability theory makes 'some intractable probability problems have now become tractable (analytical solution)'. It seems that the authors mix up the concept of tractable and analytical solutions. A tractable problem is a problem that can be solved with acceptable complexity (usually polynomial time and space complexity). Tractability has no relation with analytical solutions. An analytical solution can be intractable when there are exponential operations in the solution, and a tractable problem may have no analytical solution (there is no analytical expression of the error function but we can approximate the error function efficiently).

7. To my understanding, the contribution of this paper is IPNN as a new neural network (architecture, engineering trick, or training algorithm). The indeterminate probability theory is far from an extension of the classical probability theory. To extend the classical probability theory, the authors should at least formulate the new theory from measure-theoretic probability theory.

Limitations:
Yes.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The main contribution is the proposal of the inference architecture which creates parallel softmax outputs (the authors call “splits”), which are combined into a joint soft-label space to make classification decisions under MAP rule; the joint label space can help with sub-classification tasks.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The architecture tries to create more Bayesian information before making final classification, which is a potential Bayesian neural network approach that can be developed in the future. On the other hand, this work may be inspiring to people who are interested in large-dimensional label representation.

Weaknesses:
1.	It is not clear to see in this paper what is new to the “classical theory of probability”;
2.	The novelty is limited.
3.	The assumptions may not be very reasonable:
The assumption 2 and 3 are ok at initialization, however when the weights are updated, A and Y are not generally independent;
The assumption 4 is very counterintuitive. Normally we have a joint distributions between X and Y, but assuming that X and Y are not independent mutually, then they are not given A; and if X and Y are independent (Y is random label for example), then they are independent given A. 
4.	The main contribution of this paper is the splitting part of the architecture, creating parallel softmax outputs and combine them to make MAP decision. I think the paper should emphasize on the reasoning of this mechanism, for example, the ensemble of different likelihoods that contributes to the performance, or the geometric interpretation of the proposed label embedding/representation that makes sense.
5.	Lacking of the verification of the advantage of the new method over original softmax. After reading this paper I still do not know if this method creates more or reduce a little uncertainty in the classification compared to original softmax approach.


Limitations:
yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes what I consider to be a type of neuro-symbolic AI model involving neural networks for multi-class classification problems; the authors refer to the model as an indeterminate probability neural network (IPNN). Frankly I did not fully understand the model, but the main intent appears to be a form of latent variable modeling for multi-class classification. An important line to summarize the paper is mentioned in Section 2: “our proposed IPNN is one DPVM (deep latent variable model) with discrete latent variables and the intractable posterior calculation is now analytically solved with our proposed theory”. I have reviewed a version of this paper previously, and it looks like the paper has not changed much; this is unfortunate, because I did not understand the paper clearly at that time, and I feel that I still do not understand it well.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper claims to blend neural networks and probability theory in a novel way; if this is true then it can be seen as an innovation in neural network modeling as well as neuro-symbolic AI. Another strength is that the paper is non-standard in that it tries to do something new around modeling multi-class classification problems.

Weaknesses:
In current form, the paper suffers from a number of weaknesses. A major weakness is that it remains unclear why a new theory of probability is needed in the first place. Note that the example in Section 3 can be studied using standard Bayesian modeling where X_i is the true coin toss result, A_i the adult’s reading and Y_i is the child’s reading, all for the i^{th} coin toss. Here X_i are i.i.d random variables, and A_i and Y_i are conditionally independent given X_i. Then the query of interest can be posed in terms of these random variables. I did not understand what the new theory is and why it is even needed. Also, I find it hard to follow the paper and feel it is not appropriately positioned in the literature. For instance, to me, it appears that the proposed approach is a form of neuro-symbolic AI, yet this is not even mentioned in the paper. I feel there is far too much lack of clarity in the paper in general.

Limitations:
The authors need to write more about the limitations of the work.

Rating:
3

Confidence:
2

";0
2pVogxJyDA;"REVIEW 
Summary:
This paper introduces PromptCoT, an enhancer that automatically refines text prompts for diffusion-based generative models, improving their capability to produce high-quality visual content. The system is based on the idea that prompts that resemble high-quality image descriptions from the training set lead to better generation performance. Pre-trained Large Language Models (LLM) are fine-tuned using a dataset of such high-quality descriptions, allowing them to generate improved prompts. To mitigate the tendency of LLMs to generate irrelevant information, contamination, transfer, and even a Chain-of-Thought (CoT) mechanism are used to improve alignment between original and refined prompts. Additionally, to maintain computational efficiency, the system employs adapters for dataset-specific adaptations, leveraging a shared pre-trained LLM. When tested on popular latent diffusion models for image and video generation, PromptCoT showed significant performance improvements.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1) This paper shows an insight from the observation that prompts resembling high-quality image descriptions from the training set lead to better generation performance.  It makes sense from my perspective.  Current text-guided image generation is still with limited generalization ability.  The points near the training support samples are transferred better. 
2) The idea to utilize LLM to adapt the original prompt to the one that is more aligned with training samples makes sense. It leverages the ability of LLMs to align the distributions. 
3) Three training methods are proposed to implement the ideas including the continuation, revision, and CoT.  
4) This paper builds the corresponding datasets to help the fine-tuning, which can benefit the following research.
5) The method using the GPT-3 to build datasets is smart.

Weaknesses:
1) The finetuning of LLaMa is time-consuming.  Could it be replaced by prompt learning or LORA? 
2) Please add a discussion (e.g. in related work) with ''Visual Chain-of-Thought Diffusion Models'', though these two methods are clearly different. 
3) If the datasets with neural captions can be built.  How about adding these sentences to the training set of the diffusion model?


Limitations:
It can be found in the F part of the appendix.

Rating:
7

Confidence:
4

REVIEW 
Summary:
 In this manuscript, the authors proposed a simple yet effective framework to improve the generation quality of pretrained generative models. Generally, to align the prompt distribution with large language models, the authors present three individual solutions to align and enhance the original textual inputs, i.e., providing a compelling text continuation with given initial inputs, revising the initial inputs, and using chain-of-thought to enhance the initial inputs by 5 steps. Moreover, the authors also introduce multi-task adaptation to improve the quality of generated textual desctiptions for each dataset. The experimental results demonstrate that the generative models with PromptCoT can achieve better qualitative performance than the baseline method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The proposed PromptCoT is easy to follow.

2. The qualitative results is promising. 

Weaknesses:
1. Introducing prompting technique to improve the quality of the textual inputs for generative models lacks novelty, since intuitively, providing more detailed information indeed improves the generation quality of the generative models. The authors could clarify the novelty, e.g., how the prompting technique or the CoT technique different from other works, or why the generated text is better than human-refined counterparts.

2. As shown in Figure 4, the proposed text revision technique may generate some uncontrollable additional information (e.g., time, weather and place). I think the prompted results may be influenced by the inherent bias of the language model or the prompting dataset. The authors could provide some analysis to show that the enhanced textual inputs are unbiased with given CoT technique, or show that the enhanced textual inputs are controllable.

Limitations:
The authors have stated the limitation of the proposed PromptCoT, i.e., relying on the capability of the generative models and the quality of inital textual inputs. These limitations can be seen as the future direction of this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper aims to improve the images generated by the off-the-shelf diffusion model, such as Stable Diffusion. This is done by fine-tuning a large language model (LLaMA) using text continuation on more high-quality prompts, which are collected by hand-crafted rules on, for example high CLIP similarity and text length. Furthermore, off-the-shelf image caption models (like BLIP) are also used to curate high-quality prompts. While the proposed method seems effective, the lack of technical novelty is a concern of this work.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	The overall presentation is good, and the paper is simple to follow.

2.	The visualization is very interesting. For example, Figure 1 is very impressive.

3.	The overall pipeline is well encapsulated in Figure2-4. It is simple for the reader to understand the high-level idea of the work by looking at the figures. 


Weaknesses:
Major 
1.	Novelty is a big concern in this work and there is no technical contribution in this work. The author basically uses many off-the-shelf techniques to augment/modify the input text.  Text-continuation is also commonly used for pretraining large LLM. The adaptation techniques in section 3.4 are also not novel and the author simply leverages the existing techniques. In general, it is unclear what is the technical contribution of this work. 

2.	L258 Is there any reason why t2t-blip booster demonstrates the best performance? Any theoretical reason behind this?

Minor
1.	L224 gpt to “GPT”


Limitations:
Limitation is discussed in appendix

Rating:
5

Confidence:
4

REVIEW 
Summary:
The main motivation of this paper is to better align prompts to the textual information of high-quality images within the training set. The authors propose datasets, instruction templates and use CoT to finetune LLM to achieve this goal. Adapters are also use adapters to facilitate dataset-specific adaptation.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Application of LLMs to image generation for better text alignment is reasonable. The authors prepared datasets and conducted curated pipeline to achieve this goal. The presentation is clear.

Weaknesses:
1. the observation that prompts aligned with high-quality images withing the training set are more inclined to yield visually superior outputs needs more demonstration. The authors verified this on  LAION dataset. But it does not mean this is always the truth on all datasets. Whether it works on Flickr, Pinterest images?
2. The application of LLMs to refine texts is widely used now. Prompt engineering, Cot, adapters are also widely explored. Only the Application of these techniques seems not novel enough. 
3. The pipeline may needs more explanation on why we need text continuation first then text revision. 
4. the experiments are quite confusing. what are t2t-inter, cot_d, cot? the scores in Table 2 shows the effectiveness of each booster. How do they work together?  Which booster works the best  according to these experiments. what are Table 3 and Table 4 used for? It's really hard to summarize a conclusion that can match the design of the whole method.

Limitations:
limitations are presented in the paper.

Rating:
4

Confidence:
5

";0
NrG1fURihk;"REVIEW 
Summary:
This paper studies the important problem of representation learning, where the goal is to learn a representation that works well for all problems in a known problem class. It presents theoretical results for the special case where both the representation function and prediction function class are linear, and also provide an extension of the algorithm to the general function class setting.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The problem of representation learning is obviously important, and the paper did a good job describing the problem setting, motivation and results.

Weaknesses:
1. The discussion regarding mixed representation in the paragraph on line 61 seems not practically relevant. In most cases, it makes sense to assume that there exists a representation z that captures all necessary information from x that predict any function f in F, which implies that there is a pure-strategy Nash Equilibrium. 

2. An important piece of prior work is missing. [3] studies the exact same problem setting that is studied in this paper. Their theoretical result seems to subsume the theoretical result in this paper, as they allow arbitrary nonlinear representation and prediction functions. In addition, there is another line of work that assumes the tasks come in an i.i.d fashion, and uses uniform convergence analysis to bound the regret on a new task [1,2]. At the moment, it is unclear how the current paper compare with these prior works mentioned above. In particular, what are the differences between Theorem 2 in the current paper v.s. Theorem 2 and 3 in [3] when specialized to the linear setting.

[1] Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149–198, 2000.

[2] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 2016.

[3] Tripuraneni, Nilesh, Michael Jordan, and Chi Jin. ""On the theory of transfer learning: The importance of task diversity."" Advances in neural information processing systems， 2020.

Limitations:
NA

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper extends and formulates learning representation procedure as a three-player game when the downstream task is known. Unlike conventional unsupervised learning algorithms, the proposed framework helps the learning process in the feature space be directed. They set up a general and basic mathematic form (linear-MSE) to study, which proves the utility of prior information. And, they extend results via studying mixed strategies, which shows the importance to use randomized representations. They propose gradient-based iterative algorithms to optimize objectives and validate the effectiveness of their framework with examples (i.e., linear-MSE and linear-CE).


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
This paper presents several strengths, including a clear and well-written introduction that guides the reader to understand the problem and the motivations behind it. The findings from this paper is interesting: it shows the benefits to use randomized representation. Besides, the authors provide concrete steps to introduce iterative algorithm for their general cases to establish it, although the takeaway conclusion is not always straightforward to see, such as in the figure for Example 5.

Weaknesses:
Here are some potential improvements to the original content:

1. The paper could benefit from a more concrete approach to evaluating the effectiveness of the proposed framework. While using regret as a metric is a relative approach, it may be more informative to show performance using exact MSE. For example, the paper could compare results between a conventional representation (without prior knowledge about downstream task) and a representation found by the proposed framework, with prior knowledge about the downstream task. This could help demonstrate any advantages of the proposed framework more clearly.

2. To strengthen the evidence presented in the paper, it would be beneficial to include experiments with real-world datasets. Providing results with tabular datasets could help establish stronger findings.

Limitations:
The paper presents a foundational approach that can be applied to various scenarios. The limitations and wider implications discussed in the paper are well-defined. Finding proper prior knowledge to use and further compute the representation should be studied and explored.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies the problem of dimensionality reduction given prior information about downstream tasks via game theoretic optimization point of view. They define a three player game.  One player chooses the parameters of the representation function that reduces the dimensionality of the data. One player chooses the downstream task, constrained to respect a task distribution prior. The last player chooses the parameters of the model learned based on the dimensionality reduced inputs and the task. The authors theoretically analyze the case of linear dimensionality reduction and linear prediction functions under the MSE loss function. They then propose a greedy algorithm for the general case, which they evaluate empirically.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The framework and proposed solution are clearly explained. To the best of my knowledge, while adversarial learning between two or more agents is a well explored tool for improving generalization and robustness, the specific framework proposed by the authors is novel. In addition, even in the seemingly easy case of linear models and MSE loss, Theorem 3 is quite non trivial. The iterative greedy algorithm for the general case may also be of interest to practitioners.     

Weaknesses:
 I would have liked to see more motivation for the specific framework proposed. More specifically:

 * It is not clear why the game is not a a two player game instead of a three player game. I do not understand why we need two separate players for controlling $R$ and $Q$. In fact the paper mostly ignores the $Q$ player by focusing on the regret of the $R$ player, implicitly assuming that $Q$ best responds by finding the optimal model. Is there a specific motivation for having the $Q$ player?
*  If we were to collapse the $R$ and $Q$ player, the setting is closer to adversarial learning between two players, the one that chooses the model and the one that (with some constraints) is allowed to change a given training dataset in an adversarial way. The main difference is that in this work the adversary controls a response function $f$ instead of manipulating a training set. It is not clear to me if/when this difference is significant, especially in practical settings where the input feature and response function distributions are discrete. 
* It is not very clear to me if/how the randomized representation function $R$ in the mixed strategy setting can be used in practice. While it makes sense from a game theoretic point of view to extend to mixed strategies, I think connecting this back to the representation learning use case will help make the paper more compelling. 

Limitations:
Limitations have been adequately addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the representation learning from the multi-layer gaming perceptive. The authors propose an min-max optimization problem for learning a good representation with the prior knowledge of the latter prediction tasks. The authors provide some theoretical analyses and examples in the linear case. For the general tasks, some iterative algorithms are proposed and the numerical results are provided to demonstrate the performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- This paper is well written and easy to follow. The theoretical results are provided with several examples to demonstrate the intuition behind the theories.
- Numerical results demonstrate the effectiveness of the proposed result.

Weaknesses:
- The connection between the linear case and the general, three-player game case is not enough. In the linear case setting, it's easy to show a close form solution for this min-max problem. However, in the general setting, the authors does not fully demonstrate why the iterative based algorithm can convergence.
- I wonder the time complexity of the proposed general case algorithm since it need to call Algorithm 2 and 3 in the nested loops. 

Limitations:
NA

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper presents a game theoretic framework for learning representations in a general setup where some knowledge/prior about the prediction task is available. The representation learning task is cast as a certain game --- which is worked out exactly in the fully linear setting under the squared loss (casting new light on PCA-type techniques), and for which a minimax solution-seeking iterative gradient-based method is provided for a more general case. Theory is accompanied by examples, both worked-out and experiments, that support the theory and offer new insights into well-known representation learning questions (such as the necessity of mixed rather than pure representations etc).

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The main strength of this paper is that the game theoretic framework proposed by the authors appears to shed light onto many important questions about representations (including the necessity and type of required randomization in the representation, how to form universal representation-based initializations, how to use priors, etc.) in a very compact setup. Secondly, the theory in the linear case is worked out quite precisely and nicely, thus shedding new (to me at least) light on PCA-based representation technique. Nice worked-out examples are given throughout the paper to support the general theory. As well as, finally, not only is an interesting general representation-building algorithm given, but it is also, through a few experiments, confirmed to be implementable, reasonable, and overall sufficiently practical.

Weaknesses:
A weakness of the proposed game-theoretic framework (which could alternately be viewed as its strength, paradoxically) is that it doesn't seem to allow us to run away from the intrinsic hardness (and hardness of analysis) of finding good representations --- instead, this difficulty simply translates into the difficulty of procuring convergence analyses/rates for methods that solve for generally nonconvex/nonconcave saddle points that result out of this formulation. Now, why this could in the future prove to be a strength is that conceivably, it might allow for some future work on connecting hardness of finding good representations (in settings such as the one discussed in this paper), to the hardness of certain saddle point problems. Still, such future use is not fully clear at this point, which is why at this point in time I listed it here. (Of course, I would welcome arguments from the authors as to how this weakness could be turned into a strength.) 

That said, given the hardness of the problem and the overall amount of interesting contributions in this paper, I will not consider the above to be of too much impact for the purposes of rating this paper.

Limitations:
Yes.

Rating:
6

Confidence:
3

";0
LkTbLQDo4I;"REVIEW 
Summary:
This paper proposes a sampling method for directed protein evolution. The method includes three parts: Bi-level Gibbs sampling (BiG), graph-based smoothing (GS), and iterative extrapolation (IE). BiG improves sampling efficiency for multi-point mutant sequences, GS helps to sample toward high fitness sequences, and IE reduces the Taylor approximation error. Experiments results show that the proposed method could achieve competitive performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The presentation is clear and the idea is reasonable. The figures are beautiful and The authors clearly formalize the problem and key operations.
- The addressed problem is important. The relevant methods may be applied in different protein design scenarios.
- The experiments are convincing. I appreciate the authors' efforts in revealing the effect of each module via ablation studies.

Weaknesses:
- The proposed approach is somewhat heuristic and lacks theoretical assurance.
- There is a lack of relevant prior research on protein design [1-7].
- The proposed method does not compare with strong baselines such as the diffusion-based method [6,7]. Therefore, it is challenging for reviewers to verify its effectiveness and potential.

[1] Gao, Zhangyang, Cheng Tan, and Stan Z. Li. ""Alphadesign: A graph protein design method and benchmark on alphafolddb."" arXiv preprint arXiv:2202.01079 (2022).

[2] Hsu, Chloe, et al. ""Learning inverse folding from millions of predicted structures."" International Conference on Machine Learning. PMLR, 2022.

[2] Gao, Zhangyang, Cheng Tan, and Stan Z. Li. ""PiFold: Toward effective and efficient protein inverse folding."" The Eleventh International Conference on Learning Representations. 2022.

[3] Ingraham, John, et al. ""Generative models for graph-based protein design."" Advances in neural information processing systems 32 (2019).

[4] Tan, Cheng, et al. ""Global-Context Aware Generative Protein Design."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.

[5] Jing, Bowen, et al. ""Learning from protein structure with geometric vector perceptrons."" arXiv preprint arXiv:2009.01411 (2020).

[6] Stanton, Samuel, et al. ""Accelerating bayesian optimization for biological sequence design with denoising autoencoders."" International Conference on Machine Learning. PMLR, 2022.

[7] Gruver, Nate, et al. ""Protein Design with Guided Discrete Diffusion."" arXiv preprint arXiv:2305.20009 (2023).

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
This work applies gibbs sampling methods for generating random sequences of proteins to find mutants with high fitness. The model is compared against a set of sampling methods and the results are evaluated on GFP by fitness, diversity, and novelty. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem formulation and model construction are presented clearly. 
- The experiments were evaluated comprehensively from different perspectives.

Weaknesses:
- The motivation for random sampling the whole sequence for a protein in the context of is unclear. 
- As AAV has been mentioned more than once in Introduction, it is preferred to put them in the main text. 
- While the proposed solution to the problem is a sampling method, other baseline methods that have been used to solve the same problem might also be compared, such as the ESM-series.

Limitations:
The authors did not analyze the potential negative societal impact of their work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel method to optimize protein fitness by using bi-level Gibbs sampling with graph-based smoothing. Experiments on real datasets are used to verify the effectiveness of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The studied problem about designing novel protein is interesting and challenging. 

2. The proposed method seems to be novel.

3. Experiments show that the proposed method can outperform other baselines to achieve the state-of-the-art performance. 


Weaknesses:
The writing can be improved. There exist many typos and grammatical errors. Line 106, where is Algorithm 2? Furthermore, the organization can also be improved. For example, Section 3 can be integrated into “Experiment”. It is better to move Section 4 to be just after “Introduction” or just before “Conclusion”. 

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work considers the problem of optimization over a noisy fitness landscape of sequences. There are two main contributions: 1) Graph-based smoothing (GS) --- considering the landscape as a noisy landscape and applying L1 graph Laplacian regularization. 2) Bi-level Gibbs (BiG) --- using bi-level sampling to propose 5 indices to mutate simultaneously.

To evaluate the performance of the proposed method, the authors used design benchmarks based on oracles trained on the GFP and AAV datasets, and showed improved sequence optimization on the synthetic design benchmarks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This work proposes a new sampling method (Bi-level Gibbs with Graph-based Smoothing) in the protein sequence optimization problem. The proposed method is well-motivated. In particular, graph-based smoothing is an interesting approach to address the non-smoothness of protein fitness landscapes. The authors gave a clear description of the proposed method and presented detailed empirical results on simulated data based on real-world experimental data. In the evaluation, the authors took care to avoid overfitting the algorithm and included two datasets (GFP and AAV) to study two different protein fitness landscapes with different properties. The ideas in this work could inspire future work around this important topic.

Weaknesses:
The main weakness is in the empirical evaluation. The evaluation is based on machine learning models as oracles. Even though the oracle models are trained on experimental data, the fitness landscape in the oracle models might have very different properties compared to real protein fitness landscapes. For example, for graph-based smoothing to work well, it seems like some amount of smoothness in the landscape is needed, and machine learning models might present artificially smoother landscapes than real fitness landscapes. Therefore, one major concern is that the empirical results are tied to the oracle models.

That being said, it is notoriously challenging to evaluate protein sequence design algorithms as doing so rigorously would require collecting new data points, so this is not a unique weakness to this particular work at hand. In short of performing experimental evaluation, it could relatively strengthen the paper to study more fitness landscapes in the literature and demonstrate evidence that they do have the similar characteristics as the oracle landscapes.

Limitations:
The discussion section does mention limitations around handling variable length sequences and computational speed. It would also be helpful to include a detailed discussion on what types of fitness landscapes BiG would be well-suited for, and counter-examples of fitness landscapes where BiG fails to find optimal solutions. In particular, is it always a good idea to mutate many positions simultaneously? are there situations where it would be preferable to do one mutation at a time?

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors proposed a Gibbs sampling algorithm with graph signal smoothing that can generate high fitness protein sequence. The proposed method, BiGGS, constructs a smoothed space between protein sequence nodes, and use the gradient of a trained fitness predictor to guide the sampling of both new locations and its corresponding mutations. The authors also provided benchmark dataset with different level of difficulties to validate the efficacy of BiGGS, as well as another dataset that is of different fitness measure to showcase the generalization capability. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem of valid protein sequence generation is popularly studied in both computational biology and machine learning community. The proposed method is a combination of traditional sampling techniques, neural predictor for protein sequence, and graph signal smoothing. Overall the paper is well-written with minor typos that don't affect my reading. The problem is well-stated with each component of the method clearly described in details. The authors also find the issues with existing benchmark datasets, which are labelled as ""easy setting"" in this paper based on the distance between highest fitness sequence and the initial set of sequences, and proposed more challenge version of it. In the experimentation section, the authors provided extensive comparison with different baseline methods and BiGGS exhibits significant performance improvement. The ablation study also provide insights on which component brings the most improvement. 

Weaknesses:
Besides the strengths mentioned above, there are several areas that can be improved:
- The hyperparameters seems to be chose arbitrary and the hyperparameter tuning seems difficult. The justification of choice is also missing.
- The complexity of method is quite high, especially the retraining step after smoothing, which could be a major bottleneck of optimizing the methods in other use cases. 
- The intuition behind graph smoothing is somewhat contradicting to the ""non-smooth"" assumption in protein sequence fitness space, where changing one position will greatly shift the fitness score. Using single mutation neighbors are a bit worrying due to this reason. 
- Lack of comparison with different predictors, or evidence of the existing oracle won't work with the framework. 


Limitations:
The authors adequately addressed the limitations and I don't a potential negative societal impact of this work. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper introduces BiGGS, a new sequence-based protein fitness optimization algorithm. BiGGS employs bi-level Gibbs sampling for efficient mutation sampling, graph-based smoothing to regularize the fitness landscape, and iterative extrapolation for progressive mutation towards higher fitness. The algorithm's effectiveness is demonstrated through benchmarks on Green Fluorescent Proteins and Adeno-Associated Virus, where BiGGS exhibits state-of-the-art performance in optimizing protein fitness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The introduction of BiGGS, a novel sequence-based protein fitness optimization algorithm that incorporates ideas from MCMC, graph Laplacian regularization, and directed evolution.

2. The authors establish a novel benchmark on Green Fluorescent Proteins which presents the unique challenge of initiating the process with sequences of low fitness that require numerous edits to reach peak fitness.

3. The experimental results convincingly demonstrate that BiGGS surpasses other recently proposed methods in terms of both fitness and novelty, marking a significant advancement in the field.

4. The authors provide comprehensive details about the datasets, methods, and evaluation metrics used in their experiments. The well-structured and documented code is an added advantage.

5. The paper also discusses potential extensions of BiGGS, including improving BiG by removing the independence assumption across residues, exploring better regularization techniques for protein fitness predictors, and investigating BiGGS to handle variable length sequences, multiple objectives, and multiple rounds of optimization.

Weaknesses:
1. While the paper presents compelling results, it could be further enhanced by incorporating a case study that applies the proposed method in a real-world scenario. This would provide tangible evidence of the method's effectiveness in practical applications. For instance, the abstract asserts that ""Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set"", but this claim lacks further substantiation within the body of the paper.

2. The paper's evaluation methodology heavily depends on a fitness score, calculated through a trained, black-box neural network. This approach raises questions about the results' reliability and interpretability. It would be beneficial if the authors could explore strategies to increase the transparency and comprehensibility of the evaluation process.


3. As someone unfamiliar with protein fitness optimization, I find it challenging to gauge whether this highly specialized task would be of interest to the broader NeurIPS community. 

Limitations:
The paper does not explicitly address any limitations. However, from my viewpoint, a significant limitation lies in the disparity between the evaluation pipeline employed in the paper and the realities of practical application scenarios. The authors could strengthen their work by acknowledging and discussing this gap.

Rating:
6

Confidence:
1

";0
0QpwcDPiHT;"REVIEW 
Summary:
The paper presents evidence that LMs sometimes use а computational mechanism similar to traditional word embeddings, specifically using simple vector arithmetic to encode abstract relations. Experiments show that this mechanism is specific to tasks that require retrieval from pretraining memory rather than local context. In sum, this paper sheds light on the inner workings of LMs and provides insights into their interpretability.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- The paper is well-written, easy to follow, and is a pleasure to read.

- It is appreciated that the paper use LMs of a range of sizes to investigate the arithmetic mechanism.

- The paper provides a novel way to isolate the function application. By adding vectors $\vec{o}\_{city}$, $\vec{o}\_{upper}$ and $\vec{o}\_{past}$ to a synthesized dataset, the authors show that the model can generate outputs of the intended function.

- The results and analysis provide insights into how LMs work on a bunch of tasks that require retrieval from their pretraining memory, which is interesting to the community.


Weaknesses:
- The authors focus on the country-capital task as it is more interesting, or, due to the page limit. However, it is also encouraged to (if space allows) do an investigation of how the early decoding output token change layer-by-layer also for the other two tasks.

- Except for when and why LMs output correctly, it is also important to know when and why LMs output incorrectly. However, the paper does not include such an in-depth discussion. Thus, I would encourage the authors to include some examples of when LMs do not correctly output the correct capital of a country/ the upper case of a word / the past tense of the verb. Importantly, the authors should provide some insights into the hard cases (if exist): i.e., LMs can correctly output the capital of a country but cannot when adding $\vec{o}\_{city}$ to the corresponding country vector in synthesized data.

Limitations:
The paper does not include the Limitations or Broader Impact sections. Therefore not applicable.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper offers new findings on interpreting the internal processes of language models. Specifically, the authors identify a particular mechanism that is similar to word2vec style vector arithmetic. By decoding the next token after each attention layer and FFN layer, they examine the structure within the embedding space along the residual stream. They base their findings on three tasks: get_capital on factual knowledge recall, uppercase/past_tense on changing word morphology. The experiments cover three pretrained language models with varying sizes: GPT-J, GPT2-Medium, and BLOOM.

Interestingly, the authors find that the information flow along the residual stream can be decomposed into identifiable stages: argument preparation, function application, and saturation. Moreover, they also identify that some particular FFN is responsible for the function application, where the output vector of this particular FFN can be used independently in different contexts to replace the FFN.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
This paper provides novel insights into the internal processing of large language models. The topic of making LLMs trustworthy through the interpretation of their internal processes is timely and should attract a large audience at NeurIPS.

I particularly appreciate the creative design of the intervention experiments. The discovery of a word2vec-style arithmetic operation in the FFN output vector is intriguing. It suggests that certain FFNs produce context-invariant vectors that can serve as specific functions.

Overall, the experiments appear to be well-conducted and the limitations are adequately discussed.

Weaknesses:
I appreciate the work being done in the field of interpretability, but I have some concerns that I hope the authors can address. Specifically, I’m curious about whether the interpretability analysis holds for various models that have been pre-trained or fine-tuned on different datasets. While it’s possible to identify interesting patterns by studying information flow and neuron activation, I wonder how much of this is influenced by random initialization or dataset noise. Should our conclusions be conditioned on specific pre-training/finetuning datasets, architectures, and learning algorithms?

In addition to these general concerns, I’m also interested in whether the conclusions of this paper can be extended to other tasks. The authors focused on world capitals, upper-casing, and past-tensing, but there are many other factual relations that could be investigated, such as `get_president_of`, `get_university_of`, and `get_son_of`. Why did the authors choose to focus only on `get_capital_of` instead of exploring these other relations?

Limitations:
The authors have adequately addressed the limitations. I appreciate this a lot.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes the conjecture that Transformer-based large language models also implement the vector arithmetic (namely vector subtraction and addition) for word analogy tasks, similarly as the well-known property of word embeddings. Experiments on three word analogy tasks support the conjecture. Such findings lead to a better understanding to the Transformer-based models. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- Reasonable hypothesis about the implicit mechanisms of pretrained Transformers. 
- Comprehensive and fairly convincing experiments to verify the hypothesis. 
- Clear writing. 

Weaknesses:
While it's well known that word embeddings hold the vector addition mechanism for word analogy, the choice of such mechanism is somewhat arbitrary. 
There is work, e.g., [this EMNLP 2019 paper](https://aclanthology.org/D19-1354.pdf), demonstrating that analogical relations can be represented by vector rotation too. 
This paper only investigates the mechanism of vector addition and didn't compare with other potential ways to represent analogy---in fact, given the Transformers' residual architectures, I feel the vector arithmetic mechanism for Transformers better motivated than that for word embeddings.

In addition, a few points (see questions) are not perfectly clear to me.  

Limitations:
Yes, the authors have discussed the limitation of this work. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper investigates how a large language model (LLM) computes the vector representation of an output token. In particular, the authors focus on tasks in which the LLM is required to output a token that is related to an input token in a certain kind of relation (e.g. Country-Capital relation). The authors show that an LLM implements this computation using Word2Vec-like vector arithmetic where the vector corresponding to the relation is computed by the feed-forward network in a mid-to-late layer of the transformer. The authors also show that the computed vector is mostly context independent and can be applied to different input sequences. They also show that this mechanism is not observed when the required output token is found in the input sequence.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper presents interesting findings that should help understand the inner workings of LLMs.
- The methods used by the authors to investigate the mechanism seem to be technically sound and non-trivial.
- The findings may be used to develop a better architecture for LLMs.


Weaknesses:
- The paper presents interesting findings, but does not really give explanations as to why they are what they seem to be. 

Limitations:
Yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper examines whether the residual representations in GPT models obey word2vec-style arithmetic. For three different (head, relation, tail) relations, the authors find evidence that the transformer:
1. Writes the head into the residual stream
2. Transforms the head into the tail, observed via a sudden ""inflection"" point in the reciprocal rank of the tail.
3. Saturates in its prediction of the tail.

For those three relations, they find that (2) can be replaced by a simple vector addition, therefore indicating word2vec-style arithmetic.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
* Paper is very well written and easy to follow.
* The conclusion, if true, would be very interesting.
* The study of internal mechanisms within LLMs is an important and meaningful problem. We are in dire need of simple, elegant, and useful models that interpret their internals.

Weaknesses:
Most of my concerns have to do with whether these findings generalize beyond the three tasks they are derived from. I'm generally a fan of simple, elegant models, but it feels like an $\vec{a} + \vec{b} = \vec{c}$ framework might be _too_ simple to hold up in practice. I'd love to be convinced otherwise!

* I empathize with the fact that it's hard to come up with simple, clean evaluation tasks that GPT is proficient at, but I'd like to see more relations. It seems possible that you've just stumbled upon a very small class of relations for which, maybe, the affine operator implemented by the layers is simply a vector addition.
* In the three tasks presented, the mappings are mostly bijective, with the exception that some countries have multiple capital cities (which I'm not sure is accounted for in evaluations). How does vector arithmetic work with one-to-many, many-to-many, and many-to-one relations? If the delta applied by the operator has no dependence on the input, then this won't hold up, right?
* It would be great to include negative results as well, if you have them. There should be some analysis on what types of relations this vector addition model struggles with — and whether you can find any interesting patterns describing what works and what doesn't.
* How is this $\vec{o}$ computed by the transformer, do you know? Is it by recalling knowledge in FFN weights? Copying information from other tokens? How do you know the best $\vec{o}$ comes from FFN and not attention?
* I'd like to see more discussion of the implications of these results. The story is simple and enticing, but what does this tell us? What lessons are generalizable and transferrable to other areas of interpretability? Can you definitively say that the FFNs are recalling knowledge at the last token? How are other tokens involved?

Limitations:
n/a

Rating:
6

Confidence:
3

";0
Ai40Gvt2wj;"REVIEW 
Summary:
The authors propose a SO(3)-equivariant network operating on scalars, vectors and 2-tensors. They identify the corresponding equivariant linear layers and come up with a mixing strategy to mix different representations. Furthermore, SO(2)-equivariant linear layers are proposed to allow the scenario that SO(3) symmetry breaks into a subgroup SO(2) axial symmetry along a specific axis $\hat{j}$.  Finally, they demonstrate how it is appllied to b-tagging in High Energy Physics (HEP), where the data is rotational symmetric w.r.t. jet axis $\hat{j}$.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The authors provide a good and simple implementation of SO(3) equivariant networks on scalars, vectors and 2-tensors. The weight matrix is carefully designed to preserve the symmetry. The way to mix different representations is also intuitive and easy to understand. 
- The discussion on axial symmetry is well motivated and easy to follow, and the analysis is solid and sound. 

Weaknesses:
- [Novelty] The idea of tensor-product-based representations is not new [1]. The main method (except the SO(2) part) looks a simple variation and the technique involved is pretty standard. Add discussion and comparison with existing tensor-product-representation-based methods could make the work more solid.
- [Evaluation] To show the effectiveness of mixing and SO(2) linear layers, I think it is better to put more intermidiate results (e.g., w/ and w/o SO(2) linear layers) in the main table. 
- [Minor issues]: Eq. (1, 2, 3) use Einstein summation without declaration, which may cause confusion to readers without physics background. Line 168 should be ""isotropic linear neuron of Eq. (2)"" instead of ""Eq. (1)"".

[1] Finkelshtein, Ben, et al. ""A simple and universal rotation equivariant point-cloud network."" _Topological, Algebraic and Geometric Learning Workshops 2022_. PMLR, 2022.


Limitations:
Limitations are not included in the manuscript.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work presents a lightweight architecture based on scalars, vectors, and tensors for learning in 3D, subject to $SO(2)$-equivariance in a known direction that varies by sample. They are motivated by the jet-tagging problem in high energy physics, in which a given batch is equivariant with respect to a certain axis (which may vary between batches), and test their framework on this problem.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
Restricting to cross products and matrix products is new relative to previous work, which tends to focus more on the expressivity via irreducible representations of higher orders. The jet HEP dataset is also not commonly used in similar papers, and could provide a useful dataset for future papers. The proposed operations do indeed seem to be equivariant, and they are written out explicitly.

Weaknesses:
1. The proposed architecture is a simple restriction of many other existing architectures. The “tensor bilinear layer”, is a special case of the more general CG product that is now standard practice in other architectures, and it is not clear what benefits it has over a more general CG architecture. The nonlinearity of scaling by the vector or tensor norm is also not new: it is subsumed by the nonlinearity of applying an arbitrary function to the norm, and then scaling the vector or tensor by this value, which similarly was widespread in foundational works such as Tensor Field Networks (Thomas et al 2018) and subsequent works.
2. It does not seem like this architecture is particularly expressive, due to the use of low order features and the simple nonlinearity. (Note that prior work on the universality of point cloud architectures, and equivariant architectures more generally, usually requires making statements about polynomial approximation, where higher order tensor products are required to approximate higher degree polynomials — see e.g. Lorentz nets, Bogatskiy et al 2020, or Dym et al 2020 on the universality of point cloud architectures. Therefore, it seems to me that these simpler layers will likely have worse approximation properties.)
3. Based on the paper’s description (but the authors can correct if this is not the case), the motivating problem is really SO(2)-equivariant about a known but sample-dependent axis. Therefore, the discussion of the SO(3) CG product and other SO(3) architectures is somewhat misleading/confusing. It is also not explained clearly how using this paper’s SO(2)-equivariant architecture compares to the standard approaches in HEP based on the “transverse and longitudinal projections”.
4. The baselines and experiments are not sufficiently developed. For example, the only baseline is a DeepSet-style permutation invariant architecture, presumably on the raw 3D coordinates. However, the authors should compare to approaches using the apparently more standard “transverse and longitudinal projections”, as well as to SO(2)- or SO(3)-equivariant baselines on the raw coordinates. The paper claims that using a physically intuitive restriction of the space of possible operations is beneficial, but does not demonstrate this with an ablation study or by using e.g. a full CG product.

Limitations:
There is no potential negative societal impact. The paper does not address a potential lack of expressivity of the architecture.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper presents a method to handle complex geometric structured data, specifically SO(3) representations, through SO(3)-equivariance and judicious symmetry breaking. The technique improves computational efficiency and enhances the performance of a network operating on these representations. When applied to b-tagging, a High Energy Physics classification problem, it yielded a 2.7x improvement in rejection score over conventional methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. It makes sense in machine learning to explore more efficient representation for data with symmetric structures. It is particularly important in many scientific fields such as HEP and material.
2. Using the proposed method, it shows a significant performance improvement compared with the baseline method.

Weaknesses:
1. Some explanation in the paper is difficult to be followed by ML researchers. This work deeply involves the task of B-jet identification at LHC experiments, but I’m not sure if these are sufficiently interesting for the ML and AI community.
2. In the related works, the authors mentioned that there had been numerous prior works on SO(3) equivariant models, but in the experiment only PFN with simulated datasets is implemented for comparison.  The numerical evidence in this paper looks not sufficient.

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper proposes n permutation and SO(3) equivariant network for b-tagging in high energy physics. Permutation equivariance is required since the input data is a _set_ of track particle features, while SO(3) equivariance is required since these features are geometrically scalars or vectors whose rotational transformation law should be respected. The basic architecture is a particle flow network (PFN), which is based on deep sets. It consists of 1) a first SO(3)-equivariant subnetwork, applied to each track particle individually, 2) permutation invariant pooling via summation, 3) a second SO(3)-equivariant subnetwork, and 4) a final layer which extracts scalars. Internally, the network operates on scalars, vectors, and Cartesian 2-tensors. The first two are irreps of order $0$ and $1$, respectively, the third one is reducible (it would in principle decompose according to $1\otimes 1 \cong 0\oplus 1\oplus 2$).

The equivariant subnetworks employ a range of different SO(3)-equivariant mappings:
- Affine layers, i.e. linear maps followed by bias summation. The linear maps are made equivariant by broadcasting weights over the full representation dimension. Biases are only summed to scalars and 2-tensors since equivariant bias summation is impossible for vectors.
- Linear layers that are applied in local frames that are aligned along each individual jet's momentum axis $j$. This alignment is only specified up to rotations around the momentum axis, which is addressed by making the layers $\mathrm{SO}(2)\_j$ _gauge_-equivariant (the subscript labels the axis along which the subgroup is taken). Technically, the network seems to operate on restricted representations $\mathrm{Res}^{\mathrm{SO}(3)}_{\mathrm{SO}(2)} \rho$. Since the momentum axes $j$ are moving along with SO(3) rotations, the operation is as a whole still SO(3)-equivariant. Two explicit constructions of such SO(2) equivariant maps for vectors and 2-tensors are proposed. Due to the restricted equivariance requirement, these maps are less constrained than fully SO(3)-equivariant layers.
- Nine different _bilinear_ maps which map pairs of features of different types again to scalars, vectors and 2-tensors. This step allows (in contrast to the others) to transition between different representations.
- SO(3)-equivariant nonlinearities. For scalars, the authors use conventional ReLUs, while the nonlinearities for vectors and 2-tensors are acting on their norm (a common approach).

There is a single experiment, in which the models are trained as binary classifiers for b-tagging. The full equivariant model improves significantly upon a non-equivariant baseline and a version which ablates 2-tensor features.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and generally easy to follow. While I am not familiar with the b-tagging task and competing approaches, the empirical improvements over the baseline model seem very significant. Another strength is the use of bilinear mappings - most equivariant networks utilize only linear maps.

I really liked the idea of using locally $SO(2)\_j$ gauge equivariant operations besides fully SO(3) equivariant layers. The authors identified the additional geometric structure given by the momentum axes $j$ and addressed it appropriately.

Weaknesses:
My main concern with the approach is that the linear layers are not shown to be _complete_: in principle one could derive a basis of the most general equivariant linear maps (intertwiners) between the given representations. The authors show only the sufficiency of their layers regarding equivariance, but not the necessity. Indeed, I believe that quite some maps are in fact not the most general ones
(more details listed below).

To address this issue it is most convenient to work in the basis of irreducible representations, which the authors consciously avoid. For the following comments, note that scalars and vectors are irreps of order $0$ and $1$, while Cartesian 2-tensors are an irrep tensor product which decomposes according to $1\otimes 1 \cong 0\oplus 1\oplus 2$. Furthermore, intertwiners exist by Schur's lemma only between irreps of the same order, and are for SO(3) scaled identity matrices $\lambda\mathbb{I}$. For all non-trivial SO(2) irreps over $\mathbb{R}$ the spaces of irreps are 2-dimensional and are spanned by the irrep-endomorphisms ((1,0),(0,1)) and ((0,-1),(1,0)).

- The intertwiners for scalars and vectors (presented as ""broadcasting"") are complete since these are irreps. However, the broadcasting for $1\otimes 1$ tensors is overly restrictive, and there are actually three parameters, one for each irrep in the 2-tensor. The claim that the intertwiner space for 2-tensors is one-dimensional is repeated in line 172.
- The solutions for biases are indeed complete: biases can only be summed to trivial irreps, which exist with multiplicity 1 in scalars and 2-tensors, but not in vectors.
- It is furthermore possible to have intertwiners between scalars or vectors and 2-tensors, since the latter contain irrep orders 0 and 1 as subspaces (trace and antisymmetric part). These solutions are not used.
- The SO(2)-intertwiners between SO(3)-vectors in 3.1.1 seem complete, however, this is not proven but just claimed (""The set of all SO(2)-equivariant maps is _exactly_ ... [proposed parametrization]""). One can easily prove the completeness by observing that order 1 SO(3)-irreps decompose under restriction into the direct sum of an order zero and order 1 SO(2)-irrep, whose intertwiner spaces are 1 and 2-dimensional, respectively. The proposed parametrization has the same dimensionality.
- The SO(2)-intertwiners between Cartesian SO(3) 2-tensors are again not proven to be complete (""[equivariance] is satisfied when ... [proposed parametrization]"" just claims sufficiency). Going to the irrep basis shows again that there are more possible solutions.
- In the case of bilinear maps, there are again some missing operations, e.g. combinations of two 2-tensors that result in a vector. All possible solutions follow directly from Clebsch-Gordan decompositions, which are well known for SO(3).
- An alternative approach to TReLU would be to apply three independent nonlinearities to the irreps contained in the 2-tensor. The equivariance of TReLU is not explicitly shown (this might be quite trivial to show).

As mentioned above, addressing these concerns would be easiest by switching to the irrep basis. As this would require a major revision, I am not sure whether this is the right way forward for this submission, or should rather be addressed in follow-up work. An alternative way to address these concerns would be to explicitly discuss completeness of intertwiner bases in general and prove it for each operation in which it holds. The equivariance of operations like TReLU or Eq. (4) should also be proven.

Another issue is that it is not well explained how the overall network remains SO(3)-equivariant despite intermediate operations only being SO(2)-equivariant. This is one of the most interesting aspects of the paper and deserves more attention.

It should also be discussed how this relates to other _gauge equivariant_ / _coordinate independent_ networks. The alignment of frames along the $j$-axis with remaining SO(2) ambiguity seems very similar to the SO(2)-structure (SO(2)-bundle of frames) considered by (Weiler et al. 2021), specifically their figure 53 (right).

The transition from SO(3) to SO(2) features is currently not sufficiently explained. I believe that the authors assume the restriction function $\mathrm{Res}^{\mathrm{SO}(3)}\_{\mathrm{SO}(2)}$ - please clarify this!

The group actions and the domains and codomains of A and L in sections 3.1.1 and 3.1.2 are nowhere defined. One can read them off between the lines, but they should be stated more clearly.

I am somewhat worried about the extent of the experiments. It would be nice to have a more thorough empirical analysis, for instance giving more ablations, showing training curves, investigating whether the equivariance is exact or due to numerical errors only approximate. Not all ablations discussed in the text of section 5.2 are shown in the table.

Finally, I wondered about the input features of the baseline, are they the same as for vector and tensor PFN? There should really be two baselines, with either set of features, to clarify that the improvement is really coming from the architectural changes instead of the input alone.
How exactly are the different models made comparable? Do they have the same number of parameters or the same computational cost to ensure a fair comparison?



Limitations:
The main limitation is in my opinion that the solutions are incomplete - which should be addressed more clearly. Other limitations, like the limitations of the bilinear ops due to not using Clebsch-Gordan decompositions or the lack of investigating the space of tensor nonlinearities are  adequately addressed.

Negative societal impacts are not to be expected.

Rating:
6

Confidence:
5

REVIEW 
Summary:
Before starting, I must mention that I am not a physicist, and therefore, I have focused on the machine learning aspects of the paper.

### Summary

This paper proposes the use of SO(3) equivariant neural networks for B-tagging. The method proposed builds upon Deep Sets and provides a quite constrained formulation of SO(3) equivariance. The paper further indicates that breaking SO(3) equivariance locally can be used to get better representations for the task at hand, while maintaining global equivariance to that group.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is presented very clearly, well-structured, and it is in general easy to follow and understand.

Weaknesses:
### Concerns
* My main concern is that, to the best of my knowledge, it is not possible to obtain a certain equivariance without all networks being equivariant to that group. In particular, I do not understand how global SO(3) equivariance can be locally broken into SO(2)_j equivariance while preserving global SO(3) equivariance. In the best of my understanding, as shown in roughly all the papers on group equivariance, in order to have a network be equivariant to a certain transformation, all layers need to respect that equivariance. I think that clarifying how this is possible is *crucial* for the paper. It is important to note that the paper simply states this and does not provide proofs or analyses regarding this statement. 
* I am very concerned with regard to the expressivity of the proposed algorithm. For instance, it has been shown in several works that equivariance can be obtained in expressive ways that do not have such hard restrictions as having biases be equal to zero –note that several similar very constraining restrictions are also defined for each of the mappings in the networks–, e.g., E(n)-equivariant Steerable CNNs, among many others. From what I understand, this work builds mostly upon the Deep Sets literature. However, this is by far not the most general way to obtain equivariance to a certain symmetry. I believe that the authors should at least state this clearly in the paper.
* The paper performs multiple ablations that are not found in the submission other than by conclusive statements. For instance, in line 322, the authors state: “Finally, we note that neither family of models performs even as well as the baseline, when no bilinear operations are allowed”. I believe that clearly showing the results of these ablations will strengthen the contributions of the paper. In general, such conclusive statements in their own are often vague and not very informative.
* In the final part of the conclusion it is stated that “it should be also possible to use these models for creating equivariant Graph and attention based networks”. Given that these families of networks –as stated earlier– are much more general than Deep Sets for equivariant formulations, I believe that this statement is not really easy to accomplish in practice. 


Limitations:
See previous responses.

### Conclusion
In conclusion, I believe that the paper needs some work before I am able to support acceptance. I believe that there are several factors that need to be clarified, .e.g, how to get SO(3) equivariance with only SO(2)_j layers, for this to be an strong submission. 

Rating:
4

Confidence:
4

";0
r44obB8FY5;"REVIEW 
Summary:
This paper focuses on applying strided and transposed convolutions to point cloud data so that the deterministic network can be directly operated on points. To achieve this, a strided convolutional layer with auxiliary loss is proposed, which ensures a consistent selection of points across the whole learning process. Further, a lightweight autoencoder network is built upon the proposed convolutional operator. Experiments are conducted on KIMO3-6 for point cloud reconstruction and ModelNet40 for shape classification.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1.	The proposed idea and method of performing strided and transposed convolutions on point clouds are interesting.
2.	The related work provides an exhaustive discussion of existing point cloud CNNs, which is meaningful.
3.	Fig. 3 intuitively illustrates the point selection during the learning procedure.
4.	Code implementations are provided in the supplementary material.

Weaknesses:
**Main weaknesses**:  
1.	Poor writing quality:   
A.	The whole paper is presented with very long text without clear paragraph/subsection division, which significantly hurts the reading of the paper.    
B.	Line 228-246 state the network design for applying the proposed convolution operator, but no figure illustration is provided, making it hard to fully understand how to apply this on Point-M2AE.   
2.	This paper is highly related to SparseConvNet [1] which also uses strided convolution on point clouds. However, it is not included, discussed, or compared. This is very important to evaluate the novelty of the proposed method.      
3.	Use/comparison of Point-M2AE:      
A.	It is not clear why the proposed method is applied on Point-M2AE. Is the method only suitable for auto-encoder-style networks?   
B.	In Point-M2AE, in addition to SVM evaluation on ModelNet40, the general/few-shot classification on ModelNet40 and ScanObjectNN, part segmentation on ShapeNetPart, and 3D object detection on ScanNetV2. To fully verify the effectiveness of the proposed method, more experiments should be conducted under some of the benchmark settings as in Point-M2AE.    
4.	The reported results are not promising compared with SOTA point CNNs, as shown in Table 2, although fewer #Params are introduced. As a result, this work does not clearly prove the potential of using the proposed 2D-like strided and transposed convolutions, instead of the existing customized 3D point cloud convolutions.   
5.	Line 298 states the #params to show the lightweight property of the proposed method. However, the FLOPs and latency are also important to evaluate the efficiency, thus more comparisons of FLOPs and latency should be provided.      
6.	The model's robustness to permutation, translation, rotation, scaling, and noise is not tested through experiments, which is important for real-world applications.   

**Refs**:     
[1] 3D Semantic Segmentation with Submanifold Sparse Convolutional Networks, CVPR 2018.     
[2] A closer look at local aggregation operators in point cloud analysis. ECCV, 2020.    

**Additional comment**:    
PosPool [2] is another representative point convolution method, which is also directly applied to ResNet architecture. It should be included, discussed and compared.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a learning-based point sampling strategy to deterministically downsample point clouds, which can be used to build a U-shaped network for point cloud reconstruction and representation. To enforce a stable and meaningful sampling (or selection), an auxiliary selection loss is proposed. The auxiliary selection loss enables a network to select central points which are likely to be non-neighboring each other. With the sampling strategy, this paper finally proposes a deterministic strided and transposed convolution for point clouds. The proposed method is evaluated in point cloud reconstruction and representation learning (especially SVM classification on the representation) tasks. In the point cloud reconstruction task, the proposed method shows a lower chamfer distance than the previous methods. The ModelNet 40 experiments show that the proposed method can be integrated with various network configurations.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. [Originality] The proposed sampling strategy and its application to the strided convolutions are interesting. Since many previous point cloud networks utilize farthest point sampling as described in the paper (L62-83), the proposed sampling strategy could be a new alternative and bring robustness to those networks.
2. [Clarity] This paper provides detailed explanations of the auxiliary selection loss with a theorem, its proof, and an example. Especially the example (Table 1) explicitly shows how the attention map matrix (M) is constructed and how the selected points are non-neighboring each other.

Weaknesses:
1. [Originality] An highly relevant reference, SampleNet [1], is missing in both related work and experiment sections. Since both SampleNet [1] and this paper propose a learning-based point sampling, I recommend the authors explain how the proposed method differs from the SampleNet and evaluation results on the same experiments SampleNet did; supervised classification on ModelNet40.
2. [Quality] The writing quality and layout of the paper should be improved. For example, the proof of Theorem 3.1 and its example (Table 1) can be moved to the Appendix, although they may help readers to understand what the auxiliary selection loss is. Instead of the proof and example, detailed experiment results (e.g., downstream task results) with the proposed method would be better to be added.
3. [Significance] The current setup of experiments is not enough to show the significance of the proposed sampling strategy. Since various networks with farthest point sampling can use the proposed sampling strategy as an alternative, the downstream task results, which those networks did, should be added. For example, Point Transformers [2, 3] with the proposed sampling strategy can be evaluated in 3D semantic segmentation task on S3DIS or shape classification on ModelNet40. I recommend the authors evaluate the proposed sampling strategy with SOTA networks [2, 3] on downstream tasks (e.g., shape classification, semantic segmentation, and registration).

[1] Lang et al., “SampleNet: Differentiable Point Cloud Sampling”, CVPR, 2020.\
[2] Zhao et al., “Point Transformer”, ICCV, 2021\
[3] Wu et al., “Point Transformer V2: Grouped Vector Attention and Partition-based Pooling”, NeurIPS, 2022.

Limitations:
The authors partially addressed their work's limitations in the experiment section but did not address the potential negative societal impact. However, I don't think that there is a particular negative societal impact of this work.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents a learnable and deterministic point selection layer to uniformly downsample points and a point transposed convolution layer to upsample points. 

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
1. The auxiliary loss (Eqn. 1) proposed to supervise the point selection is interesting. 

Weaknesses:
1. Deficient theoretical soundness. The proposed downsampling layer attempts to learn a point importance score and selects the points with the highest scores. Given that the selection operation is non-differentiable, your importance prediction network is solely supervised by the auxiliary loss (which functions as a uniform sampling regularization). As a result, it appears to be optimized to output uniformly sampled points rather than points based on semantic importance. However, the authors seem to disagree and claim that their proposed subsampling layer is capable of learning how to select points based on semantic importance (L330). Please elucidate how this non-differentiable operation can learn a importance-based sampling. It is worth noting that previous work leverages Gumbel-Softmax to enable a soft learnable selection, which results in a differentiable point subsampling network (L105).

2. Implementation and trade-off details of the learnable subsampling? Is it implemented using a single linear layer as depicted in Figure 1? How efficient it is? How many additional parameters are required? How practical is it to scale this up to a large-scale point cloud?

3. Missing large-scale experiments. What is the performance of the classical PointNet++ or the latest PointNeXt with the proposed subsampling in a large-scale dataset like ScanNet?

4. Visualizations appear to be missing. It would be insightful to see how the selected points differ from FPS.

5. No improvement over FPS (Table 2). 

Limitations:
In which specific applications is deterministic downsampling critical? Based on the results presented in Table 2, I observed no performance improvement, but rather a decline, when using the proposed downsampling method compared to FPS. 

For many applications, the determinism isn't a concern as the network is resilient to minor variances in subsampled points. This is particularly true in the case of large-scale point clouds, where the variance in subsampled points is typically small. Could you clarify the necessity and advantages of your deterministic downsampling method in this context?

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper aims to propose a new type of convolutional neural network to point cloud understanding tasks. Besides, the authors present a new loss function to the sampling process of feature extraction for point clouds. And the authors provide theoretical analysis to prove that their method is better for sampling points. The authors conduct experiments on the reconstruction and shape classification tasks. The experimental results on shape classification are not satisfactory.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
- Theoretical analysis for the proposed algorithm.
- Better performance on the reconstruction tasks.

Weaknesses:
- Poor writing. 
- Unclear motivation.
- Unsatisfactory experimental results for point cloud understanding.
- Inaccurate statements.

Limitations:
Lack of limitation discussion.

Rating:
4

Confidence:
4

";0
UvIN8oQ4uI;"REVIEW 
Summary:
This paper proposes a Directional Stimulus Prompting method to provide fine-grained guidance for the output of large language models (LLMs). This method introduces a small trainable policy model (e.g. FLAN-T5) to generate hints for each query to guide LLMs towards desired outputs. This policy model can be trained via a standard paradigm including supervised fine-tuning on pseudo-labeled data and reinforcement learning (RL) to optimize the expected reward. Experiments on summarization and dialogue generation tasks show the effectiveness of the proposed method.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. This paper proposes a direct and feasible solution to guide black-box LLMs towards better generation results, which is widely applicable with the rapid development of LLMs.
2. This paper is well-written and easy to follow.


Weaknesses:
1. The technical novelty of the proposed method is limited. The training of the policy model in this paper is a standard paradigm including supervised fine-tuning and reinforcement learning. The techniques used in RL including dynamic adjustment of the coefficient and NLPO are all borrowed from existing works. Thus, I feel that the main difference falls into the usage of LLMs as an evaluation function in the reward model, which is devised for adapting this RL algorithm to the scenario of guiding LLM. Thus, the novelty of this design is mainly on the applicational side.

2. During supervised fine-tuning, the authors heuristically select the “pseudo-stimulus” for each input. The pseudo-stimulus indicates keywords / dialogue acts for summarization / task-oriented dialogue generation, respectively. I wonder whether there is a general principle to select pseudo-stimulus especially in open-ended text generation tasks, since recently proposed LLMs such as ChatGPT are mainly applied to these tasks.

3. The benchmark datasets in this paper only contain CNN/DM for summarization and MultiWOZ for task-oriented dialogue generation, which are not convincing enough to evaluate the performance of the proposed method based on LLMs. The authors should conduct experiments on broader tasks and datasets. Also, I’m curious about the motivation to choose MultiWOZ. In Section 3.2, the authors say that LLM-based chatbots such as ChatGPT face challenges in handling task-oriented dialogue generation because this task requires the chatbot to respond based on reliable information from API calls or database queries. But the proposed method still cannot interact with APIs or databases. The experimental setting of MultiWOZ seems like end-to-end response generation (i.e. generating the system response given the dialogue history), which is similar to open-domain conversations.

4. From [1], automatic evaluation metrics such as ROUGE cannot reliably evaluate the quality of LLM-generated summaries. Thus, human evaluation should be added to strengthen the experimental results.

[1] News Summarization and Evaluation in the Era of GPT-3.

Limitations:
The authors should discuss more on the risk of guiding LLMs to generate unethical contents and how to avoid it via the policy model.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach to effectively guide black-box LLMs towards generating desired outputs. The proposed approach involves utilizing a relatively small policy model to generate ""directional stimulus,"" which serves as specific information to assist LLMs in performing tasks such as summarization and task-oriented dialogue. The policy model is trained using rewards obtained from evaluating LLM outputs. Notably, this approach requires only a small amount of training data and eliminates the need to train the entire model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The writing is clear, making it easy to grasp the motivation and contribution of the paper.
- The proposed approach is simple, which suggests its potential for widespread adoption across various domains.
- The experimental results effectively demonstrate the effectiveness of the approach. Notably, in Figure 3, the model leveraging DSP achieves superior performance in terms of BLEU, METEOR, and BERTScore, despite Rouge being used as the reward metric.
- The fact that the authors have released their code is highly valuable, as it facilitates future research and allows for the replication of their findings.

Weaknesses:
- I have a concern regarding the generalizability of the method, as the experiments primarily rely on a single prompt per task.
  - It is widely acknowledged that LLMs heavily rely on the structure and format of the prompt, which raises the question of whether this technique would be effective with different prompt formats.
  - This approach would hold even greater value if it could consistently yield improvements regardless of the choice of prompt format, demonstrating its robustness.
- Additional analysis about generated clues will be helpful to get lessons from this work. For example,
  - How does the result change if the number of clues is increased/reduced? 
  - What kind of clues are generated considering the semantics/contents of the input (visualization will be helpful)
- The method is not intuitive to me, because all the information that is required to perform the task is already contained in the input. Does the result mean that LLMs are not sufficient to extract information from naive and complex inputs? It will be helpful if the authors provide thoughts about this phenomenon.

Limitations:
The authors' discussion of the limitations in the paper is not sufficient. Addressing the suggested weaknesses and questions in reviews would significantly enhance the insights provided by this paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, a prompting framework called Directional Stimulus Prompting (DSP) is proposed which provides a more fine-grained guidance and control over LLMs by adding directional stimulus into the prompt. These directional stimulus or hints are generated by a small tunable model which is fine-tuned using supervised learning and reinforcement learning. The performance of the model is tested on summarization and dialogue response generation tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1: It is interesting that they fine-tuned and used a small language model to improve the performance of larger language models such as ChatGPT. This approach cleverly bypasses the constraint of being unable to fine-tune large language models.

S2: In both experiments, they demonstrated the effectiveness of their approach by showing the improvement achieved through their framework.

S3: The paper is well written and has a good flow.

S4: A good number of related works are covered in the Related work section.



Weaknesses:
In their experiments, they only show the results for the flan-T5-large model as a model for generating directional stimulus. They could have also included the performance of other fine-tuned models in their experiments.

Limitations:
One of the good points that they motioned is that their framework could be used to guide LLMs to generate harmful or biased contents.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a interesting module named 'Directional Stimulus Prompting' (DSP). This innovative module functions by generating cues or hints to aid black-box Large Language Models (LLMs) in response generation. For instance, in a summarization task, providing keywords can guide the LLM towards generating more accurate and relevant answers.

The uniqueness of this module lies in its ability to be fine-tuned through both supervised learning and reinforcement learning techniques. In reinforcement learning, automatic metrics (reward) are applied to responses generated from black-box LLMs, which have been influenced by the DSP.

The results from experiments confirm that both the supervised fine-tuning and reinforcement learning approaches to DSP can significantly enhance the performance of black-box LLMs. This finding underscores the potential of DSP as a powerful tool for improving LLM response generation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper introduces a compact yet powerful module known as 'Directional Stimulus Prompting' (DSP). Its function is to generate hints that enhance queries posed to black-box Large Language Models (LLMs).

Two distinct approaches for fine-tuning this DSP module are presented by the authors: supervised fine-tuning and reinforcement learning.

The effectiveness of both supervised fine-tuning DSP (SFT DSP) and reinforcement learning DSP (RL DSP) is confirmed through experimental results, underscoring the utility of this novel module in enhancing LLM query performance.

Weaknesses:
Currently, automatic metrics are employed for evaluation, as well as for the reinforcement learning (RL) tuning of the Directional Stimulus Prompting (DSP) module. However, presenting these metrics exclusively in the final analysis could restrict the comprehensive assessment of performance. It may be more insightful to incorporate other evaluation metrics into the analysis. This could include human evaluation, entity word extraction, and GPT-based evaluations. Such a diversified metrics approach could potentially provide a more holistic view of the performance improvements achieved through the use of the DSP module.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces Directional Stimulus Prompting (DSP), a new prompting framework that introduces directional stimulus into the prompt, which could provide black-box LLMs with fine-grained and query-specific guidance toward the desired outputs.

The experiments on summarization and dialogue response generation tasks demonstrate the effectiveness of this approach. Notably, on the MultWOZ dataset, our framework enables ChatGPT to achieve a remarkable 41.4% improvement in its combined score with only 80 dialogues.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.It is quite novel that the paper proposed DSP, a prompting framework for guiding black-box LLMs toward desired outputs, which combined Supervised fine-tuning and Reinforcement learning to further optimize model.

2.The experiment setting is quite detailed, since this paper used varying numbers of training samples from the datasets and evaluation metrics for ease of display and comparison.

Weaknesses:
1.There is too little dataset in the experiment section. Both tasks were conducted on a single dataset and cannot fully demonstrate the effectiveness of the framework.

2.On Page 5 Line 151, the experiment section of the paper is not enough to prove this conclusion. It is not rigorous to say that the framework can be flexibly applied to various types of LMs and generation tasks, just by conducting experiments on two tasks.

Limitations:
None

Rating:
6

Confidence:
2

";1
v30HbVOxJR;"REVIEW 
Summary:
The paper first characterizes the 'zero-gradient' issue---a challenge associated with learning a model in the 'predict-then-optimize' paradigm---in terms of the number of active KKT constraints of the optimization problem. It then proposes a surrogate optimization problem for which the zero-gradient does not arise and evaluates these surrogates on 2 domains.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
* The paper addresses an important problem, i.e., zero-gradients in predict-then-optimize.
* The paper proposes a novel surrogate.

Weaknesses:
1. **The 'zero-gradient theorem' is not novel:** The paper claims to 'discover and explain the zero-gradient problem in P&O for convex optimization'. However, a number of predict-then-optimize papers acknowledge the zero-gradient issue and propose their own surrogates, e.g., Elmachtoub and Grigas [2017] and Wilder et al. [2019]. In fact, for well-defined LPs (with unique solutions), it is known that the task performance/SPO loss is piecewise constant. In contrast, it is known that if the function and constraints are strongly convex, e.g., the portfolio optimization problem in the experiments with $\lambda > 0$, there are no zero-gradients. As a result, it's not clear why this is the domain that the paper chooses to run experiments on...
1. **The experiments have no baselines:** While the paper does provide a surrogate that does not run into a zero-gradient issue, the bar for publication is typically higher, i.e., that this specific surrogate outperforms others from the literature. As the related work section notes, there are other ways to get around the zero-gradient issue, like creating surrogate problems by adding quadratic/exponential regularization terms, however there are no comparisons to any methods not presented in the paper, not even simple baselines like 2-stage, random and optimal.
1. **The paper is poorly written:** There is almost no information in the about the contributions in the abstract and introduction and, as noted above, the paper does not adequately engage with past work.

### Update (20 Aug 2023)

After the discussion with the authors, reading the other reviews and thinking about this paper more, I find myself still recommending rejection. Here are the reasons:
1. The *submitted* version of the paper lacks any discussion of or comparison to related work, and also overclaims (e.g., ""the first to discover the zero-gradient theorem""). While the authors have refined their position significantly in the rebuttals (and much improved their contribution as a result), I believe that (a) there still remain important unanswered questions, and (b) these changes lead to a paper with significantly different claims. I discuss both of these in terms of specific contributions below.
1. **Zero-gradient Theorem:** In the responses, the authors acknowledge that the zero-gradient issue has been known for optimization problems with linear objectives. The modified claim, as I understand it, is that the characterization of zero-gradients in this paper is significantly different/improved from the existing understanding. However, (a) the proof is not particularly novel (imo) because it formalizes existing knowledge in the language of KKT conditions, and moreover (b) it is not clear whether this characterization is significantly more powerful than the existing understanding (specifically, do gradients lie in the null space of the normals of the active constraints when the Jacobian matrix isn't zero, and what does this mean intuitively?). I believe that the strength of this contribution is dependent on the answers to these two (imo) unanswered questions.
1. **r-Smoothing Surrogate:** While avoiding zero-gradient issues is definitely a desirable property for a surrogate, it is by no means a *sufficient* for good performance - a number of papers in the literature do not run into zero-gradient issues. As a result, r-smoothing needs to be better motivated and compared to past work. While the authors have done an admirable job of running experiments in the author response period, there are some papers that do very similar things that the authors have not compared to, e.g., Sahoo et. al. (2022). I think there is work that remains to be done in situating their surrogate in the context of the large body of recent work on PtO.

Limitations:
There are no limitations discussed; in contrast, I believe this paper over-claims its contributions.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper identifies the zero-gradient problem in Predict and Optimize (P&O) for convex optimization and proposes a method to address it. The method is based on using a Quadratic Programming (QP) approximation for computing decisions, smoothing the feasibility region around the current solution to reduce the dimensionality of the null space to one, and adding a projection distance regularization term. The proposed method demonstrates significant improvements for convex P&O problems with many constraints and with the true optimum lying on the boundary of the feasibility set.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality: The paper identifies a previously unnoticed problem in convex optimization and proposes a novel method to solve it.

Quality: The proposed method is technically sound, and the experiments demonstrate its effectiveness in addressing the zero-gradient problem.

Clarity: The paper is well-written and clearly explains the concepts and methodology.

Significance: The proposed method has the potential to improve optimization in convex P&O problems, which are common in various domains.

Weaknesses:
Insufficient experiments: The paper might lack a comprehensive set of experiments or fail to compare the proposed method with alternative approaches. This could make it difficult for readers to evaluate the true effectiveness and novelty of the proposed method.

Limitations:
See Weaknesses

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies predict and optimize problem which utilizes machine learning to predict unknown parameters of optimization problems. The paper identifies the zero-gradient problem and proposes a method to solve this issue. Additionally, the paper conducts an experimental study to verify the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is technically sound. The claims regarding the zero-gradient problem in the paper are well-supported by theoretical analysis. The assumptions are clearly presented, and proof ideas are discussed after each theorem. The efficiency of the proposed solution to the zero-gradient problem is verified by the experimental results.
2. The paper is well-organized. It begins by introducing the problem formulation of predict and optimize and discusses the typical methods used to solve the problem. Then, the paper introduces the zero-gradient problem along with the theoretical analysis. Finally, the proposed solution and experimental results are presented.

Weaknesses:
The paper can be improved by also experimentally demonstrating the yet noticed zero-gradient problem claimed in his paper.  Demonstrating the the consistency between the theoretical findings and experimental observations can enhance the significance of this paper.

Limitations:
NA

Rating:
6

Confidence:
1

REVIEW 
Summary:
Predict+Optimize (P+O) is an emerging paradigm that lies in the intersection of classical optimization  and machine learning. Specifically, it considers the setting where a parameterized optimization problem:

$$x^{\star}(w) = \operatorname*{argmin}_{x} f(x,w) \text{ subject to } x \in \mathcal{C}$$

must be solved yet the parameters $w$ are unknown. Given observational data $o$ that is correlated with $w$, a natural approach is to train a machine learning model $\hat{w} = \phi_{\theta}(0)$ so that $\hat{w} \approx w$. Then at test time, we solve

$$ x^{\star}(\hat{w}) = \operatorname*{argmin}_{x} f(x,\hat{w}) \text{ subject to } x \in \mathcal{C} $$

The secret sauce to P+O is, instead of training to minimize prediction error $\|\hat{w} - w\|^2$, to use a loss function aligned with the actual goal {\em i.e.} that encourages $ x^{\star}(\hat{w}) \approx  x^{\star}(w)$. There are several ways to do this, but one is to simply reuse the objective function and train $\phi_{\theta}$ so as to minimize

$$ \mathbb{E}_{(o,w)}\left[f(x^{\star}(\hat{w}, w)\right]  $$

where $\hat{w} = \phi_{\theta}(o)$. Although a formula for the derivative of this loss is well-known, the core claim of this paper is that this derivative is less informative than previously thought. In fact, it is frequently zero. This idea is formalized through a theorem. The authors then propose a way to overcome this aplty named ""zero-gradient"" problem. Finally, the paper is rounded out by numerical experiments on two datasets.


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The main strength of this paper is Lemma 3.4 and Theorem 3.5, which crystalize the core claim of this paper. This result is surprising, but I checked the proof to the best of my ability and I believe it is correct. This result is an important reality check for the field of Predict-and-Optimize.
- I enjoyed reading the proofs. The result of [1] were new to me. I liked the way the strict complementary slackness is used in the proof of Lemma 3.4
- Adding to the above, the authors do a good job of making their core results accessible through intuitive explanations and diagrams.

[1] Anthony V Fiacco. _Sensitivity analysis for nonlinear programming using penalty methods_ (1976).

Weaknesses:
- Reusing the parameterized objective function $f(\cdot,w)$ as the loss function for training is not the only way to do P+O. One could also use the SPO+ loss [1], a perturbation based approach [2], or the least squares loss $\|x^{\star}(w) - x^{\star}(\hat{w})\|^2$ [3]. This should be mentioned as the zero-gradient theorem need not apply in these settings.
- I am perplexed at the stated motivation behind the quadratic programming approximation. While it is true that $f_{QP}(x,\hat{w}) = \|x - \hat{w}\|^2$ is strongly concave, and so on, it need not bear any relation to the actual problem we wish to solve, namely $f(x,w)$. So, this seems to run counter the spirit of P+O. The only case that makes sense to me is when $f(x,w) = w^{\top}x$. Expanding out we get:

$$ f_{QP}(x,\hat{w}) = \|x - \hat{w}\|^2 = -2\hat{w}^{\top}x + \|x\|^2 + \|\hat{w}\|^2 $$

So ignoring the irrelevant $\|\hat{w}\|^2$ term,  it appears the authors are simply proposing to add a quadratic regularizer, which has been explored thoroughly in the literature (see [4] and elsewhere).  Could the authors comment on this?
- I find the motivation behind $r$-smoothing a little opaque too. It seems as though the solution to the $r$-smoothed problem $P_{r}(\hat{x}\hat{w})$ might not be feasible (i.e. might not lie in $\mathcal{C}$). Is this correct?
- Using the Jacobian $\nabla_{\hat{w}} x_r^{\star}(\hat{w})$ in place of $\nabla_{\hat{w}}x^{\star}(\hat{w})$ is, as you show, essentially the same as just replacing $\nabla_{\hat{w}}x^{\star}(\hat{w})$ with the identity (independent of what $r$ is). This procedure is already well-studied, see [3, 5--7]. These papers should be cited and discussed. 

*Minor Stuff:*
- In Figure 2, the smoothed feasibility region $\mathcal{C}_r(\hat{x}\hat{w})$ is the disk (i.e. the interior of the circle) right? If yes, this should be made clear in the caption and figure. Right now it looks as though the feasible region is just the boundary.
 - In Definition 3.7, as the scale of $r$ doesn't really matter, I'd recommend not normalizing and simply writing $c = \hat{x} - r\nabla_xf(\hat{x},\hat{w})$.
 - The experiments in Section 4 feel like ablation studies (i.e. just removing one element at a time from your proposed approach). I would like to see some benchmarking results, e.g. comparing the performance of your proposed algorithm to existing P+O approaches. You may find the benchmarking software PyEOPO useful for this [8]

[1] Adam N Elmachtoub and Paul Grigas. _Smart predict, then optimize_ (2017)

[2] Quentin Berthet et al. _Learning with differentiable perturbed optimizers_ (2020)

[3] Daniel McKenzie et al _Faster predict-and-optimize with three-operator splitting_ (2023)

[4] Bryan Wilder et al _Melding the Data-Decisions pipeline: Decision Focused learning for combinatorial optimization_ (2019)

[5] Samy Wu Fung et al _JFB: Jacobian-Free Backpropagation for Implicit Networks_ (2022)

[6] Zhengyang Geng et al _Is attention better than matrix decomposition?_ (2022)

[7] SS Sahoo et al _Backpropagation through combinatorial algorithms: Identity with projection works_ (2022)

[8] Tang and Khalil _PyEPO: A PyTorch-based end-to-end predict-then-optimize library for linear and integer programming_ (2023).

Limitations:
N/A.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper focus on the topic of ""predict and optimize"" and identify the zero-gradient problem. This issue is characterized by a situation where the gradient related to the best decision concerning parameters in machine learning models might be zero. This can occur even when assuming convexity, smoothness, and strict complementary slackness. The authors introduce a QP approximation and an r-smooth technique to address this problem, effectively reducing the likelihood of encountering the zero-gradient issue. The numerical results show the merits of their proposed approach.  

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. Section 3.1 uses straightforward theoretical outcomes to shed light on a significant practical problem. This section is clear and insightful.
2. Section 3.3 stands out for its clarity and insight as well, particularly Theorem 3.9. Initially, I was skeptical that the gradient with local smoothness would accurately approximate the original gradient, but Theorem 3.9 effectively argued this point. Although the locally smoothed gradient may not ensure the fastest improvement in function value as the original gradient does, it can be guaranteed to at least be a non-decreasing direction.
3. Figures (b) and (c) look interesting, showing that the new methods outperform standard approaches during the final training phase. This aligns with the theories in Section 3.1, where, towards the end of training, the conditions described in Theorem 3.5 become more likely. This leads to training difficulties with the standard method, whereas the proposed techniques can overcome them. Further validation on this matter is needed, as indicated in the third point under ""Weaknesses.""

Weaknesses:
1. Sec 3.4, Algorithm 1. The notation seems unclear. What's the dimension of $f_x$ and $\hat{f_x}$? Why $f_x$ can be directly multiplied with $\nabla_{\hat{w}}x^*_r(\hat{w})$? Does it mean inner product of two vectors? What's the meaning of $f_x - f^0$ given $f^0$ is a scalar while $f_x$ is a gradient (vector)?

2. Sec 4.1, equation (8). What's the meaning of $w$? Does $f(x,w)$ mean the original function defined in (7) or the QP approximation defined in (9)? This is quite critical: if (8) measures the regret based on QP approximation rather than the original function, the experiment results would be meaningless. While if (8) measures the original function, the numerical results will be good.

3. Is it possible to provide the norms of the gradients you observed in the experiments? If the gradients calculated with your proposed approaches have larger norms than the traditional calculation way, it would be a more direct and strong support of your approach. 

Limitations:
see ""Weaknesses""

Rating:
7

Confidence:
4

";0
CyuhEwvSof;"REVIEW 
Summary:
This is a theoretical paper that proposes novel ideas for neural architecture search based on neuroscientific research. The author argues for the fundamental role of metabolic contains in the development of biological neural networks, which should inform the optimization strategies used for artificial neural networks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper points out interesting and neglected constraints that should be investigated for NAS

Weaknesses:
- The authors do not present any experiments that could strengthen their hypothesis
- The paper could give a more comprehensive overview of the current state of NAS, also including early work from the evolutionary community that already did incorporate energy constraints in the form of connection costs, e.g. ""Clune, Jeff, Jean-Baptiste Mouret, and Hod Lipson. ""The evolutionary origins of modularity."" Proceedings of the Royal Society b: Biological sciences 280.1755 (2013): 20122863.""
- Some of the concept, such as glia-neural networks, could be explained in more detail


Limitations:
Yes.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to neural architecture search that optimizes for energetic efficiency and draws inspiration from the role of glial cells in biological brains.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The idea of energetic efficiency objective functions for neural architecture search may have merit.

Weaknesses:
The paper does not really make a technical contribution -- there are no experiments or theoretical results.  The paper is essentially a proposal for a high-level idea about neuroscience-inspired neural architecture search, which is not adequate for a conference like NeurIPS.  Morevoer, the survey of neuroscience literature, while potentially interesting, is not presented in a way that would be comprehensible or actionable for machine learning researchers hoping to leverage these ideas.  I encourage the author(s) to make more concrete their proposed approach and evaluate it empirically if their goal is to improve upon current NAS methods.

Limitations:
I found the discussion of limitations hard to follow and so cannot evaluate it properly.

Rating:
2

Confidence:
3

REVIEW 
Summary:
This article suggests that the concept of energy-driven network architecture selection in the brain can be applied to artificial neural architecture search (NAS). It proposes redefining the relationship between artificial neurons to include the metabolic cost of computation. The article also discusses the relationship between brain behavioral states and energy management, as well as the theory of dynamic coordination and metabolic optimization in biological systems. It suggests that the optimization of energy-driven systems can be seen as the optimal solution to the complexity of biological systems. The article concludes by proposing an updated NAS strategy that incorporates glial-neuronal ensembles and energy-constrained architecture search.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
I believe that this kind of interdisciplinary perspective should be encouraged. AI has benefited greatly from biological inspiration and insight, and I commend the authors for their effort. The article proposes that two aspects of biological brains receive higher focus in modern deep learning: the contribution of glial cells and metabolism. I do believe that it is useful to clarify the differences between the biological brain and deep learning on these aspects.

Weaknesses:
The article does not sufficiently cover contemporary deep learning literature, which does make links to biological neural networks or their properties. While this is less evident for the inclusion of glial cells, there is substantial literature on network efficiency, including in NAS. It is common practice in NAS to compare architectures not only on error on a test dataset, but also measures of efficiency like parameter count or number of floating point operations. NSGA-Net, for example, explicitly searches using two objectives: one of accuracy and one of efficiency. 

Lu, Zhichao, et al. ""Nsga-net: neural architecture search using multi-objective genetic algorithm."" Proceedings of the genetic and evolutionary computation conference. 2019.

Hardware-aware NAS methods take into account the energy limitations of different hardware, aiming to find a balance between energy efficiency and performance:

Chitty-Venkata, Krishna Teja, and Arun K. Somani. ""Neural architecture search survey: A hardware perspective."" ACM Computing Surveys 55.4 (2022): 1-36.

A common motivation to compress networks, through pruning or reducing precision, is to reduce their energy use:

Yang, Tien-Ju, Yu-Hsin Chen, and Vivienne Sze. ""Designing energy-efficient convolutional neural networks using energy-aware pruning."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.

The proposed NAS method which takes ""metabolic"" constraints into account is in the same line, however the article does not acknowledge the existence of this common consideration in the current NAS literature. The claim that ""Current AI is based on optimization strategies for information (Imax ) without metabolic constraints (Emin )"" is false if we consider measures of floating point operations or computational time on specialized hardware as equivalents to metabolic constraints. The article does not detail how metabolic constraints would be calculated, asking instead ""Is Emin for AI just the cost of electricity or also an intrinsic system’s property as exemplified by the theory of dynamic coordination?"". Given that measuring energy cost in terms of computational complexity (number of floating point operations) is standard in NAS, the proposed definition of Emin should be clarified in the article.

The article is very short for a NeurIPS submission at less than 5 full pages and could have gone much further in detail both on the proposed ideas and their context in contemporary deep learning.

Limitations:
As previously mentioned, the main limitations of this article are the scope of work considered and the lack of detail of the proposed ideas. The proposed ideas should be more fully realized and more firmly grounded in the current literature. The Limitations section on page 5 does not adequately address these limitations.

Rating:
2

Confidence:
3

";0
umHruATvCD;"REVIEW 
Summary:
The authors show the random forest induced kernel is characteristic, and they empirically study the validity of the kernel for independence and k-sample testing.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The authors show the connection between the characteristic property of the kernel and random forest. The topic is interesting and important.

Weaknesses:
The presentation should be improved. Since detailed explanations about random forest are missing, how we can connect the kernel methods to random forest is not clear. I think that is the most important background of this paper, so it should be explained more. In addition, the novelty of the paper is not clear to me since the theoretical results in this paper seem to depend strongly on the random forest setting, but the setting is not clearly explained.

Limitations:
As I also mentioned in the weakness part, since the setting and the background are not sufficiently explained, the novelty and the nontrivial part of this paper are not clear.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper show how decision forest can be used to induce a kernel for k-sample testing.

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
Better-than-SOTA results.

Weaknesses:
The paper is a extremely hard to follow, particularly Section 3, which I was unable to follow without going deep into the referenced papers. The same goes for related works, which are simply mentioned en-passant in Section 5.1. A reader not directly familiar with all of them is left wondering in what they differ, and why.
Additional comments:
- L18: ""are an ensemble"".
- L18: ""They are highly effective"".
- Given that we're dealing with matrices, I'd suggest using \mathds{1} to indicate indicator functions, rather than I, which is usually used for identity matrices.
- Figure 4 only reports 2 competitors: what about the others?


Limitations:
Limitedly comprehensible.

Rating:
4

Confidence:
1

REVIEW 
Summary:
 In this paper, the authors introduced a new method called KMERF, which employs random forest for kernel construction. Through their algorithm, they were able to establish that the kernel they created has certain properties, namely being positive definite and asymptotically characteristic. The authors also demonstrated that KMERF has better statistical power than other independent methods. This makes KMERF one of the pioneering learned kernels that has been proven to be asymptotically characteristic.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I find the proposed method to be impressive and well-organized. The paper is also well-written. It's worth noting that the number of learned kernels with characteristic properties is limited, so KMERF could be a valuable contribution to the kernel learning field. While it's not unexpected for a learned kernel to show better statistical power, the decent false discovery rate of KMERF is intriguing.

Weaknesses:
1. There is concern about potential inflation from post-selection inference since the kernel being proposed is learned. To address this, I suggest that the authors conduct additional calibration tests of KMER. For instance, they could create various simulation scenarios (similar to those presented in Fig1 and Fig2) involving a mix of causal and null features, and demonstrate the well-controlled FPR/ FDR. 

2. Further details are required in Fig1 and Fig2, such as parameter values and exact equations, to fully understand how each simulation setting was carried out.

3. It would be beneficial to have a more comprehensive analysis of the runtime.

Limitations:
KMERF has been proven to exhibit asymptotic characteristics when the sample size n and the number of trees m approach infinity. However, in practical applications, m is often limited to prevent overfitting and computational issues. Providing insights on KMERF's behavior under finite n and reasonable m would be greatly beneficial.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, random forest induced kernel/proximity is combined with distance correlation and a recently developed chi-square test method to form a hypothesis testing method that is useful for independence testing and k-sample testing. The authors prove that the kernel is asymptotically characteristic and therefore the proposed method is valid and consistent for a sufficiently large sample size. Through experimental evaluation of statistical power for independence and two-sample testing on synthetic data, the authors show that their method performs better than other tests for the majority of simulations settings, and is good at identifying important features. When applied to real biomarker data, the method successfully identifies a potentially valuable marker for pancreatic cancer.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The method is easy to implement and seems powerful. It inherits the advantages of random forest, e.g., working very well on small datasets, almost no need for parameter tuning and preprocessing, easy-to-access high-quality implementations, etc.
- The paper is well-written and easy to follow.
- Theoretical properties of the method are investigated. The results seem correct.
- The chi-square test is much more efficient than the permutation test.
- Theorem 2 seems novel.
- Experimental evaluation is well conducted and looks convincing.

Weaknesses:
- The novelty of this method is limited because the combination is straightforward. The random forest induced proximity is well-known in the literature.
- The related work on random forests for hypothesis testing is not surveyed.

Limitations:
The limitations of this work have been discussed in the Appendix.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this study, the authors proposed a new kernel KMERF for independence testing.
In KMERF, multiple decision trees are constructed similar to Random Forest, and the number of trees in which two data points belong to the same leaf node is calculated.
This count is used as the kernel value between the two points to form a kernel matrix.
The p-value is then computed using a method similar to HSIC.
The authors proved that KMERF is a characteristic kernel in the limit as the number of data points and the number of decision trees approach infinity.
This property guarantees that KMERF is an effective kernel (asymptotically) for independence testing.

Through experiments using synthetic data, the authors reported that independence testing with KMERF outperforms other kernel-based methods in terms of statistical power.
They also showed that by estimating the feature importance of the Random Forest, they can estimate the input features contributing to the independence testing.
This ability of estimating feature importance is considered an advantage of KMERF.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The strengths of this study is on the theoretical anaysis of KMERF and the experimental results.
For valid independence testing, it is required to demonstrate the (asymptotic) characteristic kernel property of KMERF.
The theoretical proof in this study is therefore essential.
Moreover, the experiments report that the testing using KMERF exhibits higher statistical power compared to other methods using different kernels.
Additionally, a unique feature of KMERF is its ability to estimate the important features contributing to the testing by examining the feature importance of the Random Forest.

Weaknesses:
A weakness of this study is the absence of a review of existing random forest-based kernels.
For example, [7] presents a kernel based on random partitions similar to KMERF.
Moreover, measuring the similarity of two data points using the number of trees with the same leaf node has been used for several data analysis tasks, and packages such as rfProximity have been publicly released.
When compared to these existing methods, Steps 1 and 2 of KMERF can be considered equivalent to rfProximity.

The absence of the aforementioned review poses a problem in properly evaluating the novelty of this study.
In fact, due to the similarity with rfProximity, the novelty of this study lies not in the random forest-based kernel itself (Steps 1 and 2), but rather in the application to testing in Steps 3--5, as well as demonstrating the asymptotic characteristic kernel property of KMERF.

Limitations:
The authors mentioned some possible future directions that are not addressed in the current study.

Rating:
6

Confidence:
4

";0
oxj8mx4Sv2;"REVIEW 
Summary:
The paper presents an algorithm to learn an auxiliary model. The goal is to enhance the out-of-distribution calibration performance while maintaining the in-distribution performance of a base discriminative model. This base model is either based on a random forest or a multi-layer perceptron. The algorithm follows three main step: Firstly, it constructs an adjacency matrix using the training samples. The entries of this matrix depend on the ""rule/activation patterns"" of the corresponding discriminative model. Secondly, it performs clustering to divide the input space into regions. Lastly, for each region, the algorithm uses a Gaussian kernel to regress the output of the original model (by learning the center and the covariance matrix). The algorithm reaches convergence to the ground truth posterior distribution in the limit of infinite training data. The experiments are conducted on two-dimensional toy data and datasets from the OpenML-CC18 Benchmark Suites, thus demonstrating the algorithm's superior performance in terms of out-of-distribution calibration compared to the base models. Furthermore, it achieves comparable accuracy and in-distribution calibration performance to the base models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

1. The algorithm to identify the partition induced by a classifier (random forest, MLP) on the input space is original and novel. Indeed, it can help to shed light on the calibration properties of existing classifiers (**Originality**)
2. Asymptotic convergence of the algorithm to the true posterior is guaranteed in the infinite sample regime (**Soundness**).
3. Overall, the paper is clear (**Clarity**). The text could be improved by providing more details about the algorithm, rather than focusing on the discussion between discriminative and generative models, which seems a bit out of scope.
4. Code is available, but I haven’t tried to reproduce the experiments (**Reproducibility**)


Weaknesses:

1. It’s unclear how the algorithm performs in the finite sample regime and in high dimensions and how these two quantities are related. The scope of the analysis is therefore limited to a regime with infinite data (**Significance**). In absence of an analysis on the number of dimensions, the authors could provide an experimental analysis with higher dimensional datasets, like CIFAR10 or CIFAR100.
2. Previous works have shown that neural networks are over-confident/wrong on OOD data [1] and have suggested a set of experiments to check the phenomenon. Providing an analysis on such experimental settings might strengthen the work (**Quality**) - See Questions.
3. The authors could relate the partitioning technique with the literature on approximation based on splines (**Quality**) - See Questions.



Limitations:
The work is limited in the analysis (input dimensionality, class of neural network models).

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper tackles the ID and OOD problem by proposing Kernel Generative Forest and Kernel Generative Network for estimating the similarity $w_{rs}$ and in turn approximate the class-conditional density.

The main contributions include: 1) theoretical results of the convergence of the approximated class-conditional density under certain conditions. 2) the similarity measure with KGF and KGN between the polytopes. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper proposed a novel idea of estimating ID and OOD by approximating the class-conditional density. For that, the similarities between polytopes of the input space when data falls into is estimated. Both theoretical and empirical results show that the proposed model can successfully tell when test data is OOD while preserving the classification accuracy. 

The paper is overall well written.

This paper has the potential of contributing to the community. 

Weaknesses:
The proposed method is highly related to [4], but in the experimental part there is no comparison to any of the recent methods such as [4] (except for the ""parent algorithms"" RF and DN). At least MMC is also reported in [4]. The readers will also benefit if a detailed discussion of similarity and difference between [4] is provided.

The background and the proposed method is well illustrated, but the demonstration of the experimental results is less clear.  e.g. 1) In fig 1 the simulated yellow points overwrite the blue points and therefore do not match the true posteriors. 2) The ""Difference"" in fig 3 is hard to understand, as well as the legends of ""wins"". What is the reason not plotting like Fig 2?



Limitations:
The paper discussed the limitations of the adopted Euclidean distance measure. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes to improve OOD detection for deep discriminative models by replacing the affine function over the polytopes with a Gaussian kernel, leading to a method called kernel generative networks.
An estimation method is developed for the proposed method and some theoretical results on asymptotic convergence to the true distribution and to OOD.
Results based on simulations show that the proposed method can estimate distribution better than the parent algorithms and have benefits in terms of OOD detection or calibration in some cases.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
### originality
The method seems to be novel.

### quality
The proposed method is intuitive and sound.
The work includes useful theoretical results for the properties of the proposed method.

### clarity
The abstract downplay the main focus of the paper or main benefit of the proposed method which is to improve OOD detection/calibration.
It makes me confused while reading it for the first time.

### significance
The proposed method is simple to understand and could potentially be a standard algorithm in popular ML libraries.

Weaknesses:
### quality
The results/experiments of the paper require more work.

While the paper says the proposed method ""results in better in- and out-of-distribution calibration"", the results in Figure 3 show a contradicting or mixed results in different cases.
As this is the main claim of the paper, it requires an in-depth discussion on why the results are mixed and when do we expect the proposed method outperforms the parent algorithm.

As improved OOD detection is part of the main claim, the paper should compare some existing unsupervised method like [1].

[1] Zhang, Mingtian, Andi Zhang, Tim Z. Xiao, Yitong Sun, and Steven McDonagh. ""Out-of-distribution detection with class ratio estimation."" arXiv preprint arXiv:2206.03955 (2022).

Limitations:
Some limitations in terms of the use of Euclidean distance are mentioned, followed by a proposal to fix it by using geodesic distance.
It feels to me that the paper should actually explore this considering the mixed results from the experiments section.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper proposes a new method for confidence calibration in discriminative deep ReLU networks and random forests based on approximating the class-conditional density with Gaussian kernels.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is conceptually simple and fairly novel, and it does not require retraining the parent learner to work.
- Math and derivation are laid out clearly.
- The limitations of the work are adequately discussed.

Weaknesses:
- **Writing has room for improvement.** The writing can sometimes be unnecessarily long and convoluted. For example on lines 35-36: ""However, one can adversarially manipulate an OOD sample where the model is less confident to find another OOD sample where the model is overconfident"" and on lines 43-46: ""The general idea for the generative group is to get likelihoods for a particular sample out of the generative models for both ID and OOD to do likelihood ratio test or control the likelihood for training distribution far away from the training data to detect OOD samples by thresholding."" I would suggest the author break down these long sentences into shorter ones that are easier for the readers to parse. 

- **Some potential problems are left unaddressed.** It is unclear to me how the proposed method would be able to overcome the curse of dimensionality, as the number of polytopes can scale exponentially with the number of neurons. I also find the claim that the proposed method converts discriminative networks into generative networks misleading. Although the paper does provide an expression for the class conditional density $\hat{f}_y(x)$, it seems highly nontrivial to sample from this unnormalized density. I wish the authors would clarify on this point.

- **No baselines in the experiments.** As the authors noted themselves, the experimental analyses in this paper are limited to comparisons between the original networks and the proposed kernel generative versions. Due to the lack of baselines, it is unknown whether the achieved improvements are significant or not, especially when they are obtained at the cost of classification accuracy as evident in figure 3.

Overall, I think that the methods proposed in the paper is potentially interesting, but more experiments needs to be done to demonstrate its practical effectiveness. I think the appeal of this work is really hindered by the lack of baseline comparisons and the limited analyses of the results, as well as some confusing/potentially misleading statements.

Limitations:
The authors addressed the limitation that the Euclidean metric they used might not be the most suitable. Also, the authors are aware of the lack of comparison to other benchmark methods and commented that they will include it in future work.

Rating:
4

Confidence:
3

";0
Cb3clCaZG4;"REVIEW 
Summary:
This paper studies the problem of graph classification and proposes a method named graph Bernoulli pooling (BernPool).  Considering the complementarity of node dropping and node clustering, the paper proposes a hybrid graph pooling paradigm to combine a compact subgraph (via dropping) and a coarsening graph (via clustering), in order to retain both representative substructures and input graph info. 

Soundness:
1

Presentation:
3

Contribution:
1

Strengths:
1. The problem is important.

2. The writing is good. 

Weaknesses:
1. The results are weirdly high. I checked the code and found no validation set. First, the experimental setting is different from MVGPool, but author directly copies the number. Second, for example, authors select the worst model variant of MVGPool in the original paper. 

2. Node dropping and clustering are both well-known strategies for graph pooling. Combining them is not novel to this community. 

3. The ablation studies show your both variants highly outperform the best baseline on ENZYME, for example. So, which part works most in your framework, not the clustering or dropping? Can you explain more?


Limitations:
see weaknesses. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
Graph pooling is an essential operator in GNNs for graph classification. This paper propose a hierarchical pooling method which is based on the node clustering. Existing graph pooling methods are designed in a deterministic way which neglect the intrinsic structure feature. This paper propose BernPool, which first conducts Bernoulli sampling and then combines a compact subgraph after node dropping and a coarsening graph after node clustering. BernPool can jointly learn expressive substructures and keep graph topology. The core of this paper is the utilization of Bernoulli sampling factor for node selection and clustering.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1)	The topic of graph pooling is important in graph representation learning.

(2)	The writing of this paper is clear.

Weaknesses:
(1)	The model's complexity does not effectively demonstrate the necessity or unique advantages of each intricate design choice. The model first adopts a complex strategy for sampling, and then employs a complex hybrid pooling strategy, which lacks significant insights into their effectiveness.

(2)	Insufficient experimentation. The dataset used in the experiments is too small, and the effectiveness should be validated on larger datasets, such as ogb and zinc.

(3)	The method has excessively high complexity, which can limit its scalability in practical applications.

Limitations:
yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work proposes a probabilistic hierarchical graph pooling method called graph Bernoulli pooling (BernPool) for graph features learning.
Unlike previous deterministic works, the probabilistic Bernoulli sampling with the reparameterization trick allows BernPool to preserve certain diversity during the downsampling without losing high efficiency.
Besides, BernPool can capture representative substructures of graphs by combining the advantage of node-dropping and node-clustering techniques.
Ample empirical experiments demonstrate the effectiveness of the proposed methods, which outperform previous works with remarkable gaps and fewer parameters.
The ablation study justifies each proposed component in BernPool.

  


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. This work proposes new a hierarchical pooling method combining node-clustering and node-dropping and introduces probabilistic properties with a Bernoulli sampling framework. The Bernoulli optimization objective with soft-orthogonal constraint also contributes to performance improvement.
2. The proposed method outperforms previous techniques with remarkable performance gaps. Besides, the method is also applicable to several typical GNN backbones and shows decent improvements.
3. The proposed method is well supported by comprehensive empirical comparisons and ablation studies.
4. The mathematical derivation supports the proposed Bernoulli sampling well theoretically.

Weaknesses:
1. There are some minor confusing points in the paper writing. (listed in the question)
2. The authors explain the motivation for using probabilistic techniques in selecting reference sets and mention the drawbacks of node-dropping pooling methods. 
However, the motivation for the hybrid graph pooling is vague. The authors categorize BernPool as a node clustering pooling method but do not explain the motivation for adding back node dropping counterpart. 


Limitations:
No

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a pooling method that probabilistically generates a coarse graph using an expected sampling strategy. A hybrid pooling paradigm is designed to integrate two existing pooling paradigms. Extensive experiments demonstrate the effectiveness of the proposed method.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The method is well-organized and easy to follow.

The proposed approach is simple yet effective, as supported by the experimental results.

Weaknesses:
The stability of the proposed method is unclear. Since different runs may result in different factors z, which in turn lead to different projection matrices P and coarse graphs, the stability of the method needs to be considered. It would be important to address this issue, as obtaining different results in each inference procedure can be problematic.

In the experiments, it would be beneficial to provide examples that illustrate the reference set S, probability P, and the factor z. Including these examples would enhance the readers' understanding of the proposed method and its components.


Limitations:
Please check the weaknesses.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors introduce a non-deterministic graph pooling method called graph Bernoulli pooling (BernPool). This work comprises two main components: a probabilistic Bernoulli sampling method that allows for more diverse sampling situations and captures intrinsic characteristics of the data, and a hybrid graph pooling paradigm that combines a compact subgraph with a coarsened graph. Experimental results demonstrate the significant superiority of BernPool over the baseline methods in graph classification tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This work is easy to follow.
The idea presented in this paper is straightforward and the hybrid graph pooling technique significantly improves the performance of graph pooling. 
The authors have provided code implementation, enhancing reproducibility and adding to the persuasiveness of their work.

Weaknesses:
1. More visualizations are needed to illustrate how node dropping and node clustering work, such as which nodes are discarded during the pooling process and which nodes are grouped into the same clusters.
2. Transforming the deterministic process of pooling into sampling has been explored in previous works [1]. Although BernPool presents a more refined approach, the authors provide insufficient discussion or theoretical analysis of why Bernoulli sampling is superior to deterministic algorithms.
[1] Liu, Ning, et al. ""Unsupervised Hierarchical Graph Pooling via Substructure-Sensitive Mutual Information Maximization.""


Limitations:
The proposed method appears to be somewhat incremental.

Rating:
5

Confidence:
4

";0
42zI5dyNwc;"REVIEW 
Summary:
This paper advocates the separation of language and knowledge. It proposes a database-NLP method. The knowledge is contained in one or more databases whose internal structures can be a tree, a graph, or a hybrid. There are associated methods to query and retrieve from the database(s) the information. Finally, some ""chain"" based NLP methods connect the various tasks with the databases. In this paper, the database holds spatial relationship among various locations (e.g. Duck University is a location, Tennessee is a location, ""the fridge"" and ""Tom's room"" are locations). Overall, this reviewer feels that the approach proposed in this paper is very close to known art. 

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
This approach has a very focused usage scenario. It can be used in a situation where ""hallucination by modern language model"" is not permitted. 

Weaknesses:
Overall, this reviewer feels that the approach proposed in this paper is very close to known art.

Limitations:
The authors clearly presented the limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies a database-based natural language processing method and proposes a tree-graph hybrid model based on three types of spatial relations. The model is further applied to both natural language generation and natural language understanding tasks. 

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The insight of borrowing human cognition to develop a natural language model processing method is worth exploring.

Weaknesses:
This paper lacks experimental observations and analyses to validate the effectiveness of the proposed method or support the conclusion. 

Limitations:
Experimental results and analyses are needed to validate the effectiveness of the proposed method.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method for natural language processing. Instead of using language corpus, authors suggest to use database. Sentence generation is a linear schematization of a database-based representation. It is indeed an interesting idea. 

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
Authors propose a brand new NLP approach that is more closer to the way the human brain processes information, and very likely on the way to a brand new neural process for NLP. 

Weaknesses:
There is no experiment described in the paper. Authors shall finish experiments, then submit papers. 

Limitations:
Please read papers about mental spatial models, to see how simple spatial descriptions are generated from mental model.
For example, why most people say, San Diego is located further west of Reno? What could be the structure of the 'spatial database' in our neural mind?

Rating:
1

Confidence:
5

REVIEW 
Summary:
This paper aims to take a novel standpoint with respect to all the neural network architectures working on natural language (NN-NL). According to the authors, these NN-NLs work on the surface of the language, disregarding that the language is encoding information. In their opinion, information should be represented as entities and relations as in databases. Consequently, the paper proposes NN architectures working on trees and working on different levels of information encoding (Fugure 5). Clearly, relations have properties such as transitivity that should be used during training and inference. 


Soundness:
1

Presentation:
3

Contribution:
1

Strengths:

- The paper seems to propose a revolutionary way of thinking to neural networks


Weaknesses:
- The idea of information is a little weird. First, in this paper, the term information stands for structured information, and, apparently, the overall idea is to use structural information as the native form of encoding information as in databases. This is a little strange as there are not very large corpora of structured information. Indeed, having a large corpus is the key of success for these neural networks.

- The paper does not mention that the overall field of NLP was devoted to take Natural Language to a structured representation. From there, tasks were solved, eventually going back to natural language for specific tasks like dialog, question answering, and so on. This structured representation is at different levels: morphology, syntax, semantics, and, sometimes, pragmatics (e.g., in the form of speech acts). Only in the last decades NLP has pushed tasks from NL to NL (e.g., natural language inference, dialog) with architectures based on Machine Learning, which may be agnostic with respect to the structured representation of language utterances.

- Structured information mentioned in the paper may be correlated with the semantic representation of natural language. This is not even mentioned.

- There is a large body of studies on probing transformers to see if they replicate an NLP pipeline and to understand how they encode syntax, semantics and so on

- The basic idea of NN is to encode text, which may be retrieved and used by means of other similar text. This is encoding ""information.""

- The paper does not propose a dataset to work with.

- The paper has no experimental section






Limitations:
Not applicable

Rating:
1

Confidence:
5

";0
yVWBv0N1xC;"REVIEW 
Summary:
The paper introduces LayerNAS, which is a method for neural architecture search (NAS). The idea is to reduce the computational cost of NAS, which is exponential in the number of layers. As such, the paper introduces a layerwise search option with the idea being that the current layer can be directly determined based on the results of the previous layers. This enables polynomial computational complexity. The paper goes one step further and establishes the inclusion of the cost-constraints, e.g. in eq. 4. LayerNAS is then empirically validated in ImageNet and in a standard benchmark of NAS, which includes cifar10, cifar100 and Imagenet16 datasets. 



**Post rebuttal**: I appreciate the effort by the authors and their numerical evidence. I would urge the authors to include the references and the discussion in the camera-ready version. I would strongly encourage the authors to also include the numerical result on the cost per layer (see answer 2 in the original responses) and the additional results in transfer learning. One thing that I believe should be explained better in the main paper is the reasoning for the separation from larger architectures, e.g. the somewhat arbitrary 600 MAdds.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The reduction of the computational cost of NAS is an important aspect that makes the paper relevant for the NeurIPS community. Besides, a number of papers on NAS are published in NeurIPS and related conferences (**relevance**).  The solution proposed for the layerwise search has appeared before, but the paper makes a complete framework to support the idea and empirically validates the framework. In addition, the paper makes a clear statement of the limitations and the assumptions that led to those (**clarity**). This enables the research community to extend this paper further.

Weaknesses:
I am not sure what the novelty of the proposed method is, this is not clearly stated at the moment. 

I find that the train-free methods of NAS are more important for the story and as alternative methods than the two lines devoted to the end of the related work. Having said that, I do understand that they do not cover the contribution of this work.

Limitations:
The limitations are explicitly identified in the conclusion.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper tries to overcome a drawback of Neural Architecture Search (NAS), an enormous search space that hard to traverse whole space to design a well-optimized network.
From an assumption that a previous layer in a network doesn’t affect the subsequent layers, the paper converts multi-objective NAS to a combinatorial optimization problem.
With the proposed method, the paper designs optimized networks layer by layer, unlike other works that design the whole network simultaneously. It leads to reducing the search complexity of NAS to polynomial complexity.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- Writing is easy to follow.

- The paper compares the proposed method with other methods fairly with NATS-Bench.

Weaknesses:
- The proposed method that removes networks from search space by their costs is not novel.

- The search cost of the proposed method is still worse than one-shot NAS.

- The paper naively analyzes the cost of searched networks with MAdds, omitting other metrics such as energy consumption or latency, etc.

Limitations:
The paper refers to several limitations of the proposed method in Conclusion and Future Work. However, a thorough analysis is needed in terms of cost, too.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose LayerNAS with polynomial complexity. Namely, this work transforms the multi-objective NAS problem into a Combinatorial Optimization problem with proper assumptions. 
LayerNAS is benchmarked against recent NAS arts on ImageNet classification task, as well as on dedicated NATS-Bench in terms of quality, stability, and efficiency. 
Algorithm details and searched architectures are provided, which might benefit the community. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is clearly written and easy to follow. The detailed algorithm and searched architectures are provided, which makes the results replicable. 

2. Extensive results are reported on NASBench-101, and the results are promising. 

3. The limitation of the assumptions are properly discussed. 

4. Performing NAS in polynomial complexity is of good research impact and real-world applications. 

Weaknesses:
1. There are some format flaws, e.g., the abstract should be a single paragraph. 

2. Table 2 is not informative enough. It would be better to include more details such as training epochs, augmentations, whether distillation is utilized or not, etc. for comparison. Plus, sometimes it is hard to tell whether LayerNAS is better than previous arts when MAdds or Params are not aligned (e.g., LayerNAS has 50% more params than MobileNetV3-Small). I wonder if it is possible to strictly align both params and MAdds and compare accuracy, as anyway, LayerNAS is a multi-objective search. 

3. I wonder if there are latency-driven search results. 

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper propose a novel approach of breaking down NAS problem into a Combinatorial Optimization problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and easy to follow. The authors provide clear explanations and examples throughout the paper. 

Breaking down the search problem into a Combinatorial Optimization problem seems novel and interesting, and reducing the search cost to polynomial time, which is clearly a breakthrough to the research community. 

LayerNAS can be applied to operation, topology and multi-objective NAS search

Results on ImageNet seems to surpass state-of-the-art methods by a clear margin, evidencing their effectiveness of LayerNAS. 



Weaknesses:
I do not particular have a question, this paper seems to be easy enough to follow. 

Limitations:
Yes

Rating:
7

Confidence:
4

";0
EO1KuHoR0V;"REVIEW 
Summary:
This paper proposes a zero-shot generative editing model for audio signals. The text-based user edits include adding, dropping, replacing, inpainting, and perceptually upsampling audio signals. The audio is not limited to any particular audio domain (e.g., speech or music). Comparisons to recent text-to-audio and zero-shot editing models demonstrate a clear improvement in the state-of-the-art.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This is a notable advance in text-to-audio generation as well as audio editing. This paper outlines a framework for general purpose text-based neural audio editing that I could foresee becoming a blueprint for how this type of editing is performed. I imagine many possible subsequent works focusing on quality improvements and further editing capabilities—which there are many.

One of the challenges of producing such an editing system is that each editing capability requires its own evaluation. That is well handled here.

Weaknesses:
This is not “the first audio editing model based on human instructions” (line 43-44) as any editing model is technically following the user’s instructions. Consider “the first model to perform semantic audio editing from text-based prompts” or something similar. Also, you could have a machine generate the instructions, so they don’t have to be human users.

The evaluation of super-resolution would benefit from a comparison with a recent task-specific model (e.g., [1, 2]). I’m not yet convinced that performing super-resolution via text prompt would be preferable over having a dedicated super-resolution model.

It is not clear what each of the baseline models is in Table 7.

[1] Su, Jiaqi, Zeyu Jin, and Adam Finkelstein. ""HiFi-GAN-2: Studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features."" 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2021.

[2] Andreev, Pavel, et al. ""Hifi++: a unified framework for bandwidth extension and speech enhancement."" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.

Limitations:
Listening to the audio samples, the pitch reconstruction seems noticeably off—enough that the current system would not yet be amenable to fine-grained music editing. As well, the reconstruction of high frequencies could be improved and the context conditioning degrades noticeably in inpainting. The paper could benefit from a reconstruction task (i.e., inpaint with no mask) to better understand why context conditioning is degrading when the ground truth is provided to the model. Otherwise, it's not clear whether quality/accuracy errors can be attributed to the audio generator or the editing process.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This work proposes a new editing technique to audio data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The authors introduce a new task and a base method to tackling it. 

Weaknesses:
**Conditioning:** The authors concatenate the input audio to the sampled noise, why did you pick this option on top of sampling a Gaussian around the input sample of giving it as part of the cross attention (which will probably allow for modifications in the generation length)? Can the authors elaborate on this point?

**FAD** The authors do not show the FAD of audiogen and make-an-audio. Why? I noted that the classifier is different (PANN vs. VGGISH), but it is hard to understand what are the differences in performance. 

**Comparison to prior work:** I am having hard time understanding what is the cause for the model's performance - is it the data or the modeling. All the baselines present results when trained on different (mostly open) datasets; To do an appropriate comparison, it is expected to at least train one-two of those models on the same data the authors used.

**Subjective study** I find the experimental study not reproducible (even after reading Appendix D). Did the authors also check metrics compared to ground truth data? At least for dropping and super resolution, you have a hard gt data.

**Error bars** Some of the metric differences are small - can the authors report error bars?

Overall: I think that the task is interesting. However, since it is a new task and the paper doesn't introduce a new modeling approach, I would expect extensive experiments, which I was unable to find.

Limitations:
None

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes AUDIT, an instruction-guided audio editing model based on latent diffusion models. AUDIT is trained on triplet data consisting of an instruction, an input audio, and an output audio. The instruction is used to guide the diffusion model to only modify the audio segments that need to be edited. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks, such as adding, dropping, replacement, inpainting, and super-resolution.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
AUDIT is a novel audio editing model that can be trained using human text instructions. AUDIT is trained on a dataset of triplet data, which consists of an audio clip, a text instruction, and the edited audio clip. AUDIT can maximize the preservation of audio segments that do not need to be edited, and it only requires simple instructions as text guidance. AUDIT has achieved state-of-the-art results in both objective and subjective metrics for several audio editing tasks.



Weaknesses:
.

Limitations:
.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes AUDIT, an instruction-guided audio editing model based on latent diffusion models. The model is trained with triplet training data (instruction, input audio, output audio) for
different audio editing tasks and is based on a diffusion model using instruction and input
(to be edited) audio as conditions and generating output (edited) audio. The proposed model outperforms several baselines in the editing tasks

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The AUDIT model is able to perform audio editing conditioned on text instructions, which have rarely been studied by prior work on audio generation. The data construction strategy of using paired text-audio data and refining text instructions based on templates and LLM is also useful for making instruction datasets. In terms of performance, it is better than several baselines (SDEdit and its variants).

Weaknesses:
The main weakness lies in the evaluation. The model is mostly compared to SDEdit and its variants, which is zero-shot audio editing model. It is unclear how it compares to task-specific models such as NU-Wave2 [1] for audio super-resolution. Some of the editing tasks considered in the paper (e.g., Adding) can also be addressed via text-to-audio generation, while proposed method has not been compared to the methods of this category. There also lacks analysis on the gain of the proposed method over SDEdit and its variants. For example, how robust the model is to the category of the dropped sound in Dropping task? As both the training and evaluation data are synthesized, it is unclear how the model will perform on real data.

[1]. Han et al., NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates

Limitations:
Yes

Rating:
5

Confidence:
4

";1
SycQxJaGIR;"REVIEW 
Summary:
This paper addresses the multi-agent pathfinding problem by proposing an approach that utilizes a combination of a planning algorithm for constructing a long-term plan and reinforcement learning for reaching short-term sub-goals and resolving local conflicts. The results show that the proposed method outperforms decentralized learnable competitors and centralized planner. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The method is straightforward and concise.
2. The writing is clear and easy to understand.

Weaknesses:
1. The proposed method follows a hierarchical reinforcement learning framework, which has been extensively studied in previous works. There are limited contributions to the design of sub-goal selection. 
2. In the heuristic sub-goal decider, A* is used to construct a path, which requires global information. As the sub-goal decider will be used multiple times during the episode, the overall method seems not fully decentralized. 


Limitations:
This work has little negative societal impact. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a decentralized hierarchical approach without agent-to-agent communication for Lifelong Multi-agent Pathfinding (MAPF). The framework adds a congestion-based heuristic to an A*-planner and a low level Reinforcement Learning (RL) - based controller to follow the provided sub-goals. Experimental results show that the proposed method has a higher throughput (or rate of reach of new goals) for a range of maps.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- Easy to understand. Uses the well-studied A* planner with additional heuristics and a low level RL-based controller simply trained to reach goals promoting long term performance.
- Hierarchical framework is simple and could be effective way for decentralized control in an MAPF problem.

Weaknesses:
- The change of the heuristic in the A* planner seems weakly substantiated. While empirical results are promising, the need for hyperparameter tuning for the score and lack of guarantees on behavior may impede the use of this new heuristic.
- Confidence intervals for higher density experiments may be too large to claim better performance. (E.g. Table 1, 16 agents, proposed approach has throughput $ 0.56 \pm 0.34$ vs Primal2 having $0.31 \pm 0.14$ ). This may point to noisier behavior in the presence of more obstacles.

Limitations:
- Access to global map is assumed for use of the A* algorithm.
- Several empirically determined reward function portions may hinder generalizability to different maps.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers a decentralized multi-agent pathfinding (MAPF) problem. The main idea is to combine heuristic-based search and reinforcement learning. This work first determines subgoals and uses this information as intrinsic rewards. Empirically, it outperforms two baselines in the literature,  PRIMAL2 and PICO, in domains with different sizes and different numbers of agents. The method also demonstrates the ability to generalize to domains unseen during training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main observation is that pure heuristic search would not have a good performance in complex domains, where collaborative behaviors like congestion avoidance may not emerge. 
This work shows an inspiring combination of heuristic-based search and reinforcement learning.

The proposed algorithm also outperforms a centralized control algorithm (RHCR) when the number of agents is large or when the computational budget is small (so the centralized algorithm is expensive).


Weaknesses:
Some concerns about the practicality of the algorithm: 1) It requires some hyperparameter selection. 2) It requires pre-train a neural model. When it outperforms RHCR in some settings when RHCR is run for 10s, we need to consider the computational overhead of running the RL algorithm during training.
In terms of performance, the performance between FOLLOWER and PRIMAL2 is very close in some domains.

**Minor points.** Line 203, duplicate “node.”


Limitations:
The authors have mentioned the limitations of this work to be the assumptions of static environment, perfect perception.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel method for decentralized lifelong MAPF. The method consists of two components: a heuristic sub-goal decider, which assigns sub-goals for each agent using a heuristic (e.g., A*), and a learning-based policy network, which outputs actions for achieving the short-term subgoals. The paper compares the proposed method with both learning-based decentralized methods and the search-based centralized method on extensive setups and demonstrates the proposed method's superiority. The paper also provides insightful ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using heuristics to solve long-term planning and utilizing learning-based policies for achieving short-term sub-goal is reasonable and also commonly used in many other tasks.

2. The paper compares the proposed method with both learning-based decentralized methods and a search-based centralized method on extensive setups and demonstrates the proposed method's superiority and generalization capability. 

3. The paper also provides insightful ablation studies and verifies the necessity of each proposed component. 

4. The paper is well-written and easy to follow.

Weaknesses:
1. Since I'm not active in the MAPF field right now, I am unsure if there is literature sharing similar ideas in the MAPF tasks. But at least I know that the idea of using heuristics for long-term sub-goal assignment and learning-based policy for low-level sub-goal achievement is quite common in the RL field.

2. How the RL policy handles the collision and deadlock is unclear to me. What will happen if the agent (or two agents) choose the action(s) that will cause a collision? What will happen if there is a deadlock (e.g., two agents want to pass a narrow corridor)?

3. I am a little confused about what the RL policy can learn if the K is set to 2, which means the sub-goal is just two steps away from the current location. Are there many candidate paths to a sub-goal, which is just two steps away?

4. Lines 211-218: Since the agent doesn't know the future locations of other agents, how does the method count the ""number of times the other agents were seen"" in a future step? Does the method use a static heat map (for only the current step) to count that?

5. Lines 242 and 247: the symbol H was used twice with different meanings.

6. Figure 1 is not mentioned in the text.

7. Lines 175-176: ""as the ratio of the episode length to the number of goals achieved"" -> ""as the number of goals achieved to the ratio of the episode length""?

8. Line 203: ""node node""



Limitations:
Yes

Rating:
5

Confidence:
3

";0
7ym1abRTF3;"REVIEW 
Summary:
This works addresses two problems: first, the problem of solving a specific class of information-gathering decision processes, and second, applying the resulting algorithm in the real world by creating an intelligent tutoring system. The algorithm is a straightforward greedy optimizer that gathers information based on maximizing one-step value of information.  The intelligent tutor blends the algorithm with a UI that gives humans feedback on their decisions by comparing them to the algorithm's; the paper shows that the humans implicitly learn from this feedback and improve their decision making abilities.  The paper concludes with a few small comparisons of the algorithm to other algorithms.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ The paper is well-written
+ The problem seems reasonably well formulated
+ The algorithm is natural and reasonable
+ The algorithm seems to perform well compared to alternatives
+ The ""intelligent tutor"" is a bit of a misnomer, but does seem to improve human decision making ability

Weaknesses:
- This paper blends two distinct ideas, and does not take either very deep. The MGPS algorithm is straightforward, and the authors spend very little time (for example) analyzing its properties or connecting it seriously with the vast literature on decision processes.  (For example, I'm surprised not to see any connection to bandits).  I think you could have written a whole paper about MGPS -- establishing properties, contrasting with other algorithms, etc.  For example, a regret bound or similar form of theoretical analysis would have been nice.

On the other hand, the ""intelligent tutor"" seems like a very simple UI that gives very basic feedback.  While the authors do demonstrate some effectiveness, it too is not investigated deeply.  There is no comparison, for example, to other forms of UI, to other forms of feedback, etc.

So, it's hard to say: is this paper about MGPS, or intelligent tutor UI/UX design?  And while I appreciate that the authors probably have a vision of ""solving a real-world problem"" with this combination of ideas, I have to say that the problem formulation seems pretty far away from something that could actually be used in a business decision support system.


Limitations:
The authors do not explicitly discuss the limitations of their algorithm/UI in a dedicated section.

I do not see any potential negative societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The problem this paper tries to tackle is how to improve human decision making in the specific problem of selecting a project between a candidate set of existing projects. The strategy to improve the human's decision making is to build an agent that can solve the project selection itself by framing as a POMDP and then having that agent serve as a tutor during a teaching phase. The agent is learned by first framing the problems as a POMDP with Guassian states and the paper proposes a myopic algorithm to select actions (actions allow you to acquire information about each project on a specific dimension of the Guassian). The main evaluation of the paper is with a user study where human participants are trying to select between multiple projects in terms of estimated performance. The authors show that participants who received training with the tutor performed better than participants without a tutor or with a dummy tutor.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
- very well-designed and rigorous user study that shows that the agent and the tutoring was effective. 


Weaknesses:
- (main reason for low rating) out of scope of NeurIPS: this paper seems like a bad fit for NeurIPS, the paper does not contribute new algorithms that are broadly applicable and does not evaluate methods on broadly recognized datasets or benchmarks.  This papers models the problem of human project selection as a PODMP and proposes a relatively straightforward procedure and evaluates on a synthetic task of project selection. I don't see any insights that the community might benefit from. I think this would be a strong paper for a conference that suits it more. 
A bad (but still useful) heuristic is to look at the references where I count a single NeurIPS paper (from 2010) and one UAI paper (among a lot of management and behavioral science citations that don't include other human-ai conferences like CHI). 

- why not display the recommendation of the AI agent during test time as a baseline? since the AI agent performs well at the task, participants should be able to see its recommendations at test time. 

- the proposed algorithm while adequate for the problem is not a generalizable solution for the broad family of problems that are of interest in the human-AI space. In particular, the myopic approximation is very limiting. While section 6 does evaluate it against (a relatively old) baseline in PO-UCT, it limits the baseline to 5000 steps because of runtime constraints, however, it might be possible to optimize the performance of PO-UCT to be faster. 

- novelty: the method is a modification over MGPO [14] which introduces the myopic approximation, the proposed method MGPS modifies it to make it suitable for project selection. The authors do a good job of comparing to MGPO, but novelty overall is limited.

- value of tutoring for project selection: why not just follow the recommendations of MGPS? the tutoring is specific to the problem domain, thus I don't understand anything that the human can take away from the tutoring to future tasks except the one they will encounter. Moreover, for the real world problem presented, it is a one shot decision problem, so tutoring is not as well motivated.

Limitations:
limitations are well discussed.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper focuses on the problem of project selection (how does a human choose which, among a set of possible projects, is the best one to pursue). To address this problem, they develop an algorithm called MGPS that discovers a rational greedy strategy for solving this problem, and then they attempt to teach this strategy to a human via an intelligent tutoring system. The approach is evaluated in a real world project selection scenario.

- page 2, line 64: ""selection.To"" -> ""selection. To""
- page 3, line 127: ""we introduce explain our general"" -> ""we introduce our general""


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
It is interesting to see a paper that goes all the way from finding an algorithm to solve a problem, to teaching humans based on that strategy and then evaluating human performance after being taught.


Weaknesses:
The same strength of the paper seems to also be its main weakness: because the authors try to tackle a whole large problem end to end, I did not find each if the individual pieces that ground breaking. Perhaps if the paper focused on just one of the problems (finding optimal strategies, or just better techniques for teaching the learned strategies), maybe a stronger contribution would arise, by more systematically addressing one problem. But as it stands, the paper seems two-headed, with limited contributions, as each of the two problems is only dealt with shallowly.


Limitations:
See weaknesses.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors pose the problem of teaching decision-makers how to take a single action (picking a project from among a set of projects) based upon costly advice from experts across different, weighted criteria. The authors develop a reinforcement learning approach to creating a tutor that approximately solves the MDP (using a myopic approximation). The proposed tutor outperforms a baseline on a banking dataset. The tutor is also shown to improve decision-making of subjects in an online study, who get to learn from the tutor by watching the tutor make decisions.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
+Section 3 is very clearly written. Well done (though Line 162 could have used O-notation).
+The paper clearly presents the algorithm.
+The paper is addressing an important problem of developing a tutoring system for solving MDPs
+The paper has strong empirical results vs. PO-UCT.
+The paper shows statistically significant results in a user-study. It is nice that a user-study was done.

Weaknesses:
-The terms h, e, \lambda, N, and R_{total} are not sufficiently defined in Lines 41-50. Perhaps it would be better to give a more complete description later in the paper and abstract the presentation here to make it more intuitive just with words.
-The comment on brainstorming in Line 86 ignores relevant literature on the wisdom of the crowd, the science behind brainstorming and focus groups, etc.
-The statistical analysis does not report testing for the meeting of a Gaussian assumption for the confidence intervals. Details of the Box approximation should be provided. Further, it would have been better to also report p-values in that table. 
-For the user study, Table 1 should report how optimal (the RR) the MGPS tutor would be if run automatically (no human intervention) and how poorly a random, automatic system would be for the RR-score.
-I am uncertain that the authors are reporting the degrees of the freedom of the F-test. The F-test has two degrees of freedom, but only 1 seems to be indicated in Lines 295-299.
-I can appreciate that the authors might have thought that the results in lines 329-345 indicate that MGPS > PO-UCT and therefore not included PO-UCT as a baseline in the user study; however, that is a debatable decision. It could be that the behavior of PO-UCT is more intelligible by users, and users with PO-UCT could have outperformed those trained by MGPS. As such, I recommend the user study be re-ran (to account for cohort effects) with the PO-UCT baseline and randomizing the allocation of participants to the conditions.
-The paper isn't exactly ""tutoring"" participants. Rather, the system is providing recommendations (or making decisions) and the users have to reverse engineering the actual strategy. Considering that the strategy itself is not constrained to be a set of if-then rules, it is unclear what exactly is being learned or how. It would make the paper better to have actually analyzed (by collecting the data from users) what users are learning and thinking. I would recommend looking at methods in explainable Artificial Intelligence.

Note:
-I recommend the ""Dummy"" tutor be called a random tutor -- a ""random tutor"" is a more clear description of that condition.

Limitations:
Limitations are reasonably discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper addresses the problems of (1) sequential decision-making of information gathering through asking experts for information about the rewards for different projects (as meta-reasoning towards project selection), and (2) teaching people how to make near optimal decisions in the same problem through training in an intelligent tutoring system.  Specifically, novel problem aspects are considered compared to recent work on similar problems, including the availability of different experts of different reliabilities (instead of one information source) and multiple criteria for evaluating the quality/reward of a project.  An algorithm is developed for generating solutions guiding information gathering.  That solution is used to guide the training of humans through an Intelligent Tutoring System (ITS).  Experimental results highlight the benefits of the approach through both better training of humans in an ITS (measured through better rewards earned) and better performance in simulation of agent reasoning than the problem modeled as a belief MDP solved by PO-UCT.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The primary strengths of this paper include:

S1) The problem addressed is one that is relevant in the human-AI applications and it considers novelties not previously addressed that are important real-world complexities.  The problem of sequential decision-making will be of interest to the planning community at NeurIPs.

S2) The paper is well written and easy to follow.

S3) Modeling the problem as a POMDP is appropriate, and the MCDM component seems to be appropriately modeled.

S4) I appreciated the use of two very different sets of experiments -- both training humans within an ITS and simulating the approach directly.  The ITS experiment was well designed and the evaluation was very carefully conducted and convincing.

Weaknesses:
The primary weaknesses of this paper include:

W1) The main contributions identified in the abstract are incremental, seemingly adding some environment complexity and solution adaption to [14], rather than being entirely novel.

W2) While the choice of a POMDP was appropriate, I wasn't entirely sure why the authors relied more on the belief MDP formulation rather than a true POMDP with observations separate from belief state transitions.

The problem being solved is gathering information to ultimately choose the best task to accomplish.  This is very closely related to prior POMDP usage for problems like preference elicitation where the agent gathers information about which is the user's main preference or task they want the agent to perform.

Boutilier,C. 2002.A POMDP Formulation of Preference Elicitation Problems.In Proceedings of AAAI'02, pp. 239–246.

Doshi, F., & Roy, N. 2008. The Permutable POMDP: Fast Solutions to POMDPs for Preference Elicitation. In Proceedings of AAMAS'08, pp. 493–500.

In those models, the state space is the set of possible tasks/preferences, and actions either (1) query an information source (e.g., the user) for observations used to update the agent's Bayesian beliefs about the top preference/task, or (2) end information gathering to perform the perference/task the agent thinks is the top.  That is fundamentally the same as the problem being addressed here, but the details are slightly different.  Instead, your state space is belief states over the details of the tasks, from which a top one is selected.  It's not clear to me why the former wouldn't work in this situation and what the advantage is in your formulation, which would help strengthen the novelty of the work. (Note: what makes your paper different from [14] also makes it different from those works).

Information gathering in POMDPs in general also have special formulations, such as the \rho-POMDP and equivalent POMDP-IR, and I would think your problem model would also fit nicely in the POMDP-IR, but neither is considered in your related work.

\rho-POMDP: Araya-Lopez, M., Buffet, O., Thomas, V., & Charpillet, F. 2010. A POMDP Extension with Belief-Dependent Rewards. In Proceedings of NIPS'10.

POMDP-IR: Spaan, M.T.J., Veiga, T.S., & Lima, P.U. Decision-theoretic planning under uncertainty with information rewards for active cooperative perception. Journal of Autonomous Agents and Multiagent Systems, 29(6):1157-1185.

W3) I also wasn't sure why you chose PO-UCT as your baseline?  Its the simpler version of POMCP (whereas POMCP would have been more appropriate if you had explicit observations in your model).  Belief MDPs can also be considered continuous state MDPs, and there have been many advancements in Monte Carlo Tree Search planning in that area since PO-UCT, such as:

Sunberg, Z. & Kochenderfer, M. 2017. Online algorithms for POMDPs with continuous state, action, and observation spaces. arXiv, 2017. doi: 10.48550/ ARXIV.1709.06196. URL https://arxiv.org/ abs/1709.06196.

Finally, the \rho-POMDP has a MCTS solution that would also be relevant that is more recent:

Thomas, V., Hutin, G., & Buffet, O. 2020.. Monte information-oriented planning. In Proceedings ECAI’20.

W4) I also didn't quite understand the novelty of the ITS experiment compared with [14].  Was the only difference the change in the meta-reasoning algorithm, or were there other differences?

Limitations:
These were appropriate addressed.

Rating:
5

Confidence:
5

";0
iRiEpc7jpa;"REVIEW 
Summary:
The paper provides a comprehensive understanding of gradient rank in deep neural networks (DNNs) and how architectural choices and data structure affect gradient rank bounds. The authors highlight the emergence of low-rank learning as an inherent aspect of certain DNN architectures and propose a theoretical analysis to provide bounds for training fully-connected, recurrent, and convolutional neural networks. They also demonstrate, both theoretically and empirically, how design choices such as activation function linearity, bottleneck layer introduction, convolutional stride, and sequence truncation influence these bounds. The study not only contributes to the understanding of learning dynamics in DNNs but also provides practical guidance for deep learning engineers to make informed design decisions. The authors also discuss the phenomenon of ""Neural Collapse,"" where linear classifiers within DNNs converge to specific geometrical structures during late-stage training, and highlight the role of geometric constraints in learning beyond this terminal phase.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work theoretically analyzes the upper bound of the rank of gradients for different kinds of neural networks. The analysis can offer us great insight when we try to design new modules or activation functions. In addition, the authors provide a detailed practical analysis of the proposed methods. 

Weaknesses:
For the experiments for validation of the bottleneck, it would be better to add experiments on convolutional neural networks to make the validation more sufficient.  For the analysis of the structure, it would be better to add analysis of some normalization techniques or regularization techniques.

Limitations:
Yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors present an investigative study into the learning dynamics of neural networks, specifically low-rank neural networks. The motivation is that training dynamics of neural networks are not fully understood. Low-rank models have practical advantages (time / memory). This work provides a theoretical and empirical overview of the learning dynamics of neural networks with a focus on low-rank models.

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
The main strength of this paper is its significance, i.e. the attempt to explain learning dynamics of neural networks from both a theoretical and empirical perspective. This is a worthy and valuable endeavour which will be of great interest to the field if done correctly and thoroughly. Unfortunately, there are also significant weaknesses, as we will see, which weight heavily against the strengths.

Weaknesses:
There are a number of critical weaknesses present in this paper that severely reduce its value. Perhaps there is a valuable message in there but the presentation, clarity and writing make it impossible to recommend this as an impactful paper. Here are some of the weaknesses detailed:
- Flow and writing can be greatly improved. Very abrupt changes between sections (e.g. from 1 to 2). The sections do not have self-contained introductions to help readers orient themselves and understand the reasons and motivations for choices made.
- Related to the first point, formulae are presented without sufficient discussion. Many sections (notably 2 and 3) read like a series of statements rather than a well-constructed and clearly motivated argument. I encourage the authors to improve the flow to help the reader understand the context 
- Formatting can be improved. What is *error*? Overview of results in lines 77-88 can be greatly improved with respect to formatting.
- Missing definition, what is BPTT in line 104?
- Lack of discussion on the connection between the presented theoretical and empirical results.
- Stated contributions are not found in the paper (effect of stride is in supplementary material and I could not find sequence truncation experiments anywhere).

Limitations:
The authors discuss only one limitation of their work: the relationship between tasks and low-rank emergence. However, there are a number of limitations that need to be discussed. Not only have the experiments scratched the surface of the possible signals from learning dynamics we can collect and paint us a limited picture of what is happening but the results themselves also make us pose new questions that remain undiscussed. Refer to my questions to the authors for some possible discussion points that would improve this paper greatly and help place it within the existing literature and highlight the most obvious next steps to build on this work for future studies.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies the gradient rank of DNNs and examine how certain design choice, in particular architectural choices of the model and the structure of the data affect the gradient rank bounds. The paper mainly focuses on theoretical results with some empirical experiments to validate their empirical claims. The paper begins by studying the gradient rank of deep linear models before proceeding to explore leak-ReLU networks. They also extended their analysis to consider convolution and recurrent layers. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper seems fairly novel, as I have not seen anyone explore the gradient rank although I think it is important as it can tell us more about the learning dynamics of neural networks. The theoretical analysis is simple, and this is advantageous in my opinion. One of the strengths of the paper is they have analysed a few neural network variants such as recurrent, convolutional and leaky relu networks. This demonstrates how general their analysis can be. The experiments are good and test well the bounds. 

Weaknesses:
it is not clear how to generalise the analysis to more complex non-linearities and can thus at most only seem to study them near a limited range of input values (such as near zero) where the network behaves more linearly. The analysis also doesn't look at more modern architectures like transformers. 

Limitations:
Authors have adequately addressed and there is no clear potential negative societal impacts. 

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper explores the collapse of gradient rank in DNNs during training. Based on the simple linear network, the authors theoretically examinate the rough upper bound of the gradient rank for simple MLP, current network and CNN. Furthermore, they analyze the numerical effect of rank on Leaky-ReLU activation. 

To verify their results of the rank bounds, experiments are conducted with on two synthetic datasets and several real datasets (CIFAR10 and Tiny-ImageNet for computer vision and WiKiText for NLP) across pure linear networks and smaller-sized version of popular architectures, e.g., ResNet16, VGG, BERT, etc. 

Experimental results show that 1. Linear bottleneck layer architecture reduces the gradient rank; 2. the gradient rank of recurrent networks and CNNs is proportion to the size of sequence; and the negative slope of leaky-ReLU activation is related to the amount of gradient rank restored. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper demonstrates a high level of originality in its exploration of gradient rank in linear network, recurrent networks and CNN and extra analysis of Leaky-ReLU effect.

Weaknesses:
1. Poor writing quality: This paper has not been properly polished before submission, and there are numerous writing issues that have not been carefully checked. For instance, in line 132, the term ""phi"" should be ""\phi"". Additionally, in line 217, the notation ""FIGURE1left"" should be revised for clarity. In line 218, there are two issues: the terms ""FIGURE1right"" and ""In figure"" should be corrected (""in figure""?). Moreover, a missing ""."" in line 235 needs to be addressed. Another concern is the inconsistent section references throughout the paper, such as ""sec 3.2.2"" in line 226 and ""&3.3.1"" in line 236. Overall, this article resembles a draft rather than an official submission, and it requires further polishing and thorough review.

2. Unfollowed style: the bibliography style seems not to follow the provided template and please check your article format, because I cannot search and select your words in the main context and cannot jump to the referred figures.

3. Lack of practical network experiment: although the theoretical analysis is mainly conducted on the linear network and experiments are provided for 3-layer linear MLP and RNN network, more experiments and further analysis on popular networks are needed for providing insight for the understanding the gradient rank in practice. 

4 Lack of citation and explanation for BPTT.

Limitations:
No potential negative societal impact of their work

Rating:
3

Confidence:
3

";0
x5aH1we8Bb;"REVIEW 
Summary:
This work proposes a new attack on monocular 3D object detectors utilising NERF representation to create multi-view consistent attacks utilising Lift3D [26]. The authors show successful attacks on nuScenes dataset and the effect of their design choices on the attack success. Furthermore, a mitigating strategy is shown how to make monocular 3D object detection robust to these types of attacks.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
This manuscript includes a comprehensive set of ablation studies and detailed analyses, which effectively highlight the influence of different components in the proposed pipeline. Figure 4 is especially illustrative and insightful. Notably, the successful tackling of numerous molecular 3D detectors, each with varying detection strategies, is commendable.

The authors present an effective strategy to counteract the proposed adversarial attacks on molecular detection systems. 

The overall presentation of the paper is lucid and the figures contribute significantly to accurately conveying the intended ideas.

Weaknesses:
1- A comparison against baselines is missing, most notably the mesh-based baseline [43]. This comparison is very important since the proposed setup in Lift3D [26] is very similar when constraining the Nerf to shape and texture latents , resembling the mesh generation and texturing scheme in [43]. 

2- A crucial evaluation protocol has been overlooked. In the context of adversarial attacks, the imperceptibility of the attack is a significant factor; how discernible is the attack in contrast to the clean sample? This critical information is absent in this work. It leaves us questioning the perceptibility of image corruption in terms of pixel alteration. If the corruption is so pronounced that even a human observer fails to detect the cars, can it still be considered a successful attack? Previous studies typically provide this information [43]. Moreover, the inclusion of tests on KITTI should be considered vital to the evaluation process.

Limitations:
N/A

Rating:
7

Confidence:
5

REVIEW 
Summary:
This work develops adversarial attacks against 3D object detectors by utilizing instance-level NeRFs.  
They start with a representation of a vehicle, parameterized by a NeRF that predicts both geometry and texture and render the vehicle into a image, which they compose into the original image by copy-pasting. They use the composited image to adversarially attack 3D object detectors, which provides a gradient signal used to optimize the NeRF (texture only).  
Experiments show their adversarial examples are effective against a variety of different 3D object detectors, and they show that training on these samples improves robustness (and even overall performance).

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Novel application of NeRFs, utilizing the fully differentiability to optimize for adversarial texture.
* Work is well written and overall clear to follow.  
* Analysis provides good insights (referring to Sec 5.3 analysis of 3D detector architecture robustness and Sec 5.4 adversarial training actually boosts performance).
* Multiple architectures used in experiments.

Weaknesses:
* Section 4.4 could use more elaboration - this is a key section for the overall work and in the current revision is quite vague.
* Some of the attacks (Fig. 3) do not look photorealistic - how can the reader be convinced these adverarial samples would actually work in the real world?

Limitations:
Authors provide discussion of limitations (real world safety).

Rating:
5

Confidence:
4

REVIEW 
Summary:
Deep neural networks (DNNs) have shown susceptibility to adversarial examples, which raises significant safety concerns, particularly in safety-critical applications like DNN-based autonomous driving systems and 3D object detection. While there is a wealth of research on image-level attacks, most of them focus on the 2D pixel space, which may not always translate into physically realistic attacks in our 3D world. In this paper, the authors present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs).

The utilization of NeRFs allows for the generation of adversarial examples that possess photorealistic appearances and accurate 3D generation, thereby enabling more realistic and realizable adversarial attacks in the 3D domain. The authors train their adversarial NeRF by minimizing the confidence of surrounding objects predicted by 3D detectors on the training set. They evaluate Adv3D on an unseen validation set and demonstrate its ability to significantly degrade performance when rendering the NeRF in various sampled poses.

To ensure the practicality of the adversarial examples, the authors propose primitive-aware sampling and semantic-guided regularization techniques, which facilitate 3D patch attacks with camouflage adversarial textures. The experimental results showcase the generalizability of the trained adversarial NeRF across different poses, scenes, and 3D detectors. Additionally, the authors provide a defense mechanism against these attacks through adversarial training via data augmentation.

In summary, the authors introduce Adv3D as a novel approach that models adversarial examples using NeRFs, resulting in more realistic and realizable attacks in the 3D domain. They demonstrate the effectiveness of their method through extensive evaluations and propose a defense strategy to mitigate the impact of these attacks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The writing and presentation of this paper are good and clear.
2. The idea of leveraging NeRF in generating adversarial examples is interesting
3. The authors conducted sufficient evaluation of the proposed method

Weaknesses:
1. The practicality of the proposed attack is questionable
2. There is a lack of real-world experiments
3. The study seems to lack technical insights as the adversarial attacks are well established. There is no surprise that using some fancy new idea can lead to adversarial examples, but the essence is the same.

Limitations:
The authors has discussed the potential negative societal impact of this study, so it should be fine.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The authors proposed new generative adversarial examples in the form of NeRFs, in the context of driving scenarios. The training objective is minimizing the 3D detection confidence from a variety of views. The parameters to optimize are the latent input to the NeRF, that encodes shape and texture info. Rendering is naturally differentiable due to the usage of NeRF. To improve the physical realizability, they propose three methods: primitive-aware sampling, NeRF disentanglement, and semantic-guided regularization. The authors conducted experiments on the widely used nuScenes dataset to evaluate the performance drop. The results show that their method is able to reduce the detection performance of various detectors, whether they are FOV-based detectors or birdview-based ones. They also evaluated the transferability of their method, and the adversarial training defense method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Using NeRF as 3D adversarial example representation seems novel and interesting. The NeRF representation naturally is differentiable in terms of rendering, so it makes the adversarial attack problem easier. Also, with more uses of NeRF in 3D vision, it is important to explore the vulnerability in NeRF itself. Such adversarial attacks may highlight the potential security issues in NeRF.

The attacking framework (expectation over transformation), the NeRF rendering framework they use (Lift3D) are standard. The method is mostly built upon existing works; it seems not hard to implement their method.

The writing is clear.

Weaknesses:
My major concern is that whether the formulation of NeRF is necessarily, from the motivation perspective. In line 175, they fixed the shape and only optimized the texture latent code. The optimization is essentially finding the color, density of the volume. However, I believe most vehicle objectives are not translucent; the optimized 3D object is very hard to realize. This is evident as authors need to improve the physical realizability (line 180).

This leads to the Occam's razor principle: do we really need NeRFs to reach the effectiveness/realizability of the 3D attack? So we are missing a baseline here: optimizing the surface texture as a 3D mesh, using existing differentiable mesh renderers (such as Neural Mesh Renderer). The latter is easier to optimize (2D texture space), and more physically realizable (because it is a texture map rather than a volume). In line 166, the authors said ""enables patch attacks in a 3D-aware manner by lifting the 2D patch to a 3D box"", so we really need a baseline to showcase such lifting is necessary. Also it is not clear how rendering the NeRF into 3D scenes is done. In Fig. 3, the lighting of the NeRF object is not consistent with the environment, and we can see typical blurriness of NeRF.

Another weakness is that the setting is not sophisticated enough to be ""Driving Scenarios"". At first glance, it looks like attacking self-driving algorithms, but the point clouds are not used (correct me if I am wrong). The detection methods (FCOS3D, PGD-Det etc) are based on monocular/multi-view 2D images instead of multi-sensor. In Fig. 3, the inserted adversarial example does not seem to block the LiDAR rays. The experiment is not done through a full driving simulation software, but by rendering 3D objects into existing 3D data. Whether such mixed environment can represent the real-world driving scenario is not clear. It'll be better to claim general 3D detection scenario and do more experiments with other objects, instead of only claiming driving-specific scenarios.

In general, my decision largely depends on the first point: the NeRF representation may not be necessary under the current settings. Optimizing the texture image should just work; such volume formulation will make it harder to physically realize and does not bring much benefit other than differentiable rendering.

Limitations:
The authors addressed the limitations about dataset annotations and potential harmful consequences.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes to generate 3D adversarial examples for attacking 3D object detectors in driving scenarios using NeRF. In particular, it integrates a series of techniques, including primitive-aware sampling and semantic-guided regularization, to ensure the physical realism and realizability of the generated adversarial examples. Extensive experiments have validated the effectiveness of the proposed method in reducing detection performance and serving as data augmentation.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. As an early attempt of generating 3D adversarial examples using NeRF, this work could offer a new perspective for the community in understanding and tackling real-world 3D adversarial attacks. 

2. The extensive experiments validate the superiority of NeRF as a 3D adversarial attack generator. In particular, it is interesting to see that the generated adversarial examples can serve as a data augmentation to improve clean performance, which aligns with the previous observations in classification.

Weaknesses:
1. My major concern is the assumed attacking setting of this work, i.e., how to leverage the proposed method in real-world driving scenarios. If only a static adversarial example is attached to the scene, generating other static objects on the road may be more practical than generating a vehicle; Otherwise, the authors are expected to show a video under an egocentric view to demonstrate the attack effectiveness, i.e., whether the dynamically moving adversarial vehicles can consistently mislead the 3D detectors from different view directions.

2. The claim ""the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs)"" in the abstract may not be accurate. ViewFool [1] also models adversarial examples using NeRF although only the view direction is adversarially optimized. It will be more accurate if the authors highlight this work as the first 3D adversarial example generator using NeRF.

3. Missing references regarding the early attempts of marrying NeRF and adversarial attacks (which are mostly orthogonal with this work):

[1] ""ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints"", Y. Dong et al., NeurIPS'22.

[2] ""NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations"", Y. Fu et al., ICML'23.

[3] ""Aug-NeRF: Training Stronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmentations"", T. Chen et al., CVPR'22.

4. Minor issue: There exists some inconsistency in terms of tense and punctuation, which could be improved in the final version.

Limitations:
Although the developed adversarial attacks may cause security concerns, this work intended to gain a deeper understanding of 3D adversarial examples and improve the achievable robustness on them, thus not suffering from negative societal impact.

Rating:
5

Confidence:
4

";0
uz7JaQgAJb;"REVIEW 
Summary:
The submission considers McKean-Vlasov Stochastic Differential Equation models, which are a generalization of the more-familiar Ito processes. The difference is that the former additionally feature the evolution of the law of the process $X_t$ (denoted $p_t$) as well as $X_t$ itself. Such processes are the limit as the number of particles in a system approach infinity, and their finite-particle approximation (where $p_t$ becomes some empirical distribution) give rise to a model that exhibits temporal as well as between-particle interactions. 

A key aspect of the work is to posit a model whereby $p_t$ enters only through a term $\mathbb{E}_{y_t \sim p_t}[\varphi(X_t, y_t)]dt$ for some *interaction function* $\varphi$. The advantage of such a formulation is that the transition density of the process then reduces to the solution of a PDE.  The work discusses the properties of MV-SDEs of the aforementioned form that make them a desirable class of models (e.g., non-local interactions between paths, and the ability to incorporate jumps). 

Three neural architectures are proposed, differing in particular for their approach as it relates to the modelling of $p_t$: 
* Implicit Measure: which recasts the empirical measure as a single layer of a neural network (though I did not completely understand how this happens). The implicit measure architecture has the advantages of being able to cope with the missing data setting.
* Empirical Measure: where $\varphi(\cdot, \cdot)$ takes the form of a trainable neural network. 
* Marginal Law: which involves learning a generative model.  

Methods for parameter estimation are presented (Section 4), including methods for the missing data setting based on Brownian bridges. A result is proven (Proposition 5.1) regarding implicit regularization properties. A numerical study is conducted on a number of simulated data examples, real data (for which forecasting is also explored), as well as a generative modelling task. 



Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
* The ideas contained in the submission are highly non-trivial, yet explored in impressive depth from a number of different angles in a sophisticated manner.
* The study of such techniques is impressively comprehensive in terms of both theory and different approaches (giving a number of methodological approaches, as well as some theory). The submission appears to represent the outcome of a significant body of work and investigation. 
* The submission is very well written and presented. The numerical experiments appear well-executed. 

Despite some familiarity with concepts in the submission, I am not an expert on SDE modelling, so can not particularly comment on the novelty in that regard. However, the ideas in the submission appear like ones that are very natural to explore, have been explored well, and are worthy of publication in my opinion. 


Weaknesses:
* As much as I enjoyed the abstract presentation, it would be beneficial to have additional clarity as to settings where MV-SDE modelling may be of interest, so to contextualise the work, or where different approaches would be preferred. It is mentioned that few works have considered such models in machine learning tasks, and it would be beneficial to have a small discussion what the contribution could potentially be there (at a high level, that is). 

* Related to the above, a clearer motivation of the task at hand would be beneficial. The paper takes an SDE-first viewpoint, but isn't the overall goal to fit some sort of particle approximation to the SDE? Some additional discussion and background would be beneficial. 
 
* Some applications are mentioned at present, but it would be nice to have a small discussion something along the lines of ""MV-SDEs are most useful when the goal of interest is to model... "".

* The derivation of the implicit measure architecture was something that I could not properly parse as currently written. 



Limitations:
A little additional discussion would be beneficial (see other parts of this review for specifics). 

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper considers the problem of parameter estimation from data when the underlying dynamical system is modeled by the MV-SDE. To represent the target MV-SDE, the authors propose two strategies: (i) expressing a layer in a neural network as an expectation with respect to a density and (ii) using generative models to capture distributions that generate observations at different time stamp. With these strategies, the authors then propose to conduct parameter estimation via 1. maximum likelihood estimation, 2. using Brownian bridge for estimation, 3. explicit marginal law estimation. 


Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
Please see the discussion below.

Weaknesses:
1. Poor presentation. Overall this paper is obscure and hard to follow. Here are some detailed examples.
* When stating the underlying MV-SDE model in Eq.(2), it is not clear which terms are known and which terms are to be learned. Specifically, do we know $f$ and $\phi$? Since we are interested in the problem of parameter estimation, both terms should be learned from the observations.
* It is now clear how the proposed implicit measure architecture can be carried out: In Eq.(6), it is not clear what $\mathbb{P}_t$ and $\mathbb{P}_0$ are and why we can estimate the Radon–Nikodym derivative $d \mathbb{P}_t/d\mathbb{P}_0$. Moreover, the authors are motivating the implicit measure architecture as an alternative to the standard empirical measure approach since it can handle the situation when only samples from irregular time stamps are available. However, it is not clear why this is the case.
* In section 4.2, where the authors propose to estimate parameters using the Brownian bridge, it is not clearly how the Brownian bridge comes into play. 
* The maximum likelihood estimation in section 4.1 is proposed in previous work [Sharrock et al. 2021], but is presented as a strategy proposed by this work, which is misleading.

2. Lack concrete contributions. Partly due to the poor presentation of this work, this paper presents no clear contributions. For example, the proposed implicit measure architecture and the marginal law architecture are just two ways to rephrase the standard empirical measure.  The results presented in section 5 are well-known results in the literature, e.g. the Wasserstein gradient flow structure of the MV-SDE.

3. A contradiction between the goal of this project and its fundamental assumption. While the authors motivate the research of MV-SDE to model jump (discontinuous behavior) in time sequence data, they also assume that the drifting term of the MV-SDE is sufficiently regular. This is a clear contradiction. For MV-SDE with regular drifting term and interaction term, it can be proved that the characteristic flow is also regular.



Limitations:
Please see the comments above.

Rating:
2

Confidence:
4

REVIEW 
Summary:
The authors proposed two new methods of modelling McKean--Vlasov SDEs using a neural network, and studied its empirical performance. 

Since I'm not an expert this exact topic, I would like to ask the authors some questions first. I would be happy to raise my score further once I understand the paper better.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The authors proposed methods that do not model a finite population of the particles, which is a very interesting alternative.

Weaknesses:
Several parts of the paper seem unclear to me at the moment, and I will ask specific questions next.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a methodology for simulating McKean-Vlasov (mean-field) equations using standard function approximation techniques, e.g. neural networks. It provides mathematical intuition for these algorithms and evaluates them on a broad suite of benchmarks.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is very well written with clear exposition of its main points. The proofs of key claims seem broadly correct as well.

The methodology is broadly well justified and uses very intuitive ideas from stochastic analysis. 

In particular, the adaptation of standard neural network techniques for simulating ODEs/SDEs is not entirely applicable here, and so the derived techniques need to account for the estimation of the particle density. The resulting algorithm is novel and an important independent contribution.

The experiment evidence, especially in the Gaussian case, seems to vindicate the intuition of this algorithm and clearly outperforms the chosen baselines.

Weaknesses:
I would say that the ultimate idea is rather simple, i.e. approximating both the drift function (both interactive and interaction-free), and possibly the particle density with some kind of learned approximations.

I appreciate the inclusion of standard deviations in the Tables, however these values seem, particularly in Table 1, to be quite large relative to the proposed gains.

I have some additional questions about the methodology. To summarize, I think this paper makes fairly solid contributions to the simulation of McKean-Vlasov equations, which is an important problem. If my questions are addressed, I would be amenable to raising my score.

Limitations:
None

Rating:
7

Confidence:
2

";0
e2aCgjtjMR;"REVIEW 
Summary:
This work provides tight and computable bounds on the EOD violation of a classifier in a setting where the sensitive attributes are unknown. In addition, a post-processing technique is proposed to provably yield classifiers that maximize prediction power while achieving minimal worst-case EOD violations with respect to unobserved sensitive attributes. Experiments are done on both synthetic and real data.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
Significance: The problem under study is interesting and important. It would be useful to be able reduce the upper bound of certain fairness violation in models when sensitive attributes are unknown.

Originality and quality: the proposed solution is novel and technically sound.

Clarity: the presentation is clear with limitations analyzed in details.

Weaknesses:
1. One assumption of the proposed approach is that there exists a dataset of (X,A) to learn $\hat{A} = h(X)$ from. This could limit the use of the approach and should be discussed.

2. It would be beneficial to demonstrate the verification of Assumption 1 on the two datasets.


Limitations:
Results on the CheXpert data shows that, although the proposed method successfully reduces the worst case scenario TPR and FPR, it may actually increase the true TPR or FPR. Figure 3b shows an increase of $\Delta_{FPR}$ of $f_{opt}$ over $f$. This should be discussed as the potential outcome of applying $f_{opt}$ can be a less accurate and more biased model.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper explores the fairness problem in machine learning when sensitive attributes (e.g., demographics and gender) are unavailable for practitioners. The authors focus on the Equalized-Odds (EOD), a well-known definition of fairness for classifiers. The authors provide bounds for EOD violation in a setting where sensitive attributes are unavailable. Additionally, they propose a post-processing method to control the worst-case EOD. Finally, the authors illustrate their results via synthetic and real-world datasets experiments. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* The paper is very well written. Particularly the related work section is very informative.

* The authors explore an interesting problem for the community and propose a different view of the problem, i.e., a worst-case optimization. 

* The theoretical guarantees provide a method for practitioners to access the quality of the method (assuming a worst-case scenario).

* The proposed method is simple, intuitive, and computationally inexpensive. 


Weaknesses:
* The evaluation in the main paper is limited. The authors only apply their method to one real-world example. I suggest adding at least two more examples and as many examples as possible in the appendix. These examples can convince us that: (i) Assumption 1 is followed in multiple scenarios and (ii) your method helps to improve fairness. 

*  The fact that the Naïve approach had a better ""true"" EOD is interesting, and it might indicate that your method is not very efficient, i.e., using the sharp upper bound as a proxy for the true value is too pessimistic. Performing more experiments in real-world datasets can help the authors and readers understand when minimizing the upper bound is better than the naïve approach. 

* It needs to be clarified how to extend the approach to a setting where the sensitive attribute is not binary. In the introduction, the authors list examples of sensitive attributes as ""demographics and gender,"" which are not binary. A subsection is recommended to discuss how your results would change when $A$ is not binary. 


Limitations:
The authors discuss their work limitations. I also suggest discussing the ethics of predicting users' sensitive attributes. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a tight upper bounds for the equalized odds violation of a predictor in a setting without sensitive attributes.
It also presents a post-processing correction method to control the worst-case equalized odds violation and presents results on a variety of synthetic and real datasets.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper addresses a critical issue in fair ML. 

2. The tight and computable upper bounds for equalized odds (EOD) violation is a valuable contribution as it allows for a precise understanding of the worst-case EOD violation of a predictor.

3. The proposed post-processing correction method for controlling worst-case EOD is interesting.

4. The paper is backed by experiments on both synthetic and real datasets, strengthening the validity of the results.

Weaknesses:
1. Assumption 1 may limit its applicability in scenarios where the proxy sensitive attributes are not accurate.

2. The paper's results are limited to EOD and its relaxations as definitions of fairness. Extending the results to other notions of fairness may be non-trivial and deserves further consideration.

3. The paper does not consider how one could train a classifier from scratch to have minimal violations, which could be an important 


Limitations:
The paper relies on Assumption 1, which may limit its applicability in scenarios where the proxy sensitive attributes are not accurate. 
Despite this limitation, this work is a nice contribution.

Rating:
7

Confidence:
3

";1
etd0ebzGOG;"REVIEW 
Summary:
The paper proposes an efficient conditional 3D generation via voxel-point progressive representation. More specifically, a voxel semantic generator and a point upsampler have been created to achieve efficient generation on multi-category objects. To verify the effectiveness of the method, extensive experiments with SOTA results are achieved. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-written and well-organized. 
2. Extensive experiments are conducted, and impressive results are obtained. 

Weaknesses:
1. It seems that the paper can address point upsample task instead of upsample completion task, since upsample refers to generate dense points while completion refers to synthesize new points given partial scans.  
2. It would be better to show some ablation studies on the network architectures such as removing or replacing specific components to see how they affect the performances.  
3. It would be clearer to indicate Tab .3 in line 234 for the corresponding quantitative results. 

Limitations:
The authors have adequately addressed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work proposes to use a voxel-point progressive representation for efficient 3D generation, and it proposes a few architectures for different applications, including generation, editing, upsampling, and pre-training. Based on the reported results, the proposed method could generate various 3D shapes and could achieve competitive classification results.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The work proposes a voxel-point progressive representation, which could provide good 3D generation results on the ShapeNet dataset as shown in the reported experiments. 

2. Many modules, such as 3D VQGAN, Voxel Semantic Generator, Grid Smoother, and Point Upsampler, have been proposed.

Weaknesses:
1. The proposed method could only generate 3D shapes belonging to the categories of the ShapeNet dataset.  It does not show any 3D shape results from unseen categories even with the help of CLIP.

2. Point-Voxel representation has been broadly studied in previous methods, such as [A-C], which has already been proven to be an efficient representation for 3D point cloud analysis.

3. The classification results for both the ScanObjectNN and ModelNet40 datasets are very saturated.  Also, the improvements seem to be very incremental.

[A] Liu, Z., Tang, H., Lin, Y., & Han, S. (2019). Point-voxel cnn for efficient 3d deep learning. Advances in Neural Information Processing Systems, 32.

[B] Zhang, C., Wan, H., Shen, X., & Wu, Z. (2022). PVT: Point‐voxel transformer for point cloud learning. International Journal of Intelligent Systems, 37(12), 11985-12008.

[C] Liu, Z., Tang, H., Zhao, S., Shao, K., & Han, S. (2021). Pvnas: 3d neural architecture search with point-voxel convolution. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 8552-8568.

Limitations:
yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
Authors propose an approach to generate 3D point clouds of objects with an image or text description as input. Authors use a pre-trained CLIP model to generate text/image embeddings and use this to first generate features in voxel space (Voxel Semantic Generator). These voxel features are then decoded into a coarse voxel grid. Authors then convert the voxels to point clouds and use a smoothing network to obtain a uniform point cloud. Authors use a transformer to then convert this point cloud into a detailed 3D shape (also point cloud).
Authors show qualitative and quantitative comparison with relevant baselines (CLIP-Sculptor, CLIP-Forge and Point-E).
Authors also show several applications like text and image conditioned 3D generation, shape editing and completion.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Ideas presented in the work are technically sound.
+ Presented results outperform competing baselines.
+ Authors submitted code. Although I did not run it but this is still appreciated. Code will also be released upon acceptance.
+ Paper is mostly well written. See comments below for minor improvements.

Weaknesses:
[Technical]
1. How is the proposed Voxel Semantic Generator different than “CLIP-Conditioned Coarse Transformer” from CLIP-Sculptor? They are essentially doing the same thing, 1. Learn a voxel based encoding of 3D shapes and 2. Use a transformer to learn how to unmask the 3D features conditioned on a prompt.
Can authors clarify this better?

2. L175-178: Since the GT for Grid Smoother is generated using furthest point sampling, why can’t we use FSP at inference time to smoothen the coarse voxel grid from the 3D VQGAN decoder?
Why do we need to learn a neural network?
How useful is adding KL loss over Chamfer loss?

3. L180: The output of the grid smoother is a point cloud (I assume this because it is trained with a Chamfer loss). L169 mentions that “point tokens” are masked which are then unmasked by the transformer. What exactly are the “point tokens”? Is it just the positional encoding of points?
Fig.2 (b) mentions “semantic tokens” what does this mean? It is not explained in Sec 3.3 (Point Upsampler).

4. Eq. 1: How useful is the occupancy loss over MSE? Is this critical? Please add an ablation study in supp. mat. to support the proposal.

[Minor]
- Fig. 2: Please add symbols used in the text to Fig.2. This makes it easier to follow the text and map various components of the pipeline.
- Add dimensionality of each embedding/latent code along with the symbols, eg: C_{pr} \in \mathbb{R}^{<whatever>\times<whatever>}. This significantly improves clarity and readability.
- L120: How is the mapping performed? How is the codebook generated? If an existing work is used, please add citation here? This is present in the supp. mat. to an extent. Add a pointer so that the reader knowns where to look.
- L140: Just curious, how important is the scheduling of the mask fraction?
L147: Is there significant performance difference at inference time between direct and multi-step prediction?
- L234: Table number is missing.
- Please avoid violent objects like guns for showing qualitative results. It is understandable that it is sometimes necessary to demonstrate performance but whenever possible, let us try to avoid this.

Limitations:
Discussion on Limitations and Broader Impact is not included in the main paper but it is present in the supp. mat. Authors are encouraged to add some discussion on limitations and future work in the main paper as it allows the community to better use and build upon your research. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The VPP proposes a model for 3D generation. It utilizes both point-based and voxel-based representations. Voxel-based representations are used to generate the coarse tokens, and the point-based one further improves the result. Both of which are pretrained with MAE-like self-supervised method.

The proposed method applies to image-to-point, text-to-point, and point completion. The propsed method is novel and effective. However, some parts are not presented clearly.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
1. The limitations of existing methods are well analyzed.
2. The proposed method is carefully designed and performs well.
3. The proposed method applies to more downstream tasks than exsiting methods.
4. The experiments show the effectiveness of the proposed method.

Weaknesses:
1. The inference efficiency in Table 1 only presents hours or seconds, it is recommended to provide the exact time.

Totally, the paper is not well presented. Some details are not shown clearly. To be specific,
2. The tokenizer for points upsampler is not described. What is it structure and how to train it?
3. It would be better to refer to some important symbols in Figure2. For example, prompt embedding in Figure 2 should also be marked as C_pr.
4. The GAN's structure and loss in 3D VQGAN are not introduced.
5. As described in Section 3.2, the mask voxel transformer will be forwarded multiple times for better quality. It should also be commented in Figure 2 or 3 for better presence.

Some typos:
6. In about line 151: ""when the inputs of the mask voxel transformer are grid tokens with high masking fraction and prompt embedding we find that this will lead to the semantics of the predicted grid tokens being much more biased toward the prompt embedding"". I think the authors may miss punctuations.

Limitations:
Not applicable

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper proposed a text-driven point cloud generation model that can be used for various downstream tasks such as generation, editing, 
 completion and pretraining, while being very efficient. The method largely follows Muse [2], but adapted it to 3D point clouds. The model consists of multiple components, which are trained individually. First, a VQGAN is trained on the voxelized shapes, which embed a shape to a latent grid. Next, a masked transformer is trained on the latent grid to model its distribution, using CLIP feature as semantic conditioning. For the decoding stage, a grid smoother network moves the voxel centers to more homogeneous locations, while finally another set of point cloud VQVAE and masked transformer is trained to upsample the coarse point cloud to high resolution point cloud. The paper also demonstrated that the proposed components can be used in whole or in part to tackle a wide range of tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposed method is able to handle a wide range of tasks such as generation, editing, completion and pretraining.
* VPP significantly outperforms baseline methods on the main task of shape generation.
* The proposed method is computationally efficient compared to optimization-based methods such as DreamFusion.

Weaknesses:
* The components of the proposed method are not new -- they are mostly borrowed from previous works such as CLIP, MUSE and Point-MAE.
* The method contained a large number of stages, making it potentially difficult to implement or improve upon.
* Unlike diffusion-based methods such as DreamFusion, Dream3D, Magic3D, the proposed method does not generate surface or texture. It is also not clear if the proposed method is able to do zero-shot generation like these methods.
* The proposed method is only trained on ShapeNet, which is relatively small and less diverse compared to larger datasets such as Objaverse. It is not sure if the method is able to scale to larger datasets.

Limitations:
Limitations and societal impacts are adequately addressed in the supplemental material.

Rating:
6

Confidence:
4

";1
9SwKSvaCiP;"REVIEW 
Summary:
The paper proposes SING, a simple gradient preprocessing technique that, combined with any optimizer of choice, argues for improved stability and generalization. The paper further provides a theoretical convergence analysis of the approach 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is clear
2. The technique is simple and easy to implement.


Weaknesses:
Overall the paper proposes a straight forward extension to the gradient centralization method, where the gradients are also normalized in a pointwise fashion as in other adaptive techniques. The main weaknesses of the paper are:

1. Incremental - i do not think the contribution of this paper merits publication due to its incremental nature. Adaptive optimizers already normalize gradients in a similar way and it is not clear what is added in the proposed method. 

2. Non-convincing experiments - Only 1 experiment compares gradient centralization + AdamW (GC + AdamW), which was proposed in [1], which is the closest method to the one proposed in the paper.  By that experiment GC + AdamW already achieves approximately the same performance, hence stripping SING from any practical significance. I do not understand why GC + AdamW is not used as a baseline for other experiments as it clearly shows strong performance. At it stands, it is not clear whether the apparent improvement of SING stems from the GC part of SING, or the added normalization which constitutes it novelty. I would encourage the authors to add this ablation study to the paper to make it more convincing. Finally, results in the paper do not include standard deviation which is be a must for an empirical paper.

3. The theory can be equally applied to other adaptive optimizers, hence it is not special to SING




[1] - Yong et al - ""Gradient Centralization: A New Optimization
Technique for Deep Neural Networks""

Limitations:
Limitations are adequately addressed. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposed a simple and hyper-parameters-free way to improve the stabilization and generalization properties of optimizers used in deep-learning scenarios. They show that with gradient centralization and gradient normalization methods, SING can escape the local minima with large step sizes theoretically. The authors provide several experiments on datasets like ImageNet-1K, RDE, and some NLP tasks to show the superiority of SING together with popular optimizers like AdamW. They also give out some other theoretical results like convergence and invariance properties.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The experiments on real datasets show that SING+AdamW performs significantly better than other baselines at image classification, depth estimation, and NLP tasks. This efficient method is also simple and not requires additional hyper-parameters.
2. The authors show that SING can escape the basin of attraction of the critical point when the step size is sufficiently large, and the stepsize threshold is inversely proportional to the network's depth, while GD cannot. And the experiments result (Figure 4) show that SING can stabilize the performance of the optimizers like AdamW.
3. The paper is well-writen and easy to follow.

Weaknesses:
1. Although the empirical results are remarkable, the novelty of this paper is limited. As mentioned in the paper, gradient centralization[1] and gradient normalization[2,3,4] are common methods in the previous works, and this paper combines these two methods and systematically investigates the properties of SING.
2. The theoretical analysis just focuses on the gradient rather than combining it with momentum and scheduler, but since the gradient is normalized and the step size is large, the momentum and learning rate scheduler is critical for the global convergence, like in Thm 3.3, 3.4, $\eta$ is small, but $\eta$ needs to be large to escape local minima from Thm 3.1.
3. Thm 3.3, 3.4 requires that the mini-batch size B be some concrete value, this is too strict and it's better to relax this assumption.

[1] Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and Lei Zhang. Gradient centralization: A new optimization technique for deep neural networks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 635–652. Springer, 2020
[2] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International conference on machine learning, pages 2260–2268. PMLR, 2020.
[3] Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast evasion of saddle points. IEEE Transactions on Automatic Control, 64(11):4818–4824, 2019.
[4] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized gradient descent. Science China Information Sciences, 64:1–13, 2021.

Limitations:
The author mentions that their method cannot be used together with LayerNorm or LayerScale, and the theoretical results not incorperate with AdamW.
I don't find ethical or immediate negative societal consequences in this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, authors have proposed a method (called SING) for stabilizing the optimization algorithms used in training of deep models. The proposed method is based on only a layer-wise standardization of the gradients without introducing any additional hyper-parameters. In addition, a theoretical analysis for convergence to a stationary point is provided. Extensive empirical simulations show improvement of the training performance when the existing optimization algorithms use the proposed approach on various tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
In general, the paper is well-written, and concepts have been presented in an accessible way. Providing theoretical results, including the convergence and invariance properties provide more credibility to the proposed method. Furthermore, experiments on different architectures and on various datasets is another strength point of this paper.

Weaknesses:
I need some clarifications on the followings:

1 - As mentioned in the theorems 3.3 and 3.4, convergence is guaranteed only to a stationary point. On the other hand, Theorem 3.1 states that the algorithm can escape from a narrow local minimum. How can SING guarantee that the stationary point is a local minimum (what happens if the algorithm converges to a saddle point or even local maximum) ?

2 - What defines the narrow local minimum and the wide local minimum. There is no curvature information/notion in Theorem 3.1 to distinguish local flat minimum from the sharp one.

3 - In Theorem 3.3, $\epsilon^2$ is given by $\sigma^2/B$, so to have an arbitrary small error on the expectation of the gradient at some stationary point, $\sigma$ should scale as $\mathcal{O}(\frac{\sqrt{B}}{D})$. My question is for a very large model, (i.e., $D$ is huge), does the assumption (8) hold for every $x\in\mathbb{R}^p$? I am not sure how the assumption holds for a highly non-convex loss in a large deep model? This assumption is stronger assumption than other approaches. Typically, ADAM, and other optimization in deep learning either assume some level of convexity or use somehow reasonable assumptions like small gradient, or bounded sequence of estimates, etc.,

4 - Regrading the previous point, it is a good idea to run an experiment to illustrate the effect of $D$ on the training performance with SING.

5 - Do the results in experiment section (Table 1, 2, and 3) show the validation accuracy or the training accuracy ? Please clarify this.

6 -The convergence result doesn't provide any insight for the generalization to the unseen data. It is a purely an optimization perspective.

Limitations:
Please see my comments for Weaknesses and Questions.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents SING (StabIlized and Normalized Gradient), a new method designed to enhance the stability and generalization capabilities of the Adam(W) optimizer. SING involves a layer-wise standardization of the gradients that are input into Adam(W), and does not require the introduction of additional hyper-parameters. This makes it straightforward to implement and computationally efficient.

The authors demonstrate the effectiveness and practicality of SING through improved results across a broad range of architectures and problems, including image classification, depth estimation, and natural language processing. It also works well in combination with other optimizers.

In addition to these experimental results, a theoretical analysis of the convergence of the SING method is provided. The authors argue that due to the standardization process, SING has the ability to escape local minima narrower than a certain threshold, which is inversely proportional to the depth of the network. This suggests that SING may offer significant advantages in training deep neural networks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. As the authors have claimed, the proposed method can be applied in a plug-and-play style with impressive applicability to a lot of tasks, datasets and optimizers. Without additional hyperparameters introduced, I think this work has great potential to become a standardized training technique with big impact in the community. And the authors did provide the elegantly implemented source code in PyTorch which I think is already close to ready to be included in the standard PyTorch library.

2. Empirical performance is impressive with huge improvements over baseline optimizers in many settings.

3. Mostly the paper is written in quality and easy to follow with only a few ambiguities, which I will mention in the weaknesses part.

Weaknesses:
1. Firstly, I believe there is a misalignment between the theoretical analysis and practical method. To be specific, the analysis in Theorem 3.1 compares the learning rate needed for escaping local minima for SING and SGD. However, as the authors claimed previously, the SING algorithm is proposed to overcome the limitations of Adam(W). Therefore, it would be better to directly analyze SING against Adam(W), which is also mainly compared against in the experiment part.

2. Some issues in terms of writing. One is that the authors did not formally formulate the centralize operation in math equations but only in codes, which could be confusing for readers who are not familiar with PyTorch framework. I strongly recommend the authors to provide strict math formulations instead of ambiguous codes only. For example, at least I am still confused the mean operation is executed over which dimension and what the meaning is for that averaging. A second issue in writing is that it seems the authors interchangeably use the terms ""learning rate"" and ""step size"" in Section 1 but treat them as different things in Section 3. I hope the authors could clarify the differences or use one term consistently.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes SING, a plug-and-play approach to enhance optimizers without introducing additional hyperparameters.

The idea consists of standardizing gradients in a layer-wise manner prior to the host optimizer’s execution, and is motivated by factors such as easier escaping of narrow minima and invariance properties.

Experiments on image classification, depth estimation, and NLP tasks such as NMT and QA are used to assess the proposed method’s performance and measure improvements over host optimizers.


Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
The paper is written clearly and well-organized.

The proposed method is easy to implement and works in a plug-and-play fashion. The layer-wise gradient standardization can be viewed as a gradient pre-processing step and hence completely agnostic to the host optimizer, making the approach general and flexible.

The authors provide theoretical analyses to motivate and better understand SING. The convergence analysis considers the smooth non-convex bounded variance setting, which I believe to be a good balance between assumptions and how well it captures network training.

Experiments include different tasks from two domains and distinct network architectures, including ResNets and Transformer-based models.


Weaknesses:
The theoretical analysis is not helpful in motivating or better understanding SING’s benefits.

While SING adopts layer-wise normalization, it seems that the analysis holds given any partition of the $p$ many parameters in $D$ many sets – i.e. the fact that each of the $D$ tensors is assumed to correspond to a different layer is not necessary nor used anywhere in the analysis. We can then consider the effect of variable $D$ for a fixed $p$ (grouping the parameters in larger or smaller sets, say filter-wise, kernel-wise, or even parameter-wise), and we recover Normalized Gradient Descent (NGD) with $D=1$ and a form of sign SGD with $D=p$ (this has been studied in previous works to understand how normalizing gradients of coarse/fine-grained parameter sets can affect performance and convergence).

This has two concerning implications:

- For Theorem 3.1

Since $||\frac{g}{\Gamma (g)}|| = \sqrt{D}$, it follows that the post-processed gradients will scale (in norm) as $\sqrt{D}$, and hence $\eta_{SING}$ in Theorem 3.1 is fundamentally ‘undoing’ this scaling, hence claiming that ‘SING can escape local minima narrower than a threshold that is inversely proportional to the network’s depth’ is not very meaningful.

A similar argument would be to pre-process the gradient by scaling it up by 100 and claiming that now we can use a 100x smaller learning rate, which, although technically correct, is not useful.

While I’m aware that the layer-wise normalization will actually change the update direction and not just scale it, it seems that this change in direction does not play any positive role in the presented analysis.

Finally, one could simply set $D=p$ (i.e. artificially view each parameter as an independent layer) and Theorem 3.1 would state that the sufficient learning rate to escape narrow minima would actually decrease way more aggressively (as 1/sqrt(# parameters) instead of 1/sqrt(# layers)) – this is clearly not actually useful since we’re just scaling up the (norm of) post-processed gradients (compared to the layer-wise case) and compensating by scaling down the learning rate.


- For Theorems 3.3. and 3.4

To achieve stationarity $\delta$ independent of $D$ (i.e. $\epsilon \propto \frac{\delta}{D}$) we would set $\eta = \Theta(\frac{\delta}{D})$, $T = \Theta(\frac{D^2}{\delta^2})$, $B = \Theta(\frac{D^2}{\delta^2})$, where we are ignoring dependencies on $F(x_0)$ and $L$.

This means that, in the original case where $D =$ # layers, the guarantee requires both the number of iterations and the batch size to increase quadratically with the depth of the model, which is concerning.

Moreover, if we set $D=1$ (i.e. NGD) we actually minimize the required number of iterations and batch size. Therefore, these results do not motivate or support layer-wise normalization, and actually question this design choice by offering significantly better guarantees for NGD.

- Other points

Although the theoretical analysis considers updates following Eq. 2, the experimental studies heavily focus on AdamW + SING, which is not well-discussed. My main concern in this case is that the normalization from SING affects both $m_t$ and $v_t$ in the numerator and denominator of AdamW, respectively. It is unclear what is really happening in this case.


It seems that for long enough training time windows, if the layer-wise gradient norms remain roughly constant then the normalization effect would cancel out, reducing to AdamW’s updates (that is, if the $\epsilon$ term in the denominator of AdamW is negligible). Accounting for $\epsilon$, on the other hand, yields AdamW’s updates except with different values for $\epsilon$ for each layer, each scaled by the layer’s gradient norm.

Although it is unlikely that layer-wise gradient norms remain roughly constant for many enough iterations, this hints that SING’s normalization might be affecting the size of $\sqrt(v_t)$ compared to $\epsilon$ differently for each layer. This could result in confounding effects since the value of $\epsilon$ (compared to $\sqrt(v_t)$) can play a major role in the behavior of Adam-like methods, potentially improving the performance in multiple settings.

The experimental analysis could also be substantially improved. The hyperparameter tuning strategy (choosing best learning rate, then fixing it to choose best weight decay) can easily lead to suboptimal values, especially for SGD and any adaptive method that does not inherently incorporate AdamW’s weight decay decoupling (see Fig. 1 and 2 of Loshchilov & Hutter).

This can lead to an unfair advantage to AdamW and AdamW + SING over all other methods. The cosine schedule is also known to improve AdamW’s performance and more often than not harm SGD (compared to step-wise), hence it would be valuable to also collect results with a step-wise schedule for a more comprehensive and clear comparison.

There is also some loss in novelty from the fact that the actual method that plays the key role in the experiments also adopts LookAhead and softplus calibration. In particular, centering is not novel although it has been explored more extensively for the 2nd moment estimate (centered RMSProp, AdaBelief, SDProp, ACProp, etc), and layer-wise gradient normalization for adaptive methods has also been studied (AdaShift & AvaGrad – none of the two are cited or discussed). These methods should be included in the comparison to have a clear picture that would allow a proper assessment of SING.

Finally, there are also concerns regarding the other vision tasks. It is unclear what was the exact ResNet-18 model used for CIFAR-100: if it is a ~11M param model, then it is a wider version (DeVries ResNet-18) which differs from the one originally proposed by He et al. and achieves over 77% acc. on CIFAR-100 when trained with SGD (see LookAhead’s paper and DeVries&Taylor).

This would indicate issues with the CIFAR-100 results in Table 1, since results with SGD would be ~1.4% worse even with additional augmentation and 100 extra training epochs. As for depth estimation, the dataset is synthetic and not well studied, hindering a proper assessment of its results. Nonetheless, ViT’s are typically well-trainable with SGD if warmup and gradient clipping are employed (which is common practice for these models).

The fact that SGD achieved 0.25% accuracy suggests that the experimental setup should be revised – warmup and grad clipping should be adopted for SGD since they are common practice, especially if SGD only achieves 0.25% accuracy without them.


Limitations:
The authors discuss limitations satisfactorily.

Rating:
3

Confidence:
4

";0
3o4jU8fWVj;"REVIEW 
Summary:
The authors propose EquiformerV2, which is an improvement over the original Equiformer architecture. The main improvement is using a more efficient parameterization of the tensor products used in Equiformer which is computationally expensive for higher order representations. The more efficient parameterization involves using SO(2) linear layers instead of SO(3) tensor products. Moreover, they have three architectural improvements – adding an extra layer norm during attention, S2 activations instead of gates and ""separable"" layer normalization which differs from the previous layer normalization in terms of the denominator used. They perform experiments on OC20 to show improvements compared to other models in the literature. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Overall, the idea of using computing tensor products more efficiently using SO(2) linear layers makes good sense in its application to Equiformer in order to scale it up via higher order SO(3) representations. Empirical work around improving the architecture by appropriate use of different type of layer norms and activation functions is also valuable. The components feeding into this model aren't always original, but that shouldn't detract from the significance of putting together the full model and showing that it improves performance on a benchmark. The clarity of the writing is usually good but there are places where it can be improved (see below).

Weaknesses:
The main weakness is insufficient comparison against the original Equiformer architecture. There are four differences versus the V1 architecture as far as I can tell: (1) the more efficient parameterization of tensor products via the ideas of eSCN, (2) an extra layer norm applied to scalar features in the attention module, (3) separable S2 activation instead of gates and (4) layer normalization with a different way of computing the denominator (called ""separable"" layer normalization in the paper). 

- There is an ablation in Table 1(a) on one of the datasets looking at the effects of changes (2), (3), and (4). It is unclear how much of the difference in performance (specially, on energies) is statistically significant. I understand training each model is expensive, but since the experiments in this table are on the smaller S2EF-2M dataset, isn't it possible to have error bars here?
- Why isn't there a comparison in Table 1(a) of the effect of incorporating change (1) in EquiformerV2? The SO(2) linear layers replacing the tensor product are in principle as expressive as the tensor products but the optimization dynamics can be different since the parameterization of the architecture is different. It would be good to see a side-by-side comparison of Equiformer's V1 and V2 when keeping other architectural hyperparameters fixed (e.g. number of channels, maximum representation order etc.)
- It would be good to include EquiformerV1 performance in the other sub-tables of Table 1 too, in order to see the cummulative effect of the changes to the original architecture.
- Similarly, it would be useful to see EquiformerV1 performance in Table 2 and 3 as well (possibly with a lower $L_\text{max}$ but with other hyperparameters tuned).
- How does the setup for IS2RE in Table 2 differ from EquiformerV1's Table 3? Is one with relaxations after training on S2EF and the other is through direct energy prediction? A side-by-side comparison under the same setup would help a lot here. 
- How necessary are the higher orders compared to increasing the number of layers or channels? SO(2) linear layers instead of tensor products will improve the efficiency for lower orders too, so can we make up for the performance of higher orders by instead increasing the number of layers or channels?
- Have you benchmarked EquiformerV2 on QM9 where EquiformerV1 has already been benchmarked? It's a dataset that is different from OC20 in various ways, so it would be useful to see a comparison.

For other suggestions for improvements, see the questions below. 

Equiformer(V1): https://arxiv.org/abs/2206.11990

Limitations:
Limitations and broader impact are adequately addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper provides a new equivariant graph neural networks named EquiformerV2 to enhance the original Equiformer performance. It uses four new modules. The first module is to use the convolution in the eSCN (https://arxiv.org/abs/2302.03655) to replace the depth-wise tensor production accelerating the speed. The second module is the separable $S^{2}$ activation using the spherical grids in SCN (https://arxiv.org/abs/2206.14331) to encourage the non-linearity. The third module is the separable layer normalization which uses the variance of all equivariant feature $\ell \geq 1$ to do normalization. The fourth module is the attention re-normalization using a layer normalization before the non-linear function of the attention score branch.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well organized and written. The figure clearly shows the modification on the model architecture.
2. The experiments in Table 2 supports the proposed model architecture can achieve SOTA performance on the OC20 All training sets as well as OC20 All+MD. For example, on OC20 All, test energy MAE is improved 13meV in S2ET test and 25meV in IS2RE test. Such improvement is great. These two datasets usually take extensive training time to perform experiments on them. From the Throughput metric, the EquiformerV2 has better training efficiency compared to current baselines.


Weaknesses:
1. As a suggestion, it will be better if efficiency study includes both training time and inference time. Computational complexity metric such as FLOPs or MACs can help measure the inference complexity.
2. The description of spherical grid is not very clear in the paper. Although it is introduced in the SCN paper, I think a brief introduction can help people understand why such operation can enhance the non-linearity.


Limitations:
1. As an suggestion, to comprehensively study the improvement on original Equiformer, it will be better if there is a comprehensive experiments on the original datasets such as QM9.


Rating:
5

Confidence:
5

REVIEW 
Summary:
In this paper, the authors proposed EquiformerV2, which is an equivariant network for 3D molecular modeling. The EquiformerV2 is built on the Equiformer with several architectural modifications: 1) replace SO(3) convolutions (tensor product operations) with efficient SO(2) counterparts from eSCN; 2) Attention Re-normalization; 3) Separable S2 activation; 4) separate Layer Normalization. These changes enable EquiformerV2 to achieve good performance on the large-scale OC20 benchmark and also the new AdsorbML dataset.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The targeted problem is of great interest to the community. The EquiformerV2 provides another attempt to enlarge the maximum degree of irreducible representations and obtain performance gains on large-scale DFT benchmarks.

2. Good empirical performance. On the OC20 benchmark, the EquiformerV2 achieves state-of-the-art performance on the Structure-to-Energy-Force task. The model trained on this task further serves as a force-field evaluator to achieve strong performance on IS2RS and IS2RE tasks. The EquiformerV2 outperforms the compared baselines on all these tasks, especially on force prediction. Additionally, it also improves the success rate a lot on the AdsorbML dataset.


Weaknesses:
1. The novelty of integrating the eSCN convolution and S^2 activation into the Equiformer is limited. Among the proposed architectural changes, the eSCN convolution is the key component to enable Equiformer to use irreducible representations of higher degrees, and the S^2 activation also replaces all non-linear activations. However, these design strategies should be mainly credited to the eSCN work.

2. The motivation for the other architectural modifications should be thoroughly clarified. First, lines 174-175 in Section 4.2 suggest the ""less well-normalized"" issue, which motivated the authors to propose re-normalization and Separable Layer Normalization. It is better to provide further quantitive evidence to reveal how such an issue affects the model's performance, and why these modifications could remove or mitigate such effect. Second, the authors proposed Separable S^2 Activation because the original S^2 activation would make the training process diverge. However, it is hard to understand why such separable modifications could make the training process stable. Is there any further essential reason behind such a phenomenon? It is suggested to provide further analysis on such modifications.

3. More analyses are necessary if the computational resources are acceptable. First, the authors did not provide a performance comparison between whether using the eSCN convolution. Although the eSCN convolution is equivalent to the SO(3) counterpart, the computation processes of these two parameterizations are different. Second, it is suggested to further report the number of model parameters and memory costs for the EquiformerV2 and the compared baselines in Table 2. Third, how does EquiformerV2 perform on the IS2RS and IS2RE using the direct setting (use or not the denoising setting) that is the same as EquiformerV1?

Overall, the major weakness of this work lies in the novelty, unclear motivations, and incomplete analyses. If the authors could well address the above concerns, I would like to increase my scores.



Limitations:
The authors carefully discuss the broader impact and limitations of this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper propose EquiformerV2, a advance verison based on Equiformer and eSCN structure extend to higher degree representations, which achieve better performance in force and energy tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written and organized, presenting a clear and coherent structure throughout. The introduced EquiformerV2, an upgraded version of the original Equiformer, including three architectural improvements: attention re-normalization, separable S^2 activation, and separable layer normalization. These enhancements contribute to the SOTA performance in  OC20 dataset. 
The proposed model achieves a high degree representation with efficiency, as outlined in the paper. The authors also provide a comprehensive ablation study to support the necessity of these modifications, effectively highlighting their respective contributions to the overall performance improvement.

Weaknesses:
* A few spelling errors. For instance, in Section 6, the word ""acknolwdge"" etc.  Along with any other mistakes found throughout the manuscript.

* The experiments conducted in this study primarily utilize the OC20 dataset. While this dataset is relevant, it is essential to note that there are various other DFT-based datasets available that could provide a more comprehensive evaluation of the proposed architecture. Such as OC22, OQMD[1,2], SPICE[3], and PCQM4Mv2 etc.

[1]  Saal, J. E., Kirklin, S., Aykol, M., Meredig, B., and Wolverton, C. ""Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD)"", JOM 65, 1501-1509 (2013). 
[2]. Kirklin, S., Saal, J.E., Meredig, B., Thompson, A., Doak, J.W., Aykol, M., Rühl, S. and Wolverton, C. ""The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies"", npj Computational Materials 1, 15010 (2015).
[3] SPICE, A Dataset of Drug-like Molecules and Peptides for Training Machine Learning Potentials


Limitations:
same above.

Rating:
5

Confidence:
3

";0
nkfSodI4ow;"REVIEW 
Summary:
This work introduces XYZ Data Efficiency, a framework that combines curriculum learning and data routing techniques to improve data efficiency in training recent large models. In detail, authors implemented an efficient difficulty metric calculation method for large datasets by utilizing map-reduce, on top of which authors tried various curriculum learning techniques. Furthermore, by analyzing the limitations of existing data routing techniques, authors developed random-LTD that drops different tokens for different Transformer layers. Finally, authors demonstrate their XYZ Data Efficiency framework leads to achieve the baseline accuracy with less data, or achieve the better accuracy with the same amount of data.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Developing a general, efficient, and easy-to-use framework for curriculum learning for large models hasn't been explored before to the best of my knowledge. Given the high cost of training recent large models, such library can enable more active research in this field.
2. Their data routing technique (i.e. random-LTD) is thoughtfully designed, and seems to improve the final performance of large Transformers across different tasks.

Weaknesses:
This paper touches on multiple aspects of improving training efficiency of large models, especially from the data perspective, but none of them seem to meet the NeurIPS standard.
1. Firstly, I would argue that random-LTD has almost nothing to do with data efficiency. It looks to me that random-LTD is actually closer to some regularization, particularly dropout [1]. Just because one can achieve the same performance with 2x less data with some regularization techniques (e.g. weight decay), calling them as a ""data efficiency trick"" cannot be justified in my opinion. Since it bypasses some computations of some tokens in some layers, I believe it's closer to a computation efficiency rather than data efficiency trick. From the systems perspective, however, random-LTD consistently hurts the overall throughput as shown in Table 3 & 4 somehow.
2. I doubt the practical utility of map-reduce-based data difficulty calculation. The metrics used in this paper are all offline metrics in that they can be calculated only once before training and can be reused later. While I understand even such preprocessing can take a painfully long time with recent large datasets (e.g. Pile or C4), I don't think the value practitioners will get from this paper would be not so significant. If they can show their framework can be combined with some online or dynamic metrics (e.g. loss value for each token), I would be more convinced.
3. XYZ Data Efficiency framework seems to lack the flexibility and/or modularity, a highly important aspect in the framework. For example, the use of certain CL techniques require specific LR schedules to enjoy the maximal improvement. This essentially means that users get a reduced flexibility in choosing their own LR schedulers. Such entanglement between LR schedulers and data sampling strategies can further harm the user experience when they want to implement their custom data sampling strategies. Overall, my impression is that XYZ Data Efficiency doesn't allow much flexibility for users to try out different things, but rather enforces users to follow their predefined pipeline, in this case, composed of random-LTD and several CL strategies.

To summarize, I find two major framing issues in this paper. First, while CL can be approached from the data efficiency perspective, I believe random-LTD (or data routing) has little to do with data efficiency. Second, I believe XYZ Data Efficiency is more of a combination of two algorithms (i.e. CL and random-LTD) rather than a some general framework due to its lack of flexibility and modularity.

[1] Liu et al., Gating dropout: Communication-efficient regularization for sparsely activated transformers

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the author proposes XYZ data efficiency framework to improve the data/training efficiency in the foundation model training. The proposed framework mainly consists of two techniques, i.e., (1) the efficient data sampling via general curriculum learning library and (2) efficient data routing via random layer-wise token dropping.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The CL-based library is open-sourced and compatible with PyTorch.

* The proposed method achieves considerable training acceleration with minor or no accuracy degradation.

* The author evaluates their method on several different models, including the large language models.


Weaknesses:
* Missing the full term of “CL” in the introduction section (line 44). The first explanation shows in line 64.

* The author argues that the previous methods require changing the data loader, data sampler etc. However, the proposed method still needs to change then as well.

* Besides the TokenBypass, there are also several data routing techniques for foundation model training. E.g., [1] [2]. The author should also include those works for discussion and comparison.

[1] EViT: Expediting Vision Transformers via Token Reorganizations. ICLR 2022

[2] Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision. AAAI 2023

* I think the previous work [2] also explores the efficiency at the data sample-level and data routing level. So, it is probably not appropriate to claim the proposed method is “the first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency …”

* As the author claims the proposed framework is easy-to-use and admits being open-sourced as one of the contributions of this work, it would be better if the author could submit the anonymous code with the supplementary materials.

* Though the proposed method can achieve considerable overall acceleration, it would be great if the author can provide a discussion about the overhead of data sampling and routing part.


Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper draws inspiration from the observation of training costs increasing quadratically with data size, leading to a focus on enhancing data efficiency. To address this issue, the paper presents a framework that optimizes data utilization, improves training efficiency, and enhances model quality. The framework introduces efficient data sampling and data routing methods designed to overcome the challenges associated with data size. Extensive experimental results conducted on various foundation models confirm the effectiveness of the proposed methods, validating their ability to achieve improved data efficiency and overall model performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper introduces a framework that combines general continual learning (CL) techniques with random layerwise token dropping for data sampling and data routing. This framework aims to address the challenges of CL by incorporating the random dropping of tokens at each layer, enabling efficient data processing and routing during the learning process.

2. The effectiveness of the proposed method is demonstrated through experiments conducted on various foundation models. The results highlight the remarkable data efficiency achieved by the framework, showcasing its ability to handle continual learning tasks effectively while maintaining high performance with limited data.

Weaknesses:
1. While data efficiency is recognized as crucial for various tasks, the paper could provide a more comprehensive study and presentation of how the proposed method enhances models across different data sizes. A more thorough investigation and analysis of the impact of the proposed method on models of varying data sizes would contribute to a deeper understanding of its effectiveness.

2. It is worth noting that the paper's verification of the proposed method is limited to four models. Expanding the experimental evaluation to include a broader range of models would provide a more robust assessment of the method's performance and its applicability across different architectures. This would enhance the credibility and generalizability of the findings.

Limitations:
Please refer to the weakness and question part.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. The proposed framework features efficient data sampling, efficient data routing, and an easy-to-use practical framework that is integrated into an existing library. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The targetted application on improving the data and training efficiency is an important problem, especially in the era of large models. This paper makes a practical step in this direction. 

- I like the writing of the introduction and related works, which provides a structured review and highlights the goal of this paper and its difference from other papers. 

- The description of the proposed method is pretty detailed, which can help the reader to have a detailed understanding of how this framework is implemented and part of the time/computation overhead to run this framework. 

- Overall, I would say this paper makes good engineering efforts to make the 

Weaknesses:
- My main concern about this paper is the evaluation observations. Specifically, it seems that under lower-budget training settings (e.g., training with less data and training time), the improvement over the baseline actually shrinks. Will this make this method can only be suitable for teams with a large amount of data and computation resources?

- In the paper, the authors mentioned that previous methods for improving data/training efficiency fail to achieve satisfactory performance under large-scale settings. Is there some numerical evidence for this claim other than the one shown in Table 1? 

Limitations:
The authors may want to address the limitations of this paper in their final version. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces XYZ, a data sampling and routing framework designed to enhance the efficiency of training large transformer models. XYZ incorporates a user-defined curriculum learning metric for data sampling and leverages token dropping to reduce computational overhead. The authors propose random layer-wise token dropping (random LTD) to efficiently apply token dropping per layer, capturing attention dependencies between tokens in intermediate layers with high probability. The framework's effectiveness is validated through experiments on pretraining GPT-3, GPT-3 MoE, and BERT, as well as finetuning GPT-2 and ViT, achieving up to a 12.5x reduction in data/time/cost.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper introduces the random layer-wise token dropping technique, which demonstrates a novel approach to enhance the efficiency of large transformer model training. 
2. The evaluation is across various models of different sizes, including GPT-3, GPT-3 MoE, BERT, GPT-2, and ViT. 
3. The paper provides a comprehensive and detailed account of the training setting used in the experiments. Additionally, the authors present a thorough analysis of the results and observations obtained from the experiments. 
4. The paper shows substantial efficiency gains using the XYZ framework.

Weaknesses:
1. One notable weakness of the proposed framework is its relatively limited performance compared to the baseline when operating at a smaller data scale, as indicated in Figure 6. Further investigation and clarity on the factors contributing to this limitation would be valuable for understanding the framework's practical applicability across various data scales.
2. Despite claims of open-sourcing the XYZ framework, anonymized code or a link to access the implementation is not provided. 
3. The paper does not explicitly address the limitations of their proposed framework or discuss any potential negative societal impact.

Limitations:
The authors have not explicitly discussed the limitations and potential negative societal impact of their work in the paper.

Rating:
5

Confidence:
3

";0
50I7q86igD;"REVIEW 
Summary:
This paper aims to explore the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression, and applies it to predict Loss Given Default. It extends the Deep Evidence Regression methodology to learn target variables generated by a Weibull process and provides the relevant learning framework. By testing on both simulated and real-world datasets in the context of credit risk management, the proposed method exhibits enhanced suitability for applications in which the target variable originates from a Weibull distribution, better capturing the uncertainty characteristics of such data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.This paper innovatively extends the Deep Evidence Regression methodology to learn target variables generated by a Weibull process and provides the relevant learning framework.
2.The article provides a clear and coherent description of the proposed method, including a thorough derivation of the relevant formulas.

Weaknesses:
1.The article contains several mathematical formula writing errors, such as the missing ""dθ"" in line 51 and Equation (16), the missing ""-1"" in Equation (10), and the inconsistencies in Equation (31) and Equation (33) with their original definitions.
2.The figure legends in the article do not indicate which parts of the content reference them or provide explanations for their content.
3.The experimental section lacks clear exposition, such as the specific settings of parameters k and λ for the Weibull distribution, as well as the target or subject for the MSE metric.

Limitations:
The proposed approach serves as a valuable tool for capturing and quantifying uncertainty in cases characterized by Weibull distributions. Therefore, the key to using this method lies in determining whether the data is suitable for the Weibull distribution. Furthermore, the applicability of the proposed method to distributions other than the Weibull distribution requires further investigation.

Rating:
4

Confidence:
3

REVIEW 
Summary:
Authors tackle the problem of uncertainty quantification for predicting credit risks. Concretely, they have applied a scalable UQ-aware deep learning technique, Deep Evidence Regression to predicting Loss Given Default with uncertainty. Authors argue that the conventional methods use for uncertainty quantification are too computationally and memory intensive and therefore they adopt Deep Evidence Regression as their statistical model.


They extend the framework of Deep Evidence Regression to predict targets generated with a Weibull distribution. The original method relies on the normally distributed targets which is not fitting for credit risk applications. Authors therefore re-derive the necessary formulas based on Weibull distributed targets; concretely they focus on log likelihood needed for training and the mean/uncertainty needed for predictions. Beyond that, authors adapt the regularizers found in the original paper to their purpose.

The new method is tested empirically on a synthetic dataset with points sampled from a Weibull distributions as well as a single real-world dataset focused on peer to peer mortgage lending data with recovery rate has been used as a proxy for Loss Given Default. Both datasets were used to test the Weibull-based approach against the original method relying on Gaussian distributed targets. The results show the modification made by the authors indeed helps with lower MSE and NLL on both test and train datasets.


Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. Authors tackle an important problem of uncertainty quantification for risk assesment where robust, and efficient measures of uncertainty are crucial to ensure fair treatment of customers. 
2. Authors provide an expansion of a well established method to make it much more attractive to specialized domains such as risk assesment. This work can be directly useful for practicioners in the field of risk assesment and indirectly useful for researchers in other fields who can adapt the Deep Evidential Regression to work with different, field-specific targets distributions.

Weaknesses:
1. This paper provides an incremental improvement over the original Deep Evidential Regression (DER) work. The derivation of DER with Webull distribution instead of original gaussian target distribution is interesting but the majority of what makes the method impactful remains unchanged. 
2. Authors provide very limited empirical evaluation of their work. The single study with synthetic data provides a good proof of concept but gives little assurance of real-world impact of the work. The single real-dataset study is relatively limited and suffers from some issues: 1/ Authors do not have any baselines beyond the original methods while other methods, for example based on conformal predictions or quantile regression, could be competitive. Any other approach for uncertainty quantification would be useful to gauge the difficulty of the task. 2/ Authors only use a single dataset making the evaluation process less robust, it is possible that this method works well only for this particular dataset rather than for a generic class of problems. 3/ Authors only consider NLL and MSE as metrics for their evaluation, there are may more metrics to quantify uncertainty quantification (such as coverage) that could be used to make the evaluation more robust.
3. The presentation of the initial method is vague, I undrstand that the DER work is presented in its own paper but presenting it in more detail would make this paper more self-contained, especially given the reliance on the original work.

Limitations:
Authors provide a formulation for extending Deep Evidential Regression for problems where targets are Weibull distributed. However, they do not cover the extension of this method to problems with targets sampled from a different distribution. There is only a small class of problems where Weibull is appropriate and authors do not provide evidence that this approach generalizes beyond Loss Given Default estimation. I would appreciate additional real-world experiments thta could show whether this parametrization (choice of target distribution) can work beyond the single dataset chosen in the paper.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper introduces the utilization and extension of deep evidential regression for uncertainty estimation in credit risk prediction. The approach assumes a Weibull distribution for the target variable (e.g., LGD or a synthetic target). The authors modify the evidential regression mechanism to accommodate targets from this distribution, providing equations to illustrate the training process. Simple experiments are conducted on synthetic and real-world peer-to-peer lending datasets, showcasing potential improvements over vanilla deep evidential regression for credit risk management.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The incorporation and extension of evidential regression-based UQ into finance-related problems is the main contribution and the key strength of this paper.

Weaknesses:
Refer to Questions. 

Limitations:
The paper's experimental setup is quite limited and requires further motivation. The demonstrations illustrating how uncertainty quantification can benefit credit risk problems lack sufficient substantiation. Moreover, the paper has a limited exploration of related work, and the quality of references needs improvement. In its current stage, the paper requires significant improvement in terms of better problem motivation and comprehensive qualitative and quantitative evaluations prior to publication.

Rating:
3

Confidence:
4

";0
Gh67ZZ6zkS;"REVIEW 
Summary:
This paper presents the PreDiff for precipitation nowcasting. PreDiff adopts the latent diffusion for prediction. Besides, a special knowledge control design is used in a classifier guidance way to incorporate auxiliary prior knowledge, which can guide the model to satisfy conservation constrain. In both synthetic and real-world benchmarks, PreDiff achieves a good performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. This paper presents a practical method for precipitation nowcasting. Using diffusion model can benefit the prediction skill.

2. The proposed knowledge control is reasonable and effective.

3. This paper is well-organized.

Weaknesses:
1. The major concern is about novelty.

This paper introduces the famous diffusion model techniques into precipitation nowcasting task, including the condition diffusion, latent diffusion model and classifier guidance. 

The key design is the knowledge control. But accoding to the paper, the knowledge control requires maual designed for every specific task. And there does not exist evidence that anticipated precipitation intensity is a right prior knowledge for precipitation nowcasting. More explanation are expected.

2. More comparing baselines are insufficient.

There are some advanced deep models for precipitation nowcasting that the authors should take into consideration, especially the second baseline that is based on the GAN.

[1] MotionRNN: A Flexible Model for Video Prediction with Spacetime-Varying Motions, CVPR 2021

[2] Skilful precipitation nowcasting using deep generative models of radar, Nature 2021

3. Model efficiency comparisons are also required, including the GPU memory and running time.

Limitations:
They have discussed limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper applies latent diffusion models to the problem of precipitation nowcasting, developing a modular and flexible method for enforcing external constraints during the generating process. Constraints are general in that they can be specified in terms of any function F from the data to the reals, F:(x,y)->R. In the proposed ""Knowledge Control"" (KC) method, a separate NN is trained to predict F from the noised latent data. Conveniently this can be trained before training the diffusion model; it is then used while training the diffusion model as a regularizer, punishing the diffusion model for generating samples that violate the constraints. 

Experiments are performed on a simulation dataset (MovingMNIST) to demonstrate the advantage of the modeling choices and the KC method, and compare diffusion models with other deep learning models. The KC method is used to enforce conservation of energy, and the experiments demonstrate that KC is effective at constraining the generated samples without interfering with the other performance metrics.

The model is then applied to the SEVIR precipitation dataset and compared against a number of other deep learning models. Experiments demonstrate the use of the KC method in controlling the overall amount of rainfall.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The knowledge control method will be of high interest to those applying diffusion models to scientific applications, where there are often physical constraints.
- Experiments are well done and will be valuable to researchers in this area. Experiments are done on both simulated and real data, with multiple performance metrics, and comparisons to strong baselines. 
- The paper is well-written. 


Weaknesses:
None.

Limitations:
The authors concisely and fairly address the limitations. 

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper introduces a new method for doing precipitation nowcasting. The method is based on building a conditional diffusion model in a global latent space, which is found separately in an autoencoding fashion. Additionally, a “knowledge control” mechanism is introduced to help ensure that the generated samples satisfy known physical constraints. Numerical validations are conducted on a synthetic and a real precipitation dataset.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The problem of precipitation nowcasting is highly significant and has a lot of potential for real-life impact.
* The paper is generally well written. I find it easy to understand. Tables and figures are informative.
* Lots of details for reproducibility.

Weaknesses:
* The model itself is not the most novel. Both latent diffusion and knowledge control (similar to classifier guidance) are not new. This is somewhat compensated by the fact that the model is being applied to a specific and impactful domain. 
* Despite the seemingly rich set of baselines being compared, some of the most recent SOTA models developed specifically for the precipitation nowcasting problem are missing (despite being mentioned in the paper): Metnet2 (ref 5), DGMR (ref 28) - these models were applied to real weather data, but one should be able to adapt them to the 2nd benchmark dataset considered here. It is important to know where the model stands compared to these, especially considering that the methodology alone does not qualify for accept due to lack of novelty.
* The use of the FVD metric is questionable (see questions below).
* Value of knowledge control on the precipitation dataset is unclear.
* A few other places that need further clarifications (see questions section).

I feel in its current state these outweigh the strengths so I vote for borderline reject.

Limitations:
The authors mention limitations related to potential misalignment between visual quality and scores, as well as dependence on data quality and quantity.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a latent diffusion model called PreDiff for spatiotemporal forecasting. The main contributions of this paper are two-fold: 1) they use a conditional latent diffusion model to handle the uncertainty of spatiotemporal forecasting. 2) They introduce an explicit knowledge control mechanism to align the model with domain-specific prior knowledge.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. This paper is well-written and easy to follow.
2. The design of the knowledge control mechanism is interesting. It is decoupled with the latent diffusion model, thus providing more flexibility to the whole framework.


Weaknesses:
1. My main concern is the experiments. This paper does not conduct enough ablation studies, and the effectiveness of the diffusion model cannot be supported by experiments shown in Tables 1 & 2.
2. Table 2 only reports the mean of CSI at different thresholds, which is insufficient to prove the superiority of the proposed method. CSI with a high threshold correlates with strong weather, thus, it is important in this work to prove that PreDiff can generate better forecasts, which is also one of the main claims of this paper. I suggest authors list CSI at different thresholds in Table 2 to make a comprehensive comparison.
3. The parameter number of PreDiff is significantly higher than that of EarthFormer (about 15 times). Thus the comparison in Tables 1 & 2 is unfair. So is it possible to reduce the number of parameters to make a fair comparison with other methods?

Limitations:
Please see the weakness.

Rating:
4

Confidence:
4

";1
hh6azymUaE;"REVIEW 
Summary:
This paper proposes the quadratic gradient for privacy-preserving logistic regression. Such gradient is used together with Nesterov’s accelerated gradient (NAG) and Adagrad on Homomorphic Encryption techniques.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper tackles the privacy-preserving (in the sense of encryption) logistic regression. The paper is clear with introduction and motivation. The algorithm is easy-to-follow and well-supported by experiments.

Weaknesses:
Overall, it is hard to understand the privacy concerns in this work as there are many privacy-preserving techniques available. Without an example or experiment of privacy attack, the motivation of using HE in the first place is not backed up. In addition, only [12] is compared in the empirical results. While the new method seems promising, the lack of other baselines makes it hard to understand the limitations and benefits of the new algorithm.

Limitations:
NA.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper proposes a second-order version of Nesterov's accelerated gradient (NAG) descent and Adagrad for logistic regression by incorporating an approximation to the Hessian. The authors call this the ""quadratic gradient"". Specifically, a diagonal approximation to the Hessian for logistic regression is proposed. Some empirical results are shown to illustrate the benefit of the proposed method over vanilla NAG and Adagrad.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The proposed approximation of the diagonal Hessian may be interesting for NAG.

Weaknesses:
Unfortunately, this paper has several weaknesses.

**1.** **Limited novelty**: The proposed approximation to the Hessian in Section 3.2 seems like a trivial and incremental extension of the idea of reference [4] discussed in Section 3.1. 

**2.** **Lack of clarity**: The proposed methods are unclear to me and the presentation needs to be heavily improved.

* The enhanced NAG method described in line 150 is unclear to me -- what is $G$ here and is $\alpha_t$ the step-size here? Moreover, Algorithm 1 seems different from the discussions in Section 3.3. What are $\alpha_0$ and $\alpha_1$? They don't look like the quantity $\alpha_t$ introduced in Section 3.3. Why is $\alpha_1$ chosen to be $0.5(1 + \sqrt{1 + 4 \alpha_0^2})$? I don’t understand lines 31 and 37 in Algorithm 1 and what are $\gamma$ and $\eta$ here? What is the role of $W$ in lines 34 and 35 – it is not being used at all. In summary, the enhanced NAG method/Algorithm 1 has been presented poorly and I don't understand the method at all. 

* What is the enhanced Adagrad algorithm? The two equations after line 154 which are supposed to explain the enhancement are not very clear. What is the difference between $G^{(t)}$ and $g^{(t)}$ in these equations? Is $G = \tilde{B}^{-1} g$ here? There is no algorithm summarizing it like Algorithm 1 for enhanced NAG. Also, suddenly for Adagrad, the authors have a negative sign in front of the gradient which corresponds to minimizing the function whereas for NAG and the previous discussions in the paper, maximizing the function has been considered. Please stick to either minimization or maximization for consistency.

**3.** **Premise of enhanced Adagrad:** One way to interpret Adagrad is that it tries to maintain a diagonal approximation of the Hessian inverse and applies it to the gradients (a.k.a. preconditioning). So I'm not sure why applying a second approximation of the Hessian inverse on the *already preconditioned* gradients makes sense intuitively. Additionally, the authors themselves point out that enhanced Adagrad cannot be applied to general optimization problems (line 182) due to ""learning-rate explosion"". Then why introduce this method at all?

**4.** **Setup and experiments**: The datasets on which experiments are performed are not standard benchmarking datasets in the ML community and appear to have very few features (looking at Table 2). There are no test set statistics provided. I'd be more convinced if the authors showed empirical results in a *standard logistic regression setup without any kind of encryption* (which frankly seems irrelevant to me in this paper) on benchmarking ML datasets.

----

*Some general comments*: The introduction on logistic regression can be compressed. It is standard to consider the *negative* log-likelihood objective and apply gradient *descent* to minimize it. A couple of small typos -- in line 96, I guess it should be ""$\bar{h}_{k i}$ is the $k^\text{th}$ element in the $i^\text{th}$ row of the Hessian"" and in line 216, it should be ""public"".



Limitations:
Not in too much detail but as I mentioned in Weaknesses, the authors point out that enhanced Adagrad cannot be applied to general optimization problems. No foreseeable negative societal impact.

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper proposed a new approach to improve the gradient used by first-order optimization methods in logistic regression by utilizing a constant bound to the Hessian matrix. The authors demonstrate how to use their method under a fully Homomorphic-Encryption scenario. They test their method on many real-world datasets under non-private settings and Homomorphic-Encryption settings.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper is written clearly.
2. The experiments are all using real-world datasets which have strong practical implications.

Weaknesses:
1. The `quadratic gradient` method is not very new. As the authors have mentioned in Section 3.1 (Line 95), most parts of the method were proposed by Bonte and Vercauteren. I understand that the missing non-negative restriction is important for using the convergence results by Böhning and Lindsay (Line 92). However, using the absolute value is rather an straightforward solution. 
* A more interesting and critical question remaining to be answered is why this proposed `quadratic gradient` method is faster as the authors claimed in the conclusion (Line 238). 
* Another problem with this method is its usage being restricted in logistic regression: The authors provided a choice of $\bar{H}$ for logistic regression, while it may be very hard to generalize it to other problems, especially neural network training. 
2. The experiments have not shown much advantage of using the `quadratic gradient` method. In Table 1 and Table 2, the accuracy and AUC of the proposed method are almost always lower than the compared baseline method [12]. I understand that the learning time is reduced, but it was not a main problem in [12] as shown in the tables, and we are not sure if there is a tradeoff between the learning time and the accuracy.
* The description of the experiment details in Section 5 is very short. The authors suggest the readers refer to [12]. I think it would be better to have the details in supplementary and a discussion of the weaknesses of [12] in these experiment settings, along with why the proposed method solves those weaknesses.

Minor weaknesses:
1. The maximum likelihood estimation (MLE) is commonly referring to the estimate for $\beta$. The value of the loss function is the negative log-likelihood. That said, the y-axis of the figures could be corrected. Also, the objective function is usually the mean of the loss for each data point, not the sum.
2. Typo: Line 216, pulbic -> public.
---
I have read the rebuttal which answered my questions but did not fully address my concerns on the weakness.

Limitations:
N.A.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes a new gradient method that can be efficiently used under homomorphic encryption. The proposed method replaces the gradient $g$ by an approximation of $H^{-1}g$, where $H$ is the Hessian. This approximation is done using a specific diagonal matrix, that speeds up convergence while being possible to use under homomorphic encryption.


Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper proposes a new method for privacy-preserving logistic regression under FHE, that achieves reasonable results with less computation than existing methods.
2. The proposed method is quite versatile as it can be applied to a variety of optimization algorithms.


Weaknesses:
1. Experimental results are not convincing. Contrary to the paper's claims, the proposed method often performs way worse than existing ones (especially on iDASH, Edunburgh and pcs). Datasets are also very small, and therefore do not account for how the method scales with the dimension. Given this is an empirical paper, it seems a bit insufficient.
2. The fixed-Hessian method seems to be closer to preconditioning (see e.g. [1]), where gradients are linearly transformed before being used, than to a proper second order method.
3. The proposed method does not seem to be too novel. In particular, the paper refers to [2] (which itself refers to a paper with the same title as this manuscript), which proposes a very similar method.

[1] Preconditioned Stochastic Gradient Descent, Xi-Lin Li 2015.
[2] Quadratic Gradient: Uniting gradient algorithm and newton method as one, by Chiang, 2022.


Limitations:
Limitations are not discussed in the paper. In particular, experiments only consider a specific setting, with a choice of parameters that seems arbitrary (e.g., 3 vs. 7 iterations), and some claims are not supported with evidence.


Rating:
2

Confidence:
4

";0
srJvUWZu6L;"REVIEW 
Summary:
The paper addresses the problem of continual test time adaptation by proposing to add two branches of low-rank and high-rank adapters.
The paper claims that the low-rank adapter learns the domain-agnostic knowledge, whereas the high-rank adapter captures the domain-specific knowledge.
The paper also proposes a Homeostatic Knowledge Allotment (HKA) strategy to discern the contribution of the two branches.
Experiments are conducted for the classification problem on the benchmarks ImagenetC, CIFAR10C, and CIFAR100C, and for the semantic segmentation on the Cityscapes-to-ACDC benchmark.
The experimental results demonstrate the effectiveness of the proposed approach as it achieves state-of-the-art results using some selected pre-trained deep neural network architecture.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper tries to capture the domain-specific and domain-agnostic knowledge using two separate adapters: low-rank and high-rank adapters.
2. The HKA strategy to update the weight contribution of these adapters based on the uncertainty value of prediction is a nice idea.
3. Ablation study demonstrates the contribution of different components.
4. Experimental results for zero-shot generalization are new in the TTA setting.

Weaknesses:
1. No theoretical justification or even intuition is provided about why the low-rank adapter learns the domain-agnostic knowledge, whereas the high-rank adapter captures the domain-specific knowledge.
2. The backbone architecture is not consistent with previous works such as CoTTA. Section 3.4 in the supplementary material has only for CIFAR10C. What about the performance of the proposed approach for Imagenet-to-ImagenetC on ResNet50? What about the performance of the proposed approach for CIFAR100-to-CIFAR100C for ResNeXt-29 architecture?
3. Retraining the ""model added with low/high-rank adapters"" for some steps using the source data. This leads to the inability to use off-the-shelf pre-trained models without access to the source domain data. 

Minor Comments
1. References should be conference/journal version, for example, 57 reference can cite the CVPR conference version
2. Line 175: Figure .2 --> Figure 2, and many more ""Figure""
3. Line 237: Modal --> Model
4. Line 248: comparability. --> comparability

Limitations:
1. The experimental results indicate the effectiveness of the proposed approach for transformer based architecture and not much for convolutional neural network based architecture.
2. Addition of extra modules, such as adapters, requires further training of the model with adapters for one epoch on the source domain data. This is a limitation since the source domain data may not be available in many practical scenarios, and only the pre-trained model may be provided.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes to utilize domain-specific and domain-agnostic knowledge to tackle the error accumulation and catastrophic forgetting problem and boost the performance of continually test-time adaptation task. The proposed visual domain adaptor (ViDA) aims to adapt current domain distribution and maintain the continual domain-shared knowledge in CTTA, while a homeostatic knowledge allotment strategy is designed to fuse the knowledge. The experiments demonstrate that the method achieved SOTA.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The logic of this paper is clear and the performance is godd.
2. The whole method is easy to understand and implement.
3. They conduct sufficient experiments to prove their proposal.


Weaknesses:
1. The authors mentioned that the proposed method ensures no extra parameter increase in Section 1 (L64) and Section 3 (L195). However, extra parameters and computational costs are considered as limitations in L335, which brings contradiction and unclarity. Adding clear explanations would be appreciated.
2. The authors argues that ViDA with a low-rank prototype focuses on domain-agnostic knowledge while ViDA with a high-rank prototype concentrates more on domain-specific knowledge. Explanation on the Fig.1 is not sound enough.
3. How are the different ViDAs projected into the pre-trained model by re-parametrizations? Please provide more details.
4. Other suggestions：
 L261 Table .1 -> Table 1
 L275 Table .2 -> Table 2
 L296 Table. 3 -> Table 3


Limitations:
Limitations on extra computational costs are discussed. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper aims to address continual test-time adaptation (CTTA) with parameter-efficient fine-tuning techniques, i.e., adapter. The authors find that the low-rank adapter i.e., standard bottleneck structure, can extract domain-invariant knowledge. On the other hand, a high-rank adapter can extract more domain-specific knowledge. Experiments on three image classification benchmarks and one semantic segmentaiton benchmark demonstrate the effectiveness of the proposed method based on ViT backbone.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is easy-to-follow. 

2. The topic is essential yet the idea is moderate. Both continual test-time adaptation and parameter-efficient fine-tuning are important topics for the community and the paper addresses two formulations simultaneously.

3. A new method that seems well-motivated and performs well on classification and segmentation benchmarks.

Weaknesses:
1. The major concern is that are the learned features domain-invariant related to the structure of the adapter? That is, low-rank adapter can learn domain-invariant features while high-rank adapter can learn domain-specific features. How about all adapters adopt the same structures?

2. What is the motivation of the teacher model? Just following CoTTA?

3. More relevant works [a,b,c, d, e] should be discussed. 

4. What is the design of low-rank and high-rank adapter in CNN architecture? Why not the auhors report the result of ResNet50 in Table 1? 

5. It seems that the proposed ViDA module is memory-extensive since all experiments are conducted on A100 GPU. However, the backbone is just ResNet50 or ViT-base. Thus, does it mean that PEFT is memory-extensive or the proposed ViDA module is extensive?

6. It can be seen from the results in Table 3 that the model performance with the pre-trained encoder parameters of SAM significantly decreases. But the reason is unclear. It might be that SAM is pixel-level foundation model but the performance gains with image-level foundation model DINOv2 are limited. Also, I am interested in the results for Cityscapes-to-ACDC with SAM pre-trained parameters.

Refs:
[a] Niu et al., Towards stable test-time adaptation in dynamic wild world.
[b] Yuan et al., Robust test-time adaptation in dynamic scenarios.
[c] Song et al., EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization.
[d] Gong et al., NOTE: Robust continual test-time adaptation against temporal correlation.
[e] Döbler et al., Robust mean teacher for continual and gradual test-time adaptation.


Limitations:
The authors claimed one limitation of this work, i.e., introducing extra parameters and computational cost.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a continual test-time adaptation method by designing a visual domain adapter (ViDA) to handle both domain-specific and domain-agnostic knowledge. To adapt to different distribution shifts, a homeostatic knowledge allotment strategy is proposed to adaptively merge knowledge from each ViDA with different rank prototypes. Experiments on four benchmark datasets demonstrate the effectiveness of the proposed method for both classification and segmentation CTTA tasks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper introduce homeostatic visual domain adapter for continual test time adaptation.
- Good results are achieved on both classification and segmentation tasks.

Weaknesses:
- In general, the proposed components are not very well justified and verified. The details are listed in the following points.
- Except for the empirical results, it lacks more in-depth analyses and justifications on why a low-rank prototype focuses on domain-agnostic features while a high-rank prototype focuses on domain-specific knowledge. In addition, Figure 3 (a) shows the reduction of inter-domain divergence due to the introduction of the low-rank adapters. The reviewer was wondering what would be the results of the inter-domain divergence by using the high-rank adapters.
- In table 1, the experimental results are not clearly explained. For example, what are the difference between the last two rows?
- In Table 6, both the ViDAh and ViDAl could improve the performance significantly. However, it is still unclear why the different adapters (high-rank and low-rank) necessary. How to effectively show that they are complement to each other? It would be interesting to know whether two high-rank adapters or two low-rank adapters could also achieve good results?

Limitations:
The authors addressed some of the limitations.

Rating:
5

Confidence:
5

";0
xUyBP16Q5J;"REVIEW 
Summary:
This paper studies the incentives in recommendation systems. Specifically, it studies how to design the platform's reward mechanism to steer the creators' competition towards a desirable welfare outcome. Firstly, it shows a class of mechanisms called ""Merit-based Monotone Mechanisms"" lead to a constant fraction loss of the welfare. To overcome this loss, it introduces Backward Rewarding Mechanisms (BRMs) and shows that the competition games resulting from BRM induce the strategic creators’ behavior dynamics to optimize any given welfare metric.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper studies an interesting question in recommendation system. It shows an interesting bad effect of a wide class of mechanisms towards social welfare and then designs another mechanism to overcome the negative result. The theoretical results within the scope of the paper are complete. And there are also empirical experiments.

Weaknesses:
The applied value of the model in the paper is lack of justification.

Limitations:
The applied value of the model in the paper is lack of justification.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the game played by content creators in recommendation systems, which they call the content creator competition game. This game is centrally defined by a rewarding function M, decided by the platform, which rewards content creators based on how users engage with their content. The paper focuses on how to design this rewarding function in order to maximize user welfare. 

They show first that a practically-motivated class of rewarding functions, “Merit-based monotone mechanisms” (M3), lead to losses in user welfare by producing an equilibrium that caters to majority-group users, and fail to cater to minority-group users. Notably, the “necessary welfare loss” is only slightly suboptimal: such mechanisms can still capture a $K/(K+1)$ factor of the optimum, where $K$ is the parameter defining the top-K recommendation policy (though they do make the point that when $K$ is effectively 1 for users who care only about the top recommendation, this ratio can be 1/2). Also notably, they prove this result in a sub-class of creator competition games with the structure of a majority group and several minority groups, all with orthogonal interests (called TvN games - “trend versus niche”).

Next, they introduce a class of rewarding functions called “backward rewarding mechanisms”, which keeps the merit-based property of M3 but drops the monotonicity assumption, trading it for a a set of functions f1,…fn specified by the platform that can be tuned to encourage diversity by making it costly for too many creators to be producing the same kind of content. They show that for TvN games, there exists a backward rewarding mechanism that admits the optimal welfare. They run some simulated experiments with user preferences that constitute a TvN game.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper aims to be general: they take almost an axiomatic approach and study an entire class of mechanisms (M3) defined by two main assumptions, which encompass multiple practical mechanisms. They also study an entire class of instances (TvN games).

- They show clean equilibrium results for both classes of mechanisms

- The new class of mechanisms they propose is conceptually interesting — it makes clear why the monotonicity assumption causes problems, and offers a tunable class of algorithms that can help improve user welfare, at least in theory (it remains to be seen whether there is adequate information available in practice to set the parameters of these mechanisms well).

- The paper takes care to make abstract concepts understandable, giving examples and intuition to supplement the math

Weaknesses:
1. I do not understand the “monotonicity” property conceptually (described on Lines 43-44 as “the sum of creators’ utilities increase whenever any creator increases her content relevance”). I may be misunderstanding something here, but I interpret this to mean “when one creator benefits, all creators benefit on average”. This doesn’t seem to necessarily reflect an environment in which content creators are competing: Under competition, it could be the case that when a given creator improves her content, it greatly increases her own utility but decreases all other creators’ utilities more in total?

2. Although the paper makes some attempts to justify why the K/(K+1) loss of M3 mechanisms is bad, this doesn’t seem that bad to me, especially given that it emerges from a purely theoretical model in which many abstractions have been made. I felt that the paper oversold the magnitude of this loss in multiple places. It’s also not clear to me that the proposed fixed (BRM) doesn’t have similar loss in other natural sub-classes of games outside TvN games (as their positive results applies only to TvN games). To me, the combination of these two points weakened the motivation for the results sections later in the paper.

Small points about clarity:
- Line 34: what is a “reward signal”?
- Line 41: “and frame a class of prevailing rewarding mechanisms… Merit-based Monotone Mechanism”. It’s not clear what “prevailing” means here.
- In the explanation Lines 50-53, I’m missing a logical step: is “relevance quality” evaluated in terms of the *number* of users who find it relevant? Otherwise, I don’t see how this monotonicity property could cause concentration of creators around majority users’ preferences. 

Limitations:
yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
The development of online media referral platforms has provided a source of income for media content creators, and the incentive strategies of the platforms may influence the creators' creative trends. The incentive model that tends to reward may also invariably encourage creators to over-serve the majority user group, and the niche groups will be increasingly underserved. In order to solve these problems, this paper designs a backward incentive mechanism, which induces creators' behavior through the game structure to dynamically optimize the creation model and maximize social welfare. And its advantages are verified by simulation experiments.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
To address the current problem that rewards mechanisms in online content recommendation platforms affect creators' production choices, the platform's content distribution and social welfare. In this paper, we design a reverse reward mechanism, which can guide content creators to optimize their creation strategies and provide locally optimal results for a given welfare metric. This avoids most creators to generate a large amount of homogeneous content that caters to the majority group for the sake of rewards.
1. originality
In this paper, we provide a reverse reward mechanism to address the problem that current reward mechanisms may encourage creators to concentrate on the mass range, thus leaving niche users unserved. The superiority of this mechanism, which is performance-based but discards monotonicity, has been proven in empirical studies, and the approach in this paper is well original.
2. quality
The research problem of this paper is apparent, the method is introduced in detail, the logic is clear and the experiment is reliable. This paper has good quality.
3. clarity 
This paper clearly defines the research problem, and introduces the effectiveness of the method from the theoretical and simulation experiments in detail. The overall clarity is good.
4. significance
In this paper, it designs a reverse incentive strategy to cope with the incentive drift caused by the profit-oriented incentive strategy in online content recommendation platforms, so as to reduce the undesirable incentives that cause a large number of creators to ignore the niche groups and leave them unserved. The research in this paper has significant implications.


Weaknesses:
This paper’s related work on the content of the current study is weak, there are few references, and there are only a few works in the past five years, which is a bit difficult to explain the novelty of the current method.

Limitations:
This paper seems to have no clear statement on the limitations of the research. I have a question as to whether the creators in the current study only considered those who create for profit, and whether they considered those who create for interest. The current study seems to consider creators who create for profit, but it does not seem to be stated in the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors study strategic content creation in recommendation systems, focusing on the induced game's social welfare. The authors assume that the provider's rewards are entirely determined by the platform's payments, not clicks/engagements. This separation between the ranked results and the creators' incentives facilitates analyzing an expressive game in which the recommender system recommends a list of items (most prior work considers one item) with position bias. Notably, the authors assume that the rewarding mechanism is a mapping from a vector of relevance scores to reward vectors, i.e., $[0,1]^n \rightarrow [0,1]^n$.

The paper considers two classes of merit-based mechanisms, monotone and BRM (and also the BRCM subclass). It showcases evidence against monotone mechanisms, highlighting that in a particular class of games (TvN), the POA of any monotone mechanism is at most $\frac{K}{K+1}$, where $K$ is the length of the list, plus a small $\frac{1}{n}$ factor. Later, for BRCM mechanisms, the authors show that the welfare function is the potential function; thus, the global optimum of the welfare is a PNE (despite the POA could still be  $<1$ for some mechanisms). 

Finally, the authors describe how to optimize over BRCM mechanisms in the presence of data, and conduct synthetic and semi-synthetic experiments to demonstrate their approach.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The paper deals with a timely and important topic, and well-connects to previous literature.
2.	The paper non-trivially extends previous literature, suggesting new theory and experimental validation.   
3.	The optimization problem the paper suggests is exciting and new to this literature.


Weaknesses:
1.	Due to the abundance of mathematical objects and notations, the paper is non-trivial to follow. Perhaps this is inevitable, but I see this as a weakness. 
2.	The case against monotonicity focuses on a relatively small class of TvN games tailored to the authors' argument. Arguing against monotonicity in (more) general games would be much more convincing. 
3.	The empirical evaluation lacks proper benchmarks.


Limitations:
No limitations

Rating:
8

Confidence:
4

";1
UdrybSp67L;"REVIEW 
Summary:
In this papers the authors consider estimating the $q^{\rm th}$-moment of a function $f$ (i.e. $\int f^q(x)dx$) by observing samples of $x_i,f(x_i)$ when $x_i$-essentially follows a uniform distribution over (a compact) domain of integration. The paper first develops an information theoretic lower bound (using standard testing arguments) for estimating $\int f^q(x)dx$ for $f\in W^{s,p}$ (Sobolev Space of smoothness $s\in \mathbb{N}$ and order $p\geq 1$). In order to match the lower bounds the authors subsequently specialize the results in two domains -- high and low smoothness respectively. In the high smoothness regime, a suitably bias corrected estimator based on an initial ML based estimator of $f$ (that satisfies some desirable initial properties) is shown to be optimal whereas in low smoothness regimes, owing to existence of rare and extreme events due to unboundedness of the $2q^{\rm th}$-moment of $f$, the problem needs to addressed differently and the authors show that a truncated version of the classical Monte Carlo method can provide optimality guarantees. Finally the paper also provides some details on the noisy version of the problem (i.e. when one observes $x_i,f(x_i)+\varepsilon_i$, $\varepsilon_i\sim N(0,\sigma_n^2)$) to obtain connections to the nonparametric regression literature.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(i) A complete paper with minimax upper and lower bounds for estimating $q^{\rm th}$-moment of a function $f$ (i.e. $\int f^q(x)dx$) by observing samples of $x_i,f(x_i)$.

(ii) Considers both high and low smoothness regimes of the problem.

Weaknesses:
Not much discussion on a a very well developed non-parametric theory of estimating $\int f^q(x)dx$) by observing samples of $x_i,f(x_i)+\epsilon_i,\epsilon_i\sim N(0,\sigma^2)$ where both $1/\sqrt{n}$ and (slower than)non-$1/\sqrt{n}$-rates along with efficiency bounds  $1/\sqrt{n}$ are presented. 

Limitations:
None noted.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents theoretical results on estimates of the integral of f^q based, and when regression adjusted control variates can lead to better Monte Carlo estimators.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The results look to be novel and improve existing ones in this area. Furthermore, this is an area that is important in practice and for which strong theory can be impactful.

I am not familiar with the historical work on this problem, and thus find it hard to say just how impressive the theoretical developments are -- thus the main focus of my review has been on their practical usefulness. It seemed that there is potential for these to be important, but (see Weaknesses) the presentation currently limits this.

Weaknesses:
The paper's presentation limits its accessibility to those who are very familiar with the theoretical area, which means that it is unlikely that the work would have impact on practice. As just one example, the introduction is written under the assumption that the reader knows what a Sobolev space is.

Similarly, there is little attempt to relate the theory to practice. E.g. how would a user know what Sobolev space their function is in? This may be straightforward in some situations (but is not explained). In general settings where we want to estimate E_pi(f(X)) and area using MCMC to simulate from pi -- things are complicated if the domain of X is unbounded, as I believe the theoretical results assume require to transform X to e.g. lie on the unit hyper-cube. The expectation is still an integral of a function over the hyper-cube, but perhaps it is less clear what the function is/what its properties are.



Limitations:
NA

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies whether we can learn a control variate to reduce variance in Monte Carlo sampling. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper studies whether we can learn a control variate to reduce variance in Monte Carlo sampling. 

Weaknesses:
To someone who's not familiar with this area of research, the paper lacks introduction and is very confusing. 

Limitations:
To someone who's not familiar with this area of research, the paper lacks introduction and is very confusing. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
The authors study the theoretical properties of Monte Carlo estimators of the moments of a Sobolev function with nonparametric regression-adjusted control variates. In particular, they show that when a certain smoothness assumption is satisfied, then the regression-adjusted rule achieves the minimax optimal rate (where the minimum is over estimators, the maximum over test functions/integrands). When the smoothness assumption considered is not satisfied, the Monte Carlo estimate of the moments has infinite variance, and the authors show that to again achieve the minimax optimal rate one needs to resort to a truncated estimator. Further, the authors study the performance of the regression-adjusted estimator for integral estimation with *noisy* observations, a somewhat nonstandard setting, and provide some lower and upper bounds for the estimator's absolute error.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- A good theoretical contribution to the literature on minimax-like results for Monte Carlo estimators 
- Good effort in making the notation and results accessible to a wider audience 
- Interesting setting with noisy function evaluations

Weaknesses:
- The paper reads a bit like a laundry list where results are presented in a sequence without much of a ""story"" connecting them; in particular, sections 3 and 4 feel disconnected, the former being about minimax results with noiseless observations specifically about moment estimation, the latter being about integral estimation (q=1 ? ) with noisy observations. Why does the setting change, what is the motivation? 
- Regarding your contribution, you mention ""existing proof techniques are only applicable in scenarios where there is either no noise or a constant level of noise"" - here are you talking about results for moment estimation or integral estimation ? Can you clarify in both settings which aspects are missing from the existing literature and how your results filll that gap ? 
- While I understand that the contribution is of theoretical nature, could you make more effort motivating why the setting of noisy observations is interesting / relevant to applications of Monte Carlo ? Further on this point, could you give intuitions for when the main smoothness assumption is expected to hold or not hold (beyond rare events) ?

Limitations:
The authors should add a couple of sentences about limitations in the conclusions.

Rating:
7

Confidence:
3

";1
PXUHrqIL9O;"REVIEW 
Summary:
The authors demonstrate that selective mixup has a similar effect with resampling. This reduces training dataset bias, which may contribute to settings with distribution shifts. Extensive experiments on five datasets were conducted.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- A new perspective of (selective) mixup has been discussed.
- Extensive experiments were conducted on various benchmarks
- The paper was well written and was easy to follow.

Weaknesses:
The authors have done a great job in providing an extensive list of experiments. However, I have two major concerns:
1. The paper's contribution to the body of knowledge is somewhat weak. I believe the gist of the paper is that ""(selective) mixup has a resampling effect"". Although this is a novel and interesting interpretation of the mixup data augmentation method, I feel this is not enough compared to the recent standard level of NeurIPS papers. I believe that a more concrete methodology that reflects the authors' findings is needed to qualify as ""acceptance level"".
2. The experimental results and their discussions are confusing. While the discussion on the waterbirds dataset says that ""selective mixup performance is entirely due to resampling"", the results on the arxiv dataset says ""selective mixup is affected both by vanilla mixup and resampling"", and results on the civilComments dataset implies that ""resampling is better than mixup"". These different experimental results and interpretations do not conclude the effect of resampling and mixup. Rather, the results seem to imply that ""mixup and resampling are independent methods that may or may not have correlating effects, depending on what dataset is used"". If this is the case, the primary assumption of this paper is undermined.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors focus on explaining the working mechanism of selective Mixup in distribution-shift scenarios. They points out an interesting and novel perspective that selective Mixup benefits not (only) from Mixup itself, but rather the resampling effect it induces. They suggest that such a resampling effect can balance the class and/or domain distribution (depends on the pairing criterion) into uniform distribution, which in turn improves the model's performance.

Theoretically, they show that under the different-class criterion of pairing the data, the classes' distribution does become more uniform. Experimentally, they take the worst-group (class or domain or their combination) performance  as the metric, and conduct experiments on several datasets and models. They investigates a variety of algorithms (e.g. ERM, Mixup, conventional resampling, etc.) and pairing criterions (e.g. different class, different domain, etc.). The results confirms their raised point of view on selective Mixup.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper presents an interesting and novel perspective of  undderstanding the mechanism of selective Mixup. That is, instead of focusing on the data interpolation part of Mixup, which is conventionally considered the key of Mixup and any of its variant, this paper indicates that it's the resampling effect, which averages the data distribution, that enables selective Mixup to success.

2. In all the experiments, the paper has fully investigated all the possible sampling criterions. Furthermore, on the basis of these experiments results, this paper combines the algorithms and criterions that give the best performance respectively and obtains a even better result under its own testing metric.

3. All the figures are well displayed and clearly readable.

Weaknesses:
1. This paper is a experiment-driven work. Not many theories are provided to back the main idea up. While Theorem 3.1 shows how certain selective sampling ""uniformizes"" the class distribution, there is no evidence nor explanation of how it affects the covariate distribution. Also, there is no investigation or explanation of what roles (if any) the Mixup operation itself play in selective Mixup.

2. When performing Mixup and selective Mixup training as baselines, the hyperparameter of the Beta distribution $B(\alpha, \alpha)$ is not carefully determined, or at least the choosing process is not fully explained. If the Mixup baselines failed to reach their optimal states, then the comparison of them with other methods like resampling and selective sampling w/o Mixup might be unqualified.

3. The testing metric only considers the worst-group performance, but not including the overall performance. Normally practitioners are concerned not only with the short board of the bucket, but also all the boards as an entirety (a trade-off between fairness and overall generalization). 



Limitations:
1. The paper is fundamentally based on experimental investigation, but with little theoretical support. 

2. The datasets used doesn't include some of the most popular or commonly used ones like CIFAR or Imagenet. 

3. In the experiments, Mixup (and selective Mixup) are taken as baselines, but the value of $\alpha$ (which supervises the Beta distribution) may not have been carefully determined.

4. I personally imagine that the datasets size and\or the batch size will also have an impact on the performance of selective Mixup and selective sampling. There might be some room for further investigations in the future.

5. What roles (either positive or even negative) the Mixup operation itself plays in selective Mixup exactly have yet to be fully understood.

6. The metric is limited at the worst-group performance with no information on how the models perform in general on all the testing data.



Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper performs an analysis of selective mixup techniques, coming to the conclusion that some of the improvements these techniques provide can be obtained by the sampling procedure and not the mixing strategy. Based on some of the insights, variants of selective mixup are sometimes proposed to fit specific settings and shown to work better.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
- The considered datasets and settings are varied and extensive
- The paper proposes plausible explanations of the improved performance of selective mixup variants, and performs extensive ablations and creates experimental settings to test these explanations
- The observations about the roles of re-sampling and regression to the mean are extremely interesting.
- One of the main interesting points of the paper is carefully studying the forms of shift occurring between the training and the test distributions, and studying how specific forms of selective mixup impact them. This methodology can improve the practitioner's understanding of these techniques and how to improve them (e.g. as shown in some of the cases considered).


Weaknesses:
- As the authors acknowledge, their analysis is limited to the original mixing strategy.   
- The formatting of the paper is a bit bizarre, with plenty of extremely short paragraphs, misplaced image captions etc. I would recommend the authors to fix this issue, as it makes their work look unprofessional. 

Limitations:
Adequately Addressed

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presented an insight that selective mixup (different class or different domain) is actually equivalent to resampling. A simple mathematical proof is presented showing that the mixed up distribution is closer to uniform distribution in terms of label distribution. Extensive experimental results are presented to demonstrate the findings. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* novel understanding on selective mixup
* extensive experiments to demonstrate the understanding 


Weaknesses:
The finding is nice but seems value is limited, at least for two-class problems, selective mixup obviously leads to uniform distribution. This paper is more about empirical demonstration.

Limitations:
not new algorithm, but understanding existing algorithm, the value of the finding seems limited. 

Rating:
4

Confidence:
4

";0
1cY5WLTN0k;"REVIEW 
Summary:
Designing neural PDE sovler using deep neural networks is a challenging task for which several solutions have been proposed in the literature using for instance networks that encode the initial conditions or physics informed neural networks.

The authors propose to use Monte Carlo methods to train neural PDE solver for the solution of a general convection-diffusion equation.
Using Feynman-Kacformula, the authors derive a loss function that can be used to learn a mapping that can simulate the target fields using the input parameters and the initial condition. 

They propose a theoretical guarantee on the solution provided by the Monte Carlo solver and the paper illustrates the performance of the proposed method with a  one dimensional differential equation and a 2-dimensional Navier-Stokes equation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes a Monte Carlo based PDE solver strained via Monte Carlo approximation which can  handle  coarse time steps better than existing alternatives. 

The proposed method does not require many particles in the numerical experiment and is computationally efficient in the settings explored.

Under some assumptions, the authors propose an upper bound on the error explicit in some hyperparameters of the approach in the case of a  convection diffusion equation.


Weaknesses:
Theorem 1 is obtained under several assumptions. 
These assumptions should be discussed more, are they restrictive or common assumptions in the PDE literature ?

The claim that the approach is efficient even with few samples seems correct in the proposed experiments. However, these experiments are in dimension 1 and 2 and Monte Carlo methods can be cumbersome in high dimensional settings without tunning. 
The authors could detail the explicit advice or theoretical guarantees we have for the error with respect to M.

The authors only explore one discretization scheme (Euler), the scheme used could have an impact on the performance of the method, can this be discussed ?

Limitations:
The authors provide several research perspectives for this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose an unsupervised neural technique for solving PDEs based on the classical correspondence between (parabolic) partial differential equations (PDE) and stochastic differential equations (SDE) as given by the Feynman-Kac formula. Specifically, they propose minimizing the error between the neural approximation's deterministic prediction at timestep t+1 and the expected prediction of the neural approximation over particles that have evolved up to time t stochastically according to the Feynman-Kac SDE representation of the given PDE.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
Incorporating Feynman-Kac into the PINN framework is an interesting idea.

Weaknesses:
# Poor numerics
The experiments do not justify the claims, e.g. that long rollouts are more stable using this method. To prove this, at the very least the authors need to present results where the trajectories actually exhibit turbulence. Then, an ablation with the multi-scale framework is required.

# Poor presentation
The authors cannot expect the reader to be familiar with Feynman-Kac and need to give explanations in plain English of the significance of this result. Then, the equations in the main paper should aim to clarify this further, not give a comprehensive mathematical presentation. For example, the inclusion of the forcing function  distracts from the main result which is using time reversal to obtain an SDE that moves in the right time direction for equations that are most often solved using neural networks, e.g. ones with an initial condition, not a final condition.

Furthermore, only the most experienced reads will walk away from this paper with a clear idea of how to implement the proposed algorithm. The emphasis in the paper should be to give the reader something to implement and try, not a theoretical proof.

Finally, there are a number of tricks that are not included in the initial idea and are not sufficiently explained. For example, the Fourier interpolation. The multi-scale framework makes the model non-parametric which is a significant departure from previous work and from the presentation of this paper as a Monte-Carlo approximation to PDEs.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors present MCNP, a new unsupervised training loss for surrogate simulation networks. This loss is based on the link between stochastic processes and PDEs, sampling one-step Brownian motion to estimate the PDE solution. The learned network takes an initial state and the target simulation time to compute the state at that time in one pass. For longer periods, multiple NNs are trained, one for each sub-interval.

Soundness:
2

Presentation:
4

Contribution:
4

Strengths:
The paper is generally well-written and relatively easy to understand. It includes a good overview over related work.

The paper includes both theoretical and numerical results. The method is derived using the Feynman-Kac formula and the authors show how the errors of PSM and MCM scale when given an incorrect input state, such as predicted by a neural network.

A total of five numerical experiments are performed, covering a large range of simulation configurations. The paper contains an ablation study, giving some insight into the impacts of various parts of the MCNP method.

Weaknesses:
While the experiments are varied in the tested configurations, all but on experiment consider simple diffusion equations, some of which can be solved analytically.

The paper does not show any simulation trajectories and gives no insight into how the various tested methods behave in their experiments. Instead, only the final losses are reported. This makes it hard to determine the cause of improvement from the numerical results. I strongly recommend documenting your observations in the appendix.

The Navier-Stokes experiment seems to be mostly forcing-driven with all initial states ending in a similar configuration. A different forcing, such as Kolmogorov flow, would result in a much wider range of trajectories.

The paper does not give details as to how the numerical simulation (PSM) of the Navier-Stokes experiment was performed. Please describe the simulator in more detail if you implemented it yourself.

The source code is not part of the submission and the authors have not declared their intent to make it public. I strongly recommend doing so, especially if the employed CFD solver was implemented from scratch.

Minor:

* Eq. 3: Please indicate the x-dependence of xi
* Fig 1 caption: The formulas are missing a factor of 1/M
* L184: You mention that cutting the gradients prevents numerical instabilities but ignore the positive effects that gradient backpropagation can have.
* You refer to Eq.13 as a convection-diffusion equation but it does not contain a convection term.
* L199: The notation Δ ₜ u is confusing since Δ is already in use.
* L216: The argument that label noise helps MCNP perform coarser time steps seems unfounded to me. This requires an explanation.
* L237: By lattices, you probably mean frames or time steps?
* L268 and Eq. 17: You can simplify the forcing to a single sine term.
* Figure 2 is never referenced.
* Figure 2 shows the vorticity, correct? Please specify in the caption.

Limitations:
The limitations are adequately discussed.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors propose Monte Carlo Neural PDE Solver (MCNP Solver) which leverages the Feynman-Kac formula to train neural PDE solvers in an unsupervised manner.

I'm willing to revise my score based on the rebuttal from the authors to the questions that I raised below.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* That paper addresses and interesting problem: learning the neural operator in an unsupervised way. 
* The authors propose practical enhancements to their method such as one-step rollout, Fourier Interpolation and the use of a multi-scale framework.

Weaknesses:
* Limited set of experiments, only two cases: 1d diffusion and 2d Navier-Stokes.

Limitations:
* The limitations that I see are encapsulated on the questions that I raised above.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper proposes a new physics informed neural network based solver that utilizes the connection between PDEs and SPDEs. This is achieved through the Feynman-Kac formula and applies to a large class of PDEs. It comes with a bound on the error at each step in the rollout. The results are compared with multiple supervised and unsupervised PDE solvers on a number of 1D and 2D equations.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: To the best of my knowledge, the paper is original in combining the Feynman-Kac-based approach with a neural operator architecture. The connections to existing work that relies on the Feynman-Kac formula as well as other PINN/NO approaches are discussed in detail in the main paper and the appendix.

Quality: The work is thorough in discussing existing literature and presenting the methodology. The theoretical result gives some intuition relating to how the proposed method scales as compared to the classical solver. 

Significance: The methodology combines a number of existing techniques (Feynman-Kac formulation, neural operators, Fourier interpolation) to achieve some improvement in specific setups, e.g. where the solution of the PDE is rapidly varying in space and/or time.





Weaknesses:
Clarity: The quality of the writing could be improved, particularly in the abstract/introduction. The rest of the paper is detailed enough in describing the experiments and discussing the results. 

Significance: The proposed approach seems to give an advantage only in specific situations. While this is acknowledged in the paper, it could be beneficial to discuss specific applications where such oscillatory conditions occur.



Limitations:
The limitations and extent to which this method gives an advantage over existing approaches are discussed in detail in the final section of the paper. 

Rating:
5

Confidence:
4

";0
JG4BshgCas;"REVIEW 
Summary:
The authors tackle domain generalization for regression tasks, or DGR. DGR is often different from classification tasks in DG as well as imbalanced regression tasks due to shift in labels.

Popular feature alignment type of algorithms using IPM may not be applicable to DGR because the labels of different domains may not align, which may cause a model to fit to only one domain.
The authors propose margin-aware meta learning framework to solve DGR by weighting query based on the distance to support, which mitigates sampling bias.

The authors provide experiment results on multiple datasets compared to several feature alignment methods in domain adaptation as well as self-supervised learning and meta-learning methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The authors clearly show the relationship between the margin between support and query, and hardness of query tasks.
2. From this analysis, the authors propose a simple weighting strategy for meta-learning in regression tasks.
3. Experiments are conducted on many datasets - toy, age estimation and cross-domain rental prediction.

Weaknesses:
1. The relationship between the margin and hardness that the authors demonstrated is not something new. It is somewhat obvious. For example, in Gaussian Processes, the uncertainty increases as a query point gets far away from support points.
2. Although the authors apply the proposed MAMR to meta learning framework, only one baseline model (MLDG) is used as a meta learning model. Without comparing it with multiple meta learning models e.g. ANIL, Neural Processes, it is hard to see if the improvement is coming from the proposed weighting or different framework (feature alignment vs. episodic meta training).
3. I believe MAMR is applied to MAML framework given that there is inner and outer loop updates (needs to be clarify unless it has already been specified). Related to 2, I would like to see if MAMR improves the performance on top of other meta learning algorithms. If my understanding is correct, MAMR should be applicable to any meta learning algorithms for regression tasks.
4. It could’ve been if different backbones were used for experiments.
5. In Line 324-327, the authors mentioned “different from 5 or 10 inner steps in meta-learning for few-shot learning, fast adaptation by multi-steps is not necessary for DGR.” It is not really true even for MAML. With some frameworks e.g. learn2learn, MAML performs as good with 1 step adaptation as 5 or 10 step adaptations.

Limitations:
Please refer to the weakness and questions above.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper addressed the problem of domain generalization for regression, where the authors assume that different domains may have disjoint labeling space. The authors proposed a meta-learning-based method to learn the parameter initialization for DGR. In particular, the choosing frequency of different support and query domains relates to the label discrepancy. To evaluate the proposed method, the authors have also developed a DGR benchmark, considering both overlapping and nonoverlapping labels between the source and target domains.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Domain generalization in regression is a novel and interesting problem.

2. Leveraging task discrepancy to tackle domain generalization by meta-learning is novel.

3. The experimental results show effectiveness on a range of datasets.

Weaknesses:
1. It is unclear why using meta-learning to tackle the DGR problem in this paper. Although a range of works on DG has been proposed to use meta-learning, what are the significance and novelty of using meta-learning for domain generalization in regression?

2. The authors claim that the proposed method can enhance the exploration and interpolation capabilities. But it is unclear in the paper why the problem of domain generalization benefits from the interpolation capabilities.

3. The theoretical results in this paper are weak to me. Thm. 1 states that a larger regression margin of meta-tasks leads to more available domains. But it neglects the probability of meta-task over the set of labeling spaces. 

4. the objective of meta-learning is a bi-level optimization problem.  It is unclear why not use the task discrepancy as one of the objectives of out-loop optimization, directly. Besides, how many iterations for the inner-loop optimization and outer-loop optimizations?

Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
This work investigates and analyzes domain generalization for regression tasks and propose a margin-aware meta-learning method for DG regression, which can help learn long-range exploration and interpolation. Also, a benchmark for evaluating regression dg is built.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- the paper is presented and derived well to make it easier to understand
- the difference between DG for regression and previous classification tasks is well analyzed


Weaknesses:
- is there any possibility to apply the proposed meta-learning based DG to more general regression tasks like object detection?

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper describes an extension of domain generalization to the regression domain - traditionally, most of the research in Domain Generalization has occurred in image classification and to some degree in combined tasks such as object detection, depth estimation, and semantic segmentation and there has been a lack of research purely tackling domain generalization for regression. The authors specifically focus on the label shift aspect of domain shift, where the same input features might lead to two very different responses/regression values depending on the domain that they are in. The authors describe how the extra structure in regression problems (e.g. ordinal relatedness) is something that is mostly ignored by existing methods, and they propose a modification to the common meta learning methods for domain generalization to weight the meta-tasks appropriately based on this extra structure. The proposed method achieves SOTA among a variety of datasets in DomainBed. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
### originality
* the paper is quite original. As the authors note, this problem has not been well studied, and so most of the results and discussion surrounding how to take advantage of domain generalization for regression is relatively novel. 
* The technique proposed in the paper is fairly novel, even though it does not contain any individually novel pieces. Weighting a meta-learning strategy based off of the ""difficulty"" of the meta-task pair, defined by how related the two tasks are is certainly not something that I have seen before and is a neat strategy that could work in a variety of use-cases. 
### quality
* The paper has mostly sound experiments, and does a good job providing both empirical and theoretical evidence to support its claims. 
* The toy experiment (figure 3) is a great way to motivate why these kinds of techniques are needed and how existing methods may fail when they are presented with regression problems.
### clarity
* the paper is clear and easy to read, with both pseudo code, good diagrams/figures, and propositions that help clarify its points.
### significance
* domain generalization for regression is highly under-studied, and this work is a nice step forward to, at the very least, bring attention to this problem and how the underlying nature of it is quite different from classification.
* The technique of weighting the meta learning tasks based on some objective function feels like it could be used in a variety of different contexts, and I encourage the authors to explore this more in their conclusion/future work section.

Weaknesses:
The paper would benefit from a more nuanced discussion of the methods limitations, as well as more justification for some of the claims that are made. In particular,

* The limitations section is currently limited to 3 bullets, none of which have any justification - for example, ""most used datasets have balanced source labels, applying to imbalanced is a more practical setting"" - for this claim, what would be an example of an imbalanced dataset? Do we expect this kind of technique to still work there, or does the method fail on imbalanced source labels? Similar comments hold for the other limitations.

* Lines 36 - 38 talk about a potential outcome where feature alignment may be harmful for DGR tasks, but this would be much stronger if there was a toy experiment or example that the authors could talk about here that shows that this kind of feature alignment is a particular failure mode for popular feature alignment techniques. 

* The paper specifically focuses on experiments that are mostly single value regression tasks in nature, e.g. rental prediction - however, a large portion of regression tasks have image features as input - for example, depth estimation or bounding box regression. The paper would benefit from a discussion on whether or not these techniques could extend to those more common regression tasks, or what would need to change in order to make that happen. 

Limitations:
Addressed in the weaknesses section. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper focused on the generalization problem in the regression context and proposed an interesting setting, Domain Generalization in Regression (DGR). Compared to domain generalization for classification, DGR is a more challenging task due to its different domain shift forms between source and target domains. Compared with the traditional domain generalization for classification where the source and target domains usually shared the same label set, the range of response variables between the source and target domains in the context of regression can be very different. From this perspective, this setting proposed in this paper is novel and worthy to be explored. This paper proposed to solve the DGR problem through the meta-learning philosophy. However, the authors pointed out that when applying the meta-learning method for the DGR problem, the harder meta-tasks with larger regression margins on the label discrepancy can be less sampled and this sampling bias made these harder tasks difficult to optimize. Then, the authors proposed to address this sampling issue by measuring the feature discrepancy between the query and support samples and assigning higher weights to harder meta-tasks. Extensive experiments showed that the proposed MAMR can solve the DGR problem in an effective manner.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. This paper proposed a novel and practical setting, Domain Generalization in Regression. The generalization problem in the regression context was less explored in the community. This paper may inspire more explorations into this topic.
2. This paper analyzed the differences between the domain generalization tasks in the classification and the regression and showed the specific features of this problem, e.g., the distribution shift between the source and target domains can be under a different form, the sampling bias due to the ordinal relatedness in the regression. Based on these analyses, we can see that some existing methods cannot be directly applied to this field.
3. The motivation for the proposed methods is clear. Due to the sampling bias, the author proposed a weighted meta-learning method. The solution is reasonable.
4. The extensive experiments on some synthetic and real-world datasets demonstrated the effectiveness of the proposed MAMR.
5. This paper is well-written and easy to follow.

Weaknesses:
1. Some assumptions in the theoretical analysis can be further discussed. For example, it seems that Theorem needs to assume that the data from a meta-task distribute uniformly with respect to the response variable $y$.

Limitations:
No severe limitations.

Rating:
7

Confidence:
4

";0
gORnZ5qIsa;"REVIEW 
Summary:
This paper proposes a new technique to extract latent factors from psychological questionnaires. The method improves on previous methods both in terms of its ability to handle more flexible inputs (i.e., missing values and confounding variables) and in the interpretability of its outputs (i.e., the scale of its loadings are in the range of the original questionnaire). The proposed method is formulated as an optimization problem and an algorithm is provided to solve the problem and also to automatically determine an appropriate number of latent factors. Many experiments are provided that answer diverse sets of questions using both synthetic and real-world datasets.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The paper effectively formalizes psychology models as a solvable optimization problem that possesses better characteristics than competing methods.
2. The experiments are varied and show the usefulness across many scenarios: both when the true answer is known (synthetic data) and when it isn't.
3. Extensive comparison to existing techniques commonly used by researchers in the field of psychology.
4. They provide a Python implementation of their algorithm.

Weaknesses:
1. I realize space is tight, but many of the figures are incredibly small. Could some of the figures be moved to an appendix?


Limitations:
The paper effectively addresses common limitations with this kind of work by including a diverse set of experimental problems. If I were to give any feedback I'd say it might be worth discussing the inherent challenge in latent variable discovery. This would not be a weakness of your work but simply a challenge of the problem in general that you effectively addressed in the structure and design of the paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a matrix factorization formulation in order to extract latent features from questionnaires for psychopathology. 

The proposed formulation is very similar to the well-known Non-Negative Matrix Factorization (NMF), with an l1-penalty on the dictionary and activation matrices. The biggest major different is they include some fixed dictionary vectors in the dictionary matrix. Due to some domain-related constraints they use the Alternating Direction Method of Multipliers (ADMM) method in order to impose the constraints. 

The experimental results claim that the resulting factorization is interpretable. They also claim that the resulting factorization better preserves the clinical classification when compared to other factorization schemes such as l1-NMF and Factor Analysis. Finally the authors also claim that the proposed method better preserves correlation between the the activation matrix that is obtained from the full data and the activation matrix obtained from the subset of the data.  


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The method seems appropriate in order to obtain latent factors from a questionnaire data. 
- The method is lightweight. 

Weaknesses:
- I am not really convinced that the proposed method is novel. It is simply an l1-constrained Non-Negative Matrix Factorization method. It is true that they add known variables inside the NMF dictionary, and impose additional value constraints, but in my opinion this does not seem like a novel method to me. Also, I do not think that the presented results present any novelty. 
- The manuscript is hard to follow at times. For instance, I tried to understand how do the authors argue that the proposed factorization is interpretable, but the arguments put forth in section 4.2.3 in order to explain figure 2, remain difficult to understand for me. For instance the authors write that `While there were factors that loaded primarily in questions from one subscale, as expected, we were encouraged by finding others that grouped questions from multiple subscales, in ways that were deemed sensible co-occurrences by our clinical collaborators`. This sentence is hard to understand for me. I understand that maybe the authors' clinical collaborators might verbally confirm that these findings are sensible, but it would have really helped if the authors could do a user study to more quantitatively argue that their proposed method is superior compared to the other methods that they have compared against. 
- In my opinion Figure 2 is critical in order to motivate the importance of your proposed method. It seems like there are some patterns with respect to different classes of psychopathologies, but I do not clearly understand what is the take away message here. The alternative method also seems to retain specific patterns for each class.  
- You are comparing against two other linear factorization methods. I think it would have better to show that your method has advantages compared to deep neural network (e.g. an autoencoder with several layers).  

Limitations:
The authors do not discuss negative societal impacts of their work, but I think this is okay, given that this work does not really pose dangerous implications.  

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes a non-negative matrix factorization with a customized regularization term to identify interpretable latent factors from psychopathological questionnaires. The input data is represented in a matrix and a non-negative matrix factorization algorithm is applied to the input matrix. The factor matrices are bounded to be between 0 and 1, providing some interpretation of the presence or absence of the corresponding factor.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed method is overall sound. The optimization problem formulated and the regularization terms incorporated could well serve the desired purposes. The application of ADMM to solve the optimization problem is also reasonable. Good experimental results are shown using multiple datasets.

Weaknesses:
- The technical contribution is somewhat limited. Non-negative matrix factorization has been extensively studied for decades and widely applied to various applications, including questionary data analysis. ADMM is also a classic framework to solve matrix factorization problems with constraints. It seems to me that the optimization procedures described in Eq. (1-5) are standard for the ADMM algorithm and the convergence follows directly from the ADMM properties.
- The baselines used in the paper are very classic ones, $\ell_1$-NMF was developed in 2009 and FA-promax is developed in 1964. More recent methods for questionnaire data analysis should be compared.

Limitations:
Limitations are not discussed in the manuscript.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents an algorithm for factorizing matrix with multiple constraints desired in the study of psychiatric disorders using clinical questionnaires. These constrains include those that had been studied previously, such as sparseness in both the factor and loading matrices and non-negative values in both matrices. They also include two novel ones, that is the magnitude requirement on values in the matrix reconstructed from the factor and loading matrices and the value range requirement on the factor matrix (values in this matrix should be in [0, 1]). The authors also proposed to directly factor in confounding factors (e.g., age, gender) in the factorization. The algorithm was developed using the popular ADMM framework. Evaluation was done using both synthetic datasets and two practical datasets. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The results included in Table 2 is interesting, indicating the proposed method learns factors that are more stable than baseline methods with varying training set size. This is a desired behavior, implying it might learn the intrinsic patterns that are important.  

Weaknesses:

Other than what’s mentioned in strength, the practical significance of this approach is limited given the existence of large amount of existing works in matrix factorization research. There is no clear statistical significance among results in Figure 3 to indicate obvious advantage of the proposed method over compared ones. 

The advantage of including age and gender in the factorization is not clearly indicated. How with/without them affecting the learned factors is not clear. 


Limitations:
NA

Rating:
4

Confidence:
4

";0
BMVcW1IL9l;"REVIEW 
Summary:
The submission proposes a method to improve the quality of pseudo labels for semi-supervised semantic segmentation. The proposed approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels, rather than generating pseudo labels at each pixel independently. Experiments show improvements over state-of-the-art semi-supervised semantic segmentation methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed approach is conceptually simple and experimental results indicate it is effective.

Weaknesses:
1. It is not clear to me what is the improvement from considering neighbors when generating pseudo labels. It would be good if Table 4 (a) has an extra column with experiment using neighbor size 1x1, meaning no neighbor is used. Right now, the only comparison is with SOTA models that do not consider context, but the comparison is not convincing as there might be implementation differences. Also, why using a larger neighbor size hurts performance in Table 4 (a)?
2. The experiment setting is still a bit unclear to me. It would be great to state what images are used as labeled images and what images are unlabeled. Also, I think there misses an upper bounding experiment with the same model trained with all annotated images. Right now the SSL model is outperforming fully-supervised model, which does not make much sense to me.
3. Since Pascal VOC is a relatively small dataset, can we use more unlabeled images (e.g., COCO) to further improve performance?

Limitations:
Please see weaknesses.

Rating:
4

Confidence:
3

REVIEW 
Summary:
In this paper, the authors propose a new confidence refinement scheme called S4MC for improving the pseudo-labels in semi-supervised semantic segmentation. Unlike existing methods, S4MC considers the spatial correlation of labels in segmentation maps by grouping neighboring pixels and collectively analyzing their pseudo-labels. This approach allows for the utilization of more unlabeled data during training while maintaining the quality of the pseudo-labels, all without significant computational overhead. Through extensive experiments on standard benchmarks, the authors demonstrate that S4MC surpasses current state-of-the-art methods in semi-supervised learning, offering a promising solution to reduce the cost of acquiring dense annotations.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The paper is written in a clear and concise manner, effectively presenting the motivation and ideas behind the proposed approach. 
+ The proposed approach is rigorously evaluated through extensive experiments performed on multiple benchmarks.

Weaknesses:
- The contribution of this paper is somewhat limited, as the idea of leveraging contextual information from neighboring pixel predictions to enhance segmentation has been extensively explored in the literature.
- In Tables 1-3, it is evident that S4MC + FixMatch (Ours) demonstrates superior performance compared to S4MC + CutMix-Seg (Ours). However, the performance of FixMatch, U2PL+FixMatch, and PS-MT+FixMatch  is not explicitly mentioned in the given context.

Limitations:
The authors have adequately addressed the limitations

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work focuses on semi-supervised semantic segmentation. To generate more accurate pseudo-labels, this work attempts to leverage the spatial correlation of labels in segmentation maps and proposes a confidence margin refinement module to model the contextual information. Extensive experiments have been conducted to validate the effectiveness of this method. The proposed method achieves promising performance on different datasets and settings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Strengths:
The idea is interesting and the writing is clear.
The effectiveness of the proposed method has been validated on several standard benchmarks.


Weaknesses:
Weakness:
The time complexity of the proposed confidence margin refinement (CMR) module is necessary to be analyzed. According to L153, Eq9, and L168-170, the neighbor pixel selection will be performed for each class at every pixel location, that is the time complexity is proportional to the number of pixels, categories, and the number of unioned events. Since semantic segmentation is a dense prediction task, such a computation cost may not be neglected. 
The example presented in L155-158 is not convincing to support the argument in L154. In this example, the author only considers one specific neighboring pixel which has a higher probability for the first class, but for the second class, it should search again with the Eq9 and that is not necessary to select the previous neighboring pixel but to select a pixel whose probability for the second class can most contribute to the interested uncertain pixel in a predefined neighborhood. Especially at edge pixels where the category changes, this kind of argument is hardly guaranteed. 
L158-160 argues that the proposed CMR module can prevent the creation of over-confident predictions. However, the refined prediction \tilde{p}_c((x_{j,k})) = p_c(x_{j,k}) + p_c(x_{l,m}) (1 - p_c(x_{j,k})) based on Eq8 will definitely be greater than or at least equal to the original one p_c(x_{j,k}), that is the refined prediction will become more confident. It seems the argument is not reasonable.
The difference between S4MC+CutMix-seg and S4MC-FixMatch requires more explanation. Since the student-teacher modeling framework seems to be the default setting in this work and the CutMix operation is normally taken as a strong data augmentation in a semi-supervised semantic segmentation task[1][2], does that mean the latter method uses a stronger data augmentation to achieve a better performance? If it is, then what is the data augmentation? 
The latest comparing methods is published in CVPR2022. Due to the rapid development of the semi-supervised semantic segmentation area, it is necessary to compare the proposed approach with more up-to-date methods, such as [1][2]. It seems that the proposed method is comparable to [1] and inferior to [2] in the CutMix setting.
[1] Semi-supervised Semantic Segmentation with Prototype-based Consistency Regularization, NeurIPS2022
[2] Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation, CVPR2023


Limitations:
NA

Rating:
6

Confidence:
5

REVIEW 
Summary:
To incorporate spatial contextual information in Semi-supervised segementation, the authors proposed S4MC, which refines confidence with the help of neighbors. This interesting refinement scheme aids in both adding falsely filtered pseudo-lables as well as removing erroeous ones. Additionally, a Dynamic Partition Adjustment module is employed to enable more pseduo-labeled pixels to be used. Results on several settings verified the effectiveness of S4MC.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* the confidence refinement scheme is novel and effective
* the evaluation is comprehensive and the results achieve new state-of-art
* the paper is well-written and the sturcture is clear
* easy to be followed

Weaknesses:
please refer to the questions listed below.

Limitations:
The authors should further discuss potential negative societal impact of this work and also should discuss the limitations of this work.

Rating:
6

Confidence:
4

";0
3xRaWBD2YB;"REVIEW 
Summary:
This paper studies the required neural network width for representing permutation invariant functions on sets. Existing works either focus on the case where the set elements are scalars, or require exponentially large neural network width with respect to the dimensions of the set elements. This work proves that, under certain assumptions, both the upper bound and lower bound on the required neural network are polynomial in the set size and set element dimensions.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The authors show that the bounds in this work are significantly better than existing results.

- This result implies that moderately wide networks are expressive enough to represent set functions.

Weaknesses:
- **Lack of discussions on the practical implications of the results.** It seems that the main idea in the proof is to choose the mapping $\phi$ in a clever way. I'm curious about the practical implications of the constructions in the proof. Particularly, the authors mention _""neural networks to learn a set function have found a variety of applications in particle physics, computer vision and population statistics""_. I recommend the authors discuss how the results in this paper can provide insights and improvements on some typical network models in those areas (e.g., GNN, PointNet).

- **Lack of experimental verification.** Although the bounds in this work look significantly better than existing results, experimental verification would strengthen the argument.

- **Presentation issues.** I believe the authors should polish the presentation of Table 1 (comparisions with exsiting results), and the statement of the main theorem (Theorem 3.1)
   - Table 1:
      + $D+1$ and $D$ should be $N+1$ and $N$.
      + Use big-O notation to present the results of Segol et al. and Zweig & Brun.
      + I recommend to present the upper bound and lower bound separately as two columns.
      + The meaning of ""Exact Rep."" should be explained in the caption.
   - Theorem 3.1:
      + In lines 167 & 169, ""For some"" should be deleted as these two lines are part of the _where_ clause. 
      + Eq. (2) $w_1x\to w_1^{\top}x$. Otherwise please specify that $w_1, ..., w_K$ are row vectors.
      + The main result contains both upper bound and lower bound. I recommend stating these two separately. Particularly, the lower bound should be stated as a negative result. Theorem 2.4 in [1] is a good example of how the result should be rigorously formulated.
      + In the LLE architecture setting, one more assumption $\mathcal{K}\subseteq \mathbb{R}_{>0}$ is required. The authors should highlight this as a premise of the theorem, instead of putting it in the _where_ clause. 

      [1] Aaron Zweig and Joan Bruna. Exponential separations in symmetric neural networks.
- **Minor errors**.
   - Line 28: DeepSets$\ \ $[9] $\to$ DeepSets [9] (There are two spaces between ""DeepSets"" and ""[9]"" in the submission).
   - Line 117: suggests $\to$ suggest.

Limitations:
The limitations have been discussed in lines 375-377.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies the representative properties of DeepSets models, which are networks for length-$N$ sequential modeling that apply identical neural networks to each $D$-dimensional input $x^i$ to obtain $L$-dimensional features, sum up their outputs, and apply an additional neural network to the output. While past results have tightly characterized the setting where $D = 1$, positive results for the $D > 1$ case are less explored. A recent lower bound by Zweig and Bruna shows that $L$ must grow exponentially with $\sqrt{N}$ and $D$ in order to approximate any permutation-invariant function under the condition that the networks use analytic activations.

The primary contribution of the paper Theorem 3.1, which claims that there exists a construction with $L = \mathrm{poly}(N, D)$ that exactly represents any permutation-invariant target as a DeepSets model with $L$-dimensional features.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The introduction is well-written and the problem is set up in a clear way. If the result is indeed correct, I think it would be an interesting contribution to the literature on DeepSets approximation properties and the limitations of current lower bounding techniques for DeepSets.

Weaknesses:
In it's current form, I am concerned that the result is not correct as written. There are two primary issues, which I would like to see addressed if I am to reconsider my score for the paper.

First, I think the proof of the second part of Theorem 3.1 is false as written. The proof relies on Lemma 4.9, which claims that parirwise alignment is sufficient for union alignment, and as far as I am aware, is not proved in the paper or the appendix. I believe Lemma 4.9 to be false. 

Consider the following counter-example. For $D = 3$ and $N = 4$, let $x_1 = (0, 1, 1, 0), x_2 = (0, 0, 1, 1), x_3 = (0, 1, 0, 1) \in \mathbb{R}^N$ and $x_1' = (1, 0, 0, 1), x_2' = (1, 1, 0, 0), x_3' = (1, 0, 1, 0)$. Note that $X = [x_1^T \ x_2^T \ x_3^T ] \not\sim X' = [x_1'^T \ x_2'^T \ x_3'^T]$, since the first row of $X$, $x^1 = (0, 0, 0)$ does not belong to any row of $X'$. However, the two are pairwise aligned. 
* Note that $[x_1^T \ x_2^T] \sim [x_1'^T \ x_2'^T]$, with the permutation $\sigma = (1 \ 3) (2 \ 4) $ as witness.
* $[x_1^T \ x_3^T] \sim [x_1'^T \ x_3'^T]$ with permutation $\sigma = (1 \ 2) (3 \ 4)$.
* $[x_2^T \ x_3^T] \sim  [x_2'^T \ x_3'^T]$ with permutation $\sigma = (1 \ 4) (2 \ 3)$. 

Since the proof relies on this lemma, the proof of the lemma does not appear, and there exists a simple counter-example, I am unable to accept second part of the main theorem as true.

Second, while I have not pinpointed any major technical flaws with the proof of the first bullet, I am a unsure how the result does not contradict the lower bound on Zweig and Bruna. The theorem claims that the LP architecture (which consists of features with polynomial activations and continuous function that inverts the embedding) with $L = O(N^5 D^2)$ is sufficient to exactly represent any continuous permutation-invariant $f: \mathbb{R}^{N \times D} \to \mathbb{R}$. Theorem 3.4 of ZB suggests that if $L \leq N^{-2} \exp(O(\min(D, \sqrt{N}))$, then there exists some analytic $g$ such that any DeepSets model $f$ with feature dimension $L$ and arbitrary NNs with analytic activations cannot approximate $g$.

As far as I understand both results, the only way these two results do not represent a contradiction would be if the continuous function that inverts the embedding $\rho$ is non-analytic, and hence cannot be represented the networks described by ZB. However, if our input $x$ is from a compact set (like the complex circle, from ZB) (and hence, the aggregated polynomial features $\sum_i \phi(x^i)$ belong to a compact set) and $\rho$ is continuous, then the universal approximation results for two-layer neural nets imply the existence of a sufficiently wide 2-layer neural network with analytic activations (like the sigmoid) that approximates $\rho$ arbitrarily well. I think this would contradict the ZB result on inapproximability.

All that said, it's possible that I've overlooked or misunderstood something. If there is no contradiction or if the authors believe that the result by ZB is incorrect, please let me know, and I'd be happy to discuss further.

Limitations:
My perceived limitations of the paper are detailed above. Beyond concerns over correctness, the theoretical results are direct about their limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
Summary: The paper proves that for symmetric neural networks, specifically DeepSets, there exist exact representations for symmetric functions, where the symmetric embedding layer width can be chosen to be polynomial in the set size and input dimension, rather than exponential as shown in stricter settings.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
Strengths: The proof technique is clever, managing to continuously invert a specific symmetric embedding.  It’s also quite surprising, as the set of multisymmetric polynomial generators is exponentially large in N and D, which intuitively suggests that a map from the symmetric set to a strict subset of the generators wouldn’t be invertible.  The insight that this map can be inverted (at the price of perhaps being quite non-smooth) is a novel one.  I think this is a useful result in further understanding the capabilities of the DeepSets architecture specifically.


Weaknesses:
Weaknesses: I think the paper would benefit from a more robust discussion of the tradeoffs of this parameterization.  In particular, in the complex setting and under some standard network assumptions, the parameterization doesn’t have exponentially large width in the symmetric embedding layer L, but it must pay exponentially large width somewhere else.

For example, consider input dimension D = 1 and set size N > 1.  Assume N is odd, and let z denote a N-th principle root of unity.  Consider input sets x = 1/2 * (z^0, z^1, … z^{N-1}), and y = -x.  One can confirm that all the power sums p_k(x) = p_k(y) = 0 for 1 <= k <= N-1, and p_N(x) = N * (1/2)^N and p_N(y) = -N * (1/2)^N.

If psi_N is the map of the first N powersums as in definition 2.6, then we have d(psi_N(x), psi_N(y)) = N * (1/2)^{N-1}, but d(x,y) = O(1/sqrt(N)) (where this is using the appropriate notion of distance on sets, i.e. infinity norm modulo permutation).

All this to say, the inverse map psi_N^{-1} has a Lipschitz constant that is exponentially large in N.  So for a neural network of constant depth, with an activation with bounded Lipschitz constant and polynomially bounded weights, representing this function would require exponentially large width.

Of course this is a toy example.  But it’s extremely difficult to tell how non-smooth and nasty the parameterization given will be in general, especially when D > 1.  I understand the authors leave this question to future work, but in that case I think it’s useful to discuss that there’s no free lunch, and the given argument does not guarantee an efficient network overall.

More broadly, the parameterization is somewhat unrealistic.  The parameterization focuses specifically on mapping the set data into a symmetric embedding, inverting the embedding, and then feeding the original set information through another parameterized function.  So one struggles to see why to use DeepSets in the first place.  You still need to parameterize a symmetric function somehow - it would be silly to parameterize rho itself with DeepSets, but then how would you parameterize it?  The fact that, in practice, DeepSets works better than networks that don’t enforce the symmetry constraint suggests this parameterization is not a very practical model.


Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This manuscript proves that an embedding with polynomial width in the set size and feature dimension is sufficient to precisely reconstruct a set function, under some constraints on the embedding layer architecture. The main contribution is on the upper bound, which removes assumptions of previous studies, and reduces the gap exponentially. The scope of this study covers permutation-invariant and permutation-equivariant set functions in high-dim scenarios. 

The key idea of the proof is to construct the embedding vector of the claimed length, and show the injectivity of any input set elements with the proposed embedding layers. The weights are used to (1) save the original features; (2) form an 'anchor' of the set element, which 'shares' the same permutation with the set element features; (3) store the coefficients that mix the original feature and the anchor.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The manuscript is clearly written. The problem is sound and well-motivated.
- The results are significant, extending previous study to high-dimensions and exact representation of sets, in addition to tightening the upper bound from exponential to polynomial.


Weaknesses:
The major weakness of this study has been summarized in the limitation section already: (1) There are two important parameters in DeepSet, however only one is considered. (2) The lower bounds are trivial. (3) The upper bound depends on the specific neural network architecture.

Another issue is the proof is an existential argument. That is if we find those weights, the network can precisely represent the set function. However it is not clear if the weights belong to some space as a convergence by training the network. If the authors could provide intuition on this it would be good.

It took me longer than expected to understand the functionality of the three segments of the weights, especially the third one. I would suggest one more paragraph describing the mentality of the construction, together with the Eq 5,6,7.

From the technique side, there is no novel technique involved, and this together with the major weakness would be my personal reasonable potential to reject this paper. But fairly speaking I think the depth is OK, and the results are sufficiently impressive. Thus I recommend accept. 

Limitations:
Mentioned above.

Rating:
6

Confidence:
3

";0
K3BMejPSyQ;"REVIEW 
Summary:
The performative prediction problem targets real-world application using deployed models, and this method  studies the optimiazation method for such a scenario. Since directly optimiazing the non-convex objective is challenging, this paper utilizes weighted Markovian samples of states to estimate the gradient and update with designed timescale step size.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is in general well-written.
2. The motivation is clear for tackling the targeted problem.
3. The theoretical derivation and understanding are well-presented.
4. The authors conduct various experiments settings to examine the the proposed method.

Weaknesses:
1. The experiments are conducted on toy examples and illustrate the efficacy of the method. The question would arise for applying the method for more challenging task setting. Also, how does the method perform in comparison with other strategies that also fit with such tasks?

2. What the limitation of the proposed method? As it targets online deployed models and involves the Markov Chain sampling, what is the computational cost?

Limitations:
Please see the weaknesses and questions part.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes a derivative-free two-time scale  for performative prediction problem. The two-timescale facilitates a faster accumulation of samples to compute a gradient with smaller bias. For smooth nonconvex objective, the work proves a iteration complexity of $O(\epsilon^{-3})$ for the $\epsilon$-staitonary point. They illustrate results via numerical experiments.

Soundness:
4

Presentation:
2

Contribution:
1

Strengths:
1. This paper deals with an important problem: derivative-free optimization of nonconvex function under performative prediction setup. 

2. The convergence rate seems reasonable. 

3. The experiments are well-motivated.

Weaknesses:
1. **The applications are shown for squared loss. But it is not a bounded loss and does not satisfy Assumption 3.2. This leads me to believe that Assumption 3.2 is made for the convenience of theoretical analysis.**

In fact, **one of the major challenges in the decision-dependent noise or state-dependent Markov noise setup is that the algorithm is often not stable** for unconstrained optimization (see [1,2] below). Assumption 3.2 helps to avoid this issue.

**So Assumption 3.2 is crucial for Lemma E.3 and Lemma B.i (i=1,2,3,4). Then the theoretical analysis of the algorithm does not apply to squared loss which is one of the most popular loss functions.**

2. The proof techniques required are pretty similar to Wu et al., 2020 ([3] below). In fact, **the main result is almost same as Corollary 4.9 of Wu et al., 2020. It's just that Corollary 4.9 of Wu et al., 2020 is wrapped in the cover of reinforcement learning but the algorithm, underlying proof techniques, and result are same.**  Wu et al., 2020 does achieve a faster rate of $\tilde{O}(\epsilon^{-2.5})$ although I guess here the poor rate is due to derivative-free algorithm. 


[1] Liang, Faming. ""Trajectory averaging for stochastic approximation MCMC algorithms."" (2010): 2823-2856.

[2] Andrieu, Christophe, Éric Moulines, and Pierre Priouret. ""Stability of stochastic approximation under verifiable conditions."" SIAM Journal on control and optimization 44, no. 1 (2005): 283-312.

[3] Wu, Yue Frank, Weitong Zhang, Pan Xu, and Quanquan Gu. ""A finite-time analysis of two time-scale actor-critic methods."" Advances in Neural Information Processing Systems 33 (2020): 17617-17628.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies performative prediction when the data is stateful, in particular generated via a controlled Markov chain, and the learner's loss is possibly nonconvex. The paper develops a two-timescale derivative-free optimization algorithm for this setting and shows a O(1/eps^3) sample complexity for finding a point with squared gradient norm at most eps.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
There aren't many convergence results in performative prediction for nonconvex settings, so this is one strength. The stateful setting has been studied before in several papers, but this paper studies this setting under quite a bit of generality. The treatment of the stateful setting in this paper is probably my favorite treatment of the setting in the literature. The paper is very clearly written and easy to follow, also providing intuition for the ideas behind the analysis, which I appreciated.

Weaknesses:
There are some limitations to the conceptual novelty, in the sense that a similar algorithm has been studied outside of performative prediction, and the controlled Markov chain model for the distribution map has been studied. It would be good to be more precise about the differences to the recent works in performative prediction studying the stateful setting (bottom of page 2).

The problem is perhaps arguably a bit niche since it goes away if the performativity is not stateful, which is the most commonly studied observation model in performative prediction. In that case, a simple application of the Flaxman et al. ""gradient descent without a gradient"" algorithm suffices. The issue in this paper is that we can't sample from the stationary distribution directly so a naive application of the Flaxman et al. algorithm doesn't work. All this being said, the stateful setting is very well-motivated and deserves its own analyses, and this paper gives a clever solution.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper is on performative prediction i.e. a setting where the data distribution changes in response to the predictions of a learned model. The prototypical example of such a setting is where a learned model is used to make loan decisions, and then people or companies adjust their behavior based on knowledge of the learned model in order to secure more favorable loan terms. This paper extends prior work on performative prediction to the setting where the data distribution follows a controlled Markov process, where the control is given by the predictive model. While this setting has been studied before, this paper removes previous structural assumptions on (1) the loss function used to evaluate the model and (2) the dependence of the data distribution on model. Notably the loss function is not assumed to be convex, and so convergence is established to a stationary point (i.e. a point where the magnitude of the gradient is small).
The algorithm designed is based on standard methods for derivative-free optimization, with a two-timescale step-size modification that updates model parameters more slowly than the generation of gradient estimates in order to deal with high variance in the gradient estimator.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The extension of algorithms for performative prediction to the setting without convexity of the loss expands the range of techniques for such problems. The paper clearly discusses the differences with prior work and gives a concise and intuitive overview of the modifications required to make derivative-free optimization work for this setting.

Weaknesses:
While I do appreciate the generality of extending algorithms for performative prediction beyond convex losses (as well as beyond the mixture dominance assumption on the data distribution), there are a couple of limitations that arise from assuming so little about the loss and data distribution.

1. The algorithm converges to a stationary point of the performative risk, rather than to a point that approximately minimizes the preformative risk. Of course this is necessary in the completely unstructured setting considered here, but it is clearly a weaker statement than e.g. the initial work of Perdomo et. al. that showed (for strongly convex losses) convergence of repeated risk minimization to a point that is close to an actual minimizer of the performative risk. This paper should state more clearly that such a strong result cannot be hoped for in the setting considered. In particular, the claim on line 43 seems somewhat misleading as written.

2. Removing the convexity assumption seems to come at the cost of introducing an additional assumption. In particular, Assumption 3.3 requires that the distribution map $\Pi_{\theta}$ is Lipschitz with respect to the total variation distance, rather than the Wasserstein 1 distance used in prior work. This is a **much stronger** assumption in many natural settings e.g. the mean of $n$-independent random $\pm 1$ valued variables has a distribution that converges in Wasserstein 1 distance to the Gaussian distribution at a rate of $\frac{1}{\sqrt{n}}$, but has the maximum possible total variation distance of 1 from the standard Gaussian distribution.

3. The algorithm has a slower convergence rate than those in prior work, as the authors show is necessary in this highly general setting. The original paper introducing performative prediction shows that repeated risk minimization converges at a linear rate, and thus allows us to leverage whatever fast optimization method fits the particular problem for each risk minimization step. In contrast, the algorithm in this paper requires us to essentially sample many, many random perturbations of the model and then deploy/evaluate each one in order to obtain gradient estimates, making it highly impractical for real-world problems.

To summarize, the generality of the setting has several drawbacks in terms of the solution quality, performance of the algorithm, and most notably the additional assumption described in (2) above. While some of these drawbacks are provably necessary (again with the notable exception of the issue in (2)), taken together they suggest that perhaps this complete lack of structure is not a particularly good model of the types of problems for which performative prediction is interesting.


Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper considers the performative prediction problem, where the goal of learner is to optimize the expectation of the known loss function over a decision-dependent unknown data distribution that evolves according to an underlying controlled Markov chain. Authors presents a stochastic derivative-free optimization algorithm $DFO(\lambda)$ that achieves $O(d^2/\varepsilon^3)$ sample complexity using gradient accumulation mechanism and two-timescale diminishing step-sizes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The presented algorithm achieves a $O(1/\varepsilon^3)$ sample complexity under a challenging setting of Markovian data.
- The other possible types of gradient estimator were considered that creates a more complete picture in the presented setup.
- The presentation of the paper is clear.

Weaknesses:
- No presented lower bound for this problem, thus it is not clear is the presented rates improbable or not.

Limitations:
This is a theoretical paper that does not need to address the potential societal impact.


Rating:
6

Confidence:
2

";0
H5pwAeYAun;"REVIEW 
Summary:
1. A failure-aware gaussian process optimization algorithm is proposed. A key difference from existing GP constrained optimization is that here we cannot obtain a direct observation from the latent failure function.
2. The algorithm's regret bound is presented.
3. Some practical issues are discussed.
4. Numerical experiments are conducted, the proposed algorithm shows superiority on some benchmarks.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Problem formulation is novel.
2. Theoretical derivation is solid
3. Practical issues are considered and numerical experiments show its potential. 

Weaknesses:
1. Learning feasible set without knowing failure function is a key feature of paper's problem formulation. The NP-complete set covering problem is assumed to be fully solved in theoretical analysis while numerical experiments adopt a heuristic approximation algorithm.  It seems there is a gap between theoretical part and numerical part. 
2. Algorithm doesn't show dominant superiority on all all numerical benchmarks. 


Limitations:
No potential negative social impact is seem.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies Bayesian optimization in the setting where observing the unknown function might fail, for instance due to numerical simulator returning NaN. The authors propose a UCB-type acquisition function for this setting, which is based on first restricting the domain to a feasible regions where failures are less likely to occur, and then applying standard UCB-based acquisition function constrained to that region. The authors present a bound on the simple regret in this setting, and perform some experiments which consider only low-dimensional examples but show good performance compared to baselines.

EDIT: I've updated my score in light of the rebuttal.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
The paper's topic is important and of key relevance for practical Bayesian optimization, where observation failure is a common issue which is mostly addressed using ad-hoc methods. They have clearly described their setting and distinguished it from others, and everything is properly cast into a well-defined mathematical problem, which is clearly described.

The introduction is well-written and does a good job of introducing the reader to both the setting and the authors' approach, and gives due credit to prior work.

Figure 1 is conceptually very nice and provides a good illustration that makes it easy for the reader to see early on what is going on: the authors' algorithm splits the state space into a region where observations are likely to work, and another region where they are unlikely to work. One request: **please make the fonts bigger to match the font size of the text**, so readers with not the best eyesight can actually see everything.

The authors do a good job at focusing on their part of the contribution, and don't take up lots of space stating obvious corollaries of their result, but at the same time also do a good job at stating what else can be done, for instance by combining their analysis with prior work such as RBF specific bounds in the setting without failures.

Overall, I think this is a good paper with various fixable flaws described below, so I don't have too many additional comments.

Weaknesses:
Details of the proposed algorithm are unclear. From my reading of the paper, I understand that what you do is define a ""failure-safe space"" consisting of the full space minus a set of infinity-balls around points where failure occurred. Then, at certain points in time whose definition I still don't fully grasp, you cut the radius of the balls by half, so that points which were previously excluded from consideration because they were too close to a failure are now considered again. Is this correct? If so, this needs to be stated much more explicitly and much more clearly, using an intuitive description in addition to equations or notation-heavy pseudocode. If not, please correct my understanding.
* In particular, please provide a more intuitive explanation of what \theta_t is doing, and in what cases it gets cut by half - I am still confused how when this happens

Experiments
* Unless I have missed something, the highest dimension problem used in experiments is three. This is very limiting compared to much of the literature and much smaller than many problems of practical interest
* The quasicrystal experiment is very light on details, in spite of being a really cool illustration. This section deserves significantly more text on both the setup and results so that readers can understand and contextualize what is going on. The fact that this problem is, unless I am mistaken, three-dimensional, could also have been stated more explicitly

In the analysis, the regret bound presented seems fairly specific, since it is talking only about simple rather than cumulative regret, and has a fairly specific way through which the point returned is selected. In bandits at least this kind of setting is certainly out there, but not something I see in every paper, so I'm a bit unsure as to whether the ideas are general or very specific to this setting.
* As a result, I'm curious: how strongly does the theoretical analysis and empirical performance depend on the specific way through which the point returned is selected?

Writing: the paper tends to go back and from between parts that were very clear and well-written, and parts where I was confused. Part of this is due to typos and grammar, which is fixable. However, there are other times where grammar is not the issue, and there are many points where the paper would be much better if it provided helpful hints to the reader to orient their reading in the right way to avoid them becoming confused. Detailed presentation comments below:
* The paper has slightly too many acronyms, for instance GPC, which is not an important enough concept in this paper to make it into an acronym and should instead be spelled out
* There's slightly too much discussion on prior work, which takes up space that is needed elsewhere - I'd prefer to see this part tightened up
* When I first read this, I was confused about definition of regret, namely why is r_t = f(x^*) - min_{x\in X} f(x) not equal to zero, but then I realized this is because the authors are interested in reward maximization (bandit style) rather than cost minimization (online learning style). A line which saves the reader time by clarifying this should be added.
* Aesthetic: curly brace notation for the posterior is really ugly - consider using parentheses
* **Please number all equations, not just the ones you think are important.** The convention of only numbering important equations is archaic, outdated, and harmful to readers. Numbering everything prevents readers from having to ask about ""the second unnumbered equation on page 2"", and then the person who was asked is confused whether it's the second equation from the top or from the bottom. Doing so will significantly improve the paper.
* Why should the logdet be called the maximum information gain? If you're going to use these terms, you should connect this with mutual information or some kind of related quantity. This choice of terminology should be explained rather than just parachuted in.
* Since the notion of ""interior point"" is introduced in the definition directly below it, maybe the sentence should include ""introduced below"" or similar.
* Assumption 2.2: please add a textual note that this is an infinity-ball, not a Euclidean ball, so that someone who misses the infinity symbol doesn't get confused
* Proposed algorithm could be replaced with a more informative section title
* No need to introduce packing numbers with too much detail, the audience of this paper will know about that

Limitations:
*Almost* adequately - the main thing that is missing is a more comprehensive discussion on the dependence of their proposed methods on the dimension of the unknown function's domain.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper extends the gaussian process UCB algorithm to the case where some observations are failures. i.e. They solve for the case where x^* = argmax_{x in X} {f(x) such that c(x) =0}. Here c(x) = 1 may be thought of as an indicator of failure. The crucial part is to adaptively ignore the neighbourhood of the failure point to choose the next evaluation point. The paper does a number of experiments to show that their algorithm works with respect to the claims made.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Excellent numerical experiment on quasicrystals. This was quite novel to consider the Al-Cu-Mn crystal system.
- The idea of applications of GP-UCB to account for failures of observation was quite novel.
- The notation of failure as a simple function c() was well formulated.

Weaknesses:
- The model setup is slightly weak. For example, has consideration been given to stochastic c(·)? How easy is it to extend to unbounded domain beyond [0,1]^d?
- The definition of r_t says that when \hat{x}_t is not observable, r_t is f(x^*) - min_{x in X} f(x). This is just 0. Shouldn't regret be based on an output of the algorithm? If non-observable, the algorithm can simply output an element from observed data that it is sure is observable. Why are you defining regret to be 0 in such a case?
- There could be improvements in paper clarity. For example:

    > Please could you clean up the notations for kernel function k? Since it plays a central role in the theorems, it would be useful to setup the relationship between k(·) and the optimization function f(·) a little more cleanly.

- The authors claim that ""Then, we provide the first regret upper bound of the GP optimization problem (1)..."" But from my understanding, this well cited paper from 2009 provides an upperbound for GP-UCB (see Theorem 1)? https://arxiv.org/pdf/0912.3995.pdf


Limitations:
- Seems like a minor improvement over non-failure aware GP based optimization Algorithms. 
- If you disregard failures in GP-UCB, does it not converge? If so, why is the regret on that algorithm in your experiments never below 1 (what is the function c you have considered)?
- Seems like scaling with dimension d is quite poor? Does random restart help in such cases?

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper investigates GPBO under a robust setting, where the failure of observations is taken into account in the overall budget.  An adaptive algorithm based on GP-UCB has been proposed to address this problem, and both theoretical and practical behavior have been analyzed and verified.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall, this is a well-written paper with comprehensive analysis of the proposed algorithm. The empirical studies also demonstrate that the proposed algorithm is effective for at least low-dimensional problems.

Weaknesses:
The empirical behaviour of the proposed algorithm is only verified for dimensions less than 3, and it would be nicer to see more practical settings.  In such cases, a vanilla GP-UCB might work even better than a specially designed algorithm.  Thus, It might be interesting to explore the trade-offs in practice between the two approaches.

Minor:
line 154 ""computes""

Limitations:
NA

Rating:
6

Confidence:
5

";1
PL4WWjvm9D;"REVIEW 
Summary:
This paper proposes a framework which extends an existing pre-trained classifier to a full quantile representation of the target data. This is done by setting the pre-trained classifier as the median classifier and by optimizing over pinball losses over different quantile values. Experimental results show that the proposed framework is competitive in out-of-distribution detection and calibrating models.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper proposes an idea which extends a pre-trained classifier to a whole quantile representation of the data. The framework is based on a simple observation which connects probability with classification quantile levels. Overall, the paper is well-written and easy to follow.


Weaknesses:
- The main algorithm is based on the monotonicity property. However, monotonicity is not guaranteed in general. In particular, the statement ""Using eq.2 enforces the solution to have monotonicity property"" is misleading. This is true only when the global minimum is obtained, which is often not the case in neural networks. Even minimizing with eq.2 can lead to a model that is not monotone, and monotonicity is enforced in model design in real practice. See, e.g. [2,4]

- In the experiments, the proposed framework is only compared with a simple baseline model. Therefore the results are not too convincing.

- I do not agree with the statement that in modern machine learning, loss functions are restricted to pinball loss or MAE. Eq.2, also called continuous ranked probability score is widely applied in quantile regression as well as machine learning. 

- In order to perform algorithm 1, we need to minimize multiple pinball losses on different quantile levels (and use the data). The overall costs seem to be similar to minimizing eq.2 with the expectation estimated by sampling (similarly to eq.8) or numerical integration. Therefore I am not sure about the statement that ""pinball loss/eq.2 is difficult to optimize""

- Overall, I am not too convinced that the proposed framework is more preferable than optimizing an estimated eq.2. I recommend the authors to emphasize the importance of preserving the original property of the pre-trained classifier, and to experimentally show that in some aspects, the proposed framework can compete with a quantile regressor obtained by optimizing over an estimated eq.2.

- The related work section on quantile regression tends to only include classical paper, and does not cover enough recent papers on quantile regression with machine learning. For instance, in probabilistic time series forecasting, there is a collection of work which successfully applies quantile regression with machine learning, for example, [1-3]  

[1] A multi-horizon quantile recurrent forecaster by Wen et al. 
[2] Probabilistic forecasting with spline quantile function RNNs by Gasthaus et al. 
[3] Multi-horizon time series forecasting with temporal attention learning by Fan et al.
[4] Learning quantile functions without quantile crossing for distribution-free time series forecasting by Park et al.

Limitations:
As mentioned in the weaknesses section, the framework is based on the monotonicity of the model. However, the monotonicity is not guaranteed in real practice.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposed a method to construct quantile representations of binary classification problems. This construction does not require to use the pinball loss, resolving the difficulty of optimizing the pinball loss. The paper verified its method through experiments. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The proposed approach looks novel and uses an interesting property of pinball loss. 

Weaknesses:
The notations in this paper are confusing, making it hard for me to understand the proposed method and the proof of the theoretical result. 
1. The minimizer of (1) f_\theta is claimed to be a quantile function. I am confused about which random variable this quantile function belongs to, Y or Z? Y is a binary random variable, and Z is a deterministic function of X, according to line 73-74. That means Z should not have a conditional quantile function (since it is a deterministic function of X), and the quantile function of Y can only take values in {0, 0.5, 1} (since Y is binary). 
2. Eq. (2) and Eq. (7) are different. In Eq. (2), there is no indicator around \psi, whereas in Eq. (2), there is an indicator around \psi. So it is confusing whether Q should be a function inside the indicator, or a function outside of the indicator. 
3. In Algorithm 1, the 4th bullet point, it is not clear which loss is used to train the classifier. Note that the loss function should be important for Theorem 3.1 to hold: Theorem 3.1 is claiming the minimizers of two empirical risk minimization problems (not population risk minimization) are equivalent, and it cannot be the case that they are equivalent no matter which loss function is used.  

Limitations:
NA

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposed an effective approach to estimate the uncertainty for the classification model. By decoupling the quantile representation from the loss function, the proposed method works for arbitrary base classifier.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. this paper discovered the duality between the probabilities and quantile, and proposed a method to decouple the quantile representations from the loss function for pre-trained classification models;
1. the proposed method demonstrated effectiveness on two problems: out-of-distribution (OOD) discovery and model calibration. 

Weaknesses:
1. major weakness, as mentioned by the authors, is the scalability of the proposed method wrt large scale datasets and models.

Limitations:
no potential negative societal impact of the work is noted to the best knowledge of the reviewer

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper aims to address a fundamental question: given a pre-trained classifier $f_{\theta}(x)$ and its corresponding training dataset, how can we compute quantile representations to conduct a detailed analysis of the pre-trained classifier? The authors investigate alternative approaches to this problem beyond the classical method of retraining the model using the pinball loss.

In their work, the authors adopt a straightforward approach that involves decoupling the loss function from the computation of quantile representations. By taking this approach, they propose a novel solution to efficiently analyze the pre-trained classifier without losing its important properties.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:

The primary contribution of the paper is centered around Section 3.1. Within this section, the authors focus on the binary classification problem and highlight the duality that exists between quantiles and probabilities in this specific scenario. They recognize this duality and propose an algorithm that harnesses the power of a pre-trained binary classifier to derive the desired quantile representation

Weaknesses:
The contribution of this paper is limited for the following reasons:

1. **Specific to binary classification**: The proposed algorithm for generating quantile representations is based on the duality between quantiles and probabilities, which is evident in the context of binary classification. However, this duality may not hold true for general classification problems. As a result, the applicability of the proposed algorithm is restricted to binary classification scenarios, limiting its broader impact.

2. **Unavoidable quantile crossing**: Since the optimization of quantiles for different $\tau$ values is performed independently, it is possible to observe quantile crossing in the final solution. This can introduce challenges in interpreting the results and may affect the overall performance of the proposed algorithm. Additionally, there seems to be a discrepancy between Equation (7) and the proposed algorithm. Equation (7) suggests obtaining all quantiles simultaneously, while the algorithm indicates that the quantiles are obtained separately. This discrepancy raises confusion and needs further clarification.

3. **Extensive experiments need** :Another limitation of the paper is the lack of extensive experiments. In Section 4.3, the authors mention several methods mentioned in the references that are used to enhance the calibration of deep learning models. However, these methods are not included as baselines in the subsequent experiments. As a result, the proposed method is only compared with simple MSP, which may not provide sufficient evidence to support its effectiveness and superiority.

Limitations:
.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper tackles the problem adapting quantile regression (QR) to  deep neural networks for classification tasks. QR takes into account the distribution of the dependent variable and fits a model that gives low error for all quantiles. Logistic regression is a special case of QR where only the tau=0.5 (median) quantile is considered. In previous work, application of QR is usually coupled to the loss function or the model used. In this paper, they decouple these things, so they can theoretically apply QR based learning to any loss function and any network. They establish a duality between quantiles and estimated probabilities in binary classification. Based on this duality, they train separate classifiers for different taus based on the predictions of the median (tau=0.5) classifier. Authors experimentally validate their method in two tasks: out of distribution detection and showing robustness to distortions. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
+ Tackles an important problem: although quantile regression (QR) has a solid history in econometrics and statistics, it is relatively new in the machine learning community. Its application to modern deep neural networks is potentially useful and interesting. 

Weaknesses:
- Limited evaluation. For out of distribution (OOD) detection, they compare their results only with OneClassSVM, a method from 2017. There are stronger and more recent methods in the literature. For example, in the ICLR2022 paper titled ""Open-Set Recognition: A Good Closed-Set Classifier is All You Need"", they compare many SOTA methods for this task and arrive to the conclusion that ""maximum logit score (MLS) thresholding"" is a strong method. The authors should have at least compared their results with MLS. Of course, there are many more relevant methods, which are all cited in that ICLR2022 paper. 

- Language and presentation issues prevent an easy reading of this paper. A non-exhaustive list follows. 
- L26: ""However, these techniques aren’t widely used in modern deep learning based systems since [5]"" -> what  does this mean? Is ""since"" the wrong word? 
- L28: ""might not compatible with"" 
- L55: ""taken to in-distribution"" 
- Fig 1 is not easily understandable. E.g., an intuitive explanation as to why we get those classifier in 1 (b) for different taus? which classifier correspond to which tau? Transition from 1(b) to 1(c) not clear. 
- L76: what is l in the subscript of f? 
- In the last step of Algorithm 1, you wrote ""train the classifier"" but I think you mean ""train a separate classifier"", right? 
- I found Fig2 to be completely not understandable. I cannot even point to any questions. 

Limitations:
While listing the areas where QR is applied it would be useful to provide citations (L108). 

Rating:
3

Confidence:
3

";0
14R8QBKzFH;"REVIEW 
Summary:
This paper studies the machine unlearning problem from the perspective of differential privacy. Specifically, the authors propose to use differentially private models directly so that unlearning update is not necessary (or unlearning is an identity map), and the motivation is to make the unlearning procedure independent of side information (i.e., original training set) to avoid privacy leakage. A tight lower bound on the number of data points that such kind of DP model can be unlearned is shown in the paper, along with some new proving techniques.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper uses the idea from Renyi DP and zero-concentrated DP to optimize the DP parameters, and thus refine the lower bound result proposed in previous work by Sekhari et al. An interesting combination of off-the-shelf results from DP literature is used to obtain the improvement of deletion capacity lower bound from $\widetilde{\Omega}\left(\frac{n \varepsilon}{\sqrt{d \log \left(e^\varepsilon/ \delta\right)}}\right)$ to $\Omega\left(\frac{n \varepsilon}{\sqrt{d \log (1 / \delta)}}\right)$. Furthermore, a deletion capacity upper bound is studied when the loss function is linear, showing that the proposed lower bound is actually tight.

Besides the improvement in the lower bound, the authors also introduce the post-processing, chain rule, and composition theorem for unlearning analog to classical DP. This could benefit future works to study unlearning via differential privacy.

Weaknesses:
Although this paper may be interesting in theory, I do not think it can fit nicely into machine unlearning problems. The main motivation in the paper to use DP directly for unlearning is to avoid the usage of side information, which basically means the original training set. However, for most unlearning scenarios, having the entire training set is not a critical issue, and sometimes even a must when we need to perform model retraining. Consider the case when the size of data points that require deleting is larger than the deletion capacity. We have no choice but to retrain the model from scratch, and we need the remaining training samples to complete the retraining. So I do not think the motivation holds in the first place. Furthermore, a related discussion on the difference between DP and unlearning appears in the previous work by Sekhari et al. (Section 3.2: Strict separation between unlearning and differential privacy), where they show that if one designs the unlearning update carefully, the deletion capacity can be improved to $\Omega(\frac{n \sqrt{\varepsilon}}{(d \log (1 / \delta))^{1 / 4}})$, which enjoys better dependence on the dimension $d$ even compared to the results presented in this paper. As a matter of fact, many unlearning papers have pointed out the fact that using DP directly can lead to large overhead and low utility. Therefore, whether one can use the theoretical results presented in the paper to design better unlearning algorithms is questionable.

Limitations:
The results do not fit nicely with machine unlearning problems, and it would be hard to utilize the results to design unlearning algorithms.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper studies connections between machine unlearning and differential privacy (DP). In machine unlearning, the goal is to remove up to, say, $m$ of the examples from a dataset of size $n$, in such a way that the produced model is close (in some form of statistical or computational distance) to the model that would have been produced if we did not have the m examples to begin with.

A recent paper of SAKS'21 formulates unlearning with the same style that DP is usually defined. This paper follows the same definition.

The main question studied in this paper is about ""deletion capacity"". Namely, how many examples from the dataset can we delete while we lose the accuracy of the model up to a given parameter $\alpha$ and keep the machine unlearning ""secure"" with specified parameters $(\epsilon,\delta)$ (defined similarly to DP)? More formally, $\alpha$ is the regret in the agnostic setting, which is the extra risk compared to the best model in the family.

The main result of the paper is to establish matching upper and lower bounds (up to constant factors) on the deletion capacity for algorithms that basically do not do any deletion and when certain (natural, but still limiting) properties hold on the models and the loss function. Namely, the paper studies how to achieve $(\epsilon,\delta)$ privacy when the comparison is made between datasets that have hamming distance $m$, rather than $1$, and while the regret is bounded by $\alpha$.

At a technical level, the paper achieves its tight upper and lower bound (on the deletion capacity, within its own defined framework) by actually *not* caring about achieving differential privacy in its standard sense and directly aiming to satisfy the DP Lipschitz property over $m$-close databases out of the box. To achieve tight bounds the paper moves to other notions of DP (based on Reny divergence) first and then comes back to DP after a more effective composition theorem (that exist for such DP style definitions) is applied.

The paper also studies composition of unlearning, but only the statements are stated in the paper and no discussion is presented.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
At a technical level, studying the DP for $m$-close databases (with the motivation of doing nothing for unlearning!) is interesting, and finding tight bounds for such problem is cool. but I would have preferred a more direct depiction of the result up front by saying that what happens here is not really unlearning and is about DP for $m$-close datasets. Then, maybe depicting unlearning as a potential application would be good, since the application to unlearning comes with some limitations that prevent us from using the full capacity of what unlearning allows.

The proofs also look interesting and as far as I could tell, the difference between this work and previous work is explained well.


Weaknesses:
As explained above, I think the connection to unlearning is a bit far fetched, as it comes with strong limitations.

(has a related question)
What I understand is that the paper studies tight bounds for settings in which the unlearning is *not even done* and the closeness of the produced models holds due to the DP-like property over $m$-close data sets. I am not sure why this is equivalent to ""not storing anything besides the model itself"". If they are equivalent, this needs a proof.

Citations are not in good shape. Examples:
The work of Cohen et al is cited for initiating a formal study of ""the right to be forgotten"", while works like: https://eprint.iacr.org/2020/254 are done earlier.

The main question of the paper is about privacy vs unlearning, which is also studied in previous (uncited) works like
https://www.usenix.org/conference/usenixsecurity20/presentation/salem
https://arxiv.org/abs/2005.02205
https://arxiv.org/abs/2202.03460:


Limitations:
The paper is clear in that they only study specific ""unearning"" methods that come with limitations. It is mentioned that the limitation is not to store anything other than the model, but my understanding is that their limitation is to not do anything when unlearning. These seem different to me, but hopefully the author(s) will clarify this limitation in rebuttal.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper addresses the concept of ""machine unlearning"" within the framework of differential privacy. The authors provide tight bounds on the maximum number of data points that can be successfully unlearned without significantly impacting the model's accuracy. They also establish the analog of key properties of DP for machine unlearning. The paper introduces novel results for convex and strongly convex loss functions, as well as properties of post-processing and composition of unlearning algorithms.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper addresses the important and practical problem of machine unlearning, which enables individuals to request the removal of their data from trained models.
2. The paper builds upon previous work and provides enhanced theoretical results. It closes the gap between upper and lower bounds on the deletion capacity achievable by differentially private machine unlearning algorithms. 

Weaknesses:
1. The paper focuses primarily on theoretical analysis and proofs, but it lacks empirical experiments to validate the proposed machine unlearning algorithms. 
2. The paper considers convex loss and strongly convex loss functions in its theoretical analysis. While these assumptions may hold for some models and applications, they may not be applicable to a wide range of real-world machine learning models, such as deep learning models, which often involve non-convex loss functions. This limitation restricts the generalizability and practical applicability of the proposed algorithms.

Limitations:
1. Lack of experiments.
2. Assumptions on loss functions are constraints.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies approximate unlearning with procedures which do not store any side information (and satisfy differential privacy) in convex learning problems and establishes tight upper and lower bounds.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Machine unlearning has recently gained much interest owing to privacy regulations. The paper studies a certain formulation of approximate unlearning inspired via differential privacy, with the additional restriction of storing no side information. This formulation is particularly appealing since (a). storing side information can be expensive and prone to attacks and (b). it does not require new algorithms for learning and unlearning, but simply (existing) private algorithms. The paper studies the limits of such a formulation for convex learning problems, improving the upper and lower bounds in the previous works. The question is very natural and the authors resolve the loose ends remaining in the prior works.

2. The proofs of improved upper and lower bound are simple yet interesting: the improved upper bound follows due to the use of stronger group-privacy properties of zCDP, as opposed to approximate DP; and the lower bound establishes an interesting ""converse to group-privacy"" due the linear loss function under consideration.

Weaknesses:
1. While the problem is very natural, the final quantitative improvements are rather minor. For constant $\epsilon$, which is usually the case of DP, the bounds are same. There is also very limited discussion on the quantitative improvements compared to prior work.

2. The scope of the paper seems limited; the paper essentially ties some loose ends present in the prior analysis, which is mostly interesting for theoretical reasons. I don't know if the contributions could have impact on the larger area of machine unlearning.

3. The techniques largely borrow from the differential privacy literature. The lower bound instance and the argument of padding the sample multiple times are present in prior works.

Limitations:
The scope of the the work is limited to procedures which store no side information and satisfy approximate unlearning (in the spirit of DP). These restrictions basically leave DP procedures as candidate learning/unlearning algorithms.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work derives tight bounds for the deletion capacity of machine unlearning algorithms that are differentially private. These bounds are stated in terms of a deletion capacity formulated as a function of data points that can be removed before the estimation risk (an accuracy measure) becomes too large. The estimation risk (or the excess risk) is assessed over the sampling distribution of i.i.d. data points. The authors derived the results in detail for 1-Lipchitz convex loss functions and presented briefly the parallel result for 1-L strongly convex loss functions.

My assessment, consisting of strengths, weaknesses, and questions, can be found in the sections below.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
I find this a well-written paper. For a technical topic that is focused on the proof, the paper nevertheless walks the reader through while offering both clarity and insight. The structure of the paper is also quite reasonable, with the contribution overview very helpful. 


Weaknesses:
Please see my question in the Limitation section. I think the lack of comment for that question in this paper is its biggest weakness.

Limitations:
Looking at the form of the unlearning algorithm \bar{A} in Theorem 3.1, am I to conclude that the unlearning algorithm that is to achieve the tight bound as you present in this work is in fact an algorithm that literally *does nothing* to the deletion request, i.e. it outputs A(S) just as before? If my understanding is correct, this point calls for a moral discussion: yes the learning-unlearning pair satisfies Definition 2.5, but with the relaxation of alpha, epsilon, and delta, this technical argument will provide a slippery slope for justified inaction. Maybe your paper is not the first to consider this, but one more paper gets published without paying due attention to these ethical questions, the literature grows a bit more oblivious to common sense.

Rating:
7

Confidence:
3

";0
CWnzWMLk3P;"REVIEW 
Summary:
The authors propose a new Asynchronous Federated Learning algorithm, FAVAS. They provide theoretical convergence analysis of the proposed algorithm as well as present experimental results. The main motivation behind their framework is to handle heterogeneities in the computation speed of clients in asynchronous settings. The work’s fundamental contributions are: 1) Compared to the algorithm proposed in QuAFL (Zakerinia et al., 2022), FAVAS weights updates of each contacted client according to their number of local iterates which is allowed to be different across clients. Its asymptotic bound on the total number of clients (n) is improved. 2) Authors present that the proposed algorithm is experimentally better than baselines on several datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed algorithm seems efficient in terms of the utilization of clients. In conventional FL analysis, clients generally start computations when they are interrupted by the server. Here, clients always do computation (until $K$ local steps) and send the update when they are interrupted.

2. Although the algorithm is similar to QuAFL (Zakerinia et al., 2022), the authors redesigned local updates and local weighting so that aggregation becomes unbiased. I like how they highlighted the differences in Algorithm 1.

3. The mathematical analysis of the convergence seems strong and doesn’t use any unnatural assumptions. The intuition behind the proof abstract is well explained. In particular, I like the explanations in the main paper while the full proof is presented in Appendix.

4. Both in algorithmic and experimental analysis, the authors provide a good comparison with baselines in the literature. The authors successfully present their experimental setup details.

Weaknesses:
1. A discussion on the relation between the number of local iterates and training quality can enrich the study. In the proposed framework, the number of local iterates that slow & fast clients take are random and depend on the server interaction time and clients' computation time. On the contrary, in most baseline methods, the number of local iterates is a parameter that we set and directly affects the training time. For a fair comparison with baselines, some information (maybe the mean and standard deviation statistics) on the number of local iterates during training may be helpful. 

2. Lack of novelty: As the authors stated, the proof strategy and the algorithm are inspired by QuAFL (Zakerinia et al., 2022). I suspect the theoretical novelty in this work is above the threshold for the conference requirement.

Limitations:
Please see question 3 in Questions. The authors discussed both theoretical and experimental extends of the study adequately.
 
The authors also discussed the “Environmental footprint” of their study. The reviewer thanks them for their responsible attitude towards our environment.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a new asynchronous federated learning update scheme to deal with the biasedness of conventional asynchronous updates which favor fast clients. The proposed method involves two main contributions, one is the direct local model update and the other is to reweight the contributions for the selected clients. The authors provide theoretical convergence analysis for the proposed method under stochastic non-convex settings. The numerical experimental results of the proposed method outperform other baselines in most settings. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well structured with clear motivations. The theoretical analysis and numerical results are clear to support the intuition. 
 
2. The analysis of the proposed method can get rid of the constraint of bounded gradient and the uniformly bounded delay, which among the analysis of papers studying asynchronous FL, only very few of them do not require both conditions.


Weaknesses:
1. Some key information about the reweighted scheme need some further illustration. 

2. In my opinion, one difference between the proposed method and some previous baselines such as FedBuff and FedAsync is that the proposed FAVAS enables different clients to perform unfix steps of local training during two communication rounds, and the reweighted scheme is proposed to overcome the biasedness of unfix local steps. The independent number of local update steps to perform in each round based on the communication status shares a similar idea as [1]. Thus, the idea of FAVAS is similar to combining the QuAFL and [1] into a single framework.
 
3. One related paper [1] about federated learning could be discussed. 

[1] Yang, H., Zhang, X., Khanduri, P., & Liu, J. (2022, June). Anarchic federated learning. In International Conference on Machine Learning (pp. 25331-25363). PMLR.

Limitations:
see above

Rating:
4

Confidence:
4

REVIEW 
Summary:
Authors introduce a novel asynchronous centralized Federated Learning (FL) algorithm which corrects for clients with varying computational speeds (and delays). Theoretical convergence guarantees are provided as well as strong empirical results. Solves an interesting issue of model degradation when faster clients dominate asynchronous updates.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. Well-written paper with nice tables and clear algorithms.
2. Strong theoretical backing which align with peer algorithms (without favoring faster clients).
3. Impressive empirical results which showcase the algorithms importance in realistic (heterogeneous) settings for FL.

Weaknesses:
1. Novelty of paper seems to lie solely in the reweighting of fast/slow clients. This is important but seems to be the only main contribution. Algorithm is extremely close to QuAFL otherwise.
2. Convergence is theoretically degraded when a large subset of devices are sampled to participate (s/n ≪ 1). This is a cause for concern in realistic applications.

Limitations:
Limitations are not explicitly outlined in a section. I would recommend adding into the appendix at a minimum.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this work, the authors propose an asynchronous communication protocol based on federated averaging, called FAVAS. The proposed method builds on top of a previously released method in asynchronous settings by addressing the bias and variance limitations of the previous approach through an unbiased estimator based on local reweighting. The authors make their claims clear with both theoretical and experimental results.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The paper is easy to follow, with good structure and motivation.
 
(2) The paper also provides a thorough related work discussion, convergence analysis, and empirical evaluation.

(3) Theoretical and empirical variance analysis reduction for asynchronous settings through unbiased local model reweighing. 


Weaknesses:
(1) Missing background makes it hard to understand essential concepts of the proposed framework.

(2) The work needs additional clarifications in the numerical evaluation and convergence bounds.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel algorithm called FAVAS for asynchronous federated learning with heterogeneous clients, that is the modification of the previous QuaFL algorithm. The paper provides theoretical convergence rate of the proposed algorithm, and experimental evaluations of the effectiveness of their method. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The studied question is important and well posed. 

- Experimental results show a large improvement of FAVAS over the previous QuaFL, and some improvement over FedBuff. 


Weaknesses:
Overall the paper is hard to read. The major concerns are unclear presentation of the theoretical results, and limited experimental evaluation.

1. In experiments the learning rate was set to some specific parameter 0.5, which is not a fair comparison. For a fair comparison, it is good to tune the learning rate separately for each of the methods. Same as for fair comparison it is good to tune the value of Z too. FedBuff proposed to set Z=10 when the delays are distributed close to the realistic FL system. However in your simulations the delays are distributed differently and thus the other values of Z might be optimal. 

2. The convergence rates in Table 1 are very hard to compare between each other. First, there are no estimates of how large $C_{FedAvg}, C_{FedBuff}, C_{AsyncSGD}$. Second, it is unclear how large $\sum_{i = 1}^n a_i / n $ might be, so it is unclear if the FAVAS leading term of convergence is better or worse than previous rates. 

3. The leading term  of convergence of FAVAS seems to be worse than QuAFL, as in QuaFL it is divided by E^2, while in FAVAS it is not divided by that. 

4. It is also hard to understand how tight the proposed analysis is. 

5. There are many other algorithms for asynchronous FL missing: such as R1, R2, R3. Experimental comparison to these baselines is also missing.

[R1] Gu et. al., Fast Federated Learning in the Presence of Arbitrary Device Unavailability, 
[R2] Yan et al, Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability. 
[R3] Glasgow et al, Asynchronous Distributed Optimization with Stochastic Delays.



Limitations:
-

Rating:
4

Confidence:
4

";0
Ya8WzNF8oQ;"REVIEW 
Summary:
The paper presents a convergence (rate) analysis for a variant of policy-gradient learning of tabular-softmax policy models under finite-horizon MDP setting with discounting factor $\gamma=1$. In this variant, the policy parameters are updated in an epoch-by-epoch manner, from the last decision epoch at the horizon back to the first decision epoch. For each epoch, the learning is a vanilla (stochastic) gradient ascent process, with the objective function being the value obtained assuming fixed return in future epochs. It seems to me that most results in the paper are obtained following similar ideas with previous theoretical studies of policy gradient under discounted MDP settings (e.g. Agarwal et al., 2021, Mei et al., 2020). 

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
I appreciate the general motivation of this work which aims at analyzing RL algorithms under undiscounted setting. In my opinion (and as the authors also pointed out in the paper), many performance bounds of RL algorithms derived under the discounted MDP setting crucially depend on the factor  $\frac{1}{1-\gamma}$, thus fail to accurately characterize the performance of RL algorithm under undiscounted setting. On the other hand, many real-world problems in practice are indeed undiscounted MDP problems. Therefore, directly analyzing RL algorithms under undiscounted MDP settings, or deriving results that do not degenerate when $\gamma$ approaches 1, is relevant and important research direction.

Weaknesses:
However, several major concerns make me hard to appreciate this particular work. Specifically,

1. The so-called policy gradient algorithm as analyzed in this paper is not really the standard policy gradient algorithm as generally perceived and applied -- the latter updates the parameters of states for all decision epochs simultaneously in each gradient step while the algorithm variant targeted in this paper will update parameters only for one epochs each time. 
I think the standard co-updating strategy is much more practically relevant. For example, in RL training of language models, the problem is indeed an undiscounted MDP problem, and standard policy gradient methods (with PPO upgradation) are indeed widely used; see the InstructGPT paper (https://arxiv.org/abs/2203.02155) as an example. I thus have concern on the relevance and significance of the algorithm (not the problem setting) studied in this paper.

2. The epoch-wise updating strategy as assumed in this paper makes most of the analysis a straightforward adaptation of the standard analysis method used in the literature. Moreover, it seems to me that the epoch-wise learning problem analyzed in this paper is just a contextual bandit problem -- the paper assumes the policy $\tilde{\pi}$ for all future epochs is fixed, as well as assuming that the distribution $\mu_h$ for states encountered at the epoch is also fixed, but in this case we are essentially just maximizing an ""augmented immediate reward (with future returns counted, which is fixed)"" over actions conditioned on a context distribution. Since contextual bandit problem is a special case of discounted MDP (with arbitrary $\gamma$), I wonder if most results presented in this paper trivially hold given known results about discounted MDP.  

3. I also have concerns on the authors' interpretation of their mathematical results. For example, the paper claims (as a main contribution) that Theorem 3.8 derives a linear performance bound to the horizon H. However, the bound contains the factor c_h, and it's not clear if c_h itself can be a function of H (or of H-h). c_h can be considered a ""constant"" only with respect to n, not to H. Moreover, Theorem 3.8 only gives a per-epoch performance while previous results gives the overall performance. My understanding is that when we consider the convergence of epoch 0 (which is what we eventually care), we would need to count the time spent on epoch 1 ~ H-1 too, which would bring in another factor of H to the bound?

4. Similarly, throughout the paper it's assumed that the state distribution $\mu_h$ is given for all epoch h, which is not a reasonable assumption in my opinion because except for $\mu_0$, the state distributions for all other epochs actually depend on the algorithm. This is in sharp contrast to the previous papers which only assume that the *initial* state distribution $\mu_0$ is given (which is indeed given). Again, this $\mu_h$ is considered a ""constant"" when interpreting the bound derived in Theorem 5.2, although $\mu_h$ (for h>0) may heavily depend on structures both in the problem and in the algorithm, in a highly non-trivial manner.

5. The majority of the paper assumes that the reward is nonnegative, which is a bit of a limitation, given that the sign of the rewards can greatly affects the performance of policy gradient.

Limitations:
The paper well discussed the limitations of this work in Section 7.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proves asymptotic convergence and convergence rate of (stochastic) policy gradient descent to global optimum for un-discounted finite time Markov decision process (MDP). For the deterministic version, at each decision time, they show the error bound depends linearly on the remaining time step. For the stochastic version, they derived probability bounds. They extend their analysis to REINFORCE on discounted MDP.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The presented results are sound and complete. Both convergence and convergence rate of policy gradient methods with softmax policy are established for the finite-time MDPs.


Weaknesses:
The main argument seems to be based on Mei et al (2020) with some modifications for un-discounted finite time MDPs case. I did not go over the details of proofs in appendix. It is hard  to evaluate the significance and the novelty of the contributions.

Limitations:
See weakness

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper studies the convergence properties of stochastic policy gradient methods for finite-state MDPs for finite horizon problems with un-discounted optimality criteria. The convergence relies on the development of a weak PL condition. In the second part of the paper, the authors then extend their convergence analysis to the setting where the policy gradient is not available exactly and only a stochastic version of it can be used. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and easy to follow. In particular, I find Section 4, where the convergence analysis is generalized to the stochastic setting strong and practical useful. Proofs are derived very clearly and detailed with I appreciate.

Weaknesses:
1) Numerical example missing: In my opinion, it would be interesting to include a numerical example to the paper. In particular it would be interesting to see how tight the complexity bounds of Theorem 4.4 are on a concrete example. 

2) Finite state/action spaces often are restrictive: What can you say about the results being generalised to continuous state and action spaces? Maybe for a linear system to start with.

3) How relevant are un-discounted problems? Could you add a detailed motivation, why these problems are practically relevant?


Limitations:
Without Section 4, the work would have been limited as the gradients typically are not known exactly. But thanks to the results in Section 4, I think the paper is rather complete and I do not see major limitations.


Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper analyzes the convergence of non-stationary softmax policy gradient methods where the policy is parameterized by different parameters at each decision epoch. The convergence results on REINFORCE algorithm are provided under undiscounted finite-time and infinote-horizon cases.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper obtains new global convergence results for softmax policy under undiscounted finite-time and infinite-horizon MDPs. The assumptions used are standard in policy gradient literature.

- The analyses in the paper also cover policy parameterized by deep neural network in deep reinforcement learning.

- The paper is well-written and easy to follow.

Weaknesses:
- My major concern is about the factor $c_h$ in Theorem 3.8. The authors use it as a constant but I argue it is not. Since $c_h$ is the infimum over all iteration of the minimum output of softmax policy, it is not known or can be computed before-hand. In fact, it depends on the choice of number of iteration $n$ and the current parameter $\theta_n$. More discussion is needed in this case.

- The results in Theorem 3.8 and Theorem 4.4 are valid for the exact policy gradient which is rather restrictive and impractical. I believe having results in the stochastic case using stochastic policy gradient will strengthen the contribution of the paper.

Limitations:
I do not find clear discussion on the limitation of the work but I find the use of exact gradient is the main limitation for the key results in the paper.

Rating:
6

Confidence:
4

";0
Cs74qIBfiq;"REVIEW 
Summary:
This paper introduces RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot manner. The key idea is to leverage insights obtained from language models based on task descriptions. These insights are used to modify the embeddings, removing harmful components and enhancing useful ones, without the need for supervision. Technically, the method ensures invariance to spurious features by projecting pretrained model embeddings onto the subspace orthogonal to the subspace spanned by spurious feature descriptions. Experiments demonstrate that RoboShot improves multi-modal and language model zero-shot performance. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. **Novel and useful setting:** The setting of improving the robustness of pretrained model embeddings with task description is novel. RoboShot offers a unique approach that preserves the out-of-the-box usability of pretrained models, which is a key advantage.

2. **Extensive experiments and analyses:** The authors demonstrated the efficacy of the proposed method and setting with extensive experiments and analyses, in terms of both datasets and settings. 

3. The paper is well-written. 

Weaknesses:
1. **Limitation of method:** The robustification relies on the insights provided by language models. However, if the language model does not identify the potential failure cases of the model, the method cannot remedy it. 

2. **Gender bias:** Some experiments are targeted at gender bias. It's better to discuss the scope of evaluation here, e.g., what genders are considered and what biases remain unresolved. 



Limitations:
Limitation needs to be elaborated on. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents an innovative approach to enhance zero-shot classification inference without the need for fine-tuning pre-trained models. The authors introduce a method that partitions input embeddings into three components: harmful, helpful, and benign. By leveraging task descriptions and querying language models with harmful and helpful prompts, they successfully extract harmful and helpful components. This leads to the removal of harmful components and a boost in the helpful ones, ultimately improving robustness in zero-shot classification. The proposed method demonstrates promise and contributes to the field of zero-shot classification without extensive model finetuning.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-structured and easily understandable, with a strong and compelling motivation. As model sizes continue to increase, fine-tuning LLMs becomes increasingly expensive. This paper presents a compelling alternative by enhancing zero-shot performance while retaining the power of LLMs without the need for fine-tuning.  The innovative method of partitioning embeddings into three concepts and leveraging task descriptions and LLMs to strengthen or weaken them is intriguing and holds promise for embedding-based zero-shot text classification.

Weaknesses:
1. The proposed method is primarily applicable to embedding-based zero-shot classification approaches, while prompt-based methods like ChatGPT3.5/4 have gained popularity recently. Prompt-based methods allow humans to directly query language models based on downstream task knowledge, resulting in impressive performance. Although the proposed method is interesting for embedding-based zero-shot classification, its impact may be limited due to the current research trend.

2. Considering the above point, it would be beneficial if the authors compare their method to a more reasonable baseline, such as asking ChatGPT about predictions. This approach could be employed, for instance, by asking ChatGPT to identify if a given text contains gender bias. While ChatGPT's performance might not extend to image-based tasks, applying this baseline to text datasets would provide valuable insights. If ChatGPT achieves accurate predictions, it questions the necessity of an embedding-based zero-shot text classification method.

3. Although the method exhibits significant improvement in worst group performance (WG), it would strengthen the findings if the overall average performance also demonstrated improvement. It is worth noting that in some datasets, the average performance did not improve. This implies that the proposed method sacrifices performance for some classes to achieve better results in the WG. Ensuring a balance between overall average performance and WG improvement would bolster the method's effectiveness.

Overall, I am inclined to accept the paper if all weaknesses are properly addressed but I am also not strongly against rejecting the paper.

Limitations:
Please refer to weaknesses.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach called ROBOSHOT that aims to enhance the robustness of pre-trained models for zero-shot classification tasks. The key idea is to leverage task-specific queries to prompt the large language model (LLM) to generate textual insights about the task, which can be classified as helpful or harmful concepts. These textual insights are then used to obtain ""insight representations"" based on the corresponding encoded embeddings, which are utilized to calibrate the vector of input representation to predict the class. The proposed method is evaluated through experiments on zero-shot image and text classification tasks. The paper also includes a theoretical analysis of the bound of the coefficient of targeted harmful concept post-removal. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation of this paper is clear. The problem of ""robustifying zero-shot models without labels, training, or manual specification"" is challenging and interesting. 
2. The paper proposes a simple method to calibrate the input embedding to make predictions.
3. The paper is generally easy to read and easy to follow. The settings of experiments in this paper are clear.
4. The theoretical proof of the coefficient bound of the targeted harmful concept is a plus.




Weaknesses:
1. The paper aims to robustify zero-shot models without labels, training, or manual specification. However, the proposed method still requires the use of manually-designed helpful/harmful queries (shown in Table 6&7) to query the LLM. 
2. The paper did not explore the impact of using different prompts to query textual insights. Given that the experiments were conducted on small pre-trained models such as BERT, which are less robust than LLMs, it is unclear whether the resulting different text insights would significantly affect the model's performance.
3. The paper employs LLMs (e.g., chatgpt/LLAMA) to generate textual insights for calibrating the input embedding of a small pre-trained model for prediction. It raises the question of why not directly prompt the LLM for zero-shot text classification if it already possesses sufficient knowledge about each class. The paper would benefit from including such a baseline comparison.
4. The paper did not include and compare some relevant works, such as [1] [2], which calibrate the logits of predictions for zero-shot/few-shot text classification. 
5. Table 1 indicates that the proposed method does not improve the ALIGN model on the Waterbirds dataset, but it does improve ALIGN's performance on CelebA, VLCS, and CXR14. The paper attributes this discrepancy to the harmful and helpful insight embeddings of Waterbirds being indistinguishable in the text embedding space of ALIGN. However, it is unclear how to determine when the ROBOSHOT method is applicable to different combinations of models and datasets. The paper could benefit from providing some metrics to predict the conditions under which ROBOSHOT is effective.

[1]Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right

[2]Calibrate Before Use: Improving Few-Shot Performance of Language Models

Limitations:
please refer to weakness point 5

Rating:
4

Confidence:
4

REVIEW 
Summary:
The main objective of this research is to enhance the robustness of image/text classification by taking into account the relationships between labels. The authors utilize Large Language Model (LLM) to incorporate prior knowledge about these labels. They also propose methods to amplify useful features and eliminate harmful features induced from these labels.

In contrast, prior work require manual identification of biased features for debiasing, whereas this work leverages LLM to automatically suggest such features, thereby reducing the need for manual effort.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and can be easily understood.
- Using LLM to propose useful and spurious features is somewhat novel.

Weaknesses:
- This work aims to incorporate the knowledge of LLM into image classification. However, the method used, which is based on CLIP, does not seem to fully utilize the knowledge from LLM. The author's approach involves generating insight descriptions to establish a connection between LLM and CLIP's text encoder.  A more intuitive alternative would be to link CLIP's image encoder directly with LLM through an adapter, similar to the Flamingo (https://arxiv.org/abs/2204.14198) and LLaVA (https://arxiv.org/abs/2304.08485) methods. By doing so, the knowledge from LLM can be more comprehensively utilized, rather than solely the generated insight descriptions.
- The subspace-based debiasing methods have already been used under various contexts as also mentioned in the paper, which cannot be viewed as novel to some extent.

Limitations:
The limitations part is missing in the paper.

Rating:
4

Confidence:
3

";0
g8S53BmXE6;"REVIEW 
Summary:
The paper proposes a strategy of predicting heterodimers with ESMFold. 

Chains of heterodimers are linked by glycine linkers and inputed to esm. The output representations are then added with a learnable embedding layer, and then folded with the folding module in ESMFold. The finetuning is only done for the learnable embedding, with a weighted distogram loss.

Evaluations are done on 3 datasets, while one of them should be considered pointless. Some elevation is gained by the method, compared with directly using links.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
1. The writing of the paper is clear.

2. The proposed method is reasonable and intuitive.

3. The method achieves comparable performances with other linker-based hacking strategies. Slight elevation of performances is gained (TMscore 0.62->0.65, ~0.03) from finetuning compared with directly using linkers.

Weaknesses:
Method:

3. The proposed idea, i.e. exploiting ESM models to predict protein complexes by architecture adjusting and finetuning has already been explored previously [1]. Seems that their implementations are more ""neat"": no external linkers are involved; permutation invariances are regarded; they solve complexes with arbitrary numbers of chains, both homomers and heteromers; and the performance elevation is more significant (TMscore 0.27->0.66, ~0.39 in their benchmark). However, no discussion let alone comparison is shown in this paper.

4. In fact, I don't know why linkers are needed to fold multimers at all: simply modifying the relative position indices suffices to tell the model that the residues are from separate chains. Involving linkers will implicitly pose in the model a geometrical constraint on the C-terminal of chain A and N-terminal of chain B (as they'll have relative position of +-L). Also the running time can be (slightly) added. Also, I don't personally like the saying that connects the linker idea to ""prompts"": they are totally different things. Doing so is more of attempting to ride the wave of LLM.

Evaluation:

5. The elevation of performances is in all sense too marginal. The results simply tell me that both ESMFold-linker and the proposed method are not reliable (avg DockQ of 0.11/0.17), instead of telling me that the proposed method is useful. In this case, maybe the percentage of success is a better metric to show.

6. The VH-VL docking benchmark is totally pointless and lacks commonsense. One who has basic senses of the domain knows that all protein folding efforts on antibodies should focus on Ab-Ag instead of VH-VL, because all interaction modes between VH-VL are the same, i.e. they fold almost identically (In table 2, as one can expect, all TM-scores are above 0.92). Therefore, they shouldn't be used as heterodimer folding or protein docking benchmarks. Even if one focuses on the structures of VH-VL, the metrics on the CDR loops should be independently reported, rather than global RMSD. 

[1] Zhu et al, Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models. https://www.biorxiv.org/content/10.1101/2023.02.14.528571v1.full.pdf

Limitations:
limitations are addressed in section 6.

The suggestion would be 

11. More solid benchmarks.

12. Discussions and comparisons with [1].



Rating:
3

Confidence:
5

REVIEW 
Summary:
In this paper, the author applies prompt tuning in the heterodimeric protein prediction task. Instead of using the poly-Glycine linker, this method automatically finds the best linker in the continuous space. The author compares this method with several existing methods including the current start-of-art algorithm AF-multimer, the best PLM-based algorithm ESMFold-Linker, and the rigid docking algorithm HDOCK. The performance shows this method which is PLM-based is better than ESMFold-Linker but worse than AF-multimer.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The novelty and contribution of the paper mentioned in the paper are clear and correct.
2. The motivation behind the method and its design are well-founded and logical.
3. Overall, the writing is great.

Weaknesses:
1. Based on my understanding, the methodology seems to be limited in its applicability to the pre-trained language model-based protein structure prediction method, which is not considered the most accurate algorithm for protein structure prediction. Furthermore, the performance of this methodology appears to be influenced by the linker's position and the location suggested in the paper is not the PLM part, raising concerns about its compatibility with other PLM-based methods like Omegafold. Consequently, the paper's value may diminish if there are more robust folding algorithms available.
2. Given that all other methods are unsupervised, there is a possibility of the method benefiting from overfitting. Regarding the antibody dataset, from my understanding, antibodies generally exhibit a rigid overall structure except for the six CDR loops. Consequently, I suspect that the flexible CDR loop will not lead to significant variations in the docking of the light chain and heavy chain. Unless supported by evidence, I prefer to believe that the results obtained from the antibody dataset are not reliable and it may not be an appropriate dataset for this task. As for the Heterodimer test, while a 40% threshold seems acceptable, I believe setting a lower threshold, preferably below 30%, would be better.
3. In comparison to the state-of-the-art method AF-multimer, this method exhibits a significant decrease in performance. When people have the opportunity to utilize a substantial number of CPUs for preparing MSA information, the speed advantage offered by this method does not compensate for its accuracy limitations. Moreover, as far as I understand, there aren't a lot of downstream tasks that necessitate high-throughput calculations.

Limitations:
The limitation is well addressed and there is no potential negative societal impact of their work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Inspired by the prompt tuning technique used in the field of NLP, the authors leverage the prompt tuning to adapt the single-chain pre-trained ESMFold for heterodimer protein structure prediction. To be specific, a learnable soft prompt is placed between protein chains. With such links, the pre-trained ESMFold treats complex structure prediction as the monomer structure prediction, and the prompt is tuned on the heterodimer dataset.

They show model with such a trick can outperform ESMFold-Linker baseline by large margins on both contact and structure prediction tasks on the heterodimer test set.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 Leveraging the prompt tuning idea in multimer structure prediction using a single-chain pre-trained model is quite novel and interesting.

2. The proposed trick does improve the performance of the ESMFOLD-link baseline. The trick can potentially be applied to different single-chain pre-trained protein structure prediction models and can be considered a general trick.



Weaknesses:
1. The link-tuning idea is only validated on ESMFold. I'm curious about its effectiveness when applied to other protein structure prediction (PSP) models, e.g., Alphafold, and Omegafold. Prompt tuning is a general trick in NLP. Justifying the generalizability of the link-tuning trick on different PSP models can definitely make the manuscript stronger.

2. The idea of Linker-tuning is simple and effective, but I feel the current manuscript is not informative enough to be published on NeurIPS conference. If the authors can justify the generalizability of the link-tuning trick, I'm willing to adjust my score.

Limitations:
NA

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work predicts the structure of heterodimeric protein chains by optimizing poly-G linkers that connect two chains of a heterodimer. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This research, compared to other existing deep learning methods, finds an alternative to protein complex prediction methods, which is to make use of poly-G linkers. The idea connects closely to many biological applications and thus has more potential impacts on wider communities. 
- The evaluation metrics cover various aspects to comprehensively assess the model's performance. 

Weaknesses:
- ProteinMPNN as another famous multimeric structure prediction tool should have been compared.
- It seems very often the proposed method does not achieve the optimal performance (in Table 1 and Table 2).


Limitations:
No potential negative societal impacts were discussed in the main text. 

Rating:
4

Confidence:
4

";0
9BV9dMhRjt;"REVIEW 
Summary:
This paper considers the problem of estimating _persistence intensity (resp. density) functions_, which are topological summaries arising when considering multiples realizations $\mu_1,\dots,\mu_n$ of persistence diagrams---which are counting measures supported on an open-half-plane $\Omega$. Namely, (Chazal and Divol, 2019) proved that $\frac{1}{n} \sum_{i=1}^n \mu_i$ converges toward a limit object $E[\mu]$ called an _expected persistence diagram_ which, under mild assumptions, admits a density $p$ wrt the Lebesgue measure on $\Omega$, called a persistence _intensity_ function. Note that $p$ may not integrate to $1$ (the $\mu_i$s are *not* probability measures, only Radon measures). 

It is natural to wonder how fast one can estimate $E[\mu]$ given an i.i.d. sample $\mu_1,\dots,\mu_n$. Given that $E[\mu]$ has a smooth density $p$, while the $(\mu_i)$ are discrete, it is tempting to adopt a kernel based approach (i.e. considering the convolution of the $\mu_i$ by a kernel $K_h$, with $h>0$ being the bandwidth), yielding an estimator $\hat{p} = \hat{p}_{h,n}$ of $p$. This is the purpose of the current paper which shows that, in brief, assuming $p$ is $s$-Hölder,

$$\| E[\hat{p}] - p \|_\infty \leq O(h^s), \qquad \text{(bias)},$$

and (with high probability)

$$ \| \hat{p} - E[\hat{p}] \|_{\infty , h} \leq O(n^{-\frac{1}{2}}h^{-1}), \qquad \text{(variance)},$$

where $\| \cdot \|_{\infty,h}$ denote the sup-norm but only accounting for points in $\Omega$ that are at distance $> 2h$ from the diagonal $\partial \Omega = \{ (t,t), t \in \mathbb{R} \}$. 

As an alternative, they also study the persistence _density_ function, which is substantially similar to the above, but considering the quantity $\frac{1}{n} \sum_{i=1}^n \frac{\mu_i}{\mu_i(\Omega)}$ as a starting point, hence the limit object is a _probability_ distribution. This first normalisation step allows the authors to obtain similar results as those mentioned above, but stronger in the sense that they do not need to ignore points close to $\partial \Omega$. 

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
## Clarity

The paper is fairly well written, though it may be hard to understand for the reader that is not familiar with statistical topological data analysis problems. Theorems are precisely stated, and, with few exceptions, mathematical quantities are well-defined.  

## Originality and Significance

The authors provides quantitative results for the convergence of kernel-based density estimator for expected PD, which is new to the best of my knowledge. The introduction of the persistence _density_ may also be worth of interest, if further motivated (see below). 

## Quality

This is a competent paper in terms of the results provided (which seem correct as far as I can tell). However, the motivation behind the type of results themselves is arguably questionable. 

Weaknesses:
There are several points which fail to convince me and prevent me from supporting this paper yet. 

## 1. The use of the sup-norm. 

The paper provides results in the sup-norm on $\Omega$. It is important to stress that this norm does not account for the peculiar role played by the diagonal in the geometry of persistence diagrams, in contrast with the standard $\mathrm{OT}_p$ metric. The authors justify this by an inequality of the form $\mathrm{OT}_p^p(\cdot, \cdot) \leq C | \cdot - \cdot|$, 
meaning that being small in sup-norm is (strictly, as proved by the authors) more demanding that being close in $\mathrm{OT}_p$ metric. [Edit: fix the rendering by removing the infty symbol; the norm in the rhs is the sup-norm.]

Sure, it means that the rate obtained for the sup-norm implies the same rate for the $\mathrm{OT}_p$ metric (up to the role played by the exponent), but it also means that the task is _harder_, and that this norm does not induce the same topology as the natural metrics on persistence diagrams. Recall (as suggested in $\ell$167-171) that accounting for the diagonal is not simply a trick to compare measures with different total masses, but also has an algebraic meaning (from which we get the mentioned stability results). The sup-norm induces a topology that fails to capture the fact that one can compare diagrams ""downweighting points close to the diagonal"". 

To me, this is what prevents, for the persistence _intensity_ function, to obtained ""global"" sup bound, and get only bounds valid $2h$-away from the diagonal. The sup-norm cannot handle the noise properly. 

In addition, because performing estimation in sup-norm is _harder_ than estimation in $\mathrm{OT}_p$-metric, one only get a convergence rate of $\frac{s}{2(s+1)}$, which is natural for the sup-norm, but quite _slow_ for the $\mathrm{OT}_p$ metric. Indeed, (DIvol and Lacombe, 2021) prove that the empirical expected persistence diagram (i.e. without any convolution by a kernel involved) converges toward the persistence _intensity_ function at (the faster) parametric rate $O(n^{-\frac{1}{2}})$. Of course, the latter result considers the weaker (but more natural) metric $\mathrm{OT}_p$, but as long as there is no very strong motivation to compare persistence intensity functions using a sup-norm, it is reasonable to wonder why should we struggle to obtain slower convergence rates. 

## 2. Motivations behind the persistence _density_ function

As (interestingly) observed by the authors, statistical estimation improves when considering the normalized persistence _density_ function. However, here as well, I fail to be fully convinced by the proposed motivation, namely ""the normalized persistence measure may be desirable when the number of points (...) is not of direct interest but their spatial distribution is"" ($\ell$80-81). 

I do not agree with this claim because this normalization typically discard points away from the diagonal, asymptotically. For, consider a $N$-sample on a sphere + some tubular noise, and the Vietoris-Rips filtration on it. Then (with high probability), the corresponding (random) persistence diagram in $H_2$ (degree-$2$ homology) will have one point away from the diagonal, and a bunch of points close to the diagonal accounting for the noise---so does the corresponding persistence _intensity_ function. As $N$ increases, the points accounting for the noise get closer and closer to the diagonal, but also more abundant. As such, if one normalize the persistence measure by its mass/number of points, the bump/point accounting for the ""robust"" topological information will asymptotically be erased. In particular, this normalized persistence measure is not continuous (for, say, the vague topology) with respect to the Hausdorff distance, a central property satisfied by the Vietoris-Rips filtration. (Note : this is what we can observe in Figure 3 vs Figure 2). 

Therefore, (i) it is not surprising that one obtains stronger (this time) results with this weaker representation and (ii) it is not clear to me why would one actually consider this representation at is losses its topological interpretation, as far as I can tell. 

## 3. About the experiments

The numerical illustrations have all been deferred to the supplementary material. 

To me, the (main body of the) paper should be self-contained, in sense that one should not _have to_ look at the supplementary material to understand it at high level. Proofs, complementary results, _complementary_ experimental report can be deferred to the supplementary material, but having a **Numerical illustration** section without any numerical illustration, mostly saying ""look at the supplementary material"", is not serving the paper. Right now, the paper can be considered as experiment-less, and while numerical illustrations are not mandatory, this clearly does not support the paper. 

Note that I looked at the experiments nonetheless. While they are fairly interesting, they do not bring more motivation to support the paper (with, e.g., a ML experiment where using the persistence _density_ function is much better than using the more standard persistence intensity function).  

## 4. Complementary minor comments
- I think that there is a typo in the definition of $\Omega_\ell$, which is (I think) inconsistent with the description made below ($\ell$67) and its use in section 3. 
- More references could have been cited through the paper, e.g., when listing different linear representations ($\ell$129-131), it may be nice to credit their respective authors.  Similarly, a more precise comparison with related work (mostly Divol's line of work with Polonik, Chazal and then Lacombe), would be helpful to understand what is the paper contribution and how it differs from these works. 


Limitations:
I do not see any negative societal impact _specific_ to this work. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
The paper proves several theoretical inequalities involving the optimal transport distance, intensity, and density functions on a plane triangle with a non-standard boundary motivated by persistent homology.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper rigorously proves in appendix C six theorems and three corollaries from section 3. All definitions, statements, and proofs are written in great detail. Also, the paper is well-written overall.

Weaknesses:
Starting already from section 2 about the background, it seems that the term ""persistence"" is not really needed because there is no connection with real data. 

Borel sets, measures, densities, and other concepts of classical probability theory can be considered on a plane triangle without the ""persistent"" adjective. 

Hence it is strange to read lines 97-99 saying that ""measure and probability are not yet standard concepts in the practice and theory of TDA. As a result, they have not been thoroughly investigated"" because measure and probability are standard concepts in probability theory for nearly 100 years. 

The conclusions reveal the main theoretical weaknesses in lines 328 and 332: ""Our main focus is on the estimation of the persistence intensity function [CD19, CWRW15]."" More explicitly, line 106-109 accepts that ""[CD19] provided explicit expressions for p and p˜  ... We will refer to the functions p and p˜ as the persistence intensity and the persistence density functions, respectively. We remark that the notion of a persistence intensity function was originally put forward by [CWRW15]."" 

Limitations:
Though the paper doesn't include the required keyword ""limitation"", the limitations appear in Assumptions 3.2, 3.3, 3.4. For instance, Assumption 3.4 essentially requires that there is not too much ""little noise"". 

In a simple case of the sublevel persistence of a scalar function, this function can be perturbed only by introducing a ""bounded amount"" of pairs of adjacent local maxima and minima. More exactly, the persistence diagram allows only a bounded sum of ""noisy artefacts"" near the diagonal.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work develops a set of methods and theories for statistical inference for TDA based on samples of persistence diagrams:  
a. The work focuses on the estimation of the persistence intensity function. The work also proposes the novel persistence density function, which is the normalized version of the persistence intensity function.
b. The work present a class of kernel-based estimators based on an iid sample of persistence diagrams and derive estimation rates in the supremum norm, which is stronger than the optimal transport distance norm.
c. The work obtains uniform consistency rates of estimating linear representations of persistence diagrams, including Betti numbers, persistent surfaces, persistent silhouttes and weighted Gaussian kernels.
d. The work presents several theorems, theorem 3.1 compares the L^\infty norm and the optimal transport distance in terms of controlling the estimation error; theorem 3.5, 3.6 show the kernel estimation error bound for the persistent density function and the persistent intensity function; theorem 3.8, 3.9 show the estimation error bounds for the linear representations. 

These theoretic results are fundamental and important, they lead to novel direction for statistical inference for TDA based on random persistent diagrams.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work is very solid, and gives rigorous mathematical proofs. The formulations of key concepts, main theorems are clear and rigorous, the mathematical deductions for lemmas, theorems, corollaries are thorough and in detail. The theoretic results are convincing and impressive.

Weaknesses:
The work is highly theoretical, the heavy mathematical deductions are abstract. It will be more helpful for readers to digest if the authors further explain the motivations, the main proof approaches, the interpretations of the theorem, the potential direct applications of the results. More specifically,

1. It will be helpful for readers to better understand the article to give a table of symbols, list the major symbols and their meanings;
2. It will be helpful to give some figure to illustrate the concepts, such as persistent diagram;
3. Some math symbols and operators can be further explained, such as:
a. The two symbols in line 145 are hard to differentiate, especially on a laptop screen, maybe the authors can emphasize the shuttle differences, or use different symbols;
b. The formula in line 165, \|x-y\|_2^q needs more explanation
c. The formula in line 247 in the supplementary, the operator Proj_{\partial\Omega} needs more explanation






Limitations:
This work is theoretical, it mainly focuses on theoretical deductions. The limitations are not adequately addressed. It will be helpful if the limitations in practical applications are further discussed. 
1. In reality, how difficult to satisfy all the assumptions listed in the paper ?
2. If the point cloud include several homology generators with similar birth and death times, may the current approach mix them and cause confusion ?
3. In theorem 3.6, from samples close to the diagonal, can we get more precise estimation ?


Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper tackles the problem of estimating the persistence intensity function, describing the distribution of rando persistence diagrams, and proposes a variant called persistence probability function that integrates to one.  The paper starts with a theoretical analysis of the estimation error bound of the intensity function using the OT measure and the L-infinite norm, showing that the latter allows the definition of stricter bounds. The paper also proposes a method to estimate the persistence intensity function and the persistence probability function under the assumption of i.i.d. samples using a kernel density estimation approach. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Persistent diagrams are an important tool for characterizing topological structures (e.g. surfaces and graphs). Being able to estimate with high accuracy the distribution of such structures could indeed be beneficial to their analysis, with applications also to the learning domain.

The paper, at least from a not-so-expert reader like I am, seems very rigorous in the theoretical analysis and provides in the sup. mat. all the proofs of the introduced theorems.



Weaknesses:
The main weakness (if we want to call it so) of the paper is that it is not easy to read by nonexperts of the specific topic. It is quite dense and mostly mathematical and does not provide many intuitive explanations of why some properties could be important from a practical perspective.
In general, I would have appreciated a more gentle introduction to the problem and a wider introduction/literature review on the practical application/advantages of persistent intensity functions.


Limitations:
I don’t foresee any particular negative societal impact. A discussion about the practical applicability of the method would be interesting.

Rating:
5

Confidence:
2

";0
FM81CI68Iz;"REVIEW 
Summary:

This study primarily concentrates on refining hyperparameters in federated learning, especially concerning non-IID data. The researchers highlight two key hyperparameter groups—Batch Normalization (BN) and selective update hyperparameters—that yield impressive outcomes.

BN hyperparameters help mitigate feature shifts, a prevalent problem in federated learning. FedBN, a cited method, aids in managing feature shift more effectively by localizing the BN layers while keeping other layers global. Details of the BN layer formation and its constituents are also given.

Moreover, the study explores a global model's application, initially trained through existing federated algorithms, which is then fine-tuned to the client set. The CIFAR-10 dataset split among various clients is utilized for this process, demonstrating that their approach maintains global performance while enhancing personalized performance.

The researchers also incorporate the Jacobian product to approximate the Hessian inverse and calculate the hyper gradient, forming a part of their suggested algorithm.

For experimental purposes, ResNet-18 and SGD are consistently employed for all tests. Batch size, local epoch number, learning rate, hyper gradient clipping, maximum communication rounds, and other parameters are also predefined.

This paper contributes to federated learning by introducing a method to optimize hyperparameters, especially in non-IID data scenarios, and backs its effectiveness through empirical results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
**Originality:** The paper stands out due to its unique approach to handling non-IID data within federated learning. Implementing Batch Normalization (BN) hyperparameters to tackle feature shift is an inventive fusion of existing deep learning knowledge and federated learning's distinct hurdles. Moreover, the employment of the Jacobian product to estimate the Hessian inverse and compute the hyper gradient is a distinct application within the federated learning sphere, marking an inventive merge of established concepts to address a recurring federated learning issue.

**Quality:** The paper's quality appears robust. It reflects the authors' thoughtful research methodology and the theoretical integrity of their approach to hyperparameter optimization. Leveraging the CIFAR-10 dataset for validation is a norm in this field, and the authors thoroughly outline their experimental setup. Nevertheless, a broader evaluation across diverse datasets and juxtaposition with other methodologies could further enrich the paper's quality.

**Clarity:** The paper is well-crafted and clearly depicts the research. The authors present exhaustive descriptions of their methodology and the theoretical underpinnings of their strategy. They also lucidly detail their experimental setup and findings. Such explicitness in communication renders the paper comprehensible to a broad audience, from novices in federated learning to seasoned researchers.

**Significance:** The paper holds substantial significance. It addresses pivotal challenges, such as dealing with non-IID data and optimizing hyperparameters, vital in federated learning. The authors' solutions to these challenges could potentially lead to the more efficient and practical training of federated learning models, making a notable contribution to the field. Additionally, this paper's insights could stimulate new research pathways and methodologies within the broader realm of machine learning.

Weaknesses:
In summary, while the paper is fundamentally and conceptually solid, there are a few weaknesses that weaken its contribution and soundness. 

1. **Dataset and Model Constraints:** The study principally uses the CIFAR-10, Office-Caltech-10, and DomainNet datasets in its experiments. Although these are recognized standards, the findings may not extend to different datasets and models. Testing the proposed method across diverse datasets (such as images, text, and tabular data) and various model architectures could better showcase the method's adaptability and resilience.
2. **Comparison to Other Techniques:** A broader comparison with cutting-edge methods could enhance the paper. While some algorithms are referenced, the relative performance of the authors' method remains unclear. A side-by-side comparison could clarify the comparative advantages and disadvantages of their approach.
3. **Computational Overhead:** The authors suggest the Jacobian product for approximating the Hessian inverse and computing the hypergradient, a fresh perspective. Appendix C mentions that the computational cost compared to FO / HF-MAML is still relatively large. 
4. **Applicability:** The paper's significant contribution to federated learning cannot be overlooked, yet the universal applicability of their method warrants examination. Federated learning is applied across various sectors, each with unique data distributions, privacy stipulations, and computational resources. Discussing the limitations and potential solutions of their method would fortify the paper.
5. **Reproduction**: The codes are not available yet. 

Limitations:
In terms of broader societal impacts, it's not clear from the provided excerpts whether the authors have considered this aspect.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a new personalization strategy for the Federated Learning setting. Their key idea is to train a meta-network that takes as input the client's data and outputs optimal hyperparameters for that data. These optimal hyperparameters can then be used to personalize the pretrained model. The hyperparameters they choose to output control the batch-norm mean, std-dev and per layer learning rate. They evaluate their technique on 4 datasets, and show that their technique outperforms other personalization strategies on these datasets.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The core idea of using a meta-net proposed by the authors is quite novel and I believe it is a very promising direction.
- The hypergradient technique they propose to train this metanet is also quite novel and interesting.
- For the most part their algorithm and technique is clearly presented.

Weaknesses:
- The experiment setup is not clearly defined and confusing. Examples
   - In their CIFAR10 experiments, they say 20% of the data is used for test, it's not clear if that means 20% of the clients are not used in the FL step or 20% of the data in each client is not used in the FL step.
  - It is not clear what fraction of clients are used for training the FL model in each expt
  - Figures 2 and 3 are poorly labeled and not clearly explained. Figure 3 is completely unreadable.

- Experimental results are not very strong
  - FedL2P seems very computationally expensive (total compute used is not compared anywhere) and the results are only marginally better than other techniques.
  - No evaluation on actual non-IID datasets. There are several non-IID datasets available publicly for FL - EMNIST, Shakespeare, StackOverflow etc. Evaluating their method on these datasets would make the results much more compelling.
 - No evaluation on non-image datasets.
 
- Hyperparameters limited to continuous values - many hyperparameters like # epochs, choice of optimizer etc. are not continuous.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces a novel approach that utilizes meta-learning to learn hypernets that aim to determine the optimal fine-tuning hyperparameters for individual clients. The proposed idea is intriguing and holds promise. The results presented in the paper are compelling. However, the paper would benefit from improvements in terms of writing and clarity. Overall, it is a borderline paper. 

--- post rebuttal --

In the rebuttal, the author address my questions about clarity and positioning their work. So I increase my score from 4 to 5. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper proposes a novel idea of using meta-learning to learn hypernet that can estimate the best fine-tuning hyperparameters for personalizing local models. 
- The results show the proposed FedL2P can effectively improve performance on multiple standard FL methods and remain robust when there is a data distribution shift across clients. 

Weaknesses:
- The **clarity** needs to be improved. Especially, some details of the proposed method are missing. Please see the question section below.
- The proposed method is **limited** to one certain type of personalized federated learning, i.e., fine-tuning local models. There are other strategies of personalization that are not applicable to this work. For example, one strategy is decoupled model (global shared encoder + local predictive head) [1,2]. Another strategy could be using Hypernet to directly optimize different local models [3].

[1] Zhong, A., He, H., Ren, Z., Li, N. and Li, Q., 2022, September. FedDAR: Federated Domain-Aware Representation Learning. In The Eleventh International Conference on Learning Representations.

[2] Collins, L., Hassani, H., Mokhtari, A. and Shakkottai, S., 2021, July. Exploiting shared representations for personalized federated learning. In International conference on machine learning (pp. 2089-2099). PMLR.

[3] Shamsian, A., Navon, A., Fetaya, E. and Chechik, G., 2021, July. Personalized federated learning using hypernetworks. In International Conference on Machine Learning (pp. 9489-9502). PMLR.

Limitations:
Not discussed. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a method for fine-tuning a client's model based on a pretrained model using its local data. The proposed personalization approach involves fine-tuning the learnable weights and biases (assume, there are $M$ such layers) as well as the means and standard deviations (SDs) of batch normalization (BN) layers (assume, there are $B$ BN layers).

This fine-tuning process is facilitated by two meta neural networks. The first meta neural network predicts $2M$ learning rates, which correspond to each weight and bias layer, enabling the local model to perform several gradient steps from the pretrained model. The input for this first meta neural network consists of the mean and standard deviation values for each of the $M$ layers.

On the other hand, the second meta neural network directly predicts the means and SDs. Its input comprises the distance between the pretrained means and the running means of the model with the pretrained weights on local data, as well as a similar distance measure between the SDs.

Both meta neural networks are trained using FedAvg on hyper gradients.

--------
Post-rebuttal update: I did not have significant concerns about this paper. I acknowledge the authors' rebuttal and extend my gratitude to them, particularly for adding the new experiment. I have opted not to modify my favorable rating of 6.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper demonstrates sufficient originality by introducing a novel approach using meta neural networks for computing personalized hyperparameters, as outlined in the summary above. Notably, the concept of trainable meta nets holds promise and appears to have practical applicability.

2. The authors effectively elaborate on the algorithm's intricacies, providing comprehensive descriptions of various components of the main method. Additionally, Figure 1 illustrates a schematic depiction of the main approach, enhancing the clarity of their explanations.

3. Extensive computational experiments are incorporated within the paper to substantiate the efficacy of the proposed method. 

4. The paper is a valuable contribution to the fields of personalized Federated Learning and Meta Learning.

Weaknesses:
1. Despite the authors' efforts in providing extensive explanations for their ideas, regrettably, the text still contains several notable inconsistencies, as detailed in the ""Questions"" section.

2. A comparative analysis with the FOMAML method [1] is notably absent in the work. It is worth mentioning that FOMAML shares significant similarities with the authors' proposed method, as both utilize the MAML approach for fine-tuning the model using the training dataset and making optimization steps for the meta parameter based on the validation dataset, as observed in the current study. Additionally, in another related work [2], which builds upon MAML, the hyper gradient is explicitly addressed (see lines 14 and 16 of Algorithm 1 in [2]).

[1] ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"", Finn et al.

[2] ""Unsupervised Meta-Learning for Few-Shot Image Classification"", Khodadadeh et al.

Limitations:
Authors adequately address the limitations of the work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes FedL2P, a federated learning framework for learning personalized strategies in the federated meta-learning problem. The authors introduce meta-nets to estimate hyper-parameters for personalized fine-tuning, such as batch normalization parameters and learning rates, based on client-specific data statistics. The empirical results show that FedL2P outperforms standard hand-crafted personalization baselines in both label and feature shift situations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed method demonstrates soundness in its approach. Utilizing hyperparameter inference for federated learning shows promise and effectiveness in achieving personalization, as it tries to customize only a few crucial hyper-parameters.
- The experiments are meticulously evaluated across diverse distribution shifts, encompassing feature shift, label shift, and domain shift.

Weaknesses:
- Calculating meta-gradients involves time-consuming computations of second-order derivatives.
- The comparison is limited to variants of finetuning, neglecting the majority of personalized federated learning baselines such as [1,2]. Additionally, recent works [3] addressing distribution shifts by optimizing different blocks of the network should be thoroughly discussed in the paper.
- The sufficiency and effectiveness of communicating the selected hyper-parameters for personalized federated learning lack rigorous verification.

[1] Towards personalized federated learning. TNNLS

[2] Layer-wised model aggregation for personalized federated learning. CVPR 2022

[3] Surgical fine-tuning improves adaptation to distribution shifts. ICLR 2023

Limitations:
I'm not aware of any potential negative societal impact of the work.

Rating:
5

Confidence:
3

";1
vN9OBsZtYH;"REVIEW 
Summary:
The authors introduce an innovative framework that enhances the fairness guarantees of a classifier in the presence of both sensitive attribute noise and label noise, considering them independently as well as in combination. Their approach incorporates theoretical guarantees and involves training a fair encoder to learn a novel data representation that ensures both fairness and accuracy. They demonstrate that by imposing bounded divergence between the noisy and clean distributions, fairness can be effectively transferred from one distribution to another. Notably, their method tackles the problem from a distribution shift perspective, eliminating the need for noise rate estimation typically required by conventional noise tolerant models.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The introduction effectively substantiates all the claims made, including the contributions put forth by the authors. These assertions find validation through a thorough description of the methodology employed and the experiments conducted. The method section elaborates on the techniques and approaches considered, demonstrating how they align with the stated objectives. Furthermore, the experimental results provide empirical evidence that supports the claims made in the introduction. 
- The problem addressed in the paper is well motivated. The authors provide a comprehensive and compelling rationale for the significance and relevance of the problem. They effectively highlight the real-world implications and potential consequences of the existing limitations in the field.  
- The authors introduce an innovative alternative approach that effectively addresses the limitations of state-of-the-art (SOTA) methods. By identifying and highlighting the drawbacks of existing techniques (noise rate requirements), they demonstrate a clear understanding of the challenges at hand. 
- The methodology is clearly explained and well-organized. The paper includes sub-sections that effectively delineate different aspects of the methodology, ensuring a coherent and structured presentation. 
- The paper demonstrates commendable attention to reproducibility by providing thorough and detailed information regarding the experimental setup.
- For the experimental evaluation, the authors take into account various types of data, including both tabular and image data.
- The selection of datasets and the procedure employed to generate synthetic datasets align well with similar approaches found in the existing literature. 
- The evaluation conducted in the paper is both sound and comprehensive. The authors meticulously consider various aspects to ensure a robust evaluation.


Weaknesses:
- (Section 4, Experiments) The authors put forth a proposition to tackle the challenge of ensuring fairness in the presence of noise from a distribution shift perspective. However, in the experimental section, they fail to compare their proposal with methods that specifically address distribution shift in a fairness-aware scenario. It is worth considering that these alternative methods may also yield promising results when handling noisy sensitive and label information. Including such comparisons would provide a more comprehensive understanding of the relative performance and effectiveness of the proposed approach within the context of fairness under distribution shift.
- (Section 4, Experiments) The paper presents theoretical bounds, but unfortunately, they are not evaluated empirically. While the theoretical analysis offers valuable insights and establishes the potential effectiveness of the proposed approach, the absence of empirical evaluations leaves room for uncertainty regarding its practical applicability. Empirical evaluations would have provided concrete evidence of the proposed method's performance and its ability to meet the expected bounds.
- (Section 2, Fairness metrics) The discussion of fairness metrics lacks a clear structure, and I would suggest that the authors differentiate between individual and group notions of fairness, providing distinct explanations for each. Additionally, it would be beneficial for the authors to acknowledge the emergence of mini-max fairness notions, which are gaining popularity in the field. 
- (Section 2, Fairness-enhancing interventions)  While describing the pre-, in-, and post-processing methods, the authors primarily focus on specific techniques instead of providing an overview of the general framework. Consequently, it is not accurate to claim that all preprocessing methods aim to rectify the distribution of input features, nor is it true that all in-processing methods incorporate fairness enhancement as relaxed constraints. In reality, regarding the latter, there are variations where fairness is achieved through techniques such as fairness penalizations. While these cases are commonly encountered, it would be preferable for the authors to first describe the overarching objectives of the general workflows before delving into specific specifications. This approach would provide a clearer understanding of the broader goals before examining the specific techniques used. Moreover, the authors fail to explicitly state that their method constitutes an in-processing intervention.
- (Section 3.2) The authors initially discuss general distribution shift, but in line 167, they assert that they address covariate shift. It is important to note that these two types of shifts have distinct mathematical implications. Covariate shift specifically involves changes in p(x) between the source and target domains, while assuming that the functional form of p(y|x) remains unchanged. It would be beneficial for the authors to clarify which shift they are specifically addressing and how the mathematical characteristics of covariate shift come into play within their approach. Providing further clarity on this matter would help readers understand the specific focus and contributions of the proposed method in addressing the relevant shift.
- (Section 2, Fairness under distribution shift) In this section, the authors overlook several pertinent works, and some of the works mentioned are not even published. However, there exists a substantial body of literature that specifically addresses the challenge of ensuring fairness guarantees under distribution shift (for a comprehensive survey, the authors can refer to [1]). It is important to differentiate between methods that solely tackle distribution shift and those specifically designed for ensuring fairness under distribution shift. Furthermore, it is worth noting that different methods consider varying levels of data availability in the target domain, and not all of them assume the availability of (X, A) pairs [1]. For instance, the work [2] cited in that section assumes the target data is not available. 
- (Section 2, Related works) The purpose of this section is to not only provide a description of the related works but also to establish the connection between them, elucidate the significance of these relationships, and highlight the novelty of the proposed work or its intended aim to address specific limitations. However, despite providing descriptions of various works, the authors do not explicitly specify the precise position of their work within the broader landscape.
- Notation issues: After defining the notation at the beginning of Section 3, and Section 3.1, the authors employ symbols that have not been defined, such as, A in line 137, or $\mathcal{L}_{cls}$  in Eq (6). Regarding the latter, there is no specification regarding its meaning nor its mathematical form. 
- The paper contains several typos: line 67 after more there is a full stop, line 129 after of there should be an 'a', line 217 let should be in uppercase. 

[1] Barrainkua, A., Gordaliza, P., Lozano, J. A., & Quadrianto, N. (2022). A Survey on Preserving Fairness Guarantees in Changing Environments. arXiv preprint arXiv:2211.07530.

[2] Rezaei, A., Fathony, R., Memarrast, O., & Ziebart, B. (2020, April). Fairness for robust log loss classification. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 5511-5518).


Limitations:
The authors do not thoroughly discuss the limitations of their method, which is an important aspect to consider. Taking inspiration from the questions raised concerning the shift type and potential implications beyond binary Y and binary S could be valuable in addressing the limitations and further refining their approach.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper aims to improve the performances of fair training when the group attributes or labels in the training data have noisy information. The paper views the noisy training data problem as a kind of distribution shift, where the training data is noisy and the test data is clean. To address this issue, the paper proposes a fair representation learning method to reduce the impact of distribution differences. The paper also provides some theoretical analyses to show the relationship between the group fairness results and noisy training data. In the experiment, the paper uses three datasets and compares with several baselines to show the performance gains of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
S1. The paper solves an important research problem, preserving the performances of fair training under the noisy training data. The paper views this problem as a distribution shift issue.

S2. The paper gives some theoretical analyses on the relationship between the group fairness and noisy data.

S3. The proposed algorithm empirically shows better fairness and accuracy performances compared to the baselines. 


Weaknesses:
W1. Many important details are missing in the proposed fair representation learning. 
- In Section 3.3, the final training objective in Eq. (6) has many unexplained important details. For example, what is L_cls, and how are the input arguments (e.g., g_00, h) used in L_cls? Also, it seems the lambda values are the tuning knobs, but there is no explanation on why the loss terms should be connected by two lambda values. Including these details, a clearer rationale for design choices is needed.

W2. In experiments, the proposed algorithm is not clearly analyzed. For example, it would be much better if the paper explains the following.
- How the lambda values in Equation 6 affect the training performances
- The computational complexity of the proposed algorithm

W3. Although this work is highly related to the studies on fairness under data distribution shifts, there are no clear comparisons with them. In experiments, all the baselines are from the noisy training literature. Since many algorithms for fair training under distribution shifts have been recently proposed, it would be better to compare with them empirically or at least to be clearly discussed.


Limitations:
The paper did not discuss the limitations and possible negative societal impacts. As the limitations, this work may discuss which types of data noises cannot be handled by the proposed algorithm.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies the fairness problem under noise perturbation on both label and sensitive attributes. In particular, it considers such a problem from the perspective of distribution shift and uses the normalizing flow framework to analyze the problem. Empirically, the proposed methods achieve the best utility and fairness trade-offs under different settings of noise perturbation.    

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper presents a method for learning fair representation when there is noise on both sensitive attributes and labels. The method is straightforward and empirically shown to be effective. 
2. The theoretical analysis is sound 
3. Compared to the previous work, this work considers both label and sensitive attribute noise without directly estimation the noise parameter, which is more practical in real-world applications.


Weaknesses:
I do not find any obvious weaknesses in the paper. But there are minor points that the author could further improve their paper.
1. The assumption of invertible function in the fair normalizing flow methods might be strong. For example, In ResNet, the default activation function is ReLU, which is not invertible. The authors might need to provide more justification for this.
2. Discussion of limitations. The paper could be improved if there is a discussion of the limitations. 


Limitations:
The authors do not discuss the limitations of the work, which is highly suggested.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies noise tolerance of fairness from the perspective of subpopulation/subgroup shift, by considering the perturbation of the sensitive attributes as well as the labels _without_ the need for noise-rate estimation - by considering the noisy distribution as the source and clean distribution as the target. This leads to a ""covariate"" shift between the source and the target distributions, with the shift being a consequence of the noise. The work then proposes a fair representation learning method for fairness under noisy attributes based on normalizing flows, and presents a theoretical result showing that this method minimizes the upper-bound of the clean equalized odds. Thorough empirical evaluation is presented for both static and varying noise rates, showing the efficacy of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This work addresses an important setting of achieving fairness when both the label $y$ and sensitive attribute $a$ can be noisy - as traditional metrics can be biased under noisy data. The theoretical analysis presents a comprehensive study of fairness transfer between clean and noisy data and supports the choice of the minimizer in the proposed method. The empirical analysis is thorough - the section 4.2.2 is especially interesting as it considers both static and dynamic noise rates. 


Weaknesses:
The following points should be considered:
1. The loss function (in Eq. 6) focuses on a binary valued $a$ and $y$. How does this methodology extend to the more general case either/both can be multi-valued? Is it straightforward?
2. Is there any more intuition on leveraging normalized flows for this setting?




Limitations:
No limitations of this method are mentioned, and it would be nice if any potential drawbacks can be discussed. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper targets the problem of ensuring fairness with noises on either sensitive attributes or labels. Specifically, this paper models the noisy data training set and clean test set as a distribution shift and proposes a regularization term to improve the fairness of classifiers.
The theoretical analysis indicates that the classifier trained by the proposed framework on the noise data can be bounded when evaluating on the clean data.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The problem of ensuring fairness when training on noisy data is an important problem.

2. The whole framework makes sense.

3. The experimental results show the advantage of the proposed approach.

Weaknesses:
The writing is not very friendly to readers without a background in this specific fairness problem. Please check my questions below.

Limitations:
Please check my questions above.

Rating:
5

Confidence:
1

";0
x1FgW3vSM6;"REVIEW 
Summary:
This work proposes Flag Aggregator (FA) for a more robust aggregation of gradient in data-parallel training. FA formulates gradient aggregation as a Maximum Likelihood Estimation procedure using Beta densities. Theoretically, FA is analyzed using techniques from convex optimization. Empirically, FA demonstrates decent performance against Byzantine failure for image classification tasks (esp. ResNet-18 on CIFAR10) on a 4-GPU cluster networked with 100GbE.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
+. Proposed a simple Maximum Likelihood Based estimation procedure for aggregation purposes, with novel regularization functions

+. Provided code for reproducibility

+. Well-written: easy to follow

Weaknesses:
-. Marginal wall-clock time improvement, maybe due to heavy SVD overhead: e.g., Figure 10

-. Missing benchmark: 
1. only two small models are evaluated (e.g., ResNet18 and 2-layer CNN), how about more models like RNNs and larger models like GPT2?

2. only image classification tasks are evaluated (e.g., CIFAR10 and MNIST), not even CIFAR100 nor full ImageNet, how about more tasks like language modeling?

-. Missing modern cluster: 4-GPU cluster with one GPU per machine is not a modern setup for evaluating scalability of distributed training



Limitations:
Yes.

Rating:
5

Confidence:
1

REVIEW 
Summary:
This paper tackles the problem of Byzantine robustness in distributed learning by proposing a new robust aggregation rule called Flag Aggregator. The latter is based on maximum likelihood estimation with regularization. They empirically show that using distributed gradient descent with Flag Aggregator performs well against simulated Byzantine attacks compared to other existing solutions. 

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The problem of Byzantine robustness is important in distributed learning. Moreover, the proposed Flag Aggregator seems to follow a creative approach.

Weaknesses:
My main concern is the great lack of clarity of the paper, especially in the theoretical part. I also think that the theoretical and experimental parts lack several elements.

* Lack of clarity: the paper has several clarity-affecting issues which makes it really hard to assess the technical contributions. 
	* The paper starts (right away) with an unclear optimization problem (Equation 1): what are A, Y and C? 
	* line 99: why is $Y Y^\top G$ a ""reconstruction"" of $G$? and what is meant by reconstruction exactly?
	* lines 100-103: I could not verify the stated claims/intuitions. 
	* line 116: why does orthogonality imply efficiency? Authors seem to say that it is because we can derive a one-rank matrix factorization, but this does not require orthogonality of the matrix. In fact, $YY^\top G$ is just $G$ if $Y$ is orthogonal.
	* lines 123-135: this paragraph assumes that the reader knows what the Flag/Grassmanian manifold is, which was not the case for me.
	* Section 2.2: where does the vector $v$ come from? It is directly sent by the workers? Also, why do you assume that it follows a Beta distribution?
	* Algorithm 1: I could not find IRLS explained in the text. Also, it is strange that workers locally perform the update step. It always happens at server level in distributed SGD.
	* line 163: what is Flag Median?
	* line 188: what is a ""second order optimal local solution""?

* Lack of convergence guarantees: After all, a Byzantine-robust learning solution should have convergence guarantees, since simulated attacks are not guaranteed to be optimal; i.e. instantiate worst-case adversaries. Typically [Karimireddy et al. 2022, Allouah et al. 2023], convergence to a neighborhood of the original solution is ensured in the presence of Byzantine workers for smooth non-convex losses. 

* Experimental section: I suggest simulating more Byzantine attacks. The tested attacks (uniformly random vectors) are extremely weak compared to FoE [Xie et al. 2020], ALIE [Baruch et al. 2019] and others, which is unfortunate since the paper consider Byzantine adversaries. Also, some advanced defenses like NNM [Allouah et al. 2023] and Bucketing [Karimireddy et al. 2022] are missing although they were intended for non-iid; it is important to check how they perform against your method to assess the significance of the contribution.

[Allouah et al. 2023] Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity, AISTATS 2023.

[Karimireddy et al. 2022] Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing, ICLR 2022.

[Xie et al. 2020] Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation, UAI 2020.

[Baruch et al. 2019] A Little Is Enough: Circumventing Defenses For Distributed Learning, NeurIPS 2019.

Limitations:
Yes.

Rating:
3

Confidence:
4

REVIEW 
Summary:
Authors propose a gradient aggregation method for distributed optimization that is robust to Byzantine device failures in large scale distributed setups. In each round, given the set of gradients from each workers, the authors aim to find the optimal low-rank subspace that can explain the variance of a majority of the gradients. The authors formulate the problem as a MLE under a beta distribution setup  and solve an approximate version of the problem though SDP.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Byzantine device failures is an important concern for large scale clusters.  The presented method is well motivated theoretically and backed up with experiments comparing their robustness properties to other aggregation methods. Results demonstrate a significant advantage of this aggregation setup. 

Weaknesses:
Although it is evident that Byzantine failures can have a significant impact on gradient computation if using simple aggregation rules, its unclear how often such failures happen in the cluster sizes the authors have considered. Augmentation pipelines induce their own noise to gradient information, but its unclear if these will be adversarial in _each_ update step. The amount of noise induced and its effect on adversarial training setups is also not evident.  (See questions). This makes it unclear how the clear advantages of the method translates to real-world workloads especially considering that the method adds a potentially expensive top-k SVD computation step.

Limitations:
Yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes a new appraoch for aggregating gradients for distributed ML training under Byzantine failures, noise due to data augmentation, etc. The approach relies on constructing a low-dimensional subspace such that the proportion of variance of the gradient vectors contained in the subspace is maximized. The authors derive the loss function for their setting and formulate the problem as a regularized convex optimization problem which can be solved with standard solvers to obtain the basis for the subspace. The update direction is then obtained by projecting the individual gradients onto the basis and then averaging the result. Experiments on different datasets and number of workers show improved prediction accuracy over baselines when distributed training is performed using the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The proposed approach is principled and easy to interpret as it tries to identify the subspace which contains the maximum proportion of the variance of the gradients and is also easy to implement due to its formulation as a regularized convex optimization problem which can be solved by off-the-shelf workers.

2. The approach is extensively evaluated on a range of datasets (MNIST, CIFAR10, tiny-Imagenet) and for different noise models (random noise, adversarial data augmentation etc). I also appreciate the authors presenting results on wall-clock time to accuracy and per-iteration time thereby acknowledging the extra time required per iteration in their approach to compute the aggregated gradients. This opens the door to future research on speeding up the proposed aggregation method while retaining the accuracy gains.

Weaknesses:
1. My main concern with the approach is its novelty. Since the goal appears to be to estimate the subspace containing the maximum proportion of gradient variance, I am not sure why this cannot be done by retaining the top-k Principal Components of the gradients. The authors even acknowledge in line 109 that the idea to use the ratio of variance of projected and true gradients has been explored in the Robust PCA literature. However, they do not explain why simply considering the principal components will not work, nor do they perform experiments with PCA/Robust PCA as baselines. I would like to see at least one of the two (explanation/experiments) to be convinced of the need for the proposed approach and its gains over PCA.

2. The extra computational cost and the added time per iteration as seen in Fig 10 (b) is also a weakness. While I do appreciate the authors measuring and presenting this time, it is not clear at this point if the accuracy gain justifies the extra time. One way to demonstrate this would be to allow the other approaches to run for the same amount of time in Fig 10 (c). If it could be shown that even after running for that long these approaches cannot match the accuracy of Flag Aggregation, then the extra time required could be justified.

Limitations:
I feel the main limitations of the work are the increased computation time per iteration and the lack of clarity on novelty w.r.t PCA. I appreciate the authors' acknowledgement of the higher per-iteration time and look forward to their responses to the other limitations that I have identified under Weaknesses, above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a new method to aggregate gradients in a distributed training setting. Effectively, the proposed algorithm projects gradients onto a learned lower dimensional subspace and then aggregates the projections using standard techniques like averaging. This leads to a more robust aggregation against Byzantine failures. The projection is similar to a robust PCA, and is learnt using an approximate MLE via a Taylor expansion, leading to a computationally more feasible algorithm. Thorough experiments are conducted that demonstrate the efficacy of the proposed algorithm.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed novel algorithm empirically performs better than existing methods when measured by iteration complexity.
2. The authors provide a thorough comparison to existing methods and place their work in context.

Weaknesses:
1. The exposition in the paper lacks clarity in some places -- for example, the IRLS subroutine in Algorithm 1 is not described or even briefly summarized in the main paper.
2. The authors do not present their theoretical convergence results in the main body of the paper. 
3. As pointed out by the authors, the main proposed algorithm does not seem to perform significantly better than other existing algorithms when comparing wall clock runtimes.

Limitations:
Yes, the authors addressed the limitations of their work in the paper.

Rating:
6

Confidence:
3

";0
biLgaNKYdB;"REVIEW 
Summary:
The paper introduces a new method called YNN that transforms traditional ANN structures into yoked neural networks, promoting information transfer and improving performance. The authors analyze the existing structural bias of ANN and propose a model YNN to efficiently eliminate such structural bias. In their model, nodes carry out aggregation and transformation of features, and edges determine the flow of information. They further impose auxiliary sparsity constraints to the distribution of connectedness, which promotes the learned structure to focus on critical connections. Finally, based on the optimized structure, they also design a small neural module structure based on the minimum cut technique to reduce the computational burden of the YNN model. The learning process is compatible with the existing networks and different tasks. The obtained quantitative experimental results reflect that the learned connectivity is superior to the traditional NN structure.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. YNN promotes information transfer significantly which helps in improving the performance of the method.
2. The authors propose a model that efficiently eliminates structural bias in ANN.
3. The authors design a small neural module structure based on the minimum cut technique to reduce the computational burden of the YNN model.

Weaknesses:
1. There is a lack of ablation study, e.g., comparing the model performance using different clique size/number of cuts
2. The equations presented in Section 3.3 are excessively complex and challenging to comprehend. The authors have employed ""W"" and ""w"" for too many different variables in their notations, leading to confusion.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed a module called YNN that could exchange the information of the neurons within the same layer. The proposed module can be combined with MLP. The experiments on several small-scale datasets show that their method achieves good performance compared to previous networks.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
- The motivation of this paper is valid and interesting.


Weaknesses:
- I don't think a fundamental difference between the proposed YNN and graph neural networks. This method can be a special case by assigning a fully connect adjacent matrix to a GNN.
- The evaluation is only conducted on small-scale datasets. 

Limitations:
Please refer to the weakness section.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper propose a 'yoked' neural architecture where neurons at the same level are bidirectionally linked. They claim that optimizing this complete graph is superior to current deep neural network architectures that impose a structural bias due to the transfer of knowledge in a way that prevents structural bias. 

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
* the proposed optimization changes is simple in that it is similar to other methods like DARTS that grow neural networks and regulate their connections, assigning weights to them using ANN optimization algorithms with regularization term 
* it is clear how the forward propagation is done with the addition of the clique nodes that are computed in addition to the regular precursor nodes at each layer

Weaknesses:
* some of the terminology and abbreviations need to be defined / explained in the first appearance in the intro paragraphs (e.g. 'yoked', ANN, and DAG)
* the method of optimization doesn't seem particularly novel, employing both an L1 and L2 term to search for the best architecture. 
* the figure 2 presented does not particularly show much about the method or its justification
* Is it possible to approximate the non-differentiable minimum cut algorithm and absorb it into the training procedure? This would be similar to progressive training methods like http://proceedings.mlr.press/v119/evci20a/evci20a.pdf and other related works)
* please proofread for more typos and such (e.g. 'mata' on l204, other grammatical errors)
* results are on quite toy problems and training with regularization is not necessarily yielding the best results 

Limitations:
Authors do not discuss limitations of their work.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes Yoked Neural Networks (YNN) - an extension of neural networks, which, when calculating the value of a node, in addition to the information from the previous layer of the network, uses information from the nodes on the same layer (i.e. it ""yokes"" nodes from the same layer together). 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper describes the approach well, it is clear how it works.

Code has been provided as an attachment, so that it should be reproducible (I have not ran the code or looked at it carefully).

Weaknesses:
(Details about the mentioned weaknesses are given per line, in the field ""Questions"".)

The paper would benefit from describing in more details the contributions of the proposed method. Particularly, across the paper, some strong statements have been used, but they have not been motivated with evidence. 

Overall, the idea and the benefits of using the method needs to be better motivated.

The description of the experiments is not very clear.


Limitations:
No limitations have been addressed. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a novel neural network model that exploits a connection between the nodes of a layer. The authors’ goal is to develop a model that overcomes the structural bias posed by the classical layer structure of the NN. To do this they propose to consider the model as a bidirectional complete graph for the nodes of the same level and to define for each layer a clique. 
The authors then test the proposed architecture and compare it with the traditional NN model considering 3 datasets.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper discusses an interesting problem and proposes a novel methodology that seems promising.

Weaknesses:
Overall, the paper is challenging to read, and at times, the concepts being discussed are not adequately introduced, causing difficulty for the reader to comprehend the discussion's progression. Additionally, several critical concepts are unclearly defined.

In the introduction, the authors discuss the neural module without explaining what it is. Even the concept of “yoke”, which is central to the discussion, is not adequately introduced and explained in the context of neural networks.
In the introduction the authors also discuss the impact of the sparsity constraints, also in this case this concept has to be defined and explained to the reader.
In the list of contributions, point 4 says that the authors designed a regularization-based optimization, which at this point of the paper is very difficult to understand. Point 5 of the same list discusses the problem of computational complexity, which also in this case is not discussed before.

Another issue is the experimental campaign where the experimental setting and the metric used to perform comparison are not explained. Indeed the authors use the “variety of nodes” as a metric, but they do not explain why it has to be significant to show the advantage of the proposed approach.
From the tables, 1,2,3 seems the authors fix the number of nodes for the various architectures and train them. In general to me, it does not seem a fair way to compare the models, mainly for 2 reasons: (i) the architectures of the baselines have to be validated (in particular in terms of the number of neurons, but also considering the other hyperparameters of the model and of the optimization algorithm) in order to find the most suitable setting for the task. (ii) the comparison has to consider the number of parameters of the model since the structure of the  YNN will have many more weights than a standard model (fixing the number of neurons).
Even a description of the setting of the three proposed approaches (YNN, YNN&L1 ,YNN&L2) would make it easier for the reader to understand the proposed results.
Finally, a discussion about the computational burden and a comparison with standard NN is missing.
The experimental evaluation and the discussion of the obtained results should be significantly improved and extended

Limitations:
Not applicable

Rating:
3

Confidence:
3

";0
IfRHdBy3wb;"REVIEW 
Summary:
This paper proposes a novel local-level post-hoc GNN explainer (GOAt) that takes the issue of discriminability and consistency into consideration.  The discriminability and consistency of graph explanation are significant and meaningful for the community’s understanding of the GNN decision-process. The GOAt explainer attributes the prediction of the trained GNN to the input graph features, i.e., the node feature matrix and the adjacency matrix, based on the Equal Contribution technique. The authors implement the specific GOAt with regard to GCN, GraphSAGE, and GIN. Sufficient experiments, including both qualitative and quantitative evaluation, verify that GOAt outperforms the SOTA GNN explainers in terms of both fidelity, discriminability, and stability.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
S1: The issue of discriminability and consistency (i.e., stability) in GNN explanation methods is thought-provoking. However, it is neglected by previous methods. The GOAt explainer bridges the research gap.
S2: The mathematical derivation of the equal contribution technique is solid and ingenious, which well support the design of GOAt explainer.
S3: The experiments to verify the superiority of GOAt are sufficient and the results are discussed in detail.

Weaknesses:
W1: My main concern is about the effectiveness of the attribution strategy based on equal contribution. Though the experimental results have shown it outperformance, the intuitive reason why equal contribution can improve the discriminability and stability of GOAt explanation is still obscure for me. In other words, the motivation behind GOAt is not well-illustrated.
W2: The organization and writing of this manuscript need to be improved. Some contents are a little confusing. For example, in line 155, the authors claim that the number of occurrences is not necessarily equal to the number of unique variables. However, the example following does not explain this claim well.

Limitations:
The authors have discussed some limitation about GOAt, including the requirement of expert knowledge, the failure in explaining deep models (e.g., Transformer). In my regard, as a post-hoc explainer, GOAt has to access the detailed architecture of the GNN model to be explained, which may serve as the main limitation in real-world application.

Rating:
6

Confidence:
5

REVIEW 
Summary:
In this paper, the local explanation algorithm for Graph Neural Networks (GNN), Graph Output Attribution (GOAt), is proposed. The key idea of the algorithm is decomposing the GNN's forward propagation as sum of product form. After decomposing, authors compute the contributions of nodes and edges by proposed measure. Next, (sum of the contribution including the interest node or edge) - (contribution from global bias) becomes the contribution (explanation) of the node or edge. The experiments show (1) the explanation can capture the importance of edges (fidelity, discriminability) (2) the explanation is consistence (Stability).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is general for graph neural networks.
2. The examples help the reader understand the explanations.
3. The experiments show the performance of proposed model with regard to baselines for various criteria.

Weaknesses:
1. The paper used zero-vector as base manifold. It makes that the contributions for all variables are always equal.
2. The contribution does not consider the value of variables.
3. It would be better to explain the difference between the baseline algorithms and proposed algorithm.

Limitations:
The paper used zero-vector as base manifold. It makes that the contributions for all variables are always equal. Also, the proposed contribution consider absence or presence of variable only without the value of variable.

Rating:
5

Confidence:
3

REVIEW 
Summary:
It proposes a local GNN explanation method called GOAt by analysing the output attribution of graphs. Specifically, it proposes a novel explanation framework by the notion of equation contribution, which is transparent compared to the existing black-box type explanation models. Experiments on real and simulation data are tested on several metrics, where the results are convincing.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The point of equation contribution is quite novel and interesting. Especially its transparency compared to current baselines.
2. The experiments are extensive and convincing.

Weaknesses:
1. In line 28, it seems that ""their effectiveness relies heavily on the quality of local-level explanations"" is the main point of abandoning the global style in this paper, could you explain more about this in the rebuttal phase or say more about it in section 2 - see second con below.
2. In section 2, the differences between global and local explanatory methods should be introduced in a proper way to show the superiority of global explanations. Also, please consider including the matrices in lines 66-69 as another subsection to introduce the matrices together.

Limitations:
The authors presented the limitations in the Appendix about wider range of model architectures. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
Unlike existing methods that rely on training auxiliary models, the paper introduces a computationally efficient instance-level GNN explanation technique (called GOAt), which enables attribution of GNN output to input features by direct algebraically computation. More specifically, a GNN is represented in an expansion form as a sum of scalar product term, involving input graph features, model parameters, and activation levels. Based on the assumption that all scalar variables in a scalar product term contribute equally to the term, each product term can be attributed to its corresponding factors and thus to input features, which gives the importance of each node or edge feature in the input graph to GNN outputs. Besides that, the paper also introduce two new metrics called discriminability and stability, which assess the ability of explanations to distinguish between classes and the ability to generate consistent explanations across similar data instances, respectively. Through extensive experiments on synthetic and real data, the paper show that GOAt has consistently outperformed various state-of-the-art GNN explainers in terms of fidelity, discriminability, and stability.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.	The assumption that all scalar variables in a scalar product term contribute equally to the term is acceptable to me. And the derived lemmas and theorems are well proved and practically applicable.
2.	The proposed explanation method is computationally efficient, and does not rely on back-propagation with gradients, hyper-parameters or training complex black-box models, which is novel to me.
3.	The work contributes a new attribution method to the field of explaining GNNs (might extend to other types of neural networks, e.g., CNN) .


Weaknesses:
1.	The specific calculation expression of the attribution process needs to be carefully derived with expert knowledge for different neural network architectures and becomes very complex as the network goes deeper, which greatly limits its widespread use, though it shows superior explanation ability.
2.	The main argument that existing methods struggle with issues such as overfitting to noise, insufficient discriminability, and inconsistent explanations across data samples of the same class is not well elaborated. Though the experiments seem to have verified this argument, it will be more motivated if some detailed illustrations (e.g., by presenting a toy example) could be provided in the introduction part.


Limitations:
a)	Build an algorithm to automatically derive the attribution expressions based on the network architecture.
b)	Although the experiments seem to corroborate the main argument, bolstering the introduction with some detailed illustrations (for instance, by incorporating a simple, illustrative example) could make the paper more compelling.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This work studies the problem of GNN local explanation.  It points out that existing GNN explainers are suffering from a few limitations including insufficient discriminability, inconsistency on same-class data samples, and overfitting to noise, and the aim to address these limitations by proposing the GOAt method. Given a pre-trained GNN model, GOAt defines a contribution score on each node feature and each edge feature (i.e. values on each edge) by expanding the GNN as a sum of scalar products involving these features, and thus would be able to find the edges that contribute more to the GNN's output. The authors evaluate GOAt with the frequently used fidelity metric, as well as the newly proposed discriminability and stability metric in this paper.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Results are impressive.
2. Code is provided in the supplementary. 
3. The newly introduced discriminability and stability metric for explanation evaluation makes sense to me.

Weaknesses:
1. The writing for the method section is not easy to follow. 
2. The motivation of why the contribution should be designed in this way is not clearly explained. Why the current design can address the insufficient discriminability, inconsistency on same-class data samples, and overfitting to noise issues also remains unclear to me.

Some minor weaknesses:
1. There seems to be one typo in the appendix proof lemma 2: line 11 should be X_j not E_j?
2. The term ``feature`` is used to describe all X_{ij}s and A_{ij}s, which looks a little bit confusing for me. I think usually people use the term ``feature`` to describe the attributes on nodes and edges, but here A_{ij} is more like a weight scaler on edge instead of the edge attributes? I feel it would be clearer to put a note to clarify that the term ``feature`` is used to describe all node attributes and edge weights.
3. It would be clearer if the authors can explain what does each lemma and theorem imply.

Limitations:
Please refer to ``weaknesses``.

Rating:
5

Confidence:
3

";0
td6xbEOPLr;"REVIEW 
Summary:
This paper proposes to attack different fairness definitions for a variety of graph learning models. The task is formulated as a bi-level optimization problem, which is solved in a meta learning manner. The major advantages of the proposed framework include 1) it is feasible to any fairness notion and graph learning model, 2) it can support continuous and discrete perturbation on the graph topology. Experiments demonstrate the attack efficacy and classification utility of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed framework provides a good coverage for multiple types of (differentiable) fairness notions (e.g., statistical parity, individual fairness), and graph learning models (e.g., non-parametric, parametric);
2. Extensive experiments are conducted, considering possible fairness defenses (FairGNN and InFoRM-GNN) and transferability.

Weaknesses:
1. The clarity and rationale of methodology can be improved: there are inconsistent definitions (of the budget constraint), and the realization of budget constraint in the optimization procedure is not validated. See detailed questions;
2. The IID assumption for kernel density estimation may not hold in graph data;
3. Important baseline should be compared (a modified version of DICE based on the sensitive group); and more related works should be discussed.

Limitations:
The authors provide reasonable discussion about the limitations of this work (i.e., other fairness notions, space efficiency).

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose an attacking framework called FATE. Existing research in algorithmic fairness aims to prevent bias amplification but neglects fairness attacks. This paper fills this gap by formulating the fairness attack problem as a bi-level optimization and introducing a meta-learning-based attack framework. The authors present two instantiated examples, demonstrating the expressive power of the framework in terms of statistical parity and individual fairness, and validate the model's capability of attacking fairness through experimental verification. The paper contributes by providing insights into adversarial robustness and the design of robust and fair graph learning models.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well-structured and easy to follow.
2. The originality of the article deserves emphasis as it formulates the fairness attack on graph data as a bi-level optimization problem. This novel approach contributes to understanding the resilience of graph learning models to adversarial attacks on fairness.
3. The experimental results show that the proposed method is capable of attacking fairness without decreasing too much on accuracy.

Weaknesses:
The motivation provided is somehow insufficient in persuading me. The authors state that “an institution that applies the graph learning models are often utility-maximizing” (line 82), which I totally agree with, but then concludes that “minimizing the task-specific loss function … for deceptive fairness attacks” (line 88). While I do agree that pursuing utility will lead to a preference for models with superior performance, and if the objective of the attack is to deceive victims into selecting an unfair learning model, then it does make sense to enhance the utility of the malicious model. But it’s not the case in this article, where the attack’s aim is to poison the graph data. Unlike models, we don't have much discretion when it comes to the data, and it is exceedingly challenging for me to envision a real-life scenario wherein an institution would discard the data due to unsatisfactory performance, as a more practical solution is data cleansing or a more capable model. Consequently, my concern is that, is it truly necessary to maintain the utility for “deception”, leaving aside that I’m not convinced that preserving the utility is necessary for successful deception. Please provide a more detailed explanation.
 
The experimental findings reveal the limitations of the attack's effectiveness. Although I agree that FATE is capable of attacking fairness and is relatively more stable, in 2 out of 3 datasets, FATE exhibits incompetence in reducing statistical parity compared to FA-GNN. Although the authors argue that the victim model achieved the best performance under the attack FATE, I’m skeptical about the cost-effectiveness of this trade-off.

Limitations:
Yes, the authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel framework named Fate, which is capable of attacking any fairness definition on any graph learning model, as long as the corresponding bias function and the task-specific loss function are differentiable. Fate is equipped with the ability for either continuous or discretized poisoning attacks on the graph topology.studies. The paper provides insights into the adversarial robustness of fair graph learning and sheds light on designing robust and fair graph learning in future studies. The empirical evaluation on three benchmark datasets shows that Fate consistently succeeds in fairness attacks while being the most deceptive (achieving the highest micro Fl score) on semi-supervised node classification.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1) This paper addresses an important problem in graph learning — fairness attacks. While previous work in the field focused on ensuring that bias in not perpetuated or amplified during the learning process, the proposed framework, FATE, allows for the study of adversarial attacks on fairness.

(2) FATE, a meta-learning based framework, is versatile and can be used to attack different fairness definitions and graph learning models.

(3)The experimental evaluation shows that the proposed framework can successfully attack statistical parity and individual fairness on real-world datasets with the ability for poisoning attacks on both graph topology and node features while maintaining the utility on the downstream task.

(4) This article is well-written and well-organized. It starts with an introduction that highlights the importance of fair graph learning and the need for resilience against adversarial attacks. Then it provides some background information and defines the problem of fairness attacks in graph learning. The paper then proposes the Fate framework as a solution to this problem, providing a detailed explanation of its design and mechanism. It also presents experimental results to evaluate the efficacy of Fate. Finally, the paper concludes with a summary of its contributions and future research directions. Overall, the writing logic is clear and easy to follow.

(5) The paper provides detailed information on how they implemented the proposed framework, including the optimization process, the selection of the bias function and the task-specific loss function, and the hyperparameter tuning process. Additionally, they provide a detailed description of their experimental setup, including the datasets used, the graph learning models, the evaluation metrics, and the implementation details of the Fate framework and other baselines. The authors also provide a thorough evaluation of the proposed framework through extensive experiments and analysis.


Weaknesses:
(1) One weakness of this paper could be the limited evaluation of the framework on only three benchmark datasets and one task (semi-supervised node classification).
 Further evaluations on various graph learning tasks and datasets could provide more insights into the effectiveness and generalizability of the proposed framework.

(2) The proposed Fate framework may not be effective in attacking other fairness definitions beyond statistical parity and individual fairness, and it may not work well on graph learning models with very large graphs. 

(3) The paper assumes that the attacker has access to sensitive attributes of all nodes, which may not always be feasible in real-world scenarios.


Limitations:
see comments above

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a novel approach for introducing fairness attacks in graph learning, which is impressive. To address this issue, the article proposes an attack framework for graphs and conducts experiments on the classic GCN model. Compared to two baseline methods, DICE and FA-GNN, the proposed method is more effective in attacking graph neural networks.

The strengths of the paper include:

1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

The weaknesses of the paper includes:

1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

Weaknesses:
1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Limitations:
The article does not mention the limitations of the work. More discussions are required.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper studies an interesting problem, attacking fairness on GNN. Specifically, the authors aim to amplify the unfairness while maintaining the performance of the downstream tasks. They propose a bi-level optimization scheme with a meta-gradient poisoning attack to achieve this goal. Experiments on both statistic fairness and individual fairness show the effectiveness of the proposed method. Overall, good problem and a solid framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	Fairness attack on graph learning is an interesting problem, this paper gives an intuitive problem definition, which can extend to other bi-level optimization goal attacking scenarios.
2.	Given the bi-level optimization goal, the authors design a meta-gradient graph poisoning attack and corresponding Low-level and High-level loss functions.
3.	Experiments on statistical fairness and individual fairness demonstrate the effectiveness of the proposed method.


Weaknesses:
1. It will be more interesting if the authors provide more experiments on group fairness and Rawls fairness. How to attack some specific groups, such as best/worst accuracy group fairness.

Limitations:
None

Rating:
6

Confidence:
3

";0
zKjSmbYFZe;"REVIEW 
Summary:
This paper considers fairness in multi-class classification under the notion of parity of true positive rates - an extension of binary class equalized odds - which ensures equal opportunity to qualified individuals regardless of their demographics. We focus on algorithm design and provide a post-processing method that derives fair classifiers from pretrained score functions. 

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- paper deals with an interesting problem
- paper is technically sound

Weaknesses:
- paper does not properly compare w.r.t. the state of the art 
- novelty of the proposal is not clear
- paper is hard to read and follow

Limitations:
Paper is very hard to read, follow, and fully understand. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel post-processing approach to reduce the true positive rate parity for multi-class classification problems. It is shown on two real world data to outperform an existing baseline in terms of accuracy and true positive rate parity.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed approach is novel and technically sound.

2. The proposed approach guarantees fairness by a sample complexity bound.

3. The proposed approach outperforms an existing baseline post-processing approach in terms of reducing TPR parity.

4. The presentation is clear.

Weaknesses:
1. Some related work, such as [45], is mentioned in the paper but not compared in the experiments.

2. Fairness in multi-class problems seems to be a rather trivial/incremental problem when there are plenty of approaches solving the fairness problems in binary classification and regression problems. Approaches from fairness in regression (e.g. Narasimhan et al., ""Pairwise fairness for ranking and regression"", 2020) should be easily applied to solve the fairness problems in multi-class classification. Please justify more for why Fairness in multi-class classification is a problem worth studying. Some approaches from fairness in binary-classification (e.g. Kamiran and Calders, ""Data preprocessing techniques for classification without discrimination"", 2012; Yu et al., ""FairBalance: How to Achieve Equalized Odds With Data Pre-processing"", 2023) can also be easily adapted to multi-class problems.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The work proposes a post-processing algorithm to achieve the equal opportunity constraint in multi class classification. The proposed algorithm takes arbitrary Bayes rule estimate and only requires additional unlabelled data. The authors derive finite-sample guarantees and perform empirical evaluation to support their claims.

I did not check the math, but, having prior experience in this area it looks believable.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and the proposed methodology is sound. It extends a rather long line of works on post-processing with unlabelled data.

Weaknesses:
In the context of the paper, I do not see major weaknesses from the methodological side, but rather remarks that are presented in the next part. 

From the theoretical part, I could mention that the devision by p_{a, y} with large number of protected attribute and classes can make the bound non-informative. 

I think that the expectation in the fairness guarantee is not well placed, I expected to have  $E[\Delta(h)]$. 

Limitations:
---

Rating:
6

Confidence:
4

REVIEW 
Summary:

This paper studies algorithmic fairness in multiclass classification setting. The fairness notion considered is parity of true positive rates (TPR) which is the multi class analog to equalized odds. The paper gives a post-processing algorithm which, given a score function, outputs a fair classifier. The paper then gives sample and time complexity guarantees and experimental evaluation on benchmark datasets. 

This paper furthers work studied Alghmadi et al., which according to the authors, is the only other post-processing method available for multi-class TPR parity. 

The paper gives a general post-processing algorithm that takes as input a score function and outputs a classifier that satisfies approx TPR party. The authors show that if you begin with the Bayes score function, their post-processing returns an optimal, fair classifier. They also give results showing that if the initial score function is not Bayes optimal but instead satisfies a decision calibration condition, then the post-processing is optimal among all classifiers that can be derived from the initial score function. 

The post-processing method consists of two parts. The first part of the process estimates the feasible region of TPRs and then finds the utility-maximizing TPRs that satisfy fairness constraints (either exactly via search if we know the distribution, or via a linear program if we are estimating the TPRs from data). The second step involves finding the hypothesis that achieves these TPRs, which either corresponds to a tilting - essentially a threshold - of the score function without randomization or a mixture of two models that lie on the boundary. 

Finally, the paper gives some experimental evaluation on three benchmark datasets that serves as a proof of concept of the post-processing and showed that it is competitive with other standard techniques (notably reductions). 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Fairness via post-processing in multiclass classification settings is not a focus on most prior work on algorithmic fairness, which makes this an interesting and novel contribution. 

Although restricted to TPR parity, the paper does a complete analysis of the topic under this fairness definition. The paper addresses both not knowing the underlying distribution and so possibly starting from a non-Bayes score function and also discuss estimating the parameters from finite samples. 

Weaknesses:
The paper gets a little notationally and technically dense in Section 3. While the presentation is still fairly clear, I think additional higher level exposition to describe in particular Step 2 of the algorithm could be beneficial to readers - maybe including additional description of a tilting. 


Limitations:
The authors address one of the main limitations of this work that their method is confined to only reducing TPR disparity and not other fairness notions. 

Rating:
7

Confidence:
3

";0
3Da0eESvN1;"REVIEW 
Summary:
The paper introduces an efficient algorithm for learning halfspaces in the testable learning model in which the tester-learner first applies a test on the training data and if the test succeeds the algorithm produces a hypothesis which is guaranteed to be near-optimal. It is required that if the data comes from a target distribution, then the test must succeed with high probability.

The paper considers learning halfspaces in the case where the target distribution is Gaussian (or strongly log-concave) and where the labels are subjected to Massart noise or adversarial noise (i.e., agnostic setting). The paper builds on several ideas from previous papers by Diakonikolas et al.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
Learning halfspaces is a fundamental problem in machine learning. Even though it is one of the simplest tasks, the problem becomes non-trivial in the presence of label noise. Recently, there has been a lot of interest in developing algorithms for learning halfspaces in several settings (e.g., Massart noise, ...). The submitted paper is the first work that presents an efficient algorithm for learning halfspaces in the testable learning model of Rubinfeld and Vasilyan.

I find the paper to be clear and generally well-written, and I find the results novel and interesting.

Weaknesses:
Minor comments/typos:
- Page 2 line 71: ""an important one being that the probability mass of any region close to the origin is proportional to its geometric measure"" -> ""an important one being that the probability mass of any region close to the origin is roughly proportional to its geometric measure"".
- Page 5, line 198: Shouldn't the title of the section be ""Testing properties of isotropic strongly log-concave distributions""?
- Page 5, line 214: ""and runs and in time poly(...)"" -> ""and runs in time poly(...)""
- Page 5, line 235: There should be a comma between \tau and \delta.
- Page 7, line 307: ""Each of the failure events will have probability at least $\delta'$ "" -> ""Each of the failure events will have probability at most $\delta'$ "".
- Page 8, line 322: ""under theempirical distribution"" -> ""under the empirical distribution"".

The following relevant references seem to be missing:
- [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
- [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.

Limitations:
No concerns regarding potential societal impact of this work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This papers gives an efficient algorithm for learning halfspaces under the testable learning framework of Rubinfeld and Vasilyan (STOC'23) and facing either Massart or agnostic noise. In this setting, the algorithm is given some reference marginal distribution $D^*$ (which is assumed to be isotropic and strongly log-concave), and it may choose to ""reject"" (asserting that the actual marginal is different from $D^*$) instead of outputting a hypothesis. Naturally, the learner needs to satisfy the following two conditions:
- Completeness: When the marginal is indeed $D^*$, the learner does not reject w.h.p.
- Soundness: The probability that the learner outputs an insufficiently accurate hypothesis is low. Here, sufficient accuracy means achieving either $\mathsf{opt} + \epsilon$ (under Massart noise) or $\tilde O(\mathsf{opt}) + \epsilon$ error (in the agnostic setting), where $\mathsf{opt}$ is the loss of the optimal halfspace and $\epsilon > 0$ is a parameter.

The solution is built upon the nonconvex optimization approach to learning halfspaces under noise in the literature. The key property for this approach to succeed is that when some appropriate loss function is minimized, all the stationary points are reasonably close to the true parameter. The crux of the current work is then to identify certain testable properties of the marginal under which the above argument goes through, so that we either get a good learning guarantee, or obtain a witness for that the marginal is not $D^*$.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper studies a fundamental learning theory problem (i.e., learning halfspaces) in the newly introduced testable learning setup. The results are strong and comprehensive, and the solution is nontrivial and requires several novel ideas. The paper is very nicely written and well-strutured, and the main paper contains sufficient details (including the ""technical overview"" section) for the reader to appreciate the high-level ideas behind the work.

Despite the few weaknesses discussed below, I found the paper a strong submission that should be accepted.

Weaknesses:
- The hypothesis class is restricted to homogeneous halfspaces (without a bias).
- In the agnostic case, the error bound can be higher than the optimal error by a constant factor.

Limitations:
This is a theory paper and its limitations are in the assumptions made by the problem setting as well as the main results, e.g., the restriction to halfspaces, the noise model, and that a single reference marginal distribution is provided. These are clearly stated in the paper as well as the separate ""Limitations and Future Work"" section.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Learning halfspace is a very important problem in machine learning which has been studied extensively. However, generally some distributional assumptions like gaussianity are assumed which in general is difficult to verify. To address this issue, recently Rubinfeld and Vasilyan (STOC 23) have introduced Testable learning framework. Here the primary objective is that if the tester accepts, then the output of the learner is close to OPT + \epsilon (OPT being the optimal error), and when it satisfies the distributional assumptions, the algorithm accepts with high probability. However, when the Gaussian distributional assumption is taken (let's denote this as D^*), it takes $d^{1/\epsilon^2}$ samples, which is also tight. Thus often researchers are interested to design algorithms that have better complexity with respect to $1/\epsilon$, but the error becomes $f(OPT) + \epsilon$ for some function f.

In this work, the authors first design a tester when $D^*$ is isotropic log-concave distribution and the labels are corrupted according to Massart noise (the labels are changed by an adversary with a small probability $\eta$).  Their algorithm runs in polynomial time and has error $OPT + \epsilon$ (Theorem 4.1). Later they design testers for adversarial noise with respect to Gaussian distribution with error $O(OPT) + \epsilon$ (Theorem 1.2).

In Section 4, the authors study the case with Massart noise. The primary idea here is to minimize a non-convex smooth surrogate loss (4.1) such that its stationary points correspond to halfspaces with small error. The authors first run PSGD on this surrogate loss function to get a set of vectors L such that one vector in L is close to the optimal weight vector. Then they apply localization ideas based upon the region that is an axis-aligned rectangle T, and check if the low degree moments of input distribution D conditioned on T match with D^* conditioned on T. This will ensure that the angular distance of a stationary point w is close to the optimal w^* (Lemma 4.3). To convert closeness in angular distance to closeness in 0-1 loss, they use the fact that the distribution is isotropic strongly log-concave (Proposition 4.4).

Later in Section 5, the authors study the agnostic setting where they will call the algorithm from Section 4 several times, each time with different parameters. The idea is that in the agnostic setting, running the algorithm for only once, the algorithm might only consider points that lie within a region with small probability. This finally gives a tester with error $O(OPT) + \epsilon$  when $D^*$ is isotropic log-concave distribution as well as Gaussian (Theorem 5.1 and Theorem 5.3).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper gives the first algorithm for testable learning of halfspaces that runs in poly(d, epsilon). The algorithm is very nice. With the complexity pulled down drastically, a proper implementation and experimental results for this algorithm would be possible and it would be nice to see the relevance of the concept of testable learning in various applications.

Also the algorithm can handle both adversarial and massard noise.

Weaknesses:
The usefulness of the testable learning model in real life applications is yet to be understood.

Limitations:
It is a pure theoretical work in the paradigm of testable learning - a relatively new concept whose importance is not yet fully confirmed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This work provides an efficient algorithm for testably learning halfspaces, extending the frontier of the recently introduced testable learning which does not assume anything about the given data distribution. Specificially, the setting is as follows: the target distribution is standard Gaussian (or any fixed strongly log-concave distribution) and the label noise is Massart or adversarial (agnostic).

The main result is two-fold. 
1. For Massart noise, if the target distribution is strongly log-concave, the paper proves an algorithmic guarantee that testably learns halfspaces up to $opt + \epsilon$ error and runs in $poly(d,1/\epsilon,1/(1-2\eta), \log(1/\delta)$ time. 
2. For agnostic learning,  if the target distribution is strongly log-concave, the guarantee is that the algorithm testably learns halfspaces up to $O(k^{1/2} opt^{k/(k+1)} + \epsilon$ error and runs in ""roughly"" $poly(d^k,1/\epsilon^k, \log^k(1/\delta)$ time (ignoring some logarithmic factors). One can strengthen this result if the target distribution is standard Gaussian. Then the algorithm testably learns halfspaces up to $O(opt) + \epsilon$ error and runs in $poly(d,1/\epsilon, \log(1/\delta)$ time, a result that matches previous non-testable learning results for halfspaces.

The methodology borrows two algorithmic ideas and strengthens them. One is the algorithmic idea that runs (nonconvex) SGD on a convex surrogate (ramp function) for the 0-1 loss. Originally, given some distributional assumption, this approach would yield a hypothesis found from an approximate stationary point. In this paper, the authors check if such property is satisfied for the unknown distribution of the testable learning setting, leading to develop a three-stage testing procedure for strongly log-concave distributions. Here, the second algorithmic idea of moment matching kicks in. For the band $T$ s.t. $|\langle w,x \rangle| \le \sigma$, tests are ran on the empirical distribution conditioned on $T$ to check moments matching those of the target distribution on $T$.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- This work extends upon the recently introduced testable learning in one of the fundamental problems of learning, i.e., learning halfspaces. I believe the topic is of good importance as testable learning yields more practicality to learning algorithms. In that sense, the work studies and provides strong algorithmic result to an important problem.
- The techniques used in the paper utilize two previous algorithmic ideas (convex surrogate SGD + moment-matching to fool) and neatly tie these two ideas together to a testable algorithm. The algorithmic techniques are also practical.
- The tests are ran on the distribution conditioned on the band $T$. With this ""trick"", the paper manages to change weak additive guarantees to strong multiplicative ones.
- The results are technically strong and presentation is clear.
- The agnostic learning algorithm only modifies the Massrt one slightly, which is neat, though this may be more of contribution of previous work than that of this work.

Weaknesses:
No notable weaknesses, but refer to Questions for a potential undecided one.

Limitations:
No limtation addressed.

Rating:
7

Confidence:
4

";0
Wp7TIOaDbb;"REVIEW 
Summary:
This paper formulates a Lipschitz loss function that makes computing approximate interior Nash equilibria in normal-form games amenable to unbiased Monte Carlo estimation, opening the door to using a number of scalable stochastic optimization techniques. They also provide a loss function with similar properties but under the notion of quantal-response equilibrium (QRE). The authors also provide certain illustrative experiments to support their claims.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
This paper provides a novel approach to computing equilibria in multi-player games. In particular, the authors derive loss functions that make equilibrium computation amenable to scalable methods from stochastic optimization. Given the lack of scalable algorithms for computing solutions concepts such as the Nash equilibrium, this is a promising approach, and has the potential to bring many new insights to equilibrium computation. In particular, the idea proposed for deriving an unbiased estimator (Section 4.4) is interesting, and addresses many of the pitfalls of other commonly used loss functions in the more challenging constrained setting. Furthermore, the presentation and the writing are overall clear, and the authors accurately place their results into the existing literature.

Weaknesses:
There are a number of issues that weaken the contribution of the paper. First, the underlying assumption that there is an interior Nash equilibrium is very strong. For one, if there is an interior NE it is known that it can be computed in polynomial time via linear programming, which significantly weakens the motivation regarding the hardness of NE. There appears to be some confusion regarding this point. For example, Corollary 3 in the Appendix claims a new FPTAS for computing interior NE in polymatrix games, which I believe is known (beyond polymatrix games); there is perhaps still some benefit in using the proposed methodology in practice, but no evidence of that is provided in the paper. (As an aside, it would be helpful to clarify in the prelimaries that by interior you mean relative interior.) Beyond the very restrictive assumption of having an interior Nash equilibrium, the authors provide similar results for QRE, but that is a significantly weaker equilibrium concept. I would also strongly recommend clarifying in the abstract that your results apply for interior NE; as it is written currently it is very misleading. 

Besides the issue mentioned above, there is an underlying premise in the proposed methodology which I find unconvincing: Why should we expect local optima in the formulated loss functions to give meaningful guarantees? The fact that this turned out to be the case in many ML applications is not enough to justify this proposition. It is a significant weakness that the proposed method has no theoretical finite-time guarantees of reaching a Nash equilibrium. 

And unfortunately the experiments do not offer enough evidence to support this approach. Indeed, there are many issues in the experiments that can be significantly improved. First, the games experimented on are overly small; for example, Shapley's game is completely toy, no meaningful conclusions can be drawn from it. Since the main message of this paper is about scalability, I expected to see experiments on much bigger games. It would be helpful if the proposed theory applied in extensive-form games for which there are many large benchmark games in the literature, but the current method is tailored to normal-form games. Besides this issue, I am very confused regarding the compared benchmarks. It is claimed in the last sentence of the abstract that the method often outperforms prior state of the art, but the main algorithms compared against are RM and FTRL. These algorithms will not even find an NE in finite-time, how can you claim that those are the state of the art? In particular, in Lines 966-969 it is claimed that those are the two most popular scalable stochastic algorithms for approximating NE; I strongly disagree with this claim. I would suggest trying some other benchmarks, such as the Lemke-Howson algorithm; a mixed-integer programming approach; or the algorithm presented in ""Exclusion Method for Finding Nash Equilibrium in Multiplayer Games.""

Another issue is on the proof of Corollary 1. For a constant $\epsilon$, it is claimed that you have a poly-time algorithm (since it is a PRAS), but you also say that the temperature parameter has to be exponentially small to achieve that. So it seems that even for a constant $\epsilon$ you need an exponential number of iterations to converge. 

Overall, although the approach proposed is promising, there are a number of issues that have to be addressed before the paper is ready for publication.


Limitations:
The authors have addressed the limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a novel approach for determining the Nash equilibrium of normal form games, utilizing a solution to a non-convex stochastic optimization problem. It defines the Nash equilibria in normal form games as the global minima of a specifically cunstructed loss function. Moreover, a randomized algorithm is developed to resolve this newly proposed loss function. Finally, empirical results further verify the theoretical analysis.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Though the idea of loss function has been proposed before, this paper contributes to the discourse with several innovative insights that enhance the understanding and applicability of loss functions. For example, this paper restricts the parameter to the simplex, which is the key of making stochastic gradient unbiased.

Regarding the quality and clarity, this paper is sufficiently complete. It also provides clear backgrounds, which make it easy to understand how this loss function comes from. It is not completely new but it has something new. 

Weaknesses:
The motivation of this work is not sufficiently clear. I could understand solving a Nash equilibrium may not be efficient but I don't think proposing a NE solver via unbiased stochastic optimization will make it better. 

It is unclear how this method is better than some existing NE solver such as Lemke–Howson algorithm.

Limitations:
This is a theoretical work so there is no negative impact.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This work studies the computation of Nash equilibria (NE) of normal-form games and proposes a new loss function: the (weighted) sum of the squared norms of the projections of each player gradient onto the tangent space of the unit simplex. The authors show that this loss function is a meaningful surrogate of exploitability when the game has an interior equilibria. Then, the authors provide methods to efficiently construct unbiased estimators of the loss function via unbiased estimation of each player gradients. To extend these results to handle games with only pure equilibria, the authors propose surrogate player utility functions via entropy regularization (with coefficient $\tau$, the ""temperature"") and show how the modified loss function (based on the modified game with surrogate player utility functions) captures the exploitability of the original game. Next, the authors derive gradient and Hessian expressions for the modified loss function. Leveraging a recent bandit optimization method BLiN, and assuming a sufficiently large temperature (which degrades the convergence rate), the authors provide a high-probability convergence guarantee for computing NE using this approach (loss function + BLiN). Experiments on SGD and BLiN show the effectiveness of the proposed approaches.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Novel observation of the connection between projection of player gradient to simplex tangent space and best response, which lead to the loss function proposed in this paper.
- Extensive studies of the newly proposed loss function in terms of its gradient, Hessian, and other properties.

Weaknesses:
Some technical details seem to require further clarification. See **Questions**.

Limitations:
This is a methodological work that does not have immediate or potential negative societal impact. The limitations are on the technical contributions and are discussed above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work studies solving Nash Equilibria (NE) by stochastic unbiased optimization. The main contribution is providing a new loss function based on the gradient norm of the utility function, and finding the NE by using standard stochastic optimization methods (like Lipschitz bandit algorithms and stochastic gradient descent (SGD)). The authors also carried our experiment results on several games to show the scalability of their proposed methods. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The presentation of this work is very clear. The experiment results are comprehensive and back up the main claims of this work. The results are also significant as they point out a new way to solve the NE problem in general. 

Weaknesses:
More remarks are supposed to be added to the main text. For example, 

- What does 's' mean in the legend of Figure 3? 

- In Table 1, why the obstacle of NI method is 'max of random variable'? I did not see any max operator in the definition of the loss function of NI. 

- In Table 1, for the unconstrained method, can the authors show one concrete example to show why it 'lose the ability to sample from strategies when iterates are no longer proper distributions', as stated in line 113? 


Limitations:
This work aims to solve an open problem about the algorithmic game theory, thus it does not need to address the potential negative societal impacts of their work. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes a loss function (optimization problem) for normal-form games to estimate Nash equilibria which can be solved via unbiased stochastic optimization. They do this by relating their proposed loss function with exploitability. They also provide theoretical guarantees (under some technical conditions) of using bandit stochastic gradient algorithms to solve their proposed problem. They show the applicability of their method by conducting some numerical experiments.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper tackles an important problem in the game theory of estimating Nash equilibria using optimization. It proposes a potentially scalable solution and provides theoretical guarantees for the same. The paper is well-written (albeit a bit notation-heavy) and the content is easy to follow. 

Weaknesses:
The authors propose an optimization problem with possible unbiased estimators. However, the proposed problem is still non-convex and it is not clear to me whether it can be solved efficiently with SGD with a potentially large number of saddle points. I understand the analogy to deep learning problems but recent work ([1] and related papers) have shown that those problems carry some interesting structure. Similar properties are unknown (and are perhaps more difficult to establish) for the proposed function.

Du, S., Lee, J., Li, H., Wang, L., & Zhai, X. (2019, May). Gradient descent finds global minima of deep neural networks. In International conference on machine learning (pp. 1675-1685).

Limitations:
The limitations are addressed adequately by the authors.

Rating:
5

Confidence:
3

";0
BT03V9Re9a;"REVIEW 
Summary:
This paper presents a distillation technique called EmbedDistill for Information Retrieval, aiming to align the pooled representations of various IR models. The focus of this study revolves around two distillation paradigms: DE-to-DE distillation and CE-to-DE distillation. The former involves straightforward embedding matching, while the latter utilizes special tokens for alignment.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. Embedding matching is a practical method for distillation. This work pioneers the application of feature mapping to the distillation of Information Retrieval (IR) models, contributing to the existing body of knowledge in this area.
2. The experimental findings presented in this study reveal certain positive outcomes when applying EmbedDistill to IR models.


Weaknesses:
1. The concept of embedding matching lacks novelty and has been extensively explored in prior studies [1, 2, 3]. Pooling techniques are also commonly employed to handle the matching of intermediate features.
2. The theoretical proof, while complex, does not significantly contribute to understanding the proposed method, as it is essentially a standard feature mapping approach already described in previous works.
3. There appears to be no direct correlation between the term ""Geometric"" and the proposed method. If a straightforward matching technique can be considered a geometric algorithm, it implies that most feature-based KD methods could also be classified as geometric. 

[1] Jiao, Xiaoqi, et al. ""Tinybert: Distilling bert for natural language understanding."" arXiv preprint arXiv:1909.10351 (2019).  
[2] Romero, Adriana, et al. ""Fitnets: Hints for thin deep nets."" arXiv preprint arXiv:1412.6550 (2014).  
[3] Hofstätter, Sebastian, et al. ""Improving efficient neural ranking models with cross-architecture knowledge distillation."" arXiv preprint arXiv:2010.02666 (2020).  
	

Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper proposes EmbedDistill, a novel distillation approach for IR that directly aligns the embeddings from the student and teacher. The authors conduct a theoretical analysis of the teacher-student generalization gap, strengthening the importance of embedding alignment and suggesting the use of asymmetric encoders. The architecture can be directly applied in DE to DE distillation. For CE to DE distillation, the authors propose dual-pooling to obtain document and query embeddings from CE.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:


- The authors show theoretically that embedding matching can close the teacher-student generalization gap in IR setting, and support the use of asymmetric conﬁgurations.

Weaknesses:


- The authors claim that EmbedDistill can be used for CE to DE distillation, but the empirical results show poor performance. It only improves the score distillation method from 33.3 to 33.7 (+0.4), while the teacher score is 37.0. This raises doubts about the effectiveness of EmbedDistill in CE to DE distillation. EmbedDistill appears to only work in DE to DE distillation, which is an easier scenario where embedding matching is trivial. This significantly weakens the contribution of this work, as EmbedDistill can’t learn from CE, the stronger architecture.
    - One possible reason for this is that the embeddings extracted from a dual-pooled teacher may not be good. Since the query and document embeddings are concatenated in the re-ranker, the model can simply make the representations of the two close to each other when the query and document are relevant, without properly distributing the representations as required in the DE retrieval scenario. This poses a major challenge for embedding matching in CE to DE distillation, and the authors do not seem to address it adequately.
- There are errors and inconsistencies in the formulas presented in the paper and the appendix. For instance, in line 130, $R\left(s^{\mathrm{s}}, s^{\mathrm{t}} ; \mathcal{S}_n\right)$ is not defined (should it be the empirical risk?). In the proof of Lemma C.3, the authors use $K_Q$ and $K_D$ in Eq. (25) but use $K$ elsewhere. The last line of Eq. (26) is incorrect; it should be $=\mathbb{E}\left[\left(1-y-\left(1-\sigma\left(F(q)^{\top} G(d)\right)\right)\right) f(q)^{\top} g(d)\right]$. The authors should thoroughly check all formulas and correct any errors.
- The document needs improvement in terms of writing quality.
    - Section 5, which covers the result analysis, is difficult to follow.
        - The authors use too many parentheses to add additional explanations, which interrupts the reading flow.
        - The captions of the tables contain too much information.
        - The teacher performance is presented in the content of the table of NQ test and BEIR, but is described in the caption of the table of NQ dev and MS MARCO dev, which can confuse the reader.
    - The author uses too much italics in the introduction, which obscures the key points of the paper.

Limitations:
As pointed out in the Weaknesses, EmbedDistill does not seem to work in CE to DE distillation, which limits the contribution.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to learn dense neural IR models by distilling not only the teacher scores but also by aligning the learned representations.

The paper has a theoretic part that motivate the actual embedding distillation with two propositions. The first one shows that the teacher-student gap is bounded by an expression where some terms are the difference between teacher/student embeddings. The second bounds one of term of the expression by leveraging the $u$-covering number of function classes for teacher and student.

Experiments show that using a regularization based on embeddings improve results in two settings: dense to dense and cross-encoder to dense distillation. In the latter case, the authors propose a simple but original approach to get a document and query representation.

The experiments are conducted on the Natural Questions and MS-Marco datasets. 



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This modification of the distillation procedure can easily be applied to all dense model, so the impact of the paper might be important in changing the mainstream procedure.

There is also a nice result showing that just using embedding alignment (without the other part of the distillation loss) words pretty well.

The mathematics justifying the approach are also interesting, by integrating the distance between teacher and student embeddings in the picture, but the bound do not seem to be of practical interest (apart from motivating the approach) and thus might not need to be that central in the paper (esp. proposition 3.2).

Weaknesses:
In the experiments, the training based on query generation should not be the last model modification – it should rather the query/document embedding part, since query generation is not the core method proposed by the authors. This would also allow to see what exactly is brought by aligning the embeddings.

The cross-encoder based distillation (which is the main one used by dense models) is quite disappointing in term of performance (even the best CE model performs worse than the best dense model, which should not be the case)… and suggests that to properly train a dense model with their method, one needs a very well trained dense models (but still, distilling to a lower-capacity model is interesting in that case, although the difference between the two procedures is not discussed or experimented with.

The discussion 196-206 is not really needed (it is quite obvious that aligning with the `[CLS]` does not make sense). 

Limitations:
The limitation section is quite generic apart from the first sentence that could be more discussed. Also, the fact the CE distillation is not that performant is (at least for me) a major limitation (since cross-encoders are so much powerful), but it is also a challenge for this type of method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposed to use embedding matching task for knowledge distillation for IR: unlike in traditional knowledge distillation where the matching score of query and document is used, the embedding matching tasks try to align the embedding representation of query and document. It works both for dual-encoder model (DE) and cross-encoder model (CE). The student model learned achieves 1/10th of size while retraining 95-97% of the teacher performance.

This work presented a theoretical analysis of the teacher-student generalization gap for IR settings. The student DE model proposed has an asymmetric configuration where the query encoder is smaller than document encoder, and the later is frozen during knowledge distillation. This could reduces inference latency at query time.

To validate the effective of the proposed method, experiments on common benchmark for IR is used. The model achieves competitive results on Natural Questions, MSMARCO and BEIR (zero-shot IR benchmark).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* In-depth theoretical analysis of the teacher-student generalization gap in IR models, which inspired the embedding alignment methods.
* Extensive experimental results to validate the proposed method: it achieves competitive results on both Natural Questions, MSMARCO datasets and also the BAIR datasets in zero-shot cases. 
* Overall well written and easy to follow.


Weaknesses:
Overall, the technical contribution presented in this work isn’t super strong in my opinion. Knowledge Distillation is known to be effective for IR, both for DE and CE models. The embedding alignment techniques has also be explored in other settings. Although specific technical challenges needed to be addressed in applying embedding alignment for knowledge distillation of IR models, it does not seem to be groundbreaking.



Limitations:
The authors addressed the limitations of the work where it only studied transformer-based models (but expect it to work for other models such as MLP). It only studied IR in text and did not extend to multi-modal cases. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The present study introduces a new technique, EmbedDistill, designed to transfer knowledge from large-scale neural networks to smaller versions for information retrieval (IR) purposes. This approach utilizes the relative geometric connections between queries and documents, as learned by the more extensive teacher model, to synchronize the representations of both teacher and student models. Furthermore, it scrutinizes the data manifold to diminish disparities between the student and teacher, particularly in areas where the training data is scant. The findings suggest that this proposed method can effectively distill both dual-encoder and cross-encoder teacher models into 1/10th size asymmetric students, maintaining 95-97% of the original teacher's performance. The study's key contributions consist of a groundbreaking geometric distillation technique, a unique query creation method for enhancing distillation quality, and practical results underlining the success of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper presents a new approach to distilling knowledge from strong neural models to weaker ones for information retrieval. The paper makes several contributions that are noteworthy.

- The paper's approach is original in that it leverages the relative geometry among queries and documents learned by the large teacher model to align the representations of the teacher and student models. 
- The paper is of good quality, with a well-defined problem statement, clear methodology, and adequate experimental evaluation. The paper's contributions are remarkable in that they provide a new approach to distilling knowledge from strong neural models to weaker ones for information retrieval. The authors provide a detailed analysis of the proposed approach and compare it with existing methods. The experiments are well-designed and the results are presented in a clear and concise manner.
- The paper is well-written, well-organized, and easy to follow. The authors provide clear explanations of the proposed approach and the experimental setup.


Weaknesses:
Although there are several distillation methods proposed in this paper, the core idea of this paper is to distill representation, via L2 distance, from a strong representation model to smaller ones. And the derivation of the representation is also straightforward and brute-forcing, i.e., CLS representation or mean pooling over corresponding text. 

The backbone of the retriever is too weak, which cannot verify the generality of the proposed methodology. That is, it’s required a stronger student model to check if the validity of proposed methodology will be mitigated with stronger students. Meantime, the neural retrievers and rankers used in this work is relatively out-of-date. It’s recommended to leverage more state-of-the-art retrievers/rankers to make the experiments more convincing. 


Limitations:
Yes

Rating:
5

Confidence:
5

";0
uFpjPJMkv6;"REVIEW 
Summary:
This paper aims to achieve fair user modeling with limited sensitive attribute information and propose a general framework, FairLISA, which efficiently utilizes data with known and unknown sensitive attributes to facilitate fair model training. The authors also provide theoretical guarantees from a mutual information perspective. Extensive experiments are conducted to demonstrate the effectiveness of FairLISA in scenarios with different ratios of missing sensitive attributes.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
S1. The scenario of missing sensitive attributes in fair user modeling is both meaningful and worthy of investigation.

S2. The paper is well-written and easy to follow.

S3. The proposed FairLISA efficiently leverages unknown data without the need for predicting missing attributes, providing a simple yet effective approach, supported by theoretical guarantees.

S4. Extensive experiments, especially the RQ2 experiment on different missing ratio situations, demonstrate the effectiveness and robustness of FairLISA in two representative user modeling tasks.

Weaknesses:
W1. This work mainly focuses on the fairness definition where the mutual information between the user modeling result and the sensitive information is zero. However, there are other classic fairness definitions, such as Equalized Odds (EO) and Demographic Parity (DP). It would be valuable to discuss investigate how FairLISA performs on these metrics.

W2. The sensitive information ratio setting is missing in Table 1.

Limitations:
Although this paper addresses the issue of limited sensitive attribute information, it still requires the collection of some sensitive information as model input, which can potentially compromise privacy. Exploring the combination of fairness and privacy would be a promising direction for future research.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors investigate the problem of fair user modeling in a setting with limited sensitive attributes. Due to the lack of such attribute information, they propose a general framework called FairLISA, which efficiently applies unlabeled data to facilitate fair model training. Compared to previous works, FairLISA can directly leverage unlabeled data without the need for predicting missing attributes, thereby reducing information loss caused by predictions. The experiments demonstrate the effectiveness of the proposed model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors tackle a socially valuable problem of fair user modeling in a setting with limited sensitive attributes, which holds significant implications across various applications, such as recommendation systems and cognitive diagnostics. 

The paper provides a thorough and insightful summary of related works on fairness without sensitive attributes and fairness in limited sensitive attribute situations. Building upon existing research, this paper effectively identifies and summarizes three key challenges: ""Efficient data utilization,"" ""Theoretical guarantee,"" and ""Framework generalization."" The authors propose reasonable solutions to address these challenges. As far as I am concerned, the insights of directly leveraging unlabeled data without predicting missing attributes are straightforward and effective. 

The authors substantiate their claims with theoretical guarantees through Lemma 1 and Lemma 2. Finally, comprehensive experiments are conducted on two representative tasks, highlighting the superiority of FairLISA. In summary, this work exhibits reasonable motivation, a clear literature review, a look-nice model design, and a comprehensive validation experiments.

Weaknesses:
There have been existing works that focus on the complete absence of sensitive attributes, such as [1][2]. Although the specific problems and input data may differ, it would be valuable to explore whether FairLISA can be combined with these works to extend their potential in limited scenarios. This article lacks discussion in these two aspects.

[1]Tianxiang Zhao, Enyan Dai, Kai Shu, and Suhang Wang. Towards fair classifiers without sensitive attributes: Exploring biases in related features. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, 2022

[2] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In International Conference on Machine Learning, 2018.

Limitations:
As mentioned in the Border Impact section, FairLISA is effective only in limited scenarios. It is recommended to further explore the potential of FairLISA in completely unsupervised settings.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper propose FairLISA that learns fair user modeling using limited sensitive attribute information. Specifically, for users with known sensitive attribute information, FairLISA maximizes the cross-entropy of predicting the sensitive attribute using user representations; for users with unknown sensitive attribute, FairLISA maximizes the entropy of predicting sensitive attribute using user representations. Experiments on benchmark datasets demonstrate the efficacy of the proposed model.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. Studying limited sensitive attribute scenario is practical and important.

S2. FairLISA is supported by the theoretical analysis.

S3. FairLISA's performance is good based on the empirical evaluation.


Weaknesses:
Please see limitations.


Limitations:
L1. The authors may better illustrate why statistical parity is important in recommender system. To my understanding, many recommendation tasks is related to sensitive attribute as well. For example, a recommender system don't want to recommend feminine care items to male users.

L2. Using user modeling as a motivating example is ok to me, but I don't understand why the authors position the paper to fair user modeling specifically. How does the proposed method connect to user modeling? What is the uniqueness of user modeling (other than limited sensitive attribute) in terms of fair user embedding learning? How does <user, item, relation> triplet useful here?

L3. The theoretical analysis is based on the fact that discriminator is Bayesian optimal, which is often impossible. Thus, the practicability of the theoretical analysis needs more justification. 

L4. What if the discriminator is not optimal? Does the theoretical analysis still hold in this case?

L5. Some intuition about the theoretical analysis would strengthen the intuition. For example, how does optimizing the entropy of the discriminator's output help with removing sensitive information. My guess is that it tries to make the prediction probability of sensitive attribute to be uniform. But the authors may clarify it better.

L6. In Figure 2, why do all methods other than FairLISA converge to the same point? And it seems FairLISA converge to the same point when the ratio increases to 95% as well. Some discussion is needed.

L7. FairGNN is developed in the setting of binary sensitive attribute and binary classification, where the covariance regularizer minimize the pseudo-sensitive attribute and the output logit scores. How do the authors extend it to non-binary sensitive attribute settings?

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel adversarial learning method for fairness with limited demographics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Pros:
1. This paper focus on an important and practical problem. Fairness with limited demographics is a practical and important problem.
2. This paper provides a theoretical-driven perspective on fairness with limited demographics.
3. This paper proposed a novel adversarial learning method for fairness with limited demographics.
4. Solid experiments are conducted to demonstrate the effectiveness of the proposed method.


Weaknesses:
Cons:
1. The datasets used in this paper seem not very common. So it is a little bit hard to evaluate the effectiveness of the proposed method. It is suggested that authors also conduct experiments on commonly used fairness datasets such as ADULT and COMPAS.
2. Baselines seem insufficient. It is suggested that the authors may consider adding more in-processing methods as baselines.
3. This paper only empirically study the effectiveness of the proposed method. It is unknown whether or not the proposed method is theoretically better than the previous estimation-based method [1]. It is suggested the authors could add some theoretical analysis to demonstrate that the proposed method is better than the previous estimation method.
4. The authors are encouraged to provide code for reproduction.
5. Besides [1], the authors may consider citing some related references on fairness with limited exact demographics. Reference [2] studies the problem of fairness with limited clean sensitive attributes and mostly private sensitive attributes. Reference [3] studies the problem of fairness with active sensitive attribute annotation. 

[1] Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information. https://arxiv.org/abs/2009.01454

[2] When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes https://arxiv.org/abs/2207.08336
 
[3] Mitigating Algorithmic Bias with Limited Annotations https://arxiv.org/abs/2207.10018


Limitations:
This paper seems to not discuss the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposed an algorithm to train fair user modeling (e.g recommender systems) when given limited sensitive attributes. The idea is to factorize out the effect of sensitive attributes from the model's fair training objectives, and isolate its impact in training.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Studied an important problem in practical fair recommender system

Weaknesses:
1. The proposed algorithm only applies to generative-based user modeling, which is limited. Many user modeling methods are prediction-based.
2. The high-level idea of decomposing the mutual information between prediction/embedding and sensitive attribute into sensitive-attribute-related term and non-sensitive-attribute-related term is common in the fairness literature. It seems limited novelty in the method.
3. It seems the method still requires some samples with sensitive attributes labeled (the minimum in experiments is 20%). It would be good to clarify how practitioners can obtain those sensitive attributes. Because if the concern is about privacy, then getting 20% samples with sensitive attributes is as hard as getting 100%. In the literature, usually, the assumption is having some aggregated form of sensitive attributes rather than sample-level.
4. A simple baseline is to train a classifier on samples with sensitive attributes and use it to label other data. This should be tested in experiments.
5. Many grammatical errors and typos.

Limitations:
See Weaknesses.

Rating:
4

Confidence:
4

";1
ZqSx5vXOgC;"REVIEW 
Summary:
The paper presents a new preprocessing method for training shallow overparametrized sparse neural networks. It significantly improves the preprocessing time yet achieves same performance on query time. They also show that their algorithm is very close to optimal.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. Clearly written. Easy to understand. Well structured.
2. Story is rather complete. Lower bound is included showing that their result is close to optimal.
3. Connect ideas from applied algorithms (LSH etc.) to deep learning, insightful

Weaknesses:
1. Not all assumptions appear in the statement of theorem 1.1. Expect a more detailed description of the sparsity assumption.

Limitations:
1. The setting is limited. Overparametrization is currently largely a theoretical assumption for the convenience of proving things, rarely used in practice. Shallow network is also too limited for characterizing deep learning. Also neural networks are commonly run on parallel machines like GPU with some complications for sparsity processing, so it's hard to say whether data structures would be really helpful. But it's very common, due to the nature of things, to have these limitations in a theoretical work, so it's not a serious flaw. Maybe it's better to design the algorithm for a general problem and put deep learning as a possible use case.

They have addressed the limitation of their work in terms of shallowness.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper analyzes a specific neural network setting: two-layer neural networks with $m$ neurons in the hidden layer and a ReLU activation. Given input data of dimension $d$ and $n$ training examples, it normally requires O(mnd) operations to compute the hidden activations. This paper follows prior work in showing that this can be asymptotically reduced to $O(m^{4/5}n^2d)$, which is better in the overparameterized (large m) regime. Improving on the previous work, the proposed algorithm requires polynomial instead of exponential time preprocessing. The core technical contribution is the Correlation Tree data structure, which is a collection of binary trees that store inner products between datapoints and neurons, and allows efficiently updating based on the sparsity of activated neurons.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The paper analyzes an interesting theoretical setting, and provides an original data structure and algorithm for this problem of efficient computation. The main result that training with sublinear cost per iteration is novel.
- The paper is well-structured and written well. The problem setting is clear and the contributions are clearly decribed. Overall it is high quality.


Weaknesses:
The main weakness of this submission is the empirical practicality of the method. The authors are transparent about these weaknesses, and it is not the intended focus of this direction of research, so I think this weakness does not significantly detract from the paper. It might be interesting to comment more on the technical challenges behind extending it to more complex settings (e.g. beyond 2 layer, or other activation functions).

Limitations:
The authors discuss most of the prominent limitations, but I think it is not completely clear how practical the algorithm is. It seems like it should be relatively straightforward to implement, which could strengthen the paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In the paper, the authors proposed fast optimization algorithm for over-parameterized two-layer networks. They proved that by using the sparsity firing feature from the neural network, the proposed method requires only O(nmd) time in preprocessing and still achieves o(nmd) time per iteration.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. A thorough theoretical analysis is provided and prove the statement of the paper.

Weaknesses:
1. The proposed method require O(nm) space for storing, which is a lot when n and m is big. 
2. No experimental results shown in the paper. A toy experiment can show empirical impact of this work.

Limitations:
1. The space required by the proposed method is high. 
2. There are few works training a two-layer network and showed competitive performance with deep NN. It is non-trivial to simplify the problem and prove the convergence of NN. However, why we need to do training based on an un-empirical structure? I hope at least a tory experiment should be provided to show that this training method is viable in deep NN.

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper investigate the efficient training methods than the usual training protocol which requires the complexity $O(nmd)$ for 2-Layer ReLU networks. The authors improve the complexity in the previous study [SYZ21] by proposing the preprocessing method utilizing the tree data structure for both data and weights. Moreover, the authors successfully provide the upper bound/lower bound (for lb, the authors assume some conjecture) for their proposed preprocessing complexity. Unlike in previous study [SYZ21], this paper is the first presentation for the lower bound.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The authors efficiently improve the preprocessing time from exponential complexity $O(2^d)$ (in previous research [SYZ21]) to polynomial $O(nmd)$ (in this paper) for both data and weight parameters using the tree data structures which is popular in general computer science.

2. For the tree data structure preprocessing, both the upper bound/lower bound are provided in a solid theory under the NTK regime and the theory basically depends on the previous study [SYZ21].

3. Unlike in previous study [SYZ21], the authors also provide the lower bound for their proposed method (although assume some conjecture), so the tree-based preprocessing method is nearly optimal.

Weaknesses:
Actually, I'm not an expert in this field, but I have carefully read this paper along with the previous study [SYZ21].

Here are my main concerns:

1. Based on my understanding, the authors have successfully reduced the complexity of preprocessing from $O(2^d)$ to $O(nmd)$ and from $O(n^d)$ to $O(nmd)$. However, it seems that the per-iteration time in this paper (for example, in Theorem 4.1, the time per-iteration is $O(m^{4/5}n^2d)$, but it is $O(m^{4/5} n d)$ in previous study [SYZ21]) has actually increased compared to the previous study [SYZ21]. In fact, if preprocessing time constitutes a significant portion of the neural network's training process, this research would have more significance. Therefore, it seems that an (empirical) analysis of the portion that preprocessing takes in neural network training, in computational terms, is necessary.

2. As this study presents more advanced preprocessing techniques compared to the previous research [SYZ21], there should be experimental analysis on how much the actual preprocessing time is reduced and its impact on per-iteration time (under quite theoretical settings or even for synthetic data/architecture). This analysis seems to be necessary, even for simple neural network models and simple datasets, to further validate the improvements made by the proposed preprocessing methods.

3. In the theoretical perspective, it is necessary to provide remarks on what is the challenging points of the theory in this paper compared to the previous study [SYZ21]. When I was examining the supplementary material, it is not clear which aspects have significantly changed compared to the theoretical analysis in the previous study [SYZ21].

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
2

";1
TwLHB8sKme;"REVIEW 
Summary:
This paper proposes a protocol for verifying that a model trainer submits data and learned weights, and a verifier checks whether the weights are correctly learned from the submitted data.
Such a protocol is useful for trustworthy AI. The paper defines the problem of Proof-of-Trainind-Data (PoTD). It is inspired by the previous work on ""Proof-of-Learning,"" but the setting of PoTD requires more. The paper gives a formal definition of PoTD protocol and argues that the protocol needs to satisfy some conditions to achieve the guarantee in a practical setting.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:

The motivation of the paper is interesting. The proposed PoTD protocol seems reasonable. 

Weaknesses:

1. The paper does not explain the motivation why we need the PoTD protocol and its impact. The paper says that some attacks can be treated by existing Proof-of-Learning methods. It seems trivial since  PoTD poses stronger requirements, as written in the paper. The paper should discuss the problems of existing PoL methods and how the proposed protocol can solve them.

2. The only attack that the existing method cannot deal with but the proposed method can is a data subtraction attack. How important to deal with such an attack is not explained enough.
Moreover, experimental results show how the proposed method performs but do not compare with baseline methods. Therefore, experiments are not enough to
show the effectiveness of the proposed methods. 

3. The presentation of the paper is hard to follow. The paper seems to consist of fragments of texts relationships between them are unclear. 
For example, There is a formal definition of PoTD protocol in section 2, but the definition is never mentioned in the subsequent sections. Therefore, it is hard to judge whether the definition is reasonable or not. 
The memorization heuristic introduced in Section 3.2 seems overly complex. If we use $-L(d, W)$ instead of M(d, W) in (3), then the values of PBQ and FBQ would not change since
the first term of (2) does not depend on data d and has no effect when evaluating $\Delta_M(d^\prime, W) >  \Delta_M(d, W)$.  



Limitations:
The paper mentions to limitations in the end of the paper.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors propose Proof of Training Data (PoTD), a variant of Proof-of-Learning (PoL) protocols that focuses on training set attacks, rather than the training algorithm itself. A valid PoTD protocol should be able to, at least in theory, spot when a machine learning model has been trained on a different training set than the one declared by the learner. As in PoL, the learner is required to provide a full transcript of the training process, including training data, code and intermediate checkpoints. Unfortunately, the task of verifying a training transcript is as computationally intensive as re-training the model from scratch. For this reason, the authors propose several heuristic strategies for PoTD, which rely on the fact that stochastic gradient ascent tends to first memorize and then forget the data it observes in each batch.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
From the methodology standpoint, I truly appreciate the memorization heuristics proposed in the papers. In particular, they seems to be able to defend against a large number of different threat models.

Weaknesses:
The PoTD protocol proposed by the authors have considerable overlap with the existing PoL proposals. I am inclined to see it as a variant of these existing efforts, rather than a novel, independent idea.

Furthermore, the defenses proposed in the paper are heuristics and have been tested on large language models only. The authors mention that their techniques may work differently on other neural architectures, learning tasks and training procedures. Therefore, I believe more experimentation is needed to confirm their usefulness.

Additionally, the PoTD protocol requires the learner to disclose their complete learning process. Thus, there is no way to protect the intellectual property of the learner. The authors are honest about this limitation, and claim it will be addressed as future work. However, I feel this makes the paper weaker.

The data addition attack presented in lines 256-267 seems the most interesting scenario to me. It is unfortunate that the heuristic technique proposed by the authors cannot defend against it.

The writing style of the paper could be improved. I report here some specific examples.

Line 6, ""and flag if the model specific harmful or beneficial data sources"" is not a syntactically-correct English sentence. 

Line 30, the authors start discussing how to solve proof-of-training-data
before giving a precise enough definition that the reader can follow.

Line 87, c2 is missing from the definition of V (compare with Line 82). Also, why is the probability taken over c1, when c1 does not appear in any of the terms?

Line 172-173, please number all equations.

Section 3.3 is very dense and many important details are left for future work.

Footnotes 4-9 occupy almost a quarter of the page and contain important information. I would prefer having them merged with the main text.

Line 225, ""on trained"" should be ""trained on"".

Section 5 introduces new concepts, new attacks, new defenses, new notation. Since this happens so late in the paper, it ends up being a bit overwhelming. Why not explicitly organise the whole paper by threat model?

Limitations:
I commend the authors for beign upfront about the limitations of their work. All my concerns have been listed above.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper presents a novel protocol called Proof-of-Training-Data, which a third party auditor can verify the data used to train a model. Here, the auditor will require training data, training code, and intermediate checkpoints. Experiments on two language models have demonstrated that known attacks from the Proof-of-Learning literature can be caught by this new protocol.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This paper attempts to tackle an important security problem on trained neural network models. The proposed heuristics (i.e., memorization-based tests) is appealing and can efficiently catch spoofed checkpoints using a small amount of data. The paper is adequately structured and solid experiments have been carried out to empirically justify the effectiveness of the proposed protocol.

Weaknesses:
The concept of Proof-of-Learning has been well studied. Although the authors have discussed various Proof-of-Learning literature in the related work and the experiments section, it is still not immediately clear to me why we need this brand-new protocol (Proof-of-Training-Data). If I understand correctly, the authors are attempting to solve an even harder problem where the adversaries can have more computing power. 

Limitations:
The authors have adequately addressed the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper describes techniques and tools that can be used for verifying the ""provenance"" of large neural models, to evaluate their risks. These techniques and tools are part of ""protocols"" used by a model trainer to convince a ""verifier"" that the training data was used to produce the model parameters. The authors show experimentally that their prescribed procedures can catch a variety of known attacks from the ""proof-of-learning"" literature.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The paper addresses an increasingly important problem, as large neural models are becoming very popular, and advances practical techniques that can be used by regulators to check the provenance of large models.

* The authors present convincing evaluation using GPT-2, demonstrating that the proposed procedures are effective in catching a variety of attacks, such as glue-ing and interpolation as well as data addition and subtraction.

Weaknesses:
* I find that the use of ""proofs"" in the title and throughout the paper is misleading as the authors do not present techniques that amount to an actual proof.

* I think it is great that the techniques presented by the authors are practical and effective wrt several attacks but I wonder if this is enough for regulators. I mean they would possibly need stronger guarantees for such techniques. 

* It is unclear to me how the verification strategies presented in section 3 relate to definition 1. The authors should work on adding a theorem that clearly states that their strategies achieve the desired properties, i.e., the verifier accepts/rejects true winesses/spoofs with the desired probabilities.

* The technical contribution beyond ""proof-of-learning"" is unclear.

Limitations:
In conclusion this is very interesting work but may be too preliminary for publication.

Rating:
5

Confidence:
2

";1
qO9VagA7kF;"REVIEW 
Summary:
This paper conducts a comprehensive analysis of various quantization methods for large language models (LLMs). Some interesting takeaways were shared, for example, activation quantization is generally more susceptible to weight quantization; none of the current quantization methods can achieve the original model quality. Based on such insights, the paper also proposes an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The motivation of the paper is solid. With the rapid development of LLM, it is essential to study the methodology to deploy LLM on more accessible hardware, where quantization is an important category of the approach. Therefore, a comprehensive study of these methods is necessary.  

- The insights shared by this paper are helpful, e.g., it is interesting to know activation quantization is generally more susceptible to weight quantization.


- The proposed improvement based on the observation is reasonable. 



Weaknesses:
- The proposed method is based on low-rank approximation, which can be viewed as a sparsification-based method. It is kind of out of the scope of the proposed method. 


Limitations:
Not applicable. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper analyzes post-training quantization (PTQ) techniques in large language models, exploring various schemes, model families, and bit precision. The authors propose an optimized method called Low-Rank Compensation (LoRC) to enhance model quality recovery with minimal size increase.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. An evaluation and comparison of existing PTQ methods provide some insights for the community.
2. The paper is well-written.

Weaknesses:
Reading from paper provides a satisfying experience, but I must admit that my understanding of the LLM field is limited. Thus, my suggestions may be wrong, please directly point them out.

1. The points in Figure 1 are too dense and lack recognition.
2. Although the method is proposed for LLMs, it's also better to compare it with some traditional quantization methods on ResNet-series.

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper studied the post-training quantization method for 4-bit weight quantization and W4A4 quantization. The authors further proposed a Low-Rank Compensation (LoRC), to enhance model quality with low-rank matrices.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper is well-written and easy to follow.

Weaknesses:
The novelty is not very significant. The post-training quantization with finer-grained and zero-shift is not a new idea.

The sensitivity analysis is conducted with one particular quantization method, and the conclusion should be conditioned on that quantization method, but not general enough to conclude that PTQ exhibits the same behavior. I would suggest the authors to compare different quantization functions, such as minmax/percentile to deliver a more comprehensive conclusion.

The accuracy improvement is marginal. Figure 1 didn’t show much improvement compared to the previous naïve baseline of RTN. 


Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work focuses on systematic examination of various post training quantization techniques in large language models. Experimental analysis include comparison of different model sizes, different numerical precision, and quantization of only weights vs activations. In addition, the Low Rank Compensation method is proposed to enhance model quality recovery.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The research on LLMs is rapidly growing, but computational and memory capabilities are still limited when deploying huge models. Therefore, quantization is a must. The analysis presented in this work is definitely needed to advance LLMs deployment in multiple use cases. The paper flows logically and is well structured. The proposed compensation method is interesting.

Weaknesses:
1. Quantization may be input data specific. Same model topology when trained on different data may behave differently. It’s mentioned that you “use the zero-shot validation perplexity (PPL) differential on three datasets, namely, Wikitext-2 [23], PTB [22], and C4 [27], before and after the quantization”, however results presented in the following tables doesn’t indicate what error was achieved on which dataset. Could you clairfy?
2. Quantization may be operator specific. It would be interesting to list operators present in both model topologies and identify layers that were specifically sensitive to quantization.

Limitations:
Listed limitations and future work directions are clear and make sense.

Rating:
6

Confidence:
5

";0
G6yq9v8O0U;"REVIEW 
Summary:
This paper proposes a factorized tensor network (FTN), that adds task/domain-specific low-rank tensors to shared weights, to perform lightweight MTL/MDL. Specifically, the low-rank tensors can form a weight additive to the shared backbone weight so that each task can have its own parameters to achieve good accuracy.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is easy to follow.
- The basic idea of adding additional task-specific low-rank vectors is clear to understand.

Weaknesses:
- About the method:
    - My high-level concern about this idea is not truly an MTL/MDL work in my understanding.   
    If I understand the proposed method correctly, it is equivalent to fine-tuning a completely shared feature extractor on each task. After fine-tuning, we can get the weights offset $\Delta W = W_t - W$ between the fine-tuned weights $W_t$ for task $t$ and the original backbone weights $W$. The low-rank tensors are just a parameter-efficient way to store $\Delta W$ by three separate vectors.
    What I believe is that the proposed method is still a traditional transfer learning method with some low-rank storage design for the weight difference.

    - If it is an MTL work, it fails to obtain the two basic advantages of using MTL.
        - First, in MTL, we believe a good parameters-sharing may improve the generalization ability, i.e., the task accuracy, when training multiple tasks jointly. However, in this paper, the shared backbone weights will be frozen (as shown in Figure 1d) when training the additional low-rank tensors for each task, which means there is no truly joint training/parameter sharing across the multiple tasks. As I described in the first bullet point, each task is just fine-tuned from the shared backbone and has no communication with other tasks during training, and the final model weights for each task will be independent to each other at the end.
        - Second, the wide adaptation of MTL relies on its advantage of lower computation cost and latency. However, the proposed method in this paper has no latency saving since all the tasks still have their own parameters and have to execute sequentially even if they share the same input images.

    - About the method itself, I feel it lacks good motivation. There is no reason why we can have three low-rank tensors to get $\Delta W$ for each task in each layer. The authors may consider adding some literature that can support this choice, or have some ablation studies. For example, what will happen if we use the full-rank $\Delta W$ directly? How about using PCA or SVD on $\Delta W$ to get low-rank matrics?
    
    - More importantly, this paper has a very similar idea as [1]. The only difference is the choice of low-rank representation.    
    [1] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.

 - About the literature:
    - Missing papers and comparisons with adaptor-based MTL methods. Adaptor-based MTL is very close to this paper, in which task-specific adaptors are parameter-efficient and plug-in modules on top of an all-shared feature extractor. For examples,    
    [2] Rosenfeld, Amir, and John K. Tsotsos. ""Incremental learning through deep adaptation."" IEEE transactions on pattern analysis and machine intelligence 42.3 (2018): 651-663.     
    [3] Zhao, Hanbin, et al. ""What and where: Learn to plug adapters via nas for multidomain learning."" IEEE Transactions on Neural Networks and Learning Systems 33.11 (2021): 6532-6544.

    - Missing recent papers in MTL/MDL.    
    [4] Zhang, Lijun, et al. ""Rethinking hard-parameter sharing in multi-domain learning."" 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022.     
    [5] Zhang, Lijun, Xiao Liu, and Hui Guan. ""Automtl: A programming framework for automating efficient multi-task learning."" Advances in Neural Information Processing Systems 35 (2022): 34216-34228.
    
 - About the experiments:
    - The authors may want to compare with adaptor-based MTL methods, which are the most related works to this paper.
    - It's necessary to have more experiments for MTL, including more baselines, more backbone models, and more datasets. More baselines could be the representative and new methods like MTAN and AutoMTL, rather than just RA and ASTMT. Backbone models could be some efficient backbone like MobileNet or EfficientNet, rather than just ResNet variants. Datasets could be CityScapes and Taskonomy, rather than just NYUD.
    - AdaShare is a typical MTL model. It's weird to compare it in MDL setting only.

Limitations:
The social impact is not mentioned either in the main paper or the supplementary materials.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents the Factorized Tensor Network (FTN) as a solution to the challenge of learning multiple tasks/domains using a single unified network. The authors claim that FTN achieves comparable accuracy to independent single-task/domain networks with a smaller number of additional parameters. The method incorporates task/domain-specific low-rank tensor factors into a shared frozen backbone network. Experimental results on multi-domain and multi-task datasets demonstrate the effectiveness of FTN.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. `Efficient Approach for Knowledge Transfer`: The Factorized Tensor Network (FTN) proposed in the paper offers an effective mechanism for transferring knowledge across multiple tasks and domains. By incorporating task/domain-specific low-rank tensor factors into a shared frozen backbone network, FTN allows for efficient utilization of existing knowledge, reducing the need for extensive retraining or independent models.

2. `Minimal Parameter Requirement`: FTN demonstrates the ability to achieve comparable accuracy to independent single-task/domain networks while requiring a smaller number of additional parameters. This parameter efficiency is advantageous when dealing with a large number of tasks or domains, as it helps reduce storage costs and computational complexity, making the approach more scalable and practical.

3. `Strong Validation Results`: The paper presents experimental results on widely used multi-domain and multi-task datasets. These results showcase the effectiveness of FTN, demonstrating similar accuracy compared to single-task/domain methods while utilizing only a small percentage (2-6%) of additional parameters per task. The validation experiments provide empirical evidence supporting the proposed approach and its potential for practical application.

Weaknesses:
1. `Limited Novelty`: The paper falls short in highlighting the novelty and distinctiveness of FTN compared to other parameter-efficient tuning methods, especially KAdaptation [A] and SSF [B] . This missed opportunity leaves readers wondering about the specific advantages and unique contributions of FTN. A more comprehensive analysis of its novelty would have evoked excitement and clarified its standout features.

2. `Limited Comparison`: The paper fails to sufficiently contrast FTN with other parameter-efficient fine-tuning (PEFT) methods, such as adaptors and low-rank approximation (LORA). This omission leaves unanswered questions about FTN's comparative strengths and weaknesses. A thorough evaluation against these methods would have added depth and fostered confidence in the proposed approach. Additionally, the paper's baseline for multi-domain image translation appears relatively weak, limiting the scope of the evaluation. 

3. `Overlooking Recent Trends in Modular Learning`: The paper overlooks recent advancements in modular learning highlighted in references [C] and learning factorized knowledge with different modules [D]. 

[A] He X, Li C, Zhang P, et al. Parameter-Efficient Model Adaptation for Vision Transformers[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 817-825. 

[B] Lian D, Zhou D, Feng J, et al. Scaling & shifting your features: A new baseline for efficient model tuning[J]. Advances in Neural Information Processing Systems, 2022, 35: 109-123. 

[C] Pfeiffer J, Ruder S, Vulić I, et al. Modular deep learning[J]. arXiv preprint arXiv:2302.11529, 2023. 

[D] Yang X, Ye J, Wang X. Factorizing knowledge in neural networks[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 73-91.

Limitations:
The author should discuss some of the limitation of FTN in the paper

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles multi-task learning (MTL) and multi-domain learning (MDL). The author proposes to a general method to add task/domain-specific weights to the common shared backbone. Specifically, they add the factorized task/domain specific tensors which introduce very low storage cost for each task and domain while maintaining the original performance. They experiment in three different settings: multi-task learning with dense-prediction tasks, multi-domain learning for image classification and multi-domain learning for image generalization. They also ablate on the relationship between the rank of factorization and the accuracy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The paper is well written and easy to understand.

(2) The experiment section is well designed and they show the effectiveness of their proposed methods in three different scenarios including multi-task learning / multi-domain learning, deterministic and generative tasks. 

(3) The design of the proposed method is reasonable, easy to implement but effective to preserve the performance of each task/domain and save the storage cost. It scales well with more tasks and domains

Weaknesses:
The paper is based on the convolutional network and the author uses big gan in the generation tasks. These designs are somehow in the old fashion. Even though it is mentioned in the conclusion that the method can be extended to transformer based architecture, it is lack of experimental proof.

Limitations:
They have included limitation in the main paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles the problem of adapting a single pretrained network to multiple tasks or domains while introducing as little parameters as possible. More specifically, first a backbone model is trained which contains a shared backbone. Then,  the proposed method (Factorized Tensor Networks, **FTN**) is applied to further adapt the shared backbone to each task with a few added parameters. All task specific parameters are trained alongside each other (i.e. the BatchNorm parameter, the low-rank FTN parameters as well as the task heads)

Given a weight $W$ from the shared backbone, FTN adds low-rank parameters $\Delta W_t$ for each task/domain $t$ such that the forward pass for task $t$ uses the shifted weights $W + \Delta W_t$. Setting the rank $r$ of the low-rank parameters allow to control the trade-off between parameter efficiency and model capacity. In addition, FTN also tune batch norm layers parameters (scale and bias) for each task independently.

The proposed FTN is compared to previous methods on adapting weights during finetuning (e.g. residual adapters) on mainly three benchmarks:
- Multidomain classification from a pretrained ImageNet model to the 5 domains in ImageNet-to-Sketch
- Multidomain classification from a pretrained ImageNet model to the 6 domains in DomainNet
- Multitask from a pretrained ImageNet model to the 3 domains in NYU

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposed method is simple to implement and compared to other adapter methods it is extremely parameter efficient

* The method is experimented on both multi-domain (different inputs) and multi-task (different outputs) settings

* While the method is only evaluated on convolutional networks, there is no strong assumptions on the backbone architectures, hence it could also be used on transformers for instances 

Weaknesses:
- **Missing important related work**: As far as I can tell, the proposed FTN is quite similar to the low-rank finetuning method from ""LoRA: Low-Rank Adaptation of Large Language Models"". While LoRA is only evaluated on NLP/transformers in the original paper, it should be mentioned and discussed in related work

- **Unclear how the number of parameters is computed** The original $BA^2$ paper reports a parameter cost of ""1.03x"" in their Table 3. While this paper reports a cost of ""3.8x [1.71x]"". It seems like the baselines results reported in this paper are directly taken from the Table 1 in the  ""Task Adaptive Parameter Sharing for Multi-Task Learning"" paer:   It would be nice to clarify how baselines evaluation was conducted and where the discrepancy may come from

- **Missing results from Visual Decathlon**: Manyworks on adapters report results on Visual Decathlon introduced in the original residual adapters work where an ImageNet pretrained model is adapted to a wide variety of very different downstream tasks 

Limitations:
Limitations are explicitly mentioned in an independent paragraph and fairl ydescribe some drawbacks of the methods.
Although I partially disagree with the statement that *(line 73) ""The proposed method does not affect the computational cost because we need to compute features for each task/domain using separate functional pathways""*: This is fine for the MDL applications, however most MTL works operate under the ""feature -extractor"" paradigm instead, where the multiple tasks can be solved with a single forward pass on the fully shared backbones, followed by lightweight task heads: In comparison, FTN is much less efficient as it requires individual forward passes of the encoder for each task.

Rating:
6

Confidence:
3

";0
lENeWLXn4W;"REVIEW 
Summary:
This paper presents a novel hyperparameter tuning method in the presence of a privacy budget: linearly extrapolating from observations with very low privacy loss.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
The core technique presented here is certainly interesting and deserving of future study. The paper tackles an issue which is often unaddressed in the literature on training DP models: that of choosing hyperparameters subject to a privacy budget. This problem itself is also deserving of further study.

Weaknesses:
* A primarily empirical paper will live and die with the strength of its baselines (as well as its upper bounds in a case like this one where upper bounds on the efficacy of the technique can be computed). The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it.

* Some baseline issues: the citation problems with [51], [52] (detailed below). Lack of comparison to the 'naive baseline' of directly applying gaussian mechanism to results of grid search, say given known training statistics / optimal hparam values for nonprivate datasets (to avoid infinite regress, and here not so much of a problem since the experiments are all focused on public feature extractor settings). Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand).

* On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8\% at $\epsilon=1$ and 98.9 at $\epsilon=\infty$ (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1\% and 90.6\% at the $\epsilon=1, \infty$ for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\infty just by tuning hyperparameters (see figure 2). 

* The statements of timing on Imagenet seem wrong? The cited paper [51] seems to be pointing to a version from Nov 2022, clicking through to [52] seems to show a version uploaded in May 2022--so where are Jan 2023 and 'within the last month' coming from?

* Some more consideration required in decomposition of $r$--do we know that random decomposition 'is enough'? Presumably it's not, since we _can_ generate an $\eta$ for which the problem will presumably diverge?

Limitations:
Societal impact not immediately applicable.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a linear scaling rule for finding the optimal value of the learning rate and number of training steps for differentially private SGD (DP-SGD). The idea is simple, small amount of privacy budgets are allocated for two initial DP learning rate optimization procedures, and then the values are extrapolated to bigger epsilon-values using linear scaling (as a function of epsilon). The work is mostly experimental, and the experimental results e.g. with CIFAR-10 show that for epsilon between 0 and 1, the scaling rule seems to nicely fit the optimal values found by the grid search. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The idea seems very interesting and novel.
- The paper is mostly written well and is easily readable.


Weaknesses:
- The technique is restricted to optimising the learning rate and the length of the training. I wonder if similar extrapolation (perhaps more generally polynomial extrapolation) could be used to find optimal the optimal hyperparameter values for other hyperparameters.

- The technical part could be written more carefully. It remains unclear whether you use RDP or GDP. The hyperparameter tuning cost of the method by Papernot and Steinke is in terms of RDP, but you list theoretical results in terms of GDP. In the end of Alg. 1 you write that the total cost is ""$\varepsilon_f + \varepsilon_0 + \varepsilon_1$"". Is that approximate DP? In case you use the classical composition result where you just add up the privacy parameters, what happens to the $\delta$-parameters?

- Some conclusions are a vaguely formulated/confusing. On p. 7 you have the subtitle ""Linear Scaling is robust to distribution shifts"", but then you seem to show and also claim in the subsequent text that DP itself is robust to distribution shifts. Somehow the message is vague here.

- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge.


Limitations:
Some of the limitations are discussed in Section 5 but it could be expanded I think.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This study proposes a new algorithm for privately selecting hyperparameters subject to maximizing the model utility. The new algorithm draws inspiration from the linear scaling rule that suggests increasing learning rate as batch size increases. Given the number of hyperparameters in DP-SGD the proposed algorithm simply scales learning rate and number of iterations as the privacy budget increases. This introduces a new hyperparameter that is selected privately with a portion of the privacy budget while the rest is used to perform the normal hyperparameter search. The study provides brief theoretical intuition for why we can expect this linear scaling rule to more efficiently determine optimal hyperparamters compared to previous methods and extensive empirical evidence on 20 different benchmark datasets. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- One of the first papers to demonstrate improved privacy-utility tradeoffs that takes hyperparameter tuning into account. This is substantial as the field has mainly focused on evaluating the privacy-uility tradeoff without considering the privacy cost of hyperparameter tuning. As we move towards more practical implementations, this will be necessary. 
- Clever use of the linear scaling rule to perform hyperparameter search and the resulting algorithm is simple to use. 
- Extensive empirical evaluation and insightful analysis. For example, very few analyses have been done on the intersection of DP and distriutional shift. Yet, this linear scaling rule that is proposed holds in the presence of distribution shift. 


Weaknesses:
- “We are 165 the first to show that DP-SGD is capable of learning to handle distribution shifts without using any 166 techniques from the distributionally robust optimization (DRO) literature” -> There are a couple of other papers that draw this connection. [1,2]
- Lack of comparison to other private hyperparameter selection algorithms or hyperparameter free private learning algorithms [3, 4]
- Unclear why the initial hyperparameter search can be done with such a small privacy budget even though this is a key factor driving the performance of the algorithm.

[1] Kulynych, Bogdan, et al. ""What you see is what you get: Distributional generalization for algorithm design in deep learning."" arXiv preprint arXiv:2204.03230 (2022): 13.
[2] Hulkund, Neha, et al. ""Limits of Algorithmic Stability for Distributional Generalization."" (2022).
[3] Mohapatra, Shubhankar, et al. ""The role of adaptive optimizers for honest private hyperparameter selection."" Proceedings of the aaai conference on artificial intelligence. Vol. 36. No. 7. 2022
[4] Koskela, Antti, and Tejas Kulkarni. ""Practical differentially private hyperparameter tuning with subsampling."" arXiv preprint arXiv:2301.11989 (2023). 


Limitations:
The paper does address the technical limitations of the paper (specifically the assumption of access to public and private data). The main improvement for the limitations is to address the comparison to other tuning algorithms or optimization algorithms that don’t require as much tuning. 



Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method to conduct hyper parameter tuning for DP stochastic gradient descent. The method is based on a linear scaling rule, with two pilot runs using small PLBs and a third run chosen based on a linear extrapolation from the first two. The pilot runs are used to establish an estimate of the interpret and the slope that the total step size r would have with the PLB. The author uses this linear scaling rules to demonstrate that it works as well as grid search in optimizing for the accuracy in a suite of benchmark tasks, and attempts to apply this rule to perform empirical analysis on the potential of making existing model architectures DP and the issue of robustness against domain shifts.

My assessment, consisting of strengths, weaknesses, and questions, can be found in the sections below.



Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The best thing about this paper is that it develops a method based on an intuition that is potentially worthwhile. This intuition is captured in the small paragraph in Section 2, titled Linear Scaling is Intuitive. What the authors have proposed is essentially a dimensional reduction to the hyperparameter search, and the reason why that works, in the sense that what you end up finding may not be so far off from a greedier search, is due to the geometry where you force the updates to be more congruent with each other. The whole idea of a linear scaling would otherwise be rather unremarkable, but if the author can further develop this intuition, formalize it and expand on it, it would contribute some insight to the literature.

Weaknesses:
The most damning weakness of this paper is that it is written without due care. As a consequence, the main results and the accompanying algorithm are not correct as stated. I don’t suggest that the author is not capable of presenting the correct science -- to that question I do not know the answer. However, as things stand, the paper is not ready to be published.

The presentation in the introductory and main result sections wanders seemingly fluidly between epsilon-DP, (epsilon, delta)-DP and Gaussian DP:
1. Definition 1.1 is given in the language of (epsilon, delta)-DP;
2. The DP-SGD Definition is given without a quantification of its DP guarantee at all;
3.  Algorithm 1, which employs the DP-SGD given before, states that its output is epsilon-DP, where an alleged PLB accounting between epsilon and sigma is not supplied. (In reality, a delta would be needed, so the provided guarantee is incorrect to begin with.)
4. Then Proposition 2.1, which concerns Algorithm 1, gives a GDP guarantee in relation to sigma only, where sigma is not constructed as a function of epsilon (or the missing delta) in Algorithm 1;
5. Corollary 2.2 now qualifies Algorithm 1 as (epsilon, delta)-DP, with a one line proof given in the Appendix citing another work and has no substance on its own.

All of the above is confusing at best. For a standard reader, a student coming into the DP world for example, these are not pedagogically informative.

Back to Algorithm 1:
1. It contains four privacy loss budget expressions: epsilon, epsilon_0, epsilon_1, and epsilon_f. Based on the context, am I to infer that epsilon is the sum of the rest of the three? 
2. The quantity r on the 12th line (beginning with Decompose). Is this a generic r, as you use it on line 7, or is it in fact referring to r* on line 9?
3. When you speak of the “decomposition” or r, what is to be found exactly -- eta given r and T (my guess), T given r and eta (please explain), or both eta and T given r (please explain as well)? If my guess is correct, then do we know that the eta found here will automatically satisfy the condition given in Theorem 2.3? 

Line 143 begins with “We apply this theorem to logistic regression.” Then Line 151 continues, “While our theorem only holds for linear models…”. Nothing said between Line 143 and Line 151 constitutes a proof that Theorem 2.3 applies to linear models. This point should either be rectified with a formal analysis or deleted, so as to not be an exaggeration of contribution.

Section 3.1 is misleading and should be thoroughly rewritten to rid all expressions of “randomly”, “sample”, and “uniformly”. The author picked the experimental values. No sampling, particularly random sampling nor uniform random sampling of values took place. It is not clear to what is “r = 75” an approximation (Line 179).

In addition, based on my reading of Section 3.2 I believe it should not be presented as is.  My understanding of what Section 3.2 does is that it uses the linear scaling rule proposed in this work to construct ""accuracy hypotheticals” for the listed models and datasets as well as the domain shift situations, and compare those numbers with existing experimental results. If that is the case, this is a dangerous operation. The linear scaling rule, when used as a heuristic to make tuning faster, is fine as the worst that could happen is that one misses out on the most efficient model tuning. However, the way that the rule is employed in Section 3.2 it is taken as a scientific theory between epsilon and accuracy. The accuracy numbers you get from it is no different than a terribly extrapolated number based on a linear model fitted with two data points. If you really want to use the linear scaling rule to poke at the said questions, actual experiments should be conducted to confirm these extrapolations. Of course, I may have misunderstood what was actually done and in particular, whether actual experiments were performed — although if so, what would be the contribution from the linear scaling rule?


Limitations:
As stated before, I believe the paper is written hastily to the point that the central results presented are incorrect, significantly harming the quality of the contribution and its readability. I am also concerned with the scientific merit of Section 3.2. These points are elaborated in detail in my comment section on Weaknesses. 

Rating:
5

Confidence:
3

";0
JKhyQHpx7B;"REVIEW 
Summary:
This work introduces Vocabulary-free Image Classification (VIC), a task that aims to assign an image to a class in a large, evolving semantic space without a known vocabulary. The proposed method, Category Search from External Databases (CaSED), utilizes a vision-language model and an external database to extract candidate categories and assign the best matching category to the image. Experimental results demonstrate the superiority of CaSED over other frameworks, offering efficiency and promising future research opportunities.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The strengths of this work can be summarized as follows:
1.	Exploration of Vocabulary-free Image Classification (VIC) as a task, which overcomes the limitations of existing Vision-Language Models (VLM) for image classification.
2.	Proposal of CaSED, a training-free method for VIC that utilizes large captioning databases. CaSED does not require additional parameter tuning or fine-tuning of textual and visual encoders.
3.	Consistent outperformance of CaSED over a more complex VLM, BLIP-2, on VIC. CaSED achieves superior performance while utilizing significantly fewer parameters.
4.	Introduction of specific evaluation metrics for VIC, providing a valuable reference for future research and benchmarking in this domain.


Weaknesses:
1.	The most interesting part of this paper is the new task: vocabulary-free image classification. However, it is already studied in the existing work: https://openreview.net/forum?id=sQ0TzsZTUn [1]. The major differences between this paper and the existing work are as follows:
a)	The authors of the current paper extend the evaluation by using a larger vocabulary set -- BabelNet, compared to the existing work.
b)	The authors adapt the text-to-text score to find the best matching class.
From the perspective of the reviewer, these two differences may not be considered major advancements. Further analysis and comparison with the existing work are required to fully understand the extent of novelty and contributions of the current paper.
2.	To enhance the clarity and completeness of the paper, it would be beneficial for the authors to provide additional information on how to calculate the classical Cluster Accuracy, particularly considering the open vocabulary nature of the predicted labels. Given that the vocabulary is unconstrained, it is important to explain how the accuracy metric accounts for potential variations in the predicted labels.
3.	It is worth considering that most of the class names in vocabulary-free image classification may consist solely of nouns. Consequently, retrieving information from image captions, which typically contain more diverse linguistic elements, could be inefficient and computationally expensive. This raises a valid concern about the practicality of such an approach, as it may not yield meaningful results and could potentially waste computational resources. The authors should address this issue by discussing alternative strategies or potential optimizations to mitigate the computational burden associated with retrieving information from image captions.

[1] Han, Kai, et al. ""What's in a Name? Beyond Class Indices for Image Recognition."" arXiv preprint arXiv:2304.02364 (2023).


Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper proposes the task of vocabulary-free image classification -  image classification without a predefined set of categories (which is required in zero-shot classification). The paper further proposes the first method for this task - Category Search from External Databases (CaSED). For each given image, it first retrieves captions from an external text corpus, e.g. a large-scale caption dataset. The category names for the classification task are extracted from the retrieved captions via text parsing and filtering. For the classification, a scores fusion is conducted between 1) image-to-text matching score between the input image and the category names, and 2) the text-to-text matching score between the average embedding of retrieved captions and the embeddings of category names. 
The technical contributions are
1) The paper proposes the task of vocabulary-free image (VIC), to overcome the constraint of requirement of a predefined set of class names
2) The paper proposes an approach for VIC task via extracting category names from an external large-scale text corpus

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The paper is clearly written and easy to follow 
2) Vocabulary-free image classification is an interesting task as it requires no prior knowledge of semantic class space of visual inputs.

Weaknesses:
1) Some details of baselines and evaluation metric are missing. See questions. 
2) The choice of the caption database (as the text corpus) might have a big impact on the performance. Guidances / criteria of selecting a good text database are missing. 
3) A baseline is missing: WordNet and English words contain different semantic words than words covered by captions in the subset of PMD. It would be interesting to extract the words from the captions in the subset PMD, and apply CLIP for image-text matching. 
4) In Table 4(b) using YFCC100M (29.9M) as the text corpus leads to worse results than the case of CC12M (10.3M) and Redcaps (7.9M). Using CC12M alone leads to better results than using all the five datasets (54.8M). Further explanations are required. 
It would be interesting to report, besides the number of caption,   a) the number of objects / nouns covered in each database, and b) the relevance of captions in a database to the image dataset, e.g. by reporting the semantic similarities between captions and semantic labels of the image dataset,  semantic similarities between captions and the visual contents of the image dataset.

Minor:
1) Table 4(b):  asterisk meaning missing (I assume it means a subset of YFCC100M)

Limitations:
One missing limitation is that the method requires selection of a text database that is semantically relevant to the contents of the query image dataset. The choice of text corpus might have a big impact on the performance.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Current approaches to zero shot classification are limited by the assumption of an existing set of candidate classes. This paper presents a novel task, Vocabulary-free Image Classification, where semantic categories need to be automatically mined. This is important when generalising to a new domain, or the domain evolves over time. To address this task, the paper proposes to search for categories from an external database. While conceptually simple, this proves to be an effective approach, and performs better than using captioning models or large lexical databases like  WordNet. The paper also proposes metrics to evaluate on this new problem.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper addresses an important limitation of prior zero-shot classification approaches. 
- The importance of the proposed task, the motivation behind the method, and the method itself, are clearly laid out. 
- The components of the method are well ablated
- Multiple baselines are proposed and analysed
- It presents an avenue for future research on how to discover semantic concepts in the V+L space

Weaknesses:
1. Text-to-text score. Using CLIP as a scoring function for image-to-text makes sense. However, it is not obvious that a CLIP text encoder is optimal for the text-to-text score. L207-208 say that the CLIP text encoder ”focuses on the visual elements of the caption”. This could be backed by showing inferior performance when using a text-only model for the text-to-text score. Furthermore, an addition to the semantic space representation experiments (L136-170) would be a model that uses CLIP to retrieve captions, and then a text-only encoder to match the captions to a  semantic class. 

2. Reproducibility. The paper does not mention how BLIP was prompted to generate class names

3. Table 4a. The numbers on the last row of table 4a (retrieval with vis + lang scoring) correspond to the avg. values on Tables 1, 2 and 3. However, the first row (BLIP2) does not correspond to any row on these tables. (Related to 2. Reproducibility -- it not clear how the different settings differ)

4. Multimodal scoring results. Table 4 a) shows that scoring with the text-to-text score performs better than image-to-text score, and adding the image-to-text contributes a small amount. This means that the average caption is the most informative signal. That makes the statement on L192-193 ""The visual information is the most reliable source for scoring the candidate categories"" invalid. It is indeed an interesting finding and some intuition behind that would be useful. It would be interesting to see the performance of an ever simpler baseline that only uses text - select the class that has the highest number of occurrences in the retrieved captions. 

5. Candidate classes. What is the distribution of the number of extracted candidate classes? And how does it differ as you change the number of retrieved captions?

6. Generalization: One of the motivations behind this work is the applicability of ZS classifiers to domains that might not have a closed set of classes available. However, the current approach relies entirely on datasets of image-text pairs, from which captions are mined. For a domain that is not sufficiently represented in such datasets (e.g. medical images), the proposed method will fail. A discussion of how text-only databases can be used to handle that would be nice to see.

7. Database. In Table 4b you show that simply using the CC12M set of captions performs better than the contracted one (Ours). Why not use that?

Limitations:
Then limitations look good. A limitation not discussed is Weaknesses 6. Generalization

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents a method to perform image classification by using the similarity text and image embeddings of a large vision-language model (VLM). The method is called ""vocabulary-free"" because the large vision-language model doesn't have a particular set of classes (vocabulary) in mind, and (contrary to zero-shot approaches), the user does not need to provide a list of prompts. In particular, a set of k closest captions to the image embedding is retrieved from the VLM, and this candidate set is later scored to predict the image class. The underlying VLM is a ViT-L CLIP model.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The paper shows very good results when compared with different reasonable baselines (see Table 1, in particular). The experiments use several popular image classification datasets, and use different metrics, and the proposed method is significantly better than the baselines in virtually all cases, and certainly on average.
- The figures in the paper help understanding the proposed approach (Figure 3) in detail and the difference between ""vocabulary-free"" and zero-shot image classification.
- The paper contains results from ablation studies used to justify certain design choices (e.g. number of candidate captions retrieved from the database), the importance of the visual and language scores, etc (see section 5.3). I found these experiments very interesting (modulo some questions that were unanswered, see later comments).

Weaknesses:
- The term ""vocabulary-free"" is a bit misleading, since the VLM does have a vocabulary built-in, i.e. the so-called ""semantic space"". It's impossible to classify an image representing an object outside this semantic space. However, it's true that the semantice space is far larger than the class space of any of the datasets used for evaluation (although not necessarily a superset). What the authors mean by ""vocabulary-free"" is precisely defined in the paper (section 3), so this weakness is not critical, but as mentioned before, I find the term not accurate. 
-  My main concern with the paper is regarding the use of the datasets used to train the VLM model used by the proposed method. Section 5.1 mentions that the nearest-neighbours are searched over a collection of 5 large datasets, and the authors ablate the impact of each dataset in section 5.3 (table 4b). However, it's unclear the impact of the dataset used to train the VLM when compared with the baselines. To be more specific: is the proposed CaSED method than BLIP-2 VQA because the dataset used to perform knn is bigger, because the embeddings are better, or because knn+scoring is better than VQA (regardless of the dataset)?
- Lines 164-165 refer to the number of parameters as a reason for better/worse speed. This is technically wrong. The number of parameters can be (extremely highly) correlated with the speed, but it's not a direct cause of it necessarily. Thus, if arguments about speed are to be made, real runtime/query (on a given hardware) or -at the very least- FLOP/query (amortized over a given batch size) should be given instead.

---------
Update after rebuttal: The authors have (quite successfully) addressed my comments in their rebuttal. Thus, I'm slightly increasing my score.

Limitations:
The authors have adequately adressed the limitations and broader impact of the paper in section 6.

Rating:
7

Confidence:
4

";1
RI6HFZFu3B;"REVIEW 
Summary:
This manuscript understands the expressive power of GNNs from a new perspective of subgraph aggregation, and reveals the potential reason for the performance degradation of traditional deep GNNs due to the overlap of aggregated subgraphs. The authors propose a sampling-based generalized residual module SNR and theoretically proves that SNR enables GNNs to more flexibly utilize information from multiple k-hop subgraphs, thereby improving the expressive power of GNNs. Extensive experiments show the effectiveness of the proposed SNR module. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1）The idea of rethinking the expressive power of GNNs from the perspective of subgraph aggregation is novel and interesting.

2）The paper is well presented. The motivation, steps to construct a node-level, more flexible and general residual module to enhance the expressive power of GNNs while alleviating overfitting issue are clearly introduced.

3）The experimental results are convincible.


Weaknesses:
1）More related works on dealing with oversmoothing and overfitting issues in deep graph neural networks should be reviewed.

2）Further analysis of the experimental results is needed. For example, in Table 4, the possible reason why the proposed SNR module is weaker than InitialRes on Citeseer dataset should be discussed. 

3）There are some typos, e.g., row 221: “four data sets” should be “six datasets”. 


Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper propose a sampling-based module to enhance the expressive power of Graph Neural Networks (GNNs), which traditionally assume that information from the subgraph of the same hop is equally important for all nodes in the graph. They argue that this rigid assumption restricts the models' ability to capture complex relationships. With their proposed module, different nodes can assign varying levels of importance to their neighbors at different hops during information aggregation, which is shown to improve the performance of the GNN variants.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.	The proposed idea is interesting.
2.	The paper is well written and clear.
3.	The method is described in sufficient detail and easy to follow.


Weaknesses:
1.	Some reported baseline methods’ performances are questionable. For example, GCNII [1] shows alleviation of over-smoothing on the benchmark datasets with even 32 and 64 layers and achieved much higher accuracy than reported in Table 3. 
2.	The experiments are mainly performed on small-scale graph datasets. It’ll be interesting to test the method on larger graph datasets (e.g., the OGB datasets).
3.	Some detailed analysis of the learned mean and variance of the normal distribution would better support the proposed idea. It would be interesting to see how the distribution for nodes of different degrees changes with the increasing number of layers.
4.	To evaluate the method’s robustness to oversmoothing, it will be better to see a plot about how the performance changes as the number of layers increases than presenting the performance in a table. What’s more, the proposed method’s performance significantly dropped as the layer number increased. It suggests it still suffers from oversmoothing and/or overfitting.
5.	It’ll be interesting to see the over-smoothing analysis in Figure 2 to be done for different GCN variants with and without the proposed module.

References:
[1] Chen, Ming, et al. ""Simple and deep graph convolutional networks."" International conference on machine learning. PMLR, 2020.


Limitations:
yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of over-smooth with GNNs. It is argued that the over-smooth problem is caused by the increased overlap of the sub-graph when the respective field of GNNs becomes larger and larger.  In order to alleviate this problem, this paper proposed a method that random weighting the nodes within each layer by node-wise and layer-wise learnable parameters. The proposed method is limited evaluated on several datasets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Introducing randomness by using re-parameterization trick is interesting and makes sense to alleviate overfitting.

- Formulating different types of residual GNNs from a unified view is good.

Weaknesses:
- Some statements from this manuscript do not stand well. For example, it is argued from the 98-th line that as the k increases, the overlap between k-hop subgraph rises, making the aggregation from the k-hop subgraph from different nodes indistinguishable. It is not evident and the overlap is not sufficient for the over-smooth issues. Think about the transformer architecture where the self-attention within each layer has access to all other nodes/tokens while performing well on pixel-level tasks with several layers. 

- Additionally, the experiment that nodes with higher degrees tend to have more similar representations seem can not support the claim of sub-graph theory, as the overlap of the sub-graph does not influence by its degree.

- As for the over-smoothing problem, there are many other methods like drop-edge, which also introduce randomness when training the GNNs. It is highly recommended to include comparisons in the main results with methods along this line to make the contribution clearer (rather than just some comparisons under the setup of missing vectors). 

- It is unclear how the proposed method can help under the setup of missing vectors rather than other methods.

- As the sampling parameters are node-wise, I'm wondering how the proposed method can extend to inductive learning.

Some minor issues:

- Missing punctuation at the end of all equations.

- The quality of the figures can be improved a lot. It is highly recommended to use vector graphics for all the visualization.

Limitations:
There is no limitation discussion. The authors are encouraged to include as least the discussions about the limitation for inductive learning.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper focuses on the alleviation of overfitting and over-smoothing of deeper GNNs. It is an interesting topic and the solution seems promising with a sample-based node level residual block. Extensive experiments on public datasets verifies the applicability.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper analyses in a new perspective on the performance drop of GNN with the increase of layers, with motivating examples. It proposes a new residual block with the parameters randomly sampled from a natural distribution. The block alleviates the performance drop and seems applicable in general to all GNN models.

Weaknesses:
1. The presentation is in great need to improve. There are some long sentences that hinder the understanding of the authors's ideas, such as L54. A research article should describe facts with a 3rd party stand, but not subjective notations.
2. The paper should be self-contained even without the appendix, but it does not provide enough explanations such as on L83.
3. The figure is not reflecting exactly what is written such as L90 for figure 1, nothing shows the overlaps of aggregations via the lines and nodes.
4. Abbreviations should come with the full spellings on the first occurrence, even if it may be obvious in a specific research domain, such as GCN, but not GCNII on L194.
5. The citations should go to clear items such as a formula or a reference, but not a long section as L202 to Sec. 4.
6. It seems to be unnecessary to have Sec. 3.1 if there is only 1 sub-section.
7. Theorem 1 does not mean anything to me as it gets only formulas, but no conditions neither a conclusion.
8. It should be consistent to have the name of the proposed method or model.

Limitations:
N/A

Rating:
7

Confidence:
4

";0
ir6WWkFR80;"REVIEW 
Summary:
The paper proposes a novel mode of textual attack, punctuation-level attack, which aims to fool text models by conducting punctuation-level perturbations, including insertion, displacement, deletion, and replacement. This paper also introduces Text Position Punctuation Embedding (TPPE) as an embedding method and Text Position Punctuation Embedding and Paraphrase (TPPEP) as a search method to reduce the search cost and time complexity of determining optimal positions and punctuation types for the attack. The paper demonstrates the effectiveness of the punctuation-level attack and the proposed methods on various tasks such as summarization, semantic-similarity scoring, and text-to-image tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed punctuation-level attack expands the scope of adversarial textual attacks. By focusing on punctuation-level perturbations, the authors provide an approach to fooling text models while minimizing its impact on human perception.
2. The proposed methods, TPPE and TPPEP, not only enhance efficiency but also reduce computational costs. Additionally, the authors present a comprehensive mathematical analysis of these approaches.
3. The effectiveness and versatility of the proposed punctuation-level attack are demonstrated by the experimental results on various datasets and state-of-the-art (SOTA) models.


Weaknesses:
1. Whether LLMs can also be fooled, which should be discussed in the paper

I guess such robustness is due to the amount of training data is not large enough. It is curious that LLMs like ChatGPT will still fall into such a deficiency, since LLMs are trained on huge data.

2. Why PLMs fail on punctuations is not discussed

The punctuation-level attack does not surprise me a lot. The most interesting problem is to probe into the reason why PLMs can be fooled by punctuations.
Unfortunately, this is not in the paper, which large limits the contribution of the work.

3. The defense is not discussed

The authors do not provide the study on how to defense the punctuation-level attack. The contribution on how to enhance language models e.g. DeBERTa to be robust against punctuation attacks is much more significant than how to attack. To me, this is very important for the community to improve real-world systems against underlying attackers.

4. The attack success rate is only promising on CoLA, while limited on the other datasets.
However, CoLA is not suitable dataset for evaluation.

CoLA requires the model to decide whether the given sentence is linguistically correct. The manipulation of punctuations can spoil the label of the original sentence.

5. Punctuation modifications can change the original label on some task

Punctuation-level attack is safer than word-level modification. It still can change the original label on some task, e.g. CoLA. Even in extreme cases, punctuations can change the entire semantics. It is better for the authors to discuss this part in the paper, e.g. some bad cases. This is also important for future research.


Limitations:
see the weakness part

Rating:
4

Confidence:
5

REVIEW 
Summary:
By proposing a new type of adversarial attacks, i.e., the punctuation-level attacks, This paper can fool text models with less impact on the semantic information by human beings understanding. Its effectiveness is verified by experimental results on various datasets/tasks and victim models. What’s more, the attack method is accelerated by the proposed Text Position Punctuation Embedding and Paraphrase (TPPEP) approach, so that the attack can be accomplished with constant time cost. The efficiency has been demonstrated by the experimental studies. 


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
+ This paper is well written and easy to follow.

+ This reviewer appreciates the novelty of the proposed punctuation-level attacks, and the motivation behind, i.e., attacking text models with minimal perceptual influence on human eyes. It indeed brings some insights.

+ Besides its effectiveness in fooling various textual models, the authors also propose a TPPEP method to accelerate the attacking procedure. I believe this can significantly improve the practical use of the punctuation-level attacks.

+ The attack results on the update-to-date Stable Diffusion model are quite interesting.


Weaknesses:
- Though the proposal of punctuation-level attacks is indeed well motivated, an obvious major concern is raised that only three tasks (thus three types of victim models) are selected for attack effectiveness evaluation in the experimental studies. It would be better if more methods are selected for evaluation. The proposed approach would be fully validated and the conclusion would be more convincing.

- The current experimental results fail to provide more insights on how the method work in different scenarios. In fact, as a new type of text-attack method, there should be more in-depth analyses and explanations to quantitatively or qualitatively show the effectiveness.

- The notations in the paper are not always consistent. And some parts are not well illustrated, especially in Sec. 3.3. It is suggested to be re-organized. 


Limitations:
The current discussion about the limitations is practical but not so comprehensive. For example, the resource required by the method is not discussed. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to textual attacks called the punctuation-level attack. The method aims to fool text models while minimizing its impact on human perception and understanding. The paper discusses the effectiveness of this attack strategy, presents a search method to optimize its deployment, and provides experimental results showcasing its success. The authors also apply the single punctuation attack to summarization, semantic-similarity-scoring, and text-to-image tasks, achieving encouraging results. The paper concludes that the punctuation-level attack is more imperceptible to human beings and has less semantic impact compared to traditional character-/word-/sentence-level perturbations. The integrated Text Position Punctuation Embedding (TPPE) allows the punctuation attack to be applied at a constant cost of time. The experimental results on public datasets and state-of-the-art models demonstrate the effectiveness of the punctuation attack and the proposed TPPE. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper introduces a new approach to textual attacks called the punctuation-level attack, which is different from traditional character-/word-/sentence-level perturbations.
2. The punctuation-level attack is designed to be more imperceptible to human beings and has less semantic impact compared to traditional perturbations.
3. The paper presents a search method called Text Position Punctuation Embedding (TPPE) to optimize the deployment of the punctuation-level attack.
4. The paper provides experimental results showcasing the effectiveness of the punctuation-level attack and the proposed TPPE on public datasets and state-of-the-art models.



Weaknesses:
The adversarial attacks discussed in this paper can be categorized as non-pure white-box attacks, as the attack objective may differ from the model's evaluation metric. It is crucial to explicitly acknowledge this fact in the paper, as it is widely recognized that achieving white-box robustness represents an upper bound and is significantly more challenging than black-box robustness.

It appears that the number of attack iterations is restricted. To ensure robustness evaluation, it is advisable to ensure attack convergence by employing an adequate number of iterations.

In the experimental section, when comparing the proposed method with other approaches, a fixed number of updating steps is consistently utilized.



Limitations:
The main emphasis of the paper is placed on evaluating the punctuation-level attack's efficacy within specific tasks, including summarization, semantic-similarity-scoring, and text-to-image tasks. However, the evaluation of this attack is not comprehensive across a diverse set of NLP tasks, limiting the extent to which the findings can be generalized.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces an adversarial attack against NLP models based on punctuation perturbations. The authors introduce an attack called Text Position Punctuation Embedding (TPPE) that comprises an insertion, displacement, deletion, and replacement attack based on textual punctuation (e.g., commas or periods).
Experiments are conducted on various datasets, ranging from text classification (CoLA) to paraphrasing (QQP) and natural language inference (WANLI). The attacks are applied to ELECTRA, XLMR, and BERT-based models (DistilBERT, RoBERTa, DeBERTa). Additionally, the attack is applied to semantic-similarity-scoring (STS12), summarization (gigaword), and text-to-image tasks (prompting Stable Diffusion V2). Experimental results are promising, showing that the attack can be used to successfully attack models for the above tasks.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
* The paper provides an extensive analysis of punctuation-level attacks against NLP models. These attacks are promising since they have the potential to be less perceptible as compared to existing character-, word-, and sentence-level attacks.
* The analysis is extensive in that multiple NLP tasks (classification, summarization, text-to-image, etc.) are analyzed.
* It is interesting to see that the investigated models are vulnerable to punctuation-level attacks across tasks and domains.


Weaknesses:
* The paper does not compare TPPE against existing works and baselines. In Section 2, the authors point out various existing works focusing on punctuation attacks. However, none of these works have been evaluated and compared against in their experimental settings. To identify and support the strengths and utility of TPPE, such experiments are essential.
* I additionally think that comparisons to character-, word, and sentence-level attacks on the selected datasets would have been insightful since these experiments would provide the reader with a better understanding of how punctuation-level attacks perform in comparison to attacks focusing on other parts of a textual sequence.
* The paper does not further analyze the semantics of perturbed adversarial examples. To support the claims of semantic imperceptibility, human experiments analyzing the change in semantics between an original sequence and its adversarial counterpart would be important. The examples in Figure 2 nicely illustrate that inserting single punctuation marks can substantially impact the meaning of a sequence. Since adversarial examples are desired to preserve the semantics of an attacked sequence, quantitative experiments would be needed to evaluate TPPE in that context.
* The paper does not discuss potential approaches to mitigate the models’ vulnerability against punctuation-level attacks, for instance by assessing whether adversarial training / data augmentation (i.e., training the model on adversarially perturbed sequences) can help increase the robustness of the attacked models. This would provide additional insights into how robust the attack is, and how it can potentially be defended against.
* The results for the text-to-image task consist only of two qualitative examples. These examples are highly interesting, but to better evaluate the vulnerability of Stable Diffusion models against such attacks, quantitative results over a larger dataset would be important. 
* Overall, the paper focuses too much on introducing the attack and discussing its details and time-complexity analysis, instead of extensively evaluating its performance (the Experiments Section spans under 2 pages in the manuscript).


Limitations:
The paper briefly discusses Limitations in Section 5. However, potential ethical considerations arising from this research remain unaddressed. Since discussing these is quite important in this context (the proposed attack can be misused for malicious purposes), I would encourage the authors to add a section for this.

Rating:
5

Confidence:
3

";1
vq11gurmUY;"REVIEW 
Summary:
This paper explores online PCA methods for a certain type of non-linear eigenvalue problem.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This is a well-written paper on an interesting problem in computational science.

Weaknesses:
My main critique is that, as presented, the contribution of this paper appears not as much to machine learning or data science, rather to computational science. 




Limitations:
-

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents a new online-PCA based algorithm with some additional computational innovations to solve self-consistent systems. They add a mode-switching method and delayed calculation to improve convergence issues. The results are very good, but on a somewhat limited/niche dataset. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well presented, simple and shows very good results on a task that is seemingly impossible to solve with other methods. 

Weaknesses:
I am not an expert in the application area (electronic structures), but it seems like there could have been more extensive experiments to illustrate the benefits of the method. I don't think sec 4.1 gives a good enough picture of the benefits of the new method.

Also, the algorithm boxes on page 4 and 5 are slightly confusing and figure 4 is misplaced and is covering up some text it seems (at least in my printed version). 

Limitations:
The limitations are adequately addressed.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this work, the authors approach solving the Self-consistent Field (SCF) equation from a principal component analysis (PCA) for non-stationary time series perspective. They shows that, the equilibrium state of such an online PCA corresponds to the solution of the SCF equations. By doing so, this work is abled to achieve better convergence compared to the traditional fixed-point iteration methods for solving such equations.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- As mentioned in the paper, solving self-consistent Field (SCF) equation is of great significance in computational science for its connection to the Schrödinger equation. So proposing a novel approach, to overcome the non-convergence issues of the traditional fixed-point iteration methods for solving such equations, is important.
- The authors also mentioned that, this is the first steps in devising PCA-based algorithms for converging non-linear equations. So further study in this direction can help solving other such relevant problems.

Weaknesses:
- Please edit line 181 in the manuscript. Some part of the line is omitted by figure 4.

Limitations:
- This paper focuses on solving one important but rather niche problem.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a new method for solving self-consistent field equations - a form of nonlinear generalized eigenvalue problem in which the matrix being diagonalized is a function of the eigenvectors of the diagonalization. These equations are of great interest in quantum chemistry, and are typically solved via fixed-point iteration, but can struggle with the stability of the iterative process.
This paper proposes a connection to the Principal Component method (PCA) by viewing the function F(v) as a mapping from a vector to a data distribution, wherein the map F acts as a form of decoder/reconstruction function and PCA itself acts as an encoder/compression function. This formulation is entirely equivalent to the original problem, but allows the use of certain modified online/adaptive PCA methods to stabilize the iterative procedure. 

The authors demonstrate that this method performs superior to vanilla SCF iterations in a specific, theoretically-tractable case study, then apply the method to the more difficult case of solving the Kohn-Sham equations in electronic structure theory. The authors test their method on the QM9 dataset, which contains a large number of molecules for the purposes of electronic structure calculations. They sample 1% of the dataset at random, then evaluate their method on each case in question and compare to the existing SCF interpretation from the PySCF package. They find that their method results in convergence for all molecules considered (compared to 70-90%), while requiring roughly 2-3x more iterations.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The proposed method is interesting, and applies machine learning techniques to a foundational problem in quantum chemistry. Improving the accuracy or efficiency of Hartree-Fock/DFT calculations would be highly valuable to the quantum chemistry community, and is thus an interesting area of research.
The proposed method performs well on the theoretical case study, and the results in table 1 show a clear improvement in convergence rate over the existing baselines under discussion.

Weaknesses:
It would be helpful to have a better understanding of the significance of these results in the context of quantum chemistry, where they are most likely to be used. The results shown in Table 1 show that the existing baseline achieves a convergence rate of approximately 70-90% depending on the scenario, whereas the proposed approach achieves 100% convergence at a cost of approximately 2-3x the number of iterations. Within the field of quantum chemistry, is failure to converge a significant limitation, and is this tradeoff worth it? While I am not a quantum chemist, my understanding is that DFT calculations are already exceptionally computationally expensive, and significantly increasing the number of iterations required for convergence may be a very severe drawback.

In addition, it would be good to see a more thorough comparison. The experiments in this paper are only performed on a single dataset, and only compare to a single baseline. While I, again, am not a quantum chemist, my brief review of the literature revealed a number of existing methods seeing widespread use - including RMM-DIIS (is this the one used as a baseline in the paper?) as well as Davidson or Blocked Davidson iterations, and combination methods combining both RMM-DIIS and Blocked Davidson. Unless I am misunderstanding the applicability of these methods to the problem under consideration, it would be helpful to see a comparison to a broader range of baselines, as well as a wider range of datasets. As is, I feel like the comparisons in section 4 are too narrow to provide a compelling case for the proposed approach, but it is entirely possible that I am missing important context from the field of quantum chemistry. I am happy to revisit this if my understanding is incomplete.

Limitations:
It would be helpful if the authors gave a more complete presentation of the applicability of the proposed approach in the context of the quantum chemistry literature, and the significance of their results in that context.

Rating:
5

Confidence:
3

";1
9B57dEeP3O;"REVIEW 
Summary:
The authors aim to generate a series of coherent images given a series of text prompts resembling a visual storybook. To do so, the authors focus on two fronts: (1) leveraging the Stable Diffusion model to generate the series of images and (2) generating a diverse dataset used to train the model on a range of styles. For generating a set of coherent images, the authors condition the Stable Diffusion on both the text prompt and a set of previously generated frames, both encoded using a frozen CLIP encoder. The text conditioning is passed to the U-Net layers via the standard cross-attention module and LoRA. To insert the image conditioning, the authors introduce a Visual Context Model resembling the standard text conditioning module. The diffusion model is then partially trained to generate images that are both consistent with the text prompt and previously generated frames. To attain images ranging in style, the authors construct a new dataset, named StorySalon, consisting of Youtube videos and E-Books. These raw storybooks are filtered and re-captioned to better align with the visual content of the storybook. Qualitative results demonstrate the ability to generate new storybooks on prompts generated by ChatGPT while quantitative results demonstrate improvements over simple baselines. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The authors focus on an important task of generating a series of coherent images that follow a given text prompt. This could potentially be useful beyond a simple storybook creation, e.g., for video generation. 
- The Visual Context Module as a means for injecting image-level details to the denoising network is simple and intuitive and could be useful in other tasks. For example, in image editing where the desired edit cannot be easily described using language.
- The visual results generated by StoryGen are impressive in comparison to the evaluated baselines.  


Weaknesses:
**Dataset Creation:**   
- I have some reservations regarding the use of the term Human Feedback in Section 3.2.3 and Section 4.2. While ChatGPT was fine-tuned to align with human preference, I believe that using ChatGPT for generating additional prompts should not be considered Human Feedback. While this is an intriguing approach, I believe replacing Human with LLM is more reflective of what is actually done here.
- Regarding the ablation study performed by the authors, it seems that “Human Feedback” leads to a quite negligible decrease in FID and I am therefore uncertain if this really contributes to the curriculum learning scheme. The authors mention that more stories can be added, but I would have expected to see a bigger improvement if this stage is truly important.

**Evaluation:**   
- There are numerous essential evaluations that are missing from the current submission. Among these, the most important is a thorough evaluation and comparison of StoryGAN, Story-DALL-E, and AR-LDM. All three have publicly available code so an evaluation is needed to understand the improvement realized by StoryGen. 
- I am not sure that FID is a particularly interesting metric here since all evaluated methods in Table 1 use Stable Diffusion to generate the images. Moreover, I do not believe that FID is a good metric when trying to measure how much the image captures a given style, as is the goal here. Maybe a CLIP-based metric using a prompt depicting a style would be more appropriate here? 
- There are numerous ablation studies that I believe are required to understand the contribution of both the proposed architecture and dataset. 
    - Architecture: 
        - An ablation study on the Visual Context Module and whether a simpler conditioning is possible (see my detailed question below). 
        - Was an ablation study performed on the BERT-like masking during the multi-frame fine-tuning? 
    - Dataset
        - The impact of the visual-language alignment stage in preparing the dataset. The authors state that directly fine-tuning on the story narrative may be detrimental, but do not validate this claim. 
- Some additional evaluations could help validate the effectiveness of the method. 
    - First, the authors claim that the method can be used to generate stories of arbitrary lengths (Line 106). It would be great to quantify this by generating stories of varying lengths and validating whether there is a loss in quality after a certain length. 
- One particularly interesting component of the method is the Visual Context Module, so I would have liked to see far more evaluations performed on it. For example, the authors mention that multiple frames can be used for conditioning by concatenating their CLIP feature. Some interesting questions that could help strengthen the importance of the component include: 
    - How much was this evaluated? 
    - How much does conditioning on more frames assist in temporal consistency? 
    - How many previous frames can be concatenated without hindering performance?
- Stating that the model achieves a significant improvement in the alternative models seems like a strong over-claiming when approximately 30 participants were used for the user study. A substantially larger pool of participants would be needed to truly quantify this improvement, especially since this is the only relevant metric used to evaluate the methods. Why are the reported FID metrics between Table 1 and Table 2 different? Is a different dataset used? I would expect only “without HF” to be different if both use the same dataset. 


Limitations:
The authors include a discussion on current limitations and potential societal impacts in the supplementary. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes the model StoryGen for the task of visual storytelling. Visual storytelling is a task to generate a sequence of consistent images given a story (several sentences). StoryGen is a diffusion model taking in both image and text as conditions, and outputs an image consistent with the conditions. The training process includes pre-training for single image, finetuning for multiple image and finetuning with human feedback. On top of the StoryGen model, this work also provides a dataset, called StorySalon, which consists of 2k story books (30k well aligned text-image pairs). 

The overall structure of StoryGen model is simple. It is built upon existing well trained diffusion models and image/text encoders. To generate cartoon-like image, LoRA is adopted into the text conditioning module in a diffusion model. The author calls it the style-transfoer module. The parameters in LoRA are updated at this pre-training stage to give single cartoon image. Next is the multipe image fune-tuning. StoryGen conditions on both text and image, which is implemented by using two cross attention layers: One is noise input + text and the other is noise input + encoded previous generated image. After the second step, StoryGen if further finetuned on 100 high-quality stories. 

The author also spends some efforts to collect the StorySalon dataset. To begin with, the author downloads a huge number of videos and subtitles from online web resources with potential stories. Then give story-level description and visual level description for each story. The story level description is obtained by using dynamic time warping algorithm using subtitles. The visual level description is derived from ChatCaptioner. Finally, OCR method is applied to get potential videos captions.

The experiment section shows that StoryGen model can give consistent and story-like output images, while other methods fail.


Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The paper is clearly written. The collected StorySalon dataset could benefit the research community. 

Weaknesses:
There is not much technical novelty, and the experimental results are limited.

Limitations:
- Since there is no human labeler involved in collecting StorySalon dataset and the total number of stories in StorySalon is only 2k, I’m concerned about its quality.

- It is also unclear from the examples provided if the results is derived by just overfitting the training set.


Rating:
6

Confidence:
5

REVIEW 
Summary:
The work focuses on the application of image generation based on a given story. Specifically, the proposed model is conditioned on the current sentence and prior generated images to ensure the story is engaging and coherent. A progressive training strategy is proposed to achieve a good model. To improve the proposed method, a new dataset is collected, while a set of human-verified generative samples are also utilized to improve the generated images. 

--------------------------
I acknowledge the author's effort in the rebuttal and have made changes to the review accordingly.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ This work demonstrates the possibility of generating visual storytelling images conditioned on the given stories. 
+ A three-stage curriculum training strategy is proposed to train the proposed model. However, it would be great to demonstrate the limitation of training the model with multiple-frame (i.e., without single-frame pre-training)
+ The authors collected a large-scale dataset to enable model training for storytelling purposes.

Weaknesses:
- the technical contribution of this work is limited. Most of the components are not novel and the key contributions are the way it is combined to generate a plausible output. It is unclear what the insights generated from this work that is not previously obvious to the community.
- The description of the new StorySalon dataset is limited. Specifically, it is unclear if the collected dataset has obtained legal consensus and properly handled copyright issues. 
- The work lacks a comparison with existing work, such as those introduced in line 44 and line 93-102. The two baselines in Table 1 are too naive as both are inherently limited to generate a fair comparison with the proposed method. 


Minor:
- Fig 2 should clearly state that the Image encoder only considers a single previous frame to generate the next frame.

Limitations:
The paper (in supplementary) discusses that data bias is an issue that needs to address in this domain. Collecting a larger dataset for training is the solution discussed. This may be valid considering this work is still in the early stage of the research. 

I want to point out that the discussed approach is limited as (1) it is resource-consuming, and (2) it will face the problem of copyright in order to obtain a good dataset for training. The data ownership issue may be a major hindrance.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents StoryGen, an auto-regressive image generator that leverages text and image conditioning. StoryGen incorporates a style transfer module integrated into the text-conditioning module, along with a visual context module. The authors also constructed a substantial dataset called StorySalon, comprising 2K storybooks and 30K text-image pairs.


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper constructs a new dataset StorySalon contains 2K storybooks and more than 30K well-aligned text-image pairs. The authors have invested significant effort into filtering the data, making it a valuable resource for advancing the field of story visualization.
2. The paper is well-written and easy to follow.

Weaknesses:
1. The illustration does not align with the description provided. In line 135, the authors state that ""StoryGen generates the current frame $\mathcal{I}_k$ by conditioning on both the current text description $\mathcal{T}_k$ and the previous frame $\mathcal{I}_{k-1}$, as illustrated in Figure 2."" However, the left figure of Figure 2 shows the image conditioned on more than one previous image, which contradicts the mentioned conditioning approach.
2. The improvement of Human feedback appears to be trivial, as indicated in Table 2. The 0.19 FID score gap could potentially be attributed to different training seeds, which raises doubts about the significance of the reported improvement. (I do not agree with the statement that 200 stories are too small since the model is trained using 2k stories overall. It appears to be sufficient for human alignment and does not require an extensive amount of data.)
3. The FID score lacks precision in the test set, particularly with only 100 storylines. It is recommended that the authors expand the test set by including more stories to provide a more accurate evaluation.
4. The baselines SDM and Prompt-SDM are too weak.  It is suggested that the authors compare StoryGen with finetuned or LoRA-finetuned SDM models using the same training settings to establish a more robust baseline for comparison.
5. The auto-regressive generation approach employed by StoryGen has already been proposed by AR-LDM. Consequently, the architecture design itself lacks novelty.
6. StoryGen is only conditioned on one previous image and does not utilize the corresponding caption of the previous image. In the depicted cases of Figure 1 and Figure 4, there is only one main recurring character. If multiple characters were present, StoryGen may struggle to ground the characters in the previous images. Furthermore, if a character does not exist in the previous image, StoryGen may face difficulties in maintaining consistency between frames.
7. The language understanding capacity of StoryGen appears to be weak. For instance, in the second case of Figure 4, the rabbit appears small in the fourth frame, whereas it should be as big as it is in the fifth frame. Additionally, in the sixth frame, the boy's hair does not become brighter as described in the caption. Moreover, in the seventh frame, multiple other boys are depicted with the same yellow hair, which contradicts the previous story setting. This limitation may stem from StoryGen solely relying on the only one previous frame $\mathcal{I}_{k-1}$ and not incorporating previous captions into its generation process.

Limitations:
1. The author should provide examples of the constructed dataset showcasing different visual styles and character appearances.
2. The StorySalon dataset consists of 2K storybooks and over 30K well-aligned text-image pairs, which is smaller compared to datasets such as FlintstonesSV (24K stories and 123K image-caption pairs), PororoSV (14K stories and 74K image-caption pairs), and VIST (27K stories and 136K image-caption pairs). Despite the authors' claim that StoryGen can perform open-ended story generation, it remains unclear whether StoryGen can generate stories involving more complex scenarios with unusual entities.
3. Apart from human feedback, the authors have not conducted any other ablation studies to evaluate the effectiveness of their proposed techniques, such as word dropout and curriculum learning.
4. There are concerns regarding the legality of using web-crawled e-books. The authors should provide additional information about the sources of the e-books and clarify whether proper copyright guidelines were followed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper propose an approach for fine-tuning diffusion models for the task of story generation, where a model must generate frames for sentences in a story. To do so, they propose adding adaptors conditioned on both images and text into a pre-trained stable diffusion UNet. The authors also introduce a scraped dataset of 2k stories with 30k image-text pairs, which serves as the data foundation for their fine-tuning.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. The dataset of story text and images is a significant contribution, which can be useful for future work on visual storytelling.

S2. The paper is generally well framed and motivated. Writing and presentation is in general strong and polished.

S3. Simplicity of the method. The method uses off-the-shelf components and algorithms (LoRA, cross-attention, etc.) to enable new capabilities. I see this simplicity as a strength not a weakness. 

S4. Inclusion of human evaluation is a strength.

Weaknesses:
W1. Presentation. Figure 2 suggests that StoryGen is conditioned on all past frames. However, in reality StoryGen is conditioned on the most recent frame.

W2. Results. It appears that the model is not able to preserve style and content as well as say DreamBooth. For example the stripes on the shirt in Figure 4 are not preserved. 

W3. Evaluation. It seems that the StoryGen model without human feedback model was not evaluated in the human evaluation (Table 1). Given that the ablation in Table 2 shows similar FID scores for StoryGen with and without human feedback, it is not clear if this step is really necessary. 

W4. Lack of baselines. StoryGen is specifically fine-tuned for the desired task, while stable diffusion is not. Given this, it is perhaps not surprising hat StoryGen greatly outperforms stable diffusion. Can some other baselines, perhaps based on the DreamBooth (with the subject in the first image representing the special [V] token) be used for a stronger baseline? This is just one idea; however, comparing against some prior work may help to elucidate the strength of the method.

W5. Evaluation. Fine-tuning can sometimes hurt the generality of a model. How well are individual frames generated relative to the base SD model? Is it possible to do a human evaluation here or compute CLIP scores? Ideally, the story would be cohesive (frame-to-frame consistency) without degradation in quality of each frame.

Limitations:
The paper does not directly address limitations in the main paper, which is an additional weakness. I suggest discussing failure cases or otherwise conducting a failure analysis.

Rating:
5

Confidence:
4

";0
7sjLvCkEwq;"REVIEW 
Summary:
The paper presents a very interesting analysis of discriminative entropy clustering in the literature and their use for self-labeling highlighting clear interpretation of the conditional and marginal entropy terms as decisiveness (push to have confident predictions) and fairness (to encourage desired proportions in clusters). The paper analyzes the variants of this kind of losses and their connections. They also discusses the relationship of this loss to K-means and refines the previously mistaken connection in a previous paper to point out the distinct difference. They further point out the effectiveness of reverse cross-entropy in case of uncertainty error and forward KL term to make them more effective  in the endpoints of softmax  interval. They also propose to use the regularization of classifier weights for margin maximization similar to SVM. A closed form update is derived to compute the pseudolabels from the combined weighted loss and shown its efficacy in clustering experiments.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- I think the paper discusses beautifully the intricacies and usage of Mutual information based entropy clustering loss, which is widely used in self labeling/self supervised/ weak supervised learning. The flow of the discussion is to the point and tries to give interpretation of each terms in the loss in a concise manner by drawing connection among the variants. 

- The paper discusses the the previous link of entropy clustering to K-means and identify the distinct difference to k-means which was in fault in previous literature. On top of that they find the usefulness of the classifier weight regularization used in previous proof to link to the loss explicitly for margin maximization similar to SVM based clustering. 

- The use of reverse cross-entropy and forward KL term and the motivation behind it is explained  very well with the aid of Figure 2, so that they are more robust in case of uncertainty around the corner of the softmax simplex.

- The formulation of the EM algorithm is nice to make it work faster along with batchwise operation, showing its global optimal solution guarantee at convergence due to drawing convexity with formulated tight upper bound.
- The experimental results shows clear improvement according to their claim.

Weaknesses:

1. I would say the results of joint clustering and feature learning in Table 3 is not encouraging even when showing a very shallow network of VGG4, the improvement is not significant apart from  MNIST. But in Resnet-18,  the inductive bias learned from pretraining is helping, then the improvement from the proposed loss might not improve very significantly with the proposed loss.  Also in Table 4, the regularization on the feature extractor $\textbf{w}$ done or not in the loss or by weight decay? 

2. The experiments on semi supervised learning could also be shown to understand more when some supervision is available. How the idea of using reverse cross-entropy could be used in case of labeled one-hot y in equation 13? or it will be the regular cross-entropy for labeled set and reverse for pseudolabels with updated y_i?

3. What if the loss in 13 is directly optimized with gradient descent instead of using the EM? Although, it seems if $y_i= \sigma_i$ the reverse cross-entropy does not change anything if not updating y_i with closed-form update, is it?


Limitations:
The limitations are not discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors first presented an analysis of the relationship between the regularized information maximization (RIM) clustering framework to K-means and SVM-based clustering methods, showing stronger similarities to the SVM-based clustering than K-means. Then they proposed a new loss function and associated EM optimization algorithm for clustering leveraging the reverse cross-entropy/KL divergence to obtain more robust and fair clustering, which has been demonstrated to improve the performance on several balanced image classification benchmarks.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The authors identified an error about the missing normalization term in the proof of the equivalence theorem presented in Jabi et al. (2021).

2. The replacement of the forward cross-entropy with the reverse counterpart in the RIM loss appears novel in the clustering scenarios and has the potential to effectively mitigate the impact of uncertain/noisy pseudo-labels.

3. The proposed method showed improved performance on different image classification benchmarks over several baselines.

Weaknesses:
1. The manuscript was poorly written. The authors dedicated more than two pages to discuss the general background of the information maximization clustering framework and related methods. However, these discussions were confusing and largely limited the space for presenting the actual contributions of this work. Furthermore, many terms, including concepts like H and KL divergence, are not explicitly defined or explained, which may cause difficulties to understand the differences between the forward and reverse formulations. Additionally, the claimed conceptual and algorithmic contributions seem to be independent of each other. It is unclear if any of these conceptual clarifications contribute the discovery of the new loss function.

2. The disproof of the equivalence theorem in Jabi et al. (2021) is not convincing. While the authors pointed out an error in the original proof, this does not necessarily eliminate the possibility that the equivalence itself remains valid. Furthermore, this work focused on the standard K-means objective (including Figure 1), whereas Jabi et al. (2021) considered a soft and regularized K-means loss.

3. The authors' claim that the L2 regularization is linked to margin maximization seems questionable. [1] demonstrated that margin maximization is a property of the loss function itself rather than the regularization, which serves to control the model complexity. Indeed, certain combinations of loss function and regularization are not margin-maximizing.

4. The experimental validation is limited. A more comprehensive evaluation of the proposed modifications to the loss function would involve investigating the individual and combined effects of these changes on selected benchmarks and then comparing the results with multiple established baselines. It is still unclear how each modification contributes to the final improvement. Although the authors presented the impact of the reverse cross-entropy modification, they did so within the fully supervised setting rather than unsupervised scenarios. Furthermore, the authors only considered balanced datasets and tested the clustering with the ground truth number of clusters. A more diverse set of experimental conditions, including unbalanced datasets and varying numbers of clusters, would provide a more robust evaluation of the proposed method. Both NMI and ARI metrics used in Table 4 are capable of handling different number of clusters. Additionally, the architecture used in Section 4.2 is different from that used in the baseline methods. It would be preferable to standardize the experimental settings, including model architecture, to be able to directly compare with the results in the literature. Lastly, the inability of the proposed method to properly train a deep network-based clustering model is a concern as well. Most of the tricks should be independent of the loss function modifications, especially the reverse cross-entropy term, thus can be naturally integrated together.

[1] Rosset et al., 2003. Margin maximizing loss functions.

Limitations:
There is no discussion of limitations or potential societal impact.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper first discusses a number of general properties of  entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques.
Then the aurthors find that cross-entropy is not robust to pseudo-label errors in clustering.
Finally, this paper proposes a new loss function based on reverse KL divergence for clustering  to obtain more robust and fair clustering.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
(1) The proposed  loss function is interesting and and seems to be effective to obtain more robust and fair clustering.

(2) The authors try to establish connections between entropy clustering methods and classical methods.


Weaknesses:
(1) This paper is not well organized. There are too many details for the proposed method. Some of them can be moved to Appendix.

(2) There can be more descriptions and examples about the proposed method.

(3) The proposed method is merely tested on three image datasets.

Limitations:
I mentioned them in the Weakness section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors consider discriminative entropy clustering and produce a discussions linking previous works. They have a version of the algorithm based on EM and a modified KL-divergence term. Experiments show the modified algorithm works better than competing methods with small networks. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The authors provide a good overview of discriminative entropy clustering and its development, from the work of Bridle et al to the regularized version by Krause et al, to the more recent work using deep learning and representation learning (Asano et al and Jabi et al).  


Weaknesses:
- The contribution of this paper is somewhat limited: 
  1. The pointing out of a proof error in Jabi et al is helpful but is not significant on its own
  2. The discussion on SVM is based on previous works and simply replaces the logistic loss with margin loss, and is not particularly insightful 
  3. Section 3 is the authors' contributed new algorithm, but the main difference with previous works is changing the order of the KL term in the objective. 

- The improvements in empirical evaluations, compared to other methods, are somewhat limited. Many of the improvements are within standard error of competing methods. 

- The authors use a lot of space to discuss previous work (first 5.5 pages), and not enough space to explain what is new about their method and specifically what problems it addresses. 


Limitations:
Limitations not mentioned; potential negative societal impact not applicable. 

Rating:
3

Confidence:
4

";0
QFcE9QGP5I;"REVIEW 
Summary:
This paper presents first-order iterative methods for solving unconstrained minimization problem. The connection between the proposed methods, quasi-Newton and Anderson accelaration methods are illustrated, which gives an insight of the motivation. Under certain assumptions, the methods exhibit explicit gobal non-asympotic convergence rates adaptively using backtracking strategy. The effiency of the propsed methods is compared to a fine-tuned BFGS algorithm with line search in the numerical experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem is well-motivated.

- Under certain assumptions, the proposed adaptive first-order methods achieve explicit global convergence rates that blend those of gradient descent and cubic regularized Newton's method.

- The inputs of the algorithms are carefully discussed. It should be clear for others to implement.

- The algorithm complexity is analyzed and compared in the numerical experiments.

Weaknesses:
- Too many assumptions and requirements in the theoretical part. Can those requirements be verified? Seems the requirements are post to let the proof go through.  

- The algorithm setting for the numerical experiments in section 6 is a bit confusing. For example, what online techniques are used for 'Iterate Only', 'Accelareted Forward Only', 'Forward Estimate Only' and 'Greedy'?

- The performance of the accelarated algorithm is suboptimal and unstable.

Limitations:
- The performance of the accelarated algorithm is suboptimal and unstable.
- Analysis is provided only for Algorithm 1 not Algorithm 2 while the later seems better. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a generic framework for developing new iterative schemes for smooth optimization in the deterministic setting. The derived new methods show some similarity to quasi-Newton methods and Anderson acceleration, while using a backtracking line search for estimating the Lipschitz constant and having step sizes adaptively determined by minimizing an upper bound of the objective function or the gradient norm. The explicit, global and non-asymptotic convergence rates are established for one type of the derived methods. Numerical results are presented for some specific implementations of the framework.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a novel framework inspired by the recent development of cubic regularized Newton's method. This framework leads to several optimization methods that can be viewed as new variants of quasi-Newton methods and Anderson acceleration. The theory seems to be solid. The explicit, non-asymptotic convergence rates are established under different assumptions of the optimization problems, justifying the introduced techniques. The content is informative and the main idea is easy to follow. Numerical results are presented to support some claims of the paper.

Weaknesses:
The weaknesses are listed as follows:

1. The theory requires the Lipschitz continuity of the Hessian in $\mathcal{R}^d$, which is a strong assumption in practice.

2. The theorems only consider the iteration number towards convergence, which may overestimate the efficiency of the algorithms. Since the number of function evaluations can be large in each iteration, it needs to be clarified whether the claimed convergence rates are still valid when taking these hidden calculations into account.

3. The biggest drawback of the proposed methods is their suitability for solving high-dimensional problems. To well approximate the Hessian which is critical for the convergence, the memory usage needs to be large, which can cause failure of the algorithms when the memory resource is limited. What's worse, the algorithms are very complicated. They use a backtracking line search to estimate the Lipschitz constant and solve a minimization problem to determine the stepsizes in each inner step of the line search. It is unknown how many times the minimization problem needs to be solved during each iteration. It is likely that the computational cost of each iteration is much higher than the classical quasi-Newton methods or Anderson acceleration. However, there is no discussion about this issue.

4. The algorithms were only tested for solving small-scale logistic regression problems. Some numerical results do not support the theory. For instance, the accelerated method does not exhibit any acceleration in practice. In many cases, the simple BFGS with the default setting still achieves the best performance, while the proposed methods are less efficient due to higher costs.

5. Since many additional calculations are hidden in the subroutines of the algorithms, it is more convincing to report comparisons of the considered algorithms with respect to running time. However, these results are missing.

There are also some other concerns about the paper:

1. In Section 2, the paper claims that quasi-Newton methods and Anderson acceleration share the common property that the iterates are combinations of previous iterates and the current gradient. This claim may be true for quadratic optimization but seems to be wrong in general cases. There is no justification for this property throughout the paper. The presented framework is more like a generalized heavy-ball or Nesterov-like method, or can be viewed as an adaptive version of the subspace Newton method. Its connection with quasi-Newton methods and Anderson acceleration is not clear. It is misleading if such a connection is not valid.

2. Although the presented framework does not use the traditional line-search or trust-region technique to guarantee global convergence, it uses a more costly backtracking line-search strategy to estimate the Lipschitz constant. Such complexity can impair the significance of this work.

3. Section 1.2 says that Anderson acceleration does not generalize well outside quadratic minimization. It is not true since Anderson acceleration is well-known for its usefulness in accelerating fixed-point iterations in scientific computing.

4. Some descriptions of the classical methods are not standard. The formulas of BFGS and Anderson acceleration in Section 2 are quite strange. The formula of BFGS below Line 97 seems to be wrong. The formula of Anderson acceleration below Line 104 is incorrect since $d_0$ and $g_0$ are undefined. The formula below Line 513 is not the Anderson acceleration.

5. Many notations are not defined clearly, e.g., $P_i$ in Equation (2), $r_i$ below Line 104, and $R^\dagger$ in Line 600. 

6. The paper discusses many possible methods derived from the framework, but the pros and cons of each method lack clear clarifications. For example, the orthogonalized greedy method seems to outperform BFGS in some cases, but it also doubles the memory usage. So the comparison in the experiments may be unfair.

7. The proofs need to be reorganized to make them easy to follow. It is better to give more examples of the introduced notations and assumptions.

Limitations:
Some limitations have been mentioned in the main paper, but there is no discussion about the total computation cost and memory cost.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors introduce a generic framework for developing novel quasi-Newton and Anderson/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including accelerated convergence on convex functions. They also provide empirical results in the numerical experiments.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The structure is clear and the theoretical analysis and proof are correct. The results presented in the numerical experiments section are consistent with the theoretical results.

Weaknesses:
There is the following weakness regarding this submission.

1. First, the authors ignores a lot of existing quasi-Newton type methods or accelerated versions that achieve the non-asymptotic global convergence rates of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$ such as:

“Practical inexact proximal quasi-Newton method with global complexity analysis”. Katya Scheinberg and Xiaocheng Tang. Mathematical Programming160(2016), pp.495–529.

“Proximal quasi Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates”. Hiva Ghanbari and Katya Scheinberg. Computational Optimization and Applications69(2018),pp.597–627

“Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization”. R. Jiang and A. Mokhtari. https://arxiv.org/abs/2306.02212

Recently there also exists a paper proposing the quasi-Newton type method that could achieve a global explicit superlinear convergence rate:

“Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence”. R. Jiang, Q. Jin, A. Mokhtari. Conference on Learning Theory (COLT), 2023.

However, this global superlinear convergence rate assumes strong convexity of the objective function. The authors should compare the results in this submission to all these related literature in detail to check if the proposed algorithm could achieve an improvement in the aspect of convergence rate or computational cost per iteration.

2. The authors didn't compare the results of the this paper to the global convergence rates of first-order gradient descent and accelerated gradient descent. The authors claimed that the proposed algorithm could match the results of accelerated gradient descent, but is the constant in the convergence rate of this proposed method better than the constant of accelerated gradient descent? What is the improvement of this algorithm compared with the accelerated gradient descent? Notice that the computational cost per iteration of this QN-type method is worse than the computational cost per iteration of accelerated gradient descent.

3. It's better for the authors to use the notations of $s_t = x_{t + 1} - x_t$ and $y_t = \nabla{f}(x_{t + 1}) - \nabla{f}(x_{t})$. These notations are more commonly-used in the quasi-Newton methods.

4. The notations used from equations (3) to (7) are quietly confusing. What's the exact definition of $y_i$ and $z_i$? Is it $y_i = x_{t - i + 1}$ and $z_i = x_{t - i}$. Also it seems that $D$, $G$ and $\epsilon$ depend on the iterations index $t$. Then it should be $D_t$, $G_t$ and $\epsilon_t$. These notations are messed and make the readers difficult to understand and follow the ideas of the authors.

5. What is the definition or function of the parameters $M_0$, $M_t$ and $M_{min}$ presented in the algorithms? The authors said that the $M_0$ is the initial guess of the smoothness parameter, So $M$ is an estimation of the parameter $L$ in assumption 1? What's the value of $M_0$ in all the numerical experiments presented in this paper? What theoretical conditions should this $M_0$ satisfy?

6. The authors presented some designed requirements in section 3.1. These expressions of requirements are a bit strange for the theoretical results. The authors should either make these requirements as the assumptions of the algorithms or make these requirements as some lemmas/theorems of the theoretical analysis. However, these requirements are too strong or restrictive as assumptions. On the other hand, the authors didn't give any strict mathematical proof to give any conditions to satisfy these requirements. The authors argue that these requirements are not restrictive in the text of section 3.1. But this is not enough for the theoretical analysis of the algorithm. We need strict and clear mathematical proof or empirical results from the numerical experiments. It is not clear how to make these requirements to be satisfied. There is no formal proof.

7. The authors claimed that $N$ could be small around line 149. However, as expressed in the theorem 6, it seems that $N$ should be comparable to the dimension $d$ to reach a good convergence rate. But when $N = \mathcal{O}(d)$, the computational cost of solving the sub-problems could be as expensive as $\mathcal{O}(d^3)$ as presented in line 148. This is as costly as the Newton's method.

8. From line 203 to line 210, the authors argue that some propertied needed to be satisfied to retrieve the convergence rate of Newton’s method with cubic regularization. However, it seems that when these properties are satisfied, the computational complexity is the same as the Newton's method. Then, what is the advantages or improvements of this method compared to Newton’s method with cubic regularization?

9. What are the definitions of $b_i$ and $\lambda_t^{(1, 2)}$ in the equation below line 213? The authors should explain these parameters for the accelerated algorithms.

10. The most significant weakness or drawback of this submission is the lack of formal algorithm for the update of matrices $Y, Z, D, G$. Although the authors presented the online and batch techniques in section 5, these descriptions of the update scheme is just an outline and too introductory. We need a lot of details and formal description of the algorithm for the implementations or updates of matrices $Y, Z, D, G$. Without explicit algorithm like Algorithm 1 in the paper, there are a lot of unclear operations regarding the update of the corresponding matrices. The authors should present a formal algorithm regarding the implementations of these updates in the appendix. There also exists a lot of issues for these algorithms. For example, in the deviations of Iterates, Forward Estimates and Greedy updates after the line 239, it seems that column numbers of matrix $Y$ and $Z$ are $N + 1$ instead of $N$. The dimensions are not consistent. Also to implement the orthogonal iterate in the equation under line 236, it needs the matrix $P_{t - 1}$ defined in equation (12). However, this definition in equation (12) needs the operation of matrix multiplication and matrix inversion, which could be very expensive in high dimension condition. Also, the authors argue that Orthogonal Forward Estimates can ensure that the condition number $\kappa_D = 1$ and the norm of the error vector is small. But this observation is not obvious and need detailed explanations. There is no formal or strict mathematical proof and theoretical analysis to ensure that these updates of matrix $D$ could satisfy the requirements presented in the section 3.1. At last for the batch technique, the authors apply the QR decomposition. However, the computational cost or complexity of QR decomposition could be very high in the high dimension condition and make the implementation very slow in practice.

Limitations:
Please check the weakness section for limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In the paper, the authors propose a new framework that connects Quasi-Newton methods with Anderson acceleration. By exploiting the Cubic Regularization technique, the authors achieve new competitive convergence rates comparable to first-order methods in the worst case and second-order methods in the best case. The paper describes various problem setups with their respective convergence rates: non-accelerated methods for non-convex, star-convex, and convex functions, as well as an accelerated method for convex functions. The authors also propose different variants of approximation. Experiments are presented for both convex and non-convex functions.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
I believe the results presented in the paper are both original and significant. The novel connection between Anderson averaging, Quasi-Newton methods, and subspace sketch-Newton-type methods is both new and prospective. The provided proofs seem mostly correct.
Furthermore, introducing online approximation techniques adds value to the research. The thorough comparison of these techniques in practical applications showcases their effectiveness and underscores their relevance.
In conclusion, this paper contributes valuable insights to the field, and its innovative approach holds great potential for further advancements in optimization methods.

Weaknesses:
I may guess that the paper was created in a hurry before the conference deadline, which might explain why it contains a lot of small mistakes and misprints. While the mathematical results are sound, The mathematical results are sufficient but the overall presentation and clarity need improvement. A thorough review and revision by the authors are necessary to address these issues. Next, I will present some of the mistakes and misprints that I find.

1) Page 6, line 191, Theorem 5. The brackets on the right side are unnecessary. On the right side, $f(x_t)-f^{\ast}$ should be changed to $f(x_0)-f^{\ast}$. The same is in the Appendix. 
2) Page 8, line 239. All $Z_t$ and $Y_t$ contain $N+1$ elements, but they should contain $N$. So, I think the indices should be corrected.
3) Page 36, line 785. $r_i$ was not defined anywhere before, moreover, $r$ is used in other places for different things. It probably should be $G_i$.
4) Page 36, line 787. The last formula in the line is incorrect; the left side is a vector, and the right side is a number.
5) Page 36, line 791. The inequality is incorrect because $(r_i - \nabla^2 f(x) d_i)^2$ is a vector and shouldn’t be squared in such a way.
6) Page 36, line 795. The third transition is incorrect because $\|w\|$ is missing for the $L$ term.
7) Page 37, line 800. I believe it is better to explicitly prove that the term $\alpha^T D^TG\alpha$ could be upper bounded by $\alpha^T (D^T G+G^T D)\alpha/2$; otherwise, it may be confusing. 
8) Page 5, line 174. The notation $\varepsilon_t$ as a vector is confusing because a page before $\varepsilon_i$ was a number, the coordinate of a vector $\varepsilon$. 
9) Page 4, line 127. $x_{+}$ is defined in Theorem 1 but never used inside. So, I think it can be removed. On the other hand, $x$ is used inside but not really specified. 

Next are the moments that caused confusion for the first read. 

10) Page 3, Motivation. The transitions between line 97 to formula (2) and then to formula (4) are very confusing. It may help to specify what are the $\alpha_i$ and $P_t$ for that case. For formula (4), the author may say that it is a special case of formula (2) when $H_0=0$. It may help to understand such a transition.
11) Page 4, line 142, formula 10. The norm of $\|D\|$ is not defined, which may cause some confusion because $D$ is a rectangular matrix and could have different norms. 
12) Page 7, Algorithm 5. The notation $(M_0)_1$ is very confusing, especially when $M_0$ and $M_1$ also exist. Moving index $t$ of the step to the upper level, like $M^t_0$, may solve both this moment and $\varepsilon_t$ moment, but it is up to the authors how to resolve these issues with the notation.
13) Page 15, line 498, formula (19). As a small comment, I would suggest using the keyword “subspaced” or “sketched” instead of “stochastic” to avoid confusion with stochastic optimization methods such as SGD and others. 


Limitations:
Yes


Rating:
6

Confidence:
4

";0
CxzCoFDeQf;"REVIEW 
Summary:
The authors propose an ML based attack scheme for the learning with errors post quantum cryptography scheme. Their proposed attack is able to recover sparse, binary, and ternary Gaussian secrets in a more efficient fashion than the Salsa and Picante schemes in prior work. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
I have to admit that I am not an expert in post quantum cryptography, nor I am familiar with previous literature. Therefore, I am not able to confidently appreciate the strengths of the paper with respect to prior work. I like the clarity of the presentation in most parts; the paper is written well and provides a decent introduction to the problem of learning with errors.

Weaknesses:
1. To me the purpose of the paper is not clear. The authors claim that they improve on previous schemes. But these schemes have already demonstrated that LWE is breakable and won't be used in practice anyway, so what is the novelty of the paper? This should be made clear in a better way.

2. It seems that the difference to previous schemes is just a different matrix factorization in combination with techniques from [14] and [17], as explained in Section 2. I don't think that this contribution alone is sufficient for acceptance to NeuRIPS.

3. The part of the paper related to learning is not well developed and presented as an afterthought, despite the choice of the architecture may significantly affect the performance. The proposed scheme leverages the same architecture as in previous works without giving any rationale why a transformer architecture is the best choice and why attention is necessarily required. It seems that in the LWE encoding the noise sequence is drawn iid from a Gaussian distribution, so I don't understand why the learning architecture needs to impose a Markovian structure.

Limitations:
The authors have stated the need for post quantum cryptography, and thus, the societal impact of breaking classical cryptography via Shor's algorithm. Their work contributes to the task of finding the best post quantum crypto scheme which does not depend on factoring large numbers into products of primes.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper introduces a novel ML attack that has the capability to recover sparse binary, ternary, and small Gaussian secrets. The proposed approach exhibits effectiveness in attacking LWE systems with larger dimensions and smaller moduli, while requiring less preprocessing time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-written.
2. The SALSA VERDE achieves a harder attack with larger dimensions and smaller moduli, which makes the attack more practical on LWE-based systems.
4. The improved preprocessing method significantly speeds up the attack process, improving its efficiency.
5. The authors provide a theoretical explanation for the performance of the attack, adding to its overall understanding.

Weaknesses:
1. It would be beneficial to include an explanation or insights regarding the reasons why full recovery of Gaussian secrets is infeasible.

2. More ablation experiments and comparisons should be conducted to showcase the individual contributions and benefits of each proposed enhancement.

3. Regarding the model architecture, additional results or explanations are necessary to justify the replacement of the positional embedding method and beam search, especially when the performances are similar.

Limitations:
Please refer to the weaknesses section.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of adapting machine learning for the cryptanalysis of LWE --- an important cryptographically hard problem that forms the basis of several modern post-quantum cryptosystems. The LWE problem requires recovery of a secret \bf{s} given many noisy inner products b = \bf{a} \cdot \bf{s} + e mod q.  LWE is also an ideal candidate problem for cryptanalysis using ML techniques because of the linear structure in the computation. 

The high-level approach is to train an ML model using samples {(\bf{a}, b)}. Next, this trained model can predict a value close to \bf{a} \cdot \bf{s}  mod q for new values of \bf{a}.  Finally, this prediction can recover \bf{s}. This approach was first considered in prior work Salsa/Picante. Salsa could only work for binary secrets with 3/4 non-zero entries. In contrast, Picante supplemented the above approach with some insights from lattice algorithms. Specifically, Picante showed that by appropriately using BKZ lattice reduction algorithm in preprocessing, they could attack binary secrets but with larger non-zero entries. 

However, a significant gap remains between the parameter choices these attacks can handle and those used in LWE-based cryptosystems. This paper makes progress in reducing this gap.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The paper makes progress on various strengths. 
+ The paper can beat state-of-the-art lattice attacks not using ML. This demonstrated that ML could play a crucial role in the pipeline for improvement on lattice attacks. 
+ I like the effort in trying to give a theoretical explanation of why the attacks only seem to work for working

Weaknesses:
- The paper is a bit dense and talks about various parameters. Having some text that helps provide a unifying framework from the various parameter choices the authors try to use will be helpful. 


Limitations:
- The improvements in the paper are still not significant enough to show that ML/transformers can help significantly improve the attacks against LWE. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors present VERDE, an improved ML based attack on the LWE problem, a source of mathematical hardness underpinning post-quantum cryptography schemes such as Homomorphic Encryption. Their method improves upon prior versions of this form of attack, SALSA and PICANTE, by improving the runtime characteristics of the preprocessing and the performance and generality of the secret recovery step. Additionally, their error analysis yields insights into the hardness of SALSA-style attacks on the LWE problem, revealing an avenue for potential improvement based on improving the distribution of the training data to increase the chance of success for such attacks.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Introduces key performance improvements to the existing previous art by subtle but impactful modifications: reducing the complexity of the preprocessing matrix operations, and improving their floating point error tolerance.

- Secret discovery procedure is improved by applying a simple (but not trivial) fact from machine learning - generalization to a shifted distribution is hard. Switching to verification on a held out set of preprocessed vectors. i.e. ""in-distribution"", yields improved performance.

- Secret discovery procedure is extended to ternary secrets by taking a novel two step approach, where step one is similar to the binary secret recovery problem. (novelty is taken for granted, not very familiar with literature)

- (Highlight) Error analysis yields NoMod, which allows them to first descriptively develop an explanation for different success rates as a function of modulus, but then further admits a potential improvement via attempting to increase the NoMod factor of a given training set by permutation.

Weaknesses:
- The encoder-only model architecture proposal empiricals are mixed, and theoretical or intuitive justification is missing. Would be useful to include training runtime differences in Table 8 or their own table, my sense is that epochs until success doesn't tell full story since training complexity for the two architectures is likely different. Claim that the encoder only model may generalize to larger n is not sufficiently supported by Table 8, could be omitted, or such discussions moved to future work section as more experimentation is necessary. 

- (Minor) Rationale for omission of other secret recovery techniques - direct, cross-attention - is not fully convincing. Latter could be included for the seq2seq architecture as they are proposing the encoder-only variant in this work, rather than adopting it as a baseline.

Limitations:
- Key limitation, as the author's state, is gap between the settings they can attack successfully, and the realistic parameter schemes in standardized PQC. However, this is not actually any failure of the authors, and they do in fact achieve results at more realistic settings than prior work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper continues a recent line of work on leveraging machine learning (specifically, transformers) for cryptanalysis. The focus is specifically on the learning with errors (LWE) problem. The LWE problem is one of the core problems in lattice-based cryptography, and the basis for many of the recently standardized post-quantum cryptographic systems. The paper targets one particular version of LWE (with *sparse* secret keys).

The (search) LWE problem is to learn a noisy linear function, where the noise is over a ring: given samples (A, s^T A + e mod q) for a secret vector s, random (wide) matrix A, and small (discrete) Gaussian noise e, recover the secret s. The variant of LWE considered here is where the secret s is sparse. While this is not the standard version of LWE, it is used as an optimization for some fully homomorphic encryption (FHE) schemes.

Compared to previous approaches (Salsa and Picante), this paper describes generalizations to a ternary and (sparse) Gaussian error distributions as well as a better preprocessing step (responsible for generating more examples via the given LWE challenge, which is in turn used to train the transformer). The paper provides some empirical measurements showing that the attack is successful in recovering the secret for some small instances.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Studying and understanding the hardness of lattice assumptions is an important problem in cryptography, especially given the current move towards standardizing and implementing post-quantum cryptography. This paper improves upon an ML-based approach for understanding the hardness of certain LWE instances.

While previous approaches considered the more restrictive setting of LWE with sparse binary secrets, this paper extends the analysis to support LWE instances with sparse ternary secrets. This is a more useful setting as this is a more common mode used in certain FHE implementations (though far from what I would consider a ""standard"" LWE instance). The idea is to first use the binary distinguisher approach from prior work (which focuses on identifying the zero vs. non-zero indices of the errors) and then applies a post-processing step that identifies whether two non-zero entries in the candidate secret are equal or not (to identify the set of coordinates that are 1 vs. -1).


Weaknesses:
I found the contributions in the paper to be somewhat incremental relative to the previous work. The main improvements seem to be better preprocessing relative to the previous Picante system. While I appreciate that there is a significant speed-up in the new approach for generating LWE samples from a small initial challenge, the core idea is very similar: resample followed by lattice reduction. A more refined application of lattice reduction compared to Picante yields the performance improvement.

The second feature that distinguishes this work from prior work is the extension to ternary and sparse Gaussian secret key distributions. The extension to ternary builds on the binary distinguisher approach of prior work and essentially add a post-processing distinguisher that decides whether two components are equal/unequal. This technique is very tailored to the ternary case, and it is not clear how general it is or whether this has implications on more standard LWE secret key distributions (e.g., uniformly random or (non-sparse) discrete Gaussian).

From a cryptographic perspective, I view LWE with sparse binary/ternary secret keys to be a very risky assumption. The known reductions from LWE to worst-case lattice problems (e.g., gap shortest vector problem) apply only to LWE with uniform secrets (and also LWE with discrete Gaussian secrets). I do not see a solid basis for the hardness of LWE with very sparse binary/ternary secrets, except for the fact that we do not yet know how to break these assumptions. From this perspective, I am not sure if the results in the paper provide insight on the hardness of the standard LWE assumption where we have better evidence for hardness (and which is the basis of schemes in the NIST post-quantum standardization process).

The paper describes attacks on LWE with sparse Gaussian secrets, but this seems like an unusual choice. Normally, when we consider LWE with Gaussian secrets, there is no sparsity condition. Are there concrete settings where this version of the LWE assumption is used? The attack here seems to rely on identifying the non-zero entries (using the binary distinguisher approach from earlier) and then brute force guessing the values in the non-zero indices. This seems rather inefficient and unlikely to scale to even somewhat-dense solutions. The attacks in the paper only consider a very sparse regime, which again corresponds to a setting where I consider an attack to not be particularly surprising.

From the machine learning perspective, I think the paper mostly relies on existing models and architectures as opposed to introducing new architectures tailored for cryptanalyzing LWE. I think this is a fine approach to take, albeit one with less novelty from a machine learning perspective.

Limitations:
I found the comparisons with other classical approaches to be one of the more interesting aspects of this work, and would have liked to see some of this discussion in the main body of the paper. It would be great if the paper could provide some more fine-grained information on the amount of compute needed for the ML-based approaches and what the implications these attacks have for improving our hardness estimates for LWE with sparse secret distributions.


Rating:
4

Confidence:
4

";1
tpIUgkq0xa;"REVIEW 
Summary:
Large language models lack expertise skills and this is reflected in their limited capability for arithmetic etc.

The paper proposes a method to integrate a CoNN (compliled neural networks) into an LLM via gating. Such integrations allow better performance for rule intensive tasks such as symbolic reasoning, arithmetic reasoning etc. To perform this, the paper proposes a gating mechanism although the implementation seems rule based triggering (line 167).

With the proposed mechanisms, the authors evaluate on arithmetic tasks (5.1) where the model achieves consistently 100% accuracy. On symbolic reasoning (5.2) and arithmetic reasoning, the method also performs better than finetuning appraoches in terms of performance or data efficiency. 







Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper correctly identifies the limitations of LLMs and propose a novel approach to tackle the problem. The solution consists of a neural machine that serves as expert for symbolic/arithmetic operations.

Empirically, the paper demonstrates strong results over competitive baselines on arithmetic/symbolic reasoning approaches.

Weaknesses:
The novelty introduced in the paper doesn't quality a Neurips paper. The novelty of the paper concretely is using the rule based trigger to combine a CoNN with an LLM; neither components consist of the paper novelty. 

There are various presentation issues that make the paper quite hard to follow:
- The second contribution item is put in Appendix, I read it but please note that even reviewers are not obliged to read those materials in Appendix to judge the paper. Related to this remark, CoNN is only introduced and referenced, for people not familiar with the technology, there is nowhere in the paper to know how it works.
- Section 3.2 introduces the gating between LLM and CoNN, I think equation (1) has a flaw since it involves choose argmax from a matrix which seems ill defined and I think the authors mean to concatenate HL and HC instead. The gradient flow (3.3) doesn't give further insight rather than it is a gating mechanism. 
- I am confused by the illustrative figures in the paper. Figure 2 has the input text on top which is easily confused to a paper title and I feel Figure 1 is not relevant to the paper.



Limitations:
Unless Neural Comprehension machines are widely used, I don't see why this approach is particularly useful: I don't see the advantage compared to API calling approach adopted by today's industry. Particularly, as shown in this paper, many operations have to be implemented individually (subtraction, addition, etc.)

Rating:
3

Confidence:
3

REVIEW 
Summary:
Authors have proposed a novel way to augment large language model called Neural Comprehension to improve symbolic reasoning in tasks where rule-based execution is required by design such as numbers summation. The core idea behind their method is to augment the LM with compiled NN (CoNN) for a specific task is a manner of mixture of experts where they design a policy which determines LM or CoNN will be executing the next token prediction at each time step. In addition, they described how their method could be used with in-context learning (ICL). Authors perform set of experiments in symbolic operations (parity, reverse. addition, subtraction), symbolic reasoning (concat, coin flip) and arithmetic reasoning. They show empirically how their method outperforms stand-alone LMs finetuned on corresponding tasks data. Finally they show a potential of combining multiple CoNN with the given LM to increase the task capability of the final Neural Comprehension model.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This work proposes an original way to combine LMs with CoNN networks and analyze the performance using multiple correlated or uncorrelated CoNNs together. 
The wide range of symbolic experimental tasks show that authors performed high quality experimental investigation.

Weaknesses:
I think major weaknesses of this work is (1) a hardcoded structure of CoNNs under consideration and (2) hard coded policy of choosing the LM vs CoNN component by connecting that to task-based properties. I believe the most interesting part would be to learn the beta factor which also seems to be very challenging. 

Authors claimed that their work suggests potential improvements from using Neural Comprehension in other tasks, but they did not mention how to get CoNNs for these tasks? In general, the discussion about CoNN design and implementation is somewhat skipped in the paper while it seem to be a crucial factor in this paper's impact.

Limitations:
Authors discuss some statistical limitation aspects of LMs in the appendix and how their proposed method alleviates that. However, I did not find explicit discussion of limitations of their own approach except for describing future work. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
While Large Language Models show promise for a wide swath of tasks, they are lacking when applied to symbolic reasoning tasks. To overcome this limitation, the authors propose to employ Compiled Neural Networks (CoNNs). They create networks specialised to arithmetic and symbolic tasks and propose a mechanism by which an LLM can propagate the gradient through CoNNs to better learn to solve symbolic reasoning tasks. They demonstrate improvement in pure symbolic manipulation (parity and reverse), arithmetic (addition and subtraction), and more complex symbolic reasoning (coin flip and last letter concatenation). They demonstrate better generalisation to out-of-distribution examples (proxied by digit length or sequence length for the first four tasks), a considerable improvement on LLC, and parity with a larger LLM when augmenting a smaller one (T5 small + NC v vanilla T5 large). The improvements are comparable to external tool-based approaches, however, hold a promise of better integration (as gradients can propagate without surrogates), and the mixture-of-experts style combination can be learnt rather than rule-based.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- By construction, CoNNs are interpretable from their basic building blocks, ensuring that paths that do go through them can be interpreted according to the rules they encode.
- The reduced number of parameters holds promise for reducing the cost of language models (relative to GPT-3)
- Even with simple gating, there is a non-trivial improvement on tasks when multiple CoNNs are employed (Section 5.4)

Weaknesses:
- There is an implicit assumption on practitioners to know what rules to expect and generate appropriate networks that then get used MoE style.
- Expanding on the previous, practitioners should be able to translate their rules, from, for example, regular expressions, to RASP to enable compilation to a NN.

Limitations:
The authors have address most limitations that arose as questions during the reading of the paper spare the one I listed under Questions with regards to the cost of producing CoNNs.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper shows a strategy to improvise ICL by including CoNNs in the learning pipeline, which enable the LM to learn symbolic operations in addition to standard autoregressive LM generation. The resulting model is trained by a hand designed gradient accumulation technique and results are compared on symbolic tasks.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The paper shows how symbolic tasks can be included with general autoregressive training and therefore provides a way to train models following a few fixed symbolic tasks in mind. The paper provides a solid training recipe with proper mathematical justification. The results correlate with the claims and justify using the method.

Weaknesses:
It is unclear from the paper how different gating mechanisms are being derived in this network and how they are being included in the training framework. The authors say that \beta is not learned in the algorithm and essentially rule calculations are assigned to CoNNs. If that is true, the applicability seems a bit ad-hoc as different rules will then need to be hand written and not derived, and the benefits of the network will only be applicable to scenarios that are symbolically encoded. In other words, this seems to be a scalability challenge in terms of letting LMs learn rules. The experimental evaluation to justify the benefits seem very limited.

The paper also suffers from poor presentability with multiple grammatical errors, spelling mistakes etc. Also more context on training the overall CoNN+DNN system is missing from the paper or appendix. 

Limitations:
None

Rating:
4

Confidence:
4

";0
dw6xO1Nbk5;"REVIEW 
Summary:
This paper studies the generalization error of neural operators that contain kernel operations. Under the basic setting of neural operators (such as FNO), this paper establishes upper bounds of the excess risk of neural operators. How to apply NO to irregular domains, and the error analysis of super resolution are also discussed. The techniques of this paper is rather standard. Some parts of the upper bounds are not clear. I think the contribution of this work is incremental.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper provides an upper bound for the excess risk of neural operators.

2. The upper bound in this paper improves the one in [14]

3. Extension of NOs to irregular domains is discussed.

4. The super resolution error is studied.

Weaknesses:
1. The technique used in this paper is pretty standard. The author should emphasis their novelties. 

2. Theorems in this paper is not impressive and is unclear. For example, the error bound in Theorem 3.1 depends on the network structure, parameters, and the covering number of the space $B$, which is infinite dimensional. If the norms of network parameters are all larger than 1, the upper bound can be very large. Since the space $B$ is infinite dimensional, the covering number can also be very large, which makes the result less attractive. On the other hand, the magnitude of the training loss is also unclear. I believe the training loss depends on the network's width and depth, which relates closely to the upper bound in this paper. It will make the paper much stronger if these relations are analyzed clearly and a more clear upper bound is derived.

3. For applying NOs to problems with irregular domains, the authors only give an example on unbounded domain. The case for arbitrary irregular domains is only briefly discussed. However, the authors claim the construction of NOs on arbitrary domains as the second contribution. More details on this part should be given.

Limitations:
Limitations are not disucssed in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
A theoretical analysis of neural operators (NOs) is presented, which provides further insight into the construction and performance of NOs.  The theoretical insights are validated by numerical experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A thorough theoretical analysis of NOs is presented, with significant improvements in the tightness of generalization bounds compared to prior work.  The implications of the theory and insights that may be drawn are discussed.  While some of these may seem obvious, it is important to base such insight on robust underlying theory, as presented.  The insights gained from the theory are validated by numerical experiments.

Weaknesses:
The summary of the overall model is not the most clear (Equation 1), which suggests the non-linear activation is applied first, rather than following the kernel and linear transforms.

Typo: ""project"" -> ""projection""

Typo: sometimes $\mathcal{L}$ used to represent loss, sometimes $L$.

Typo: ""Conbining"" -> ""Combining""

Limitations:
No special societal concerns.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The image super-resolution (SR) is a recurrently used task, nowadays, since the SR images can improve the accuracy of downstream tasks like object detection. Many proposals rely on heavy Deep Learning models or lightweight models based on efficiently designed architectures. This work studies neural operators based on examining the orthogonal base. The operations proposed in the manuscript, according to their theorems and demonstrations with the appropriate orthogonal bases and the grip points, reduce the time of convergence and make the network adapt faster to the  irregular domains.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposal section in the manuscript  is easy to read and follow.
* Theorem 3.2 and 3.3 are properly defined. For theorem 3.3, this will assist in re-planning new proposals for improving image SR accuracy.

Weaknesses:
Despite a short evaluation performed that shows a better performance of the proposal, I feel it is not sufficient to validate the generalization ability in the super-resolution task. To this end is necessary to validate with the standard evaluation metrics used in the SR domain. Personally, I feel that more explanation and motivation is needed for equation 3.

Minor error:
The machine learning articles, mostly, are organized Introduction, related works, the proposal, material, and experiments. It should be convenient to use this structure in the manuscript.

Limitations:
There are no limitations addressed in the manuscript

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this paper, the authors propose to analyze neural operators from the orthogonal bases in the kernel operators, which helps to guide designing kernel operators and choosing grid points, analyzing generalization and super-resolution capabilities, and adapting neural operators to irregular domains.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This is an interesting paper providing not specific models but new analysis perspectives and design principles on Neural Operators.


Weaknesses:
- I feel there are still gaps between the overall claims, the theoretical results in Section 3, and the implication and experiments in Sections 4&5. For example, the ""NOs on Unbounded Domain"" section is not what I would expect for ""adapting neural operators to irregular domains""; the ""Combining Multiple Bases"" section seems more like analyzing kernel operators, instead of providing insight and principles in ""designing kernel operators"".
- The writing is not always good. For example, in the abstract, similar contents reappear three times.


Limitations:
This work is a bit beyond my capability. I think it is an interesting and important work, but I don't feel like it has reached its perfect state.


Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors consider a new perspective to neural operators, by examining the role of orthogonal bases. The kernel operators are constructed such that their eigenfunctions are predefined orthogonal bases, with the eingenvalues as trainable parameters. That is, a neural operator can be seen as a mapping from input coefficients to output coefficients of the orthogonal basis functions. The authors show several theoretical results using this new perspective, backed by empirical results:
- Improved generalization bounds.
- Super-resolution error bounds.
-  Irregular domains - they show that neural operators can be extended to irregular domains using random Fourier features and polynomials on irregular domains.
- Choosing other orthogonal bases. The authors show that Fourier bases can be combined with wavelet or polynomial bases.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper can be impactful both because of the four concrete results they show (generalization bounds, super resolution bounds, irregular domains, and choice of orthogonal bases), but also because of the novel perspective of neural operators. In particular, the novel eigenvalue / orthogonal basis-based perspective of neural operators can be a useful view for studying neural operators in the future, both theoretically and empirically.

While several other works have studied neural operators for irregular domains / topologies, the other results by the authors are much more novel. Not much prior research has studied generalization bounds, super resolution bounds, or combining bases before this work.

Weaknesses:
- Some of the results are fairly obvious or have only limited empirical value. For example, the super-resolution theorem/experiments, although it is novel to study super resolution, the main punchline is that super-resolution depends on the accuracy of the integration method, and the density of the low-resolution grid (i.e., the results in Figure 2), which is not so surprising.
- Section 4 shows implications of the theory, motivating the experiments in the next section. However, some of these experiments have only tangential ties to the theory. For example, “Guiding the choice of Orthogonal Basis” is justified because Theorem 3.1 impacts their expressiveness. I would argue that we would decide to research the choice of orthogonal basis even if we didn’t see Theorem 3.1 first.
- It would be nice if there was a bit more intuition for each of the proofs, in the main text.
- Did not discuss limitations.
- The authors could have released the code (anonymously during submission).

Limitations:
The authors did not discuss the limitations of their work, and they answered “no” without explanation in the OpenReview paper checklist to that question. The NeurIPS call for papers states that authors can answer no to a checklist question, provided they give a good explanation. But I cannot think of any explanation why an author would be justified in not describing the limitations of their work; I think it is strictly better to do so.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper attempts to provide theoretical analyses of Neural Operators (NOs) mainly considering the following aspects: 1. The generalization bound of NOs; 2. The discretization error of NOs; 3. The ""super-resolution"" (trained on low-resolution grid then evaluate on the high-resolution grid) error of NOs on the uniform grids. Several numerical experiments are carried out to validate the theory. The theory results also motivate the authors to come up with improved designs on existing NOs, including bases/integration schemes that suit specific PDE better, and generalization to unbounded domains

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* A tighter and more general generalization bound compared to prior works;
* Advance in the understanding of discretization errors and super-resolution errors in NOs;
* The improved design of NOs is validated by extensive numerical experiments.


The paper in general has analyzed several crucial properties of NOs, which is timely, and its practical implication I believe will benefit the scientific ML community. Specifically given that the concept of doing super-resolution with NOs is often vaguely studied in many relevant works.

Weaknesses:
The overall presentation of the paper is fairly clear and easy to follow. However, some of the discussion in the experiment section is relatively vague, especially in section 6.3. As revealed by Theorem 3.3, the integration scheme along with basis selection has a crucial effect on the super-resolution error. The authors only briefly covered what basis and quadrature rule are used for the harmonic oscillator example, but then skim through the 3D DFT experiment, which makes it difficult to interpret the improvement in the results shown in Table 1. These details might be trivial for someone who is an expert in DFT, but they can help other audiences better understand the practical implication of the theorems.

Limitations:
The paper did not discuss any limitation. While this is somewhat understandable as a theory paper, the authors can discuss the limitation of their theory results in terms of the application scope and its assumption.

Rating:
5

Confidence:
4

";0
SfXjt1FtMQ;"REVIEW 
Summary:
This paper introduces the Gaussian multi-Graphical Model (GmGM) as a novel method to construct sparse graph representations of matrix- and tensor-variate data. It simultaneously learns the representation across several tensors that share axes. The authors demonstrate that GmGM outperforms previous methods in terms of speed when applied to matrix data. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	GmGM extends the application of Gaussian Graphical Models to multi-tensor datasets, presenting a novel approach in the field.

2.	GmGM exhibits significantly improved speed compared to previous methods when dealing with matrix data.

3.	The results of GmGM on five real datasets are well explained.

4.	Especially, I appreciate the comprehensive discussion provided in this paper. The authors present cases where the results are excellent, as well as cases where the results are not as impressive, such as the performance on higher-order tensor data (fig 4b) and the E-MTAB-2805 dataset (fig 6a). This in-depth analysis helps readers gain a better understanding of the method and be aware of the situations in which it should be employed.

Weaknesses:
One major concern I have relates to the evaluation. Although the authors present many intriguing findings on the datasets, it would be beneficial to include some more quantitative analysis.

Limitations:
Yes. It is well discussed in the study.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose Gaussian multi-Graphical Model (GmGM), a novel approach to constructing sparse graph representations of matrix- and tensor-variate data. It stands out from previous models by learning representations across multiple tensors that share axes simultaneously, a feature crucial for analyzing multimodal datasets, particularly in multi-omics scenarios. The GmGM algorithm utilizes a single eigendecomposition per axis, which results in a significant speedup over previous models. This efficiency enables the application of the methodology on large multi-modal datasets, such as single-cell multi-omics data, a task that was challenging with previous approaches.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Fair and Interesting Motivation: 
The paper's motivation on model multi-tensor decomposition with shared axis is rooted in the real-world need for handling multi-omics scenarios, which often involve multi-tensor data with shared axes. The GmGM is introduced as a solution, addressing a significant gap in existing data analysis methodologies and providing a fair and interesting motivation for the study.

2. Reasonable solution and impressive improvements in Efficiency
The GmGM model stands out for its impressive efficiency improvements, achieved through the use of the KS decomposition of the precision matrix and transiting it to the eigen-decomposition over each dim. This approach results in a substantial speedup over previous models, enabling the handling of large multi-modal datasets, This efficiency, coupled with the model's ability to maintain state-of-the-art performance, underscores the strength of the paper.

Weaknesses:
1. **Limited Technical Contribution**

While the problem setting proposed in the paper is reasonable, the algorithm's strict assumptions about data integrity (no missing data) and quality (no noise) somewhat limit its potential for broader application. The authors are encouraged to consider relaxing these assumptions or proposing strategies to handle missing data and noise, which are common issues in real-world datasets. Addressing these issues could significantly enhance the model's practical utility and broaden its applicability.

2. **Improvements Needed in Representation and Flow**

The paper could benefit from substantial improvements in its representation and flow. The omission of important concepts and content significantly hinders reader comprehension. Some sentences appear casual and can lead to confusion. The overall logical flow of the paper is not clear, making it difficult to follow. This is particularly evident in the following areas:

   - Concepts such as the Kronecker product and Gram matrix are not clearly introduced.
   - Many notations and their subscripts and superscripts in the algorithm table are not clearly defined.
   - The task setting and metric definition in the experimental section are vague, reducing the persuasiveness of the validation part.

Overall, the authors are encouraged to make a concerted effort to reorganize and polish the paper's presentation, improve the flow, and highlight the key points of the work and problem. This could significantly enhance the readability and impact of the paper.

Limitations:
See weakness parts

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes the Gaussian multi-Graphical Model, a novel method to extend the use of Gaussian Graphical Models to multi-tensor datasets. It generalizes Gaussian graphical models to the common scenario of multi-tensor datasets. For the single-tensor case,  the proposed algorithm is faster than prior work while still preserving state-of-the-art performance.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper considers an interesting and still challenging topic, extending conventional Gaussian graphical models (GGM) for complex systems like multi-modal data models. The paper has been generally well-written and the problem has been clearly defined. Indeed, the theoretical parts that extend the GGM to multi-tensor datasets have proper quality.  This algorithm is significantly faster on lower-order tensor data (reported for the synthetic data sets) and its efficacy is slightly better in the real-world data sets. 

Weaknesses:
- Some parts of the paper should be checked again. For instance, line 52 starts to explain the computational costs of the state-of-the-art methods. The parameters n and p have not been defined before. It seems it uses the defined parameter in the main reference paper (Kalaitzis et. al. 2013), where n and p are the numbers of observations and features, respectively. Indeed, the computational costs of the other baselines need a piece of clarification. For instance, O(n^2 * p^2) in BIGLasso represents the number of non-zeros in the Kronecker-sum (KS) structure. It would be better if the authors consider the full cost of the algorithm for the proposed method and available baselines.

- The paper models each tensor as being drawn independently
 from a Kronecker-sum normal distribution. It makes sense to see this assumption reduces the computational cost at least in small-order data sets. However, it does not describe how this strong assumption still preserves state-of-the-art performance.

- As has been reported in the paper, the proposed solution can not improve the complexity of higher-order tensor data sets (fig. 4b). Indeed, its performance can not significantly outperform the other baseline (Fig. 5a). By decreasing the sparsity, the performance of the model suffers and it seems it works properly only on high sparse graphs (Fig 7).

Limitations:
The authors addressed the limitation of the work in the paper. 

Rating:
6

Confidence:
3

";0
2ZtGWNn37W;"REVIEW 
Summary:
In this manuscript, the authors propose a multi-fidelity active learning scheme based on GFlowNets.
The work mainly aims to tackle scientific discovery problems, where one often faces exploring a huge high-dimensional space to identify novel, diverse, high-quality solutions.
In many scientific applications, accurately evaluating the quality of the potential solutions (or properties/characteristics of novel candidates) is expensive, hence lower-fidelity surrogate models are frequently adopted for efficient cost-effective evaluation.
The current work investigates how to carry out active learning - more specifically, in the context of de novo query synthesis instead of a pool-based active learning scenario - when multi-fidelity oracles/surrogates are available in order to efficiently identify a diverse set of high-quality candidates within a given budget.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The multi-fidelity active learning scenario investigated in this work is of interest in various scientific discovery/design scenarios.
This work proposes how GFlowNet, a popular generative flow network model that can serve as an amortized sampler for drawing high-reward samples from a high-dimensional distribution, can be utilized for active learning under multi-fidelity setting.
According to a relatively simple and intuitive procedure outlined in Algorithm 1, this work shows that the proposed MF-GFN has the potential to identify a diverse set of high-scoring candidates at a lower acquisition cost compared to active learning schemes that rely on a single-fidelity acquisition function.


Weaknesses:
Although the proposed approach MF-GFN is reasonable, there are several major concerns regarding the current study.

1. While the authors claim that the proposed MF-GFN outperforms single-fidelity active learning as well as other multi-fidelity AL schemes, the evaluation results presented in the current manuscript (e.g., Figure 1, 2, 3) are not yet very convincing.
It appears that MF-GFN doesn't necessarily outperform other alternatives in a consistent manner, and when it does, the performance gain doesn't seem to be very significant.


2. For single-fidelity AL, the authors only consider the use highest fidelity oracle, which quickly consumes the AL budget.
Unless the high-fidelity samples lead to substantial learning improvement, it would be more desirable to use low-fidelity samples.
Of course, the actual relative value of high-fidelity vs. low-fidelity samples (considering the acquisition cost) would be different case-by-case, hence it is unclear whether the current examples provide fair comparison between MF vs. SF active learning.
To be fair, single-fidelity active learning performance should be evaluated at each of the considered fidelities to provide a more comprehensive picture of how SF AL would work at different fidelities.


3. On Hartmann function, MF-PPO clearly outperforms MF-GFN significantly. What are the characteristics of the Hartmann function that may lead to this discrepancy unlike some other examples considered in this work?


4. Comparisons across different examples should be more consistent.
Currently, different sets of methods are evaluated in different examples, and different K values were used for evaluating the top K samples.
This looks quite arbitrary and unless there is a clear reason for these choices, the same set of methods should be evaluated based on the different examples using the same K value (or same set of K values).


5. There should be further discussion on the computational cost of fitting h to D and retraining the GFlowNet in each iteration (of batch acquisition).
Considering that the multi-fidelity oracles may often be computational models with different computational cost, it may be sometimes (or often) more desirable to reduce training the GFlowNet multiple times and use this computational budget for a larger number of oracle evaluations.
As a result, this training cost should be considered in practice when designing and performing AL campaigns, and these practical aspects need to be discussed further.


6. There is currently no discussion regarding the impact of the batch size on the performance of MF-GFN and its comparison to other alternatives.


7.  Although (2) is a simple yet reasonable way of evaluating a potential sample considering both the acquisition cost and its value, it is not clear whether this would be a reliable estimate of the ""value"" of a given sample normalized by its acquisition cost.
There should be a better justification for this cost-adjusted utility function or at least some empirical evaluation.



Limitations:
The manuscript briefly discusses some potential limitations of the current study and its broader impacts.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduce an algorithm for multi-fidelity active learning with GFlowNets and demonstrate that the proposed algorithm outperforms the baseline methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well written and includes two synthetic benchmark tasks and four practically relevant tasks for extensive experiment analysis.

Weaknesses:
The novelty of the paper's contribution may be questioned as it appears to bear similarities to existing works such as BMFAL (Li et al., 2022) and D-MFDAL (Wu et al., 2021) with the exception of the GFlowNets component.

Regarding the experiments, several existing multi-fidelity active learning baselines are missing, including DMFAL (Li et al., 2020a), BMFAL_Random (Li et al., 2022a), BMFAL (Li et al., 2022), D-MFDAL (Wu et al., 2021), and MF-BALD (Gal et al., 2017). Furthermore, the variants of the proposed method, namely GFlowNet with random fidelities and GFlowNet with the highest fidelity, seem more akin to an ablation study rather than proper baseline comparisons.

Additionally, the evaluation metrics employed, such as the mean score and mean pairwise similarity, are specific to GFlowNet. To ensure fair comparisons, it is recommended that the author considers adopting the evaluation metrics used in previous multi-fidelity active learning papers.

Limitations:
The limitations are included.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors adapt the standard GFlowNet framework to include a fidelity measurement for the oracle, and demonstrate on synthetic, biological and chemical datasets that, in almost all cases, MF-GFN outperforms relevant baselines in terms of achieving sampling performance within a fixed budget.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
### Originality

The paper applies multi-fidelity ideas from Bayesian Optimization to the GFN framework. This is the first time such a thing has been done, and the standard GFN framework needed to be updated to sample a fidelity as well.

### Quality

The quality of the paper and results is sufficient for publication. The synthetic benchmarks are standard for this area of research, and the biological examples for aptamers and peptides are biologically relevant. The QM results for small molecules are less relevant than other tasks (e.g. ADMET in drug discovery) could have been.

### Clarity

The paper is very well written and easy to understand.

### Significance

The paper is moderately significant since the fidelity configurations are contrived and simple and likely do not represent experimental drug discovery fidelities (see Weaknesses below).

Weaknesses:
- the multi-fidelity framework is rather simple and would apply in situations when the oracle is computational (e.g. running DFT) rather than experimental (e.g. running biochemical assay), since computation costs are easily assumed to be uniform and applicable per sample, whereas experimental costs are more complex, can require batch acquisition rather than single sample, and the results can be far noisier in general. While it is infeasible to perform such a study for this paper, a synthetic example could be constructed with such properties.
- In the main paper, each task is only run with a single multi-fidelity configuration, but it would be interesting to run the same task but with different MF configurations in order to understand how the distribution of fidelities effects convergence per fraction of budget spent. Appendix D.1 does this once for the aptamer example, but a more comprehensive study, perhaps on synthetic data, would be enormously instructive.

Limitations:
There is a sufficient discussion of limitations in the paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this submission, the authors proposed to apply GFlowNets as a sampler to sample for active learning based on the selected acquisition functions, instead of directly optimizing them in the procedure of Multi-fidelity Bayesian Optimization (MFBO). Even though focusing on active learning applications, the authors presented the method more in the MFBO setting, which aims at optimizing a target function by iteratively querying it as well as several different low-fidelity low-cost surrogate functions. In this work, a multi-fidelity Gaussian Process was used as multi-fidelity surrogates and multi-fidelity MES was chosen as the acquisition function. The main focus of the submission is to adopt GFlowNets for MFBO to query according to the acquisition function and the authors claimed that it improves preferred diversity of queries samples. 

The authors tested the performance of the proposed method with single fidelity BO with GFlowNet, random fidelity with GFlowNet, random selection, and Multi-fidelity PPO on synthetic Branin and Hartmann functions as well as real-world tasks on DNA Aptamers, protein design, and small molecule design. Although the proposed method does not always achieve the best performance, the authors claim that it has better sample efficiency with comparable score optimization performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
GFlowNet was implemented for MFBO, specially for active learning tasks. The authors tested the performance of the proposed method with single fidelity BO with GFlowNet, random fidelity with GFlowNet, random selection, and Multi-fidelity PPO on synthetic Branin and Hartmann functions as well as real-world tasks on DNA Aptamers, protein design, and small molecule design. Although the proposed method does not always achieve the best performance, the authors claim that it has better sample efficiency with comparable score optimization performance.

Weaknesses:
1. The main concern is that the methodological contribution is limited. The authors are mostly using the existing acquisition functions as the reward function in GFlowNet to solve multi-fidelity active learning. There is not much theoretical analysis of this GFlowNet-based active learning strategy throughout the submission. A more serious concern is that the submission is very much similar as [1], by considering multi-fidelity settings but the fidelity was simply considered as an additional input variable. The whole pipeline and all the methods are very much the same. 

[1] Jain, Moksh, et al. ""Biological sequence design with gflownets."" International Conference on Machine Learning. PMLR, 2022.

2. The explanation of using GFlowNet can be improved. As described in the 194th line of the main text, the joint posterior distribution of the input $X$ and fidelity index $m$ was modeled but with the constraints that a fidelity $m>0$ of a trajectory must be selected once and only once, from any intermediate state. The authors may want to first define the DAG (Directed Acyclic graph) of this GFlowNet model, explicitly explain the allowable state transition, forward/backward policies, etc. 

3. The design of the multi-fidelity kernel may need further explanations. Especially, adding fidelity indices as additional input by adopting  $K_2(m_1, m_2)$ defined between lines 559 and 560 of Appendix does not seem to capture the difference of $m_1$ and $m_2$ and not really invariant when permuting the fidelity indices. How can this guarantee to select appropriate fidelity to query? The authors may want to discuss the intuition behind this design. Also the reference 68 does not exist in Appendix or main text.

4. The provided code does not seem to have GFLowNet-based implementation but only has random and PPO implementations. 

5. Since the diversity of the queried data was advertised as a reason for utilizing GFlowNet, the authors may need to provide such validation results in the main text, for example by explicitly comparing the diversity at the end by different competing methods for the tasks. Also it may be better to provide more information on the PPO setup, for example whether it selects one sample or a batch of samples, because the `diversity' should be automatically taken care of by the acquisition function in the sequential setting and it may be tricky to select a batch of samples in this case.

6. The author used the Top-K score of K candidates in each active learning round. The hyper-parameter K, and other hyper-parameters in GP kernels, could be influencing the results and conclusions. Sensitivity analysis should be provided.

7. If GFlowNet-based sampling is the important contribution, then other MCMC sampling methods by acquisition functions besides PPO should be also benchmarked. 

8. There are language errors, for example, 1) there are typos in the 148th and 149th lines of the main text; 2) in the 610th line of the Appendix: ‘where C is the cost if the highest fidelity oracle’ should be corrected as ‘where C is the cost of the highest fidelity oracle’; and many others. 

Limitations:
Both methodological limitations and societal impact were discussed. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper offers a new framework for multi-fidelity active learning using Generative Flow Networks. Given the recent success of GFlowNets as models for sampling diverse candidates among terminating states in a DAG, the authors attempt to leverage this property to put a new spin on active learning, where instead of sampling from a pool of unlabeled candidates, new objects are sampled from the entire construction space. Furthermore, the GFlowNet is also responsible for sampling the fidelity at which to evaluate the object, which they later show can provide advantage over just sampling the objects themselves. Two important components of this framework are 1) the multi-fidelity GP proxy model, which is a surrogate for the true measure of 'goodness' of a sampled object, and 2) the multi-fidelity acquisition function, which is used as input to the reward function of the GFlowNet to encourage exploration of the construction space. The authors then justify this framework with results from a variety of domains showing that MF-GFN offers promising results in terms of its ability to effectively leverage the lower fidelity oracles to reduce the total cost of exploration compared with only using a single oracle.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
The paper provides an original way of injecting the desirable properties of the generative flow network into the hot field of active learning, which is especially important for guiding modern research in being able to know what experiments to run next. The paper communicates the main ideas relatively clearly and effectively, and is not limited by restrictive assumptions. The paper also backs up their claims with experimental evidence from a variety of domains.

Weaknesses:
I felt like the biggest weakness of the paper was probably the lack of thorough results and as noted in the Limitations section, the lack of practical oracles. To be an effective framework for active learning, I think it would have been much more compelling to tackle some real problems that domain specific scientists are working on, instead of the synthetic (Branin and Hartmann) tasks where MF-PPO seems to do just as well as the proposed model.

Additionally, I felt that the paper was a bit rushed, as there were some glaring typos (e.g. line 303) and some important aspects that were not entirely clear, such as the actual reward function that was used by the GFlowNet. This was not made explicit until the supplementary material and made it difficult for me to understand how the acquisition function precisely tied into the rest of the framework.

The last thing that I think would be helpful for the reader to understand is to give details on the budget; things like how long does it take the GFlowNet to sample one object, how long does it take for the oracle to evaluate the object, etc. would be helpful for the reader to put the timescales of things into perspective.

Limitations:
yes.

Rating:
7

Confidence:
3

";0
maSAKOKXTi;"REVIEW 
Summary:
The paper introduces a Generative Evolution Optimization (GEO) algorithm to black-box optimization, introducing. The GEO algorithm is claimed to combine the strengths of Evolution Strategy (ES) and Generative Surrogate Network (GSN) to address the limitations of Bayesian optimization and other existing methods. Some benchmark functions are tested to verify the performance of GEO.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Originality: The paper introduces GEO which combines the strengths of L-GSO and Evolutionary Generative Adversarial Networks (EGAN). I did not see such method before.
Quality: The authors provide explanations of the GEO method, including its foundational concepts, operation steps, and algorithmic structure. Some benchmark functions have been tested.
Significance: The paper addresses a significant challenge in the field of black-box optimization, e.g., the optimization of non-convex, high-dimensional, multi-objective, and stochastic target functions. 


Weaknesses:
Some claims and concepts are not adequate, like the O(N) complexity. Without the target of finding global optimal, we can design various methods that achieve O(N) complexity easily.
Some related works are not cited adequately, like Xavier and He initialization.
The experiments seem to be limited to specific test functions. Performance on such few benchmark functions are not convincing
The paper discusses the potential application of GEO in other areas of machine learning, such as reinforcement learning. However, it does not provide any empirical evidence or case studies to support these claims. Including real-world applications or case studies could strengthen the paper's significance and practical relevance.
The paper does not clearly outline the limitations of the GEO method, which could be beneficial for future research and application of the method.
The paper mentions the tendency of GEO to collapse towards one side while optimizing certain functions, but it does not delve into why this happens or how it could be mitigated. A more in-depth analysis of this issue could improve the paper's quality.


Limitations:
Societal impact of the work is not discussed in this paper. Furthermore, limitations of the proposed technique are not discussed clearly.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper investigates a new integrated optimization method targeting at black-box optimization within high-dimensional spaces scenario, called Generative Evolutionary Optimization (GEO).
Different from the popular black-box optimization method - Bayesian optimization, GEO exhibits a linear time complexity.
Intrinsically, Geo a black-box optimization method that combines an evolutionary strategy with a generative surrogate neural network (GSN) model,
and the two basic components could function in a synergetic manner.
Specifically, evolutionary strategy helps to deal with the stability of the surrogate network training for GSN,
while GSN improves the mutation efficiency (sample efficiency) of the former.
Since the fitness results are combined and ranked using non-dominated sorting in GEO, it can be applied to multi-objective scenario.
Besides, the age evolution strategy is leveraged to dominated sorting step when the target function is stochastic.
Finally, the experimental findings reveal that GEO can accomplish the mentioned objectives: optimizing convex, high-dimensional, multi-objective, and stochastic target functions while maintaining O(N) complexity.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. This paper is well-written and easy-to-follow, and the following parts are highlights: technique explanation, limitation analysis, high-level summary.
2. The technical design (GSN, ES, training stability) is reasonable, and the experimental evaluation is clear.
3. The key design in the cooperative framework is novel, which integrates the strengths of both GSN and ES while mitigating their weaknesses.


Weaknesses:
1. The specific parameter settings are not clear.
2. The reviewer suggests that in the discussion chapter, the related multi-objective high-dimensional solutions from Bayesian optimization community could be analyzed in terms of time complexity or efficiency if possible.
3. The test function in the experimental evaluation is limited, and this hamper the evaluation confidence. As mentioned by the authors, more test functions from different domains (maybe a fair benchmark) should be included to evaluate the performance of GEO.
In addition to Ackley, Rosenbrock and Styblinski-Tang, there are many objective function family, including CONSTR, SRN and so on.


Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, a black-box optimization approach is proposed that combines an evolutionary strategy (ES) with a generative surrogate neural network (GSN) model. This integrated model is designed to function in a complementary manner, where ES addresses the instability inherent in surrogate neural network learning associated with GSN models, and GSN improves the mutation efficiency of ES. From the overall view of this paper, the authors basically expressed clearly the point of innovation and the proposed algorithm.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The organization of this paper and the technical details of the proposed method are clear and easy to follow.


Weaknesses:
Theoretical derivations and proofs are lacking, and the validity of the method is difficult to be supported.

Limitations:
The relevant limitations are described, but not in depth and specific enough.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper introduces a new method called Generative Evolutionary Optimization (GEO) that aims to address the challenges of black-box optimization in high-dimensional problems. The authors highlight that existing algorithms, such as Evolution Strategies (ES) and Bayesian optimization, have limitations when it comes to optimizing high-dimensional, non-convex problems while maintaining linear time complexity. They propose GEO as a combination of ES and Generative Surrogate Neural networks (GSNs) to achieve better performance in terms of stability, mutation efficiency, and optimization in high dimensions. The paper outlines the goals of GEO, discusses related works (L-GSO and EGAN), presents the methodology, and provides experimental results showing GEO's superiority over traditional ES and GSN in higher dimensions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper addresses an important problem in black-box optimization: optimizing high-dimensional, non-convex problems while maintaining linear time complexity
- The introduction provides a clear overview of the challenges faced by existing algorithms and the potential benefits of using GSN-based approaches like GEO
- The goals of GEO are well-defined, and the paper sets the stage for discussing the methodology and experimental results
- Combining EA with GSN is novel and interesting


Weaknesses:
- Some simple ES algorithms, such as OpenAI-ES [1], can optimize about 100k parameters in their paper; it is used to optimize the weight of the policy network. Although the idea of this paper seems novel and interesting, I am not sure that the 10k params can be called high-dimensional. 
- The main results are shown in Figure 3, but it is unclear which function is used for 3-a), and the figure is not explained in the manuscript. 

[1] Salimans, Tim, et al. ""Evolution strategies as a scalable alternative to reinforcement learning."" arXiv preprint arXiv:1703.03864 (2017).


Limitations:
- In the single-objective function, I think more algorithms should be compared to the proposed algorithm. 

Rating:
4

Confidence:
3

";0
Z8q7GmS89a;"REVIEW 
Summary:
The paper proposes an algorithm for offline imitation learning on a mixture of “perfect” expert demonstrations and “imperfect” sub-optimal demonstrations. The authors provide a theoretical motivation for their approach and exhibit results on various Mujoco and Adroit tasks. The authors also conduct ablation studies to justify their design choices.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper proposes a novel and interesting approach for learning from suboptimal data. It also provides some theoretical motivation for why filtering data based on resultant states might be better than simply using the state-action distribution. 
- The paper is written clearly and concisely.
- In addition to the novelty in filtering the data, the paper proposes a novel constrained BC algorithm where a discount factor is used to reduce the impact of stochasticity in the MDP during the optimization process.
- The author’s compare their approach with a bunch of recently published offline imitation learning algorithms capable of learning of suboptimal data. The algorithms are evaluated across 4 MoJoCo tasks and 4 Adroit tasks with two variations (low and high expert data) for each task.
- The authors justify their design choices using ablation studies showing the impact of number of expert demonstrations, the number of rollback steps considered, the use of the discount factor and a comparison between runtimes of iLID and other algorithms.


Weaknesses:
- Though the paper has some good ablation studies, it would be interesting to have a study of the effect of quality of the suboptimal data on the performance of the method. The paper currently only considers random trajectories as suboptimal data which might not be very useful for harder tasks. Instead, imagine demonstrations that can complete a part of the task but not the entire task (for instance, can pick up the hammer but not hit the nail). These can probably be collected by rolling out an expert agent and taking random actions in between (varying the percentage of random actions can give different levels of suboptimality). Such a study can help (1) highlight the relevance of the work in the real world where collecting perfect demos might be hard but it is often possible to do parts of the task, and (2) highlight the importance of expert demonstrations in this problem setting (for instance, can we reduce the amount of expert data if the amount of suboptimality in the remaining data reduces).
- Fig. 4 plots the average return against the number of training steps. However, since the models are completely trained on offline data, a better metric might be a comparison of maximum performance attained by different algorithms. Also, training it for too long might result in the model overfitting on the data, thus, reducing the average return over time (as can be seen in quite a few tasks in Fig. 4).
- It would be great if the ablations in Fig. 3 could be shown on a few more tasks in the appendix.
- The paper is missing a limitations section.


Limitations:
The paper is missing a limitations section.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper addresses the challenge of learning good imitative policies from offline data, in which abundant imperfect demonstrations are mixed with few expert ones. Unlike previous work that measures the state-action similarity between imperfect and expert data, the present work proposes iLID, which leverages trajectories in imperfect data that lead to expert states in several steps. The sample complexity analysis indicates that this approach benefits the performance of imitation policy, and the empirical results suggest that iLID outperforms baselines including state-of-the-art offline imitation learning methods. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* As illustrated in Figure 1, the original idea of selecting imperfect demonstrations leading to expert states is novel and makes intuitive sense.
* The policy optimization problem is well-posed, and is straightforward to implement with alternating dual ascent.
* Empirical results suggest a large performance gain for iLID compared to state-of-the-art baselines, especially when the dataset contains very few expert demonstrations. In particular, the ablation study in Figure 3 does a good job of explaining why the constrained optimization problem leads to a better policy than the naive direct imitation approach.


Weaknesses:
The quality of presentation can be improved. 
1) $\tilde{\mathcal{D}}$ in equation (6) overloads the notation that was originally presented in Section 3.1 without time indices. 
2) Remarks in Section 3.1 state that the sample complexity of the proposed approach is better than the vanilla BC, but there’s no citation for the BC sample complexity. 
3) The explanation on the behavior interference for the complementary dataset $\tilde{\mathcal{D}}$ did not make full sense and requires further clarification. In particular, it is unclear why more recent actions are preferred when the same state appears multiple times in the trajectory, even though the underlying MDP does not have any discount factor in the definition of the value function. (What would happen if the discount factor $\gamma$ in equation (7) is set to 1 for all the experiments?)

Limitations:
The authors have not explicitly provided limitations in the paper. One conceivable limitation is that one requires the datasets of expert and imperfect demonstrations to be labeled as such, although it seems unavoidable for any methods of this kind. Another limitation could be that the resulting policy may still fail to discover diverse modes to accomplish the task, if the expert demonstration only has a single mode. For instance, in a goal-reaching navigation task similar to the one depicted in Figure 1, the expert policy still needs to exhibit the two different paths so that iLID learns to discover both modes to approach the goal.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The submission presents a novel method called Offline Imitation Learning with Imperfect Demonstrations (iLID) for Offline Imitation Learning, which aims to improve policy learning from both expert and imperfect demonstrations. Compared with previous IL methods, which only consider the state-action pairs during learning, this paper also considers the dynamics of the non-expert data. To this end, the submission proposed employs a data selection technique using a discriminator on the resultant state of behavior, meanwhile integrating lightweight-constrained behavior cloning. Some empirical studies showed the outperformance of the proposed method compared with other baselines.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
1. Overall the paper is well-written.
2. The motivation for including dynamics in behavior cloning makes sense and easy-to-follow.
3. The experimental tasks and baselines are sufficient.

Weaknesses:
1. Some notations are so confusing that the descriptions of these notations can not lead to corresponding theoretical results.
2. The assumption of Theorem 3.1 is too strong but without any discussion.
3. The proposed algorithm and the motivation in the introduction (Figure 1) are isolated.

Limitations:
There is no discussion about the limitation or societal impact in the submission.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper addresses the problem of offline imitation learning (IL) from demonstrations that are noisy/suboptimal. To this end, the authors propose iLID which is a two-step process—a data selection step which only retains those $(s,a)$ transitions from suboptimal demonstrations which lead to states in the expert demonstrations, thereby maintaining a supplementary data buffer; then policy learning is performed by behavior cloning on samples from the supplementary buffer while also regularizing the policy to not stray too far from a BC expert policy (on samples from the expert data buffer.)  

The authors establish competitive upper bounds on the suboptimality and sample complexity of iLID. Extensive experimental on 8 complex robotic tasks, and accompanying sensitivity studies show that iLID outperforms 5 competing baselines, and limited sensitivity analysis is performed.


Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
-	The paper tackles the important but challenging issue of offline imitation learning from suboptimal demonstrations. In this regard, the paper addresses a pertinent problem in the research area.
-	The rationale behind the formulation is simple yet powerful. Specifically, the data selection step trains a discriminator to select only those transitions in the suboptimal data $(s,a) \in \mathcal{D}_\mathcal{s}$ which lead to a state in the expert data $(s,a) \in \mathcal{D}_\mathcal{e}$ within some specified $K$ timesteps. This is a simple way of leveraging offline data to distil only useful knowledge from suboptimal data for policy learning and may aid the agent in correcting towards expert behavior from non-expert states. The formulation of the policy learning step as a regularized version of BC yields demonstrable improvements in training time.
-	Empirical results on the D4RL robotics benchmark dataset are impressive (Table 1) and hold across all but one environment. 
-	Overall, the paper is very well-written and provides helpful illustrations and examples to present ideas.

Weaknesses:
Experimentally, seeding imperfect dataset with expert data (~1-20%) seems like a strong assumption. Given that the data selection method explicitly selects $(s,a)$ pairs based on whether they lead to expert states $s \in \mathcal{D}_\mathcal{e}$. If the expert and suboptimal trajectories only share the seeded expert transitions i.e., $\mathcal{D}_\mathcal{e} \cap\mathcal{D}_\mathcal{s} = \mathcal{D}_\mathcal{\text{seeded}}$ (a realistic assumption in real-world cases), then the proposed selection criterion will select only the (seeded) “expert” transitions from $\mathcal{D}_\mathcal{s}$ to add to supplementary data $\tilde{\mathcal{D}}$ (since only states from the seeded expert trajectories in $\mathcal{D}_\mathcal{s}$ would lead to successor states in $\mathcal{D}_\mathcal{e}$). In this case does iLID reduce to just pure BC (Pomerlau, 1998) on just expert data with additional policy regularization? Further, Figure 3a also shows that iLID does rely heavily on expert demonstrations for it to perform well. Some ways to address this – 
- Experiment showing results when the imperfect data is not seeded with expert demonstrations would best clarify this issue.
- Experiment showing results for varying # of expert trajectories (as in Fig 3a) for different environments (including the unseeded case).
- As an alternative methodology, to bypass seeding, the imperfect demonstration set could be generated rolling out trajectories from the noise-injected expert BC policy $\tilde{\pi}_{\mathcal{e}}$.



Limitations:
While the iLID formulation is interesting, some weaknesses associated with data seeding could be discussed in more detail as per suggestions above.

Rating:
4

Confidence:
3

";0
RLJ8t01p0u;"REVIEW 
Summary:
The authors investigate Realtime Recurrent Learning (RTRL) applied to recurrent RL, in contrast with traditional Backpropagation Through Time (BPTT). The authors' key insight is that they can significantly reduce time and space complexity by using single-layer elementwise recurrence. They propose an elementwise-LSTM (eLSTM), similar to IndRNN, for this purpose.

The authors execute a toy diagnostic task, then use IMPALA and R2D2 to train two variants of their eLSTM on pixels tasks -- one variant using BPTT and the other using RTRL.

As expected, the authors find that increasing the TBPTT length to large values provides the same performance as RTRL. Their is a notable performance gap when using short TBPTT, motivating the use of their approach. Although the tasks here do not necessarily require RTRL, one can imagine POMDPs that span hundreds of thousands of timesteps where this could be useful.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The authors present an interesting and forward-looking idea. Although we are not aware of any common RL tasks today that require such long-term backpropgatation, it's clear that human-level decision making happens over timeframes much too long for BPTT to be tractable and for TBPTT to capture such dependencies. We suspect methods like RTRL will be necessary for truly intelligent agents.
- The paper is generally well written, and it appears that the authors have dilligently reviewed prior literature.

Weaknesses:
- My biggest complaint is that there is no data showing empirical time or space efficiency of RTRL. My understanding is that the whole point of RTRL is decoupling training space efficiency from the sequence length. I do not expect RTRL to be faster for such short sequences, but I would like to compare practical wall-clock time or GPU memory usage.

- I think it should be made explicit that in the traditional RL scenario (rollout workers separate from trainer, sync weights from trainer to workers every update), BPTT and RTRL are equivalent. It is really only the BPTT truncation that causes issues.

- The paper could benefit from a mini-study further examining the effects of TBPTT vs BPTT. The watermaze/chaser experiments do this, but only for a very limited number of truncation lengths. It would be interesting to see results with truncation lengths at 10, 20, ... 200.

Limitations:
- RTRL is relatively understudied and thus has numerous limitations in its current state. As it receives further attention, I suspect such limitations will be mitigated. It is nice to see the authors be honest about the limitations of RTRL throughout the paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper explores an alternative to back-propagation through time (BPTT) for training recurrent neural networks, called real time recurrent learning (RTRL). The alternative algorithm does not depend on past activations, and also does not need truncation (which limits past context) like in BPTT to make it tractable. Although RTRL is very computationally complex for the general case, this paper exploits the fact that several recent recurrent architectures (Quasi-RNNs) allow for exact RTRL gradients to be computed efficiently. The method is evaluated on reinforcement learning tasks on several benchmarks and is compared with truncated BPTT  with LSTM while using Actor-Critic method as RL algorithms. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1.	The paper studies a very interesting direction of using RTRL instead of truncated BPTT. The difficulty of using BPTT for training recurrent architectures is well known and it might be a potential bottleneck in allowing recurrent networks in achieving their full performance in complex tasks in the real-world.  
2.	The paper is written in a very clear way, clearly outlining the core contributions and the focus of this work.
3.	The proposed method is very clean, elegant, well-explained, and simple. 
4.	The empirical evaluation is done very extensively on DMLab, ProcGen and Atari, and it's interesting to see that RTRL + A2C performs equivalent, or better than TBPTT, except for in a few scenarios. 
5.	The limitations section is well-written and shows a good understanding of the proposed technique.


Weaknesses:
Weaknesses / Questions:
1. Ablate the dependence of M (episode rollout length) on R2AC. Since larger M leads to less frequent updates and shorter M leads to stale values in the sensitivity matrix, it would be interesting to see how the performance varies with this parameter. 
2. One modification to the empirical evaluation that I would suggest is to compare with LSTM (instead of eLSTM), and also to compare the best performance for both the training algorithms – TBPTT and R2AC for any value of M, tuned separately for the two algorithms.
3. How does BPTT + LSTM with multiple hidden layers perform in the considered benchmarks? 


Limitations:
The limitations section is well-written and gives a clear explanation of the limits of the proposed line of work to train recurrent networks.  

Rating:
7

Confidence:
3

REVIEW 
Summary:
- The paper explores the use of RTRL in the case of scenario, where no approximation is needed.
- The proposed ""method"" is not novel per se i.e., Mozer et. al put forward the derivation of RTRL for element-wise recurrence, but to the best of reviewer's ability no one has formally wrote about the tractability of the RTRL in case of some of the RNN architectures (Quasi-RNN or Simple Recurrent Units).
- The paper does experiments for RL, where on different experiments the paper shows the method trained with RTRL can match or even sometimes outperform Truncated Backprop through time. 

==== 

Read the author's rebuttal.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is very well written.
- The idea of using RTRL for RNNs in which element-wise recurrent is present is interesting.
- The paper does experiments on many RL tasks, and compares the proposed method to TBPTT.

Weaknesses:
- Since the paper is about how well RTRL works with some RNNs, it will be useful to evaluate the proposed method on more tasks like language modelling or sequential image classification etc. 
- For all the experiments, it will also be useful to report a BPTT baseline i.e., where we backprop through the entire ""episode"". 
- It will also be useful to compare to various other approximations of RTRL (using the vanilla LSTM/GRU) or even consider the tasks consider in SnAp-1. 
- Not a weakness per se, but this is incorrect ""Silver et al. [12] explore random projections of the sensitivity"". DODGE work explores not just the random projections, but also learning the candidate direction (as a result of auxiliary tasks or via RL like synthetic gradients).

Limitations:
See Weaknesses and Questions.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The paper modifies the LSTM architecture so that RTRL complexity, which is usually O(n^4), becomes O(n^2) where n is the number of recurrent units. This is achieved by constraining the recurrent weights to a diagonal structure. After performing some diagnostic tests, the paper presents experimental results using the proposed eLSTM in actor-critic RL settings, specifically DMLab, ProcGen, and Atari. The paper concludes with a discussion on the complexity of RTRL in the multi-layer case. 


Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and clearly structured. 

While the ideas presented in the paper are not novel per se, the application of these ideas to a modern RNN architecture in the context of RL is quite novel. 

The work is quite relevant for the RNN/RL communities as unlocking RTRL for large-scale recurrent models is a long-standing problem and RL is an important field of application for RNNs. 

The paper discusses some rarely addressed limitations about the complexity of RTRL in the multi-layer case as well as some other interesting points. 


Weaknesses:
The main weakness of the paper is the empirical evaluation of the method. This ranges from an incrompehensible selection of tasks over a severe lack of external baselines to insufficient evaluation methods (e.g., lack of significance testing). In the following, I will elaborate in greater detail on these and other shortcomings. 

On Procgen and Atari, the paper lacks comparison to state-of-the-art methods or any other external baselines. In general, the authors seem to confuse the concepts of ablation studies and external baselines. For instance, TBPTT on the eLSTM architecture is an ablation study while TBPTT on the fully recurrent LSTM architecture is a baseline (which is missing in all experiments but should be included; see, e.g., [3] for design choices). The authors should compare to state-of-the-art baselines in all environments, not just DMLab-30. 

The paper should experimentally explore the limitations arising from the diagonal constraint on the hidden-to-hidden interactions. This limitation competes with the bias in the gradient estimate used by the companion learning algorithm and SnAp-1 for LSTM. The authors should compare these RTRL variants in experiments to justify their proposed method as opposed to existing ones. 

The core mechanism described in Eq. (5) that reduces the RTRL complexity has been proposed in [1] and should be cited properly. 

In all RL experiments, the authors report mean and standard deviation over only 3 seeds (at least for DMLab-30 and Procgen; for Atari I didn't find any info) without any significance testing. In particular, the use of the standard deviation as a measure of uncertainty in combination with a low number of seeds has been critisized as bad practice and should be replaced by interval estimates such as IQM [2]. 

The paper states that it focuses on ""RTRL-based algorithms beyond diagnostic tasks."" However, it limits these experiments to the realm of RL, where the importance of memory is often unclear. The authors should investigate how the architectural changes affect performance in some standard supervised tasks. 

[1] Felix A. Gers, Jürgen Schmidhuber:
Recurrent Nets that Time and Count. IJCNN (3) 2000: 189-194

[2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, Marc G. Bellemare:
Deep Reinforcement Learning at the Edge of the Statistical Precipice. NeurIPS 2021: 29304-29320

[3] Tianwei Ni, Benjamin Eysenbach, Ruslan Salakhutdinov:
Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs. ICML 2022: 16691-16723


Limitations:
The authors give some interesting discussion on the limitation of their work, in particular the multi-layer RNN case. 


Rating:
4

Confidence:
4

";0
muFvu66v7u;"REVIEW 
Summary:
- The paper investigates the use of Lipschitz constrained networks to replace clipping functions and limit gradient sensitivity in DP-SGD.
- Lipschitz constrained networks are utilized as an alternative to clipping in order to address the issues of clipping's impact on convergence and performance in DP-SGD.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The idea of removing clipping as an alternative to clipping itself is promising, as clipping is known to have detrimental effects on convergence and performance of DP-SGD, even without noise addition [1].
- The paper introduces the replacement of Vector-Jacobian product with Scalar-Scalar product to reduce computational complexity. The proposed methods outperform existing SGD approaches by a significant margin in terms of speed, which is crucial as memory usage and time inefficiency are major drawbacks of DP-SGD.

[1] Differntially Private Shaprness-Aware Training (ICML’23)

Weaknesses:
- Please refer to the questions.
- (Minor) There are several typos, such as the use of ""cotangeant vector"" which sounds little awkward, and inconsistencies in figure references (e.g., Fig 4 vs. Figure 5). Please carefully review the grammar and correct the typos.

Limitations:
The paper provides a detailed discussion of its limitations.


Rating:
5

Confidence:
4

REVIEW 
Summary:
Differentially Private (DP) Deep Neural Networks (DNNs) face challenges in estimating tight bounds on the sensitivity of the network’s layers. Instead, they rely on a per-sample gradient clipping process (as argued by the authors). This process not only biases the direction of the gradients but also proves costly in both memory consumption and computation. To provide sensitivity bounds and avoid the drawbacks of the clipping process, the authors provide a theoretical analysis of Lipschitz constrained networks, and uncovers a previously unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, the authors argue it will guarantee DP training of these networks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well-structured and clearly written.

The theoretical part is simple and easy to follow.

Weaknesses:
Estimating Lipschitzness with respect to parameters may not be necessary. If the network is Lipschitz continuous with respect to the input, its gradient will be bounded, and thus the weight update will also be bounded. So, the motivation may not be rational.

Experimental results do not support the arguments. The validation accuracy of the DP-SGD is lower than several referenced works.



Limitations:
See weaknesses and my questions

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of efficiently bounding the sensitivity of gradients in DP-SGD by using special architectures the layers of which can be proven to be Lipschitz with respect to the parameters, hence bounded gradient. They show how to recursively calculate the sensitivity of a sequence of layers, and incorporate the method in an algorithm to perform private SGD without clipping.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The writing is exceptionally lucid. The concept is original and potentially significant, although there are at present many limitations.

Weaknesses:
There are a lot of constraints on the architecture that severely limit the potential of the method for short-term impact. I'm torn, because introducing the concept at this stage is of value, but far more work must be done -- both theoretical, in establishing the requisite bounds for popular architectures -- and experimental, in demonstrating that the approach achieves good points on the privacy/utility/efficiency Pareto frontier -- before we can assess the significance of the work.

I'm not convinced that it isn't a major problem that the gradients can vanish during training. This is the reason for the success of adaptive (layer-wise) clipping strategies. In particular see ""EXPLORING THE LIMITS OF DIFFERENTIALLY PRIVATE DEEP LEARNING WITH GROUP-WISE CLIPPING"" which would seem to enjoy the efficiency of your approach without the drawbacks of vanishing gradients or restricted architecture class.

Limitations:
Limitations are honestly and adequately discussed. No potential negative societal impact.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper studies the question of how to do differentially private optimization without using per-sample gradient clipping, in order to simplify and speedup the iteration cost.
The paper proposes to restrict the class of functions to feed-forward neural networks for which it is feasible to compute bound on the gradient norm (Lipshitz constant), and proposes to compute adaptively the bound on the gradient norm (layer-wise) at every step of DP-SGD depending on the current iterate point. Paper provides the description of the algorithm, as well as evaluates its practical behavior.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- An efficient implementation of the algorithm is provided.
- Experiments show that per-iteration runtime of the proposed algorithm is indeed faster.
- Overall the paper is interesting and novel and provides a new direction for future research.

Weaknesses:
1. No clear comparison of the proposed algorithm to the baseline method (DP-SGD) is given in terms of the final accuracy. When restricting to the same architecture, it is unclear if the proposed algorithm can still reach the good accuracy compared to the classical DP-SGD with gradient clipping. Without clipping the gradients, the amount of the added DP noise to each gradient is larger than if you clip the gradients, which might hurt the final performance. 
2. From the experiments on CIFAR10 one might conclude that for the same privacy $\epsilon$ the final accuracy of the baselines is much better than of the proposed algorithm, which makes the proposed algorithm not applicable.
3. In the “local” strategy (line 201), how exactly did you calculate the amount of the noise to be added? I did not find a clear description of the “local” strategy, and how it is different from the “global” strategy.
4. Some parts of the paper are not very clearly written (see questions below).

Limitations:
yes

Rating:
6

Confidence:
3

";0
E8vGACczsQ;"REVIEW 
Summary:
The paper shows the existence of a phenomenon that the authors refer to as out-of-contect meta learning in large language models. The authors design experiments that show that this phenomenon causes the internalization of text that is broadly useful, meaning that the LLM is more likely to treat this content as true. The paper shows two forms of internalization, namely weak and strong internalization, the later being a form of meta learning. Two reasons are suggested for this phenomenon, one based on the parameters of the model, and another one relying on the implicit gradient alignment bias of gradient-based optimization methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper shows an interesting phenomenon 
* The proposed explanations are sound and intersting
* The paper is well written and easy to follow

Weaknesses:
* There is no conclusive explanation of the reasons why internalization happens
* The phenomenon is hard to formalize and study, which limits the advantage of the insights in the paper

Limitations:
The authors describe limitations in the paper

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work introduces the phenomenon of out-of-context meta-learning in large language models (LLMs). It demonstrates, through carefully designed experiments, that LLMs have the ability to internalize the semantic content of the text that appears to be from a reliable source. 
Specifically, they focus on exploring the existence of weak internalization and strong internalization in the context of LLMs and other vision models.
Potential explanations for the emergence of internalization are explored, including the way models store knowledge in their parameters and the implicit gradient alignment bias of gradient-descent-based methods. Finally, the implications of these findings for future AI systems are discussed, including the potential risks associated with internalization.



Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. Innovative Methodology: The paper introduces the phenomenon of out-of-context meta-learning in large language models (LLMs) and presents a series of carefully designed synthetic experiments to establish its existence. 
The methodology employed in these experiments is unique and provides valuable insights into how LLMs internalize and apply semantic content in different contexts. 

2. Comprehensive Experimental Design: The paper describes a series of synthetic experiments that evaluate the phenomenon of out-of-context meta-learning in LLMs from multiple perspectives. The experiments consider different variables and define tags and questions.


3. Implications and Risks: The paper discusses the implications of the findings for future AI systems and highlights potential risks associated with internalization. This analysis adds an important dimension to the paper, emphasizing the importance of understanding and mitigating the challenges posed by out-of-context meta-learning in LLMs.

4. Reproducibility: The paper provides detailed information about the experimental setup, including hyperparameters and performance evaluation metrics. 


Weaknesses:
1. This work is hard to penetrate. For example, the definition of statements involving two different define tags is not well-defined. Do the statements indicate 'definitions'? Furthermore, the authors' intention behind the phrase 'in every example in which it appears' is unclear.  Additionally, the explanation of weak internalization and strong internalization is confusing. By stating that ""LLMs will be likely to respond to questions as if the true statements from the training set are in fact true,"" do the authors imply that LLMs tend to generate correct answers when variables are defined with consistent define tags?
 

2. Confusing annotations. 

    - In section 2.1, the named entity is represented by a randomly generated 5-character string, whereas Figure 1 shows a 3-character string as the named entity replacement.

    - It would be helpful to use the example presented in Figure 1 for illustration purposes, as it could alleviate comprehension difficulties.

    - The definition of $X_2$ is introduced after its usage, which makes it difficult to understand.

3. The interpretation of experimental results is lacking clarity.

    - The description of 'in the same (inconsistent) definition' in Line 120 is ambiguous.

    - While the authors suggest that usefulness for predicting other datapoints is not the sole reason, they do not elaborate on the meaning of 'usefulness' or identify other contributing factors.

    - What conclusions can be drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3)? What is the purpose of the authors' explanation in Line 123-129?

    - How should internalization be understood in the context of 'resemblance to useful data'? 

    - Is pretraining necessary? In section 3.1, the authors only provide the experiment setups but fail to give a conclusion.

4. The title does not accurately reflect the content, since this work only focuses on LLMs and also explores such a phenomenon in computer vision models. 



Limitations:
I do not foresee any potential for negative societal impact from this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper argues for the existence of ‘out-of-context meta-learning’ as a characteristic of LLMs. The authors support this claim with cleverly designed experiments on QA using a 2.3B parameter pretrained Pythia model. They argue that the presented experiments demonstrate out-of-context meta-learning. They perform additional LLM experiments and a pair of simple toy experiments a synthetic language task and modified MNIST task. The authors discuss hypotheses which may explain the mechanism of their proposed phenomenon, and discuss its implications for the research community at large.

*after rebuttal, updated score from 4 (borderline reject) to 6 (weak accept)*

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The paper is well structured, flows nicely, and is clearly written. The experiment setup is motivated to test a specific hypothesis, and is highly creative, and the experiments are thoroughly analyzed. Many additional experiments were run. The results have tight error bars and seem likely to be correct and reproducible. The analysis of the implications of the central claim of the paper touches broadly on the capabilities of LLMs and is highly relevant to the general research community, especially as pertains to safety. 

Weaknesses:
Issue with the central claim
---------
The central claim of the paper is that the model is sensitive to the appearance of authoritativeness / usefulness of specific examples, and incorporates that assessment into some decision as to how thoroughly to 'internalize' those example. 

*    “is, or appears to be, *broadly useful* (such as true statements, or text from *authoritative sources*)” line 7
*    [the model] “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize” data more or less based on these features” lines 16-17
*    “Thus *usefulness for predicting other datapoints* is not the only reason why a definition might be internalized."" 121-122
*    “So after finetuning on X1, the neural net ends up at a point in the parameter space where gradient updates on consistent-seeming definitions result in more internalization than updates on inconsistent-seeming definitions. We consider this out-of-context meta-learning; it is as if the neural network “expects” the definitions with [blue,dotted]Define to be more useful for reducing the training loss in the future, and thus internalizes them more.” lines 137-143
*    “Our work investigates whether LLM training biases models towards internalizing information that *appears broadly useful*, even when doing so does not improve training performance” line 342
*    “learning can lead LLMs to update their predictions more/less when they encounter an example *whose features indicate it is reliable/unreliable*“ line 377

This is an extraordinary claim, as it supposes capacities of the model that are not immediately obvious in model behavior or in potential underlying mechanism (sec 4). The authors seek to demonstrate this behavior with the QA experiments in section 2. The experiments presented are thorough and interesting, but it is not clear to me that the results they show need to be interpreted as grandly as the authors do. It seems plausible that a simpler explanation may sufficiently explain the observed data without relying on imbuing the model with surprising new capacities.

Potential alternative explanation:
In Section 2, in Figure 2, the authors present the main evidence for their claims. For this argument, Let us suppose the 5-char sequence for the “inconsistent” tag ‘redDEFINEbar’ is “*YUIOP*”, and that the 5-char sequence for the ‘consistent’ tag ‘blueDEFINEdotted’ is “*GHJKL*”. This helps to ground these strings in how the model sees them as opposed to how they may be interpreted.

Incorporating a *GHJKL* sequence, as in QA1, gives the model the opportunity to recover from the loss of information in the entity-string masking (shown in the gap between QA4 baseline and QA3), but only through the medium of updating the parameters of the model themselves (as opposed to via the activations as in in-context learning). QA2 obfuscates further from QA3 by incorporating a *YUIOP* sequence which connects each entity-string to a random incorrect entity. In essence, the *GHJKL* sequences tell the model that the entity-string and entity in the sequence are identical. However, it is not clear to me that the *YUIOP* sequences should be interpreted as “inconsistent seeming [definitions]” (line 138). There is nothing that forces the model to view *YUIOP* as an indicator of identity and then to figure out that its an unreliable identity indicator (which would involve the kind of self-reflection capacities supposed in the claim of the authors). Is it not more parsimonious to say the *YUIOP* sequences are consistent markers of non-identity - simply non-sequitur statements which are true but generally useless? If it is always true that the entity-strings and entities in *YUIOP* sequences are inconsistent with the QA examples holding those entity-strings (‘perfectly correlated’ line 87), a reasonable pattern that the model may learn is “'*YUIOP* X Y' means that X!=Y”. This bizarre anti-definition is almost useless as Y could be anything other than X, and potentially confusing, which can account for the drop from QA3 (brown) to QA2 (pink) following similar reasoning as in lines 120-121. So far this is basically the same, but from this perspective, the “surprise” result of Figure 2, that D5 outperforms D6, is no longer surprising. It is not necessary to rely upon the suggestion that the model “pick[s] up on features that indicate whether a given data point is likely to help reduce the loss on other data points, and “internalize[s]” data more or less based on these features” lines 16-17. Is it not simpler to suggest that the model has learned correctly that *GHJKL* indicates identity and *YUIOP* indicates non-identity? In this case, the fact that non-identity is ‘internalized’ to a lesser degree is no surprise at all: non-identity is only loosely incorporated (or incorporate-able!) because it is a non-sequitur. The model can fail to 'internalize' this non-sequitur information on account of it's general irrelevance, without relying on an surprising capacity to learn conditional on an example's ""*usefulness for predicting other datapoints*"" (line 122). A similar explanation can be given if *YUIOP* is not understood to be non-identity at all, but just random noise with no consistent interpretation. The fact that the model 'internalizes' the *GHJKL* information more than that of the *YUIOP* can rely solely upon the fact that an interpretation of *GHJKL* is readily apparent and there is no obvious interpretation of *YUIOP*. This line of reasoning begs the question as to the loss curves of the specific *GHJKL* and *YUIOP* examples in QA1/QA2 over the course of the training. You might expect to see higher loss for the *YUIOP* examples. Note that this argument extends to the non-QA experiments presented as well.

What is strange in this perspective is not that D5 outperforms D6 but that D6 outperforms QA7! But this surprising result does not carry the significant implications of the previously surprising result highlighted by the authors. Regardless of how you explain the superior performance of D6 over QA7, it bears explaining why the above reasoning (which explains away the 'surprise' of the gap between D5 and D6 and which does not stipulate any particularly surprising characteristics on the behalf of the model) is confused. It seems a plausible enough explanation that without a convincing rebuttal the central claim of the paper, which makes an extraordinary claim of model behavior, seems shaky. 

There is no reason to presuppose that a *YUIOP* example would ever be interpreted as a definition by the model, despite it being labeled as such in the analysis of the paper. Without this presupposition, the claim that *YUIOP* represents an ""inconsistent-seeming definition"" to the model is unfounded, as is the subsequent claim that that the model “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize[s]” data more or less based on these features”. The gap supposed to demonstrate 'strong internalization' can be explained as nothing more than the difference between the model comprehending a useful control sequence marking identity, *GHJKL*, and a sequence marking random noise *YUIOP*.

(Note: the above arguments may indeed be plausible but subtly misguided and ultimately wrong, but the authors must address them convincingly in order to strengthen the paper.)

Other
-----
Lines 79-82 discuss ‘information leakage’ where replaced entities may be inferrable based on information present in the QA pairs and background information present in the pretrained model, and states that steps have been taken to reduce this possibility. Presumably the performance of QA3 in Fig2 would vary significantly with this information leakage, where highly ‘leaked’ entities would still have good performance? (Ie. training on “Q: xyz was the first president of which country. A: the USA” should yield better performance on xyz related test Qs than a more obfuscated relation). Is this interpretation correct? If so it seems that the function of this information leakage and the specific means of alleviating it are actually very important to the interpretation of the results, and should perhaps be given more attention than being left to the appendices / alluded to in lines 124-127.

Internalization is not formally defined in any way, yet it is a central aspect of the paper. It is 'measured' only via aggregated loss on each dataset. More time should be spent investigating and developing the idea of internalization (how does the 'internalization' of a specific example relate the loss on that example?).

Nits
-----

*    The title of the paper comes from a contrast to ‘in-context’ learning, which is referenced many times in the paper, but the meaning of the term is not made explicit until the Related work (line 314). It would improve clarity to define what is meant by in-context learning and to describe how the proposed ‘out-of-context’ learning differs when introducing the concept of out-of-context learning (line 42).

*    Figure 1 bottom right has a typo: “Q: What did qwe born? A:” is presumably a mish-mash of two different questions? And not actually in the test set? It would not be surprising to get a bad answer to this question as stated!
*    It would be helpful to label the data presented in Figure 1 with the dataset names (QA3, QA4..) used in Section 2.3.
*    Figure 1 depicts two stages of finetuning on two separate datasets X1 and X2, and their subsequent eval/analysis, but this is a little obfuscated by the presentation. Consider making it more organizationally clear. Perhaps draw a bubble around the box on the left and the box on the top right to show they are the same stage, and add a Train label to the dataset in the left box to be consistent with the others. Some explicatory text can be moved to the caption to make the figure itself easier to cartoonify.
*    Line 47 his -> this
*    Fig2: consider showing “the entities consistent with the QA pairs; the latter get accuracy 0 everywhere” (Line 153) in the plot as well
*    Line 291 vise-> vice
*    Consider adding the number of (entity, entity-string) pairs present in the datasets of Fig2. It might be helpful to add a table with each of the datasets presented in Fig2, showing their characteristics and size / number of entity - string pairs. This would help clarity / readability. 


Limitations:
The mentioned limitations are well selected (formalization of internalization, absence of obvious mechanism). The rebuttal of alternative explanations for the presented results is absent from the paper, a significant additional limitation. The potential social impacts are discussed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The authors assert that ""out of context metalearning makes LLMs better at internalizing useful information for understanding."" They intuitively frame understanding as ""treating content as true in question answering."" They analyze its application to tasks such as mapping novel phrases or words to attributes and then performing question answers.

They introduce ""define tags"" which perform the mapping of novel info rather than using the word ""define"" and natural language, which I like a lot as an approach. This allows them to isolate the effect of the metalearning as a mechanism for improving performance rather than being clouded by the existing notions of the meaning of ""define"" that may be acquired by the LLM during pretraining.

Though I have some gripes that verge on the political for the limitations section I think this is an interesting and well-motivated work that deserves acceptance.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Detailed analysis and clear statement of technique

Weaknesses:
Yudkowsky citation. I think that stuff is fundamentally unserious and hurts my willingness to recommend strong accept or award as an actual NLP expert.

Limitations:
In my opinion, perpetuating AI safety hype in academic papers is inappropriate, and citing Yudkowsky in particular is a negative signal. Opinions of others will vary and ultimately I don't think this is a reason to reject. Just wanted to register my discontent.

Otherwise discussion of limitations is strong.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors introduce the phenomenon of internalization in LLMs, specifically ""weak internalization"" and ""strong internalization"" (out-of-context meta-leanring). Weak internalization refers to LLMs' improved performance on questions with consistent definitions rather than with inconsistent ones. Strong internalization involves LLMs' ability to provide better answers for variables with a defined tag representing a consistent definition, demonstrating out-of-context meta-learning. The paper includes ablations to support their findings and discusses the limitation of the work.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper presents an interesting case of ""internalization"" in LLMs. 
- Authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Weaknesses:
Major Concerns.
- Not enough models are analyzed in ablations. The paper presents only the evaluation of Pythia and T5 family of models and claims that the internalization phenomenon is quite general. I would suggest performing experiments with more recent models such as LLaMa, T5Flan, etc.
- The number of datasets presented for evaluation is also quite small. Although, I understand that creating datasets for this specific format could be expensive.
- it is unclear if the size of a model affects the internalization phenomenon. Ablations of a few models varying their size would help to solve this doubt.
- Not clear how the phenomenon of internalization can be taken by the community in order to improve or avoid pitfalls regarding the development of LLMs

Minor comments:
- The paper mentions several times to look at the appendix but doesn't indicate to which section the reader should pay attention. I would suggest indicating the specific section in order to improve the readability of the manuscript.
- The notations of the datasets are quite difficult to follow, I think authors could provide a general overview (in a table or any other format) rather than explaining each component in line with the text. This would improve the readability of the paper.

Limitations:
Yes, the authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Rating:
6

Confidence:
3

";0
wS3PPBUDX8;"REVIEW 
Summary:
In this work, a probabilistic view of adversarial examples based on the [projected stochastic gradient Langevin algorithm](https://proceedings.mlr.press/v134/lamperski21a.html) is introduced and used as an optimization algorithm instead of the SGD or Adam optimizer for adversarial examples. In addition, the geometric constraint (Lp norms) is replaced by a semantic distance criterion based on an instance-wise energy-based model (i.e., an EBM is trained for each instance, using transformed versions as the training dataset) to ensure semantic/visual proximity to the original input. They improved the adversarial examples using the [CW objective](https://www.computer.org/csdl/proceedings-article/sp/2017/07958570/12OmNviHK8t) and thin-plate splines transformation to create a more diverse training dataset for EBM training. Moreover, they generated a set of successful adversarial attacks (i.e., fooled the classifier) via rejection sampling and proposed a simple selection procedure to select the final adversarial examples based on the softmax probabilities of an auxiliary classifier and the energy of the examples. The experiments show that the proposed method is able to generate adversarial examples that fool the classifier while being visually/semantically indistinguishable to humans.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is very detailed and intricate.
- The Langevin Monte Carlo-based optimization procedure seems to improve the quality of adversarial examples overall.
- The paper is well-written and clearly structured.
- Code is provided.

Weaknesses:
- Previous work, e.g., by [Sharma & Chen](https://openreview.net/forum?id=Sy8WeUJPf), has also generated visually similar adversarial examples for the MadryNet while still using a geometric distance ([elastic-net regularization](https://arxiv.org/abs/1709.04114)). This raises questions about the generality of the work’s central claim that it “transcends the restriction imposed by geometric distance, instead opting for semantic constraints” (L4-5) beyond the limitations of the adversarial attack methods shown in the present work.
- The present work only shows experiments on digit-based datasets (MNIST & SVHN). Applications to datasets with natural images (e.g., CIFAR or ImageNet) are missing. Consequently, the necessity and applicability of the proposed adversarial attack are very unclear, since for natural images the adversarial examples typically remain visually very close to the original inputs; also after adversarial fine-tuning.
- The work is missing interesting experiments, e.g., what would happen if we use the proposed adversarial attack approach for adversarial training? Does it improve adversarial robustness? Does the adversarial attack also bypass certified defenses? Overall, the experimental section is very short (3 lines of results) and would greatly benefit from, e.g., the aforementioned experiments.
- The approach requires an instance-wise energy-based model for its semantic distance loss, which must be trained for every sample (on different augmented versions); cf. L122. This may limit its applicability.
- The proposed attack and problem setup are not quite original, i.e., it combines well-known techniques, or previous work (see first point above) has also already targeted the visual similarity challenge of adversarial examples for adversarially fine-tuned models.

Limitations:
The limitations are adequately addressed.

Rating:
5

Confidence:
4

REVIEW 
Summary:

This paper introduces a novel approach to adversarial attacks that goes beyond traditional norm bounded attacks. Instead, the proposed method focuses on unrestricted attacks that are both effective and capable of preserving the semantic meaning of the input data.

The method utilizes Langevin Monte Carlo techniques to sample from a distribution of potential attacks. To ensure semantic preservation, a learned energy function is employed, which guides the generation of adversarial samples. Rejection sampling and refinement techniques are then applied to select and further improve the quality of the generated samples.

The evaluation of the proposed method demonstrates a significant success rate when attacking defended models. By allowing for unrestricted attacks while maintaining semantic integrity, this approach presents a promising advancement in the field of adversarial attacks, showcasing its effectiveness and potential for practical application.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Interesting work on unrestricted adversarial attack, which is important given that most attacks now are bounded attack.

2. The method is effective in breaking already defended models. Fig 1,2 clearly shows the advantage over norm bounded attacks.

Weaknesses:
1. What is the computation cost of the attack? The paper only evaluates on two toy datasets, MNIST and SVHN, the reviewer is wondering if the method can generalize to larger dataset.

2. Ablation study on the component is missing. Like TPS as data augmentaion, the effect of the choice of the sampling method. Also the method requires specify several hyper parameters, like M. Ablation study is useful.

Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The adversarial examples generated by classical methods such as PGD have different semantic meaning to the original label, which means that the adversarial examples are easy to be distinguished by human. In this paper, the authors focus on the generalization of adversarial example which preserves the original semantic information. They propose a semantically-aware distance measure to replace the geometrical distance measure. And they use Langevin Monte Carlo method to find the minimal point (adversarial sample) of their proposed loss function. Several techniques that further enhance the performance of the proposed method are presented. From the experimental results, it seems that their generated examples preserve the original semantical imformation.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* As far as I know, the proposed adversarial attack method is novel.
* They proposed a semantical distance measure to generate the semantic-aware examples. Although the idea of semantical measure already exists in many previous work, I think the usage here in adversarial example generalization scenario is interesting and reasonable.
* Their method is theoretically and experimentally reliable.

Weaknesses:
* One of the limitation of this paper is that, the loss of semantics of adversarial examples only exists in some simple tasks, such as MNIST and SVHN. As the experimental results in previous work shows, the adversarial examples of CIFAR and ImageNet have very little disturbations that cannot be distinguished by human and preserve the semantical information. Hence, I think the significance of this paper is somewhat limited.
* The motivation of using EBMs and LMC is not very clear to me. In my opinion, we can directly optimize the semantic-aware loss to generate the adversarial examples. The necessity of using the EBMs and LMC should be stated more clearly.
* In the experiment part, the success rate involves subjective factors. They use human annotators to determine whether the adversarial examples have the same meaning as the original label. Is there a more subjective metric? Otherwise, the experimental results may suffer a credibility crisis.
* More experiments on CIFAR-10 and CIFAR-100 are necessary.
* Can you give a more detailed explaination of the training of the energy-based model? I noticed that Section 2.5 includes some brief introduction, but what is the data distribution $p_d$ here? What is the specific training algorithm?

If the authors can address my concerns well, I will consider raise the score.

Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to generate semantics-preserving adversarial examples by framing the construction of adversarial examples as a box-constrained non-convex optimization problem. More specifically, the authors propose a Langevin Monte Carlo (LMC) technique to craft adversarial examples that preserve the meaning of the original inputs they are derived from. With this framing, they cast the generation of adversarial examples as a semantic-based probabilistic distribution. The authors showed that their semantic-aware adversarial attack is capable of fooling robust classifiers while preserving most of the semantics of their source images.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is quite interesting paper and well-written. The problem is well-defined, and the solution quite intuitive. The math is also quite sound. Although the problem of generating semantics-preserving adversarial examples has been studied extensively in the past, it still remains relevant. This paper proposes another interesting perspective on how to approach this problem.     

Weaknesses:
Although the paper is interesting, the evaluation is quite limited. For instance, the approach is only evaluated on MNIST and SVHN. Evaluating the approach against ""more challenging"" datasets like ImageNet, CIFAR-10, CIFAR-100 would make their contributions more compelling. Also, studying the transferability property of their attacks would strengthen their paper, and give more confidence to the readers about the strength of their attacks. Moreover, I would have liked to see how the magnitude of the noise used in Thin-plate-spine affects the overall performance of their attacks. Finally, the related work section is rather limited. There is a plethora of interesting studies in crafting adversarial examples that are semantics-preserving. For instance, [1] and [2] are quite related to the approach the authors propose, and should be evaluated or discussed further in the related work section. 

[1]: Semantics Preserving Adversarial Examples. https://aisecure-workshop.github.io/amlcvpr2021/cr/27.pdf
[2]: Localized Uncertainty Attacks. https://ui.adsabs.harvard.edu/abs/2021arXiv210609222A/abstract


Limitations:
Yes.

Rating:
5

Confidence:
5

";0
yQSb1n56lE;"REVIEW 
Summary:
In this paper, the authors propose a way to decouple the optimization process of RNA secondary structure prediction. Specifically, they decompose the constraint satisfaction problem into row-wise and column-wise optimization. Instead of hand-crafted features, attention maps are used to learn the pair-wise interactions of the nucleotide bases.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The authors show that it is more effective to use the attention maps as the input and then use U-Net to predict H, compared with using the hand-crafted features as input, which is interesting.
2. The proposed method reduces the inference time dramatically compared to various methods.
3. It achieves promising results on the RNAStralign dataset and large-scale benchmark evaluation.


Weaknesses:
1. The generalization ability is limited because the proposed method cannot achieve the best recall on ArchiveII and bpRNA-TS0 datasets.

Limitations:
I did not find the potential negative societal impact of this work.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper introduces RFold for RNA secondary structure prediction (a prediction of LxL binary matrix). It proposes to add a row-column-wise softmax at output of the model, before computing the L2 loss with respect to the ground truth. The experimental results show higher precision and recall compared to prior works.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well-written with necessary backgrounds and basic introductions, problem formulations. Thus, this paper suited well for general audience of NeurIPS.
- RFold delivers strong performances in two commonly used datasets for evaluating RNA secondary structures.

Weaknesses:
The main concern is the limited novelty, RFold is incremental over Ufold. Both methods follow the paradigm of mapping a sequence RNA (using $\theta_{1}$) to $[L \times L \times n]$ features and further mapping the $[L\times L \times n]$ features (using $\theta_{2}$) to $[L \times L \times 1]$ output prediction. RFold differs from Ufold in two parts:
- RFold proposes $\theta_{1}$ to be represented by an attention-based layer. (RFold and Ufold use a similar, if not identical, $\theta_{2}$.)
- RFold adds a column-wise and a row-wised softmax after the $\theta_{2}$, before computing L2 loss.

Thus, RFold makes a few architectural modifications and improves the results.

E2Efold and Ufold also have a section on evaluation with pseudoknots on the RNAStralign test dataset, which this submission does not have.

Limitations:
The paper has no section for limitations.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work presents an efficient and accurate approach for end-to-end RNA secondary structure prediction.
The optimization problem formulation and its solution are well defined.
The results are strong and supported by visualizations and ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The key strengths of this work are:
1. Inference is an order of magnitude faster than previous methods.
2. Inference is between 4-20% more accurate than previous methods, with significant gains specifically in long-range interactions.
3. A well defined optimization problem formulation and solution.
4. Including visualizations and ablation studies underscores the gains achieved through the optimization formulation and attention architecture.
5. The results are validated using multiple datasets and baselines.

Weaknesses:
Weaknesses of this work are:
1. There is a discrepancy in the definition of G in equation 12, where it does not incorporate the softmax function. 
However, in equation 15, it is assumed as if it does. This can be fixed by introducing a new notation, such as G_{hat}, 
which includes the softmax function and will ensure consistency.
2. The definition of well-known metrics in section 5 is redundant.
3. The comparison between Rfold and Ufold could be more comprehensive, 
describing their similarities and differences.
4. There are a few minor typos, and the writing may be improved.

Limitations:
The limitations are adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes RFold, a simple and effective RNA secondary structure prediction algorithm. It adopts attention maps to learn informative representations for RNA rather than hand-crafted features. Then, based on a decoupled optimization process, RFold simplifies and guarantees satisfying the hard constraints on the formation of RNA secondary structure. Through the empirical experiments, the authors demonstrate that RFold achieves state-of-the-art performance with better computational efficiency compared to the previous works.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The proposed decoupled optimization seems simple, but surprisingly effective for RNA secondary structure prediction. To the best of my knowledge, the proposed method is novel in the domain and might be promising for the broader machine-learning community.
- The proposed method shows great performance in three RNA benchmark datasets outperforming the previous state-of-the-art method by a significant margin. Some issues need to be addressed regarding the experiment setup (please refer to the weaknesses), but the improved performance seems truly impressive.

Weaknesses:
Major comments:
- [Data Split] To best approximate real-world applications that may require the prediction of novel structures, RNAs from the train/val/test set should bear minimal sequence and structural similarities. In contrast, it seems the authors have split datasets so that each RNA family has a similar fraction in each set. I think it may overestimate the true prediction performance of RFold. Likewise, “generalization to other datasets” experiments do not provide information about sequence/structure similarities between the datasets. If they are similar, it may not be a fair evaluation of generalization performance.
- Since the authors stated deep learning methods do not ignore the biologically essential structure such as pseudoknots, can you provide additional separate evaluation under the (non-) existence of pseudoknots?
- According to UFold, the bpRNA dataset contains mostly within family RNA species and does not adequately show the true generalization performance of the models. Can you provide additional evaluations with cross-family experiments?
- [Inference Time] It’s unclear whether the results are credible. The inference time can be quite different based on what type of machine (CPU, GPU, etc.) is used for the measurement. Since the other results seem to be excerpted from the UFold paper, the environments of UFold and RFold are likely to be different.
- [Reproduciblity] Architectural hyperparameters are missing. In addition, training codes do not seem to be included in the supplementary.

Minor comments:
- [Data Split] The authors stated that they split the RNAStralign dataset following the E2Efold paper. Can you confirm that all the methods including RFold used the same data splits? RNA sequences often have high sequence and structure similarities, so if you used different data splits it might affect the performance. 
- As the authors stated, other algorithms often post-process the outputs to satisfy the constraints. Can you also show how the results are improved for RFold-E/S with the post-processing?

Limitations:
The authors have not discussed the limitations of the work.

---Post-Rebuttal Comments---

I appreciate the authors' dedication evident in their comprehensive responses. They have effectively addressed many of the concerns I had about the paper. Overall, while some concerns persist, I am inclined to believe that by incorporating the authors' responses, the manuscript's quality would be improved. Hence, I've adjusted my rating to 5.

Rating:
5

Confidence:
3

";0
gZiLCwFT61;"REVIEW 
Summary:
This paper presents a new approach to automatic curriculum learning designed specifically for multi-agent coordination problems.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The main strength of the paper, in my opinion, is the well-formulated approach to the curriculum learning problem. To the best of my knowledge of the related literature, the non-stationary contextual bandit as the teacher and the population-invariant skills for the students are both original and useful contributions to the literature.

Weaknesses:
- My general problem with this paper is that I am finding it hard to evaluate the significance of the work in the automatic curriculum learning sphere without an adequate baseline provided for GRF. Whilst the authors do argue that VACL is not used on GRF due to requiring prior knowledge, it seems unreasonable to therefore provide no baselines that are actually designed for these larger settings. For example, if VACL was unusable, then I would have maybe liked to have seen a comparison to population-based approaches in MARL or any of the other automatic curriculum learning approaches mentioned in Sec. 4. Overall, it is hard to properly evaluate the gains from this automatic curriculum learning framework without seeing the performance of baselines in an environment that actually requires automatic curriculum learning (MPE does not need it according to line 297-298).

I am happy to update my score if the authors can make a reasonable argument against the lack of other baselines in the work.

Limitations:
The authors briefly make mention to the limitations of the work. I agree with the over-design of the framework for simple tasks, so would definitely like to see its performance in more difficult environments that it is designed for.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents Skilled Population Curriculum (SPC) which is a method for learning a curriculum to help a team of agents complete a complex task. SPC models the problem of choosing tasks for agents as a contextual bandit problem, and builds on top of the Exp3 algorithm to solve this bandit problem. SPC also uses an attention-based communication approach, and a hierarchical policy framework. Experiments are performed in the Multi-agent Particle Environment (MPE) and Google Research Football environment (GRF). While MPE does not seem to benefit much from SPC, in the more complex GRF domain the authors show using their SPC approach can accelerate training relative to MARL baselines.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is clear: it is well-structured, and provides a good balance of intuition and detail. It is clearly motivated, and addresses an interesting problem.

- The presented results show clear benefits to the authors' approach.

- The authors use suitable baselines approaches, and suitable environments.

Weaknesses:
- SPC seems to add significant computational complexity vs. baselines like IPPO. While the authors can justify focusing on sample complexity, for completeness they should also record information about the wall-clock time / computational resources needed to train their different baselines.

- In their research question stated on lines 52–53, the authors highlight their desire to consider complex sparse-reward settings. However, the MPE domains are a sparse setting, though fairly simple, and the results here show little benefit to using SPC. On the other hand, the GRF experiments appear to have a somewhat dense reward (the GRF checkpoint reward, which while not necessarily active every timestep, it could be argued is 'somewhat dense'). It seems like absent the checkpoint reward, SPC would struggle because there would be little information in the returns for the teacher agent to use — and I expect this would be the case in most complex (very) sparse reward environments

- Line 277 the authors state MADDPG/MAPPO would not be suitable in these experiments. This might be true in general, but for GRF specifically the critic input size actually would be the same across all tasks (as it pads observations if agents are absent). But since GRF is fully observable, MAPPO is equivalent to IPPO so this is not an issue for this work — though the authors may wish to revise their statement.

- Though the GRF environment is complex and difficult to solve, its level of complexity is somewhat deceptive, as evidenced by the video on the project website. The rollouts show that the agents have learned a simple ""force an offside and run in a straight at the goal line"" strategy which exploits deficiencies in the GRF bots. This behaviour has been observed before by Song et. al (http://arxiv.org/abs/2305.09458). However, this is not a fault of the authors, and is more broadly an issue in the MARL research community. Because of this, it's not clear what skills the agents learn in the training tasks that are useful in the target task. It would be interesting to see a plot of training task performance throughout training.

- It's unclear why the IPPO baselines have a sharp step change in performance around 80 and 90 million timesteps. The authors should investigate this, and perhaps make a comment (at least in the appendix) about why it occurs. In my experience, things like this sometimes occur when training runs stop unexpectedly before the full 100M timesteps, and so the remaining timesteps are aggregating over fewer seeds with lower performance. I would encourage the authors to produce plots reporting the interquartile mean of their results, and produce a plot showing the disaggregated training curves for each seed. These can go in the appendix.

- The authors state (line 301): "" InFig. 5b, we omit the curve of QMix as its mean score is low and affects the presentation of the figure"". I don't expect QMix to perform worse than the presumably near-uniform policies at the start of training for the other agents. So it's not clear how including QMix would disrupt the graph. Is it the case that QMix has a worse average goal difference than -2?

- Can the authors clarify: the target distribution for GRF is ""100% 5vs5""? What is the target distribution for the MPE tasks? (I see now that these are mentioned later in the text: they should be mentioned when introducing the environments)

- It doesn't seem like there's a pattern to the task distribution (Fig. 6a) beyond ""academy_pass_and_shoot_with_keeper becomes less common"". It would be good to see the same plot for other trials. This possibly explainable by academy_pass_and_shoot_with_keeper requiring coordinated passing and shooting, whereas the 5vs5 rollouts (see video on project website) show a very simple GRF-bot exploiting strategy which does not closely resemble the behaviour required in academy_pass_and_shoot_with_keeper.

	- Where the authors claim ""For example, the proportions of 3vs1 and Empty-Goal tasks gradually drop as the student becomes proficient in these scenarios"", it is difficult to support this by looking at Fig. 6a.

- In my opinion this approach is over-engineered, but the authors do acknowledge this. Stripping some components (e.g the hierarchical RL) and focusing on deeply investigating the remaining components would improve this work

- Minor writing fixes:

	- line 42/43 ""more scores"" → ""more goals""

	- line 43: ""4v11"" → ""4v1"" (I assume)

	- line 305: ""tons of"" → ""many"" (more formal tone)

Limitations:
- The limitations section is quite limited, and limitations and assumptions could be more clearly stated throughout.

- The authors recognise that their approach is complex and computationally intensive, so might not be applicable in simple environments. Testing in a complex environment like Google Research Football is a good choice, although due to issues with Google Research Football (such as the exploitability of the built-in AI) it is perhaps not as complex as the authors may hope, even though it has presented a challenge to past MARL research. However, this is a broader issue within the MARL community and the authors of this paper cannot fairly be singled out for this.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces a new automatic curriculum learning framework, Skilled Population Curriculum (SPC), for multi-agent reinforcement learning. The algorithm includes three major components: (1) a contextual bandit conditioned by student-policies representation for automatic curriculum learning; (2) An attention-based communication architecture for policies to learn cooperation and behavior skills from distinct tasks with varying numbers of agents; (3) A hierarchical policy architecture to help agents to learn transferable skills between different tasks. The experiments are conducted in Google Research Football environment and Multi-agent Particle environments, which demonstrate the efficiency of the proposed method to IPPO and VACL.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed method is simple yet efficient in the complex Google Research Football environment. 
2. The motivation of the components are also clear and make sence. 
3. In exepriment, several ablation studies demonstrate the effectiveness of the proposed components; 
4. Also, the paper is overall easy to follow to me. The key idea is easy to understand.

Weaknesses:
This paper could benefit from further improvements in the following aspects:

1. It seems that the manuscript introduces various components. While each one appears to be intuitive and rational in isolation, I recommend that the authors should provide a unifying theme or framework to better connect these components. Presently, it appears as if these components are addressing three discrete issues: a) efficient curriculum learning, b) policy architecture development, and c) communication in varying agent scenarios. It is noteworthy that a paper does not necessarily need to devote substantial attention to the innovative aspects of each introduced components. In the case of this paper, the hierarchical structure, for instance, appears to be a standard approach with limited novelty. The authors can highlight how they design efficient automatic curriculum learning in the context of variable agent scenarios.

2. In the section discussing related work (Line 221), the authors mention various curriculum learning mechanisms without a detailed discussion. Could the authors provide an expanded explanation on how these works conduct curriculum learning and how they relate to or differ from the proposed methodology?

3. There is room for improvement in the experiments section. Specific recommendations are detailed in the questions section.

4. The paper could be further polished, for instance:
    - There are several instances where a capital letter follows a comma, such as in line 40: ""For example, In the football environment, when we…""
    - The legend of Figure 6(b) lacks clarity. It would be beneficial if the authors could provide a detailed explanation of what the labels 0,1,2,3 represent.

Limitations:
NAN

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work introduces the Skilled Population Curriculum (SPC), an automated curriculum learning algorithm designed for Curriculum-enhanced Dec-POMDP. The goal of SPC is to enhance the student's performance on target tasks via a sequence of training tasks provided by the teacher. The SPC functions as a nested-HRL method, where the teacher serves as the upper-level policy and is modeled as a contextual multi-armed bandit. At each teacher timestep, the teacher selects a training task from the distribution of bandit actions, with the context derived from the student policy's hidden state. The teacher's bandit is optimized using the student policy's test reward. The lower-level policy, also known as the ""student"", is in itself a hierarchical policy. The high-level policy implements population-invariant communication using a self-attention communication channel to manage messages from a number of agents, and all students share the same low-level policy.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- This paper is well-presented. Figure 1 is well-designed. I can get a good understanding of this paper's method just by reading this figure.
- The algorithm is implemented with Ray RLlib, though the code is not currently available.

Weaknesses:
- **This study seems to be an overcomplicated amalgamation of pre-existing methods.** SPC stacks three layers of hierarchical policies (teacher 1 + student 2), the teacher is modeled as a multi-arm bandit with a fixed output dimension (number of tasks), and the lower-level control policies of the students are shared. The intricacy of this pipeline leads me to question its generalizability and practical applicability.
- **More rigorous comparison with current MARL algorithms, and need benchmark results on SMAC**, which is de facto the most standard benchmark for MARL algorithms. Please consider adding [MAPPO](https://github.com/marlbenchmark/on-policy), [HARL](https://github.com/PKU-MARL/HARL), and their multi-agent communication variant as your baselines.
- Line 236-238, “However, current approaches that extend HRL to multi-agent systems or utilize communication are limited to a fixed number of agents and lack the ability to transfer to different agent counts”, this is an inaccurate claim because it has been done in the ICLR 2022 publication, [*ToM2C*](https://arxiv.org/pdf/2111.09189.pdf), which similarly uses the HRL with a population-invariant multi-agent communication mechanism. AFAIK this cannot be treated as ""communication limited to a fixed number of agents"". Please consider citing this work and changing your statement regarding the previous work.

Limitations:
The limitations of this paper are only briefly mentioned in the last section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the multi-agent RL problem with sparse reward and a varying number of agents. The authors propose a novel automatic curriculum learning strategy to solve complex cooperation tasks in this setting. Their curriculum strategy involves a teacher component and a student component. The teacher component selects the sequence of training tasks for the student component using the contextual bandit algorithm with predictive representation of the student’s current policy as context. The student component is endowed with a hierarchical skill framework and population-invariant communication. They empirically investigate their proposed strategy in two environments (MPE and GRF).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is overall well-written, and the related work is extensively discussed.

The theoretical results in this paper seem correct; I haven’t checked the details of the proofs.

The population invariant communication module is an interesting contribution to dealing with the varying number of agents across tasks. It would be interesting to compare its effectiveness (on its own) against the existing methods to deal with varying numbers of agents [23, 24].

Weaknesses:
I am unsure about the broader applicability of the contextual representation of the student policy using an online clustering algorithm. How much information will be lost in this process for a high-dimensional policy (e.g., that operates on image inputs)?  

Presented experimental results are not sufficient to validate the effectiveness of the proposed curriculum strategy (specifically the teacher component) in complex scenarios, given that in the MPE environment, the impact/necessity of curriculum is negligible.

Limitations:
The paper is of an algorithmic nature and does not have any direct potential negative societal impact.

Rating:
5

Confidence:
4

";0
zJMutieTgh;"REVIEW 
Summary:
This paper introduces a novel inference attack algorithm for face recognition models that do not have classification layers. The proposed attack consists of two stages: membership inference and model inversion. The membership inference attack analyzes the distances between intermediate features and batch normalization parameters to determine if a face image belongs to the training dataset. The model inversion attack reconstructs sensitive private data using a pre-trained generative adversarial network (GAN) guided by the attack model.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The paper introduces a novel two-stage attack algorithm for face recognition models without classification layers. 
2) The proposed method outperforms state-of-the-art similar works and can recover the identities of some training members. 
3) This research has implications for the development of more robust and privacy-preserving face recognition models.

Weaknesses:
1) This paper would be beneficial to compare the proposed method with more state-of-the-art techniques to demonstrate its superiority.
2) This paper does not provide any code implementation.
3) The model performance used in the experiment is relatively low, and we hope to conduct experiments with models at a higher level of accuracy. (For example, ResNet200/VIT-Large on WebFace260M).

Limitations:
Please refer to the weaknesses section.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a membership inference attack against face recognition (FR) models in the white-box scenario where membership information is known for some records and white-box model access is available, but without access to a classification layer. The attack utilizes information stored in batch-norm statistics and using a meta-classifier, the authors demonstrate the effectiveness of the proposed attack. They also extend the attack to improve model inversion attacks by utilizing their membership-classifier to ""reject"" generated samples that fall below a certain threshold.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The utilization of an attack meant for one kind of privacy leakage (MI) in another scenario (model inversion) is interesting.

- The paper is well written, and proposed techniques/motivation are explained properly.

Weaknesses:
- L16: ""..very first....without a classification layer."" This is not the first work to explore FR models that do not use a classification layers. Please refer to [1, 2]. Similar claims appear on L39 about necessarily requiring logit access for good model performance. The authors in [1] report near-perfect detection for 3 different kinds of learning algorithms/models, none of which require classification logits.

- L25: ""attribute attacks (also known as property inference attacks)"" - these two are not the same at all. Similarly, L31-32 claim that all inference attacks on ML can be categorized into membership and model inversion attacks. Please refer to [3] for a detailed explanation and to better understand these differences.

- L188: ""If it is from the training data set, then the attack model should output 1, otherwise we expect it to output 0"" -how is this membership information obtained? As also outlined in Algorithm 1, the attack very clearly assumes access to not only batch-normalization parameters (which can only realistically come from full-model white-box access), but also knowledge of $m$ members and $n$ non-members. While the latter is reasonable, assuming knowledge of members is a very strong assumption (on top of an already-strong access model).

- L292 says ""...theoretically analyze.."" but nowhere in the paper did I see any theoretical analysis?

# Minor comments
- Figure 2: Why is Stage 2 on the left? It seems counter-intuitive.

## References

[1] Chen, Min, et al. ""FACE-AUDITOR: Data Auditing in Facial Recognition Systems."" USENIX, 2023

[2] Li, G., S. Rezaei, and X. Liu. ""User-Level Membership Inference Attack against Metric Embedding Learning."" ICLR 2022 Workshop on PAIR2Struct 2022.

[3] Salem, Ahmed, et al. ""SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning."" 2023 IEEE S&P, 2023.

Limitations:
- Section 3.1 - most FR models are not trained in the way that the authors describe (and assume in their experiments) here. The norm is to focus on training good embedding models so that models can be scaled easily to enroll future participants. As expected, training the model repeatedly whenever a new user (""class"") is added is not optimal. 

- Figure 3: Model inversion is supposed to recover actual records from the training data, but looking at the images in the figure that doesn't seem to be the case. Many faces (like second from left, second from right in first 2 rows) are not the same people, but rather people that ""look like"" each other.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper presents a novel method for inference attacks against face recognition method. In particular, it advocates two-stage inference attack, where the first stage identify the membership and the second stage involves model inversion attack that recover the input from embedding. Experimental evaluation shows that the proposed method can largely identify the correct membership and the model inversion sees good as well.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. It proposes a novel two-stage method for inference attack of face recognition systems.

2. Experimental evaluation shows the proposed method has promising results.

Weaknesses:
1. The evaluation is not thorough. It only conduct some ablation study with comparing to the existing attack methods.

2. The paper does not cover some import topic for inference attack of face recognition, such as the black box attacks for face recognition.

Limitations:
unavailable.

Rating:
4

Confidence:
3

REVIEW 
Summary:
In this submission, the authors advocate an inference attack composed of two stages for practical FR models. The first stage analyzes the distances between the intermediate features and batch normalization parameters. The second stage reconstructs data using a pre-trained generative adversarial network (GAN) guided by the attack model in the first stage.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.The writing and presentation are good and easy to follow.

2.The experimental results also demonstrate some effectiveness of the proposed method.

Weaknesses:
1.The overall technical contributions are somewhat limited, firstly, the way of using BN to perform membership inference attack has been explored for a long time. And secondly, the inversed training data are from a pretrain GAN, which is heavily depending on the strength of the pretrain GAN. And optimizing the synthesized face data is too weak only by the single supervision from the first stage. 

2.And from the Figure 3, I don’t think the recovered face data is visually close to the original data for some of the pairs. Therefore, I doubt that whether the authors really achieve the initial goal, recovering the similar enough or effective enough face training data, by their proposed method or not.

3.The experimental comparisons are too simple and rough, lacking some important state-of-art related competitors. 

Limitations:
This submission has adequately addressed the limitations.

Rating:
3

Confidence:
4

";0
5uIL1E8h1E;"REVIEW 
Summary:
This paper studied the problem of learning a graph neural network based policy network as a construction heuristic for solving job shop scheduling problems. This paper proposed an idea called residual scheduling to remove irrelevant operations and machines from the graph based state representation. This has been shown experimentally to perform well on several benchmark job shop scheduling problem instances.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
With the fast advancement of deep learning technologies, it becomes increasingly interesting to develop deep neural network models that can effectively solve complex combinatorial optimization problems, including job shop scheduling problems. The newly developed deep learning system in this paper appears to be very effective and highly competitive in performance, compared to similar approaches from the literature.

Weaknesses:
The idea of using graph neural networks or other forms of deep neural networks trained through reinforcement learning to solve job shop scheduling problems has been studied in many past research works. The main text of this paper lacks a comprehensive review of these research works, making it hard to clearly understand the key technical novelty and contribution of this paper, compared to other recently published works.

The development of the residual scheduling technique is not strongly motivated in this paper. It is hard to understand why it is essential to remove irrelevant operations and machines from the graph based state representation. The development of this technique also appears to be very highly intuitive and lacks thorough theoretical analysis. Hence, the technical contribution of this development remains largely questionable.

The authors stated that in order for the process of learning the construction heuristic to be formulated as an MDP, they introduced several attributes for the operation nodes and machine nodes in the graph based state representation. However, it remains largely unknown whether, with the introduced attributes, the state representation can satisfy the Markov property of the MDP. The importance of using any newly introduced attributes should be more thoroughly evaluated experimentally. The associated technical contributions should be clarified and strongly justified.

Some other key aspects of the new system design should also be justified more. For example, the use of GIN needs to be supported with more convincing reasons. The focus on learning a construction heuristic rather than an improvement heuristic, which is gaining increasing popularity and attention, should be better justified and experimentally validated in this paper.

Limitations:
I do not have any concerns regarding this question.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel approach called residual scheduling for solving the Job-shop scheduling problem (JSP) and its variant, flexible JSP (FJSP), focusing on removing irrelevant machines and jobs from the consideration set. Despite these problems being NP-hard, the proposed method demonstrates state-of-the-art performance across standard benchmarks, even performing well when scaled to larger problem sizes, achieving a zero gap in 49 out of 50 instances with more than 150 jobs on 20 machines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method developed claims that for the 98% of the cases, the zero gap is achieved for fairly large instances.

Weaknesses:
The zero gap is mentioned. Upon reading, readers find out that the gap refers to ""makespan gap."" Understandably, significant bulk of existing papers on job-shop focus on makespan rather than tardiness. Ignoring tardiness may lead to poorer on time delivery, which is a weakness in itself, but with respect to the ""makespan gap"" measure it is not clear whether zero gap will result in zero (or small gap) is tardiness is considered or, generally, if the standard duality or MIP gaps are used instead. The gap is misleading at best.

Limitations:
In light of the above comments, the limitations are 1. only makespan is considered, 2. only one type of gap seems to be introduced and considered, and 3. major heuristics are not used/compared with.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed DRL based method to learn dispatching polices for (flexible) job-shop scheduling problems (JSP/FJSP). The main idea is to remove the completed operations from the state embedding, which is called residual scheduling, so as to improve the representation accuracy. The DRL agent uses a graph representation, which is processed by a Graph Neural Network (GNN) architecture. Experiments on JSP and FJSP benchmarks show that the proposed residual scheduling scheme outperforms recent DRL baselines.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The idea of residual scheduling makes great sense and is interesting.

2. The method is generally applicable to both JSP and FJSP, which are important scheduling problems.

3. Good empirical performance, comparing to recent DRL based scheduling methods.

Weaknesses:
1. The main weakness is that the technical contribution is incremental. While the redisual scheduling idea is interesting and novel, a large part of the proposed method is similar to existing works. Specifically, the graph representation and heterogeneous graph neural network in Section 3.2 and 3.3 is similar to the heterogeneous graph and heterogeneous GNN in (Song et al., 2023). This is not mentioned in Section 3, and the differences between the proposed method and existing works are not discussed.

2. Some design choices need to be justified. Please see the below questions.

3. Empirical evaluation needs to be improved. 

* It is unclear whether the models in other works (e.g. L2D, SchN, DRL-G) are retrained using the same dataset as the proposed method. If not, then directly comparing their performance (even on the same benchmark instances) is not fair due to different training data.

* The discussion for Figure 4 is not surprising. It is well known that for JSP, problems with larger $n/m$ ratios are easier to solve (Taillard 1993). That is why the gaps on large problems in Figure 4 are smaller. Actually this is true for most algorithms, and cannot be claimed as a major advantage of the proposed method.

* Training time is not reported.

4. The authors made several inappropriate statements in the paper, mainly in introduction. The authors should be more precise about the related concepts.

* In the first paragraph, it is better to describe JSP as a combinatorial optimization problem, instead of mathematical optimization.

* In the second paragraph, Constraint Satisfaction Problem (CSP) should be not stated as a type of mathematical optimization. In addition, Constraint Programming (CP) here is more suitable than CSP. 

* In the fourth paragraph, the approaches that automate the design of heuristics should be hyperheuristics, instead of metaheuristics.

4. The language needs to be improved. Besides, there are quite a few grammar errors and typos.

Limitations:
The authors did not give a particular discussion on the limitations. One limitation could be the computational efficiency. As shown in Table 13 in the appendix, the runtime increase rapidly with the number of jobs. This could affect both training and inference efficiency, and limit its applicability to larger problems.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a deep reinforcement learning-based constructive heuristic to solve the (Flexible) Job Shop Scheduling Problem. An instance of the problem is represented as a graph and fed into a Graph Neural Network-based model which outputs a score for each candidate (operation-machine) pair. The model is trained with the REINFORCE algorithm with as baseline a classic Priority Dispatching Rule-based heuristic. The novelty of the paper lies in the update of the state after each action: the graph is updated by removing the operations which have already been executed to focus on the most relevant information, which is the residual operations and remaining times of the ongoing ones. The proposed approach is experimentally shown to outperform RL-based constructive heuristics on classic JSSP and FJSSP benchmarks.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Sound and interesting idea of removing irrelevant information from the state 
1. The paper is fairly clear and I appreciated the illustrations Fig 1-3.
1. The model was proposed for the JSSP and easily adapted to the Flexible JSSP
1. The approach outperforms deep RL-based construction heuristics on classic benchmarks

Weaknesses:
1. Limited novelty: the main contribution is the definition of the residual state at each step of the construction process by removing irrelevant operations and resetting the time reference. This seems to me an incremental improvement of the approach L2D [1], which already proposed a similar state graph representation and the use of the Graph Isomorphism Network architecture for the JSSP.
1. The proposed approach seems very specific to (F)JSSPs (state representation, baseline) and it's not clear what could be transferable to DRL heuristics for solving other optimization problems.
1. In the experiments, the presented non-learning-based baselines seem pretty weak: only greedy (see more in Questions)
1. Lots of English typos (I noted a few per page)

[1] C Zhang et al, Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning, Neurips 2020

Limitations:
Not explicitly addressed by the paper.

Rating:
6

Confidence:
5

";0
SJw4Da8BuR;"REVIEW 
Summary:
The paper proposes a subspace projection algorithm spanned by the features' principal components. 
The projection's fusion with the different network layers is presented.
A gradient-free pruning approach is further suggested based on the parameters and activation statistics.
Finally, the proposed framework is experimented on BERT and T5 and achieves a compression ratio of 44% with at most 10 1.6% degradation.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is fairly written.
The apparent main novelties of the paper are the low-rank approximation of the features and a statically based pruning approach. 

Weaknesses:
Low-Rank approximation:\
The main weaknesses seem to arise from the comparison to prior/competing works.
For example, low-rank approximations of features have already been presented see for example 
https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf \
Also, I am unsure why lines 37-38 are true: performing PCA (/Kosambi–Karhunen–Loève) is a pretty old technique for model acceleration, the subspace being defined by the principal components of the parameters or activations, this is a low-rank approximation.\
Thus, the low-rank approximation contribution of the paper should be narrowed to the definition of the data matrix.

Pruning: \
it's unclear how these simple statistics perform compared to other pruning methods or heuristics.

Experiments: \
the subspace dimension as well as the compression ratio are not given which leaves the speed-up metric subjective.
The method performance on mid-size LLM is not very good compared to old methods. \
Random projection as ablation is a very weak baseline.

Clarity:\
The paper can be refined in terms of clarity (also typos (e.g., lines 235, 282))

Limitations:
No limitations discussed

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents TCSP, a model compression approach for transformers by reducing hidden size via low-rank factorization. In addition, TCSP is compatible with other compression methods such as model pruning and head size compression. Experiment results demonstrate the effectiveness of proposed method, achieving a high compression ratio while incurring rare performance drop.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
$\cdot$ This paper is well-structured and clear to understand.

$\cdot$ The algorithm is general enough, and is compatible with other compression strategies.

$\cdot$ Experiment results verify the effectiveness of the proposed method.

Weaknesses:
$\cdot$  The novelty of this paper is limited, the core idea resembles low-rank factorization with SVD, and the approach is more like a combination of SVD and model pruning.

$\cdot$ The author claims it is the first work to reduce the hidden size, but I doubt if the method can be successfully implemented in the industry since the lack of experimental results related to inference speed of compressed model.


Limitations:
The author has addressed the limitations and social impacts in Appendix.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes a decomposition-based method, called Transformer Compression via Subspace Projection (TCSP), for compressing transformers. By decomposing the feature matrix extracted by some sample data, the model is projected onto a subspace to reduce the size of hidden dimensions. Experimental results on the datasets GLUE and SQuAD show that TCSP enables 44\% parameters reduction with at most 1.6\% accuracy loss and surpassing existing methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper compresses the hidden dimension of the transformers, which is less explored.

The overall presentation of the paper is easy to understand.

Weaknesses:
TCSP is indeed just principal component analysis (PCA) or compressed sensing (CS), all working with the dominant subspace derived from SVD. Why another name?

I have concerns about the following aspects:

1. Motivation: From lines 51-59, this paper discusses the compression methods for transformers. Also, it mentions ""We do not delve into knowledge distillation and weight sharing, as they involve training models from scratch"". However, knowledge distillation (KD) includes both task-agnostic and task-specific schemes. For task-agnostic KD methods, they do not involve training models from scratch, see e.g., Wu T, Hou C, Zhao Z, et al. Weight-Inherited Distillation for Task-Agnostic BERT Compression. Meanwhile, it is a normal setting in KD to reduce the hidden size of the transformer model. Therefore, task-agnostic KD methods should be compared, too.

2. Computation: As TCSP requires SVD decomposition for a large matrix, more discussion about the computing cost and scalability is needed, especially in Table 2.

3. Robustness: Regarding the quality of subspace, how is the performance if we add noise or adversaries to the input data when generating the projection matrix? How do you ensure the sample data are representative? The timing overhead and complexity of the SVD to ensure a good projection subspace should be explicitly characterized and quantified.

Indeed, there are recent decomposition-based compression algorithms applied to transformers which the authors may benchmark against, e.g., Ren, Y., Wang, B., Shang, L., Jiang, X., \& Liu, Q. (2022). Exploring extreme parameter compression for pre-trained language models. arXiv preprint arXiv:2205.10036. 



Limitations:
The models used are relatively small in size, e.g. T5-base, BERT-base. There are no experiments on the Large Language Models.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to compress the hidden size of a transformer model using subspace projection. On a high level, the paper aims at projecting the transformer model into a lower dimensional subspace using a projection matrix that is computed using a sample of the training data. This method is compared against other compression techniques using the T5 and BERT models on GLUE and SQuAD datasets and it is shown that the proposed method performs on par or better then the methods under comparison. The highlight of the experimental result is that the proposed transformer compression via subspace projection technique is able to compress models by as mush as 44% with only 1.6% degradation in performance. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper addresses an important problem of transformer compression. In the age of ever-increasing model sizes, it is vital to develop methods that compress large models with minimal loss in performance, if any. This paper presents a simple yet effective approach to leverage linear subspace projection for compression. The paper is easy to follow, offers sufficient literature review, and presents convincing experimental results. The experimental results are particularly strong -- 44% compression with only 1.6% loss in performance. 

Weaknesses:
Some notation is used before definition. Could include more recent literature in Related Work section -- see below.

Limitations:
NA

Rating:
7

Confidence:
4

";0
ztqf6bzuqQ;"REVIEW 
Summary:
This paper introduces a new hybrid distillation method for Vision Transformer. It is hybrid in the sense that two teacher models pre-trained by two different approaches, namely Contrastive Learning and Masked Image Modeling, are adopted in the distillation. In this way, the student model inherit both the diversity from MIM pre-trained teacher model and the discriminability from the CL pre-trained teacher model. Further more, a progressive masking strategy is adopted to reduce redundancy during distillation. Experiments on multiple popular classification/detection/segmentation benchmarks demonstrate the effectiveness of the proposed approach.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The idea of adopting two homgeneous teacher models is interetsing. Intuitively, a good design allows the student model to learn the advantages of both teacher models.

Weaknesses:
1. I find many experimental details in the explorative model evaluation part (section 2) are missing, making it hard for readers to assess the soundness of those experiments. Note that these experiments are crucial for justifying the motivation of this work. My questions are listed as follows: 
a) What kind of distillation approach is used here? Is it the same as the one used in Hybrid Distillation, or just naive feature distillation? Also, details like where to distill and distill objectives are missing. 
b) Why are the average head distance and normalizedmutual information good indicators of the discrimination power and diversity of the learnt representations? And how are the so called `discrimination and diversity` related to the downstream performances? So far I have only seen very vague definition of these two metrics, and I am not sure if they are solid proofs that teacher models pre-trained by different objectives do have different advantages that the student must learn. 
c) The notations need further clarification. For instance, in Figure 2(a), the caption says 'various decoder' , indicating the curves are the values of NMI of the decoder. While there is actually a curve for `no decoder`, indicating the curves are the values of NMI of the encoder. These notations are very confusing.
d) The authors claim that `The Increase in Diversity Derives from the Asymmetric Designs`, but there are no ablation on if the symmetric design of MIM architecture would lead to the same diversity. The closet design is in section 2.4, but its objective is to reconstruct high-level features from the teacher model.

2. Similarly, some implementation details about the Hybrid Distillation itself are missing. For example, I do not believe the student is solely trained by the distillation objective. Is there also a supervised training objective like cross-entropy loss? The authors should make it more clear. 

3. I wonder how did the authors get the results of baseline methods. For example, in table 1, CLIP ViT-B achieves an accuracy of 83.6 on IN-1K, and CLIP ViT-L achieves an accuracy of 86.1. These numbers are too good to be zero-shot results, so I have to assume they are fine-tuning results. Yet, based on [1], fine-tuning CLIP ViT-B achieves an accuracy of 85.7, and CLIP ViT-L achieves an accuracy of 88.0 on IN-1K, which have achieved better or on par performance comapred to the Hybrid Distillation (85.1 for ViT-B, 88.0 for ViT-L). In this case, the proposed Hybrid Distillation approach does not seems to show enough empirical gain.

4. As already discussed in the limitation section in the supplemental material, the gain of Hybrid Distillation over Distill-CLIP is not so significant (only 0.3\% on IN-1K with ViT-B).

References
[1] Dong X, Bao J, Zhang T, et al. CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet[J]. arXiv preprint arXiv:2212.06138, 2022.

Limitations:
The authors have included discussion on the limited gain over Distill-CLIP.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work presents Hybrid Distillation, which attempts to distill from both supervised/CL and MIM frameworks. The work begins by revealing certain observations regarding the interplay between self-supervised pre-training and the concepts of diversity and discrimination. Subsequently, the authors propose the Hybrid Distillation technique that leverages token relations from the MIM teacher and feature maps from the supervised/CL teacher for knowledge distillation purposes.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The findings on the relationship between diversity and architecture are interesting

Weaknesses:
1. The presentation quality of this work is bad due to the following reasons:

i) Section 2 lacks an explanation of the experimental setup, For example, what is the design and architecture of the distillation? How are different decoders used for DeiT distillation? 

ii) The description of the metrics, such as the average head distance and normalized mutual information, is inadequate. There is insufficient clarification regarding the existence of multiple attention distances for each layer and how they reflect diversity. Furthermore, the explanation of how NMI reflects discrimination is absent.

iii) The analysis of the figures lacks details, making it challenging to comprehend the meaning conveyed by the illustrated figures.

2. The authors state that ""Mask Reconstruction of High-Level Semantics Does not Help Improve Diversity."" However, previous studies [1][2][3][4] have distilled knowledge from high-level semantics and achieved high performance. Does this imply that high performance can still be attained with low diversity? If so, why should we care about diversity?

3. The correlation between the observations made and the design of the proposed Hybrid Distillation method is weak. For instance, how does the discovery of the ""Asymmetric Decoder"" inspire the proposed Hybrid Distillation approach?

4. The absence of a discussion and comparison of relevant works is noticeable. Numerous related works, such as [5][6][7], should have been included and compared.

5. Unfair comparisons are made in this work. While the proposed approach is distilled from multiple networks, it is only compared with methods that distill knowledge from a single network. Strong baselines that employ distillation from multiple networks should also be incorporated for a fair evaluation.


[1] IBOT : IMAGE BERT PRE-TRAINING WITH ONLINE TOKENIZER

[2] DINOv2: Learning Robust Visual Features without Supervision

[3] Masked Feature Prediction for Self-Supervised Visual Pre-Training 

[4] BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers

[5] Mimco: Masked image modeling pre-training with contrastive teacher.

[6] Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations

[7] Contrastive Masked Autoencoders are Stronger Vision Learners

Limitations:
None

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper introduce a new distillation method that complimentary harmonizes two distillation method of different properties.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* Hybrid Distillation obtained higher accuracies than DeiT, MAE, and CLIP using them.
* Explanation with analyses (NMI, AHD, and attention map visualization)
* The paper is clearly written

Weaknesses:
* When I read the explanation in the method section, the proposed method seems very inefficient compared to methods without distillation (e.g., MAE, DINO). It would be better to compare quantitatively with throughput and total training hours.

* Some values in tables are different to original values in references.
    * The COCO detection performance of MAE in this paper and MAE paper are different. The AP^box and AP^mask of MAE are reported as 50.3% and 44.9% in MAE paper while they are reported as 48.4% and 42.6% in this paper, respectively.
    * The transfer learning result of MAE toward Naturalist19 is also different to the value in MAE paper. MAE paper report it as 80.5% for ViT-B while this paper report it as 75.2%.

* Are the explanations provided in the preliminary section the author’s own contributions or are they similar analyses  conducted and explained in other references? If they are the author’s contributions, which points can be regarded as novel?

* Some points are not understandable
    * The authors distilled the features of the supervised and CL models and MIM models to features of the different layers in student model regarding diversity and discriminatively. However, in the current design, the distillation performed on the last layer affect the front layers without detaching back-propagation of the distillation loss. Is this the intended situation?


Limitations:
* Its computational inefficiency was my primary concern, and the authors addressed it in the limitations section.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper conducts sufficient experiments and theoretical analysis on diversity and discrimination. 
Meanwhile,  the authors propose a simple yet effective hybrid distillation that combines contrastive learning pre-train and MIM pre-train.
This hybrid distillation achieves significant improvement on downstream tasks.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written, with sufficient experiments and analysis. 
- The accuracy improvement is significant.

Weaknesses:
- The paper has some minor typo errors.


Limitations:
- Hybrid Distill does not improve CLIP as much as DeiT after introducing the MAE teacher. The authors could spend more pages analyzing the relationship between the MIM branch and the CL branch, as well as the underlying reasons.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper explores the subject of representation learning, focusing on two aspects: discrimination and diversity. Contrastive learning (CL) exhibits superior discrimination capabilities but suffers from limited diversity. Conversely, masked image modeling (MIM) offers greater diversity but shows weaker discrimination abilities.

The paper presents three insightful observations and integrates the benefits of both approaches, optimizing them for downstream tasks. In this context, ""hybrid distillation"" refers to the process where models are distilled using both contrastive learning (CL) for discrimination enhancement, and masked image modeling (MIM) for improving diversity. To reduce the training cost, the paper also proposes a token dropping strategy.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper has a solid experimental design to first re-examine discrimination and diversity, then propose its method and last to show its improvements on different downstream tasks. The idea is simple but effective and intuitive.

Weaknesses:
- Please ensure that the term ""asymmetric X"" is used consistently throughout the paper. The document refers to several variants of the term, including ""asymmetric attention,"" ""asymmetric architecture,"" ""asymmetric decoder,"" and ""asymmetric designs."" It would be beneficial to differentiate between these concepts and clarify which is the primary focus of the paper.

- On line 104, the paper introduces the notation I() and H() to represent the mutual information and entropy, respectively, but does not explain how these quantities are calculated. For clarity, consider adding a brief explanation or citation for the methods used to estimate mutual information and entropy from the data. 

- Similarly, on line 156 the notation S' is introduced without explanation. Please consider defining or explaining how S' is derived.

- When ""feature distillation"" is mentioned on line 106, adding a reference to the specific feature distillation approach used in the experiments would help clarify this concept for readers. Providing a citation would allow readers to refer to details of the feature distillation method.

Limitations:
-

Rating:
6

Confidence:
5

";0
9ych3krqP0;"REVIEW 
Summary:
This paper presents MultiFusion, a multilingual multimodal image generation model, which can be effectively trained by fusing existing pre-trained visual models, language models, and stable diffusion models. Using a multilingual autoregressive language model as a bridge, MultiFusion follows MAGMA to enable multimodality by learning adapters. Before connecting the language model with the stable diffusion module, it learns semantic embeddings with a contrastive learning objective in a parameter-efficient setup. Finally, it connects the language model with the diffusion model with monomodal data. i.e., an image or caption.

Experimental results demonstrate that the trained MultiFusion model can generate high-quality images with multimodal interleaved prompts. Besides, with modular design and the fusion of pre-trained models, the training can be quite effective compared to training from scratch. Several analyses such as attention manipulation also provide insights into the multimodal language models.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- The paper makes a clever combination of pre-trained models and adapter learning techniques, including 1) MAGMA for cross-modal adaptation/fusing, 2) contrastive learning before fusing LM with the diffusion module, 3) cross-attention learning of SD to align the conditioning with the new embedding space. These operations delicately combine the pre-trained modules into an end-to-end multimodal-text-to-image model.

- Experimental results show that MultiFusion can produce high-quality images conditioned on multimodal and multilingual inputs, with a wide range of applications and use cases. 

- The analysis on attention manipulation is quite interesting.

Weaknesses:
- It would be great to improve the presentation of the paper, especially methods and implementation details. Although Figure 1 presents an overview of the architecture, I have to guess some of the implementation details and carefully find clues from a large amount of text. Some suggestions: (1) you could provide some figures to show the details of how the adapters connect the pre-trained models and how you learn them; (2) you could also clarify the training tasks, and data in tables.

- Existing works have explored how to learn adapters to connect pre-trained modules. For example, Flamingo learns gated adapter modules to connect language models with visual models, and generate text conditioned on multimodal inputs. The paper provides a full solution to the problem with careful design but it is kind of an integration of existing adapter methods.

Limitations:
As mentioned in the paper, the model always produces variations of input images, which limits its applications in image editing. I think it is worth mentioning this limitation, which provides further understanding of MultiFusion.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors present a novel approach to expressing complex concepts with arbitrarily interleaved multimodal and multilingual input. Their approach leverages pre-trained models and allows an efficient fusion of different component without training a model from scratch.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow
2. The experiments are well-designed and allow one to use existing pre-trained models while reducing the demand to train a system from scratch.
3. The result on various benchmarks are promising and would invite more discussion in this line of work.

Weaknesses:
The motivation to attempt such a problem is rather weak. Under what circumstances, would one want to have interleaved multimodal input to generate images? Is it because we want to control the input? If so, why not compare the proposed approach with similar models such as ControlNet and DreamBooth? 

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors introduce MultiFusion, a novel approach that enables the expression of complex and nuanced concepts in text-to-image diffusion models (DM) through arbitrarily interleaved inputs of multiple modalities and languages. The “fusion” concept is at the core of the whole work: to fuse modalities together, pre-trained models (a LLM and a stable diffusion backbone) are fused together. Experimental results highlight the efficient transfer of capabilities from individual modules to the downstream image generation module. Notably, MultiFusion empowers the image generation module to effectively utilize multilingual, interleaved multimodal inputs, even when trained solely on monomodal data in a single language. The contributions of this work include the fusion of modalities for image generation, experimental evaluations, and the introduction of a benchmark dataset for further analysis and comparison regarding the multimodal compositionality of the models.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
1. **Innovative model fusion approach**: The paper introduces an innovative approach by combining a partially frozen multilingual Language Model (LLM) with a stable diffusion backbone. This fusion results in an interesting multilingual and multimodal encoder capable of seamlessly interleaving between input items, treating them as a modality-agnostic sequence.
2. **Multilingual alignment investigation**: The authors conduct an investigation into the model's multilingual capabilities by translating the prompts from the DrawBench dataset. This exploration demonstrates an understanding of the importance of multilingual alignment. While there is a question regarding the accuracy of the translations, the authors acknowledge the potential benefit of utilizing literal translations in training the multilingual encoder, even though nuances in meaning may not be fully captured. This highlights the authors' attention to addressing the challenges and complexities of multilingual representation.
3. **Contribution of benchmark dataset**: The authors contribute to advancing research in multimodal compositionality by producing and sharing the MCC-250 dataset. This benchmark dataset, described in detail in the supplementary material, serves as a valuable resource for assessing the compositionality of multimodal inputs, specifically comprising English text and images. The production and release of this dataset demonstrate the authors' dedication to promoting reproducibility, comparison, and further progress in the field of multimodal compositionality.

Weaknesses:
1. **Lack of clear architectural design and novelty**: The paper suffers from a lack of clarity in explaining and justifying its design choices. While references are provided, the underlying motivations and problem-solving aspects of these choices are not adequately explained. While Figure 1 attempts to illustrate the model structure, it is not accompanied by a clear rationale and explanation for the chosen modules and their interactions in the text. Enhancing the clarity of the architectural design would elevate the novelty and originality of the proposed approach. It is suggested to provide a high-level description that guides the reader in understanding the motivations behind specific choices. By focusing on the ""why"" rather than the low-level details, readers can, for example, grasp the purpose of unlocking only the biases in the LLM. Currently (line 130-131), it is unclear if this is a crucial step to obtain good results while keeping a parameter-efficient regime, or if it is marginal in that regard. The supplementary material can be utilized to provide additional low-level details for interested readers.
2. **Lack of clarity in Figure 4 and semantic search paragraph**: While Figure 4a demonstrates higher similarities of translated prompts in the authors' method compared to competitors, it does not provide insights into the similarities between the reference and other negative samples. This additional information is crucial to establish the range of similarities that can be considered as genuinely low. Furthermore, Figure 4b indicates that the AltDiff competitor generates potentially more consistent images in each language, suggesting that the embedding similarity between references and translations may not be entirely representative. Clarifying these aspects would enhance the understanding of the results and provide a more comprehensive evaluation of the proposed method's performance.
3. **Missing standard deviation in tables**: Including standard deviation in the results would provide important information about the variability and statistical robustness of the findings. By incorporating this measure, the paper would strengthen the reliability and credibility of the reported results.
4. **Performance comparison and insights**: In Figure 4b, the AltDiff method demonstrates better performance, raising questions about the potential benefits of adding more languages to the MultiFusion method. While the authors suggest that alignment remains similar despite MultiFusion being fine-tuned using only English data, additional experiments are needed to provide substantial evidence that adding more languages to MultiFusion indeed yields improved results. Further investigation and insights in this area would enhance the value and understanding of the proposed method.

In offering these critical observations, I would like to emphasize that my intention is not to be harsh, but rather to provide constructive feedback. I acknowledge that explaining such a complex pipeline can be challenging and that significant effort has been invested in this work. However, I strongly believe that there is room for improvement in describing the architectural choices and highlighting the strengths of the paper, and I’ve tried my best to give possible suggestions in this regard. I’m convinced that addressing these aspects would significantly improve the quality and impact of the paper, but I don’t think this is something that could be fixed within the rebuttal period. In any case, I remain open to reconsidering my recommendation if any relevant insights emerge during the discussion.

Limitations:
The authors adequately addressed the limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposed a novel method to build multilingual multimodal generation models that supports prompts composed of interleaved text and image. It combines a strong pre-trained multilingual language model with the image generation model from Stable Diffusion (SD) and achieves alleviated capabilities such as prompting with text and image combined. It also shows that the new model does better in composition generations, as one can provide reference images as part of the prompt. 

In this work, a multilingual language model is trained in the first place (13B encoder-decoder structure model trained on 400B tokens), which itself is a strong multilingual model. It then adds an adapter module to the LLM model to support input in image format, following methods proposed in MAGMA. Finally, it aligns the trained encoder with the diffusion model taken from Stable Diffusion, with 15M text-image pairs. As a result, it can support multilingual, multimodal prompt for image generation, without training on massive text-image pairs dataset.

In experimentation, it showed that using both text and image as prompts can be beneficial, especially in composition generations. It also shows superior performance to existing multilingual text-image generation model AltDiffusion, possibly due to better alignment in multilingual embeddings. Further, the support of taking image as prompts can enable varies applications such as negative prompting with image, image composition, image variation and style modification.  

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
* Novel and efficient method: fusing different pre-trained models works very well which can bootstrapping existing models such as stable diffusion to achieve different input format, avoiding the heavy cost of training model from scratch.
* Enables prompting using both image and text and generates better images both in terms of metrics such as FID and human evaluation,  comparing to baselines that only takes text prompt.
* Better results on composition generations from considering reference images in prompts.
* Well written overall and addressed limitations of the work very well.

Weaknesses:
* Some of the details such as model parameter size, training data source and size are not presented in the main paper (included in appendix), which can be less clear when interpreting results presented in experimental section. It would be better to point those factors out when comparing with baselines in the main paper.



Limitations:
The authors addressed the limitations relatively well in the paper: 
1. The generated image cannot do copy of exact prompt images;
2. Sometimes the image prompts need to be carefully chosen and do not always work;
3. It suffers the same shortcomings (such as inappropriate content) as other generation models trained on very large-scale crawled dataset (LAION).


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents an approach for creating a model that can take interleaved sequences of images and multi-lingual text as input, and generate novel images as output, by fusing together pre-trained models: (1) a ResNet image encoder from CLIP (2) an encoder-decoder text Transformer LM and (3) a Stable Diffusion (SD) image decoder. Most of the weights of the models are frozen, with some fine-tuning of adapter layers, the biases of the LM, and the cross-attention layers of the SD U-Net. Multimodal training is done on a combination of large scale image captioning, and VQA datasets, using the standard text-conditioned diffusion objective. The encoder-decoder text model was pre-trained on multi-lingual data, making the resulting image generation model also multi-lingual. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
S1) The aim of this work, enabling an image generation model to take both images and text (in multiple languages) as input, is exciting and well-motivated by some of the qualitative examples in the paper: multi-modal inputs give complementary info, and multi-lingual text capabilities should broaden model accessibility. 

S2) I found the experiments on compositional robustness (MCC-250), with the improved results from the combination of this text encoder and the image inputs, interesting and think it has the potential to be a timely addition to the ongoing conversation about the role of the pre-trained text-encoder in compositional robustness of image generation (but see suggestions on baselines below). Doing a human evaluation user study was also a real strength of these experiments.

S3) The qualitative results were compelling, particularly Figure 5 in the main text and Figure 4 in the appendix. 

Weaknesses:
W1) The experimentation was a bit thin. 
- Although there is definitely a shortage of current benchmarks for the new capabilities presented by this model, the contribution of the paper would be stronger if it were able to reappropriate existing benchmarks or create new ones to evaluate some of these capabilities (e.g. negative prompting with images, multimodal image composition). 
- The method has a few steps (e.g. contrastive fine-tuning on a natural language inference dataset; training on a large number of multimodal datasets, both VQA and captions; and using attention manipulation), but I couldn't find any ablations on these components. This, in combination with the lack of details on the [apologies, the rest of this sentence was missing earlier] datasets, makes me worried about whether the overall approach will benefit future work.
- The quantitative results that are presented here would be more convincing with a few (hopefully) easy-to-run variants of the current settings (another classifier-free-guidance weight; ablating image inputs in the MCC-250 experiment); see questions below. The compositional robustness results are interesting, but giving an image as input is a pretty strong (and potentially unrealistic) source of supervision.

[update after response] : I still feel that point a) above, about capability evaluation, is a weakness, but the author response definitely helped address the other points. Thank you!

W2) The method relies on proprietary datasets and models for the language model (and possibly also for the image datasets, see questions below). I don't think this would be a crucial weakness except that almost no information is given about these datasets and models, even in the appendix. Given that the LM is frozen when doing the multimodal training, and that the capabilities of the fused system (with respect to multi-linguality, and the compositional robustness experiments) seem very likely to me to depend on the properties of this LM, more openness (ideally, using a publicly-released multimodal encoder-decoder transformer, like mT5-XXL, which also has 13B parameters) would really enhance the scientific value of this paper.
- The encoder is described as a ""13B transformer encoder-decoder similar to GPT-3"", but GPT-3 is a decoder-only model, trained with a language modeling objective.
- The LM dataset is described only as ""400B tokens of English, German, French, Italian, and Spanish"", and it's unclear whether the multimodal training data includes datasets other than the ones listed in lines 17-18 of the appendix. 
- The German-English versions of SNLI and MNLI used for the semantic embedding objective also seem to be proprietary. 

W3) The writing was somewhat unclear. In particular, a lot of details about the model (the pre-trained models used, the training data for the full approach) were unspecified in the main text, although outlined in the appendix (Section A). Some details about the experiments were also unclear, see questions.

[update after response]. The response effectively addressed both W2 and W3 -- thanks!

Limitations:
I felt that the limitations section was pretty solid in qualitatively outlining weaknesses of the approach, although I'd appreciate experiments to quantitatively support the claim that attention manipulation can help prevent image context overriding text context.

Rating:
6

Confidence:
4

";1
biaOpY5gAo;"REVIEW 
Summary:
This paper proposed a low-rank way of training LLMs which allows more flexibility than most if not all popular low-rank training methods. It's a parameter-efficient way which only consumes limited gpu usage so it has the potential to be applied to larger models.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The rationale behind the idea is reasonable. We'd like to have a higher rank models to provide capacity and in the meantime keeping # trainable parameters small.

2. The Introduction reads well.



Weaknesses:
Overall, I like the idea but somehow there are few concerns that I think needs to be improved in order to accept it.

1. It's not well written as many there are many notation unexplained (L85 r), linkage error (e.g., L106 sec 3), and the algorithm is not well explained in a line by line fashion. Many details are skipped, which leads to the problem that reading the code is the only way to fully capture the proposed method.

2. The motivation is not convincing. Rank(A + B) < Rank (A) + Rank(B) doesn't guarantee Ranks(A + B) will be larger than min( Rank(A) + Rank(B)). Without any further constraints, there is no guarantee L93 is true. We just know the upper bound is higher but to claim it, we need to bound lower-bound, which is not discussed at all. The idea itself is interesting but rationale is wrong. 

3. Experiments are limited, as shown in Question sections.

Limitations:
It's discussed in the paper and it reads well. 

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper focuses on low-rank training techniques and introduce ReLoRA that uses low-rank updates to train a high-rank network (<=350M parameters). The main idea is to employ LoRA during training and ""restart"" it in order to artificially increase the rank, which is a nice idea. The difficulty remains in the optimization process due to the gradient after the ""reset"". ReLoRA performs a partial reset of the optimizer state during merge-and-reinit and sets the learning rate to 0 with a subsequent warmup.

The proposed method is elegant and novel. It is a nice trick to increase artificially the rank of LoRA.

Overall, it is unclear to me what we mean by ""pre-training"". From the text, it seems we are talking about fine-tuning large models with adapters. In the experiment section, the authors talk about training on C4 with a model similar to LLaMA, which indicates pre-training. However, ReLoRA is initialized from a full-rank training at 5k steps. My guess would be that the authors initialize randomly a model (e.g., BERT) and pre-train using an adapter approach.

I am disappointed with the experiment section. While I understand the limitation in resource computation, training models <= 350MB is disappointing when playing in the league of training large neural networks (as suggested by the title) or comparing with LLaMA. Nowadays, 8  GPU days of compute allows to fine-tune a LLaMA or similar model (in 1 day), not to pre-train.
In Table 2, it is missing the training time for each model to understand the benefits of ReLoRA over the control, full training, and LoRA. If the method is 2 times slower than Control, it does not necessarily makes sense to use it since the performance at 350M is less than 1ppl (which I expect to be even smaller with larger models).

Evaluating LMs only on perplexity is not sufficient. Please compare pre-trained models on GLUE/etc (as commonly done for adapters/prompting papers) using fine-tuning and PEFT approaches. This will show that:
- The base model after pre-training with ReLoRA is (maybe?) better than full-rank pre-training or standard pre-training
- Higher performance can be obtained with ReLoRA during fine-tuning.

missing references:
[1] Wang et al. 2023, LEARNING TO GROW PRETRAINED MODELS FOR EFFICIENT TRANSFORMER TRAINING (ICLR)

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- Nice trick to increase artificially the rank of LoRA
- The proposed method is sound

Weaknesses:
- the paper writing and clarity must be improved
- the experiment section is insufficient


Limitations:
Yes

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel approach, ReLoRA, for training large-scale neural networks. Recognizing the limitations of conventional low-rank matrix factorization (LoRA) in training high-performing transformer models, the authors propose ReLoRA that employs a high-rank network training through multiple low-rank updates. This new method uses a full-rank training warm start followed by a merge-and-reinit (restart) strategy, jagged learning rate scheduler, and partial optimizer resets, making it efficient particularly for large networks. The research finds that the efficiency of ReLoRA increases with the network size, positioning it as a potential candidate for efficient training of multi-billion-parameter networks. The paper's results suggest that low-rank training methods can potentially improve the efficiency of training large language models and provide valuable insights for deep learning theories. These insights could further our understanding of neural network trainability and their exceptional generalization capabilities in the overparameterized regime.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Important work of applying LoRA in pre-train
2. Good reproducibility: Code is released for readers; Hyperparameter settings are available. But still some parameters are missing, see below.
3. Sophisticated design of ablation study

Weaknesses:
1. What's the number of trainable parameters in the experiments? Rank r of LoRA is also not reported (or hard to find). I assume 60M, 130M, 250M, and 350M are the total number of parameters.
2. The perplexity reported for Control with 250M parameters appears to be an outlier, greater than that of 130M parameters. Please double-check. If it’s real, then the fluctuation range of perplexity could be too large for us to draw reliable conclusions with the results.
3. One important missing baseline is LoRA with warm-start, which is reported in the ablation study, but not in the main results of Table 2, as well as in Figure 3 and 4. As the gain of the main techs developed in the work for ReLoRA can only be seen when compared to the proper baseline of LoRA, unfair to compare with LoRA with no warm-start.

Limitations:
1. Initial warm start is important/indispensable to reach a performance comparable to the full training. If one start from scratch, with no warm started checkpoint, they may still be limited by the computing resource to warm start. The proposed method may just save some time later after warm start, which is, however, not shown in the results.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes an extension LoRA. The main insight in the paper is  LoRa ca be initialization multiple times during training layers and this in the end will produce a high rank update. The authors show that it is quite challenging to re-intialize the layers mostly due to the internal initialization state of Adam.

The authors then propose a pruning based technique to overcome this limitation when using Adam. The also propose a learning rate schedule which helps to avoid the divergence of the network. The authors show that this leads to better performance than Lora on the C4 datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes an interesting technique which might be useful. 

The experiments performed by the authors are reasonable and even with limited compute resources they clearly show a clear picture. 

The authors have used the C4 dataset. Which is quite reasonable for a start

Weaknesses:
* The methods proposed is more of set of approaches to avoid divergence due to Adam.

* The techniques used a bit of trial and error, is there more principal.

* Would have been better to see what is the downstream performance of ReLoRa transform. 

* Can you provide a better understanding why the second to last row in Table 3 performs very similar to No attemto

Limitations:
I think the paper is well written. I understand the lack of compute to perform large experiments. 

Rating:
5

Confidence:
4

";0
IklhryC2up;"REVIEW 
Summary:
The paper studies the idealized phenomenon of lossless compressibility, whereby an identical
function can be implemented with a smaller network, in the setting of single-hidden-layer hyperbolic tangent networks.  It introduces the notion of rank as the minimal number of hidden units (used to measure the ""size"" of a network) required to implement a given NN function and a constructive algorithm for computing it, as well as the notion of *proximate rank* as the rank of the most compressible parameter vector within a small L_infty norm ball and an algorithm for upper-bounding it. The paper further shows that bounding the proximate rank of a given parameter vector is an NP-complete problem. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
While I'm not an expert on algorithms and cannot comment on correctness or the novelty of the approach, 
I followed the high-level argument and believe the contribution is significant. The paper studies a fundamental aspect of deep learning, i.e., the compressibility and description length of neural networks, which is mostly dominated by empirical research and demands better theoretical understanding. The paper goes beyond the classical setting of lossless compressibility (c.f. Sussmann 1992) by introducing the notion of proximate rank and proving a basic hardness results for it, which will hopefully lay a foundation for future studies of this topic. 

Weaknesses:
Section 6 (Computational complexity of proximate rank) is a bit hard to follow for non-experts. One possibility could be to include a discussion at the beginning of Section 6 that summarizes the proof at a high-level. 

Limitations:
The authors adequately addressed the limitations.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper studies the replication of the same function using a smaller network. It provides a procedure for achieving optimal lossless compression in the context of single-hidden-layer hyperbolic tangent networks.

This paper introduces the idea of 'rank' of a parameter, defined as the least number of hidden units needed to replicate the same function. The paper further defines the 'proximate rank' of a parameter as the rank of the most compressible parameter within a limited L-inf neighborhood.

The paper also demonstrates that estimating the proximate rank is an NP-complete problem through a reduction from Boolean satisfiability using a geometric problem that involves covering points in a plane with small squares.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This is a theoretical treatment of a very important problem and the results are interesting.

2. The paper shows that the problem of bounding the approximate rank below a given value is an NP-complete decision problem.

3. Paper is well presented and clear.

Weaknesses:
1. The results do not seem to be important. Specifically, does anyone care if instead of totally lossless compression, we have an infinitesimal error in representation?

2. The algorithm given in this paper is only applicable to single-hidden layer hyperbolic tangent networks, without obvious ways of extending it to more general cases.

3. The paper aims to provide a theoretical ground for network compression, but did not argue extensively on its connection on the empirical success of existing network compression literature.

Limitations:
1. In practice it is allowed that the compressed networks have slight performance drop compared to the original network, but this paper does not consider this problem.

2. The proposed algorithm only considers the single-hidden layer hyperbolic tangent networks, which is not often employed in practice.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The authors propose two notions for studying neural network complexity with, on single hidden layer neural networks. The first is _rank_, which corresponds to the smallest number of parameters that can produce a network that is functionally equivalent to the original, and the second is _proximate rank_, which is the rank of the network with smallest rank in an $L^\inf$ neighborhood of the original model('s parameters). The authors propose an algorithm for computing rank, and propose a heuristic algorithm for upper bounding proximate rank. They also show that exactly computing proximate rank is NP-complete, so any future study will have to rely on approximations / bounds on this quantity.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The notions the authors introduce are interesting and future work can indeed utilize them in interesting ways. The metrics they propose importantly diverge from metrics such as $\ell_p$-compressibility in that they are defined based on functional equivalence, which might imply them being more useful for future learning theoretic research.
- The paper is very well structured, and the exposition is clear and easy to follow. Connections to previous literature are made clear (but see below).

Weaknesses:
The paper is beset by two important problems:
- As the authors widely acknowledge, their notions are defined on single layer neural networks with tanh activation, with no clear map for generalization to more realistic setups. It would be valuable to see a roadmap for this, as it would help evaluate whether proximate rank is a potentially useful theoretical notion for future research or in the worst case a ""dead end"" in terms of applicability to realistic ML models.
- The authors do not discuss how or why their notions of rank and proximate rank could be useful for future research on generalization in deep learning (or robustness etc.). Given the venue the paper is submitted, I am assuming that the authors believe that these results might have learning theoretical implications down the line, but they are mostly silent on this issue. They correctly observe that approximate compressibility research has been utilized for such aims, but do not opine on how their alternative notions should be superior or even have qualitatively different contributions to such research. While this is not a problem per se in general, it is more so for a machine learning conference.

Limitations:
The authors are transparent about the limitations of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper deep-dives into the concept of lossless network compressibility. It presents an algorithm for optimal lossless compression in single-hidden-layer hyperbolic tangent networks, which can in part generalize to other more relevant feedforward architectures. The authors introduce the novel concept of ""proximate rank"", which measures network complexity, and demonstrate that bounding the proximate rank is an NP-complete problem, thereby suggesting that the problem of finding highly compressible networks is very hard.         

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- originality: The paper introduces novel approaches to obtain lossless compression and introduces the novel concept of proximate rank as a measure of neural network complexity. Furthermore the paper also introduces novel techniques to prove that bounding the proximate rank measure is an NP-complete problem. In summary, many novel contributions which, as the authors argue, lay the foundations for future work identifying losslessly compressible parameters in deep learning structures.   

- quality: The quality of the paper seems sound. The topic is a bit away from my area of expertise and I did not check the math in detail though.

- clarity: the paper is very well written. 

- significance: Lossless compressibility is a fundamental topic in deep learning, particularly since most recent state-of-the-art research involve very large foundation models. Having a theoretical framework and some robust approaches to measure network complexity and compressibility are key to advance the field of deep learning and the applicability of modern architectures.   

Weaknesses:
Single-hidden-layer hyperbolic tangent network can be a good subject to start developing a theoretical framework and a set of tools to measure and optimize network complexity and compressibility, but as the authors point out, this architecture is of little relevance otherwise (for current research). This issue, which is raised by the authors, is a minor weakness but it does limit the potential impact of the present work.

Limitations:
Authors have addressed all the limitations clearly. 

Rating:
6

Confidence:
1

";0
IQ6GI7fM2z;"REVIEW 
Summary:
This paper investigates the issue of noisy gradient-based explanations caused by high-frequency content. The authors empirically identify that several down-sampling operations, such as MaxPooling or stride convolution, could be the primary source of these high frequencies. To address this, they present FORGrad, which removes high-frequency content in gradients by leveraging low-pass filter on the Fourier spectrum. The results on three faithfulness metrics further show the effectiveness of FORGrad in improving existing gradient-based explanation methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written, and the motivation is sound. 
2. The analysis regarding the high-frequency noises in gradients is interesting and could benefit the further designs of explanation methods.
3. The proposed FORGrad achieves promising results on three faithfulness metrics, potentially improving existing gradient-based methods.


Weaknesses:
1. Throughout the paper, the term “High frequencies” is extensively employed to describe the phenomenon in gradients. However, how to clearly define “high” in actual deployment is not well explained. This could become especially problematic considering the significant variations in model architectures and datasets, thus degrading the significance of this work.
2. The argument, “High frequencies are just noise in the gradient,” is too strong. The analysis in Section 3.2 can only show that the high-frequency information has less effect on model decisions. This does not necessarily mean that such high-frequency information is always noise. Moreover, the perturbation-based analysis also has many limitations, e.g., the OOD problem [1]. It’d be better if the authors could provide another set of experiments to validate their findings.
3. It is nice to see that FORGrad could co-work with many gradient-based methods. However, in many cases, the improvement introduced by FORGrad is minor/marginal, e.g., Int.Grad* on ConvNeXT. It thus remains unclear whether FORGrad is genuinely effective or merely a result of randomness.
4. The source of high-frequency gradients is not systematically investigated. The analysis in Section 3.3 only covers two mechanisms, i.e., downsampling and training. This narrow focus fails to provide substantial evidence for the claim that these mechanisms are the main reasons, as other relevant factors are not adequately discussed.

[1] Peter Hase, Harry Xie, Mohit Bansal. The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations. NeurIPS 2021.


Limitations:
NA

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper analyzes the two types of attribution methods: gradient-based/ white-box methods and prediction-based/ black-box methods. The authors observe that the faithfulness of black-box methods surpasses the white-box methods despite white-box methods accessing the internals of the classifier being explained. The authors suspect the presence of noise in the gradients to be a contributing factor to its limited faithfulness. They further perform Fourier spectral analysis to examine the distribution of attribution signal frequencies. This analysis sheds light on the existence of high-frequency noise signals in the gradients, a by-product of pooling and stride operations, which are propagated as attribution signals. The obtained Fourier spectrum is filtered using a low-pass filter with an optimal cut-off threshold learned based on optimizing the Insertion and Deletion metrics, which are proxies for the faithfulness of the explanations. This yields a faithful explanation from the white-box methods surpassing the faithfulness that black-box methods achieved earlier.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The methodology is effectively presented, incorporating suitable figures and accompanying text that elucidate the significance of these visuals. The paper follows a systematic approach in introducing concepts, starting with the motivation, followed by the hypothesis, supporting evidence, and ultimately presenting a solution to the problem. This coherent structure enhances the elegance of the methodical exposition.

* The analysis of the gradient signals is sound, and the link between noise in the gradient signals and explainability is adequately established. 

* The solution’s strengths are its simplicity - performing a FFT and identifying a suitable frequency threshold; and modularity - can be integrated with many whitebox explainability approaches. It is a simple post-processing strategy to extract high fidelity explanations.

* The experiments are somewhat comprehensive - with comparison against some of the prevalent explainability approaches.


Weaknesses:
- *Possible misrepresentation of the related literature:* Reference [3] is described as both a black-box method and a white-box method in different sections. This inconsistency is also reiterated in Line 76. Similarly, in Table 1, Grad CAM and Grad CAM++ are categorized as prediction-based/black-box methods despite their reliance on gradients. These methods employ gradient backpropagation (typically) up to the last convolution layer to generate the CAM, which contradicts the authors’ characterization as purely black-box approaches.

- *Missing related literature and experiments:* The analysis of attribution-based explainability approaches overlooks significant contributions that introduce litmus tests for these methods, such as the work by Adebayo et al. NeurIPS 2018 and Sixt et al. ICML 2020. Sixt et al. explores the convergence of activations in saliency maps of gradient-based techniques when subjected to litmus tests proposed by Adebayo et al. It would be intriguing to investigate potential parallels between the convergence analysis presented by Sixt et al. and the spectral analysis conducted in the current paper. The application of Adebayo et al.'s litmus tests are necessary, and a comparative evaluation of the results before and after applying the proposed frequency cut-off indicating an increase in the faithfulness of the explanations could further strengthen the contribution. This is important considering the sweeping claim of the paper that gradient based attribution is reliable.

- *Missing experiments:* While the authors’ have conducted quantitative evaluation to highlight the effectiveness of the solution, the absence of qualitative evaluation is stark. It is evident from the Figures that the attribution map generated looks ‘better’ after the frequency cutoff. However, whether this attribution map measures up against other approaches (both white box and black box approaches) qualitatively needs to be checked. Human subject experiments to this effect are necessary.
 
- *Disconnected theoretical analysis:* The theoretical analysis seems disconnected from the empirical decisions in the current draft. The need for the theoretical analysis has to be well-motivated through a thorough rewrite. 

**Minor suggestions**
- References [1] and [2] are just the same, with just a difference in the year. The author may need to check their bib source to remove the irrelevant bib entry and modify the paper just to retain the relevant entry.
- Line 84 activations have been mistyped as activities. 
- Line 96 appears like a colloquial lingo. Please rewrite to make the statement formal to the research community. 
- Please check for possible typos in Line 244. 


Limitations:
An analysis of the limitations has been honestly presented

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper explores the differences between prediction-based and gradient-based attribution methods for explaining the decisions of deep neural networks. The authors observe that these two approaches produce attribution maps with distinct power spectra, with gradient-based methods exhibiting more high-frequency content. This discrepancy raises questions about the origin and relevance of high-frequency information and why its absence in prediction-based methods leads to better explainability scores. By analyzing the gradient of visual classification models, the authors identify downsampling operations in Convolutional Neural Networks (CNNs) as a significant source of high-frequency content, potentially due to aliasing. To address this issue, they propose applying an optimal low-pass filter to improve gradient-based attribution methods, leading to enhanced explainability scores.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. `Simple yet Effective Solution`: One of the key strengths of the paper is its proposal of a straightforward yet highly effective solution. By applying a frequency cut low-pass filter to the attribution maps, the authors successfully remove the high-frequency noise and improve the interpretability of the results. This simple intervention offers a practical ""plug-and-play"" approach that can provide meaningful and understandable attributions for human interpretation. The demonstrated enhancements in explainability scores serve as compelling evidence of the efficacy of this approach, showcasing its practical value in real-world applications.

Weaknesses:
1. `Flaw in Observation`: The paper's strong observation regarding the higher frequency content in gradient-based methods compared to prediction-based methods may be misleading. This difference is primarily attributed to the **distinct instances that each method focuses on, rather than being inherent properties of the attribution categories themselves**. Prediction-based methods typically concentrate on `image patches, super-pixels, or high-level features of specific receptive fields`, whereas gradient-based methods calculate derivatives with respect to each pixel individually. If gradients were calculated with respect to patches, super-pixels, or features, the resulting attributions would also exhibit dominance in low-frequency components.

2. `Naiveness in Theoretical Justification`: The paper's theoretical justification appears simplistic and lacks depth. It adopts a traditional signal processing approach, assuming the presence of noise with a specific form (such as uniform or Gaussian) and demonstrates the efficacy of a linear low-pass band cut filter. However, several critical questions remain unanswered. Firstly, the paper fails to address **why gradients with respect to the input are inherently noisy and exhibit high variance**. This noise can be attributed to the non-linearity and no-smoothness[A] of the network itself, and the downsampling operation might be just one contributing factor among others. Secondly, the derivation of the proposed solution seems to be a mere reproduction of standard textbook content on signal denoising, without providing any specific insights or considerations tailored to the problem at hand.

3. `Methodological Simplicity`: The application of a low-pass filter to gradients may be considered too simplistic for publication in a prestigious conference like NeurIPS. Similar techniques, such as those presented in reference [5] and other variance reduction methods, have already been established in the field. The paper does not introduce any novel or sophisticated methodologies, which might limit its overall contribution and impact.

[A] Wang Z, Wang H, Ramkumar S, et al. Smoothed geometry for robust attribution[J]. Advances in neural information processing systems, 2020, 33: 13623-13634.


Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper investigates why gradient-based attributions contain more high-frequencies compared to prediction-based
attribution. The authors argue that these high-frequency components are not relevant for the model and can
therefore seen as noise. Different saliency methods are analyzed on three different image models (ConvNeXT, ResNET, ViT). They further investigate the source of the noise and find that the downsampling-block is the the main cause for this.  Finally, the authors find that removing the noise from saliency maps yields an improved performance on
various explanation metrics.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- In general, I think the paper is novel. The noise in attribution maps and its effect on interpretability metrics
are not completly understood yet.
- The paper analyzes the causes of the noise in detail
- FORGrad makes it easy to interpolate between detailed and smooth attribution maps.
- The focus of the paper is clear and generally well executed.

Weaknesses:
- **Missing relevant work**: The work does not discuss the findings of [(Balduzzi et al. 2017)](http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf). This work investigates the effect of Residual connections on the noise of neural networks. Although this work has a focus more on saliency maps, the findings
of (Balduzzi et al. 2017) should be considered in the discussion.


- **How relevant are high-frequencies?** : ""As anticipated, our observations reveal that the curves exhibiting reduced high-frequency content (from σ < 224 to σ = 10) closely align with the one of the non-filtered gradient (σ = 224)."" (line 148). However, the distance between the curves σ < 224 and σ = 10 are quite substantial in Figure 4.
For ConvNeXT, the distance is almost one magnitude (the log-scale makes it look closer together).
Could you please report the maximum error between the curves in the supplementary material?

- **The conclusion is a bit off**:

  > (333) Overall, our work leads to a surprising result – that the almost forgotten gradient-based methods turn out to contain all the information needed to provide a faithful explanation of a model’s decision and that they can be as interpretable as the newest methods.

  I disagree with multiple points in the conclusion:
  - Gradient-based attribution almost forgotten? The integrated gradients paper received over 1000 citations last year (500 citations this year already). (source: semantic scholar)
  - I would be cautious about the faithfulness claim. It could also be that the metrics are biased and prefer a smoother attribution map.

- **Why not change the model?**: I would consider the raw gradient to be just the most faithful local representation of the model. If you prefer a smoother version, would it not make more sense to make changes to the underlying network architecture? This proposed methods feels like treating the symptoms instead of resolving the core issues.


To summarize, I am cautious about the main-take away that apply smoothing to gradient-based attribution methods makes them faithful. It would be great to also analyze if the faithfulness metrics are biased toward smooth attribuiton maps. Furthermore, I want to see the maximum error is between the different sigma-curves. It might be more significant
than suggested by the authors. A large error would questions the assumption that a smoothed gradient is more faithful.


Limitations:

The limitation section reads very technical and misses a very important point: the Deletion, Insertion, and Fidelity metrics are only proxy metrics for interpretability. Performing well on them does not mean that the attribution map is actually interpretable. Only a user study could answer this question.

Rating:
4

Confidence:
3

";0
9Rhopbm4qu;"REVIEW 
Summary:
The paper addresses the overlap violation problem in observational datasets for causal inference by presenting an interpretable balancing method for overlap violation identification and causal effect estimation for binary treatments. The method BICauseTree adapts decision tree classifiers to the stated problem by recursively splitting the data population into non-overlap-violating subgroups based on covariate dissimilarity and treatment heterogeneity. The major advantage of the presented method in comparison to existing balancing methods is the interpretability of the prediction process.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
• The authors evaluate their method on both synthetic and real-world benchmarking datasets.

Weaknesses:
- The proposed method is highly similar to the work in reference 12. Furthermore, related work is not discussed appropriately (section 2). It is thus unclear how this work significantly differs from previous contributions in the literature. The originality of the submission has to be considered very limited.
- The manuscript presents a complete piece of work. Claims about the performance of the proposed method are supported by experimental results.  Nevertheless, the an experimental study with sophisticated baseline methods is missing. A performance comparison with a Causal Forest model would be desirable.
- Claims aiming at motivating the method are neither supported quantitatively nor experimentally (e.g., an analysis with different levels of overlap violation; unbiased estimation). 
- The submission lacks clarity due to multiple grammar errors and many nested arguments. The mathematical notation (section 3.1) lacks formal correctness.
- The citation style does not agree with the required format for NeurIPS  submissions.
- The statement of a high ASMD indicating a confounder (line 164) needs to be justified.
- The paper would profit from a revision of the language and the consistency of the presented arguments (e.g., lines 84, 181/182). Furthermore, the figures and sections need to be referenced correctly, as also recognized by the authors in the appendix.
- The authors claim interpretability of the method but do not provide evidence for the statement. Here, a user study would be desirable etc. 
- The method is limited to ATE. 


Limitations:
- The authors have stated the limitations of their work. The section could be improved by highlighting the general weaknesses of single trees in prediction settings which naturally devolve to the proposed method.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method called BICauseTree for interpretable causal effect estimation. BICauseTree is a hierarchical bias-driven stratification method that identifies clusters where natural experiments occur locally. The method is designed to reduce treatment allocation bias and improve interpretability. The authors evaluate the performance of BICauseTree on several datasets and compare it to existing approaches. They find that BICauseTree performs well in terms of bias-interpretability tradeoff and outperforms existing methods in some cases. Overall, the paper presents a novel and promising approach to causal effect estimation that could have important applications in various fields.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novelty: The paper proposes a novel method called BICauseTree for estimating causal effects from observational data. The method is based on a hierarchical bias-driven stratification approach that identifies clusters where natural experiments occur locally. The method builds on decision trees to reduce treatment allocation bias and provides a covariate-based definition of the target population. The method is interpretable and outperforms other state-of-the-art methods in reducing treatment allocation bias while maintaining interpretability.

2. Significance: Causal effect estimation from observational data is an important analytical approach for data-driven policy-making. However, due to the inherent lack of ground truth in causal inference, accepting such recommendations requires transparency and explainability. The proposed method addresses this issue by providing an interpretable and unbiased method for causal effect estimation. The method has the potential to be applied in various domains, including healthcare, social sciences, and economics.

3. Experimental Evaluation: The paper provides a thorough experimental evaluation of the proposed method using synthetic and realistic datasets. The authors compare the performance of their method with other state-of-the-art methods and show that their method has lower bias and comparable variance. They also conduct sensitivity analyses to evaluate the robustness of their method to violations of the assumptions. The experimental evaluation provides strong evidence to support the claims made in the paper.

Weaknesses:
1. Limited Scope: The paper focuses on a specific method for causal effect estimation from observational data, and the scope of the paper is relatively narrow (especially related to the tree-based models). While the proposed method is novel and has some advantages over other methods, it may not be of interest to a broad audience: (1) The method relies on the quality of the data and the assumptions made in the model. If the data is noisy or contains missing values, the method may produce biased estimates.; (2)The method may not be suitable for high-dimensional data, as the number of covariates may increase the complexity of the decision tree and lead to overfitting; (3) The method may not be suitable for datasets with small sample sizes, as the stratification may lead to small sample sizes in some subgroups, which may affect the accuracy of the estimates;(4)The method may not be suitable for datasets with complex interactions between the covariates, as the decision tree may not capture these interactions effectively.

2. Experimental Evaluation: While the paper provides an experimental evaluation of the proposed method, the evaluation is limited in scope and does not provide a comprehensive comparison with other state-of-the-art methods. The experimental evaluation would benefit from a more comprehensive comparison with other methods (such as TARNet from the machine learning domain) and a more detailed analysis of the results.


Limitations:
See weakness.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper focuses on achieving interpretable causal effect estimation, where the goal is to ensure that each decision within the algorithm is explicit and traceable. The authors propose a decision tree-based balancing method to address this problem, which identifies clusters where local natural experiments occur. The effectiveness of the proposed algorithm is empirically evaluated using synthetic and semi-synthetic data. The paper also did several ablation studies on the trade-off between interpretability and bias and the consistency of the decision tree.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall, this paper is well-executed and demonstrates several notable strengths.
- This paper is very clear. It effectively presents complex ideas in an easily understandable manner.
- The problem addressed in the paper is well-motivated and interesting.
- The authors display a strong grasp of the related work in the field, effectively positioning their contribution within the existing literature.
- The empirical analysis conducted in the paper is thorough and yields valuable insights.
- The method is intuitive and well explained.

Weaknesses:
- Style file: One issue with this paper is that it doesn't follow the NeurIPS style guidelines, specifically regarding paragraph spacing. The paragraphs are not well-separated, which makes it harder to read and understand the content. This affects the overall flow and coherence of the paper. Additionally, the excessive content allowed due to the spacing issue may be seen as unfair to authors who followed the style guide correctly. This may be a potential ground for rejection.


- Method: The rationale behind considering features with the highest ASMD as potential confounders is not well-explained. This is an important assumption in the paper, but it lacks a clear justification or empirical investigation. Providing additional explanations or conducting empirical studies would strengthen this aspect of the paper.

- The experiment section of the paper is not self-contained. Although the motivating problem revolves around identifying subpopulations with natural experiments, this aspect is not adequately illustrated in the experiment section. Instead, the focus is primarily on bias analysis, with the analysis related to interpreting the causal effect estimation process deferred to the appendix. This undermines the fulfillment of the paper's fundamental promise to the readers. To address this issue, it is highly recommended that the authors integrate the analysis into the main paper, ensuring that the key components align with the paper's core premise.


Limitations:
n/a

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces a decision tree methodology to identify regions where selection bias no longer ensures covariate balance. These regions, which have some level of interpretability, can then be removed in subsequent analysis.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The paper presents an interesting decision tree methodology.

The paper contains a significant amount of simulation experiments to validate the procedure.

Weaknesses:
Despite a focus on covariate balance, there is no guarantee of balance unlike competing methods (rerandomization, matching, etc). Furthermore, there is limited analysis to show the claims of balance are fulfilled, especially for high dimensions.

The paper explores a bias-interpretability tradeoff, but provides no rigorous definition. The proposed model often is more biased than alternative models, most notably IPW, and it's not clear that the resulting decision trees, or their interpretations, are actually sensible. No discussion of estimator variance is given or how that might factor into a tradeoff, despite the high variance generally expected from decision tree estimators.


Limitations:
The paper touches on some limitations, most notably the increases bias that is expected. The paper does not discuss the variance of the estimator in detail relative to other methods, which is another potential limitation.

The method advocates for trimming nodes that violate positivity. These nodes could contain sensitive subpopulations and could lead to fairness concerns.

Rating:
3

Confidence:
3

";0
4mwORQjAim;"REVIEW 
Summary:
In this paper, the authors are addressing a very critical need in topological data analysis (TDA), vectorization of multiparameter persistence (MPH). Persistent homology (PH) is the key method in TDA, but in its current form, it allows only a single function to use in its key process, filtration. By enabling multiple functions, MPH is a natural generalization of (PH) with much finer information, however, there are several mathematical obstructions to use them effectively in applications. 

In this paper, the authors propose an effective way to vectorize the barcode information in the MP module by sacrificing some to keep the computation feasible and process practical. They offer two versions of MP vectorizations where one being kernel and the other being direct vectorization, both can be effective in different settings. The authors made extensive experiments in several settings, including point cloud, graph classification, and virtual screening to compare their model with other MP vectorizations and SOTA models. Their model consistently outperforms existing MP vectorizations.



Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The authors use a recent idea of signed barcodes cleverly to obtain practical vectorizations. There is a significant need to employ MPH idea with good and feasible vectorizations, and this paper makes a valuable contribution in this direction.

They propose two versions of their vectorizations, a kernel (MP-SW) and a direct vectorization (MP-C). In the past years, depending on the domain, we see that one can perform better than the other from our experience with the applications of single persistence. So, this versatility of outputs is very valuable for ML applications.

Computational time is significantly better than existing MP vectorizations. Combining with its good performance, this might be the most important contribution of the paper from ML perspective.

Their extensive experiments in various settings show that their model consistently outperforms the existing MPH vectorizations.

Weaknesses:
The main weakness is that the paper might be too technical for non-experts and ML audience. However, considering the depth of the problem, I can see that the authors did an enormous effort to make this important subject accessible to a wider audience.

While new vectorizations consistently outperform the existing MP models in point cloud settings, the performance is not as strong in graph classification and specifically, virtual screening cases (TODD also uses a version of MPH).  

As MPH is an involved process, hyperparameter tuning might need serious expertise in TDA in real life applications.



Limitations:
As mentioned in weaknesses, the main limitations are scalability and hyperparameter tuning which could be very tricky, especially choosing the right grid size. Hence, performance vs. computational feasibility can be a problem in large datasets.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper vectorizes data descriptors coming from multiparameter persistent homology for classifying point clouds and measuring similarity between graphs extracted from databases of times series and molecules. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper includes rigorous definitions and proves (in the appendices) three theorems from section 4.

Weaknesses:
The paper seems to over-advertise the strengths of persistence.

Lines 1-2 claim that ""Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs"". If the authors really meant a topological classification of graphs, this classification has been known to Euler in the 18th century, so there is no need to re-invent topological invariants of graphs by using persistence.

Lines 22-23 claim that ""TDA methods usually require the sole knowledge of a metric or dissimilarity measure on the data"". In fact, the definition already of persistence requires a choice of filtration of simplicial complexes on given data points, which seriously affects the resulting persistence whose further analysis requires many more extra parameters, see lines 351-352: "" Our pipelines, including the choice of hyperparameters for our vectorizations, rely on the cross validation of several parameters, which limits the number of possible choices""

Examples 1-3 are definitions or comments, not illustrative examples that could help the readability. 

The questions and limitations below include further concerns about experiments and comparisons with past work. 

Limitations:
The main limitation of the 1-parameter persistence is its weakness as an isometry invariant, which should have been clear to all experts in computational geometry many years ago but was demonstrated only recently. The paper by Smith et al (arxiv:2202.00577) explicitly constructs generic families of point clouds in Euclidean and metric spaces that are indistinguishable by persistence and even have empty persistence in dimension 1. These examples can be easily extended to more than one parameter at least for some filtrations.

Though Topological Data Analysis was largely developed by mathematicians, the huge effort over many years was invested into speeding up computations, rather surprisingly, instead of trying to understand the strengths and weaknesses of persistent homology, especially in comparison with the much simpler, faster, and stronger invariants of clouds under isometry. 

Persistence in dimension 0 was extended to a strictly stronger invariant mergegram by Elkin et al in MFCS 2020 and Mathematics 2021, which has the same asymptotic time as the classical 0D persistence and is also stable under perturbations of points.

A SoCG 2022 workshop included a frank discussion concluding that there was no high-level problem that persistent homology solves. Is there such an ultimate problem for multi-parameter persistence? In fact, persistence as an isometry invariant essentially tries to distinguish clouds of unlabeled points up to isometry, not up to continuous deformations since even non-uniform scaling changes persistence in a non-controllable way. 
  
On the other hand, the isometry classification problem for point clouds was nearly solved by Boutin and Kemper (2004), who proved that the total distribution of pairwise distances is a generically complete invariant of point clouds in any Euclidean space. The remaining singular cases were recently covered by Widdowson et al in NeurIPS 2022 and CVPR 2023. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors introduce first vectorizations of multiparameter persistent homology (MPH) via signed barcodes, that are easy to compute and shown to be stable. The two proposed vectorizations often outperform the state of the art MPH methods on a variety of data sets.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
(S1) The paper is clearly organized and written, it reads really well.

(S2) The experiments are rather extensive and the results seem convincing.

Weaknesses:
I did not identify major weaknesses in this work, besides some confusion about the experimental settings addressed in question (Q2) below. I am open to raising my score if this issue is properly addressed.

Limitations:
The limitations and future work are discussed in detail.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work promote the use of signed barcodes for feature generation, and proposed the feature generation pipeline based on the signed bar codes, 
a. the work introduces two general vectorization techniques for signed barcodes;
b. the authors prove Lipschitz-continuity results that ensure the robustness of the proposed entire feature generation pipeline
c. the practical performance of the proposed pipeline is compared to other baselines in various supervised and unsupervised learning tasks.

All of my questions are fully addressed by the rebuttal. I am satisfied with the explanations. I think the work is solid and promising.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This work generalizes the single parameter bar codes to multi-parameter bar codes, introduces general vectorization techniques for signed barcodes, and prove Lipschitz-continuity results to ensure the robustness of the proposed pipeline. The theoretic results are important and convincing. 
The paper is well written, the key concepts are explained thoroughly, the proofs of the main theorems are clear and in details. The experimental results are analyzed in details.

Weaknesses:
The motivation for multi-parameter persistent homology could be further emphasized. From theoretical point of view, it is unclear what extra-information can multi-parameter PH bring compared to single-parameter PH. 

It will be helpful to add some experiments to validate the stability theorems, and estimate the Lipschitz-constant.

The definition of the Kantorovich-Rubinstein norm for point measures in proposition 1 is inconsistent with that in definition 3. The definition 3 allows point mass splitting, namely one source point can be mapped into multiple target points, hence \psi is a transportation scheme; but in proposition 1, each source point is mapped to a single target point, \gamma is a transportation map. This needs to be clarified.

Limitations:
The authors have adequately addressed the limitations, which are helpful for practical applications.

Rating:
6

Confidence:
3

";1
zGdH4tKtOW;"REVIEW 
Summary:
The authors present a new optimal individual treatment regime (ITR) within the proximal causal inference framework, which avoids the strong assumption of no unmeasured confounding. Instead, one assumes the effect of the unmeasured confounders flows exclusively through proxy variables, as defined through outcome-inducing and treatment-inducing confounding bridges. Compared to prior work, this optimal ITR that is defined with respect to a more flexible function class that depends on known confounders X, treatment-inducing confounding proxies Z, and outcome-inducing confounding proxies W.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The proposed ITR is a natural extension of existing ITRs by using a function \pi(x) that selectively chooses between two existing ITRs based on known confounders x. Under the proximal causal inference frameowrk, the proposed ITR is proven to be superior to existing ITRs in the literature. (Existing ITRs from Qi 2023 can be viewed as special cases of the proposed ITR.) The authors introduce a simple plugin estimator for the proposed ITR and show that the value of the resulting estimator is determined by approximation error of \pi and the gain from using \pi. Simulation studies show that the proposed ITR is either superior or comparable to existing ITRs. The manuscript is clearly written. The authors provide a nice review of prior work in this area and clearly describe how their work builds on existing work.

Weaknesses:
1. The proposed extension of the ITR function class appears quite incremental. The value of the proposed ITR follows directly from application of the tower rule. The paper would be greatly strengthened if the authors can show that this is the best one can do, e.g. showing that the value of a more complex ITR function class would be unidentifiable without much stronger assumptions.

2. In the simulation studies, the improvement in mean value when using the proposed optimal ITR over existing ITRs is large only in scenario 2. In all other scenarios, the improvement is small. Can the authors explain the behavior in this simulation study? Also, can the authors explain settings in which the proposed ITR is expected to substantially improve over existing ITRs? My guess is that the gain is biggest when (i) there are large differences between expected value at each X for the ITR with domain (X,W) and the ITR with domain (X,Z) and (ii) the optimal pi function has high variance (e.g. pi(X) is equal to 1 half of the time). Does this correspond to scenario 2?

3. The authors perform a real-data analysis in Section 5, which illustrates how the proposed ITR is different from existing ITRs. However, the authors do not calculate the values of the estimated ITR, so readers cannot compare the performance of the proposed ITR against existing ITRs. Do the authors have estimates of the values of the estimated ITRs?

4. The number of treatment-inducing confounding proxies and outcome-inducing confounding proxies were small in both the simulation studies and real-world data analysis. However, the practical appeal of the proximal causal inference framework is its use of proxies for unmeasured confounders, which would suggest the use of many variables as potential proxies. Can the authors include simulations that reflect more realistic settings where more proxy variables are used? How does the proposed method perform as the number of these proxies increases?

Limitations:
The authors have not discussed limitations of the work.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The goal is to learn an optimal individual treatment rule (ITR) where the data suffer from unobserved confounding but where the researcher has a treatment proxy and an outcome proxy. 

While the general problem has been studied before by Qi et al (JASA 2023), this paper’s contribution is to broaden the class of ITRs. For a broader class of ITRs, the authors identify the value function and show that it exceeds the value function of the narrower class.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
My comments are brief because this is a strong paper.

Originality: The essence of the improvement is that, for different covariate values x, one may either use the “outcome” ITR or the “treatment” ITR. This departs from previous work, where either the “outcome” ITR or the “treatment” ITR is used across covariate values.

Quality: The proofs look correct, and the results are easy to interpret. Rates for the objects in Proposition 1 would be an improvement; see the question below.

Clarity: The paper is well written and well referenced.

Significance: The paper contributes to two popular literatures: proxies and ITRs. While its theoretical contribution is modest, it does appear to have practical relevance.

Weaknesses:
The theoretical contribution is somewhat incremental.

Take a pass to fix typos, e.g. “netwrok” on line 73.

An extra sentence in Remark 1 would be welcome, that explains the point summarized as “originality” above.


Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper discusses the optimization of treatment rules in the context of observational data and under assumptions of proximal inference. Various theorems are introduced, and a real data analysis performed using a healthcare example.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Below is a list of perceived weaknesses. 

The paper is overall sound and the topic of importance. I appreciate the presence of the real data application. Assumptions and results clearly stated.

Weaknesses:
Below is a list of perceived weaknesses. 

It was not clear to me how the empirical results compare to competing methodological baselines from other approaches (I don't believe the different values presented in the figure represent different algorithmic approaches). 

The paper is quite heavy on notation and, at least to me, light on intuitive explanation for findings as they are discussed, limiting insight into the inner workings of why the method works.

I don't know the proximal causal inference literature well so am not well-positioned to discuss the contribution in that subfield of causal inference. 

I don't see a discussion of uncertainty estimation in the theoretical or empirical results. Uncertainty estimation in optimized treatment effect regimes can be difficult (e.g., the bootstrap may not be appropriate or may have poor coverage) but may be important to usefulness in practice.

Limitations:
I see no ethnical limitations here.

Rating:
6

Confidence:
2

REVIEW 
Summary:
Most estimation methods for individualized treatment rules (ITRs) assume no unmeasured confounders for valid causal inference. However, such an assumption can be unreasonable, such as when estimating ITRs from observational data. Previous work has applied proximal causal inference to estimate ITRs when this assumption is violated, but is restricted to policy classes that either exclude treatment-inducing confounding proxies or exclude outcome-inducing confounding proxies [1]. To this end, the authors propose estimating a stochastic mixture of both policy classes from [1] to yield a more flexible ITR. Theoretical and simulation results demonstrate the superiority of the proposed method. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The assumption of no unmeasured confounders is nearly ubiquitous across ITR estimation methods, despite frequent violations when dealing with observational data. This makes the problem the authors are trying to solve - estimating ITRs when this assumption is violated - very significant. Moreover, the theoretical and simulation results are of sufficient quality to convince me that the method outperforms [1], the existing state-of-the-art in proximal learning.


Weaknesses:
While I believe the merits and potential contribution of the paper outweigh its limitations, the theoretical and empirical results of the paper are weaker than that of previous work, and the clarity of the paper can be improved. I go into more detail below:

1. **Theoretical guarantees are much weaker than those of previous methods.** Convergence rates, finite-sample error bounds or asymptotic normality is often derived for ITR methods [2-5], including the method this work seeks to improve over [1]. However, while the authors prove that the proposed method will converge to a policy with better value than that of [1] asymptotically, they do not derive a rate of convergence or establish any finite-sample error bounds. Moreover, the asymptotic analysis from Appendix G assumes convergence of several estimators in $L\_\infty$ space. This is a much stronger assumption than the assumptions made in previous work, which only assumes convergence of estimators in $L\_2$ space (e.g. see Assumption 12 from [1] or Condition B5 from [3]).

2. **The real data analysis does not strengthen the validity of the proposed method.** When applying ITR estimators to real datasets, it is common practice to assess performance by using either (1) an estimator of the value [2-6] or (2) arguments based on domain knowledge that support the validity of the proposed method [1,6]. For example, in [1], the authors argue that the estimated policy is accurate by demonstrating that the estimated coefficients and interpretation of the policy is consistent with findings from previous literature. In contrast, the only conclusions that these authors draw from their real data analysis is that the proposed estimator gives different results than previously proposed methods. Such a conclusion says little about the validity or superiority of the proposed method. One way this analysis could be greatly improved is to look at the patients from Figure 3 where the recommended treatment differs between the proposed method and that of [1], and use domain knowledge or previous literature to argue that the recommendations given by the proposed method is more sound. Alternatively, the authors could make this conclusion by comparing the coefficients of the proposed policy with that of [1]. 

3. **Empirical comparisons were relatively limited.** The authors only benchmark the proposed method against those of a single previous work, [1]. To conclude that the proposed method achieves state-of-the-art performance, the authors should benchmark the proposed method against additional baselines as well. For example, there are many methods that assume no unmeasured confounders which the authors could evaluate to demonstrate the utility of using a proximal causal inference framework (e.g. [7,8]). There are also other methods that relax the no unmeasured confounders assumption or have shown robustness when such assumptions are violated, such as instrument variable (IV) methods [3,9] and M-learning [4]. How are the assumptions made by proximal learning less restrictive than those made by IV approaches, and can such methods be applied to the simulated datasets? If so, the authors should benchmark the proposed method against these method. And if these methods are not applicable, the authors should explain why in the paper. In addition to the number of competing baselines being limited, the simulated datasets from this work all have the same sample size and behavior policy. When deriving new ITR methods, it is common practice to evaluate the method on datasets of different sample sizes (and if observational data is of interest, varying behavior policies) so as to demonstrate robust performance [2-5,7-11]. 

4. **The implementation uses very simple estimators.** In theory, $d_z,d_w$ and $\delta$ could be estimated by any weighted classification and regression algorithm. However, in their empirical experiments, the authors only explore estimating $d_z$ and $d_w$ with linear policies and estimating $\delta$ with a Nadaraya-Watson estimator where the bandwidth is chosen based on a heuristic. Moreover, while $h$ and $g$ were estimated using neural networks, the architecture and number of training iterations was fixed a priori. This contrasts to previous works which use more cutting-edge machine learning approaches to estimate the ITR, such as kernel machines, random forests or neural networks, and adopts hyperparameters to the data at-hand using cross-validation [8]. Such works are especially prevalent in top-tier ML conferences [10,11], and better demonstrate broad applicability and flexibility of the proposed method. 

5. **Many parts of the paper need to be better written to avoid confusion and address some open questions.** For example, for the real data analysis, it is not clear what assumptions proximal learning is making and how it is useful for the analyzed dataset. It is mentioned that patients were arranged in a ""control group"" on line 289. Were patients randomized to receive a treatment? If so, wouldn't the no unmeasured confounders assumption hold, as treatment assignment is not being affected by any unmeasured covariates? Also, what is the logic behind the choice of $Z$ and $W$ on line 296 (e.g. why are the variables in $Z$ expected to affect treatment but not outcome), and what specifically are the unmeasured confounders $U$ that we are trying to adjust for? Finally, it is stated that the outcome is censored. Does this mean that 30-day survival rate is censored for some of the patients? It is well-known that optimizing censored outcomes without adjusting for the censoring mechanism can yield bias [5]. 

>> Here are some other suggestions to reduce points of confusion and improve readability:  
a. The explanation of how the proposed method improves upon [1] in the Introduction section (lines 56-65) is confusing. For example, it is stated on line 69-61 that [7] maps an ITR with domain $\mathcal X\times\mathcal W\times\mathcal Z$ with the domain being restricted to $\mathcal X\times\mathcal W$ to $\mathcal X\times\mathcal Z$. While this makes more sense after reading section 2.2, these statements initially appear contradictory. Also on lines 59 and 63 ""two optimal in-class ITRs"" should be changed to ""these two optimal in-class ITRs"" to clarify that the authors are referring to the classes mentioned on line 57.  
b. The paper has many typos. For example, ""Tchetgen Tchetgen et al"" in line 34 should include the year and a link to the reference, ""netwrok"" on line 73 should read ""network"", and on line 169 the authors should add $V(d_{z\cup w})$ to the argmax.  
c. On line 142-143 the authors state that Assumption 3 assumes ""Z has sufficient variability with respect to the variability of U"". But isn't assumption 3 actually assuming that U has sufficient variability with respect to the variability of Z?  
d. On line 173 it is stated that $\mathbb E[Y(a)|X,U]$ may not be identifiable under proximal causal inference. But doesn't assumptions 1-5 allow for such identifiability?  
e. Remark 1 is not true. For example if $\pi(X)=0.5$ then $\pi$ is constant but the policy class will not be in $\mathcal D_{\mathcal Z}\cup \mathcal D_{\mathcal W}$.   We actually need the restriction that $\pi$ is both constant and in the set $\\{0,1\\}$.  
f. The results of Appendices E and G should appear in the main paper as propositions, theorems or corollaries.  
g. For sections where over 10 references are cited back-to-back, I think readability would be improved if these citations appeared in chronological order.  
h. The authors should add a Discussion section that summarizes the results of the paper and proposes important avenues for future work (also see my comments on the Limitations section).




References:

1. Qi Z, Miao R and Zhang X. Proximal Learning for Individualized Treatment Regimes Under Unmeasured Confounding. JASA. 2023.

2. Zhao Y, Zeng D, Rush AJ and Kosorok MR. Estimating Individualized Treatment Rules Using Outcome Weighted Learning. JASA 107 (499): 1106-1118. 2012. 

3. Qiu H et al. Optimal Individualized Decision Rules Using Instrumental Variable Methods. JASA 116 (533): 174-191. 2021.

4. Wu P, Zeng D and Wang Y. Matched Learning for Optimizing Individualized Treatment Strategies Using Electronic Health Records. JASA 115 (529): 380-392. 2020.

5. Zhao YQ, Zeng D, Laber EB, Song R, Yuan M and Kosorok MR. Doubly robust learning for estimating individualized treatment with censored data. Biometrika 102 (1): 151-168. 2015. 

6. Raghu et. al. Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning Approach. MLHC 2017.

7. Zhao YQ, Laber EB, Ning Y, Saha S and Sands BE. Efficient Augmentation and Relaxation Learning for Individualized Treatment Rules using Observational Data. JMLR 20: 1-23. 2019.

8. Zhou X, Mayer-Hamblett N, Khan U and Kosorok MR. Residual Weighted Learning for Estimating Individualized Treatment Rules. JASA 112 (517): 169-187. 2017.

9. Pu H and Zhang B. Estimating optimal treatment rules with an instrumental variable: A partial identification learning approach. JRSS-B 83 (2): 318-345. 2021. 

10. Yoon J, Jordon J and van der Shaar M. GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets. ICLR 2018. 

11. Chen Y, Zeng D, Xu T and Wang Y. Representation Learning for Integrating Multi-domain Outcomes to Optimize Individualized Treatment. NeurIPS 2020.

Limitations:
I did not notice the authors acknowledge any limitations of the proposed work. I would recommend adding a ""Discussion"" section that summarizes the results of the work, and addresses limitations by discussing important avenues for future work. In addition to what was discussed in the Weaknesses and Questions sections, another important limitation is that the proposed method assumes that assumptions (1)-(5) hold and that both $h$ and $g$ are correctly specified, while $d_z$ and $d_w$ appear to only require some of these assumptions to hold. 


Rating:
6

Confidence:
4

";1
z9NLqoFvZ0;"REVIEW 
Summary:
This paper discusses an important topic about safe reinforcement learning, which explores the state-wise issue. It is a significant problem because, in the real world, state-wise constraints are one of the most common and challenging constraints in safety-critical applications. Most safe RL methods focus on cumulative safety, which may need to be improved to ensure safety during deploying RL in real-robot applications. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The writing quality is good.
2. Theoretical results sound good to me.
3. The code is provided in this study.


Weaknesses:
1. The experimental results are very weird, especially for the reward performance. For example, in Figure 1, all the baselines’ reward values are almost the same; please check the environments' reward settings. This is also shown in Figure 4.
2. The method cannot ensure safety while deploying the method in real-world applications; demos that the authors provided also confirm this point, that is, sometimes the agent will violate safety constraints and crash into obstacles.


Limitations:
1. It is hard for me to find some new insights from this study; the problem and the solution are not new.  As for the problem, it is already defined in many papers, e.g., CPO, PCPO, and CRPO; as for the solution, it just revises the cost setting from a trajectory, and does not ensure safety at each step.
2. As for the theoretical analysis, most of the findings could be known from CPO by revising the cost optimization settings.


Rating:
4

Confidence:
3

REVIEW 
Summary:
This work tackles the problem of state-wise safety in the reinforcement learning problem. To this end it introduces the framework of Maximum Markov Decision Process and an algorithm State-wise Constrained Policy Optimization (SCPO) to solve the problem. Numerical results illustrate the performance of the authors algorithm.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well written and supported by extensive numerical results.


Weaknesses:
Novelty is slightly limited. Similar state augmentation mechanisms have been studied before. The algorithm developed here arises from a somewhat straightforward application of trust region methods to such state augmentation mechanisms.


Limitations:
Addressed.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel approach to solve safe RL tasks. This approach is based on a new method SCPO and corresponding MMDP framework. Authors show the efficacy of this method by providing mathematical guarantees. Additionally, authors give useful practical implementation tips to improve reproducibility of their work.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
originality
  - This paper extends CMDP framework to account for state-wise safety guarantees. It's a fairly natural extension, and authors did a great job of explaining the shortcomings of existing methods and how theirs addresses the gaps. 

quality
  - The paper is well-written and includes thorough mathematical analysis and practical tips for implementation. 
  - Experimental section includes fair comparison with existing SOTA methods.

clarity
  - The paper uses consistent notation with previous papers (i.e. TRPO).
  - Including pseudocode (in appendix) is useful.
 
significance.
  - The paper has a good significant impact on safe RL. The presented method is novel and beats existing methods on the selected benchmarks. Authors also provided concrete future direction for this work.

Weaknesses:
- Not necessarily a weakness, but including intuitive explanation of proposition 1 and 2 would be helpful to increase overall readability of the paper.
- Paper heavily relied on concepts TRPO, but didn't give explanations about them in the background section. 

Limitations:
Yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel framework called Maximum MDP to address the problem of state-wise constrained policy optimization, namely the authors considers limiting the expected maximum state-wise cost rather than the cost for each state. Similar to the TRPO/CPO framework, the authors derived a worse-case constraint violation guarantee and practical algorithm which were shown in be effective in a set of robotic locomotive tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- I really like the simplicity of the idea proposed by the authors, state-wise safe RL is a much more common requirement for real-world safety critical problems but most literature on safe RL focus on constraining the cumulative cost. Reformulating the problem as constraining expected maximum state-wise cost eliminates the scalability issues associated with state-wise safe RL and effectively transforms the problem into a cumulative cost problem.
- Furthermore, this formulation allows the authors to easily adapt existing algorithms for cumulative constraints (CPO) with little modifications.
- Experiment section is well-structured and answers key practical questions associated with the proposed algorithms.

Weaknesses:
- Theorem 1 doesn't really depend on the specific formulation of MMDP and is really a finite-horizon variant of the policy improvement theorem from TRPO/CPO (which I think is a nice contribution in itself), I think the authors should try to convey this
- There is a major error in the proof in the appendix, note that equation 20 is not correct since $I - P$ is not invertible

Limitations:
Yes

Rating:
7

Confidence:
4

";0
HwWkIwzzKF;"REVIEW 
Summary:
This work studies the CBwK problem beyond the worst case scenario, presenting two results regarding the worst-case locations and log rate respectively. The proposed algorithm utilizes a re-solving heuristic that achieves $O(1)$ under full-information and $O(\log(T))$ regret under partial-information with some regularity conditions. The worst-case guarantee is also presented.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This is a well written paper. The key ideas and main contributions are clearly presented. The proofs seems complete and easy to follow. 

Weaknesses:
This reviewer has doubts on the exact contribution of this paper. 
Firstly, the problem studied (CBwK) is well-established in the literature. And the setting that the authors choose might be not so realistic and sort of marginal. For example, one major concern from my understanding is that the budget is ""soft"", since the budget constraint is only required in expectation, and in summation. However, in reality, ""hard"" constraints should be considered prevalently, where the constraint should not be compensated, and the expectation might also be unnecessary. The assumption of null action might not be met in reality as well.

Secondly, the algorithm and the re-solving heuristic are from literature. There's something new in the proofs since extending from BwK to CBwK requires a more complicated estimation.

To add to these, there are existing work on constrained reinforcement learning, e.g. [1] that considers similar setting. Since RL is typically considered more general a setting than CB, the results there should also be at least comparable.

Lastly, the assumption for full-information seems overly strong, and the results does not seem very pertinent to the main claims. Maybe it could be better moved to the appendix to make the contribution more clear.

[1] Sobhan Miryoosefi, Chi Jin, A Simple Reward-free Approach to Constrained Reinforcement Learning

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This works considers a general setup of Contextual Bandits with Knapsacks (CBwK). The authors identify sufficient conditions under which constant of logarithmic regrets are possible (while previous authors were mainly dealing with $\sqrt{T}$ worst-case regrets). The studied algorithm is rather intuitive and is based on a sequential refinement of the underlying best static LP problem. 



Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. Identification of the conditions for $o(\sqrt{T})$ regret is a great contribution.
2. The algorithm does not need to know whether the conditions are satisfied---it is adaptive.
3. The main body exposition is rather clear, but more details would be appreciated.
4. The approach is intuitive and computationally tractable.
5. Overall I liked the proofs, they are a bit hard to read due to heavy notation, but once the reader is comfortable, the presentation is rather good.

Weaknesses:
Here are the weaknesses in no particular order

1. Bounds are in expectation (which is fine per-se, but from my experience the results are often formulated with high probability)
2. Little to no comments on the actual stopping time of the algorithm, is it much smaller than T? (I only found this information in the appendix)
3. O(1) is great, but what kind of 1 it involves is not present in the main body.
4. Regret is not the only part of the story: e.g., Agrawal and Devour 2016 allow o(1) budgets, instead of constant budgets. In this case comparing to their result is not really fair. 
5. While computationally tractable, the algorithm is not very efficient, one needs to solve a potentially huge LP problem at each step. 




Limitations:
I was not aware of the work of Chen et al 2022, but upon skimming it looks very similar to the current submission (tools and techniques-wise). A broader discussion would be appreciated.

While being honest and correct, the extension to the continuous case, is not very satisfactory (but maybe it is a matter of taste).



Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies stochastic contextual Bandits with Knapsacks. The authors provide an algorithm that guarantees regret smaller than the worst-case $\tilde O(\sqrt{T})$ in non-degenerate instances. In particular, they provides an algorithm that achieves $\tilde O(1)$ regret when the fluid LP has a unique and non-degenerate solution. Moreover, in the worst-case the algorithm maintains $\tilde O(\sqrt{T})$ regret.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The main paper is clear and well written. The problem under study is interesting. The technical results are not straightforward.

Weaknesses:
Most of the results in the paper are based on the existence of an unique and non-degenerate solution (Assumption 3.1). The formal definition of this assumption is missing. Moreover, while this assumption is commonplace in the linear programming literature, it is not clear whether it makes sanse in the bandit with knapsack problem. Finally, previous work suggests that large regret is unavailable in many standard cases.

The main paper does not give a clear intuition of the techniques used to prove the theoretical results and the appendix is difficult to follows. Moreover, the appendix is heavily based on results in other papers, e.g., Chen et al. [2022]. This makes the proofs hard to follow. Moreover, it is not clear which are the main technical contributions of the paper, and to what extend the results follow directly from the application of known techniques.

Limitations:
Yes

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of contextual bandits with knapsacks and provides an algorithm that goes beyond worst-case (i.e., provide logarithmic regret) under the mild condition of unique optimal solution and non-degenerate solution. This also simultaneously enjoys an optimal worst-case regret bound when the conditions aren't met. The paper also shows a Omega(sqrt(T)) lower-bound when the instance has a unique optimal solution and a degenerate solution.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The paper considers a significant generalization of the results in Sankararaman and Slivkins 2021, in that it extends the logarithmic regret from two resources and best-arm optimality to arbitrary resources and unique best solution.

+ The paper also proves that the new algorithm derived from resolving the program enjoys worst-case regret bounds that are optimal (in the interesting regime of B ~ O(T)).

+ The results also applies to full policy-based contextual bandit problems, as opposed to prior work with logarithmic regret bounds which only applies to the linear contextual bandits setting.

Weaknesses:
- The algorithm needs to (a) solve a program at each round (can this be removed?) unlike UCB type algorithms where the program needs to be solved only when the ucb of any single quantity changes by a constant factor and (b) even in a single round the run-time of solving the program is unclear (I might be wrong, please correct me). Although the proposed results are interesting, this seems like a major downside from the algorithm front.

- [Some rewording of comparison with prior work] The proposed algorithm is very similar to that of [Flajolet and Jaillet, 2015] (the paper mentions this, although the wording could be more generous to the prior work). Likewise, the paper mentions that the setting considered in Sankararaman and Slivkins '2021 is ""and
almost surely excludes all problem instances"" which is not technically true. Note that the prior work paper uses the same argument of perturbing LPs in the case of d=2. Thus, the key difference is extending to d > 2 and not about the number of instances. 

- Although the lower-bound presented in this paper is interesting, I think some of the lower bounds are implied from the instances presented in Sankararaman and Slivkins '2021.

---

EDIT: 

I still don't know how to implment this algorithm apart from the trivial way of having one arm per (context, arm) pair. This is important, since then the algorithm is trivial, and the results then likely follow from prior works, in particular [1] which implements this for the k-armed bandit setting. Thus, the paper needs to explain clearly why this is better than that trivial reduction, and as a result, the bounds are significantly better. I believe multiple reviewers get at this same point, and i am not able to see a  convincing argument. Please make sure to incorporate this detailed comparision in the next version of the paper.


[1] Logarithmic regret bounds for Bandits with Knapsacks - Arthur Flajolet, Patrick Jaillet

Limitations:
This is a mathematical paper and no societal impact.

Rating:
4

Confidence:
4

";0
K10zWxlEGI;"REVIEW 
Summary:
This paper develops a new latent variable model for neural recordings ('ODIN') that uses a flow-based readout to map from the latent space to neural activity. The authors motivate their method by the injectivity of such flow-based methods and compare it to baseline models with linear and MLP readouts. ODIN exhibits superior performance on synthetic and biological datasets across a range of performance metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It is important to improve the ability of our neural data analysis tools to capture the neural dynamics giving rise to our high-dimensional recordings. This paper uses new ideas from machine learning to this effect and develops a method that shows good performance across a synthetic and a biological dataset.

The authors are also very thorough in their characterization of the different models, where they include an impressive breadth of evaluations including reconstruction of neural activity, latent trajectories, activation functions, and learned fixed point structure.

Weaknesses:
As a reader/reviewer, I am left unconvinced that injectivity is a key problem with existing methods.

Linear models $Y = WX$ are for example injective provided that $W \in \mathbb{R}^{N \times D}$ is at least rank D, which will happen with high probability unless there are specific mechanisms to prevent it.

Similarly, while the paper does demonstrate that 'ODIN' performs better than MLP-NODE, it is not clear that this is due to limitations related to the injectivity of the MLP-based model. Indeed MLPs are perfectly capable of learning injective functions - and even if the mapping learned in this case is not injective, it is hard to know whether the performance difference is due to a lack of injectivity without exploring a much broader set of models and showing that injective models consistently outperform non-injective models (for example, the linear model considered in this work is injective with high probability, yet performs poorly).

Additionally, ODIN is motivated as being more interpretable than existing methods, but it uses a fairly complicated generative model, which makes the final learned mapping more opaque to the user. In this sense, it could be seen as less interpretable than simple linear methods, where the learned mapping is easily accessible and understandable (although this of course comes at the cost of a significant decrease in performance). Indeed, the authors state in L46 that ""versions of a latent system's dynamics $f$ and embedding $g$ that are less complex and use fewer latent dimensions can be much easier to interpret than alternative representations that are more complex and/or higher dimensional."" They do address the question of dimensionality, showing that ODIN captures neural data with fewer latent dimensions than linear methods, but this comes at a substantial increase in the complexity of $f$ and $g$.

Limitations:
The authors have addressed the limitations and potential negative impact of the work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This manuscript looks at the challenge of capturing latent dynamics from neural data, a common modeling challenge in neuroscience that has attracted significant recent attention.   One common approach is to use deep learning methods, and recent highly predictive methods have been proposed that rely on high-dimensional latents to capture the structure.  This manuscript proposes to use an injective readout from the latent dynamics to encourage learning simpler and more realistic patterns from data, and qualitative results on the trajectories of the dynamics appear promising.

Updating to acknowledge the rebuttal. My review generally remains the same.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The major contribution of this manuscript is to focus on the readout layer of a commonly used model in neuroscience with promising results.  As this technique is largely generalizable, this is a useful trick that could be applied in many situations.

The experimental results look at many facets of the performance, including predictive performance on multiple tasks as well as the properties of the inferred dynamics and fixed points.  There is a worthwhile discussion on the greatest utility of machine learning methods in neuroscience, which often needs cleaner interpretation than slightly improved performance.  The fact that this method can get solid performance will a small latent dimension and capture real dynamics is promising.

Weaknesses:
The theoretical backing of this work is sparse, at best, and the proposed technique is very heuristic.  When will this technique be good enough?  When will it theoretically hold?  The long-term impact of this work would be greatly enhanced by answering these questions.

The experimental results are promising, but it is worth noting that there remains a gap in the predictive performance between a much more complicated model, AutoLFADS, and the proposed approach.  It would be very useful and expand on the contribution of the manuscript.

In the results, it appears AutoLFADS and GPFA are only shown with a single setting.  It would be worthwhile to examine how these models change as a function of their parameterization as well.

There should be a greater discussion of model selection, as it can be a difficult problem when considering many metrics of performance.

Limitations:
The discussion seems largely fair, and there is no concern on negative societal impact.

Code is not included in the submission.  While the authors claim they will release if accepted, it would have been a stronger claim had anonymized code been included.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes Ordinary Differential equations autoencoder with Injective Nonlinear readout (ODIN) model, a model for inferring latent representations underlying high-dimensional neural spikings. Additionally, the ODIN model is designed to recover the latent dynamics and generative process. Through evaluations on synthetic and real neural datasets, ODIN is shown to be able to recover the true generative process.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written, with all details clearly stated for understanding the model and the experiments;
- The proposed model seems to work well on synthetic data;
- Visualisation of the fitted latents is sounding; 

Weaknesses:
- The effect of injectiveness in the generative process is not evaluated;
- From Figure 2C, it seems like with three-dimensional latents, the linear model is able to yield similar performance as the full ODIN model (with normalising flow) in terms of latent inference, hence hindering the validity of the motivation of the paper, which is the non-linear embedding from the latents to the high-dimensional spikings;
- The proposed ODIN model is overly complicated comparing to standard neural manifold finding methods, with complicated neural architecture, I suspect related models would also yield similar high performance with comparable neural architecture and compute;

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents ODIN, a new autoencoder model of population neural dynamics that uses a neural ODE (NODE) to model latent dynamics. The core innovation is the use of a simplified invertible ResNet to ensure that a nonlinear and approximately _injective_ readout from latent dynamics to population neural activity. The authors argue that approximate _injectivity_ encourages the model to learn simple and interpretable latent dynamics that can help explain the underlying neural computations. The authors evaluate their algorithm on both synthetic and real neural recordings, comparing its performance with related algorithms.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
* The focus on readout injectivity is to the best of my knowledge novel in this field.
* The paper is well-written, with excellent and easy-to-understand figures.
* The authors evaluated their algorithms extensively, assessing the reconstruction of (i) neural activity, (ii) latents, (iii) neural activation function, and (iv) fixed-point reconstruction on synthetic and real neural recordings.

Weaknesses:
* It's unclear from the experiments whether ODIN significantly outperform MLP+NODE.
    * In figure 2C, the improvement of ODIN over MLP+NODE at the true dimensionality ($\hat{D}=3$) is small and it's unclear whether the improvement is significant. Some statistical tests here would make the argument more convincing. The claim that ODIN significantly outperforms MLP+NODE at $D>3$ is not convincing for me, as it would be fairly easy to pick the true dimensionality for MLP+NODE using State $R^2$ in Figure 2C.
    * MLP+ODE and ODIN both able to identify fixed-points accurately in Figure 3.
    * Figure 4C seems to be a clear case that shows ODIN outperform MLP+NODE. However, there are no error bars in the figure, which makes it hard to assess significance.
    * MLP-NODE is not evaluated in Figure 5?
* The paper could benefit from a more extensive discussion on how the hyperparameters were chosen. Did the authors conduct a hyperparameter search? My worry is that the small improvements of ODIN over MLP+NODE could be attributed to _unlucky_ hyperparameters.
* One common approach to encourage ""simple"" dynamics/readout is through weight regularization. The paper would be more convincing if it included more detailed discussions of/comparisons with such approaches.

Limitations:
yes

Rating:
5

Confidence:
4

";0
w91JqNQLwy;"REVIEW 
Summary:
The authors introduce a novel second-order method for sparse phase retrieval. Compared to previous algorithms, it exhibits faster convergence and better recovery. The method leverages sparsity to reduce the size of the linear system that needs to be solved at each iteration in order to determine the approximate Newton direction (reduced from n^3 to s^3), and a second-order approximation of the intensity-based objective.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper presents strong results and a theoretical analysis of the algorithm in both the noisy and noise-free case.

Weaknesses:
The sample complexity required for initialization and refinement is sub-optimal. The experiments are only on toy data.

Limitations:
The authors discuss the limitations of their method.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors propose a second-order algorithm based in Newton projection for the sparse phase retrieval algorithm. The proposed algorithm is similar to Hard Thresholding Pursuit, where the free variables (i.e. the support) is first identified by a hard thresholding step, followed by an update on the free variables via a Newton projection step.
As is standard for approaches to phase retrieval, the proposed method first performs an initialisation stage to ensure that the initial guess is sufficiently close to the true signal, then applies the proposed second-order method to obtain global convergence. There is are theoretical results proving quadratic convergence for the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The performance show substantial gains compared to previous methods, moreover, it establishes a quadratic convergence rate.


Weaknesses:
This work is incremental compared to HTP of [28]. HTP can already to be interpreted as a second order method. In terms of per-iteration complexity, the proposed method is the same as HTP. The comparison in ‘iteration complexity’ is somewhat unclear because the complexity given in HTP is for exact recovery, whereas the rate given in Table 1 for the proposed method is to obtain accuracy \epsilon — is it just that [28] does not prove a quadratic rate, or do we expect [28] to have worse convergence behaviour in general? Moreover, [28] proves finite convergence for their method, does the proposed method also achieve finite convergence?

In terms of practical performance, the convergence plots show that the proposed method has faster convergence compared to HTP, but the performance for HTP here is worse than the performance reported in [28]. Perhaps it would be useful to replicate the exact experiments in [28] so that a clear comparison can be given? In general, it would be useful to have a discussion on the differences with HTP and an explanation as to why the performance is superior to HTP, given that both are second-order methods. I also had a look at the proof and it is again similar to the proof given in [28], so it would be useful again to have a discussion on the differences and novelty over [28].


Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper focuses on the sparse phase retrieval problem and introduces an efficient second-order algorithm based on Newton‘s method. The algorithm aims to recover sparse signals and offers a quadratic convergence rate while maintaining the same per-iteration computational complexity as first-order methods. Experimental results demonstrate that the proposed algorithm outperforms popular first-order methods in terms of convergence rate and success rate in recovering the true sparse signal.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The authors' algorithm exhibits a lower complexity per iteration and a higher convergence rate compared to popular first-order methods. It is noteworthy that this is the first algorithm to establish a quadratic convergence rate.
2. The experimental results clearly illustrate the superiority of the proposed algorithm.
3. The paper effectively communicates the motivation behind the development of the second-order algorithm and highlights the complexity reduction achieved by restricting Newton's step to a subset of variables.

Weaknesses:
1. The authors mention two prevalent loss functions but do not provide an explanation regarding the difference between these functions in the numerical experiments. It would be beneficial if the authors clearly explain the distinction between the two functions, particularly why the first function is used for initialization and the second one is used in Newton's update.
2. Equation 12 introduces J_{k+1}, which seems to be highly dependent on the choice of S_0, the initial support. This raises concerns about the algorithm's sensitivity to the initial point. It would be valuable for the authors to address this issue and discuss the potential impact of the initial point on the algorithm's performance. 
3. Regarding the overall contribution, this paper focuses on approximating the objective function using a quadratic function, which can be limited. Also, this paper may be interested to only a few people attending this conference. 



Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
The work proposes a new algorithm for phase retrieval of sparse signals. 
Specifically, it focuses on a faster algorithm targeting quadratic convergence with the same number of measurements that are also needed in other algorithms. A proof of a quadratic convergence rate is established and experments illustrate the benefit also in experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper focuses on aspects in phase retrieval that are often ignored. In particular, a proofable faster convergence rate has not been the focus of other works so far.
It is well-written and easy to follow. 
It may well become a new standard for phase retrieval (or a starting point for other similar algorithms) if other researchers can reproduce the excellent performance.

Weaknesses:
The novelty is limited in the sense that second order algorithms are well known. However, adaptation and convergence proof for the phase retrieval setting are indeed novel and interesting.
It is not clear why this subset of existing algorithms has been used for the experiments.

Limitations:
The authors present no drawbacks of their method compared to the existing algorithms. In particular, the fact that existing algoritms need fewer measurements for refinement but perfom worse in the phase transition Figure 2 is surprising. Eventually, this is an artifact of the restriction to maximally 100 iterations for success (if the initialization is bad, significantly more iterations might be needed and could still make an algorithm successful, although for a significant computational cost). 
It is strange that a faster algorithm is in this sense also more ""robust"".


Rating:
7

Confidence:
3

";0
8vuDHCxrmy;"REVIEW 
Summary:
This paper focuses on the task of open-vocabulary 3D instance segmentation, which involves predicting 3D object instance masks and their corresponding categories. The authors highlight the limitations of traditional closed-vocabulary approaches that operate within a predefined set of object categories, which restricts their ability to handle novel objects and free-form queries.

To address these limitations, the authors propose OpenMask3D, a zero-shot approach for open-vocabulary 3D instance segmentation. OpenMask3D utilizes predicted class-agnostic 3D instance masks and performs mask-feature aggregation using CLIP-based image embeddings. This allows the model to reason beyond pre-defined concepts and handle open-vocabulary queries.

This paper demonstrates its superiority over closed-vocabulary counterparts. The proposed method has the potential to enhance various applications such as robotics, augmented reality, scene understanding, and 3D visual search.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors propose OpenMask3D as the first zero-shot approach for open-world 3D instance segmentation, offering a unique solution that leverages predicted class-agnostic 3D instance masks and CLIP-based image embeddings. This combination of ideas and techniques demonstrates originality in problem formulation, model architecture, and feature aggregation for open-vocabulary 3D instance segmentation.

The methodology is presented in a clear and structured manner, allowing readers to understand the proposed approach easily. The experimental evaluation is conducted on the common benchmark datasets, and the authors perform ablation studies to gain insights into the design choices of the model. 

The results demonstrate that OpenMask3D outperforms other open-vocabulary counterparts, particularly in scenarios with a long-tail distribution of objects, which advance the state-of-the-art 3D instance segmentation in the complex real world.


Weaknesses:
While the paper demonstrates several strengths, there are also some areas that could be improved to further enhance its contributions and impact. 

Evaluation on a Single Dataset: The paper conducts experiments and ablation studies on the ScanNet200 dataset, which may limit the generalizability of the findings. It would be beneficial to evaluate the proposed OpenMask3D method on multiple datasets, including those with varying object distributions, to assess its robustness and performance across different scenarios. A more diverse evaluation would strengthen the claims made in the paper.

Long Pipeline: another weakness of this work is the long pipeline of the proposed OpenMask3D model. The pipeline consists of multiple stages, including a class-agnostic mask proposal head, mask-feature aggregation module, and multi-view fusion of CLIP-based image embeddings. The length of the pipeline may introduce computational complexity and potentially impact the efficiency and real-time applicability of the model.

Lack of Computation Cost Comparison: While the paper discusses the architecture and methodology of OpenMask3D in detail, it does not provide a thorough comparison of the computational resources required and the speed achieved between different methods.

Problematic Structure of section 4.1: sec 4.1 is named as ""Closed-vocabulary 3D instance segmentation evaluation"", while both closed-set instance segmentation results and open-vocabulary instance segmentation results are presented in this section. 


Limitations:
The paper briefly mentions limitations of  OpenMask3D. It would be valuable to provide a more thorough analysis of the limitations, and discuss potential method / direction that could be explored to solve it.

Potential negative societal impact is not applicable for this work.


Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes to perform open-vocabulary instance segmentation by utilizing class-agnostic 3D instance segmentation masks from a 3D instance segmentation model trained on scannet200 and generating class labels for it using CLIP. The paper proposed to first obtain class-agnostic instance masks from a supervised Mask3D model (without using class annotations). Then it finds the images where the objects are best visible. It then projects the 3D segmentation masks in those views and refine them using SAM. Finally they average features for each object from multiple views and at multiple scale to arrive at one feature vector per 3D instance mask. The class label can then be obtained by doing dot-product with the obtained feature vector and the language embedding of the class. The results show that the proposed method is superior than prior point-based methods when also supplied with class-agnostic instance segmentation mask in closed-vocabulary setting.  The paper also shows some qualitative results with free-flowing natural language.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written, I especially appreciate the helpful supplementary video and text content which made the nitty gritty details of the pipeline very clear. 
- Open-Vocabulary 3D instance segmentation is a very useful task which hasn't been tackled before -- this paper brings attention to it (that said, I have some concerns here as mentioned in weaknesses)
- The proposed method obtains better results than prior point-based methods like OpenScene

Weaknesses:
- Open-Vocabulary: The proposed method relies on 3D instance segmentation predictions which in turn relies on 3D segmentation annotations. This, however, is not available for wide variety of objects, hence I am unsure if we can conclude that this model is indeed “open-vocabulary”. For example: If instead of scannet200, the proposed model uses class-agnostic masks from a model trained on 20 scannet classes, would it be able to achieve decent results on Scannet200? Would this model trained on scannet work on a different dataset like MatterPort3D? Additionally, since 3D datasets are significantly smaller than their 2D counterparts, doesn’t relying on 3D instance segmentation mask a serious bottleneck which wouldn’t scale? The point-based models are open-vocabulary in the sense that they are not bottlenecked by any 3D-specific annotations — at the same time I do agree with the point of this paper that they can only do semantic and not instance segmentation. However, using instance masks via 3D annotations might not as well lead to “open-vocabulary instance segmentation” proposed in this paper. Ideally, it should primarily have results on held out 3D categories that the model or any of its components have never seen during training.  
- In continuation of the above, the results for 3D instance segmentation in open-vocabulary setting is only qualitative and not quantitative. I understand though that prior methods too show only qualitative open-vocabulary results, and so this is said as a minor point and not a major complaint. However, the lack of quantitative results on categories outside the training data is concerning — especially since this proposed model particularly used labels from scannet which may not generalize beyond the 200 categories they were trained on, and on out-of-domain 3D scenes. 
- Unfounded Claims: 
    - L304: The paper highlights that when given access to oracle masks at “test time” to their model, it outperforms the supervised Mask3D model on tail AP by 9.1%. As the paper concludes in L305-308, this result indicates that if somehow we are able to obtain high quality class-agnostic masks, we do not need supervision for class labelling as their method can outperform supervised Mask3D. In my opinion, this is a misleading claim because while their baseline “Mask3D” has access to ground truth masks and classes during training, it does not have access to oracle masks during test time. This makes the comparison unfair. AP is very sensitive to quality of the segmentation mask and if supplied oracle masks to Mask3D it may do much better. A very crude way to supply that would be — computing Hungarian matching between the predicted masks and ground truth masks (without class labels), and for each predicted mask replace it with the matched ground truth mask while keeping the class label same. In general though, this comparison of supervised mask3d and the proposed method needs much more care to make balanced conclusions.
    - (Minor) L290-291: Claims that “the ablation study show that effect of 2D mask segmentation is less significant than the effect of multi-scale cropping”. Based on this, one might expect the row 2 of component analysis section to be (significantly) better than row 3. However, on some metrics row 2 wins while on others row 3 wins. And the difference is not that much.

Limitations:
The paper do not discuss limitations. I think the biggest one is their reliance on class-agnostic instance segmentation mask annotations which would be good to discuss in the ppaer.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a method for 3D open-vocabulary instance segmentation. It proposes to use a class-agnostic Mask3D to get some instance mask proposals, project the instance points to 2D views to get some 2D segments, and extract some instance features based on the 2D segments by CLIP image encoder. Then, text descriptions can be used as queries to find the best instance proposals in an open-vocabulary way by comparing the similarities with the instance mask features. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well organized with good figures and easy to understand. 
2. The paper is the first method for open-vocabulary instance segmentation in 3D scenes. 
3. The experiments and analysis in the paper show the great zero-shot ability of the method. 

Weaknesses:
1. The method relies on RGB-D images, and thus cannot generalize to 3D scenes (e.g., 3D scenes collected by LiDAR) without 2D images. 
2. The experiments are conducted only on the ScanNet dataset. Experiments on more datasets like the Matterport3D used in OpenScene are expected to prove the generalizability of the method. 
3. The inference speed of the whole framework seems slow as the method uses some heavy models (e.g., SAM and CLIP) for multiple views.  SAM model is even used multiple iterations for each mask. A comparison of the inference time between this method and OpenScene is expected.

Limitations:
Some of the limitations are discussed in the last paragraph of the paper. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper addresses the problem of text-based 3D instance segmentation. To tackle this problem, the authors propose OpenMask3D, a zero-shot approach for 3D instance segmentation that utilizes class-agnostic 3D instance masks and multi-view fusion of CLIP-based image embeddings to aggregate per-mask features. The model's performance is evaluated through experiments and ablation studies on the ScanNet200 dataset, where it outperforms OpenScene in some metrics.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

**Clarity and quality**: The paper is well written and explains all the components with clarity and in detail. The figures are excellent. The results are very neatly presented and overall the paper is engaging to read.

**Empirical Evaluation**: The paper includes extensive empirical evaluations on the ScanNet200 dataset. The paper not only demonstrates the performance of OpenMask3D but also presents ablation studies on the number of frames used, the use of 2d masks and multi-scale crops to understand the contribution of these components.

**Relevance**: The work addresses a highly relevant challenge in computer vision and autonomous systems. As these technologies become increasingly prevalent, the ability to understand and interact with a diverse range of objects becomes critically important, underscoring the relevance of this research.

Weaknesses:

**Novelty of the task**: The paper makes strong claims about being the first to introduce ""open-vocabulary 3D instance segmentation"". However, given OpenScene and others I don't think it can be claimed that the problem of text-based 3D instance segmentation is novel. OpenScene might not have used the word ""instance"" but it does show results for ""Open-vocabulary 3D object search"" which is similar in meaning. 

**Significance of the Contribution**:  It is true that most existing segmentation approaches have relied on a closed-set of objects in 3D annotated datasets and thus a system that can perform open-vocabulary segmentation would be quite a significant contribution to the field of 3D scene understanding. However, the technical contribution of the proposed approach is somewhat limited considering its similarity to OpenScene. The main differences seem to be the frame selection and feature aggregation strategy, but if this is the case I would expect these components to be more prominently handled in the paper (rather than claiming a whole novel task).

Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper proposes to solve open-vocabulary 3D instance segmentation. It uses a class-agnostic 3D instance segmentation model to obtain instance masks, then gather multi-scale image features from multiple frames by CLIP and SAM to do the open-vocabulary classification task.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper proposes to use instance-level features for 3D open-vocabulary instance segmentation, which is not attempted by previous works
2. The proposed module exhibits reasonable improvements, as shown in Tables 2&3.

Weaknesses:
1. The design of the framework may be complicated for real-world usages. It uses SAM to do segmentation for multiple frames, and then use multi-scale images for CLIP to inference. Each component like SAM and CLIP is a large foundation model and takes a while to inference, not mention that they are used multiple times.
2. The idea is not so novel. Although such idea of using instance mask features is not attempted in 3D instance segmentation, it has been widely adopted in 2D open-vocabulary segmentation tasks, like OpenSeg[1], ODISE[2], ZegFormer[3].
3. The experiments are not thorough. For example, the details of using SAM and RANSAC are not studied.


[1] Scaling Open-Vocabulary Image Segmentation with Image-Level Labels, ECCV2022
[2] ODISE: Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models, CVPR2023.
[3] Decoupling Zero-Shot Semantic Segmentation.


Limitations:
See weakness

Rating:
4

Confidence:
4

";1
fifeeUmV4Z;"REVIEW 
Summary:
This paper introduces a novel approach to tackle the challenge of noisy label learning through a generative framework. Firstly, it presents a new model optimization technique that establishes a direct association between the data and clean labels. Secondly, the generative model is implicitly estimated by leveraging a discriminative model, thereby eliminating the need for training a separate generative model and enhancing efficiency. Thirdly, the paper proposes an informative label prior inspired by partial label learning, which serves as a supervision signal for noisy label learning. Extensive experiments conducted on various noisy-label benchmarks demonstrate that the proposed generative model achieves state-of-the-art results. Remarkably, it achieves these results while maintaining a comparable computational complexity to discriminative models.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- This paper introduces an informative prior for the latent clean label in noisy label learning, which is interesting.
- Experimental results show the effectiveness of the proposed method.


Weaknesses:
1. My concern is whether it is reasonable to build a generative noisy-label learning model, which only assumes that Y causes X. The previous work, where the latent feature Z and Y cause X, seems more reasonable.
2. From the perspective of the algorithm, it seems unnecessary to limit the algorithm to image data, while the notation at Line 138, Page 4 and the experiments focus on the image data, which is required to be explained or conduct more experiments on other kinds of datasets.
3. Some symbols need to be explained. For example, at Eq.(9), Page 5, $y_i(j)$ and $p_i(j)$ is very confusing. So is $|\mathcal{Y}|$ at Eq.(12), Page 5.
4. Some paragraphs, especially for the approach, need to be polished up to better explain how to address the proposed three issues in the abstract. 


Limitations:
The authors analyze the limitations in the conclusion of the paper. It is suggested that more explanations should be made on the reasonability of their generative model. Besides, some paragraphs need to be polished up.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper discusses the solution of learning with noisy labels by directly optimizing P(X|Y) relying on associating the data with clean labels directly. A informative label prior is derived with experimental results on several benchmark datasets.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The author derive a solution for generative noisy label learning under some very strong and unrealistic assumptions.

Weaknesses:
(1)	The paper is very difficult to understand partially because the method is not well motivated. For example, there are many generative model based noisy label learning methods such as
(i)	Label-Noise Robust Generative Adversarial Networks, CVPR 2019
(ii)	From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model, ICML 2022.
Moreover, the approaches that donot directly optimize the P(X|Y) usually because it is not tractable under general conditions. This paper is adding many strong but unrealistic assumptions to directly optimize P(X|Y) may not sound like a reasonable approach. Namely the contribution point 1 is not clear.
(2)	I have strong doubt about the theoretical soundness of the paper. For instance, in the equation (10), why the clean label prior can be defined as the linear combination of the three terms \tilde{y}_i, c(i) and u_{i}(j), as these three terms may not be mutually exclusive and they may overlap with each other. 

(3)	Another issue is in the equation (12), the authors said “ the label ui is obtained by sampling from a uniform distribution of all possible labels proportionally to its probability of representing a noisy-label sample”, this assumption is way too strong and cannot be true. As a matter of fact, there are very few cases that the label distributions are uniform. Frequently, the noisy label distributions are highly imbalanced. Please take a look at the reference [ii] in ICML 2022, the usual way to assume noisy label distribution is multinomial instead of uniform distribution.

(4)	The contribution is not clear. Besides the contribution 1 is not really a contribution, in the second point of their claimed contribution, they said “Our generative model is implicitly estimated with a discriminative model, making it computationally more efficient than previous generative approaches.”
This is also not valid as there are many generative adversarial network based noisy labeling which applied both generative model and discriminative model and let them collaborate with each other (for instance, the reference [i]) Thus, the second point is also very ordinary and not novel as well.



Limitations:
As mentioned before, as the contribution of this paper is not clear. Both of the points (1) and (2) are actually done by previous work with a wider scope and less strict assumption.

In addition to that, assuming the noisy label distribution to be uniform is too restrictive, making the solution has little usage.

Last but not least, the solution is derived based on very strong and unrealistic assumption such as equations (10) and (12), making the experimental results not convincing.

In summary, the paper is too far away from the level of a Neurips paper.

Rating:
1

Confidence:
4

REVIEW 
Summary:
This paper focuses on improving the efficiency of the generative model in the context of learning noisy labels. To achieve this, the authors first introduce a generative framework whose loglikelihood given a variational posterior can be extended into a label transition term and two KL-divergence terms. Then, the authors demonstrate that the optima of one of the KL-divergence terms could be used to transform a discriminative model into an implicit generative model without extra computation cost. 

To further improve the performance of the proposed framework, the authors also present an informative label prior which combines benefits from both the high coverage (sample from Categorical distribution) and the low uncertainty (sample from Uniform distribution) as well as the information contains in the noisy labels.

The proposed work has two main contributions: 1) derive a KL-divergence term from the generative model that allows a discriminative model to be transformed into an implicit generative model, which guarantees the performance of a generative model and the efficiency of a discriminative model at the same time for learning noisy labels; 2) propose a novel clean label prior which allows a tradeoff between the label coverage and uncertainty. Although the second contribution is not as novel in terms of its simplicity, the visualization of its coverage and uncertainty enhances its impact.

The results of the wide range of experiments on benchmark datasets for noisy labels demonstrate the effectiveness of the proposed framework. The ablation studies are also carefully designed and conducted to validate the contribution of each component of the framework. 

###########################################################################

######################### Post Rebuttal ######################################

###########################################################################

The authors have addressed all of my concerns. I am happy to raise my score from 6 to 7.



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-presented and easy to follow, with detailed descriptions of each component. The ablation of the framework and the analysis for the proposed clean prior are well performed. The performance gains on CIFAR benchmarks are significant, especially when the instance-dependent noise ratio is high. This indicates that the proposed model could indeed improve the model's robustness against noisy labels.

Minimizing the KL-divergence term KLD( q(y|x) || p(x|y)p(y) ) to estimate the generative model parameters using discriminative model parameters is novel. In addition, the contribution regarding the clean label prior makes this paper a valuable addition to the community.


Weaknesses:
Minor:

1. The authors should elaborate more on how the modeling P(X|Y) contributes to the informativeness of noisy labels in the introduction part.

2. Please check the references. Some of them are incomplete (e.g., no journal or conference name for [37]).

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
Most previous works address learning with noisy labels with discriminative models while this paper takes the generative approach which maximizes directly on associated data and clean labels. This generative model is implicitly estimated with a discriminative model, making it computationally more efficient. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper successfully addresses several issues that might exist for approaches rely on generative models e.g. challenging to train and tend to use uninformative clean label priors.

- The experiments are extensive with results on many datasets with both realistic and synthetic label noise. All results show the effectiveness of the proposed method.

Weaknesses:
- The method is not well motivated. Even though most previous methods often adopt discriminative models and generative models are less discussed, it is still very hard for the reviewer to understand why generative is better than discriminative models for noisy label problems. 

- The author argues that the small loss hypothesis offers little guarantee of successfully selecting clean-label samples, however, this hypothesis is very related to the early-learning phenomenon in which the clean labels tend to fit earlier in the training than the noisy labels. This paper still uses this early learning to estimate the clean label prior. 

- Figure 2 is difficult to read and it does not make understanding the method any easier, the presentation of the paper should be significantly improved. 

Limitations:
The authors adequately addressed the limitations.

Rating:
5

Confidence:
5

";0
kJHKkRAZ0W;"REVIEW 
Summary:
The paper presents a look-ahead strategy for more efficient active learning when used with self-supervised learning features. The approach uses Neural tangent kernels and pseudo-labels generated by clustering self-supervised or active learning features to estimate an approximation of the empirical risk of each unlabeled data sample. It then selects those examples for label annotation that will maximally reduce this empirical risk in each iteration. The paper demonstrates the validity and performance of this approach on 5 image datasets, showing that the approach outperforms other baseline active learning methods in most cases, and remains dominant over a larger range of training budgets over earlier SOTA methods for low and high budget strategies.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Originality: To my knowledge this is the first look ahead strategy for active learning that combines a neural tangent kernel with clustering based pseudo-labels to estimate an approximation of empirical risk of each unlabeled data sample to select for  active learning. 

Quality: The paper is of average quality. It is incremental, building on earlier concepts of NTK and applying it to active learning with self-supervised features. It does a good job motivating, and validating the approach. The presentation in section 3.3 with a lot of new terms and notations is tiring and could be simplified. 

Clarity: The paper could be improved with a few more editing passes to complete some incomplete sentences and polish the grammar for better readability.

Significance: The work is significant since it shows an approach that is at or improves on SOTA for active learning with self-supervised features. 


Weaknesses:
I see the following weaknesses:

1. Section 3.3 is currently dense with a lot of new terms and notations that are not motivated or explained well. I suggest the authors refine this section, explaining more how the arguments lead to their proposition. The appendix is not very helpful as it currently stands to understand this proposition well. 

2. A major dimension missing in the paper is how the time taken for active learning with NTKCPL compares with other SOTA methods in the different budget regimes. I suspect that NTKCPL is faster, but it is not clear if it indeed is faster, and if so, by how much. Since there is also a clustering algorithm run for each iteration, it is not clear how the overall approach scales with size and dimensionality of data, number of classes etc. 

3. In Algorithm 1, I think line 6 should read “min(b_0/2,”. It now reads “min(b_i/2,”

4. Figure 34 2 compares various methods with NTKCPL. However, different panels use different colors for the same method, making it hard to read the (already dense) plots. Please use consistent colors/line types for the same method in each plot



Limitations:
I don’t see any significant negative societal impact of this work.


Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposed a new NTKCPL method to reduce the approximation error and it has a wider effective budget range in the setting of active learning on top of self-supervised model. The experimental results on several Computer Vision datasets (e.g., CIFAR-10, CIFAR-10, SVHN) validate the effectiveness of the proposed methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well-written. The problem this paper focuses on is important, and the proposed method is interesting.
2. This paper provides both theoretical and empirical results, which is great for a top machine learning conference like NeurIPS. The experiments are sufficient, and the conclusion is convincing.


Weaknesses:
1. A case study is suggested. For example, in CIFAR-10 dataset, which class or classes are much better than others or all the classes become better with the same scale? Providing detailed case study about the datasets together with improved metrics will make your conclusion further convincing.  

Limitations:
Some potential limitations are suggested to add. For example, a better approximation method may need more computational resources. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
Active Learning is a crucial problem that focuses on selecting a subset of examples from an unlabeled dataset to be labeled. The primary objective is to ensure that when the model is trained using these selected examples, it achieves a lower empirical risk upon evaluation, assuming all the unlabeled data points are eventually labeled. 

While getting labels is a difficult task, current foundation models that utilize self supervision are on-par with many supervised learning procedures. This work aims to use the existing self supervised trained models as the feature extractor, to train a classifier network with the active learning. To get the samples to train the neural network, the paper proposes to use NTK to get the classifier's output, if it was trained on an example say $x^{\prime}$ from the unlabeled pool. Then based on a criterion that depends on the accuracy ( 0-1 loss ), algorithm returns a set to be labeled. 

This process is done iteratively. Paper shows improvement against common baselines such as Random/Coreset/BADGE/Entropy/Lookahead and shows that there are improvements. Overall, the problem is well motivated, however, there are several confusions and in particular writing issues, that makes this submission not fit at this stage to be published. 






Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- Idea of using NTK on top of self supervised trained network is good, as it also provides a scope for theoretical guarantees. 
- Wide spectrum of baseline has been covered, and gains are decent in low budge regime.
- Adaptive strategy for refining clusters is interesting.

Weaknesses:
Major weakness of this work is lack of clarity and writing. The notations at many places are not clear to me at all, with interchanging subscripts with ""t"" time and indexing for label at many places. I will summarize the places where there is clear notational abuse, or inconsistency in the upcoming lines. While the work is interesting, the mathematical notations and algorithm needs to be written very precisely and clearly, which in current form does not seem fit for the publication. 

1. Section 2 never formally defines what $f$ is mathematically. What is the input space and what is the output space? Does it output class logits, or features? 
2. Wherever an $\operatorname{argmin}$ is written, it should always have the space of optimization 
3. Section 3.2 near equation 4, it is written ""We denote the predictions of NTK with the dataset $D_{C}$ as $\hat{f}_{DC}$"". then does $f$ also outputs a class, or a one hot vector? 
4. In summary the proposed algorithm NTKCPL chooses examples to be labeled such that when added to the labeled pool, they'd minimize the empirical risk? Since the backbone is not trained, and they're also using NTK, how different is it in spirit from Mohamadi et al?
5. In NTKCPL Algorithm, I don't understand what exactly is $f_{self}$ and $f_{al}$. It is never defined. Is it the feature extractor and the learned classifier? If yes, then why define a new variable? 
6. In NTKCPL Algorithm line 6, $b_i$ is never defined. 
7. In NTKCPL Algorithm line 13, subroutine for NTK calling is never defined. Moreover, it seems like an overloading of $\hat{f}$ argument. Lastly, there doesn't seem to be utility of $f_{0}$ other than this routine. 
8. Line 20 seem to be performing vacuous setminus from the unlabeled pool, where as in the labeled pool it seems incorrect to have a tuple of input and its pseudolabel as well.
9. Where is $f_t$ being used? There needs to be a full subroutine of AL procedure starting from scratch (that is $L = \emptyset$ going all the way to the required budget, in batches, if needed). 
10. $g$ is never mathematically defined in the proposition. 
11. I don't understand the meaning of dominant labels. Moreover, the usage of $D_{dom}$ is incorrect if it is indexing over the set of class labels, as previously $D_{.}$ is being used for the dataset. 
12. Lots of unnecessary notations introduced such as $ymap$ which can be avoided by appropriately defining $g$
13. What is the meaning of $nff$ subscript, and similarly, $fnf$? 
14. Why should clusters based on classifier being trained be reliable? That is usage of $f_{al}$?
15. Experiments did not mention the pretrained data for each of the dataset/arch, neither mention about the MLP arch. 
16. What's the reason for Entropy and other popular methods to underperform even at decently high budget such as order of 1000s? 
17. How is max number of clusters determined? 
18. How different is coverage estimation from accuracy?

Lastly, the work would've been benefited, if there were experiments from CLIP models, which are one of the most popular available pre-trained models. 

Limitations:
I think the paper should've had the use-cases where experiments involve pre-trained CLIP ResNet models. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper proposes a active learning strategy that combines self-supervised learning with NTK approximation to estimate empirical risk more accurately. The proposed method outperforms state-of-the-art methods and has a wider effective budget range. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Well-written: The paper is well-written, informative, and easy to understand. The authors provide clear explanations of the proposed method and the analysis, making it accessible to a wide audience.

Comprehensive analysis: The paper presents a comprehensive analysis of the proposed method, including an ablation study and experiments on various datasets.

Experimental results: The paper presents experimental results that demonstrate the effectiveness of the proposed method on various datasets. The results show that the proposed method outperforms state-of-the-art methods in most cases and has a wider effective budget range.


Weaknesses:
* Table 2 is misssing

* More self-supervised learning method + active learning should be compared

* Novelty seems not strong enough for NIPS, as the author mentions, self-supervised + active learning has been worked before, and would you explain why the previous methods are not competitive as this



Limitations:
The proposed method requires a pre-trained self-supervised model, which may not be available or feasible to obtain in some scenarios. This limits the applicability of the proposed method to certain domains and datasets. 

Additionally, the paper does not provide a detailed analysis of the computational complexity of the proposed method, which may be a concern in some scenarios where computational resources are limited.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel approach that combines active learning with self-supervised learning, known as neural tangent kernel clustering-pseudo-labels (NTKCPL). The method leverages the power of the neural tangent kernel (NTK) in conjunction with self-supervised learning features to enhance the estimation of lookahead. Additionally, clustering-pseudo-labels are employed to estimate the classification error. The paper includes a thorough analysis of the approximation and presents comprehensive experimental results, comparing the proposed methods against benchmark techniques.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
As demonstrated in the comparison experiments, NTKCPL exhibits substantial performance gains.

The analysis of CPL error is important, as it effectively reveals that errors arise from over-clustering and impurity. Building upon this analysis, the proposed cluster generation algorithm effectively addresses these issues, displaying a logically concrete solution.

Weaknesses:
The paper is motivated from the concepts of ""phase transition"" and ""effective budget range"" in active learning. However, it lacks analysis regarding why the proposed method can increase the effective budget range. Additionally, in Table 2, the absolute percentage of the ""Effective Budget Ratio"" is dependent on the chosen total annotation quantity, e.g. a larger total annotation quantity leads to a smaller ""Effective Budget Ratio,"" suggesting that the ""Effective Budget Ratio"" is not well-defined.

Limitations:
The authors have addressed the limitations of this paper. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper aim to develop an active learning method effective across various budgets and compatible with self-supervised learning. The proposed approach, NTKCPL, a look-ahead active learning strategy, selects a subset that are expected to train network to minimize error of unlabeled data pool. For efficiently estimate the model prediction trained with candidate set, they employ NTK. To do so, they freeze the network's backbone and train only the classifier. Pseudo labels are assigned to the unlabeled data pool for empirical risk calculation, achieved by applying a constrained K-means algorithm to self-supervised features. The loss between pseudo labels and approximated predictions is calculated to select data that will likely helpful to minimize the unlabeled pool's loss. The proposed method is evaluated on five datasets and outperformed the baselines on most of the datasets and budget ranges.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Combining NTK and pseudo label from clustering to estimate empirical risk is new to me.
- The proposed method is evaluated on various datasets, and in most cases, it demonstrated superior performance compared to the baseline approaches.
- They analyze the approximation error of the empirical risk when using NTK and CPL

Weaknesses:
- Limited technical contribution
    - An essential element of the proposed method stems from earlier work [26] that utilizes NTK approximation of DNN prediction for look-ahead active learning.
    - The most prominent distinction from [26] is that this work utilize expected error reduction instead of expected model output change for active selection, and proposed a method for assigning pseudo labels to facilitate this.
    - Given that both expected error reduction and pseudo labeling through feature vector clustering are widely used techniques, the technical contribution of the proposed method could be seen as incremental.
- Concerns about practicality
    - The method needs to freeze the backbone to ensure the accuracy of the NTK approximation (line 151 - 154). However, according to the existing self-supervised learning literature [a], there is a substantial performance gap between fine-tuning the entire network and those that only train the classifier.
    - Moreover, the proposed method appears to be dependent on the quality of the features learned through self-supervised learning. Although it is claimed that the active learning feature is used to improve clustering purity, the results in table 1 reveals a performance advantage for self-supervised features in low-budget situations.
- Need for comprehensive baseline comparisons
    - It would be beneficial if the performance of [24] was also reported, given that the proposed method follows the training configuration of [24].
    - It appears that the results using self-supervised features on Cifar 100 may be missing. It would be advantageous to see these results.
    - Furthermore, I am curious as to whether the baseline methods were also trained with the frozen backbone, and how their performance would vary depending on this factor.
- I encountered difficulties in smoothly following the provided script. I believe there is room for improvement in the writing.
    - I found the paper's main point to be somewhat confusing. On lines 49 - 52, one of the stated goals of the proposed method is an active learning strategy with a wider effective budget range, yet on line 226, there is a focus on the low-budget regime.
    - Furthermore, the notations in the method section have not been clearly defined, and they are somehow confusing. For instance, in the script, 'f' denotes a neural network (line 125), 'f_0' represents the network's output (line 127), 'f_self' and 'f_al' are features (line 1 of algorithm 1), 'f_t' signifies the classifier (line 21 of algorithm 1), and '\hat(f)_cpl' is the prediction (line 170).

[a] He, Kaiming, et al. ""Masked autoencoders are scalable vision learners."" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.

Limitations:
The paper appropriately states its limitations and broader impacts.

Rating:
5

Confidence:
3

";0
ldulVsMDDk;"REVIEW 
Summary:
The authors provided a theoretical analysis for independent subnet training and provided a convergence analysis in the case where communication compression is present.

The authors discussed the scenario when bias is not present and provided two analyses in the homogeneous and heterogeneous case, respectively before extending their theorems to the case with bias.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Sections 3 and 4 are well written and clear, which break the scenarios down in an intuitive way and presented the theorems and outlines clearly.

Weaknesses:
1. Introduction could be more straightforward and can dive straight into the main technical contributions of the work. It was not clear why the problem is well motivated and what are the main technical hurdles until reading section 2 and onwards. A clearer presentation in the intro can make the paper much more readable and well-motivated.

Limitations:
The work is theoretical and does not seem to have any potential negative societal impact.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper provides a theoretical analysis of the convergence properties of Independent Subnetwork Training (IST), for distributed Stochastic Gradient Descent (SGD) optimization with a quadratic loss function. The analysis considers both the cases of homogeneous and heterogeneous distributed scenarios, without restrictive assumptions on the gradient estimator.

The work characterizes situations where IST converges very efficiently, and cases where it does not converge to the optimal solution but to an irreducible neighbourhood. Experimental results that validate the theory are provided in the Appendix.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides a solid analytical treatment of the important problem of distributed optimization with reduced communication overhead by means of Independent Subnetwork Training (IST).

Compared to previous work, the analysis of the paper does not rely on the restrictive assumption of a bounded stochastic gradient norm.

The paper is well written – the exposition is clear, and the material is well structured and well presented.

Weaknesses:
The work considers distributed Stochastic Gradient Descent (SGD) training with a quadratic loss. As mentioned by the authors in Section 3, a simple quadratic loss function has been used in other work to analyze properties of neural networks. While this loss function can still provide interesting theoretical insights, it would be valuable to extend the analysis and the experimental results to more generally used loss functions.

Minor comments:

-- Line 17: ""drives from"" may be changed to ""derives from"".

-- Equation after line 217: it seems that the first part of the equation
""$\mathbb{E}[g^k] = \bar{\mathbf{L}}^{-1} \bar{\mathbf{L}} x^k \pm \bar{\mathbf{L}}^{-1} \bar{b} - \frac{1}{\sqrt{n}} \tilde{\mathbf{D} b} = $ ..."" 
should be rewritten as
""$\mathbb{E}[g^k] = \bar{\mathbf{L}}^{-1} \bar{\mathbf{L}} x^k - \frac{1}{\sqrt{n}} \tilde{\mathbf{D} b} = $ ..."".

-- Equation after line 221: it seems that the first part of the equation
""$\mathbb{E}[x^{k+1}] = x^k - \gamma \mathbb{E}[g^k] = $ ...""
should be
""$\mathbb{E}[x^{k+1}] = \mathbb{E}[x^k] - \gamma \mathbb{E}[g^k] = $ ..."".



Limitations:
The work relates to distributed training of large-scale models, which generally correspond to significant power consumption and CO2 emissions. However, the IST method studied in the paper aims at allowing distributed training with reduced communication overhead, corresponding to potentially reduced power consumption.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Independent Subnetwork Training (IST) is a technique that divides the neural network into smaller independent subnetworks, trains them in a distributed parallel way, and aggregates the results of each independent subnetwork to update the weights of the whole model.
This paper aims to analyze the behavior of IST theoretically. Specifically, it considers a quadratic model trained by IST. It conducts convergence analysis under both homogeneous and heterogeneous scenarios and shows that IST can only converge to an irreducible neighborhood of optimal solution.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This is probably the first work providing a thorough theoretical analysis of IST.

Weaknesses:
1. This paper should include a more comprehensive motivation for the theoretical study of IST. This could involve discussing the potential limitations of current IST architectures and how a theoretical analysis can guide future modifications to improve their performance. By doing so, reviewers will have a clearer understanding of the significance of the paper's findings and how they can be applied in practice.
2. The main body of this paper does not have any experimental results. The authors should include some key experiments to validate their theoretical analysis.
3. The authors should consider expanding the scope of their experiments beyond quadratic models to include other types of models that are commonly used in SOTA IST papers, e.g., ResNet, and Graph Convolutional Networks, as listed in this paper's reference. This would allow reviewers to better understand the generality of the paper's findings and how they can be applied to real-world applications.


Limitations:
Future works are discussed in the conclusion section. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This submission presents a theoretical analysis for the independent subnetwork training (IST) algorithm for training models under both data and model parallel settings. The convergence guarantees are analyzed for quadratic loss functions, when using permutation sketches as the model compressors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Optimization using data and model parallelization is an important practical problem, for which more theoretical insights are welcome 
- The submission presents a fairly thorough analysis of the convergence guarantees for IST for the quadratic loss function analyzed
- The authors highlight some limitations of IST, which would be useful to be aware of (e.g. the fact that in the general case of the quadratic function the algorithm does not converge to the solution)
- The submission is overall well written, and the authors are careful to introduce the setup and all the assumptions used in their analysis


Weaknesses:
- Overall the analysis presented in this submission is very limited, as it only deals with a specific type of loss function, which is not a practical instance where model parallelization would be useful
- While the authors argue that the quadratic model has been previously used in the literature for studying neural networks (lines 140-144), in my understanding this model still relies on a Taylor approximation for the loss function of a neural network (for non-linear models). It is therefore not clear how the error introduced through this approximation would translate into the convergence analysis presented in the submission
- The authors use additional simplifying assumptions, such as the fact that each node can compute the true gradient of its submodel, which would be infeasible in the case of large datasets. Additionally, the results are only presented for Perm-1 sketches when the number of nodes matches the dimension of the model, which is again not a practical use-case. While the authors argue that their results can generalize beyond these limitations, a more general formulation is not provided in the submission.
- Other works have analyzed the convergence guarantees for IST, notably [28] has done so for a one hidden layer network with ReLU activations, which is a more general case than the one from this submission. It is not clear what are the additional insights presented here, compared to the previous work.


Limitations:
I believe the authors have properly addressed the limitations of their analysis. However, I am not convinced that the contributions presented in this submission are broad enough, which ultimately motivates my score.


---------------------------------
**Edited after rebuttal**

After reading the authors' answers, and the other reviews, I decided to increase my overall rating to 5, as well as the scores for Soundness, Presentation and Contribution.

Rating:
5

Confidence:
3

";0
6fuZs3ibGA;"REVIEW 
Summary:
There exist (many) healthcare programs where it is impossible to compel individuals to take treatment; rather, the problem of solving for an optimal policy takes the form of providing beneficial services and recommendations to patients. This paper formalizes a causal framework for these cases and develops statistically improved estimators and robustness checks for the setting of algorithmic recommendations with sufficiently randomized decisions. The contributions of this paper are both theoretical and empirical in nature.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
I believe that this paper is clearly written. While I struggled to understand the work in this paper, I believe that my challenges were due to not being an expert in this field. In fact, I have learned something about causality from reading this paper.

The problem that this paper is attempting to address is very interesting to me, and the empirical results on a variety of applications illustrated the use and significance of this paper’s contributions. 


Weaknesses:
I have not identified any core weaknesses in this paper.

There’s an unattributed quote block on line 33.


Limitations:
Yes.

Rating:
7

Confidence:
1

REVIEW 
Summary:
The authors study the problem of providing treatment recommendations in systems where those receiving recommendations posses the ability to dissent or deviant from the suggested course of action. Within these types of problems the authors develop a method for providing treatment recommendations which can both account for this ability in recomendees and adhere to notions of fairness in terms of successful treatment adoption. The authors provide several theoretical results characterizing the problem space and their approach as well as experimentally demonstrating its efficacy.  On top of this the authors adapt the classic two-player cost-sensitive learning optimization technique found in prior works to their problem and introduce several interesting additions to account for the additional considerations of their problem.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

- The problem studied in the paper is important and helps bridge the gap between works on treatment recommendation which ignore autonomy of the recomendee and the real-world in which humans will inevitably express dissatisfaction or apathy towards recommendations. 

- The paper is well written and the authors clearly outline their approach, relevant background, and possible limitations.

- The authors provide a mix of theoretical results (characterizing both the problem and priorities of their approach) and experimental results.

- The assumptions made in the authors model are reasonable. The least standard assumption appearing to be Assumption 2, which may not always hold in practice (as the authors point out), but this assumptions is a good jumping-off point for the type of problem the authors wish to investigate. Moreover, in most “reasonable” domains I would expect that the majority of the effect of the recommendation is its ability to alter treatment adoption rates.

- The modifications to the traditional reductions approach are interesting and provide some useful new ideas from a technical standpoint. 


Weaknesses:
- The experimental results are missing baseline comparisons. For example, what would happen if we assumed a 100% compliance rate in terms of recommendations; applying such an approach would provide information on how much we gain by considering agent autonomy. 

- It would be informative to see some type of running time analysis or discussion on how easy it is to solve the objectives presented on line 219. For example, computing a best response amounts to cost sensitive learning at each round, which I would expect is extremely costly unless only a few epochs of cost sensitive learning are performed at each round. Although this approach is heavily inspired by (and similar to) the method in Agarwal et al., which is very fast in practice, it is hard to see how much extra compute is required as a result of the difference between the two approaches. The authors approach feels more similar to adversarial training which is known to be a quite expensive and scale poorly with larger models and feature spaces (but perhaps I am wrong about it). Any clarification would be appreciated!


Limitations:
See weakensses

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper addresses consequential decision making situation where we have a decision-making policy that outputs decisions (referred to as recommendatinos) R, individuals who may adhere to the decisions and realize a treatment T, which then leads to an outcome Y.  The problem arises, when individuals do not adhere to the decisions (i.e., follow the recommendations) and thus do not realize a treatment T, thus not realize any outcome Y. The decision making policy does not have direct control over treatment, but can only provide decisions (recommendations for treatment). Fainress comes into play, when amongst those poeple that received a positive decision (recommendation) certain social groups are less likely to adhere to the decision and thus realize an outcome. The paper suggests a method to learn a decision making policy that assigns recommendations to individuals as to fulfill certain fairness criteria in the treatment realized, e.g., reducing the disparity in treatment. The authors present a case study on ""failure to appear before court"" data (PSA-DMF).




Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
The paper studies an interesting problem, that can be relevant to the ML community working on consequential decision making). In the classical loan example, we generally assume that an individual decided to be granted a loan will utilize it, overlooking the possibility that some individuals might choose not to realize the loan. When individuals from certain demographics systematically underutilize positive decisions, this has fairness implications that are important to be considered, when designing decision-making policies.

Weaknesses:
While I believe the problem tackled is interesting, I strongly believe the paper is not yet ready for publication. The paper lacks structure and clarity. I am enlisting some important points below. 

On structure: 
1. Section 1: The last paragraph of the introduction summarizes the contribution. It would help, if the authors could point to the specific sections, where these contributions are made and provide an overview of the structure of the paper.
2. Section 3: There is no background section. The paper would benefit from a background section to understand the introduced method better. For example, I do not find the Neyman-Rubin potnetial outcomes framework introduced that the work claims to use (ln. 95-96). There is also no introduction to policy value functions, doubly robust optimization/policies.
3. Section 3: The introduction of notation and concepts may benefit from introducing concepts as definitions, e.g., the cost function. There is also a lack of introduction of notation, e.g., what $p_{t|r}$ refers to. 
4. Section 3: The mathematical definitions / equations do not carry any equation numbers. The work could benefit from adding equation numbers, such that certain formulas or definitions are easier accessible. 
5. Section 4: This section could benefit from an introduction, what this section is about. I am not understanding, why this is called ""Method"" and then section 5 also introduces a method (""We now introduce general methodology to handle ..."", ln 214)
6. Finally, there is no discussion/limitations/outlook section. The paper would benefit from a section that summarizes the work, discusses its contribution and limiations and provides an outlook. 

Clairty: I find it, in general, very difficult to follow the authors in their main idea. I am sharing a selection of difficulties I had with respect to clarity:
1. Section 1: I am missing clarity in the pipeline of decisions, why is human in the loop that makes the final prescription a problem? I understood the problem is that people, even if they receive a prescription, may not comply with it. So I understood the problem to be rather on the subject side that receives the treatment. Could you clarify?
2. Section 2: The related work section cites work relevant to the field. However, I am missing clarity in how the previous work relates to the current work. There is a list of work and description of it, but often I am missing how this relates. For example, work [37] is cited twice in line 62 and then again in line 65, I am not understanding that split. In line 72 there is also a mentioning of ""supervised release"", without explaining it. You aslo write ln. 81 ""a different line of work studies counterfactual risk assesment, which models a different concern"", how is that different?
3. Section 3: There is an introduction of social groups ""regarding fairness, we will be concerned about disparities in utiltiy and treatment benefits across different groups"", but then I find ""utility"" and ""treatment benefits"" not explained.
3. Section 3: While assumption 2, 4, 6 are addressed in lines 126 ff. I am missing an explanation of the rest of the assumptions.
4. Section 4 & 5: I am failing to follow the story and propositions. The paper would benefit from a clear explanation in words of i) what the proposition says / what the formulas express and ii) why it is important for the remainer/goal of the paper. 
5. Section 6: I am not understanding the motivation or story of the case study in the experimental evaluation. I am missing a description of the experimental setup.
7. Section 6: The figures are missing an explanation what the different parameters are, e.g., $\tau$ etc.
6. A general point, I find the usage of the word ""recommendation"" difficult in the context of fair decision making (and here the authors use binary decisions as far as I understand). This is because there is a different field of fairness literature that concerns recommendations in teh context of ranking and that is different from decision making.

In addition I have the following comments/concerns about citations:
1. I am missing citations in the introduction. For example, in line 38ff. ""a common strategy is to conduct an intention-to-treat analysis""; in line 47 ""previous work in algorithmic accountability primarily focuses on auditing recommendations"".
2. I looked up citations [20, 21] and did not found ""the well-understood notion of non-compliance/non-adherence"". Can you point me to this one? Also I found [21] to be a presentation on causal inference, not a (published) paper or book, is this correct? I am not sure about the quality of that citation. 

About formatting: 
1. line 33 ff. is differently formated than the text below, I am not understanding why.
2. Margin violations, at proposition 2, 5, and 6

Typos: 
* ln. 85 ""don't"" -> do not
* ln. 108 do you mean $\mathcal{T} = \{0,1\}$ instead of $\mathcal{T} \in = \{0,1\}$
* ln. 129 a blank to much after X
* ln. 291 ""arbitrarily""

Limitations:
A discussion or outlook section is missing. The authors do address limitations of some of their assumptions in lines 126

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper focuses on fair optimal decision rules, enhancing statistical estimators, and robustness checks for algorithmic recommendations with randomized decisions. It introduces a two-stage procedure with a complexity bound for optimizing within a constrained policy class, ensuring less conservative out-of-sample fairness constraint satisfaction.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The paper addresses an interesting topic, considering the randomness caused by humans in the loop and providing fairness guarantees. The technical aspects of the paper are robust and well-founded.

2. The paper explores two settings: one where R is randomized and satisfies overlap, and the other where R is deterministic but does not satisfy overlap. Comprehensive results are presented for both settings.

Weaknesses:
1. The fairness constraint considers the expectation of treatment but not recommendations which are the outcome of the algorithm. The reason for this is not clearly explained.

2. The assumptions made in the paper are not adequately cited or explained. While the author states that most assumptions are standard, some of them appear to be quite strong, such as assumption 5, which requires a strong decomposable linear function, and it's unclear if it can be satisfied in general.

3. There are some minor points that need clarification:
a. In Assumption 6, the symbol '\leq' may need to be changed to '\geq.'
b. Line 110 defines a cost function, but it seems like it should be referred to as a utility function, as indicated in line 157.
c. The conditions of each proposition are not clearly stated within the propositions, which causes some confusion, considering the numerous assumptions and settings.



Limitations:
I may raise my score if the questions/weaknesses are properly explained.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors characterize optimal and resource fairness-constrained optimal decision rules, and develop a doubly-robust estimator for the optimal decision rules.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I'm not very familiar with the areas of doubly-robust policy learning and am unable to assess the paper adequately.

Weaknesses:
The authors have not discussed the limitations of their work.

Limitations:
See **Weaknesses** section.

Rating:
6

Confidence:
1

";1
6oiux75UDj;"REVIEW 
Summary:
The paper introduces a novel Bayesian Optimization algorithm for the case where we select a subset of variables to query at each iteration. Furthermore, each subset will incur a different _cost_. Examples of this include: control of soil nutrients in farming, advanced manufacturing, and targeting specific subgroups for ad revenue. Previous work has focused on optimizing across subsets, but does not consider the cost. Due to the method being based on Thompson Sampling, it is not simple to extend to the cost-variable setting.

The problem setting considers the case where we can choose a subset of inputs (a control set) and its corresponding values, then the remaining inputs are samples randomly from a _known_ distribution. This setting differs from multi-fidelity problems because lack of information comes from randomness in the choice of inputs when they are not selected as part of the control set, not from querying a cheaper approximation. Our objective is to find the control set and its corresponding values that maximize the expected value of the function (with respect to the randomness in the remaining inputs). 

The algorithm is based on the upper confidence bound (UCB) acquisition function, and mainly works in four stages:
 
(a) We find the maximum expected UCB across all valid input subsets

(b) We build a set of possible control sets that have a maximum value $\epsilon_t$ close to the maximum from (a)

(c) We build a set of that includes all the control sets with minimal cost from (b)

(d) Finally, from the remaining possible control sets, we choose the set with the maximum expected UCB to query

Note that an important hyper-parameter sequence, $\{ \epsilon_t \}$ as been introduced. Then four theoretical results are presented. Theorem 4.1 shows that the algorithm is no-regret for appropriately chosen $\beta_t$ and a sub-linear $\sum_{t = 1}^T \epsilon_t$. Proposition 4.2 shows that provides an alternative condition for which the no-regret property hold. Then Lemma 4.3 and Theorem 4.4 show that the bound depends on the variance of the input distributions, with larger variance allowing for more efficient exploration. They further show that by choosing an $\epsilon$-schedule that increases the number of times cheaper controls are played, we can make the bound tighter. The $\epsilon$-schedule was useful for theoretical analysis, but in practice it is dropped instead for a simpler idea, which focuses on fixing the number of plays for each cost-group, choosing the lowest possible cost group that still has plays at each iteration when all cost groups have been explored, the algorithm reverts to the standard version with $\epsilon_t = 0$.

Experiments are carried out in four benchmarks: a 3D GP sample, Hartmann3D, a plant growth simulator, and a simulator built from the airfoil self-noise data-set. Experiment show that when there are subsets with small or moderate costs, the algorithms give an advantage to the Thompson Sampling baseline. When costs are expensive, algorithm performance drops when cost-group sizes are fixed, but if made adaptive the performance is still competitive. In addition, the simple baselines perform well when the control sets are not subsets of each other (and therefore they are not constantly querying the most expensive one). Finally, the results show that having a larger variance for the random inputs tends to give better regret, but also appears to be problem and algorithm dependent. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Originality: The author propose an algorithm that tackles a problem not fully considered in Bayesian Optimization, where there are subsets of inputs that can be queried and each subset has a different cost. They also include a thorough theoretical analysis of the regret of the algorithm, and show empirical evidence of its effectiveness. Although I have not seen this specific problem solved before, there are other works could perhaps tackle the problem (see more later).

Quality: The algorithm proposed is sound, combining known Bayesian Optimization ideas that are known to work well. The theoretical analysis is very good and complete, exploring the effect of different aspects of the problem (e.g. $\epsilon$-schedule and variance). The empirical analysis is perhaps limited to a few examples, but they provide a varied number of costs and variances which is important.

Clarity: Generally the paper is very clear, and well written. Important equations are well explained, and the implications of all theoretical results made clear. Figure 1 is very good at explaining the problem setting, and helping understand notation. Empirical results are explained and analyzed thoroughly and clearly. Perhaps a few paragraphs of the paper could be rewritten, and result exposition improved (see later).

Significance: The problem is significant to the wider scientific community, and relevant examples are given. Although it seems limited by certain assumptions and perhaps giving more detail on the examples would be better (see later).

Weaknesses:
- The paper seems highly relevant to Causal Bayesian Optimization [1, 2, 3], where they (a) choose a subset of inputs and the rest are observed from the environment, and (b) they consider the cost of the varying subsets. In particular, they do not assume that the input distributions are known but that they instead follow an unknown causal structure which appears to be a more general of a case. Indeed it could be argued that all motivating examples mentioned by the paper fall under this umbrella (i.e. there is an underlying causal structure). Empirical comparison against their baselines would be very strong, but at least a discussion of where the settings differ should be included.

- Following to the previous point, the assumption that the input distributions are known and independent of the chosen values for the control set appears to be very restrictive and limits the application of UCB-CVS. I assume the distribution could be estimated from data, so perhaps there are extensions for the proposed method when this is the case. Based on this, the significance of the proposed paper appears to be far more restrictive than claimed as it only applies when the distributions are known.

- The theoretical analysis is perhaps the strongest part of the paper, however in the end the authors opted for for a rather arbitrary way of selecting which subsets to query (i.e. by defining cost groups and selecting an allocation for each). The link between the theoretical results and the new heuristic should be made clearer, in addition, the heuristic appears to me to be sub-optimal especially in the case where an expensive subset leads to clearly sub-optimal observations but we keep querying it anyway due to the requirement to query each cost group a certain number of times. 

Minor weaknesses:

- I found the first paragraphs of section 4 (lines 115 to 138) to be a little convoluted and difficult to read. I understand there is a lot of notation to introduce, but perhaps it would benefit from a small re-write. Figure 1 does make an excellent job of summarizing it all and was very helpful.

- I found the plots in Figure 2 to be a little difficult to read, the plots are small and pixelated, and the font sizes could be made bigger (there seems to be a lot of repetition in the titles so perhaps things could be condensed to use less words and make space for larger fonts). 

- Only using 10 runs per example seems a little bit low, would be better to use more. 

[1] Aglietti, Virginia, et al. ""Causal bayesian optimization."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.

[2] Branchini, Nicola, et al. ""Causal entropy optimization."" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.

[3] Tigas, Panagiotis, et al. ""Differentiable Multi-Target Causal Bayesian Experimental Design."" ICML, 2023.

Limitations:
Limitations are addressed in the appendix: the theoretical assumptions may not hold in practice, and the assumption that the input probability distributions are independent and fixed may be violated (this is considered by Causal BO). The authors also mention poor scaling when the number of control sets is large.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The submission studies the problem of Bayesian optimization (BayesOpt) where we can choose to control a subset of the decision vector while the other variables are randomly selected.
Unlike previous works on contextual BayesOpt and BayesOpt with uncertain input, this paper leaves the choice of which variables are set to the user, and accounts for the associated cost of controlling a given set of variables.
As far as I know, this is the first work to tackle this problem.
The authors then proposed an Upper Confidence Bound (UCB) style algorithm that seeks the subset of variables that yields the highest expected upper bound on the objective function's value while incurring the lowest cost.
Theoretical guarantees are derived for the proposed algorithm, showing the algorithm achieves sub-linear cumulative regret.
Numerical experiments show that variants of the proposed algorithm perform competitively against two baseline policies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The submission tackles an original problem that is motivated by many real-world scenarios.
The paper is well-written, and the exposition is clear.
The theoretical results are explained well and are shown to be helpful in interpreting results from numerical experiments.
Numerical experiments on the other hand show strong performance from the proposed algorithm.

Weaknesses:
I don't have many complaints about the paper.
The authors can consider including the other baselines shown in Appendix B in Sec. 5 instead of simply referring to them.
The authors can also consider including a simple baseline that randomly selects the subset of variables to control for at each iteration.

Limitations:
The authors discuss the difficulty of setting the $\varepsilon$-schedule in Sec. 4.3 and recommend an explore-then-commit strategy.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work introduces a new black-box function optimization setting where only a collection of the subsets of variables can be optimized while the values of the complement variables for each set are randomly sampled. The authors propose a new BO framework based on GP-UCB, called UCB-CVS, to solve this optimization setting. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1.	The problem setting is new and hasn’t been fully investigated before. 
2.	The algorithm proposed in this work is clearly described and the authors also provide theoretical guarantees. 
3.	Experimental results show good performance of this new method. 


Weaknesses:
1.	The extent of the problem setting's relevance and interest to the Bayesian Optimization (BO) community is not apparent to me in this work, despite its novelty. The authors evaluate their methods using two real-world datasets, namely plant growth and airfoil self-noise. However, the control sets, cost values, and probability distributions are still simulated by the authors themselves. Consequently, they have not demonstrated the application of real-world problems that can be effectively addressed within this setting using the new UCB-CVS algorithm.
2.	It is not so convincing to me that the terms in the cumulative regret $R_{T}$ should be multiplied by the cost $c_{i_t}$. The objective function presented in line 138 does not incorporate the cost coefficient, and it seems natural for the cumulative regret $R_{T}$ to also exclude the cost coefficient. Moreover, this approach would align better with the single regret mentioned in line 190. The cost information is already encompassed within the value of $T$, namely, for a fixed budget $C$, if we consistently select high-cost sets, $T$ would be small, resulting in insufficient observations. Conversely, if we consistently choose low-cost sets, although $T$ would be large, the optimization procedure is more likely to converge to sub-optimal solutions. Therefore, there is no necessity to introduce an additional cost coefficient for penalization purposes.
3.	The font size of Figure 2 is too small to be read. 


Limitations:
The authors don't specifically discuss the limitations of this work. The authors can add a paragraph in their manuscript to discuss the limitations of their work based on the reviewers' feedback.

No significant negative societal impact of this work.


Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors study the problem of Bayesian optimization with cost-varying variable subsets (BOCVS) where in each iteration, the learner chooses a subset of query variables and specifies their values while the rest are randomly sampled. Each chosen subset has an associated cost. The authors analyze how the availability of cheaper control sets helps in exploration and reduces overall regret.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is with good structures and is well-written. The problem is interesting and potentially has wide applications. The authors provide a profound theoretical analysis. The experimental results show the benefits of the proposed method. 

Weaknesses:
Experimental results on a real-world application could make the paper stronger. 



Limitations:
N/A

Rating:
7

Confidence:
3

";1
m9uHv1Pxq7;"REVIEW 
Summary:
This paper contributes to the field of unsupervised face animation by introducing a novel motion refinement method to overcome the limitations of existing prior-based motion models, especially in estimating detailed facial movements.

The paper's approach introduces a new method which uses a structure correlation volume built from keypoint features. This provides motion information that does not rely on prior data. This information is used to iteratively refine the coarse motion flow estimated by a previous motion model.

The authors have conducted numerous experiments on challenging benchmarks to test the effectiveness of their approach. The results indicate that this new method enhances the capability of prior-based motion representation by learning how to refine motion. This suggests that their approach can effectively increase the accuracy of unsupervised face animation tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
In this research, the authors present a new unsupervised face animation approach that concurrently learns both coarse (global) and finer (local) facial motions. Their method integrates a local affine motion model to learn the global, coarse facial motion and a novel motion refinement module to compensate for the local affine motion model's ability to model more detailed facial motions in local areas.

The motion refinement process is based on the dense correlation between the source and driving images. To achieve this, a structure correlation volume is first constructed using the keypoint features of the source and driving images. The authors then train a model to iteratively generate minor facial motions from low to high resolution.

The learned motion refinements are combined with the coarse motion to generate the new image. After performing extensive experiments on widely used benchmarks, the method was found to deliver the best results among existing state-of-the-art methods. The authors have also committed to making the source code for their method publicly available in the future.

Weaknesses:
1. The goal of this article is to learn fine facial motion, but using the VoxCeleb dataset may not be sufficient for this purpose. In my view, finer facial motion refers to the ability to reproduce wrinkles, eyeballs, and micro-expressions at high resolution, which may require higher resolution datasets.

2. As noted by the author, the reliability of keypoints estimation heavily influences the quality of finer facial motion. It is not clear whether the improvement of existing models comes from more accurate keypoints estimation or the proposed module.

3. This paper is very similar to RAFT from ECCV2020 in terms of insight and specifically for the module. A clearer comparison between the two works and their essential differences would be helpful.

4. It is important to note that fine facial motion may not equal to better image quality. Therefore, a more thorough evaluation of the estimated motion would be beneficial. Additionally, conducting this task on higher resolution facial images would provide more convincing results.

5. The author sets the iteration number to 8, which is an important hyperparameter. It would be useful to know why this setting was chosen and whether it significantly affects the model's runtime.


Limitations:
Both limitations and social impact are discussed in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper anaylizes the limitations of existing face animation methods in capturing the finer facial motions, and hence design a non-prior-based motion refinement approach to achieve finer face motion transfer in local areas. A correlation volume between the source and driving images is constructed as non-prior motion evidence, and a refinement module is introduced to generate the fine facial motions iteratively. Extensive experiments show the effectiveness of this paper.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The idea of involving a non-prior based motion refinement module is effective to capture the fine motions in local areas.
2. The manuscript is well written and clearly states the main idea and contribution in face animation.
3. This paper performs extensive experiments, and the results of the same-identity video reconstruction task outperform many previous methods. 

Weaknesses:
1. This paper concentrates on the motion issue, but the video results of this paper are not impressive enough. No obvious improvements can be seen given the video results on the self reconstruction.
2. This paper does not show the quantitative results in terms of some usual metrics, i.e. CSIM and FID, on the cross-identity reenactment task. I wonder the performance of this work in preserving identity.
3. The introduction should be improved, as it only concludes limited contributions. 

Limitations:
This paper is effective to refine the motions to obtain better results, but is still limited to the quality of the learned keypoints, as the limitaions in the paper stated, which I think is more urgent to solve for unsupervised face animation. And this paper did not give comprehensive evaluations on the performance in identity preserving, which is important for face animation in practical applications. 
I'm not sure whether it is qualified for the conference.  If the authors could give reasonable response for the problems stated in **Weakness** and **Questions**, I would like to consider improving my rating. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a method for face animation via learning to refine a coarse motion field. The refinement is performed in a recurrent manner using the previous iteration's motion, occlusion map and structure correlation volume.  Both the qualitative and quantitative results show improvement over prior art.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1) Qualitative results show that the proposed method is clearly model finer facial movement much better than prior art, especially around the eye. FNeVR is very close but the proposed method has fewer uncanny artifacts.

2) Quantitatively as well, the proposed method outperforms prior art.

3) The paper is well written.

Weaknesses:
1) Across all the results, I notice a strong identity shift during animation. While this is common (and seemingly worse) in prior art as well, it is important to quantify. One way to measure this is to report the FaceIDLoss or L2-loss as the head is rotated 15, 30, 60 degrees from the original head-pose. I believe this is an important evaluation to include for the sake of completeness. 

Limitations:
1) An important limitation of this method is that it is sensitive to the scale of the face of the driving video. For example, around the 1:47 of the supplementary video, the face structure of the animated face deviates significantly from the source and is actually closer to that of the target. This is unavoidable due to the use of 2D key points in the method, in fact it is expected. The authors must include this as a limitation of their work. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents an unsupervised method for learning to create continuous video animations of faces given a single input image and a driving video of the same or a different subject. The method relies on the optical flow between the driving and source images to warp the features of the source image, which are then passed to a generator to create the final output frames. The main contribution of the proposed method lies in the two-step formulation of the flow estimation, in which starting from a coarse prediction using a known prior-based method (Affine Transforms, Thin Plate Splines) it iteratively produces updates to reach a more fine-grained flow (and occlusion) estimation with more local details. The flow updates are learnt in an unsupervised way without using prior models, but by building a correlation volume between the source and driving images. The paper includes experiments, generated images and supplementary videos that validate the authors' claims for more detailed animations in comparison to similar methods. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written overall and it is easy to follow along with the presented concepts and results.
- The paper includes experimental results that validate the authors' claims, as well as ablations that offer a better understanding about model design choices.
- The method produces higher-quality generated images/videos than the compared methods for unsupervised learning of face animation.
- The idea of using motivation from RAFT to built a similar correlation structure for estimating motion flow for face animation is interesting.

Weaknesses:
- Some design choices are not empirically or experimentally justified in the paper. For example, why are specifically 6 iterations used to update the initial coarse estimations? Why does it make sense to sample from the same correlation structure for all iterations, while each iteration operates at a different image scale? 
- Even though the method performs well compared to previous methods, there is still some evident identity shift in cross-subject experiments, meaning that the shape of the face slightly changes from the original to the one of the driving frame. This is evident in Figure 5 (for example 2nd row) as well as in the supplemental video. This effect makes sense because the estimated flow between different subjects possibly includes more deformation information, rather than only deformation because of expressions or rigid motion. Disentangling identity from other motions is a common weakness of cross-identity animation methods, which exists in this method, too. 

Limitations:
The authors have included a section discussing the method's limitations and possible negative societal impact. A mentioned limitation (the correlation matrix relying on learned keypoints) might also be the reason for changing the facial shape in cross-subject animations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
1. This work proposed a  new unsupervised face animation approach to learn simultaneously the coarse and finer motions.
2. The results outperformed the state-of-the-art methods on two representative datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The target task is important.
2. The method is intuitive and reasonable.
3. The results are good.
4. The paper is well-written.

Weaknesses:
1. Compared to prior model. The proposed non-prior models seems like a non-linear version of prior model based on affine transformation. When the number of keypoints increasing and the grid of affine transformation being much finer, will it approach the proposed non-prior model? Another related question is that why the proposed model works better than a local thin-plate-spline motion model (such as [38])?

2. Visualization. 1) In the visualization of comparison in video, the results of the proposed model looks close to FNeVR. Authors could also provide user study in visual quality for comparison. 2) Also, the results on CelebV-HQ is really important, since it is a much challenging dataset. Visualization results on this dataset is necessary, in comparison, ablation study and video. 3) Why didn't report the results of FNeVR on CelebV-HQ on Table 1?

I am inclined to accept this paper, if the concerns can be solved.

Limitations:
Yes, limitations are discussed in the main paper.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper presents a new approach to generate human face videos based on a source image and a driving video, which simultaneously learning both coarse and finer facial motions. The proposed approach utilizes a structure correlation volume constructed from keypoint features to provide non-prior-based motion information, which is used to iteratively refine the coarse motion flow that is estimated by a prior motion model. 

The main contributions of the paper are: 
1. A non-prior-based motion refinement approach to compensate for the inadequacy of existing prior-based motion models.
2. Utilize the keypoint features to build a structure correlation volume that represents the structure correspondence between the source and driving images across all spatial locations.
3. Extensive experiments on challenging benchmarks that demonstrate the effectiveness of the proposed approach in enhancing the capability of prior-based motion representation through learning the motion refinement.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper presents a novel approach to generate human face videos that simultaneously learns both coarse and finer facial motions. The proposed approach utilizes a structure correlation volume constructed from keypoint features to provide non-prior-based motion information, which is used to iteratively refine the coarse motion flow estimated by a prior motion model. The approach addresses a significant problem in the field of unsupervised face animation.

2. The paper is of good quality, and the method proposed is clear. The authors provide a detailed description of the approach, including the structure correlation volume and the motion refinement module. The experimental results demonstrate the effectiveness of the proposed approach in enhancing the capability of prior-based motion representation through learning the motion refinement.

3. The paper is well-written and easy to follow. The authors provide a clear and concise description of the proposed approach, including the key components and the experimental setup. The paper is well-organized, and the authors provide a clear summary of the contributions and limitations of the proposed approach.

4. The proposed approach has significant implications for the field of unsupervised face animation which addresses an important problem in this field: the inadequacy of existing prior-based motion models to capture detailed facial motions. The proposed approach is effective in enhancing the capability of prior-based motion representation through learning the motion refinement. The approach has potential applications in creating imaginative image animations for entertainment purposes, but it also has the potential to be used in creating deepfakes, which could have negative impacts. The authors acknowledge this limitation and provide recommendations for future work. 


Weaknesses:
1.	The pictures in Figure 1 are too small, especially the optical flow map is not clear enough, no obvious changes can be seen before and after refinement. Also, is the schematic diagram of affine transformation in Figure 1 drawn based on a real example? Why does the affine transformation change so much after refinement?

2.	Straightforward combination of existing techniques. The innovation of this paper is not enough, the main innovation point lies in the motion refinement module. However, the method of correlation volume and iteratively refine optical flow used in it is very similar to the correlation matrix and iteratively update in some flow estimation[1,2,3] and correspondence estimation[4,5] methods, and this set of process of first constructing a 4D correlation volume, and then iteratively updating and optimizing optical flow has also been used in some neural style transfer(NST) methods[6,7], but the author did not clarify the differences between the module they used in this paper and related modules in these NST methods.

3.	There is an error in line 180 of the article: the author claims that the look up operation on the correlation volume is shown on the left side of Figure 2, but there is no corresponding content in Figure 2. Should Figure 2 be changed to Figure 3? 

4.	In the comparison video provided in the supplemental material, the video of each method is too small, and it is difficult to see the tiny facial deformation details. It is recommended to arrange the videos of each method in the form of a nine-square grid, and enlarge the size of each video window.

5.	The experiments of verifying the effectiveness of the proposed method are insufficient. The framework used by your method is the framework of unsupervised image animation [8,9,10,11], which should be able to generate animation videos on any object category. In addition to human faces, this framework can also be applied to human bodies and animals. Previous unsupervised image animation methods [8,9,10,11] have also been tested on related datasets of different subjects, including TaiChiHD[9], TED-talks[10], and MGif[8]. Your method has only been tested on face-related datasets, but from your overall framework, I don't see any modules that restrict the method to only work on human face. Therefore, I think that the non-prior-based motion refinement module should be applied to datasets of different objects to further test its effectiveness.

6.	Lack of quantitative experiments on the Cross-identity task, such as user study that have been used in previous methods [8,9,10,11].

References

[1] Dosovitskiy, Alexey, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. 2015. “FlowNet: Learning Optical Flow with Convolutional Networks.” In 2015 IEEE International Conference on Computer Vision (ICCV). doi:10.1109/iccv.2015.316.

[2] Teed, Zachary, and Jia Deng. 2020. “RAFT: Recurrent All-Pairs Field Transforms for Optical Flow.” In Computer Vision – ECCV 2020, Lecture Notes in Computer Science, 402–19. doi:10.1007/978-3-030-58536-5_24.

[3] Xu, Haofei, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. 2022. “GMFlow: Learning Optical Flow via Global Matching.” In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr52688.2022.00795.

[4] Kim, Seungryong, Stephen Lin, SangRyul Jeon, Dongbo Min, and Kwanghoon Sohn. 2018. “Recurrent Transformer Networks for Semantic Correspondence.” arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, October.

[5] Zhang, Pan, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. 2020. “Cross-Domain Correspondence Learning for Exemplar-Based Image Translation.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr42600.2020.00519.

[6] Liu, Xiaochang, Xuanyi Li, Ming-Ming Cheng, and Peter Hall. 2020. “Geometric Style Transfer.” Cornell University - arXiv,Cornell University - arXiv, July.

[7] Liu, Xiao-Chang, Yong-Liang Yang, and Peter Hall. 2021. “Learning to Warp for Style Transfer.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr46437.2021.00370.

[8] Siarohin, Aliaksandr, Stephane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. “Animating Arbitrary Objects via Deep Motion Transfer.” In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr.2019.00248.

[9] Siarohin, Aliaksandr, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. “First Order Motion Model for Image Animation.” Neural Information Processing Systems,Neural Information Processing Systems, January.

[10] Siarohin, Aliaksandr, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. 2021. “Motion Representations for Articulated Animation.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr46437.2021.01344.

[11] Zhao, Jian, and Hui Zhang. n.d. “Thin-Plate Spline Motion Model for Image Animation.”


Limitations:
Yes, they have adequately addressed the limitations.

Rating:
4

Confidence:
4

";1
1xPsn2gCOe;"REVIEW 
Summary:
This paper introduces a new measure of visual artificial network computation 'time', $\xi_{cRNN}$, as the time-averaged uncertainty of a convolutional RNN trained with an evidential deep learning loss (Sensoy et al.). The authors proceed to analyse the dynamics of this network as it solves a range of classification tasks. Importantly, beyond the network learning to solve the tasks, $\xi_{cRNN}$ is generally well correlated with human reaction times, with which it also shares several qualitative features across tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Deep network models of the visual system have often focused purely on classification accuracy and neural representations when comparing to biological data. However, this neglects an important aspect of human behaviour, namely the time it takes to arrive at a decision. The present paper tackles this important question using recent ideas from the machine learning literature and represents an interesting step in the direction of capturing the temporal variability in human behaviour.
The authors analyse an impressive breadth of tasks and behavioural data and include many interesting analyses, both qualitative and quantitative.

Weaknesses:
The major weakness of the submission is that while there are strong _correlations_ between $\xi_{cRNN}$ and human reaction times, there is less evidence that $\xi_{cRNN}$ is mechanistically similar to human reaction times, since it is fundamentally a measure of uncertainty rather than computation time. In particular, human reaction times generally involve a tradeoff between computation/evidence accumulation and decision making (as in common drift diffusion models). On the contrary, the cRNN has a fixed computational budget and has no need or even capacity for evaluating this tradeoff. This is in contrast to a few previous deep learning models in the literature that are capable of explicitly trading off computation and actions (e.g. Graves et al., Pascanu et al., and refs by the authors in the ML literature, and Jensen et al. in the Neuro/Cogsci literature). These considerations are important because e.g. task difficulty is likely to correlate with both uncertainty and reaction time, and this raises the question of whether a model of one is automatically a model of the other.

It might the interesting to compare the current model to alternative models that more explicitly have adaptive/variable computation time, such as a cRNN that computes until $\epsilon$ reaches a certain threshold (akin to classical drift diffusion models).

As the authors mention in L326, another potential weakness is that the present approach does not easily generalize beyond classification tasks, which form a small subset of the types of problems humans and animals are faced with during natural behaviour.

_References:_ \
Graves et al.: ""Adaptive computation time for recurrent neural networks"", arXiv 2016.\
Pascanu et al.: ""Learning model-based planning from scratch"", arXiv 2017.\
Jensen et al.: ""A recurrent network model of planning explains hippocampal replay and human behavior"", bioRxiv 2023.\
Bansal et al.: ""End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking"", NeurIPS 2022.

Limitations:
The authors have adequately discussed the limitations of the work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors in this work propose a combination of model output uncertainty predictions along with stable recurrent vision architectures in order to derive a proxy for models' reaction time to process input static images. The authors use the previously published horizontal GRU architecture combined with stable RNN training methods (contractive RBP) in their experiments. The proposed work is technically sound with extensive evaluation on 4 diverse datasets (some inspired by prior work in visual psychology) where the computed proxy model reaction time ($\xi$) trends correlate positively with that of humans performing the same task. Overall, this paper contributes a new way to estimate output uncertainty of recurrent convolutional networks and to evaluate similarity between model & human perception by taking into account temporal dynamics of processing static inputs.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ The authors make several key contributions: (1) combining Evidential Deep Learning-based output uncertainty prediction with convolutional RNNs and using the AUC of the output uncertainties over time as a measure of the model's reaction time; (2) training stable recurrent vision models using the above EDL-based readout + objective function in order to obtain models whose temporal dynamics of processing visual inputs matches that of humans performing the same task.
+ The experiments performed are thorough on all datasets, it is clear that the positive correlation between model and human reaction times is present on the tasks evaluated.
+ If the authors were to release the code for some of these datasets (especially the incremental grouping task in Fig 2), this would make the contribution even more valuable as it would encourage further exploration of these understudied grouping problems
+ Clear presentation; The authors have exactly stated their contributions and have written the paper with good clarity and detail. The figures are intuitive and appreciate the video presentations showing model activations and uncertainty through time in the Supplementary.

Weaknesses:
- Choice of tasks: While the presented 4 visual cognition tasks are interesting and relevant to cognitive scientists, only the scene recognition task concerns real-world stimuli and natural vision. It would be great if the authors found any interesting patterns of reaction time on natural images or more naturalistic tasks than the ones shown here.
- Advantages of similarity in reaction time: It is very interesting that the authors are presenting recurrent networks that are correlated with humans in terms of reaction time (and the benefits as a model of biological visual processing are clear), but does this similarity translate to any significant advantages for machine learning? Are models with better RT correlation with humans also well calibrated, or more robust compared to others? In summary, inter-model comparison based on how well they rank on RT similarity with humans would be an interesting direction that the authors don't comment about here.

Limitations:
The authors have adequately addressed limitations of this work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors of the paper present a novel concept of crafting a metric for analyzing the temporal alignment between Recurrent Neural Networks (RNNs) and human behavior. The innovative approach, rooted in the estimate of task uncertainty via the application of the Derelict distribution, is both interested and pertinent. In an era where understanding the interface between machine learning models, specifically RNNs, and human behavior is of utmost importance, the authors' research is timely and bears the potential for broad impact. A commendable aspect of this study is the diverse range of tasks that the authors have attempted to cover. Their inclusive approach elevates the practical relevance of their research, thus ensuring the applicability of their findings across multiple contexts. However, there are two crucial areas that the authors need to address to improve the overall quality and comprehension of the manuscript. Firstly, in the same/different dot object task, the authors discuss a correlation between the task's difficulty, reaction time, and dot distance. It is important to note that human reaction time also ties strongly with factors such as the object's topology and occlusion. The authors assert that their network and measurements reflect human reaction time concerning these more intricate features. Nevertheless, the demonstration of this claim in the manuscript is not lucid enough. To solidify their claim, it is crucial for the authors to condition the different stimulus conditions on the distance between dots. They should explicitly illustrate how elements like the narrowness of the object outline and occlusion correlate with human reaction time and their RNN metric. Secondly, the specificity of the proposed metric to the particular cortically inspired architecture employed in this research should be clarified. It would be beneficial to understand if this metric can be generalized to other architectures. For instance, would it be feasible to use this metric if one were to implement a convolutional LSTM or another vanilla RNN? The authors' response to this concern could significantly influence the wider application of their proposed metric.  In conclusion, while the paper is insightful and tackles a highly relevant topic, it is recommended that the authors address the concerns raised to improve the robustness and clarity of their research. By doing so, the potential impact of the manuscript can be further augmented, providing a more substantial contribution to the field.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A relevant and timely paper extending the comparison between human behavior and ANNs. 

Weaknesses:
Not clear how well they demonstrate that their metric predicts human reaction time between the more straightforward relationship of dots distance, task difficulty and reaction time. In other words do they predict reaction time and object occlusion, and more complex topology of objects (narrowness of segmentation boundaries etc). 

Does their metric extend to other architectures beyond the specific one they use here. 

Limitations:
Yes

Rating:
7

Confidence:
5

REVIEW 
Summary:
This study introduces a metric for evaluating the alignment between model and human behavior wrt task complexity reflected in reaction times. The metric is easy to compute and shows qualitative correspondence with human RTs in different tasks.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and easy to understand.

The study attempts to go beyond choice responses to establish correspondence between model and behavior. For image-computable models, this is both novel and important.

The idea of using model uncertainty as a proxy for RT is intuitive and appealing.

Weaknesses:
The models are trained using an algorithm that imposes attractor dynamics which causes instantaneous  uncertainty to become zero. This is critical for their proposed metric to remain bounded. But this is a bit odd because subjective uncertainty should be non-zero for difficult tasks even if given a long time. This raises the question of whether the metric is meaningful for other tasks e.g. with ambiguous stimuli.

The authors make a convincing case for the utility of this metric for evaluating models. But the metric itself is artificial and might not have a correlate in neural activity. An alternative (and more brain-like) but admittedly more challenging approach would be to  learn a policy that chooses left/right/wait at each moment (e.g. via RL) based on the latent representation such that you can directly get a reaction time from the model. This might even work with BPTT.

Limitations:
As stated above, would be useful to give some examples of the challenges which may arise when applying this approach to other types of tasks.

Rating:
8

Confidence:
3

";1
TKXMPtCniG;"REVIEW 
Summary:
this paper proposes a gradient descent technique for learning deep neural nets with large batch sizes. the authors focus on the setting where the model size is small to medium (10M to 300M parameters) and the dataset size is also small to medium (up to 1M images in vision or ~ 3B words in NLP) but with as large batch size as possible. the goal of this works to accelerating the learning of such models(e.g., ResNet on Imagenet, BERT on the original BERT-used corpus) by using less training steps and less training time. The authors were able to show the proposed method can achieve good results (without big drop in eval metrics) while using larger batch sizes than prior arts. And when the proposed method is compared with prior arts at the same batch size, the proposed method seems to outperform prior arts as well.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The papers covers standard benchmarks like ResNet on ImageNet and BERT pretraining. and therefore can be fairly compared against many prior arts on the same tasks

Weaknesses:
This paper claims ""training acceleration"" as a key contribution. But throughout the paper, the comparison on speed up is based on number of steps or number of epochs. It is unclear what are the speed advantage in terms of wall-clock time by using the propose technique. I also checked the supp. pdf. In other works (such as LARS and LANs, which are cited by this work), the authors usually report actual wall clock time speedups as they increase the batch size and the compute infra. It was disappointing to not see any mention of that given the authors are using 768 GPUs (therefore I expect very interesting scaling behaviors)

Limitations:
the proposed technique is only validated on small-medium size model on small-medium size dataset. it is not applicable (at least no evidence provided) to large scale model training (either large in dataset size or large in model size.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors strive to propose a heuristic training strategy, called variance reduced gradient descent technique (VRGD), based on the gradient signal to noise ratio, i.e. the ratio between the norm and the variance of gradient. Compared to vanilla training, VRGD scales the learning rate with the gradient signal to noise ratio during each iteration. Then, the authors prove that the proposed method can converge within finite training steps, and claim that it will give better generalization gap compared to the vanilla training. Finally, the authors show the effectiveness of their methods via Bert-training and ImageNet training with up to 96k batch size.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
***Strengths***

1. The paper is clearly written and easy to follow.

2. With the development of hardwares, training with large batch would gradually become a basic requirement for training large-scale models on gigantic data, such as GPT. In my opinion, the authors are focusing on a very topic worthy to probe.I believe that this paper may have a potential significance not only in academia but also in the industry. However, some modification may be required currently.

3. The authors show some interesting and valuable experiment results, but not comprehensive enough.

Weaknesses:
***Weakness***


1. (Major) The authors repeatedly mention that large batch training can lead to sharp minima in Abstract and Introduction, which seems to suggest that the proposed method can avoid such problem. However, I have not found discussions or observations regarding the proposed method can solve this issue. So can the proposed escape these bad minima?

    Considering that many recent works that focuses on guiding training to converge to flat minima i.e. SAM family/gradient norm regularization, I am quite curious what would be like to adopt the proposed method in these algorithms when given large batch training. And from the results, I find that using the proposed method alone would not result better performance than these flat-minima-based methods. I have listed some typical works below.

    [1] Foret, Pierre, et al. ""Sharpness-aware minimization for efficiently improving generalization."" ICLR2020.

    [2] Kwon, Jungmin, et al. ""Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks."" ICML2021.

    [3] Zhuang, Juntang, et al. ""Surrogate gap minimization improves sharpness-aware training."" ICLR2021.

    [4] Zhao, Yang, et al. ""Penalizing gradient norm for efficiently improving generalization in deep learning."" ICML 2022.

2. (Major) Continuing with the previous comment, I find the proposed method is rather heuristic, lacking a clear motivation. I could not find clear rationale and sufficient analysis that the proposed method can benefit training. Essentially, the proposed method is simply to scale the learning rate adaptively based on a specific parameter (Eq. 10). For me, I am not quite convinced that such a learning rate scaling policy will lead to reasonable performance gain. Meanwhile, the author argue that the proposed method can lead to smaller generalization gap. However, the core that supports this claim is based on an empirical observation, which makes the mathematical proof not quite rigorous and the claim much weaker and unhelpful. 

3. (Major) I would like to discuss the convergence of the proposed method. Firstly, to my understanding, the convergence analysis focuses on analyzing to what extent can training converge on the given training samples, not testing set. So, it is not quite appropriate to use the convergence curve on the testing set to demonstrate the conclusion regarding the convergence analysis. i.e. Figure 2. Secondly, from Figure 2, the authors state that the proposed method converge 1.7~4 times faster than the conventional optimizers. But, I could not observe such a big gap between them from Figure 2, so could the authors explain how to measure the convergence here. Thirdly, a tighter bound in convergence would not give any information regarding the testing performance. A looser bound and slower convergence rate can give better testing performance in many cases, for example SAM. The author can refer to the paper below. 

    [5] Andriushchenko, Maksym, and Nicolas Flammarion. ""Towards understanding sharpness-aware minimization."" ICML 2022.

    Note that I am not saying a faster convergence is harmful. In my opinion, the core meaning of this convergence section is to prove that the proposed method can converge in finite time. And it is not surprising that The proposed method shares the same convergence rate as that in SGD, i.e. O(1/sqrt{T}) given that this method is to scale the learning rate compared to SGD. In the current version, it appears that the convergence section seems to heavily imply that the proposed method outperforms SGD without a promise of testing performance, which I disagree with.

4. (Minor) Line 32. ""However, Keskar et al. [2017] theoretically analyze the LB training and finds that it can be easily trapped into sharp local minimum, leading to strong generalization gap"". Actually, the cited paper (Keskar et al. 2017) has not provided theoretical analysis.

5. (Minor) Line 95. To my understanding, the variance of random vectors is a matrix, i.e. covariance matrix. Why is a scalar here?

6. (Minor) It is highly encouraged to show the results of training vision transformers with the proposed methods.

Limitations:
I have not found any discussions about the limitations and potential negative societal impact. But in my opinion, this may not be a problem, since the work only focuses on the learning method in deep learning. Still, it is highly encouraged to add corresponding discussions.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper examines the improvement of training throughput in a large batch parallel training setting. By employing the gradient signal to noise ratio (GSNR) as a measurement of the generalization gap during training, the authors introduce a variance-reduced gradient descent method designed for large batch training scenarios. The authors provide theoretical analysis to substantiate that the proposed variance-reduced gradient descent (VRSGD) method exhibits superior generalization compared to stochastic gradient descent (SGD) and potentially achieves faster convergence. Additionally, experimental evaluations on BERT pretraining, ResNet training, and DLRM training are presented to demonstrate the superiority of the proposed method over alternative approaches for large batch training. Furthermore, orthogonal experiments and analyses pertaining to the behavior of GSNR and sensitivity to hyperparameters are conducted to further support the superiority of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The research topic of large batch training is of considerable interest.

2. The proposed method is both simple and effective for large-batch training.

3. The theoretical analyses of the convergence rate and generalization are persuasive.

4. The proposed method consistently demonstrates improvements over other baseline approaches.

5. The paper is well-written and easy to understand.

Weaknesses:
1. The contribution is limited. It seems that the authors just introduce GSNR into the large batch training, but do not highlight why GSNR is important to large batch training.

2. Although the theoretical analysis and experimental results presented in the study are persuasive, the rationale behind the necessity of the GSNR method for large-batch training remains unclear. It is essential to provide a more comprehensive explanation of why GSNR is specifically relevant and advantageous in the context of large batch training. While GSNR can potentially enhance generalization in various settings, a more explicit justification is required to elucidate its particular suitability and effectiveness for large-batch training scenarios.

Limitations:
NA

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a new method for large batch training. It is based on the insight that the gradient-to-signal-noise-ratio for each parameter should be reflected in its learning rate, and hence modifies gradient descent to reduce the variance of the gradients. The paper then shows convergence rates of VRSGD, which are the same as SGD asymptotically, and states that VR-SGD is particularly suited to large batch training where the GSNR will be high. The paper also shows that the proposed method has a smaller generalization gap than SGD in the large batch setting. Experiments are then performed to support these claims on a variety of tasks and architectures. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The method is based on a solid insight.
2. The empirical results indicate decent gains over baselines.

Weaknesses:
1. The assumptions of smoothness and bounded gradients seem a bit too strong.

Limitations:
See weaknesses.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper focus on using large-batch training to accelerate the training of neural network. Specifically, the authors try to use variance reduced gradient descent technique to scale up the batch size and therefore to accelerate the training. The experimental results illustrate that the proposed method can scale up to larger batch size and further accelerate the training of ResNet, BERT and DLRM. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Strength:
1. This paper focuses on an important problem, accelerate neural network training. especially for large-batch training. 
2. The proposed method is very easy to follow.
3. The authors provide some results to verify the performance of proposed method.

Weaknesses:
Weakness:
1. The authors can provide more visualization and analysis about the proposed method and why the proposed method can further scale the batch size. For example, the proposed method can help the model converge to s flat region?
2. The authors should provide more results about the wall time of each experiment and verify whether the proposed can save the training time.
3. I'm not sure whether you should compare your method with more baselines since LARS and LAMB is not current SOTA. 

Limitations:
N/A

Rating:
5

Confidence:
5

";0
LJ4CYEagg3;"REVIEW 
Summary:
This work introduces two attention modules for video frame interpolation that achieve promising results in terms of quantitative and qualitative evaluation. The researchers conducted numerous experiments to provide a comprehensive evaluation of the proposed approach.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The proposed attention mechanism enhances the interpolation quality of intermediate frames.


Weaknesses:
(1) While this work does not present many theoretical contributions or fundamental insights, it is technically sound and offers improvements and modifications to existing methods. For example, the shift windows attention applied to spatial-temporal aggregation is a relatively simple and incremental approach that builds upon similar ideas explored extensively in previous research.

(2) Table 1/2 should specify the training setups of previous state-of-the-art video frame interpolation methods, as the proposed method uses six input frames. As far as I know, existing models are generally trained on Vimeo-Triplets, which only consist of 51K three-frame samples.

(3) Additionally, the specific contributions of SGuTA are unclear, as SCubA significantly outperforms it with fewer parameters and lower multi-adds. Finally, while many previous works have demonstrated arbitrary interpolation ability, this point does not appear to have been discussed in this work.

Limitations:
Please see the weakness and question parts.
No significant negative societal impact in this work.

Rating:
4

Confidence:
5

REVIEW 
Summary:
In video tasks, the computational complexity and memory requirements of Transformer is challenging. This paper employ two different Transformers in VFI task resulting in new state-of-the-art results. Furthermore, this paper conduct an analysis of existing embedding strategies, and put forth a novel half-overlapping embedding strategy. The author carried out the experimental analysis carefully.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. I believe the author contributed efficient components to the multi-frame video transformer.
2. The author has open sourced the relevant code.
3. The resulting visuals look good. Overall, the paper has a good impression.

Weaknesses:
1. The author did not discuss and compare a series of works on single frame interpolation. CVPR20-SoftSplat, CVPR22-IFRNet, CVPR22-Many-to-many Splatting, ECCV22-RIFE, CVPR23-AMT. This greatly weakens the credibility of model evaluation. Overall, the authors cite very little recent relevant literature.
The model mainly compares the ""video frame interpolation transformer"", which is not as popular as SoftSplat or RIFE as far as I know, so I hope the author will add more comparison experiments to make the evaluation more solid.
2. The video submitted by the author has only one 720p scene, and there is no other method to compare it.
3. The author has discussed that this model currently does not support multi-frame interpolation.

Limitations:
Previous video transformers often performed mediocrely at 2k or 4k resolutions, or had a large memory overhead, and it is unclear how well this work in HD scenes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper aims to address the computational complexity and memory requirements of Transformers to enhance their suitability for the video frame interpolation (VFI) task. The authors propose two novel methods, SCuTA and SGubA, which are integrated into a multi-stage multi-scale framework. SCuTA leverages the correlation between spatial information and temporal sequences, while SGubA incorporates a 3D local self-attention mechanism. Through extensive experiments, the proposed methods demonstrate superior performance in terms of peak signal-to-noise ratio (PSNR) compared to other approaches, while also exhibiting a reduced parameter count.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. Intuitive motivation: The paper provides a clear and intuitive motivation for modifying Transformers to better adapt to the VFI task, addressing the computational complexity and memory requirements.
2. Reasonable design: The introduction of SCuTA and SGubA, which leverage the correlation between spatial information and temporal sequences, is a sensible approach. Additionally, SGubA's utilization of a 3D local self-attention mechanism aligns well with the requirements of the VFI task.
3. Comprehensive experimentation: The paper presents a substantial number of experiments to support the proposed methods. The results consistently demonstrate that the proposed approaches achieve the highest PSNR while also reducing the parameter count, effectively addressing the computational and memory challenges of Transformers in the VFI task.

Weaknesses:
1. Lack of clarity in Figure 2: Figures 2d and 2f require further clarification. It is recommended that the authors explain G-MSA and SC-MSA before referring to these figures to enhance reader understanding.
2. Inference time comparison: The paper should provide information on the inference time for all the methods evaluated. This will provide a more comprehensive evaluation of the proposed approaches in terms of both performance and computational efficiency.

Limitations:
The authors do not address the limitations. Please refer to weaknesses.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper tackles video frame interpolation (VFI). It particularly aims to deploy a transformer architecture for VFI tasks. To address the computational complexity and memory requirements in transformers, it proposes two transformer networks, SGuTa and SCubA. While SGuTa uses the spatial (global) information of video frames to establish temporal correlation, SCubA focuses on local attention. Both methods exhibit linear computational complexities. The authors also introduce a half-overlapping embedding strategy to balance the trade-off between computational complexity and memory usage. Experimental comparisons are presented on several VFI benchmarks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The work attempts to tackle an important trade-off (performance vs. computation) in using transformers for the VFI task
* The proposed transformers exhibit linear time complexity which makes them easily deployable
* The qualitative results look good
* The ablation studies are thorough and show the effect of the different design choices in the proposed networks
* Code is shared in the supplementary material for reproducibility 

Weaknesses:
* The motivation of the work is not convincingly justified

   There are several works [2,3] that used a transformer architecture for VFI. However, the authors introduce two new transformer networks without drawing any motivation from the progress that has been made in this line of research. It is also not very clear why the authors need to introduce two different networks in one paper. The introduction part of the paper should be rewritten by positioning the proposed framework in comparison with existing literature and hence filling in the big jump in L38-39.

* The technical novelty of the work is limited

   The proposed modules are heavily copied from other relevant works such as VideoSwin Transformer [1].
   
   It is also not clear why the proposed multi-head self-attention (MSA) is very different from other MSAs in previous literature. As far as I understand, the key difference is in reshaping the input tensor (HW X TD instead of HWT X D) in SGuTa. The remaining operations are the same as other MSAs. The local attention in ScubA is also similar to the one used in [1]. 

* The experimental comparisons are limited and unfair

   Experimental results are simply copied (quoted) from previous works. However, the experimental settings the authors used to conduct experiments are very different (6 adjacent frames, different patch sizes) from previous works. Hence, how can the authors convincingly justify that the performance gain is coming from the proposed method and not the different experiment settings?

   Intuitively speaking, using more adjacent frames (6 versus 2 or 4) should provide more context during training, hence, it benefits the proposed method. A fair comparison would follow the common experiment protocol used in previous VFI works. 

   The benefit of half-overlapping in Table 3 is not clear. The performance gain is really negligible (0.04dB) compared to the computation overhead (significant increase in memory usage and FLOPs compared to non-overlapping baselines).  how did the authors come to the conclusion in Eq. 11? There has to be a more convincing explanation (proof) than simple observation, otherwise, the claim in L62-64 should be toned down. 

   The video presented in the supplementary file only shows one video result for the proposed method. The authors should provide a side-by-side video comparison of their method and competing approaches on different test videos.

* The writing of the paper needs improvement 
  
   The methodology part is very confusing with too many coined terms. It would be better to rewrite this part in a clearer manner.
   
   It is also not a recommended practice to introduce new acronyms in the title of the paper. 

References

[1]. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, CVPR 2022

[2]. Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and Ming-Hsuan Yang. Video frame interpolation transformer, CVPR 2022

[3]. Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer, CVPR 2022

Limitations:
The authors do not discuss the limitations of their work. 

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposed two types of Transformer for video frame interpolation: Spatially-Guided Temporal Attention (SGuTA) and
Shifted-Cube Attention (SCubA). SGuTa merges the temporal dimension and the embedding dimension during the self-attention process to explore the inherent correlations between the spatial and temporal dimensions. On the other hand, SCubA is focused on reducing the computational complexity and adapts Video Swin Transformer for frame interpolation. Both approaches show significant performance improvements compared with the recent VFI models.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
1. Good performance. Both of the proposed Transformer models---SGuTA and SCubA---outperform the existing works by a notable margin. As the authors have mentioned, given that the performance of the existing works for VFI is almost plateaued, it seems that the proposed architectures are quite effective.

2. Writing is clear and easy to understand. Also, the figures are neat, and the experiments logically match the paper's claims. The motivations of the proposed approaches are sensible.

Weaknesses:
1. Main focus of the paper is a bit confusing.

This paper has two main contributions - SGuTA and SCubA - which are separate contributions, and how to combine these two ideas are not introduced, which makes the paper look incomplete. I would like to suggest three options to remedy this issue:

a) If both SGuTA and SCubA can be combined into a single framework, then this will be the best approach - each module is shown to be effective and can show synergies when combined together.

b) If SGuTA and SCubA cannot be combined, then we should at least discuss when to use which method. It seems like SCubA is quite consistently better than SGuTQ in terms of PSNR/SSIM and also computational complexity - then what is the point of proposing SGuTA? Should we just use SCubA all the time?

c) If neither a) nor b) option is available, then I think we should consider separating SGuTA and SCubA into two distinct papers.

2. Use of more input frames and incapability of generating arbitrary intermediate time step.

The proposed method uses more number of input frames (thus, more information) to predict the middle frame. While methods like QVI [30] also uses more input frames, for truly fair comparison with the other works, I think the authors should also report the performance with triplet-based evaluations.
Also, I have one simple question: it is written in the paper that ""septuplets"" are used - does this mean that the from the frames 1~7, the input frames are [1, 2, 3, 5, 6, 7] and the GT is 4-th frame? Or, do you use [1, 3, 5, 7]-th frames as the input?

Also, most the existing flow-based works can generate arbitrary time-step intermediate frames, while the proposed SGuTA and SCubA cannot. In my opinion, at least a short discussion regarding this issue is needed.

Limitations:
The limitations of the current work and its potential negative societal impact is not adequately discussed.
The authors are encouraged to think about these issues more seriously and write them, even if the potential societal impact seems to be minimal.

Rating:
4

Confidence:
5

";0
Og2HCj3V1I;"REVIEW 
Summary:
Paper proposes an evaluation metric for generative models which compare the distributions of real and generated images using a predefined set of attributes, or pairwise occurrences of attributes. The advantage of these metrics over the previous work is that they provide explicit visibility of which aspects contribute to the final metric value.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea for the proposed metric is novel – it uses language-image models to measure alignment of distributions of real and generated images given a set of text attributes. Additionally, the metric is highly customizable for downstream tasks because users may define their own set of attributes that are of interest to the specific task and drop irrelevant attributes.

Weaknesses:
Failure modes of the metric are not discussed (or limitations of language-image models and how they affect the metric). 

Consider a scenario where there are two models, A and B, with the same SaKLD. Model A produces essentially perfect alignment on all attributes other than one which fails dramatically, causing a large spike in SaKLD histogram, and this attribute is the only contributing to the final score. Model B on the other hand performs poorly across all attributes but averaging over attributes yields the same SaKLD score as the model A. In this scenario SaKLD would potentially not agree with human judgment, since failing in a single attribute might not be visible when inspecting large image grids. Can this kind of scenario occur in practice, and if it can, what would be your recommendation for the user of the metric in that case?

Fig. 4 shows that SaKLD and PaKLD are dominated by few attributes of attribute pairs. Is this usually the case in practice? Fig. 5 (b) also indicates that adding new attributes contribute to the metric with diminishing strength. This might be misleading for the user of the metric. Intuitively, adding a large set of attributes should correspond to more thorough evaluation of the model, however, this might not be the case if few attributes are dominating the final value of the metric.

The empirical effectiveness of the attribute based metric is not fully demonstrated. The authors advocated for an interpretable metric but unfortunately end up comparing modern generative models using single scalar numbers (as the existing metrics do), instead of taking advantage of the interpretability of the metric and showing a more fine-grained analysis of the models.


Limitations:
The authors adequately addressed the limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new metric to evaluate the quality and diversity of generated images based on
interpretable embeddings. To obtain the interpretable embeddings for selected attributes, the cluster
centers of the encodings of two separate encoders, one for images and the other for the text attributes
are calculated and the interpretable embeddings are the difference between the encodings of each
image and attribute and their respective cluster centers. The interpretable embeddings represent the
direction in which a particular embedding lies with respect to it’s cluster center. The CLIPScore
between the interpretable embeddings of an image and an attribute is calculated and is named as
’Directional CLIPScore’, since the interpretable embeddings represent the ’direction’.
The authors have proposed two metrics : 1. SaKLD - to quatify how closely the attribute distri-
bution in generated images matches with that of training images. 2. PaKLD - to quantify correlations
between different attributes. The KL divergence between the probability density functions of the
Directional CLIPScores for all the images in the train data and generated data.
Using SaKLD, the KL divergence between the attribute distribution in training and generated
images is calculated. PaKLD calculates the KL divergence similar to SaKLD, except that the presence
of a pair of attributes is required in the training and generated images.


Soundness:
1

Presentation:
3

Contribution:
1

Strengths:
The motivation for the idea is good and has a huge potential impact for improving evaluation of generative models.

Weaknesses:
- Although, the motivation for developing this metric is valid, the overall methodology and the
experimental results are not convincing for the use of this metric. Also, the experiments performed
are inadequate and do not sufficiently justify how well the metric performs compared to the previously
proposed metrics. Additionally, previous metrics can directly evaluate quality of generation based
on the generated images alone, but this metric heavily relies on the attributes in the form of text
descriptions. Thus, it limits the applicability and generalizability of this metric.
- The proposed metrics use CLIPScore (which already exists) for interpretable embeddings and then
applies KL Divergence for the PDFs of the ClipScores of images and attributes, thus, showing limited
novelty.
- Most of the paper is easy to follow but some important parts like, how is the center of text attributes
calculated, results from table 1 (what does accuracy stand for) etc. are a bit ambiguous. There is a
lot of scope to improve the technical soundness of the paper. Although some of the popular metrics
1are mentioned, there has been a lot of work in the generative modelling domain which the literature
survey must cover. The proposed methodology is not very sound and is not well supported with the
experimental setup. The results are also not sufficiently explained. Diversity is the main motivation
for the paper as it is mentioned in the abstract but any theoretical or empirical work to support it is
completely missing.
- Motivation and methodology including the steps to evaluate generated samples is understandable but
some parts are ambiguous (please refer above comments).
- Based on the current state of the experiments, the contributions don’t seem to be significant as there
is not enough validation to support the claims.

 typos:
- Line 43 : Instead of ”Figure 1 (b)”, it should be Figure 1 (a)
- Line 168 : Section 3.3 First letters of all the words in the heading must be capital
Other remarks :
- Line 56 : Instead of ”If the model lacks essential attributes” the following sounds technically
correct ”if model lacks ”representation” of essential attributes”.
- Line 56-58 : This claim does not seem to be correct.
- Line 147 : Link to the specified figure is missing
- Figure 2 is never refered to in the text, is it an unnecessary figure?
- Line 253 : Generated set has non-smiling men and non-smiling women. But Figure 4 caption
says otherwise

Limitations:
limitations have been addressed

Rating:
4

Confidence:
2

REVIEW 
Summary:
The embedding space used for calculating FID is computed with Inception V3 which is trained for image classification as the target task. This means it is more likely to capture discriminative features, raising doubts about its ability to effectively evaluate generative models. There is also a need to devise a new evaluation metric that can interpret underlying factors. This paper introduces a method called Directional CLIP Score (DCS) to properly evaluate this. The pre-trained CLIP is used as the embedding space. In particular, the paper proposes ""Single attribute KL div (SaKLD)"" to measure single attribute alignment and ""Paired attribute KL div (PaKLD)"" to measure multiple attribute alignment as new metrics. This paper provides some insightful measurement result using the proposed metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The effort to evaluate generative models from various perspectives through the introduction of such metrics can be considered novel and a contribution. Especially considering the current issues such as the bias in stable diffusion [1], the proposal of such metrics can bring benefits to the field from the perspective of trustworthy AI.

Weaknesses:
It appears to be an intrinsic limitation that a significant number of samples (50k) are still required to obtain stable results. Nonetheless, thanks to the reported findings, we can gain more insights, and I appreciate that. 
Remaining concerns are written in [Questions] section.

Limitations:
It seems that there are no particular specific limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This is a very interesting research work. The main contribution of this paper is to consider the attribute information in the original training data when evaluating the quality of images generated by the model. There are two benefits to this approach: 1) determining whether the model can correctly imitate the distribution of the training data; 2) explaining which attributes the model does not perform well on. In implementing this idea, the authors found that directly calculating the CLIPScore between the image representation and the text representation of the attribute does not yield distinctive results. Therefore, they proposed Directional CLIPScore (DSC). The main idea of this approach is to move the reference point for calculating vector similarity to a more reasonable point. At the same time, they proposed two methods to apply DSC, one is Single attribute KL Divergence, and the other is Paired attribute KL Divergence (PaKLD) considering the combination of attributes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. I think this research question is very interesting. It is very meaningful to use the attribute information of the dataset to help evaluate the quality of the model's generation.
2. The Directional CLIPScore (DSC) proposed by the authors is very concise and appears to be quite effective in the case study.

Weaknesses:
1. In Section 3.3, I agree with the extraction of attributes using the BLIP and manual annotation methods. However, one method of extracting attributes is to generate an attribute list with GPT and then filter it with training data. I think the attribute list generated by GPT in advance may bring biases. The core of this paper is mainly to study the correlation between the model and the training data. However, if the list generated by GPT is not the most representative attribute, the results may be biased.
2. The experimental part in Sections 5.1-5.3 does not seem very convincing. The authors mainly verify that the proposed method can indeed be consistent with some expected experimental designs, but there is a lack of more convincing quantitative indicators to show that their proposed evaluation metrics are better than those proposed by others previous research works. I would prefer to see the authors analyze the correlation coefficient between their proposed evaluation metrics and human evaluation, as well as whether their evaluation metrics have improved in terms of correlation coefficient compared to previous evaluation indicators. This is my biggest concern for this work.


Limitations:
see weakness

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes two new metrics allowing to measure and explain the diversity of a generated set of images w.r.t a training set. Instead of the usual distributional distances relying upon an embedding space from a pre-trained model, these metrics rely on a set of textual attributes. The similarity between an image and an attribute is computed using their representation in a common semantic space, via the CLIP model - vectors are shifted using a centre of training images/attributes to make similarity scores more meaningful. Several ways to obtain attributes (Captioning, User-based or GPT-based) are investigated. The usefulness of the metrics is tested with an experiment injecting images correlated with target attributes in data, an experiment aiming to detect a specific attribute relationship in a curated dataset, and in a comparison of several generative models. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- This paper proposes an interesting approach, aiming at using the common semantic space between images and text proposed by CLIP to measure attribute and image relatedness, in order to provide interpretable representations of generated images and measure attribute-based metrics between a reference and generated dataset; such an application seems relatively original to me. 
- The two first experiments demonstrate well the usefulness of the approach, able to detect a shift when the data has been curated by human, based on which attributes. 

Weaknesses:
- The presentation of the paper could be improved upon. This include the writing, and the readability of the figures, as well as the quantity of information provided. This last points concerns mainly the presentation and framing of the problem of interpretability of representations, as well as the presentation and motivation of the experimental settings. 
- In particular, the paper lacks related work on concept-based representation for interpretability of images. While building metrics dedicated to generative model seems new to me, there are based on an idea which has been explored extensively before. See for example ""Concept Whitening for Interpretable Image Recognition, Chen et al, 2020"". 
- The paper focuses on a narrow choice of methods to generate attributes, which, to me, should be one of the key experimental investigation of the paper. Notably, the previous literature explores using different kind of attributes, coming from existing data (for example, ""Interpretable Basis Decomposition for Visual Explanation, Zhou et al, 2018"") or to be learnt (""A Framework to Learn with Interpretation"", Parekh et al, 2021). The authors only (very shortly) argue about the number of needed attributes.
- The toy experiments seem relevant but are very fastly presented and should be expanded upon. The remaining experiments are too short to be convincing and only focus on a handful of models. 

Limitations:
- Previous distributional metrics are several times referred as relying on external models in your paper. However, the attributes that you use also rely on external models (except the USER one, of course - but in this case, the computing of DCS still relies on a captioning model). How would you address this issue ? 

Rating:
6

Confidence:
3

";0
aS2Yl8s5OG;"REVIEW 
Summary:
The authors propose a new approach called Subset Adversarial Training (SAT), which differs from traditional adversarial training methods that generate adversarial examples on the whole training set. Instead, SAT applies adversarial training on a subset of the training data. They studied two variants of subset adversarial training (CSAT and ESAT). They found that robust training in one class could generalize to other classes that were not adversarially trained, which is surprising. The also found that ESAT, where they adversarially trained on harder examples, gives surprising boost to downstream robust performance with much less data.

The paper also discusses the concept of loss balancing, which is used to counteract an imbalance between the adversarial subset and non-adversarial subset when the training split is not even. The authors found that loss balancing is important for the adversarial robustness transfer observed.

In conclusion, the paper presents a novel approach to adversarial training that could help us better understand the underlying mechanism of robust learning as well as having potential implication to more efficient adversarial training.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- The setting of experiment is interesting. It is surprising that adversarially training on a single class yields adversarial robustness to other classes. The originality of the experiment is strong.
- The experiments has demonstrated possibility to decrease the cost of adversarial training.
- The paper is also very clear with thorough experiments and analysis

Weaknesses:
- Even though the finding is interesting, I think the paper could do more in terms of understanding its implication to  robust generalization. What does robust generalization to other classes imply or show about the process of adversarial learning?
- While the experiment demonstrate possibility of decreasing cost of adversarial training, it doesn’t demonstrate this in more challenging scenarios. I understand that the paper’s intention is to understand how adversarial generalization happen as opposed to achieving the best performance, but if it is the paper’s intention to gain further understanding of the adversarial training process, I am hoping for more analysis and comments about its implication to robust generalization.

Limitations:
The authors have adequately addressed the limitation of the potential negative societal impact of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper demonstrates an interesting observation: when we conduct adversarial training, we can only choose to generate adversarial examples on a subset of the training data, if this subset contains the hardest examples, then adversarial training on a subset can achieve competitive performance in robustness over the whole dataset. In addition, models trained in such a manner demonstrate good transferable feature.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The observation is surprising and interesting, which indicates the transferability of adversarial examples across different classes.

2. The authors conduct comprehensive experiments on various datasets to validate the findings.

Weaknesses:
1. One major concern is the contribution, the proposed method neither improve the robust accuracy (or clean accuracy) nor improve the training efficiency (because SAT still uses PGD-7 to generate adversarial examples, which is inefficient).

2. All the experiments are conducted on the $l_2$ bounded adversarial perturbations, more types of adversarial perturbations should be included, especially the $l_\infty$ bounded ones which is popular for benchmarking. In addition, for CIFAR10, the adversarial budget $\epsilon = 0.5$ is very small when considering the dimensionality of the input image. Experiments based on larger adversarial budgets should be included, e.g. $\epsilon = 2$ for CIFAR10.

3. Similar to the first point, the experiments does not demonstrate the advantages of the proposed method. In addition to adversarial training, is the method general and compatible to other popular robust learning method, such as TRADES? Is the observation the same in this context?

4. It would be better if the authors can provide some intuition or explanations for the observations in this paper.

Limitations:
The limitations and the broader societal impacts are not adequately discussed in the current manuscripts, although ethnicity should not be an issue for general research like this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors proposed the use of Subset Adversarial Training (SAT), a technique that splits the training data into A and B and constructs AEs only for data in A. Using SAT, they demonstrate how adversarial robustness transfers between classes, examples, and tasks. The authors report several insights: 1) that they've observed robustness transfers by difficulty and to classes in B 2) hard examples to provide better robustness transfer, and 3) Generating AEs on part of the data (e.g, 50%) is enough to get the standard AT accuracy.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is relatively easy to follow
2. Existing empirical results seem sound

Weaknesses:
1. If I understand correctly, the experiments were done only on L2, even though the most common AT is done using L_inf. Can the authors present results using L_inf? 
2. I'm missing many details about the AT process, you need to be much more specific for reproduction purposes. which AT is the baseline? did you try other methods? which method did you use? Madry's/TRADES/Other? the paper needs to be much clearer. Many important implementation details are missing.
3. It's hard to validate the results without supplying code/models.
4. The novelty is marginal, due to the fact that much prior art exists on the transferability of AEs, revisiting hard examples, or pruning a part of the training examples throughout the training. The paper will benefit from a comparison of these methods to SAT, so we can see the differences in performance/resources requirements/etc.

Limitations:
No discussion on limitations, I suggest the authors to add one.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work considers the transferability of adversarial robustness for partially adversarially trained models. The authors examine 3 variants of subset adversarial training (SAT): Class SAT, where only samples from selected, difficult classes are adversarially perturbed in training; Example SAT, where only examples with the highest predictive entropy are perturbed; Source-task SAT, where SAT-trained, robust models are fine-tuned on downstream training sets and evaluated for downstream adversarial robustness. They further draw connections between SAT and loss balancing, thus proposing a method for sample-efficient, low-cost adversarial robustness transfer between datasets in foundational settings.  
  
The authors report interesting insights from various experiments. From CSAT, it is noted that difficult classes transfer best; class-wise transfer gains are asymmetric; and robustness transfers between seemingly unrelated classes. From ESAT, the authors concur with previous findings that harder examples contribute more to training robust models; the gain in robust accuracy is more rapid than CSAT with respect to the size of subset A; hardness rankings suffer from a possible lack of sample diversity and its performance is matched by random rankings. From S-SAT, they find that SAT on the source dataset with only 30% of AEs can match the robustness transfer gains using normal adversarial training, on the downstream dataset; both clean and robust downstream accuracies are transferred and they are positively correlated under appropriate loss balancing.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. **Data efficiency.** The proposed SAT greatly reduces the amount of data required for adversarial training, which is promising for resource limited or real-world settings. ESAT with only 50% of AEs matches normal AT performance; S-SAT with 30% of AEs matches AT (on the source dataset) as well.   
2. **Loss balancing.** I appreciate the discussion on connections between the SAT formulation and loss balancing. I also recognise that both clean and robust accuracy transfer positively  from source to downstream tasks, under S-SAT with appropriate loss balancing, which is rare and difficult for adversarial training.  
3. **Experimentation.** The experiments are relatively thorough (except that only ResNet-18 and ResNet-50 are SAT-trained) and many details (such as the inter-class robustness transfer statistics for CSAT, or the difficulty rankings of classes) are provided, which give rise to valuable insights.  
4. **Presentation.** The presentation of this work is exemplary. It is well-organised, logically-coherent and persuasive.

Weaknesses:
### 1 Cost and efficiency  
1.1 $\hspace{5pt}$ SAT relies on meticulous pre-processing to discover hard classes and requires access to the per-epoch model weight snapshots of a normally trained classifier, to compute the difficulty metric of Equation 3. This shifts the partial cost of adversarial training to the pre-processing stage and can be costly for large models / datasets in foundational settings.   
1.2 $\hspace{5pt}$ More importantly, SAT relies on this non-robust classifier, presumably with identical architecture and training data as the target model for subset adversarial training. This means that all the pre-training and loss balancing procedures have to be repeated for every single new model-dataset combination, which might end up being more costly than normal adversarial training.    
1.3 $\hspace{5pt}$ One also notes that hardness ranking is important for the guaranteed performance of SAT, especially for CSAT and to a lesser extent for ESAT (where the authors acknowledge that it is ""possible to accidentally select poor performing subsets"", as per the easy rankings experiment).    
### 2 Experimentation
2.1 $\hspace{5pt}$ SAT is only verified for ResNet-18 and ResNet-50, which is a non-negligible shortcoming, for the reasons described above.   
2.2 $\hspace{5pt}$ Does SAT hold for other convolutional and non-convolutional architectures of variable capacity?   
2.3 $\hspace{5pt}$ Is it not cost and time prohibitive to run SAT for more than 2 baseline models? Would this also be a barrier that impedes the practical adoption of SAT?  
### 3 Scaling up
3.1 $\hspace{5pt}$ SAT experiments are notably performed on smaller datasets with fewer (or a subset of) classes. The computational complexity of SAT seems to scale non-negligibly with the number of classes (CSAT) and the size of the dataset (ESAT), which is not ideal on real-world datasets with fine-grained labels in foundational settings.   
3.2 $\hspace{5pt}$ As aforementioned, even considering the S-SAT setup (where one does not need to do SAT on every downstream dataset), it is very costly to add a new model to the SAT experiments because of the method's dependence on snapshots of a normally-trained, non-robust version of the same model, for hardness rankings and loss balancing.

Limitations:
The authors have adequately addressed the societal and ethical limitations of their work. This work strives to improve the foundational adversarial robustness of AI systems in practice; experimental and implementation details have been documented.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates the transferability of adversarial robustness among different classes and different examples. Different from previous studies, authors split the training dataset into two groups and only apply adversarial training on one group while another one using clean training. Based on experiment results, authors obtain several interesting observations, including classes without adversarial training can still have some capacity to defense against adversarial attacks, hard classes and examples can provide better robustness transferability than easier ones, and only 50% of training data is sufficient to recover the performance with vanilla adversarial training method in terms of robustness, etc.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper explores the transferability of adversarial robustness from a new perspective and hence proposes a novel training mechanism to study.
2. This paper conducts a series of experiments to investigate the robustness transferability. Based on experiments, some interesting observations are obtained, which may give some new insights for future works.

Weaknesses:
1. The motivation of this work is not quite clear. Although authors find that some classes still can obtain capacity to defense against adversarial attacks without adversarial training, data samples of these classes are available in the training dataset. Hence, applying adversarial training on all data samples of all classes directly can achieve much better robustness, comparing with the transferred robustness obtained in this paper. Hence, it's not clear why authors study this kind of transferability when all training data are available and which scenarios are suitable for the problem studied in this paper.
2. Based on experimental results, authors claim that utilizing only half training data can achieve comparable robustness performance with vanilla adversarial training methods. However, related experiments only report robustness of different methods. Considering the trade-off between clean accuracy and robustness in adversarially trained models, it would be better if corresponding clean accuracy of each method can also be provided.

Limitations:
Questions about how to apply the proposed training mechanism and observations obtained from experiments in real applications to boost the robustness of models need to be discussed in detail.

Rating:
5

Confidence:
3

";0
NbkjMn7X8H;"REVIEW 
Summary:
The paper studies robust nonparametric regression under poisoning attacks. The input is samples from some fixed distribution and the goal is to approximate some unknown function on the input space. It is assumed that the values observed in q of the samples are adversarial. In this setting, classical approaches such as k-NN estimators can fail. This is because a single adversarial sample can affect the prediction on many points. In order to avoid this issue, the paper proposes robust variants of nonparametric regression.

The first result is a bound on the convergence rate of an M-estimator based on Huber loss minimization: the initial estimator has minimal optimal ell_infinity risk. When the number of adversarial samples is not too big, the ell_2 risk is optimal.

It is also shown that the estimator can suffer when many of the adversarial samples are concentrated within a small region. In order to resolve this issue, the paper proposes a correction step that projects the estimator into the space of Lipschitz functions. Upper bounds on the rate of convergence of the corrected estimator are established. Numerical results show that both estimators outperform standard methods in a simple low-dimensional synthetic scenario.



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Nonparametric regression is a fundamental problem. The paper studies a natural adversarial setting where a small number of input samples are corrupted adversarially. This research direction can produce more robust useful data-analytic methods.

The upper and lower bounds match up to polylogarithmic factors when the number of adversarial samples is not too large.


Weaknesses:
The experimental evaluation is very limited. This is unclear whether the proposed methods can lead to significant robustness improvements in real-world scenarios.

No runtime analysis is given for the corrected estimator. This is a continuous optimization problem, so it is not clear how easy it is to solve in practice, especially in high or moderately-high dimensions.


Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
A robust version of the classic non-parametric problem is studied, when the training data is under an adversarial poisoning attack. The work is primarily theoretical, and makes assumptions on Lipschitzness and boundedness of the underlying function, boundedness of the data density, restrictions on sharpness of the data domain boundary, sub exponential noise. With some restrictions on the kernel, and appropriate parameters, upper bounds are shown on the $\ell_2$ and $\ell_\infty$ loss for the robust regression fit using Huber loss minimization. Lower bounds are obtained under the same assumptions, which match asymptotically with the upper bounds for $\ell_\infty$ and a correction is presented to match the upper bound for $\ell_2$ loss as well. Numerical simulations show usefulness of the algorithms on simulated one and two dimensional data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The authors study robust nonparametric regression under poisoning attacks for smooth underlying functions under certain assumptions on the data and domain, under which asymptotically tight rates are obtained for $\ell_\infty$ loss by suitably parameterized Huber loss.
2. A novel correction method is proposed for which tight rates are obtained for $\ell_2$ loss as well. Together with above, the optimal algorithms under the given assumptions are obtained for both losses.
3. Numerical simulations are performed to verify the theoretical results.

Weaknesses:
1. The assumption needed for proving the theoretical result, specifically assumption 1(b) is too strong. Effectively, it reduces the possible test distributions to ""near-uniform"" distributions since there is both an upper and a lower bound on the probability density function. This assumption is too strong to make the results useful in most practical situations.
2. No insights is provided into the proofs of the theoretical results; in particular there are no proof sketches in the main body. This makes it challenging to understand the contributions, given the work is primarily theoretical.
3. Experiments only involve very specific numerical simulations. How does the approach work on real regression datasets? The effect of changing the hyperparameters is not studied and how they are selected is not described.
4. Huber loss is already known as a technique for robust nonparametric regression [1] and is not novel, the paper should mention that only the analysis under the present assumptions is new. The novel correction method is computationally intractable.

[1] Maronna, R. A., Martin, R. D., Yohai, V. J., & Salibián-Barrera, M. (2019). Robust statistics: theory and methods (with R). John Wiley & Sons.

Limitations:
The authors note several limitations of their work in a dedicated section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose two new nonparametric regression methods that and study their resilience under poisoning attacks.  The baseline for comparing the performance of the proposed methods is kernel regression (aka Nadaraya-Watson estimator).  The proposed methods are two, since the initial idea that the authors have, turns out to be vulnerable to poisoning attacks that are concentrated in small regions and a large budget is spent there.  Hence, the authors arrive at a `corrected' estimator which behaves better and is closer to optimal behavior.


After rebuttal:

For the largest part I believe that the authors have provided sufficient clarifications to various issues that were raised and moreover are willing to integrate comments and clarifications that came up during the discussion period in the final version of the manuscript. Therefore, I am increasing my score from reject to borderline accept; I am also increasing the soundness from fair to good as well as the presentation from poor to fair.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ New methods for regression that are resilient to poisoning attacks.
+ Both theoretical and practical results.

Weaknesses:
+ Missing a section with preliminaries or background knowledge, where notions and notation that the authors use in the paper is well-defined. For example, in the section for preliminaries you can give information such as:
   - define loss functions, 
   - discuss kernel regression and separate the presentation from your initial estimator,
   - discuss notions that you use when it is unclear what these things are; e.g., ""bandwidth"".
   - define functions that you use without explanations at the moment; e.g., in (18) we see Clip and med; what are the arguments and what do these functions do?

+ References not in alphabetical order.

+ In line 52-54, you indicate that your approach is similar to a combination of an $\ell_1$ and $\ell_2$ loss functions. At that point in the text, I was expecting a comparison with ""elastic nets""; you may want to consider adding a small comment, or consider rephrasing and avoid such expectations from the readers.

+ The order by which some things are presented should probably change. For example, in lines 115-124, I think you need to give a slightly better explanation so that the reader can get better intuition, and moreover, this discussion should come up before you lay out the equations of the method that you propose.

+ You cannot start sentences with as in lines 167 or 168. You could add a word like ""Part"" or ""Parts"" in the beginning and make it read more naturally even if it takes a bit more space.

+ Larger font size in Figure 1 is expected.

+ It would be nice to see some commentary on the bounds and argue how they compare against each other and potentially compared to other methods that you cite in the literature. 

+ In general, you have no comparison with parametric methods, either in theory, or in the experiments. Alternatively, provide an explanation as to why you don't have such comparisons.

+ It is unfortunate and normally it should not be an issue, but the authors need help on writing a paper that has fewer spelling or expression mistakes. The additional problem here is that the authors also have a big appendix, which I suspect is written similarly along the main text. So, fixing the main text is not enough. The paper needs to be proofread by someone in its entirety, including the appendix of the authors.

I think that you have a very interesting story to tell, but the paper needs restructuring and better presentation of what you have accomplished.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies nonparametric regression, where an adversary can corrupt $q$ samples from the training set. The paper proposes robust estimators for this problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

The paper studied the Huber loss minimization approach under adversarial noise, giving theoretical upper bounds and lower bounds. 



Weaknesses:
The organization could be improved, e.g. the last two paragraphs of section 3 could conceivably be placed elsewhere. 

Some aspects of this paper are not really my area, so I cannot provide many helpful comments. 

Limitations:
The author addressed limitations. 

Rating:
6

Confidence:
2

";0
Bto5a6w06l;"REVIEW 
Summary:
This paper proposes a comprehensive stable diffusion compression pipeline, including architecture compression and knowledge distillation. The authors apply this technique to general text-to-image generation and subject-driven image generation tasks, showing competitive performance as compared with the original stable diffusion while reducing parameter numbers and training dataset size.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. The approach proposed in this paper is straightforward.
2. The writing of this paper is easy to understand.

Weaknesses:
1. The most important issue is that this paper is just a marginal improvement based on stable diffusion, so it cannot be generalized to other text-to-image synthesis frameworks, which severally narrows its application scope.
2. Reducing the SD parameter number from 1B to 0.76B or 0.66B is not so significant that this approach may not bring leaping progress in industrial applications.
3. The output and feature distillation are just classical distillation strategies that are widely adopted in other domains, e.g., image classification or GAN-based image generation. So here is a lack of innovation.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies model compression for stable diffusion models (1.4 specifically). Specifically, they study 1. removing entire layers from the diffusion U-Net, 2. fine-tuning from a small subset of LAION to recover the loss, and 3. knowledge distillation from intermediate feature layers. They show that they can get a significant (~30-40%) reduction in both parameter count and inference latency at a modest output quality degradation, at a modest training cost (60-300 hours on a single GPU) that individuals can have access to.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper works on a timely topic, especially given that Stable Diffusion has widespread use in the open source community, and reducing the inference cost will not only have a broad impact on usability for these models, but also the environment. The paper is easy to understand, and seems to be first to evaluate structured pruning (broadly) on Stable Diffusion to the best of my knowledge. The analysis includes several different metrics and ablation studies that are useful to the community.

Weaknesses:
I found it hard to intuitively understand why the layers were specifically chosen to remove. One of the interesting results the authors show in this paper is that removing the mid-layer without any additional fine tuning already results in decent quality (competitive with some of the smaller models that are fine-tuned). Then, a natural question that might come up then is if we were to remove every layer and run an analysis of how much cost (in quality loss) the removal of each layer ends up with, would we then be able to confirm that the layers that were removed are in fact the best layers to remove in terms of their quality cost? This would be much cheaper of an experiment to run than fine-tuning the entire removed architecture, and would be very helpful to know if the ""unsupervised"" structured pruning is a good indicator for the end performance after fine-tuning. 

It would at least be interesting if there could be an analysis of how much the fine-tuning (and KD) adds (in terms of quality) to an 'unsupervised' baseline, whether that means running an ablation specifically with the mid results removed, or adding an 'unsupervised' baseline with all layers removed (the orange and red in Figure 3). 

This is very much concurrent work (and does not affect my score), but it could be worth adding https://arxiv.org/abs/2305.10924 to the citations. 

Limitations:
The limitations are adequately addressed. It might also be interesting if the authors can comment whether model compression itself could have risks in society, for example if people prefer worse but slightly faster models over better models, then that could potentially exacerbate the bias that these models have (and of the models people use in practice). 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents an technique to compress the network structure of a diffusion model. Specifically, the method combines a static block pruning strategy and a knowledge distillation retraining to learn compact diffusion model. Experiments on text to image generation and customization shows the effectiveness of the approach. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper shows that conventional compression techniques leveraged in CNNs/GANs can also be used for diffusion model.
2. Some experiments are conducted to show the effectiveness of the method.

Weaknesses:
1. The paper is not novel at all. Pruning and distillation has long been used for vision network compression since the era of CNNs and later GANs. Nothing surprising comes out from this paper, where the author just simply combine two techniques and makes it work. Block pruning and output + feature distillation is developed far long ago and naively combines them show little academic value.
2. The paper lacks insight significantly. There's no theoretical proof nor ablation study to judge the design choice. For example, why shall we remove the second pairs of R-A instead of the first pair of A-R in down blocks? Why shall we retain the third R-A pair in the up blocks? Why can't we do selective removal where some blocks only remove R and some blocks only remove A? The pruning design choice is totally ad-hoc where nothing systematic and insightful comes out. Due to this, the method seems to only be applicable on SD-v1.4. I see no way that how this method can be applied to SD-v2.1 / SDXL or any other new SDs where we simply add one/two more modules of R/A into a block.   
3. The use of distillation-based pre-training sounds weird. Training/fine-tuning with distillation to recover the performance is a more appropriate wording.
4. In Sec 4.1, the paper says that the BK-SDM is only fine-tuned on 0.22M image-text pairs. Why only fine-tuned on such smaller scale of dataset? How about fine-tuning on 2.2M or 22M? Would the quality get improved? How about 22k or even fewer? Will the quality stay the same? No judgement is presented here as well.
5. In Figure 5, the paper should compare more diffusion method instead of showing the first 3 column of GAN-based approach. Since the speed of the network falls into the scale of diffusion model, it does not make much sense to compare GANs.

Limitations:
Yes, the author addressed both. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
This submission proposes a light-weighted network for stable-diffusion-based text-to-image generation.
The proposed network is named BK-SDM, which became light-weighted thanks to removing unnecessary layers
and knowledge distillation.
An interesting finding is that removal of the entire mid-stage (the most low-resolution part of the u-net) does not seriously harm
generation quality.
As a result, 30% computational cost reduction was achieved while retaining 97% scores of generated image quality.


Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- Light-weight diffusion models are, despite their importance,  not explored. I appreciate pioneering this direction.
- Training using a small subset (0.22M image-text pairs from LAION) is a nice point that is easier to reproduce, while keeping the generation quality.

Weaknesses:
-  Methodological novelty is limited because layer removing and knowledge distillation are well-used technics, while they were not done in diffusion. I did not find special challenges in applying them to diffusion models.

- Trades-off between speed and quality are not aggressively investigated; the only presented model is the  30% faster / 97% worse version. For example, what happens when the model is 90% faster? Is it possible to achieve any speed-up with 100% quality? Perhaps too aggressive speed-up results in the uselessly worse generation quality, but trying and clarifying it would be a useful contribution for a paper.

Limitations:
Limitations and potential social impacts are properly discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work explores traditional model compression for diffusion models, aiming to mitigate their considerable computational overhead. By employing block removal and knowledge distillation, the authors are able to reduce parameters by over 30% with only minimal performance loss with the 0.22M LAION dataset. The paper further showcases its applicability for personalized generation tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed hand-crafted compression strategy, that leverages block-removing knowledge distillation, 
simple, but effective.  
2.  This work demonstrates the effectiveness of their proposed method on different model sizes, different tasks, including text-image generation and personalized text-image generation. 
The paper provides robust evidence of the approach's versatility across varying model sizes and tasks, which include both text-image and personalized text-image generation.


Weaknesses:
1. The main concern lies in the hand-crafted block removal process from the original DDPM. It may be more reasonable to calculate an importance score for each block and remove them accordingly. The simplicity of the current approach, while appreciated, seems to compromise on the degree of novelty. Please further clarify this.

2. A comparison of different sampling steps is missing. Will the model compression influence the trade-off between fidelity and sampling efficiency? 


3. The proposed methods seem to be general for the diffusion model.  Could the authors elaborate on the reason not to include experiments on general image generation?

Limitations:
Yes

Rating:
5

Confidence:
4

";0
KexMPvrFgJ;"REVIEW 
Summary:
This paper explained and analysed the issue of unsynchronous RGB and depth measurement under the UAV city modelling scenario, which makes it challenge to incorporate depth supervision for NeRF optimisation under such scenerio. The authors proposed a novel solution to it by modelling the continuous RGB camera trajectory as an implicit time-pose function. Under the prior knowledge that both RGB and depth are generated on the same trajectory, poses of depth images can be queried from the time-pose function. Based on this time-pose function, the authors further proposed a 3-stage optimisation pipeline to train the NeRF model with depth supervision. Qualitative and quantitative results show the proposed method achieves better results compared with previous methods that use RGB inputs only.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. This paper studied a very interesting and important problem of synchronising RGB and depth measurements, especially under large scale UAV scenerios, which posed challenges to incorporating depth supervision for NeRF optimisation under such scenerios. 

2. The authors proposed a novel solution by modelling the continuous trajectory as a time-pose function, and designed a 3-stage optimisation pipeline to leverage the synchronised RGB and depth meaurement for NeRF training.

3. Experimental results showed the effectiveness of proposed methods compared to previous RGB-only methods. Ablation studies also show  naively using unsynchronised depth image could hinder the performance. 

4. The paper is very well written and easy to follow. The storyline and motivation are very clear.

Weaknesses:
1. Although the experimental results in Tab. 1 and 5 showed the proposed method achieves better results than Mega-NeRF and NeRF-W, it's not compeltely fair as the baseline methods only take in RGB images. The authors need to show that the problem could not be easily solved by trivial efforts such as jointly optimising the poses of depth images. For example, in tab. 5 the author showed the results of Mega-NeRF with depth but the joint optimisation was switched off. Also in line 104, the authors mentioned that soft synchronization cannot fully address the misalignment issue, but it would be good to also show how good can all the methods perform with this simple alignment. 

2. Most of the experiments and results are shown in synthetic dataset. Only Fig. 1 and the supplementary video showed results from real-world scenes. It would be good to show more results on real world sequences.

3. It would be good to show some run-time comparison and analysis.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a way to train NeRF with asynchronous RGBD videos. Specifically, three technical contributions have been made:
1. New problem formulation for NeRF training from async RGBD video.
2. Propose a time-pose function to use async RGB and Depth stream, resulting in better pose estimation and NeRF training.
3. Propose a new synthetic dataset for this task.

---
**After rebuttal**: I have read authors' rebuttal and it addresses my concerns.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow.
2. It is novel to formulate the pose estimation issue of an RGBD stream in a time-pose function, which constrains the challenging issue better. Since ground truth poses for RGB stream is known, this paper essentially proposes a novel way to use an async depth stream to 
    1. improve NeRF quality;
    2. refine RGB poses; 
    3. optimise poses for depth images from interpolated RGB poses.
3. Although the method requires poses for the RGB video, which is a strong assumption for un-posed NeRF training, the formulation is still novel and smart to me.

Weaknesses:
In real datasets, I think there should be a self-motion distortion caused by the motion of the mounted drone. I am wondering how would this LiDAR depth distortion affect the performance?

Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose a method to reconstruct aerial scenes with Neural Radiance Fields that are supervised with RGB images and asynchronous depth images. To address the asynchronicity they propose a novel time-pose function that provides a prior to optimise the poses of the depth images. To validate their method a new synthetic dataset is introduced. They outperform one relevant baseline and show that asynchronous depth captures are an issue worth considering in NeRF reconstructions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* S1. Relevancy. Depth supervision is an important method to improve 3D NeRF reconstruction and asynchronous depth has not been considered before. The Problem formulation is novel and interesting with significance in the field of UAV reconstruction.
* S2. Method Idea. The proposed time-pose function is an original approach to exploit the prior that depth and rgb captures lie on the same trajectory and it is shown that jointly optimising the NeRF reconstruction and time-pose function can significantly improve RGB and Depth reconstruction.
* S3. Presentation. The paper is written in a cohesive manner with a clear structure.

Weaknesses:
* W1. Missing baselines for pose optimisation. The authors ablate design decisions for their proposed time-pose function but do not provide baselines for pose optimisation, which is, in this reviewers opinion, their main contribution. A qualitative comparison to methods like BARF (https://github.com/chenhsuanlin/bundle-adjusting-NeRF) or other state-of-the-art methods (https://nope-nerf.active.vision/, https://prunetruong.com/sparf.github.io/) would support the decision to use the time-pose function.
* W2. Questionable assumptions for poses. As far as this reviewer understands the poses for the depth are assumed unknown and only initialised by the time-pose function that was trained on the rgb capture timesteps. This reviewer questions this assumption in general, as in both the synthetic and real-world setting poses for the depth can either be obtained from the simulation or from GPS measurements. A quantitative comparison between GT/GPS poses, BARF optimised poses and the time-pose function would strongly support the authors decision to use an implicit function to represent the drones trajectory. 
* W3. Missing Implementation details for reproducibility. Some important implementation details are missing from both the paper and supplementary, to be precise:
   * The number of images & depth images in the generated and real datasets.
   * Is the time-pose function using positional encoding / fourier features ? This is not clear. 
   * No training parameters are given for the proposed method and baselines. 
* W4. Missing Discussion of related work. Related work in the field of representing signals with MLPs is not discussed (e.g. SIREN) and not considered in the design of the time-pose function.

Limitations:
The authors do not discuss limitations of their method in the paper but include a very small discussion in the supplementary. Overall limitations cannot really be discussed since there is no objective baseline to compare the method to. For broader impact only potential military uses are mentioned, whereas the method also has implications for surveillance. Environmental impacts of large-scale neural network training are also not mentioned.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to include depth supervision in NeRF in the UAV city modeling scenario. The key problem is that the images and depth maps are asynchronous. This paper exploits a prior that RGB-D frames are sampled from the same physical trajectory. It fits a time-pose function to the available RGB cameras and computes depth map cameras by this function and a pre-calibrated pose transform between sensors. Then it trains a NeRF with RGB loss and optimizes it further with RGB-D supervision. A new synthetic dataset is proposed for evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper tackles a new problem in NeRF modeling on UAV datasets. it defines a scenario where RGB and depth frames are asynchronous. 

2. Instead of directly adding new parameters to predict depth camera poses together with the NeRF optimization, it identifies a prior of the relation of RGB poses and depth poses.

3. It designs a network with 1D hash encoding to fit a time-pose function for RGB cameras.

4. It generates a new synthetic dataset to evaluate the proposed method.

5. It designs a new speed loss in pose learning.

Weaknesses:
1. A key contribution of this method is the learned time-pose function. It actually can be seen as an interpolation function to interpolate the RGB poses on depth timestamps. Since camera interpolation is a common practice in 3D software such as Blender, I am curious about whether other simple interpolation methods such as linear interpolation or the interpolation in Blender could get worse or better accuracy.

2. As stated above, on easier datasets, the simple interpolation methods may still get good results. It will be better to construct more challenging datasets to demonstrate the effectiveness of the proposed time-pose function. This situation can be considered when generating synthetic datasets.

3. Directly adding extra parameters to estimate depth poses are also an option as stated in the paper. What is the accuracy like if we directly use this method? It would be not surprising if it gets worse results because the poses are not well initialized.

4. The prior that ""RGB-D frames are actually sampled from the same physical trajectory"" can be explained more in the main paper such as in the Introduction section. This can make us understand clearly that there is a fixed and known pose transformation between the depth and image sensors so we can estimate the depth poses by interpolating image poses. 

Limitations:
The authors addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a pipeline to build depth-supervised neural radiance fields (NeRF) using asynchronous RGB-D sequences. Since the task is novel, the paper also contributes a synthetic dataset and demonstrates that it outperforms certain baselines in the experiments.


Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
## Originality
The task is novel and practical for lots of real-world settings.


Weaknesses:
## Quality
- The most obvious baseline is missing. What if authors simply initialize the depth frames with their (misaligned) poses from the sensor and perform BARF [1] style joint optimization on the depth frames’ poses (note that depth frames can have different poses from the RGB images after optimization)?


- Since the above baseline is missing, it’s hard to understand why the time-pose network is necessary.

- In my opinion, the baselines authors compared against are in the wrong direction. NeRF-W and Mega-NeRF are not solving the problem of inaccurate camera poses. Authors should look into works that try to solve pose estimation and neural radiance fields simultaneously such as BARF, NeRF–, and iNeRF [1, 2, 3].

- Line 156-157’s motivation for using Quaternion since other rotation representations are not continuous is weird. Quaternion is also not continuous and is known to not be the best representation for rotation regression. See this paper [4]


### References
- [1] BARF: Bundle-Adjusting Neural Radiance Fields, Lin et al.
- [2] NeRF--: Neural Radiance Fields Without Known Camera Parameters, Wang et al.
- [3] INeRF: Inverting Neural Radiance Fields for Pose Estimation, Yen-Chen et al.
- [4] On the Continuity of Rotation Representations in Neural Networks, Zhou et al.


Limitations:
Yes

Rating:
3

Confidence:
4

";0
1NY5i5fq5e;"REVIEW 
Summary:
The authors analyze the performance, efficiency, and robustness of free-space optical dot-product engines for Transformer accelerations. Measurement results on an SLM-based optical system are demonstrated on some layers in a GPT-like model. System performance/efficiency are estimated and compared to digital computers. Scaling of optical processors are discussed to show the scalability of optical computing platforms.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
1. Experimental results on SLM-based free-space optical system has been demonstrated for matrix multiplication.
2. Scalability with future technologies are discussed to show the benefit of optical computing in the future. 

Weaknesses:
1.	The novelty of the paper raises some concerns as no new hardware design or algorithm innovations have been shown. The SLM-based system and its experimental demonstration are not new. No customized hardware is shown for Transformer. The claimed optical hardware is designed for CNNs/MLPs. On the algorithm part, device quantization, the LUT-based training method, noise analysis, and 4-pass multiplication are standard methods for analog computing. NeurIPS community usually requires certain machine learning contributions. What is the main ML contribution? Probably other venues in the optics community are more suitable for this paper.
2. The demonstrated system is weight-in-place which needs a large number of parallel MVM to amortize the weight programming/encoding cost. However, the dynamic attention operations in Transformer and fully-connected layers usually have low arithmetic intensity, especially GPT-like architecture with KV cache, which cannot provide enough batch dimension to amortize such cost for weight-in-place systems. More justification for the usage of the weight-in-place system needs further discussion. A weight streaming system might be the suitable architecture for Transformer.
3.	In Fig. 2, only a small part of layers in the Transformer block are implemented by optics, while other operations are on all digital computers. In this hybrid case, how large are the efficiency/performance benefits, or is it worthwhile to use optics?
4.	For the noise analysis, only shot noise is emphasized, which is much smaller compared to other variations in the system both on the electrical and optics sides. A simple Gaussian added to the output results might be oversimplified as system error modeling. 
5.	In Line 291, the system assumes a 10 GHz light modulator array. If I understand correctly, the spatial light modulator typically has high resolution but very low switching frequency. This 10 GHz modulation speed needs further justification. How fast is the switching freq for weights and input feature maps? The modulation energy cost is based on thin-film lithium niobate modulators, which are fairly large. How many such large modulators are required to modulate a million pixels?
6.	Also, the light source/TIA/ADC power consumption in the camera will be very large if working at such a high frequency. The incoming data fetched from memory will also be a bottleneck, which might not be able to fast enough to feed the 10 GHz optical core. In Line 297, the memory part is completely ignored when compared with digital computers, which might not be a fair comparison even in the near future. Only multiplications are done in optics, the partial product summation is done digitally, especially when it requires 4-pass, which raises concerns about the benefit of this SLM system for Transformer acceleration. More discussion on the system performance/efficiency is recommended.
7.	The paper title optical Transformer is very broad, however the current paper only focuses on SLM-based free-space optics, which suffer from bulky optical setups (low compactness) and noise/alignment/sensitivity issues in practical deployment. More discussion and comparison on other integrated photonic/diffractive hardware is important and necessary. Otherwise, the scope/title of the paper is more suitable to be narrowed down to an SLM-based Transformer acceleration platform.


Limitations:
Yes.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper propose a photonic hardware accelerator to process the inferences of large language models, i.e., transformers, using optical multiply-accumulate (MAC) operations. Optical MACs are suitable for computations with large operands, thereby leading to asymptotic energy advantages over the digital hardware accelerators.




Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The paper is well-organized.
2. The paper works on an important problem.

Weaknesses:
1. The paper does NOT consider the energy consumption of analog-to-digital converters, digital-to-analog converters, and various memories such as on-chip SRAM and off-chip DRAM. I totally agree with the cornstone of this paper, which is optical MACs or matrix multiplications are super energy-efficient. However, gaining this energy advantage is not easy. Reading 530B parameters of a transformer, converting these many digital parameters to analog optical signials, and converting the analog optical result signials back to digital values may donimate the energy consumption of an inference. As a result the energy efficiency improvement may not be very large.

Please check the comparison in this paper:
W. Liu, W. Liu, Y. Ye, Q. Lou, Y. Xie and L. Jiang, ""HolyLight: A Nanophotonic Accelerator for Deep Learning in Data Centers,"" 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), Florence, Italy, 2019, pp. 1483-1488, doi: 10.23919/DATE.2019.8715195.

Limitations:
No potential negative societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper explores the feasibility and benefits of employing optical computing techniques for machine learning and specifically focusing on large language models (LLMs).  The paper builds upon earlier work on optical neural networks, primarily [61] (Wang et al., Nature, 2022), which experimentally demonstrated the feasibility of performing dot products optically in a two layer neural network applied to MNIST while achieving around 90% accuracy at about one photon per multiplication optical energy.  As LLM computations involved a large and rapidly growing number of multiply accumulate operations, the objective of the paper is to explore whether optical techniques can yield benefits over existing CMOS-based accelerators (GPUs, TPUs).   The paper tackles this by employing a simulation based methodology where the simulator attempts to model the various noise sources (systemic, shot noise) along with limited precision of a potential electro-optical system.  The evaluation shows that as LLMs continue to scale such optical systems may potentially yield many order of magnitude benefits in terms of energy efficiency over current approaches.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Makes a reasonably strong case for further exploration of optical computing for large language models.

Weaknesses:
Not enough discussion of the remaining challenges that need to be overcome to make such systems competitive in reality.

Somewhat limited contributions in-so-far as earlier works have already explored using optical for ML.  

Unclear how accurate the simulation methodology is.

Some aspects unclear.

Limitations:
There is some discussion of limitations.  If the paper is meant to rally others to work on optical techniques for machine learning it would be helpful if the authors could more systematically highlight the remaining challenges to making such systems practical.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors perform experimental analysis with a spatial light modulator to optically perform the computations of the linear components of the Transformer architecture. These measurements allow them to create a noise-model that is then used to simulate a GPT-2 like model and measure performance (validation perplexity) in function of model parameters (system noise, not including optical shot noise). These optical systems are physically constrained by ""optical shot noise"" that dictates the minimum number of photons to achieve a target precision. This optical shot noise scales favorably with larger models, and the authors then extrapolate their observations to existing large Transformer models (like PaLM), and further to even larger hypothetical future models, showing a substantial advantage of a large scale hypothetical optical system over existing electrical systems (the energy advantage scaling with the width of the model).

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. While there have been previous works that examine smaller scale optical neural networks and the optical shot noise scaling behavior seems to be well established (I'm inferring this from the works cited in the submission, I'm not familiar with the area), the authors highlight the importance of the scaling behavior in context of the Transformar architectures that are typically used with large language models. I think the combination of innovative research in the field of optical computation with the computation requirements of applied machine learning models at the largest scales is particularly interesting.

2. The authors used small scale experiments to establish realistic system properties and then simulated an entire Transformer to check how the physical properties would influence overall (validation) performance of the model. They did this at a scale (GPT-2 like quantized model with 15M-416M parameters) that seems large enough to gather realistic predictions.

3. The interpolation of the simulated data to much larger models is necessarily based on many assumptions, both in scaling from the simulation to larger models, and with respect to the hypothetical hardware that would run very large models on optical hardware. The appendix goes into some detail on the different assumptions that led to the conclusions summarized in the main part of the article.

4. The overall presentation of the work seems adequate for a public that is knowledgeable in machine learning, but probably has much less experience with physical properties of optical computational hardware, which is no easy task given the relatively large gap between these domains.


Weaknesses:
1. All the discussion of the optical vs. electrical implementation of the computation is in the lens of energy consumption. After reading the paper I'm not sure how the proposed architecture would fare with respect to latency/speed, or other constraints (e.g. if there is a theoretical limitation to the size – or cost – of different components that scales very differently between the currently used IC technology of purely digital microchips vs. optical components). Latency is mentioned in Appendix G, but there is no mention of these other dimensions in the main text.

2. The authors highlight the scaling behavior of optical shot noise, but other sources of noise (called ""systematic errors"" in the paper) are simply measured for the system at hand, and no information is given how these other sources of noise would scale when the system is scaled. For example, I assume that any realistic scaling would require miniaturization of the optical components, and I would assume that this miniaturization also causes the systematic errors to change in relative magnitude. It would be interesting to at least briefly discuss the scaling behavior of these other sources of noise.


Limitations:
I don't see a negative societal impact of the presented work, other than making Transformers more energy efficient would allow to scale them further than otherwise possible, and accordingly accentuate any potential opportunities and risks of models such as LLMs. I don't think it's required to point these out explicitly in the paper.

There seem to be a number of technical limitations with respect to the predictions in the sense that there is a lot of uncertainty whether the hardware to run optical neural networks could be scaled up as much as state of the art digital circuits. But the authors make it clear already in the abstract that their main interest is in establishing a scaling law and not concrete predictions about the future of the implementation of large scale optical neural networks.


Rating:
8

Confidence:
3

";0
RrdBNXBUIF;"REVIEW 
Summary:
This paper studies relational knowledge distillation (RKD) on semi-supervised learning setting. This is different from the previous theoretical results on knowledge distillation, which mostly focus on feature matching (not relational). They introduce a new notion called low clustering error, which quantifies the difference betweeen the predicted and ground truth clusterings, and show that RKD can provably produce low clustering error. They also have a few other results, including the label efficiency theorm on a cluster-aware semi-supervised learning framework with low clustering errors, and unifying data augmentation consistency regularization with RKD, showing that consistency reuglarization focuses on the local perspective, while RKD improves over the global perspective. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: Since we already have seen a few papers on connecting spectral clustering with contrastive learning or other learning methods, the spectral clustering part of this paper is not particularly surprising. However, I do like the relational part, which focuses on the relationships between the features, rather than the feature matching itself. I think this is a very interesting and important direction to pursue. Moreover, the authors also investigate the benefit of clustering awareness in the semi-supervised learning scenario, which is novel. 

Quality: I think this is a well written paper with rigorous definitions, theorems and proofs. 

Clarity: Since this is a theory paper, I will not say it is straightforward to read. However, I think the authors did a very good job in the introduction to demonstrate the main contributions of this paper, and present the technical theorems and assumptions in a structured way. Therefore I would say this paper is well written. 

Significance: I think this paper is indeed interesting, because it provides an interesting connection between relational knowledge distillation learning and spectral clustering, and also provide theoretical characterization on various aspects. 



Weaknesses:
It is a bit difficult for me to understand the intuition of Assumption 4.2, and the toy example after assumption 4.2 is too special, which does not explain much about the complicated definitions. It would be nice to further elaborate on the intuitions, and perhaps give a non-trivial example of this assumption, and why this assumption is a mild assumption. 

Personally I like Theorem 4.1 better, and I do not feel it is really necessary to consider the limited unlabeled samples setting (because usually we may assume the unlabeled samples are cheap). Therefore, it might improve the presentation of this paper, if the author can add more motivations on Sec 4.2. (if there is no such motivation, it's also fine, I think it is a minor spot.)

The current connections between different results are weak. Specifically, I cannot easily understand how the last DAC result connects with the previous two. I might be wrong, but I initially felt that the third result says the combination of DAC and RKD can improve the performance. However, the statement of Theorem 6.2 seems only contains DAC, not RKD. Therefore, it seems to me that the first and third results are in parallel: both of them are providing upper bounds of clustering error. Is it true? 
-- if this is true, I think the presentation of the introduction should be updated, to avoid potential misconceptions. 



Limitations:
Not applicable.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper points out that RKD over a population-induced graph given by a teacher model leads to low clustering error (Spectral Clustering), which provably benefits to semi-supervised learning and other downstream tasks. Consequently, it provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-supervised classification setting from a clustering perspective.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper mainly focuses on expounding the close relationship between RKD and Spectral Clustering from a theoretic perspective, which definitely contributes to the increase of attention on RKD. According to this seminal work, RKD can be regarded as a principle choice to extract knowledge from a pre-trained large model.

Weaknesses:
The weakness of this paper is obvious: The experimental part is too weak. There is no empirical study in the main text, and the experimental results in the appendix are limited. In addition, in some cases, RKD even deteriorates the Top-1 performance. The authors provided no explanation on these cases. Besides, CIFAR-10 and CIFAR-100 are not quite challenging datasets, and they are too similar. It is suggested conducting experiments on the large-scale datasets to better demonstrate the effectiveness of the proposed method.

Limitations:
I agree with the limitations provided in the last section. Besides, please see the Weaknesses and Questions parts for my concern. I hope the review is helpful for this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper provides a comprehensive theoretical proof of the relationship between RKD and clustering from the perspective of spectral clustering. The theoretical proof presented in the paper is detailed and reasonable. Firstly, RKD is regarded as a spectral clustering problem of the teacher model, and then the introduced clustering error is used to prove that RKD leads to low clustering error. Finally, the paper presents the sampling complexity bound of RKD in the case of limited unlabeled samples.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.The theoretical proof presented in the paper is detailed and reasonable. 
2. Label efficiency of cluster-aware SSL is demonstrated as well.


Weaknesses:
1. It appears that the author attempts to convey numerous viewpoints in this paper, resulting in a somewhat confused logical structure. As a result, the paper is somewhat difficult to read and understand, possibly due to my unfamiliarity with the writing style of purely theoretical papers.
2. While the author's arguments are generally reasonable, there seems to be a lack of strong correspondence between the presented problem and the method of reasoning. It would be beneficial to understand the author's motivations and thought process behind exploring RKD at a theoretical level, particularly in relation to demonstrating its label efficiency in the SSL setting.

Limitations:
1. The research presented in the paper holds significant theoretical value. It would be beneficial to enhance the overall clarity and organization of the paper's structure. This will improve the readability and friendliness of the paper for fellow researchers in the field.
2. In the field of machine learning, it is highly recommended to conduct necessary experiments to strengthen the scientific rigor of the study.

Rating:
6

Confidence:
3

";1
T47mUw8pW4;"REVIEW 
Summary:
This paper is an extension of a result from Chazal and Soufflet, which states that the Hausdorff distance of a set to its medial axis is lipschitz-bound under ambient deformations. The authors extend the result from C2 sets and C2 deformations to arbitrary closed sets and C1,1 diffeomorphisms which preserve a bounded sphere in the set. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Medial axis play a central role in vision, and 3D geometry and investigating may lead to novel approaches and algorithms. 

Weaknesses:
Unfortunately, I do not believe it is within my capacity to evaluate the full correctness of the theorems presented in this paper, as such I feel uncomfortable to recommend its acceptance. I am a first-time reviewer hence will reassess my thoughts given other reviewers' input, but in a sense, placing the proofs in the supplementary is a somewhat odd choice to me, as the paper is a completely theoretical paper with its crux being the proofs themselves. Additionally, I find the theorem somewhat esoteric, being both a slight extension of Chazal and Soufflet, and requiring the ambient deformation preserve a bounded sphere to only yield a lipschitz bound. Given the above, I find this paper more suiting to a computational geometry/mathematical journal than NeurIPS. 

Limitations:
Yes

Rating:
3

Confidence:
2

REVIEW 
Summary:
The medial axis of a closed set $\mathcal{S} \subset \mathbb{R}^d$ is defined to be the set of points in $\mathbb{R}^d$ which do not have a unique closest point on $\mathcal{S}.$ The authors develop a notion of stability for such sets with respect to ambient diffeomorphisms of $\mathbb{R}^d.$ The main result proves stability with respect to $C^{1,1}$ diffeomorphisms under additional assumptions about the set $\mathcal{S}$ (for instance, that $\mathcal{S}$ is bounded, See Assumption 3.8 for a full list.) This result is considered a generalization of an earlier result, which makes stronger smoothness assumptions (namely $C^2$) on both the set $\mathcal{S}$ and the class of ambient diffeomorphisms. The authors argue in the early parts of the paper that this extra generality is needed for (unspecified) applications in astrophysics.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The expository style in the early parts of the paper is inviting, where it does a good job of illustrating some basic notions, including familiar ones such as the medial axis and less-familiar ones like the generalized tangent space. 

Weaknesses:
There are a few different criticisms one can make of this paper:

  1. The topic is niche for a NeuRIPS audience.
  2. The main result is technical and difficult for non-experts to verify.
  3. The main result, as described in the introduction, is a marginal improvement over the current state of the art in reference [13], in the sense that one gains only less restrictive assumptions about the regularity of the functions and shape that appear.
  4. The connection to applications is tenuous at best, and no experiments are provided.

With regards to 1 and 2 above, let me draw attention to the statement of the main result in Theorem 3.9 and the preceding assumptions it requires, which take nearly a page to write down even with many prerequisite definitions that appear before it. One would at least hope based on the promises of the introduction that a simple definition of ""stability"" would be available for use in the statement of the theorem. With regards to 3 and 4, there is very little given to convince the reader that the $C^2$ results are insufficient for applications. 

I would not necessarily suggest that a paper with one or more of these deficiencies be excluded from NeuRIPS. However, given that all four issues are present, it seems better to focus on resolving some of them, or to send the paper to another venue (eg. a pure mathematics journal) where some of these criteria are judged to be less important.

Limitations:
n/a

Rating:
4

Confidence:
1

REVIEW 
Summary:
The authors prove that the medial axis of closed set is Hausdorff stable without any further assumption on it. In this proof, the authors achieve stability without pruning the medial axis which is a significant advantage. Meanwhile, the results hold for sets in arbitrary dimensions.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In originality, this work holds for sets in arbitrary dimentions and removes the limitation of manifold assumption when in proof, and it does not need to prune the medial axis which is a significant advantage.

The quality and clarity is good enough, it is easy to understand the motivation, outline and contribution.

The proof in this paper implies that the medial axis of an imprecise shape is stable. The medial axis plays an important role in the field of computational geometry, computer vision and graphics. 

Weaknesses:
The result of this work shows the numerical stability of medial axis, but there is little analysis about the impact from noise size and quantity in real world data.


Limitations:
From line 36 to 39, the authors give the limitation about the work, but it could be more clear if there is more explanation. I also asked the questions about it in section Question.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proves that the medial axis of closed sets is Hausdorff stable, this extends existing stability result on the stability of the medial axis of C^2 manifolds under C2 ambient diffeomorphisms. The contributions are

1. This work makes no assumptions of the set except the closedness. The stability of the medial axis of smooth manifolds has been intensively studied in the literature, this work omits the manifold assumption.

2. The stability is achieved without pruning the medial axis. Large body of works have to prune the medial axis.

3. The stability results hold for sets in arbitrary dimensions and are insensitive to the dimension of the set itself.

This theoretical result plays a fundamental role in many fields, the generalization is important  to many practical applications.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The theoretic results are much general, it doesn't require the manifold assumption, it doesn't need to prune the medial axis, the results hold for any dimensions. 

The work is clearly represented. All the key concepts are explained thoroughly, the lemmas, theorems, corollaries are explained in detail, and rigorously formulated. The proofs are step by step, clean and easy to follow. 

Weaknesses:
The theoretical results are elegant and convincing. It will be helpful to give some numerical experimental results. 

Limitations:
The current stability result assumes the diffeomorphisms is a small perturbation of the identity, and it preserves the bounding sphere. This constraint seems to be artificial and inconvenient for practical applications. Maybe this requirement can be weakened or the bounding sphere is pushed to infinity.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proves the Hausdorff stability of the medial axis of closed bounded sets. This is a mathematics paper. The authors set up a foundation of their problem, then applied Theorem 2.6 (from [19]) to complete their proof. The end result is quite beautiful in fact. The authors also show that the results in [13] is a special case of their result.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is written well. Despite not having a mathematics background, I am able to read and understand the majority part of the proof. (Nitpick: there are small typos, for example, some \pi_{S}(p_4) are annotated incorrectly in Figure 1.)
- The authors proved a difficult result (as an indication, [13] is a special case of the result). The proof seems to be correct to me.

Weaknesses:
- I have a hard time understanding how this result can be used in machine learning / computer vision / computational geometry applications. Yet the motivation is explained in ln45 - ln73. However, I still do not see how this result can be applied. For the benefit of the readers, I think applications need to be demonstrated, otherwise Neurips might not be the right audience.

Limitations:
N/A

Rating:
4

Confidence:
1

REVIEW 
Summary:
In this paper, the authors analyze the stability of the medial axis of a set S, when S is perturbed by a map that is lipschitz with lipschitz derivatives.

This stability result is of interest in numerous applications in machine learning, such as astrophysics.

The author's results improve upon an existing result by Chazal and Soufflet in a few ways:
1. The authors remove an assumption that the set S must be a piecewise smooth manifold; here they only require S to be closed and bounded.
2. They do not require pruning the medial axis.
3. Their result holds in high dimensions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I think the result is significant, and of interest to the neurips community. Compared to Chazal and Soufflet, I think another significant aspect of this result is that this result is quantitative whereas Chazal and Soufflet's result is only qualitative.

Weaknesses:
I have some questions about how this paper's results compare to existing results, as well as about several aspects of the result (see below). These may not be considered weaknesses if the authors can address them.

Limitations:
n/a

Rating:
7

Confidence:
3

";0
u6BYyPuD29;"REVIEW 
Summary:
This paper aims to ease the distribution problem from a theoretical perspective, which uses margin loss and a scoring function to describe the relationship between domains, and the generalization bound in terms of functional class complexity is subsequently analyzed. Based on their theoretical analysis, a margin-based adversarial framework, which is developed upon the classical DANN method, is further proposed. Results conducted in the Domanbed benchmark show their results are competitive among existing arts. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. It is always good to see some theoretical analysis for DG.

2. Experiments will the five datasets are appreciated.


Weaknesses:
1. The motivation in the introduction is problematic. The literature is happy to see inspiring theory, but the theory itself is not the purpose. Rather, the paper should emphasize what part of existing work requires a proper theoretical explanation. After all, how to solve domain generalization is the problem. The authors may consider revise the introduction part.

2. The idea of adversarial training for DG (either DANN [77] or MMD [79]), is shown to be less effective than ERM according to different benchmarks [32, a]. However, this work shows significantly better results than ERM, what is the advantage of the proposed MADG compared with them?

3. According to Line 282, and the implementation details in the supplementary material, I think the comparisons are unfair, as the Domainbed uses randomly selected hyperparameters (batch size, learning rate, etc.), which are fixed in their experiments.

4. More effective approaches should be compared, such as SD [b], and Miro [c], and it is suggested to reevaluate their code (at least some of them) on the same device. 

[a] OoD-Bench: Quantifying and Understanding Two Dimensions of Out-of-Distribution Generalization, in CVPR'21.

[b] Gradient Starvation: A Learning Proclivity in Neural Networks, in NeurIPS'21.

[c] MIRO: Mutual Information Regularization with Oracle, in ECCV'22. 

Limitations:
It seems domain labels must be available during training, this should be listed as a limitation as domain labels are not always available.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a new adversarial learning objective using a margin based approach for domain generalization. The goal of domain generalization broadly is to build classifiers that are trained on one or more source domains, and are expected to generalize to an unseen target domain. The key idea is to leverage Margin disparity discrepancy (MDD) which quantifies the level of disagreement between decision boundaries of classifiers using their margins. MDD is used as a proxy to understand generalizability of classifiers in this context across multiple domains. Next, the paper establishes an upper bound on the generalization error on any unseen target domain that is within the convex hull of the source domains and MDD. In other words, the objective is to get the decision boundaries across different source domains to agree as much as possible, while also minimizing the empirical errors on them simultaneously.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The margin perspective for domain generalization is a fresh perspective to this, and the paper takes an interesting approach at using an adversarial learning strategy to optimizing this problem. 
* Detailed theoretical setup and formulation, which builds on existing work and setup the formulation for MADG
* Results are impressive — on several benchmarks, the proposed method appears to perform competitively. 
* good ablations and analysis of the proposed method.

Weaknesses:
* I found the paper to be hard to read in general, at its core, the paper is proposing to model error on the unseen target using a convex hull of the source domains — which is a standard idea in many generalization papers. The novelty, in my opinion, is to use the MDD as an objective for determining discrepancy between domains, and the adversarial game. This can be clarified significantly to make sections 4, 5 more readable. There is too much of notation and terminology that obfuscates the reader from understanding the key contributions of the paper. I am taking the proofs and theorems at face value, and have not verified them.   
* As far as i understand, MADG requires to train significantly more models (1 per domain, a feature extractor and the main classifier) in order to compute MDD and perform training. This is vastly more complex than any existing method. It may be that making progress in a hard problem like domain generalization requires this, but i think this trade-off needs to be made more explicit — compute vs generalization performance. Whereas a simple ERM or Mixup have little to no overhead and perform very closely on the metrics considered in the paper.

Limitations:
some what, yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
- The paper presents a margin-loss based analysis of domain generalization.
- First, the paper derives a bound on margin disparity discrepancy (MDD) of any unseen domain within the convex hull of source domains in terms of the margin loss on source domains, ideal margin loss, and max MDD between any two source domains.
- The paper relates this to any unseen domain via projection onto the convex hull of source domains, and an additional factor $\gamma$
- The paper then realizes the bound for the empirical setting using a bound based on the Rademacher complexity of the function class
- Based on this bound, the paper proposes an adversarial learning method to regularize training by minimizing MDD in addition to classification loss.
- The paper demonstrates the efficacy of the method on a variety of real world datasets in the DomainBed benchmark.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper proposes a principled method for domain generalization, and the empirical method follows neatly from the derived generalization bound.
2. The proposed bound for error on an unseen domain based on margin disparities between source domains is novel to my knowledge. 
3. Empirical results show modest improvements over ERM.
4. The paper presents empirical ablations for some design choices used in the algorithm.

Weaknesses:
It is not clear if the single optimization step for the adversarial models f’ is sufficient for tightly approximating MDD. The method effectively regularizes a lower bound on the MDD term, which requires the adversarial models to be effective maximizers — this may require many steps for the adversarial models per main-model step.

Table 1 presents results using the model selection strategy of [71], which uses out-of-distribution data as the validation set for picking hyperparameters. Using labelled OOD data for hyperparameter selection limits the conclusions we can draw about if the proposed method works in practice, where we do not usually have access to labelled test data. While the AD, GD, and M metrics allay some of this concern, presenting results with some methodology that could be used for model-selection in practice, such as leave-one-out domain validation would make the empirical results stronger.

Limitations:
It is not completely clear how significant the empirical results are. The method slightly outperforms ERM on average, but ERM outperforms the method on 2 out of 5 considered datasets, and is competitive on 2 others. Additionally, the reliance on labelled OOD data for model selection limits the conclusions one can draw about the efficacy of the method on the DomainBed benchmark.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper is commendable for its innovative use of a margin-based theoretical framework to solve domain generalization problems, which contrasts with the largely heuristic and empirical approaches adopted by existing methods. By grounding their approach in a theoretical foundation, the authors provide more interpretable solutions that can contribute to the field of domain generalization. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The claim presented in the paper is strongly supported by theoretical analysis, as well as the extensive ablation studies presented in section 7. This speaks to the methodological rigour of the study and strengthens its credibility.
- The development of a theoretical framework for solving domain generalization problems is indeed a significant strength of this paper. This approach not only enhances the comprehensibility and reproducibility of the method, but also contributes to the broader understanding of the problem.

Weaknesses:
- While the average performance of the proposed method is the highest, the performance gap across different tasks is quite significant. The method shows the best performance on the Office Home dataset, but this alone is not enough to convincingly demonstrate the overall superiority of the method. Therefore, the claimed significance of the proposed method appears to be overstated.
- The authors' claim that a theoretical approach is necessary is quite a strong statement that seems to undermine the importance of empirically validated methods in the field. This makes it hard to fully agree with their motivation.
- While the authors make a reasonable argument for the use of margin as a metric in generalization, they do not provide a sufficient justification for its relevance in domain generalization problems, particularly where style shifts are involved. This makes it difficult to understand the authors' motivation for their margin-based approach.
- Similarly, the reasoning behind the use of adversarial learning (min-max framework) is not clearly explained. The proposed method appears to be similar to the one used in [1], and a clear explanation of the differences between these two methods, apart from the task, would be beneficial.
- The paper lacks a comparison with recent Domain Generalization (DG) works [2-4]. This makes it difficult to assess how the proposed method stacks up against the state of the art.

[1] Maximum Mean Discrepancy Test is Aware of Adversarial Attacks

[2] Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution

[3] Domain Generalization by Mutual-Information Regularization with Pre-trained Models

[4] SIMPLE: Specialized Model-Sample Matching for Domain Generalization

Limitations:
The authors have well described their limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper investigates domain generalization (DG) using a margin-based theoretical framework. The authors first formulate the generalization upper bound by leveraging the margin disparity discrepancy (MDD). Then, an adversarial learning strategy (MADG) is devised to minimize the empirical MMD between source domains. The experiments on the DomainBed benchmark further demonstrate that the proposed MADG could outperform the current state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is clearly written, and the method is well-designed. 
2. The theory of margin-based generalization bound is interesting and could benefit the further designs of DG methods. 
3. The proposed MADG could outperform recent state-of-the-art methods.


Weaknesses:
1. The primary concern of the reviewer is in its effectiveness. While MADG outperforms most baselines in the DomainBed benchmark, the improvement is marginal/minor. This made it unclear to me if the method actually works better, or it is just a product of optimizing some hyperparameters. 
2. The motivation, “very little work has been done in developing DG algorithms that are well-motivated by theoretical”, lacks soundness. In fact, there have been notable works in the literature that utilize theoretical frameworks to analyze the generalization problem, such as gradient matching [71, 88] and invariant risk minimization [80]. It could be beneficial if the authors could provide a detailed comparison with such theoretical methods.
3. In addition to the accuracy comparison, could the authors provide a more in-depth analysis regarding the training dynamics? This would help shed light on the inner workings of the MADG approach.
4. Many advantages, such as efficient optimization, stated in the abstract are not well verified. Specifically, following Eq. (17), it seems that the estimation process could introduce a significant computational cost, as it iteratively computes for all domains. 


Limitations:
See above comments. 

Rating:
5

Confidence:
4

";1
QSJKrO1Qpy;"REVIEW 
Summary:
In this paper, the authors first use Dirichlet energy minimizations on simplicial complexes (SCs) to interpret their effects on mitigating the simplicial oversmoothing. Then, through the lens of spectral simplicial theory, they show the three principles promote the Hodge-aware learning of this architecture, in the sense that the three Hodge subspaces are invariant under its learnable functions and the learning in two nontrivial subspaces are independent and expressive. Moreover,  the authors prove it is stable against small perturbations on the strengths of simplicial connections, and show how three principles can affect the stability. Lastly, we validate our findings on different simplicial tasks, including recovering foreign currency exchange (forex) rates, predicting triadic and tetradic collaborations, and trajectories.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written. The introduction and background give a nice overview and motivation for the problem.
2. The problem of simplicial complexes learning is really interesting and in many aspects understudied.
3. In the simplex prediction, the proposed SCCNN achieves promising empirical performance. 
4. The proposed SCCNN is supported by a theoretical analysis.


Weaknesses:
1. Limited applications and examples. It would be interesting to see more applications of the proposed SCCNN over widely used datasets (e.g., citation networks - Cora, CiteSeer, PubMed) for node and graph classification tasks. Moreover, although SCCNN achieves promising performance compared with SC-based model, can the authors compare it with other the state-of-the-art graph neural network (GNN)-based models?
2. Can the authors provide the running time of SCCNN and compare it with the state-of-the-art baselines?
3. How to select the dimension of $k$-simplex in the SCCNN model?

Limitations:
In general, I think this is a good paper with tackling a well motivated task. It would be helpful that this paper explores more datasets and compares with more advanced graph neural network-based models. 


Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors identified the limitations of non-Hodge aware learners on simplicial complex (SC) data and proposed a convolutional structure that 1) decomposes the upper and lower k-Laplacian and 2) takes the inter-simplicial couplings into account. The paper has presented a justification for the performance by analyzing the Dirichlet energy and oversmoothing. Additionally, they provided theoretical perturbations bound to study the robustness of the proposed convolution layer. The claims are supported by experiments on synthetic and real SC datasets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. [Originality] Incorporate the well-known Hodge theorem into the learning task on simplicial complex. The Hodge theorem provides a good intuition and explanation for the learning of the simplicial signal on SC. 
1. [Quality] Empirical examples on the synthetic datasets on Dirichlet energy and stability bound to support the theoretical claims. 
1. [Clarity] Great overview of the simplicial complex and Hodge decomposition. The authors also provided a motivation/justification for why the proposed layer works with Dirichlet energy minimization. 
1. [Significance] Being able to learn the simplicial signal in different Hodge subspaces is an important task.

Weaknesses:
1. If this framework needs to be applied to graph only having edges (i.e., SC of order 1), one usually can apply something like clique-complex (or any other methods to fill in the Simplicios) from that graph. In this case, $n_k$ is generally large (worst case $n_k = \mathcal O(n^k)$), resulting in a huge $L_k$ matrix. How practical is it to use the proposed method under this scenario?
1. Can you provide a definition/discussion or citation of Hodge-aware? It is not clear to me where it is defined throughout the manuscript. I can get some high-level ideas by reading Theorem 7, but I think it would be nice if you could explicitly call it out at the beginning (e.g., in introduction or background).
1. The discussion for preventing “over-smoothing” in Section 3 is great, it provides some high-level motivations of the choices you made. However, I am not sure if that it can support the claims.  Specifically, to really prevent “over-smoothing” of the Dirichlet energy, shouldn’t we bound the $D(x_k^{\ell+1})$ in other way around, i.e., with a lower bound rather than an upper bound? If we can show that $D(x_k^{\ell+1})$ can be lower bounded, the claim can be more convincing. 
1. Consider adding some high-level intuition on what harmonic flow is using the edge space example (e.g., flow cycling around global topoplogical structures); this will give readers having no background in Hodge decomposition a better understanding of what a “harmonic flow” is.
1. [Typo] L186 there is typo/grammatical issue, do you mean “$\tilde{h}_k = \text{diag}(...)$ is the frequency response of $\mathbb H_k$”?


Limitations:
The paper discusses some of its limitations and requirements/assumptions. No significant social impact is identified from this work. 


Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper aims to propose a convolutional architecture which incorporates the Hodge theory. Specifically, the proposed architecture incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions.




Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper claims a new architecture that incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions, but the clear differences with respect to existing simplicial neural models are unclear.

Weaknesses:
- Errors: The paper is very poorly written. The typographic, punctuation, and grammatical errors throughout the paper make it hard to follow. Lines 21, 25, 31, 32, and 37 (in the first page of the paper) are some examples of lines that containing such errors. Many abbreviations are used before defining them. For example, NN (line 30), SCCNN (line 51), MLP (line 65), SCF (line 264) etc., were never defined. Many variables, like \mathcal{V} in line 70 and most of the variables in eq. (4), and terms, like alternating map in line 87, were never introduced.
- Contributions: The contributions of the paper are unclear. In line 104, for example, the authors say, “we inherit the names of three edge subspaces to general k-simplices”, while the Hodge theory is already in place for simplices of all dimensions. The main contribution of the paper, which is supposed to be an architecture that incorporates the Hodge theory, is also not clearly presented. 



Limitations:
Overall, the paper is very hard to follow. The motivation and contributions of the work are not clear. The errors in the paper make it more difficult to follow

Rating:
2

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel architecture designed to operate on simplicial data, drawing its foundation from the Hodge decomposition. This decomposition ensures that features associated with a simplicial complex of order $k$ can be represented by three distinct quantities: a curl-free quantity, a divergence-free quantity, and a harmonic quantity.
The authors develop SCCNN, a new architecture for simplicial data that abides by these decomposition principles. The authors demonstrate the relevance of these principles by examining the Dirichlet energy, serving as a measure of oversmoothing within simplicial networks. The authors show that this new architecture reduces oversmoothness.
Theoretical guarantees for the stability of the proposed method when subjected to small perturbations are proven. The proposed approach is benchmarked against two exploratory tasks: a forex test and simplex prediction. The findings indicate superior performance of the SCCNN over previous architectures in these tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper delivers a substantial theoretical advancement to the field of neural networks operating on simplicial complexes. It introduces compelling theorems within a well-structured framework. Theorem 6. on the expressiveness of the SCCNN model is a result of potentially good interest for this subfield.
-  The authors have thoroughly compared their approach to existing methods, including standard graph neural networks (GNNs) and other simplicial neural networks. The results on toy examples are compelling.
- The paper does a good job of introducing the key concepts in a clear and precise way. I appreciated the background on Hodge decomposition. 
- Due to the simplicity of the overall principle behind SCCNN,  it has the potential for broad application within the field and formalizes new fundamental design principles for future architectures.

Weaknesses:
- The paper will benefit from enhanced clarity. Currently, it contains abundant results, which, while potentially insightful, obscure the core message of the research. Streamlining these results and focusing on the most salient points would aid in transmitting the core of the research, which is the SCCNN architecture, more effectively. The discussion around stability, while interesting, feels out of place. There are no clear motivations for studying it so thoroughly in the main text. On the other hand, the critical Theorems, such as Theorems 6. and 7., will benefit from a lengthier exposition. In particular, giving intuition behind the proofs of Theorem 6 would be appreciated. While I appreciate it is a theoretical paper, the results go in every possible direction with no clear target. In such a short paper, one should focus on a few key ideas and move as much as possible to appendices.

- The forex example is somewhat tailored to align with the proposed method. While it proves the paper's point and should be kept, it feels too synthetic. I am happy to be contradicted by the authors on that.

Limitations:
Limitations are discussed. Some points need to be clarified. See my question above. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The authors propose a general convolutional architecture with principles of uncoupling the lower and upper simplicial adjacencies, which accounting for the inter-simplicial couplings, and performing higher-order convolutions for learning of higher-order structure and simplicial signal.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors show that the proposed SCCNN structure demonstrates awareness of the Hodge decomposition and performs efficient learning on simplicial data
- Effect on mitigating simplicial complex oversmoothing is explained with Dirichlet energy minimization
- Comprehensive experimental results 
- Stability against robustness is also studied in this work

Weaknesses:
See questions

Limitations:
n/a

Rating:
5

Confidence:
2

";0
LTbIUkN95h;"REVIEW 
Summary:
This paper focuses on a specific class of Markov Decision Processes (MDPs) in which a state can be decomposed into a stochastic component and a pseudo-stochastic component. The transition dynamics of stochastic states depend solely on the current stochastic state and the action taken, while the transition dynamics of pseudo-stochastic states are determined deterministically by the current state, the next stochastic state, and the action. Such MDPs are suitable for modeling applications like queueing systems.

The authors initially consider a scenario with finite state spaces and batch reinforcement learning. Given a set of independent and identically distributed (i.i.d.) transition and reward data, they propose a data augmentation algorithm that generates m new virtual samples from each data point. This is achieved by replacing the pseudo-stochastic components with newly sampled ones. Subsequently, they propose the fitted Q iteration with an augmented dataset. Under certain coverage assumptions for stochastic states and with n initial samples, they establish an error bound on the Q function, which scales as 1/\sqrt{n} + 1/\sqrt{m}. The authors extend their results to scenarios with an infinite pseudo-stochastic state space, along with additional assumptions. Simulations are conducted to demonstrate the benefits of the proposed algorithm.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
Using reinforcement learning to determine optimal policies for systems with large state spaces is a well-known challenge. However, this paper presents a valuable and intuitive framework that effectively captures the inherent structure of environments, specifically queueing systems, through the introduction of pseudo-stochastic states. This innovative modeling approach is commendable. The concept of utilizing fixed transition functions to generate new data points is novel and has practical relevance for systems influenced by limited stochastic inputs. The theoretical results provided in the paper are sound and contribute to the overall strength of the work.

Weaknesses:
In Section 2.1, the authors provide an example where the pseudo-stochastic state space is not finite, which goes beyond the scope of the model as indicated by the assumption of a finite state space in line 148. Although the authors later extend their results, it remains unclear how this example aligns with Assumption 1.

Building upon point 1, Assumption 1 poses challenges in designing the function class F for the Q function. It seems difficult to define a class where the value of an atypical state under any function within the class is consistently close to the optimal Q value. The authors need to provide better justification for why this property can be easily achieved. One approach could be to establish a broad enough function class that satisfies Assumption 1 for a simple queueing system. Justifying the reasonableness of Assumption 1 is crucial, especially considering the paper's claim that the theory and method are applicable to queueing systems with unbounded state spaces.

The simulation section lacks sufficient explanation and some of the plots appear unreasonable. Further details and clarification are needed to address these issues.

There are several typos that need to be corrected throughout the paper.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper introduces a novel setting called mixed systems, in which environments comprise both stochastic and deterministic transitions. This leads to two types of states: stochastic states and pseudo-stochastic states. Stochastic states follow a stochastic transition kernel, while pseudo-stochastic states have deterministic transitions given the stochastic states/transitions. Such systems are common in real-world reinforcement learning (RL) applications such as data centers, ride-sharing systems, and communication networks.  

Existing RL approaches can suffer from sample inefficiency due to the curse of dimensionality. The paper proposes an augmented sample generator (ASG) to improve sample efficiency. ASG augments transitions by retaining stochastic states while sampling new pseudo-stochastic states.  

The paper provides a motivating example of a wireless downlink network, where stochastic states are the number of data packets arriving and the number of packets that can be transmitted at each time step. The pseudo-stochastic state is the length of the queue, which can be computed deterministically given the stochastic states.  

The authors offer a convergence guarantee in the form of an optimality gap, connecting the learned value and optimal value with the number of real samples and augmented samples per real sample.  

Experiments are conducted on a criss-cross network and the motivating example. The results demonstrate that combining ASG with Q-learning or policy gradient-type algorithms significantly improves performance.  

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The paper presents a new setting that is widely observed in real applications.  
2. A highly suitable algorithm (ASG) is proposed for this setting.  
3. The effectiveness of ASG is demonstrated through solid theoretical proof and several experiments.  


Weaknesses:
The main weakness of the paper is that the difference between the new mixed systems setting and existing MDPs with exogenous inputs is not entirely clear. Although the analysis in Line 133 is sound, the environments used in this paper can be modeled with MDPs with exogenous inputs. For example, in the wireless downlink network, stochastic state is independent of actions taken. Seems that all environments used can be viewed as MDPs with exogenous inputs, and also a special case of mixed systems. However, the new setting is valuable, and more real-world application examples that can only be modeled with mixed systems are desirable. **If the authors address this issue, I recommend strongly accepting the paper**.  

A minor weakness is that the algorithm assumes access to the reward function $R$ and pseudo-stochastic state transition function $g$, potentially limiting its scope. However, this assumption seems reasonable in most examples mentioned.

Limitations:
The paper does not extensively discuss limitations or potential negative societal impacts of the work. No further limitations need to be addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper consider the sample efficiency of RL in a setting involving ""mixed systems"" where the state space is factored into a stochastic component and another much larger component that is deterministic when conditioned on the stochastic component. The authors develop an algorithm for this setting relying on generating relevant synthetic states that have not been encountered before. This approach when applied to fitted q-learning is able to show sample efficiency bounds for which it is not necessary that the data includes all pseudo-stochastic states. The authors also demonstrate the practical utility of their approach in application to the deep function approximation settings based on queuing networks. 

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- The paper is well written and makes a focused contribution. 

- Not only is significant theoretical analysis provided, but also it is shown that this theory makes a meaningful impact for deep function approximation. 

- While the empirical domains are not very complex, they do represent real-world use cases, which makes up for this deficiency in my mind. 

Weaknesses:
- I think the approach will make a big impact where it is applicable, but I worry that this setting is a bit niche. I don't think this is a big problem, but just am a bit less excited as a result. 

- It is unclear to what extent the approach can be extended to partially observable domains with more complex input spaces.

- Given the similarities, I would appreciate a more clear connection with the factored MDPs literature. 

Limitations:
The authors do a good job for the most part of discussing assumptions and limitations. I agree that the assumption for the case of an infinite number of pseudo-stochastic states seems mild. Looking at appendix A, the data coverage and completeness assumptions also seem mild, although I think at least more intuition about this should be provided in the main text. One thing I think should be further highlighted and discussed though is that not only do these components of the state space need to exist but they also need to be provided to the user. In my mind, this knowledge about the state space is relatively hard to come by and limits the areas of application. Maybe taking inspiration from the literature on factored MDPs, the authors can account for the cost of the discovery process as well. 

Rating:
7

Confidence:
3

";1
CWdxHxVAGG;"REVIEW 
Summary:
The authors consider 3 rejection models in the OOD detection setting and establish theorems about the optimal rules for these models. Due to the commonly used metrics that may partially only focus on either OOD detection or misclassification, they proposed a double-score method to consider both aspects.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The authors extend the rejection in the non-OOD setting to the OOD detection setting.
* The 3 different rejection models share similar optimal strategies, and those strategies outperform other baselines.
* Since previous works only focus on either OOD detection or misclassification, the authors proposed a novel double-score method to consider both aspects. 


Weaknesses:
* Now that we know all the distributions, including OOD and ID, why put effort into eq 3? Can’t we just treat augmented data of OOD + ID as the new collection with $K + 1$ ""inlier"" classes, then follow the regular rejection analysis under the closed-world assumption?

* The optimal rules heavily rely on the clear information (the distribution in the theorem or estimated from the sample) on OOD data. For example, the prior $\pi$ in the bounded precision-recall rejection model, the conditional probability (of OOD) in the bounded TPR-FPR rejection model, or even both in the cost-based model. However, it is challenging to access OOD data and estimate these probabilities in real-world applications within an open-world setting. This raises concerns about the practical guidance provided by these optimal rules.

* I have a reservation about the constraints in equation 8 or the setup of problem 1. In practice, it is more likely the threshold or the selective function is determined by only one constraint (even in your proof of theorem 2, there is nowhere about the constraint $\rho(c)$). For example, once the $\lambda$ is determined by one constraint, it does not necessarily satisfy another one, which is similar to the trade-off between the type I and type II errors or Neyman-Pearson classification. A similar argument applies to Problem 2.

* Optimal rules: the authors claim the optimal rules related to the Bayes classifier (lines 126, 170), but from theorems, it holds under the condition that the given $(h, c)$ is optimal. If we cannot find the optimal $h$, can we still say the Bayes classifier is optimal?


Limitations:
See my concerns and questions above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper provides a unified viewpoint of reject model evaluation metrics under OOD-ness. Despite different performance metrics being used in practice, they can all be factorized into both the misclassification prediction on ID datapoints as well as the discrimination performance between ID and OOD. The experimental section provides some evidence that this proposed two score decomposition can provide competitive OOD detection and input rejection performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- OOD detection and classifying with a reject option are both timely and important issues in trustworthy machine learning. ML algorithms need to be aware of their limitations and signal if they are deployed on previously unseen data.
- The unified framework / viewpoint makes sense and yields an interesting decomposition result showing that the optimal uncertainty score is a function of both misclassification prediction on ID datapoints as well as distinction performance between ID and OOD. Although this appears like an intuitive desiderata for a prediction uncertainty score, explicitly tying it back to the different rejection models seems new.

Weaknesses:
- The paper assumes that the OOD distribution is known. This is a strong assumption and severely limits the applicability of the approach in practical settings where knowing or explicitly estimating this distribution is often prohibitive.
- Even though the paper does provide a nice unification of three performance measures for reject option models in an OOD setup (cost-based rejection, TPR-FPR rejection, precision-recall rejection), these metrics are not new and routinely used in existing applications. To me it seems like this kind of unified viewpoint would be better suited for a survey-style paper.
- Section 3 could have been motivated better. To me, it appears a bit sudden without much justification or transition from the previous section.
- Section 3.1 could have significantly profited from a figure showcasing the 1D Gaussian example for added intuition.
- While the optimal uncertainty score is unified across error models, the experimental section showing the efficacy of the double score method is very lackluster. Details about models, training methods, and hyper-parameters area all missing (and also not documented in the appendix). Results are also based on a single run which makes it impossible to judge the statistical significance of the presented results. Moreover, the considered datasets do not all share the same sample shapes (e.g. MNIST vs CIFAR-10) which makes me wonder how OOD scores were obtained here in the first place. Real life shifts, like from the WILDS dataset collection would have been a better place to assess OOD-ness of samples. I am not convinced by this evaluation.

### Post-rebuttal

I have read the author's rebuttal and found that although many of my concerns were adequately addressed, the experimental documentation issue remained unaddressed. I increased my score only slightly as a result.

Limitations:
Weaknesses of the approach are not adequately discussed in the paper. See weaknesses and questions above.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes three reject option models and introduces double-score OOD methods that consistently outperform state-of-the-art methods. The authors also propose novel evaluation metrics for comprehensive and reliable assessment of OOD methods. The proposed metrics simultaneously evaluate the classification performance on the accepted ID samples and guarantee the performance of the OOD/ID discriminator, either via constraints in TPR-FPR or Precision-Recall pair. The authors argue that setting these extra parameters is better than using the existing metrics that provide incomplete if used separately, or inconsistent, if used in combination, view of the evaluated methods.  Overall, the paper's contributions provide a significant improvement in OOD detection and evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes three reject option models for OOD setups, which extend the standard reject option models. These models define the notion of an optimal OOD selective classifier and establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. This is an original and creative approach to the problem of OOD detection, and the proposed models are well-motivated and clearly explained. The paper introduces double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The paper proposes novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models. These metrics provide a comprehensive and reliable assessment of OOD methods. 

Overall, the paper is well-written and easy to follow, with clear explanations of the proposed models, methods, and evaluation metrics. 

Weaknesses:
One weakness of the paper in the experimental results is that the dataset used in the experiments is relatively small, and the proposed methods do not show significant advantages over the baseline. This raises questions about the generalizability of the proposed methods to larger and more diverse datasets. Additionally, the paper could benefit from a more detailed analysis of the limitations of the proposed methods and how they can be improved in future work.

Limitations:
The authors did not explicitly discuss the limitation of the proposed method.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper addresses out-of-distribution detection (ID/OOD discrimination) and misclassification detection (selection classification). The authors introduce double-score OOD methods that leverage uncertainty scores from OOD detector and misclassification detector. For evaluation metric, this paper proposes to use PR curve.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1) Existing works mainly focus on OOD detection, which this paper simultaneously considers OOD and selection classification. This is more practical and useful for high risk applications.

(2) Theoretical analysis of OOD detection and selective classification is valuable.

Weaknesses:
(1) The analysis of Bayes-optimal OOD selective classifier as well as the Bayes-optimal misclassification selective classifier has also been investigated in [1]. The authors are suggested to demonstrate the difference.

(2) The experiments are insufficient, only evaluating on cifar-10 and mnist. Results on CIFAR-100 is valuable.

(3) What is the advantage of PR curve over AURC (risk-coverage) curve [2] for evaluating rejection model?

[1] Narasimhan, H., Menon, A. K., Jitkrittum, W., & Kumar, S. (2023). Learning to reject meets OOD detection: Are all abstentions created equal?. arXiv preprint arXiv:2301.12386.

[2] Geifman, Y., & El-Yaniv, R. (2017). Selective classification for deep neural networks. Advances in neural information processing systems, 30.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a formal analysis of three distinct models for classifiers with a reject option in the presence of out-of-distribution inputs at test time. All three models, viz. 1) cost-based rejection, 2) bounded TPR-FPR, and 3) bounded precision-recall, share the same form of optimal selective classifier (see Section 2.4). 

This selective classifier consists of the Bayes in-distribution classifier $h_B(x)$ (which is optimal for a given in-distribution), and a selection function score that is a linear combination of the conditional risk $r(x)$ and the likelihood-ratio of OOD to ID $g(x)$. Based on this analysis, the authors point out the limitations of current OOD detection methods which only have a single score, and instead propose double-score OOD detection methods which can focus on both mis-classification detection and ID/OOD separation.

Using a concrete synthetic example, they discuss the limitations of existing metrics such as AUROC and AUPR in evaluating selective classifiers in the OOD setting. They propose a novel metric (one each for the bounded TPR-FPR and bounded precision-recall models) which calculates the selective risk, subject to a given minimum TPR and maximum FPR (or a given minimum recall and minimum precision). The proposed metric is shown to be better at capturing the overall performance of selective classifiers in the OOD setting.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The proposed reject option models and evaluation metrics are developed systematically and connected well with summary sections. Found the paper to be well written. 

2. The analysis builds upon the prior work [6] which deals with reject option models but not in the OOD setting. In this work, they consider rejecting inputs due to both mis-classification and being OOD.

3. Although the proposed work makes a strong assumption of a known OOD distribution, the analysis is useful to draw insights about the need for a double-score OOD detection method, and also to highlight the shortcomings of existing evaluation metrics.

[6] https://www.jmlr.org/papers/volume24/21-0048/21-0048.pdf

Weaknesses:
1. The analysis and proposed new metrics (parts of the new metrics like the FPR and precision) depend upon knowledge of the OOD distribution. This is a strong assumption for practical settings. 

2. For the proposed double-score OODD method, it seems to me that we need access to OOD data in order to set the hyper-parameter $\mu$ in the combined score $s_r(x) + \mu s_g(x)$. The authors should clarify if this is set based on validation data from a different OOD distribution than the test data.

3. Minor: it is a bit tedious to keep track of all the notations needed for the analysis.

**Update after author-reviewer discussions:** \
I have read the authors rebuttal. The paper lacks sufficient discussion on the experiments. Not enough details are provided in the appendix as well. Hence, I decrease my rating to 6.

Limitations:
Some limitations of the proposed work are mentioned in Section 3.5. Another limitation to include is the fact that the proposed analysis and novel metrics depend on the OOD distribution which is usually unknown. One could use a validation set (as in the paper) with a mix of in-distribution and auxiliary OOD data, but the distribution of auxiliary OOD data in the validation and test sets should be different. 

Negative societal impacts has not been discussed, but may not be applicable here.

Rating:
6

Confidence:
3

";0
8S6ZeKB8tu;"REVIEW 
Summary:
The paper considers the problem of estimating the accuracy of noisy judges/classifiers in a streaming fashion, using only unlabeled data. Specifically, the goal is to compute the accuracy of each judge while processing items and the judge predictions for each item as part of a stream, without any associated labels for the items.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The problem of evaluating noisy judges that is being explored by the paper is an interesting and important one.

Weaknesses:
1. The main method presented in this paper (i.e., section 3) is not novel. For example, it can be seen as a special case of the approach presented in *Platanios, E. A., Blum A., and Mitchell T. ""Estimating accuracy from unlabeled data"" UAI (2014)*, which relaxes the independence assumption (though does not consider the streaming setting) and is also not cited in this paper. In fact, a lot of related work is missing including derivatives of the aforementioned paper (e.g., a direct follow-up in ICML 2016).

2. The premise of section 4 is weak. The idea that the method of section 3 is “self-alarming” because when you have dependent classifiers the accuracy estimates will be invalid is not completely correct. While this may capture some cases, there are still a lot of cases where you can have dependent classifiers, and where there exists a valid solution to the presented system of equations. Thus, I am not convinced by the main claim of this section.

3. The paper considers a streaming setting but it does not provide motivation for it. For example, it was not clear to me why we cannot store the predictions of the classifiers as items are processed in a database and then perform accuracy estimation periodically. If we have 8 classifiers and 2 possible labels, this would require 1MB per 1 million items, which does not seem expensive (and we can also perform random sampling if space becomes an issue).

4. The paper is presented in a manner that is hard to follow and could be significantly improved. I The whole paper would be presented in a simpler and more organized manner, but section 5 was particularly hard to follow without spending a significant amount of time to understand the argument that was being made.

5. The experimental evaluation is a bit lacking in that only toy datasets are being used, there is no explanation for what they are and why they are interesting, and there are very limited results being presented. Ideally, I’d like to see an “Experiments” section in the paper that describes the setup targeted at testing some hypotheses, the datasets, and the evaluation metrics, and then presents and discusses the evaluation results.

Limitations:
There is no discussion of limitations and potential negative social impact in this paper. One recommendation would be to try and think about what the implications could be for say voting systems, and also in situations where these methods are used to evaluate people whose income may depend on this evaluation (e.g., crowdworkers). In this case, the independence assumption being made by the paper may be too strong and yield in incorrect evaluations that could negatively and unfailrly affect the income of those people.

Rating:
2

Confidence:
5

REVIEW 
Summary:

This paper introduces a new inferential evaluator for evaluating noisy binary classifiers on unlabeled data in a streaming manner. Specifically, compared to the evaluator based on majority votes, the new evaluator gives a more complete and reasonable modeling of the true label prevalence and each classifier’s accuracy. In addition, the property of the new evaluator is also mathematically discussed, and the relationship between error dependence and the evaluator estimate is empirically discovered through experiments.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:

1. The paper addresses a significant problem in machine learning - evaluating the performance of binary classifiers on unlabeled data.
2. The proposed methods could have wide-ranging applications in various fields where machine learning is used, making the paper highly relevant.
3. The author provides mathematical proof to support the proposed methods and conducts empirical tests on several datasets.
4. The proposed generic framework that is based on algebraic geometry can cast a positive influence on evaluation methods on unlabeled data.
5.  The new algebraic evaluation method bypasses the representation and OOD problems in ML.

Weaknesses:
1. The paper is very hard to read and lacks the background to help the reviewer understand and improve the reading experience. Moreover, the supplement mentioned in lines 124, 171, 236, and 292 is missing from the paper. There are no detailed proofs for all the theorems.
2. The organization of the paper is confusing. The logic chain of the whole paper needs to be improved, better briefly introduce the outline and main content of each chapter at the beginning.
3. The aiming research problem needs to be explained formally in math language and to be explained clearly with intuitive explanations, better with a toy example or case study.
4. The current limitations of existing research, the proposed solutions (contributions of this paper), and the aiming experimental questions are not clearly listed, making it hard to catch the author's idea.
5. More experiments are needed. There are no experiments supporting that the performance of a majority vote-based evaluator is worse than that of the inferential one. And from the perspective of experiments, the advantage of the new evaluator is not made clear.
6. Only label prevalence is formalized in the paper, while there are no formulas for classifier accuracy.
7. The complexity of the concepts and the heavy use of mathematical proofs might affect their clarity. The author could consider providing more background information, intuitive explanations, or visual aids to improve the paper's accessibility.
8. The experiment part fails to show the superiority of the proposed method compared with other baselines.
9. The equations need to be carefully edited using formal math language. The space should be used for some meaningful and essential equations, not for simple ones such as summation or average operations.
10. Typos: In line 167, ""it could have been a βitem, not an αone"" should be ""it could have been a β item, not an α one"".

Limitations:
   The paper concludes with a brief discussion of how algebraic stream evaluation can and cannot help when done for safety or economic reasons.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of evaluating noisy binary classifiers on unlabeled streaming data. It aims to estimate the prevalence of the labels and the accuracy of each classifier on them, given a data sketch of label predictions by the members of an ensemble of noisy binary classifiers. The authors propose two algebraic evaluators based on the assumption of error-independent classifiers: the first is based on an additional assumption of majority voting, and the second is fully inferential and is guaranteed to contain the true evaluation point.  

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The results of this paper seem to be well supported by the rigorous mathematical analysis as well as empirically demonstrated on three benchmark datasets.

Weaknesses:
- The independence assumption may not be satisfied in practice, especially when the ensemble of classifiers consists of different models trained on the same or overlapping datasets, or even the same model but trained for different durations.
- The significance of this paper is unclear. It would be better for the authors to provide some concrete real-world examples that fit the problem setting of this paper and explain the possible uses of the quantities desired to be estimated in these examples.
- The proposed evaluators may be sensitive to noise or corruption in the data sketch. The solutions of the algebraic equations will change if the data sketch is not perfectly recorded, leading to mistakes in distinguishing between independent and correlated evaluations. 

Limitations:
The authors has discussed the limitations of this paper.

Rating:
4

Confidence:
2

REVIEW 
Summary:

The paper addresses making decisions based on the outputs of three binary classifiers. More precisely, it focuses on evaluating the performances of noisy classifiers. It considers majority voting on one hand, and a proposed evaluation scheme based on the classifiers' accuracies. The paper establishes several theorems so as to demonstrate the superiority of the second (proposed) evaluation scheme. Then, experiments are conducted to test the ability of the proposal to avoid making decisions in problematic situations—e.g., correlated classifiers. 


Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
I see no particular strength in the paper that would mitigate its flaws. 

Weaknesses:
The proposal suffers from several major flaws. 

First of all, it is badly written. The problem is not clearly stated. The mathematical objects (typically, the prevalence of the labels, and the label accuracies) are not properly introduced and defined. Some key notions (such as the ""evaluation variety"", the precise definition of correlated classifiers, among others) are also not defined. There are frequent references in the text to notions which have not been exposed yet (e.g., ""the evaluators for binary classifiers"" in the introduction, Theorems 1 and 2 in the introduction as well, Theorem 3 in Section 1.2). 

Besides, the paper ignores a large amount of literature. The problem addressed has connections with computational social choice (voting schemes), of which some works are mentioned. But it also relates to classifier combination (boosting, error-correcting output codes, weighted averaging, racing algorithms, etc): the problem of evaluating the performance of an ensemble has been addressed in a number of works which are ignored here. 

Last, but not least, the paper focuses on a very specific case—the ensemble has only three classifiers. This is very restrictive and such ensembles are hardly used in practice. Some claims are not supported—e.g., in Section 1.1, ""Seemingly correct estimates are estimated values that seem to be correct because they have this real, integer ratio form. Estimates that do not have this form are obviously incorrect."" This is not the case of the $F_\beta$ measure, for instance. 


Limitations:
The authors have addressed the limitations of their approach, but in my opinion only in a restricted way. They have not addressed the potential negative societal impact of their work, but I do not think that this is crucial here. 


Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of evaluating an ensemble of binary classifiers on unlabeled data in a streaming setting. The authors first describe a baseline which treats the majority vote as the correct label and evaluate each classifier accordingly. Then they propose an evaluator based on an assumption that the classifiers are independent. The algebraic expression for this evaluator should return rational numbers if the assumption holds (as they should correspond to ratios of integer counts). Thus, this evaluator has failure modes that can be detected clearly unlike the majority-voting baseline that may return incorrect but seemingly sensible values.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
While I am not an expert in evaluation using unlabeled data and cannot speak definitively, the proposed algebraic evaluation and characterizing failure modes by algebraic failures seem creative and novel.  

Weaknesses:
I found the paper overall quite difficult to follow, and thus my assessment of its technical contributions may be limited. More thorough motivation and background on the problem setting (e.g. using real-world applications), as well as careful characterization of the proposed algebraic evaluator in contrast with existing approaches, would help make the paper much more approachable. I also had a hard time following most of Section 1 (especially 1.2) without the technical details in Section 3.

The paper is also missing some related work discussion, and its contributions with respect to prior work is not very clear. I struggled to see the connection to the works mentioned in the first paragraph of Section 1.3. Another work that appears very relevant is [1]; how does this paper relate to their approach?

Throughout the paper, only the setting with three binary classifiers is considered. A more general formulation may be helpful. As far as I can tell, this approach would scale exponentially in the number of classifiers which could limit its impact in practical settings.

Empirical evaluation was limited only to analyzing the failure rates, and there were no experiments on how well the proposed approach performs as an evaluator (i.e., how close are the error rate estimates to the true error rates?). 

[1] Platanios, Emmanouil Antonios, Avrim Blum, and Tom Mitchell. ""Estimating Accuracy from Unlabeled Data."" 2014.

Limitations:
Overall, yes. One limitation that was not discussed is that the approach seems to scale exponentially in the number of classifiers.

Rating:
2

Confidence:
2

";0
UZTpkfw0aC;"REVIEW 
Summary:
This paper presents a tissue specific transformer based splicing prediction model, TrASPr along with a Bayesian Optimization algorithm, BOS, capable of designing RNA with desired properties. The authors start by demonstrating the performance of TrASPr on RNA splicing data from both mouse and human tissues. Next, the authors assess the model’s ability to detect condition specific regulatory elements using ENCODE data involving three RBP Knockdown (KD) in two human cell lines, and data for tissue-specific regulatory elements from a mini-gene reporter assay. At last, TrASPr is used as an Oracle for BOS to generate AS event sequences with desired properties and an evaluation of BOS performance is presented.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
[-] Originality; This paper utilizes recent advances in large language models (LLMs) to define a splicing prediction model. While this is not the first attempt to use LLMs for nucleotide encoding, the authors improved existing models and incorporated existing prior knowledge through dedicated features. Furthermore, the authors use the model as an oracle for a generative model for RNA splicing design. 

[-] Quality; The paper is well-written and presented. The framework seems well thought through; combining both expert prior knowledge regarding the problems tackled and state-of-the-art computational models. 

[-] Clarity; The paper is self-inclusive, presenting the reader with all necessary information from the background regarding the biological problem, its complexity, and motivation regarding its importance. Following that, all framework parts are clearly presented.

[-] Significance; This work provides promising results towards utilizing LLMs for predicting splicing events and further using such these to train an RNA design model. While this work is only the initial step towards obtaining a robust, reliable framework that solves this task it is of great significance as it advances the field. 


Weaknesses:
[-] Reproducibility code; the authors claim for reproducibility however no code was provided. Providing the code could improve the understanding and evaluation of the presented framework.

[-] Structure; the paper is generally well structured in providing all necessary information however the organization could be improved. For example, the background section already contains model implementation details and prior attempts are split between the introduction and related work. In line with the latter, it would be beneficial if the authors could provide a more elaborate description of previous work, specifically for the prediction task. 

[-] Evaluation; the author’s explanation regarding the degradation in performance over the “Strict” test set is not convincing, and following the rationale behind the necessity of the “strict” test set raises some concerns. It would be insightful if the authors could look more into this point, potentially testing on alternative data to obtain a better understanding of this. 

[-] minor (these are provided to improve the manuscript’s readability);

[--] incomplete/unreadable sentences; a few sentences in the text are incomplete or unclear (e.g. line 78, lines 256-259)

[--] Space after TrASPr is omitted in many places in the manuscript (probably as a result of latex macro).

[--] Figure 1; Figure 1a is never referenced (and its components are hence not explained), the relationship between 1b-1c could be depicted better (in line with many transformer visualization models). 


Limitations:
Yes, limitations have been addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors develop a new framework to predict alternative splicing of RNA. They then deploy it with adaptations and Bayesian Optimization to design new sequences.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
I find the validation using the RBP KD experiment interesting. It is great that the knowledge of the biological system can be used to inform your computational experiments.

“We repeated this process 5 times with different random mutations and the prediction results where then averaged and compared to the wild type (WT) sequence prediction.“ - Good to do lots of permutations!

Good to try to remove batch effects with ENCODE data, but going to be hard.

Levenshtein distance between designed sequences is good!

Figure 2 comparison to Pangolin is pretty good and convincing


Weaknesses:
Table 1 results of rAUPRC and AUROC are confusing. Can the “feature” and “Model” terms be a bit better defined?

Figure 4b is a bit confusing, and I feel like we need a bit more context. Should things be above or below the line? Can we have a legend for the figure as well?

BO part of paper barely discussed, required changes to core algorithm, and not validated–I would remove. Moreover, the baseline algorithm to compare BO (random mutation) is not a sufficient baseline. What about evolutionary strategies? What about Gibbs sampling?


Limitations:
It is unclear how much the BO section is needed. With a sufficient predictor, do evolutionary strategies work?

“Specifically, we only…” Background section is not complete…sort of just trails off.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a new machine learning framework called TrASPr, which is a transformer-based architecture with pretrained RNA language models that is tailored for the prediction and design task of RNA alternative splicing. The authors demonstrate that TrASPr accurately predicts tissue-specific `percent spliced in’ (PSI) scores, and the trained model can facilitate RNA design for specific RNA splicing outcomes. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Overall, the authors introduce and explain the problem of alternative splicing, its significance, and the dataset they used to study this problem quite well for a general reader at a machine learning conference.
- The evaluation setting is generally rigorous, as the authors carefully curated the test set to avoid any information leakage.


Weaknesses:
- Some additional work and its relationship to this research should be discussed, such as ""RNA Alternative Splicing Prediction with Discrete Compositional Energy Network,"" which also focuses on the prediction of PSI scores in a tissue-specific setting.
- When evaluating the effect of RBP KD and mutations, the authors first identify a set of RBP motif matching sites that might affect alternative splicing and then check if their model can accurately predict those sites through in-silico mutations. However, this measurement only assesses the model's ability to recover positive samples. The authors should also evaluate and present examples of in-silico mutations on non-regulating motif matches and demonstrate their models' predictions. This is important to show that the model is genuinely learning context-dependent sequence features and not just memorizing motif matches.
- While the formulation of the RNA design problem and the utilization of language models for RNA sequences can be considered novel in the field of RNA splicing, similar techniques have been introduced and used in protein sequence design and protein language models before; it would be nice to discuss some of those (e.g., for a review, see https://doi.org/10.1016/j.cels.2021.05.017), perhaps in Related work.


Limitations:
The authors discussed the limitations of the noisy labels obtained from biological experiments and concluded that this work ""should be viewed more as a proof-of-concept outlining exciting directions for future developments rather than a finished product."" It would be helpful if the authors could comment on how many datasets exist and are expected to be generated, and whether these limitations would be addressed with more data or more careful model design. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles two tasks in alternative splicing of pre-mRNA, where multiple unique mRNAs are produced by including different segments. First, the authors proposed a transformer-based splicing prediction model, TrASPr. A 6-layer transformer model is pre-trained with 1.5M pre-mRNA sequences centered in splice sites. TrASPr utilizes multiple pre-trained transformer encoders for the splicing prediction in a tissue-specific manner. Second, the authors used TrASPr as an Oracle to train and evaluate splicing sequence design based on the Bayesian Optimization algorithm. Through the experiment results, they argue that TrASPr significantly outperforms state-of-the-art models and BOS can generate sequences with desired characteristics.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
-	The authors tackle important problems in RNA biology. In particular, they argue that the sequence design for RNA splicing is a novel task and it can benefit therapeutics for correcting splicing defects and synthetic biology.
-	To tackle the problems, they adopt a couple of machine learning methods that have shown successes in other domains: pre-training and fine-tuning of language models for biological sequences, and latent space Bayesian optimization (LSBO) over structured and discrete inputs.

Weaknesses:
Major comments:
- [Originality] While the methods are novel for their first use for RNA alternative splicing, they still seem like mostly direct applications of widely known machine learning methods. For example, pre-training and fine-tuning of language models seem trivial even for the biological sequences. As referenced in the paper, DNABERT proposed a pre-trained language model for DNAs. There are also plenty of previous works that use pre-trained language models for various protein biology tasks.
- [Quality] While the paper includes a couple of experiment results, I do not think they are sufficient to verify the effectiveness of the proposed methods. It lacks strong baseline models for comparison and ablation studies for thoroughly understanding the proposed methods.
- [Significance] The paper does not bring significant and novel ideas that would be valuable to the broader NeurIPS community. 
- [Clarity] I don’t think this is the best version of the paper, considering the broad NeurIPS community is not familiar with computational biology. The explanations are not detailed enough to easily understand the problem and significance of the experiment results. 

Minor comments:
- Typo in L10: on Oracle -> an Oracle
- Sec 2.1: Incomplete. It suddenly ends with “Specifically, we only.”
- Sec 7: The authors mostly use the Discussion section for summarizing their contributions rather than discussing notable observations, limitations, and future directions. (except for the last paragraph where they discuss the inherent limitation of experiment data)

Limitations:
The authors adequately addressed that the experiment data are inherently noisy and limited in number, such that this work should be viewed more as a proof-of-concept rather than a finished product. 


---Post-Rebuttal Comments---
Overall, I am inclined to believe that incorporating the authors' responses would indeed enhance the manuscript's quality. Consequently, I have adjusted my rating from 3 to 4. Upon reviewing the comments from other reviewers and the authors' clarifications, it seems I'm not the only one who has struggled to understand the authors' contributions and has concerns about the presentation, especially regarding the BO for sequence design. This suggests that substantial revisions are needed beyond the inclusion of additional experimental results. Although the authors' responses have been comprehensive, I could not give a higher score as I respectfully believe resubmission after revision is more appropriate for this manuscript.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors propose two approaches to deal with the problems of alternative splicing (AS).  A transformer architecture-based tissue-specific splicing code model, TrASPr, and a Bayesian Optimization algorithm were performed on the latent variable space of VAE to address the design of RNA sequences with specific splicing characteristics.  The architecture is not so novel, but applying LLM to the AS is worth noting.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The proposed methods are exciting and computationally novel.  Using BERT with masking to pre-train the model was a nice touch.  Using VAE with Bayesian Optimization is interesting.

Weaknesses:
The proposed method is a nice modeling experiment exploring using LLM on a novel application. However, it lacks reliability as a tool to deal with biomedical problems that can be used for biological research.   The evaluation the author provides shows that the method fails.  The authors mention that TrASPr significantly outperforms AE-MLP in a particular situation but also point out that the performance degrades based on the filtering criteria.  There also seems to be a performance difference in BOS sequence generation based on the edit distance, a hyperparameter that lay users would not know how to tune on their particular problem.

Minor comment:
Line 78 is incomplete.


Limitations:
The proposed method has a potential to be a hypothesis generation method.  However, it lacks the biological soundness and it is unclear how to pinpoint the problem when they arise.  Authors are very casual about the evaluation and the problems that arise when the hyper parameters are chosen inappropriately.  

Rating:
5

Confidence:
3

";0
lDI3ZuyzM9;"REVIEW 
Summary:
This paper proposed a new NAS algorithm where a performance predictor is built for acceleration. In addition, the experiments are conducted for verification.
============================
Thanks for the authors' rebuttal. Unfortunately, my concerns are still not addressed. For example, 1) the used data are not a benchmark, the reference also did not say it is a benchmark, while the authors believed it is a benchmark. I still believe the novelty of the work is very limited to the community, and cannot address the real concerns in the community.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The search space is built on the segments of CG of DNNs.

Weaknesses:
Currently, ""performance predictor"" is more common than ""neural predictor,"" though they refer to the same thing.

Some claims about neural predictors are not correct. For example, ""Neural predictors treat NAS benchmarks as datasets."" In fact, the early works on this aspect did not use the NAS benchmarks as the dataset, and even these works were earlier than the NAS benchmarks. A baseline in this topic is [1], which is often compared with peer competitors.

[1]  Sun et al., ""Surrogate-assisted evolutionary deep learning using an end-to-end random forest-based performance predictor,"" TEVC 2020.

The predictor is built on the subgraphs minded on NAS benchmarks. To this end, the constructed predictor cannot be generalized to other search spaces and can only be used for the same search space. As a result, the novelty of the work is limited.

The whole algorithm is very similar to the existing NAS algorithms, while the difference is that existing NAS algorithms use the search space composed of architecture units, while the proposed algorithm is based on the segments of CG of a particular architecture. While the motivation is not clear, why the use of segments of CG is more suitable?

The adopted optimization is indeed a multi-objective evolutionary algorithm, while it is called ""A Pareto front evolution strategy."" In this case, the two objectives, i.e., accuracy and a chosen hardware-friendliness, are treated as conflicting objectives? If it is so, I would ask about the contribution of this work compared to the NAS algorithms falling into the multi-objective NAS algorithms, such as NSGA-NET.

For the example given: ""a Conv 3x3 node with incoming edges from Add and BatchNorm operations and an outgoing edge to a ReLU operation as ""conv2d3,in,add,batchnorm,out,relu""."",  the information of kernel size and stride size have been removed. This involves the encoding of architecture, and there are multiple works in this aspect. Different architecture encodings have different impacts to the performance. Clearly, the one proposed in this paper lost information regarding the architecture.

The experiments on NAS-Benchmarks are not necessary enough because their search spaces are too simple. I suggest the authors check recent works on performance predictors.

Limitations:
This paper ignores many more existing works on performance predictors, including the encoding of architectures in this topic. Compared to these existing works, the proposed algorithm in this paper has a very limit contribution.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper presents the AutoGO framework, which operates directly on the Computation Graph (CG) of a given DNN architecture. It splits the CG into segments and conducts a search process. Through extensive experiments, the paper shows that AutoGO effectively improves the performance of the top architectures in various public architecture benchmarks. Furthermore, AutoGO demonstrates its capability to automatically optimize different types of large CNN architectures and achieve enhanced results in various computer vision tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The whole framework from tokenization and mutation to estimation is reasonable and technically sound. 
2. It operates the complex problem of directly processing the computation graph and verify on several difficult tasks.

Weaknesses:
1. The verification for the ImageNet task is missing. 
2. The searched model is highly limited in the current segment database since it only contains the benchmark architectures which are not widely used in different tasks.
3. The training of the accuracy estimator highly relies on the existing collected accuracy and model data pairs. And it would be hard to transfer with only limited accuracy numbers for other datasets and other tasks. 

Limitations:
Please refer to the weakness part. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces AutoGO, an innovative method for evolving neural networks that addresses the challenges of efficiency, low power consumption, and hardware compatibility. AutoGO represents deep neural networks (DNNs) as computational graphs (CGs) comprised of low-level primitives and employs an evolutionary segment mutation algorithm. Notably, AutoGO employs subgraph mining from CGs while utilizing efficient tokenization through Byte Pair Encoding (BPE) from natural language processing (NLP) instead of Weisfeiler-Lehman (WL) kernels. For the evolutionary mutation process, AutoGO leverages neural prediction to explicitly consider positional and contextual information when replacing segments within a CG.

The experimental results demonstrate that AutoGO performs exceptionally well on NAS benchmarks and exhibits promising applications in various domains, including classification, semantic segmentation, human pose estimation, and super resolution.

In summary, this paper presents a novel and effective approach. The writing style is particularly engaging, making it a pleasure to read. I recommend accepting this paper.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:

1. The motivation behind this work is excellently articulated, providing a clear understanding of the research objectives and driving factors.

2. The paper effectively describes recent works in the field, highlighting their significance and comparing their novelty to the proposed method. The related work section is comprehensive and enjoyable to read, showcasing a thorough understanding of the existing literature.

3. This method is technically robust, demonstrating impressive performance across various evaluations. The experimental results validate its effectiveness and reliability, further strengthening the credibility of the approach.

Weaknesses:

1. Although the method presented in the paper is highly technical, the focus seems to be predominantly on the technical details rather than providing a comprehensive analysis and intuitive explanations. While I acknowledge the complexity of the method, I believe that enhancing the final manuscript with more analytical insights and intuitive discussions would significantly improve its overall quality.

2. Consider including a simplified pseudocode or algorithmic representation of the method. This would greatly facilitate the understanding of the algorithmic steps and enhance clarity for readers. A concise and structured representation of the method's flow would be beneficial in aiding comprehension.

3. I recommend revising Figure 1 to present the information in a horizontal format. This adjustment would enhance the visual clarity and make it easier for readers to follow the different components and relationships depicted in the figure.

Limitations:
Please provide a limitation section for future researchers!

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes to optimise neural networks by exploiting common subgraphs mined from existing NAS benchmarks - this is achieved by building a vocabulary from networks encoded into topologically sorted sequences and using byte-pair encoding (BPE) to obtain common sequences of operations. After that, a given neural network is segmented and different mined segments are considered as replacement for different identified segments in the given network, all done while taking care of shape propagation. Searching for the best replacement segments is done with a variation of a multi-objective evolutionary algorithm which optimizes for Pareto-efficiency using a proposed (GNN-based) PSC predictor.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
 - generalizing blocks to segments is an interesting and sensible step towards more flexible NAS
 - the proposed system seems technically advanced, taking care of quite a few corner cases in a convincing way (e.g., solving resolution propagation with linear programming)
 - encoding neural networks as sequences and using BPE is an interesting take on representing neural networks (but could be studied in more details)
 - the proposed PSC predictor seems like an interesting variation of the more standard GNN-based predictors
 - experiments are designed to support claims made in the paper (but results are somewhat hard to interpret, see below)
 - the method (at least after all one-time cost) seems fast, finishing within a few of hours at most

Weaknesses:
 - Clarity could be improved in certain places
    - ""First, benchmarks (...) requires training the new architecture from scratch"" - why is this relevant for the presented work?
    - ""(...) predictors learn using high-level cell representation (...), In contrast, AutoGO can mutate an architecture (...)"" - why are these two things compared to each other? The ability to mutate beyond an original design space (by the way, this is a tricky thing to formally define, I would appreciate an attempt at that) is orthogonal to a predictor's ability to capture spatial information of a network. Many NAS algorithms achieving similar (or even greater) coverage of architecture than the proposed work, e.g., LEMONADE or $\mu$NAS seem particularly relatable since they utilise mutations towards a similar goal as AutoGO (mutating away from the original design).
    - there seem to be some contradictory information presented regarding what operations are used, first we read (line 43): ""(...) by evolving its underlying computation graph (CG) using its original primitive operations."", but then (line 58): ""A vocabulary of segments are mined from a large number of CGs from several NAS benchmarks"", please clarify
    - The provided definition of PSC does not seem to properly cover nodes parallel to $s_i$.
    - If $s*$ has more than one input, how does the method handle assigning P's outputs to a replacement's inputs? (and analogously for S) Is it a part of the LP problem?
    - it is unclear if a randomly initialized or a pretrained architecture is expected; I couldn't find any information about pretraining a network, but then line 225 says ""we retrain all the segment replacements"" suggesting the original segments might be trained already (?); it is also unclear why this retraining of segments is needed, considering a performance predictor is used, and how it is done
 - there is some overlap between the proposed method and blockwise NAS works, such as DNA, DONNA or LANA; I think it would be better if the authors acknowledged existence of this line of work, right now it is completely ignored, despite high-level similarities
 - I think the authors should discuss in more details their choice of using toposort+BPE to mine for subgraphs - this approach is bound to fail to recognize many isomorphic subgraphs as the same segments (hinted at the beginning of Section 4), why do you think this is not a problem? How does this greedy approach compare to other alternatives?
 - Results are, generally speaking, hard to compare to the rest of the literature. More specifically:
    - apart from the common benchmarks (NB101, NB201), the paper uses HiAML, Inception and Two-Path - to the best of my knowledge, these are only used by a single, very recent (AAAI'23) paper; however, I don't see any benefits stemming from this choice while it does make comparison to other works harder
    - FSRCNN and U-Net experiments use proprietary networks and tools and, on top of that, only relative improvements are reported in some cases; rendering these experiments basically unverifiable and unusable by the community
 - at the same time, some of the reported results are not particularly convincing, such as:
    - baseline EDSR 2x upscaling performance is actually significantly worse than reported in the original 2017 (!) paper, $\Delta$ PSNR of: -1.25, -1.35, -0.93 and -3.79 for Set5, Set14, B100 and Urban100, respectively. Results are better for DIV2k (I have to assume the authors mean DIV2k validation set), but that's just one dataset out of 5 (and the one used for training), 
    - the proprietary FSRCNN also achieves significantly worse results than its parent model, while requiring approx. 150x more FLOPS (!!!)
    - ResNet-50 and ResNet-101 baselines are also worse than reported in the original paper (ImageNet), not to mention any recent improved training recipes
 - it is actually not very clear, but following on the information presented in Section 4.1, it appears that the results in Table 2 were obtained by using a predictor pretrained on 80% of the data available for each benchmark (at least that's the only information we are presented about predictor training, so I'm assuming that's the case for subsequent sections as well); this means that the improvements presented are actually occupied by a very high, hidden cost of having lots of in-domain training data, while in many cases they are not significant
 - perhaps I missed that, but I couldn't find information about the cost of all the pretraining etc.


Limitations:
No discussion about limitations - the presentation is actually quite one-sided, by only considering benefits of the proposed method. For example, using BPE is said to ""bring several benefits"" over methods like WL but not a word about possible downsides (worse handling of isomorphic graphs).

Rating:
4

Confidence:
4

";1
7FitZnnnu8;"REVIEW 
Summary:
This paper uses optimal transport for learning the parameters of a DAG structure that represents a Bayesian network given samples drawn from it (data). In the proposed algorithm the data can be incomplete (i.e. some random variables may be latent). 
The algorithm can be seen as a generalization of the existing ""Optimal transport-based divergence minimization"" on target distributions that are factorized as a product of conditional univariate densities. 
  

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The generalization of the Optimal Transport to Bayesian networks is interesting and as far as I can say novel (though, more or less straightforward).
2. The paper is well-written (In particular the first two sections).
3. Several experimental models are presented.  
  

Weaknesses:
1. This method is only suitable for learning the parameters of Bayesian network DAGs. It can not be used for learning the DAG structure. 
2. The description of the proposed algorithm (section 3 of the draft) is quite compact and many details are missing.
More details are presented in the supplementary material (Section B). Still, even there, the cost function and push-forward divergence measure that are used in the paper's experiments are not spelled out.  

 
 

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this submission, the authors propose an optimal transport-based method to infer the parameters of probabilistic directed graphical models from partial observations. 
In particular, given a DAG associated with the target model, the proposed method reparameterizes the probability of a node conditioned on its parents by an encoder with external perturbation. 
Accordingly, a stochastic decoder is applied to map each node to the conditional density of its parents. 
The above two modules lead to a model with an auto-encoding architecture, which can be learned like a Wasserstein autoencoder (WAE), as shown in Eqs. (2, 3).
Experiments on the inference of LDA, HMM, and discrete representation models demonstrate the potentials of the proposed method.

--- After rebuttal ---

Thanks for the authors' efforts in the rebuttal phase. After reading other reviewers' comments, my main concern about this work is still its similarity to WAE, especially the theoretical part. Although the authors claimed that WAE can be viewed as a special case of this work, in my opinion, it is more likely that this work is a special case of WAE. I am satisfied with the other part of this work, so my final score is kept as ""borderline accept"".

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow.

2. The proposed method is reasonable --- the objective function is based on the theoretical part of WAE, whose rationality has been guaranteed. In addition, the implementation of the proposed method is simple.

3. The authors consider various applications, demonstrating the universality of the proposed method.

4. The limitations of the proposed method are discussed, and the potential solutions are provided at the same time.

Weaknesses:
1. If my understanding is correct, Theorem 1 in this submission is a special case of Theorem 1 in the WAE work [a]. The final objective (Eq.(3)) approximates the Wasserstein distance by relaxing the constraint of phi_i to a regularizer, which is also similar to the strategy of WAE. The authors should discuss the connections and the differences between the proposed method (and theory) and that in [a]. 

[a] Tolstikhin, Ilya, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. ""Wasserstein Auto-Encoders."" In International Conference on Learning Representations. 2018.


2. As the authors mentioned, the proposed method leverages the amortization strategy to reparameterize the conditional distributions. Therefore, in the experimental part, the authors should consider some amortization methods as baselines, e.g., those in [b, c, d].

[b] Kim, Yoon, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. ""Semi-amortized variational autoencoders."" In International Conference on Machine Learning, pp. 2678-2687. PMLR, 2018.

[c] Agrawal, Abhinav, and Justin Domke. ""Amortized variational inference for simple hierarchical models."" Advances in Neural Information Processing Systems 34 (2021): 21388-21399.

[d] Huang, Chin-Wei, Shawn Tan, Alexandre Lacoste, and Aaron C. Courville. ""Improving explorability in variational inference with annealed variational objectives."" Advances in neural information processing systems 31 (2018).


3. The datasets used in the experimental part are relatively simple and small. Especially in the experiments of discrete representation learning, I wonder 1) whether the proposed method can deal with images with larger sizes, e.g., face images in CelebA, and 2) besides reconstruction, whether the proposed method can generate images with tolerable qualities.

Limitations:
The authors discussed the limitations of using the amortization strategy at the end of the submission. 
Some potential solutions are proposed at the same time.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a method for learning parameters $\theta$ of a DAG within an optimal transport (OT) framework, minimizing the Wasserstein distance between the data distribution $P_d$ and the model distribution $P_\theta$ in $\theta$. The Kantorovich formulation of this problem is a minimization of an expected cost $c(X,Y)$ over all joint distributions on $X,Y$ such that marginally $X \sim P_d, Y\sim P_\theta$; this is implemented in practice by empirically drawing $X_i$ from the data, and then $PA_{X_i}$ conditionally on $X_i$ to satisfy $PA_{X_i} \mid X_i \sim P_\theta(\textrm{PA}_{X_i})$ through the use of stochastic “backward” mappings. This makes optimization tractable over the space of backward mappings $\phi$ and model parameters $\theta$, so long as the constraint above is relaxed to a regularization term.   


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* DAGs represent an extremely rich family of models, and the work also generalizes to settings with unobserved variables, making this method easy to apply in a variety of settings.
* Unlike variational methods, evidence bounds need not be computed; the proposed method is more “direct” in this sense. 
* The final optimization objective is easy to compute and computationally cheap.
* The framing of the problem from the lens of OT is novel to my knowledge, and provides an interesting formulation for optimization.
* The most significant strength of the paper is the evaluation of the method on a rich test suite of interesting problems such as LDA and Poisson time series segmentation. Comparisons show that the proposed method outperforms existing methods such as Batch EM and SVI in a variety of scenarios and metrics.


Weaknesses:
* The method reuqires that the random variables be reparameterizable in the sense of the equation at line 122 (this equation should be numbered); this may limit the family of joint distributions that can be considered. Though the authors say on line 167 that discrete variables can be used, reparameterization is tricky in these cases (as the authors acknowledge in the limitations section). 
* The backward maps $\phi_i$ are a confusing quantity (and potentially hard to fit); see the Questions section. This could be due to a lack of understanding of my part. As I understand it, however, it raises questions about the efficacy of the proposed method.
* The formality of the OT framing is appealing, yet this formality is ultimately dropped for a regularized analogue that does not solve exactly the same problem that is posed.
* Some notation is confusing; for example, does $PA_{X_i}$ include exogenous variable $U_i$? The discussion around line 120 and line 147 suggest so, but then notationally the equations at line 122 and 136 suggest otherwise.


Limitations:
The settings where the work can be applied are discussed by the authors. A limitations section is included that proactively assesses some limitations of the proposed method. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a framework for learning the parameters of directed graphical models based on the idea of fitting by selecting the parameter values that minimize the Wasserstein distance (WD) between the data and model distributions. They prove (Thm. 1) that these distances can be characterized as the result minimizing a cost functional over a family of constrained stochastic 'backwards mappings', which yields a solvable objective when the constraint is relaxed and a regularization term is added to the objective.  

The approach is illustrated via a series of experiments, including 3 real-world datasets, in which the proposed OPT-DAG method outperforms various baselines across disparate tasks. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I'm am not particularly familiar with the literature on DAG learning, nor on learning with 'optimal transport' objectives, so the extent of the originality of this paper is impossible to gauge. Operating under the assumption that this is the first work to introduce an OPT objective in this context, I would feel confident in stating that this is a fairly significant innovation. 

The presentation of the paper is, in my opinion, of a high standard (modulo one or two concerns that I will voice in the next section). The appendices provide a considerable depth of exposition of their procedure, and I am satisfied that the most pressing immediate concerns are addressed, either there or in the main body. 

Weaknesses:
I believe that the experimental section as well as the discussion could be improved. Again, I am unaware of what constitutes the benchmarking standard for DAG learning methods, but it seems to me as though the following are not adequately addressed:

a) The authors mention that their goal is not to achieve state-of-the-art performance, but rather to demonstrate the inherent versatility of their method. Situations in which their method might not be feasible are alluded to in Section 5 (Limitations), but the discussion here is extremely terse. Do these situations pose problems for competing alternatives as well? The chosen examples strike me as being somewhat simple. Do the authors claim that these examples are roughly representative of DAG learning problems generally? 

b) For the topic evaluation example (239-250) OPT-DAG is outperformed by the baselines in the 'diversity' metric in all three datasets, and outperformed on 'coherence' on the DBLP data. These results are reported with precisely no discussion or explanation. 

c) Section 4.2, first part (251- 266) could use some clarification. For example, the authors say ""we generate a synthetic dataset D with 200 observations at rates {12, 87, 60, 33} with change points occurring at times (40, 60, 55)."" If the model is as they say, (with a change of state happening with probability $1-p$, of which the lowest value is when $p = 0.95$, then a) why are there so few change points? At $p = 0.95$ would we not expect 10? Perhaps I am confused about what the authors are doing. Have they fixed the dataset, and computed the estimates of the rate parameters assuming that $p$ is fixed at the values indicated in the table (i.e. they fit the model 6 times, each time with a different assumed p)? Also, averaged over just the values $p = 0.75, 0.95$, OPT-DAG is actually inferior to MAP. Again, no explanation or discussion is forthcoming. 

d) Generally, it is not clear to me if the baselines being compared against are the most appropriate. For instance, in the final example, VQ-VAE is used as a baseline, and its poor performance is linked to a phenomenon called 'codebook collapse'. The paper provided as a reference in fact proposes an extension to the vanilla version (of VQ-VAE) that they seem to be comparing against, which seems to indicate that not only is their baseline not state-of-the-art, it is in fact very well known to not be so... 


Limitations:
Limitations are discussed, but as per my comment above, it does not seem to me as though this discussion is adequate. The authors seem to have a variety of situations in mind in which their method will either not work or not be competitive, and a more explicit discussion here would be welcome. 

Rating:
6

Confidence:
3

";0
o8FCeFpipg;"REVIEW 
Summary:
This paper studies the polynomial filters in GNNs. Specfically, the authors propose a Adaptive Power GNN which employs exponentially decaying coefficients. A theoretically generalization  analysis of  the proposed framework is conducted. Experiments demostrate the proposed method can outperform some baselines on the selected datasets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The background provided is detailed and aids readers in comprehending the nuances of the paper.
2. The authors offer a generalization analysis of the proposed method.
3. The paper is well-written and easy to follow.


Weaknesses:
1. The primary concern is the lack of novelty. The concept of polynomial filters in GNNs is well-established. Additionally, the paper looks at infinite orders, a domain where any function can be approximated by infinite polynomial functions. Also, the topic of GNN generalization has already been examined in several papers, such as [1], [2], and [3].
2. The method proposed in this paper comprises decoupled GNNs, which implies that the framework and analysis may not be applicable to coupled cases, such as GCN and GAT.
3. The hyperparamters $\alpha, \beta$ are hard to choose. For instance, GPR-GNN struggles to learn these hyperparameters effectively without proper initialization. How do the authors suggest these hyperparameters be selected?
4. The authors opted for fixed data splits across all datasets, which may predispose the model to overfitting on the valid/test sets. Have the authors considered random data splits? Additionally, the study could benefit from incorporating larger datasets like the OGB datasets.

[1] Baranwal, Aseem, Kimon Fountoulakis, and Aukosh Jagannath. ""Graph convolution for semi-supervised classification: Improved linear separability and out-of-distribution generalization."" arXiv preprint arXiv:2102.06966 (2021).

[2] Verma, Saurabh, and Zhi-Li Zhang. ""Stability and generalization of graph convolutional neural networks."" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2019.

[3] Ma, Jiaqi, Junwei Deng, and Qiaozhu Mei. ""Subgroup generalization and fairness of graph neural networks."" Advances in Neural Information Processing Systems 34 (2021): 1048-1061.


Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors propose a spectral GNN with geometrically decaying weight $\alpha^k$ and $P$-hop polynomial basis. They study the Lipschitzness and generalization bound of the model. Experiment results demonstrate the good performance of the proposed method over baselines

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(+) Interesting generalization analysis.

Weaknesses:
(-) The recent spectral GNN literature is not discussed or compared, where the advanced methods are also polynomial graph filters but with a more careful design (i.e., different basis).

(-) The motivation of the proposed new designs $\alpha$ and $P$-hop seems not convincing

(-) The spectral GNN baselines are completely missing in the experiment.

(-) The experiments do not demonstrate the benefit of the proposed design besides prediction accuracy. Since the authors talk about polynomial graph filters, I wonder how the proposed methods can learn different graph filters.

## Detail comments

I have mixed feelings about this paper. On one hand, I find the discussion on requiring the Lipschitz graph filter and generalization analysis interesting. On the other hand, I feel the authors did not conduct a comprehensive literature survey on the recent development of spectral GNNs, which makes me feel the paper is a bit outdated. 

Note that the idea of learning polynomial graph filters has been extensively studied recently, as capable of learning high-pass graph filters enables good performance on heterophilic graph datasets [2]. Since GPR-GNN (using a monomial basis), many advances have been made. For instance, Bernnet [He et al. 2021] argue that using Bernstein polynomial basis has better properties. [Wang et al. 2022] further improve this line of work by using Jacobi polynomial basis. Both of them not only demonstrate great performance on homophilic and heterophilic datasets but also conduct experiments to directly examine how well spectral GNNs can learn each target graph filter such as low-pass, high-pass, band-pass and more. That is, these works show how to make $g_{\theta}$ learn better with different polynomial basis and thus constraints on the coefficients. I feel the proposed APGNN is very relevant to this literature and should be compared with them in both related works and experiments. Furthermore, Since APGNN is claimed to be a ”universal principle”, I wonder how well it can learn on heterophilic datasets and different graph filters (i.e., the experiment in [Wang et al. 2022] Table 1).

Also, I find the motivation for using $\alpha$ and $P$-hop not very convincing. For $\alpha$, I agree that if we set $K = \infty$ then the geometric decaying weight is needed. However, as the authors also mentioned in Section 4, we cannot use $K= \infty$ in practice. In this case, I wonder what benefit would $\alpha$ bring to us. Can not we just learn it automatically? On the other hand, the argument for motivating the use of $P$ seems problematic. Intuitively, we should compare two methods with the same number of hops ($K$). In this point of view, $P$ does not help in any sense. Also, if we fix $T$ (the maximum hop range), then the effect of increasing $P$ is merely reducing $K = T//P$, which apparently will make the generalization bound smaller. However, this is at the cost of increasing the term $\hat{R}$, otherwise one should simply set $K=0$. 

Even from the experiment section, we can see that choosing $\alpha\approx 1$ gives roughly the same best-tuned results (Figure 3-(b)). A similar observation can be made for $P=1$ in Figure 3-(c), albeit the difference seems to be a bit larger. According to the argument above, I conjecture that the benefit of $P$ comes from having a smaller number of learnable weights (in the graph filter $g_{\theta}$). It is interesting to investigate deeper of how we can improve along this line but the current results and explanation are unsatisfactory. At last, I also hope the authors can repeat experiments of Figure 3 on heterophilic datasets and even on experiments of learning different graph filters (maybe too much to do though). I feel the finding therein can motivate the authors to some ideas to further improve the work.

Minor:

How can the authors answer ""yes"" in the reproducibility but not include their code (only after acceptance)? I feel the authors should be more serious about answering the checklist questions.


## References

[He et al. 2021] Bernnet: Learning arbitrary graph spectral filters via Bernstein approximation, He et al. NeurIPS 2021.

[Wang et al. 2022] How powerful are spectral graph neural networks, Wang et al. ICML 2022.


Limitations:
I do not find potential negative social impact.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper theoretically studies the criterion for the graph filter formed by power series using Lipschitz smoothness and then proposes a novel Adaptive Power GN architecture. Some convergence and generalization analyses are covered. Experiments also show the advantages of the proposed methods, with some ablation studies on some parameters. Overall, it is a work that combines theory and practice. I think this work can benefit from some revisions. I am going to raise my score if my concerns are addressed. I would like to see a revision in the manuscript (it is OK to appear in the appendix) or a plan for the revisions (if the revision uploading is not allowed).

------------------------------------------------------------------------------------------------
After rebuttal, I increased my score to 6, which is based on my first impression and the responses. I am generally satisfied with the responses.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. It is a paper that combines both theory and experiments.  I like the logic from theory to practice. Hence, the writing is excellent and clear to follow.
2. As a reviewer from the theoretical area, I think the generalization analysis on non-GCN graph neural networks is novel and exciting. One great part is that this analysis seems to cover a lot of existing graph neural networks. So it is general. 
3. The experimental results support the theory and show the advantages.

Weaknesses:
1. Some parts of the paper are not very clear. It is mainly around lines 235 to 242. (a) For example, in line 239, it says, ""larger $\alpha$ leads to a higher bound. "" It is not clear what this ""bound"" refers to. At first, I thought it referred to $1-\alpha^K$ in line 235 or $1-\alpha^{T/P}$ in line 237. Later, I feel it should be the last two terms (or the second to the last term) in Equation 24. This needs clarification. (b) Another thing is at the end of this paragraph, it claims, ""$\alpha$ should be moderate to...$. The discussion about small $\alpha$ is missing. I guess it is because a small $\alpha$ makes the filter trivial and not powerful (line 157). This also needs clarification. 

2. Some discussions about theoretical works of generalization analysis on Graph Neural Networks are needed. I would like to know the comparison and theoretical novelties beyond existing works. Here are some recent related works.

[1] Esser et al., 2021, ""Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks.""

[2] Cong et al., 2021, ""On Provable Benefits of Depth in Training Graph Convolutional Networks.""

[3] Li et al., 2022, ""Generalization guarantee of training graph convolutional networks with graph topology sampling.""

[4] Zhang et al., 2023, ""Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks.""

[5] Tang et al., 2023, ""Towards Understanding Generalization of Graph Neural Networks.""

Limitations:
There is no potential negative societal impact of this work. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
In GNNs, designing a graph filter or propagation mechanism plays a critical role. Spectral-based GNNs formulate graph filters in the graph Fourier domain, and those filters are usually in the form of polynomials or power series. The manuscript argues that a well-defined graph filter should be convergent when represented as power series and have desirable analytic properties such as the Lipschitz continuity. Graph filters of four existing GNNs are analyzed based on the proposed conditions; it is shown that they have some limitations. The proposed APGNN introduces an exponentially decaying rate to coefficients of the learnable graph filter. Due to the decaying rate, the filter of APGNN is guaranteed to converge, and APGNN can theoretically be extended to an infinite-depth GNN. The filter of APGNN also satisfies the Lipschitz continuity, implying that the model is stable and robust. A mathematical analysis of the generalization bound of APGNN is provided. In practice, a truncated polynomial instead of the power series is utilized as the filter of APGNN to avoid the infinite number of learnable parameters. A multiple P-hop strategy, which uses the P-th power of the adjacency matrix, is introduced to enlarge the receptive field of the graph filter while maintaining the same computational complexity. Experimental results on five real-world benchmark datasets show that APGNN has comparable performance to 11 baselines in accuracy.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The manuscript points out that there exist inconsistencies between GNN models and their infinite-depth versions. A criterion for the polynomial-based graph filter is proposed with two constraints: the convergence and the Lipschitz continuity of the graph filter. The criterion reflects the stability of a GNN model with respect to the input graph and the consistency between a GNN and its infinite-depth version.

2. The motivation of APGNN is well described. The criterion for a polynomial-based graph filter is proposed, and the formulation of APGNN is provided as an instantiation of a graph filter satisfying the criterion. A truncated polynomial filter is suggested for practical use where the number of parameters should be finite.

3. The effectiveness of the P-hop strategy is shown in experiments. In Figure 3 (c), the P-hop strategy allows the model to keep the performance while decreasing the computational cost by omitting the terms that are not multiple of P from the graph filter polynomial.

Weaknesses:
1. There should be a comparison between APGNN and other GNN methods with an infinite depth. Existing approaches utilize a residual connection [1] or formulate the state of equilibrium [2, 3] to model GNNs with an infinite depth. Those methods should be theoretically and empirically compared to APGNN. 

   [1] Chen et al., Simple and Deep Graph Convolutional Networks, ICML 2020.

   [2] Gu et al., Implicit Graph Neural Networks, NeurIPS 2020.

   [3] Liu et al., EIGNN: Efficient Infinite-Depth Graph Neural Networks, NeurIPS 2021.

2. The node classification accuracy of APGNN is relatively low compared to the existing GNN models such as GCNII [1] and G$^2$CN [4]. For example, the accuracy of GCNII is 85.5 on Cora, whereas the accuracy of G$^2$CN is 73.8 on Citeseer. In addition, GNN models with an infinite depth should be compared to APGNN.

   [4] Li et al., G$^2$CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters, ICML 2022.

3. The authors mentioned the hyperparameter sensitivity of PPNP in lines 130-131: ""However, the performance of PPNP is heavily dependent on the hyperparameter $\beta$, which must be carefully tuned to achieve optimal performance."" However, the same applies to APGNN since the node classification performance heavily depends on $\alpha$, a decay weight. Figure 3(b) shows that the accuracy gap between the best and the worst cases is more than 20% on Cora.

4. The polynomial order K is fixed to 10 for some baseline methods, such as ChebNet and DAGNN. However, the optimal value of K can vary depending on the baseline methods. In [1], the node classification results with various depths (Table 3 of [1]) show that the optimal depths are different for each combination of model and dataset. Therefore, the authors should show the node classification results with various polynomial orders, as in [1], or tune the polynomial order for each baseline method, as in [5].

   [5] Liu et al., Towards Deeper Graph Neural Networks, KDD 2020.

5. Minor Comments
- In line 54, $i = [n_l]$ should be modified to $i \in [n_l]$.
- There is an inconsistency between Equations (2) and (3).
- In line 93, ""L-Lipschtiz"" should be changed to ""L-Lipschitz"".
- In line 136, “DAGGN” should be modified to “DAGNN”
- In Equation 13, $(-1)^k$ should be modified to $(-1)$.
- $\rho$ indicates the spectral radius in Theorem 1, whereas it represents a probability measure in Section 4.
- In Lines 192-195, there is no description and definition of $c_{\mathcal{X}}$, $c_{\mathcal{U}}$, $c_{\mathcal{L}}$.
- In line 206, ‘the hypothesis set over __ is described as’ should be modified to ‘the hypothesis set over \mathcal{X} is described as’.
- In line 266, APPNP is mentioned twice.

Limitations:
1. There might be some cases where capturing the long-range dependencies is essential to understand the context of the given graph [6]. However, the proposed model might not be appropriate for those graphs. To capture long-range dependencies, either the decay weight $\alpha$ has to be large enough, or $\beta$ assigned to the low-order should be small. The former increases the generalization bound of the model, and the latter might lead to a vanishing gradient problem.

   [6] Dwivedi et al., Long Range Graph Benchmark, NeurIPS 2022.

2. The performance of the model is highly sensitive to the choice of $\alpha$. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a regularized learning framework for creating deep Graph Neural Networks (GNNs), including the Adaptive Power GNN (APGNN) that uses exponentially decaying weights to aggregate graph information of varying orders. The proposed multiple P-hop message passing strategy efficiently perceives higher-order neighborhoods, and the APGNN can be extended to an infinite-depth network. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed APGNN model effectively captures higher-order neighborhood information, which is a crucial aspect of many graph-based tasks. This is a significant contribution to the field.
2. The regularized learning framework utilized in the proposed model provides a valuable theoretical guarantee for convergence. This adds to the credibility and reliability of the approach.
3. The experimental results presented in the paper demonstrate that the proposed method outperforms other state-of-the-art GNN models on several benchmark datasets. 

Weaknesses:
1. It would be beneficial for the paper to include a more detailed comparison with other recent GNN models that also aim to capture higher-order neighborhood information. This would provide readers with a clearer understanding of how the proposed APGNN model compares in terms of performance and capabilities.
2. The paper lacks a detailed analysis of the computational complexity of the proposed method. Considering the potential concerns related to large-scale graphs, it is important to provide insights into the computational requirements of the model.
3. Similarly, the paper should include a more detailed analysis of the sensitivity of the proposed method to hyperparameters. Understanding how the model's performance varies with different hyperparameter settings is crucial for practical implementation.
4. The paper lacks a thorough analysis of the interpretability of the learned graph filters. Providing insights into the interpretability of the model's learned representations would enhance the understanding and trustworthiness of the proposed approach.


Limitations:
Considering the increasing importance of robustness in real-world applications, it would be valuable to evaluate the model's performance under adversarial scenarios and discuss its limitations and strengths in this regard. 

Rating:
5

Confidence:
4

";0
cx8lw7WXY4;"REVIEW 
Summary:
The research introduces ITEM3D, a model that enhances texture editing in 3D modeling, addressing challenges like complexity and text ambiguity. Leveraging a diffusion model, ITEM3D uses images to bridge text and 3D representations, optimizing texture and environment map. It employs a relative editing direction, reducing noise difference and semantic ambiguity between source and target texts, and adjusts direction during optimization to limit texture domain deviation. Experiments show ITEM3D outperforms previous methods and can effectively control lighting.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The research introduces an optimization pipeline for texture and environment map editing in 3D models, adhering to text prompts. 
+ This paper presents a relative direction diffusion-based approach for 3D texture optimization, mitigating issues of noisy details and inconsistent appearances resulting from semantic ambiguity between texts and images. 


Weaknesses:
1. This paper asserts its realism and efficiency in generating new textures through optimization, yet its claims are evaluated solely on rendered images. It remains uncertain whether this method is applicable to in-the-wild objects. I suggest that the authors conduct additional experiments using real 3D objects, such as those in the DTU dataset. Furthermore, I find a lack of supportive material regarding its efficiency claims. Given that the method works based on optimization, I question whether it can truly generate high-quality textures swiftly, as stated in the introduction.

2. The introduction lacks clarity and adequate context. The paper outlines two challenges when applying the diffusion model to 3D objects, but it doesn't discuss any related works aimed at addressing these challenges. This omission makes it difficult to gauge the novelty of the proposed method in relation to existing solutions.

3. The proposed relative direction loss appears strikingly similar to that of NeRF-Art[1], albeit the latter employs the CLIP model to implement this constraint. The authors should highlight the distinguishing factors between their method and NeRF-Art in the introduction and add a direct comparison.

	[1] Wang, C., Jiang, R., Chai, M., He, M., Chen, D. and Liao, J., 2023. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics.

4. In Figure 4, the examples provided are not representative enough. It would be beneficial to showcase results with brown, golden, and porcelain materials. The current examples make it difficult to evaluate the method's performance.

Limitations:
This paper has acknowledged its limitations.

Rating:
4

Confidence:
5

REVIEW 
Summary:
Given a set of multi-view calibrated images, the authors aim to first reconstruct the 3D objects based on the input images and then edit them via a pre-trained diffusion model. To accomplish this, they developed ITEM3D, which utilizes a differentiable marching tetrahedron (DMTet) as its 3D implicit representation. To achieve the editing task, the authors proposed two methods: (1) relative direction based optimization that respects both descriptive and editive prompts, and (2) a gradient direction adjustment that integrates changes from previous iterations to improve network robustness. Qualitative and quantitative experiments demonstrated ITEM3D's superior capabilities compared to current state-of-the-art methods.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The strengths of this paper can be summarized as:
1. The paper employs both descriptive and editive directions to effectively address the editing task. Although it's adapted from the recent DDS paper, I believe the tricks proposed by the authors are useful.
2. Additionally, the authors integrate changes to adjust the gradient direction, enhancing the network's robustness.

Weaknesses:
The weaknesses of this paper can be summarized as:
1. The paper overlooks several noteworthy recent studies, such as Make-it-3D, DreamBooth3D, DreamAvatar, Instruct-NeRF2NeRF, Rodin, and so on, which should be included in the references.
2. The authors fail to provide a thorough discussion and qualitative comparisons with Instruct-NeRF2NeRF, the most recent and relevant paper.
3. the ablation study presented in Figure 4 may not be immediately apparent. The ""w/o adj"" may be a better result for the ""A brassy cattle"" scenario.

Limitations:
Limitations and potential impacts have been discussed in the submission.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes ITEM3D, a model for automatic 3D object editing according to text prompts. ITEM3D bridges the gap between 3D representation and natural images using rendered images. It optimizes disentangled texture and environment maps using a relative editing direction to bypass ambiguous text descriptions. It gradually adjusts the editing direction to reduce deviation caused by texture projection. Results demonstrate ITEM3D outperforms SDS-based methods on various 3D objects. It also allows for explicit control over lighting with text-guided relighting. This paper also suggests directly applying the diffusion model to 3D objects is challenging due to conflicts in optimization goals and extreme semantic bias.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Optimizing disentangled texture and environment maps using a relative editing direction to bypass ambiguous text descriptions looks interesting. Results demonstrate ITEM3D outperforms SDS-based methods on various 3D objects. The explicit control over lighting with text also looks interesting. 

Weaknesses:
One major weakness of the proposed method is the lack of comparisons with other important related works, such as CLIPNeRF, ARF[1], and SINE[2]. While CLIPNeRF is cited, the others are not. Additionally, there are several related works that do not use text descriptions for editing NeRF, including NeRV[3], NeRD[4], NerFactor[5], and EditNeRF[6], that are also missing from the comparison. Another related line of work, SDFusion[7], is not discussed at all. Although the authors may have overlooked recent related works in the crowded field of text-based editing with NeRF + Diffusion models, it is crucial to compare the proposed method to CLIPNeRF and ARF to understand its advantages. Comparisons to intrinsic image decomposition-based methods are also important, especially for relighting, to determine the strengths and weaknesses of the proposed method. While the expectation is not for the method to outperform NeRD or NeRFactor, comparing it to these methods is important to identify what needs to be done to achieve similar results.

Another important and simple baseline is missing: how do the results compare to a simple 2D-based image editing method without using explicit 3D texture and illumination map optimization?  

[1] Zhang, Kai, et al. ""Arf: Artistic radiance fields."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. \
[2] Bao, Chong, et al. ""Sine: Semantic-driven image-based nerf editing with prior-guided editing field."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. \
[3] Srinivasan, Pratul P., et al. ""Nerv: Neural reflectance and visibility fields for relighting and view synthesis."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. \
[4] Boss, Mark, et al. ""Nerd: Neural reflectance decomposition from image collections."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.  \
[5] Zhang, Xiuming, et al. ""Nerfactor: Neural factorization of shape and reflectance under an unknown illumination."" ACM Transactions on Graphics (ToG) 40.6 (2021): 1-18. \
[6] Liu, Steven, et al. ""Editing conditional radiance fields."" Proceedings of the IEEE/CVF international conference on computer vision. 2021. \
[7] Cheng, Yen-Chi, et al. ""Sdfusion: Multimodal 3d shape completion, reconstruction, and generation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.


Limitations:
The paper lacks several key related works and comparisons to them, including simple 2D-based comparisons.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper tackles the task of texture editing for 3D models with the guidance of text. The task aims to manipulate the surface properties based on the text guidance to create corresponding visually realistic appearance. The prior works that this work targets to improve upon is the SDS loss related methods. 

In high-level, this paper represents the scene with a 3D representation, differentiably render the representation into 2D images, and use a pretrained diffusion model to guide the editing through an optimization process. The proposed method contains two key components. First, the 3D scene is a decomposed representation, containing triangle meshes, texture map, and environment map (following the prior work nvdiffrec). Second, the paper explores for a better editing direction during the optimization process. Specifically, the relative direction and a gradual direction adjustment scheme are proposed to achieve better editing results. 

Evaluation is done on synthetic object data. The results are qualitatively visualized, and quantitatively evaluated with CLIP-based scores and user study. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
This paper contains a good amount of originality: 

- The task definition is well motivated. 
- The combination of the decomposed representation in nvdiffrec is novel and a well-grounded design choice. 
- The analysis and method design of edit direction are informative and sensible. 


Quality: 

- The qualitative performance of the current model still has a significant room for improvements. But it’s acceptable considering the challenges of the task. 

Clarity: 

- This paper is well written and easy to follow. 
- The analysis is informative and convincing. 

Significance: 
- The functionality this paper achieve is among the first works in the literature. 


Weaknesses:
- The quantitative evaluation is relatively weak. There is no quantitative ablation for the proposed method designs. 

- The “illumination-aware” in the title is slightly misleading -- the major results and the demo video do not show editing capability for illumination. Fig.5 is one experiment to show simple editing of light intensity. But the quality is still preliminary, and it does not show evidence to edit lighting effects such as reflections and shadows. It fits the acronym quite well, but maybe not the most accurate in describing the method. Maybe is it more like “reflectance-aware”? 

- The method adopts the decomposed representation following nvdiffrec. Can the proposed method properly handle the ambiguity of material and light? To give a concrete example, if the original reconstructed object is under a flat and uniform lighting, but the intended edit result is with a directional lighting and thus contain strong shadow. In this case, will the proposed method still bake the shadow directly into the texture map? If this is the case, it’s best to also explicitly mention into main paper as a limitation. 

- The current editing is mainly done on synthetic objects. Can it handle real world reconstructed objects? 

- The editing is often quite simple, such as editing of color or brightness. How does the model work for more complex editing? E.g. add a mustache or a hat to the cow / fish? 



Limitations:
The authors discussed some limitations at the end of the paper. I would encourage the authors to also properly discuss the model's ability and limitation on editing of lighting effects. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces ITEM3D, an illumination-aware model for automatic 3D texture editing based on text prompts. The authors address the challenges of complex 3D models and ambiguous text descriptions in texture editing. They propose leveraging the power of the diffusion model and optimizing disentangled texture and environment map representations using rendered images as an intermediary. The paper introduces a relative editing direction based on noise differences between source and target texts to improve appearance consistency. The authors also gradually adjust the editing direction to mitigate unexpected deviations caused by texture projection. The contributions include an efficient optimization pipeline for texture editing, the introduction of the relative editing direction, and the proposal of gradual adjustment to handle deviations.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper presents a novel approach to automatic 3D texture editing based on text prompts. It combines the power of the diffusion model with the use of rendered images as intermediaries, introducing a relative editing direction and gradual adjustment techniques. This combination of methods and the focus on addressing challenges specific to 3D modeling make the approach original and distinct from previous works.

The paper is well-written and provides detailed explanations of the proposed method and its components. The authors offer clear insights into the challenges of texture editing in 3D models and provide a sound rationale for their approach. The experiments and evaluations are thorough, demonstrating the effectiveness of ITEM3D in generating visually natural textures and enabling relighting. The use of qualitative and quantitative analyses strengthens the quality of the results.

The paper addresses an important task in 3D modeling—automatic texture editing—and offers a practical solution with significant implications. By leveraging the diffusion model and incorporating text prompts, the proposed method enables users to manipulate the surface properties of 3D models in a realistic and visually appealing manner.

Weaknesses:
While the paper has several strengths, there are also a few areas where it could be improved:

The paper mainly focuses on the editing of textures in synthetic nerf dataset. However, it would be valuable to investigate the generalization of the proposed method to more complex 3D models and real world scenes, such as those with intricate geometry or high levels of detail. Assessing the performance and scalability of ITEM3D on such complex models would enhance the practicality and applicability of the proposed method.

While the paper mentions the importance of efficiency in texture editing, it would be beneficial to provide more insights into the computational requirements and optimization strategies employed by ITEM3D. Specifically, discussing methods to improve the computational efficiency, reducing memory consumption, and addressing scalability issues would enhance the practical usability of the proposed method.

Comparison with Alternative Approaches: The paper compares the proposed ITEM3D with diffusion-based editing methods and mentions their limitations. However, it would be valuable to include a more comprehensive comparison with other state-of-the-art methods for texture editing in 3D models, such as Instruct-NeRF2NeRF. This would provide a clearer understanding of the advantages and limitations of ITEM3D in relation to existing alternatives.

Limitations:
The limitation is discussed.

Rating:
5

Confidence:
3

";0
iQhtJD1l7C;"REVIEW 
Summary:
Model-based planning agents such as MuZero leverages a learned model of the environment dynamics to learn a policy to follow in the actual environment. By planning using the learned model dynamics, the agent may generate a stronger policy than when limited to only experience from the real environment. The drawback to this approach lies in the agent being forced into decision states it has not seen before. The agent's model will have incorrect value predictions for these states and the planning stage will result in weaker or sub-optmal policies in these states. This paper proposes an exploration scheme that ensures the agent explores sub-optimal regions of the search space to build better value estimates of the states within.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
**Originality:** The ideas introduced in this paper have not been published before as far as I can tell.

**Significance:** The algorithm proposed by the authors is nominally significant. It is a naive implementation of the explore-first-then-exploit type of algorithms.

**Clarity:** The writing itself of the paper was fair but could use considerable improvement.

**Quality:** It is hard to assess the strengths in the quality of this paper.

Weaknesses:
First off, I think it needs to be said that this is not a paper that can be improved upon with changes in the writing or presentation. The algorithm introduced is only a minor change to an already existing algorithm and it provides very little new insight.

That being said, I do want to encourage the authors to continue learning and working in the field. For the authors, I would recommend attending workshop or course on technical writing and building a stronger background in RL and planning.

For the submission specifically, I have a number of critiques. However, I will touch on only a few because as I've said this is not a paper that I think can be reworked and resubmitted:

1) The introduction of the paper goes well into page 3. A strong problem statement and motivation section can go along way in emphasizing the importance of the work but the paper needs to focus more on the actual contributions.

2) A similar note on the background and related works section. A lot of it deals with the history of the body of the work and not their relation to this particular paper. Furthermore, the background section does not provide the actual background needed for the paper. A good background section typically formalizes the problem state; clarifies any assumptions being made; defines notation and how the problem is being modelled; and provides a primer on the literature specifically needed to understand the rest of the paper. This submisison fails in those regards.

3) It is difficult to understand the algorithm introduced because nothing has been defined.

4) The experiments are limited a single small domain and does not give any indication of how the approach scales to larger ones.

5) The experiments do not provide comparisons to other baselines, that is, to other algorithms that take the explore-first approach which are, in my opinion, a direct competitor to proposed algorithm.

Limitations:
The authors provide a list of limitations of their work.

Rating:
1

Confidence:
5

REVIEW 
Summary:
The paper presents a method to improve the exploration of the MuZero agent in games. The authors propose an hybrid policy that mixes an exploratory policy and the optimized policy. The exploratory policy is meant to reduce brittleness of optimal policies.
The new method is demonstrated on Tic-Tac-Toe.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper addresses maybe the most important issue in decision making. The authors' approach to encouraging exploration in the decision tree addresses an important problem that could have significant implications if successful. Adversarial examples in games and self-play is certainly an interesting domain to investigate this.
The motivation is good, and the authors also evaluate their approach on a small game. The idea is simple and makes sense. The appendix is also nice, and the source code is included. Prior work is also well researched.
I like that the authors analyzed an example gameplay in Section 6.2

Weaknesses:
Section 2 Recent Historical Background

- For relevant work on adversarial policies, I like that you included “Adversarial policies beat professional-level 330 go ais. arXiv preprint arXiv:2211.00241, 2022.”. Another paper that investigates this issue and I think should also be included is then “Timbers, Finbarr, et al. ""Approximate exploitability: learning a best response."" Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). 2022.”
- When it comes down to AlphaZero extensions, a good addition in this section would be ""Player of games."" arXiv preprint arXiv:2112.03178 (2021).”, as it extends AlphaZero to imperfect information games


Paper sometimes does not read that well and individual sections sometimes feel bit disconnected (i.e. little connection to other sections in terms of semantics and flow). E.g. the word curiosity is only really used at the beginning, and my understanding is that authors really just mean exploration (and indeed exploration is used in the rest of the paper). Also note that ""curiosity"" clashes with prior work on Intrinsic motivation (Oudeyer et al., 2007; Schmidhuber, 1991, ...).


The idea is very simple, adding exploration at the beginning, which AlphaZero already does using different methods. I am not sure if the main evaluation in Section 7, namely Figure 5 is entirely fair (I might be wrong). The figure compares exploration-on vs exploration-off, but I don’t think that exploration-off in this case collapses to the “baseline” case? (e.g. MuZero or AlphaZero, as both do also force exploration at the beginning)


This issue connects to the second one, which is that the experiments are not that well explained. It is fine that the authors moved many details to the appendix (which contains a lot of interesting information), but the main body of the paper still should include enough for people to understand what is presented.


But my biggest issue is that the main challenge in balancing exploration in learning in games is when the approximator can’t really memorize the full game. If it could, than any exploration just works as we get to see all the states and memorize them (i.e. no generalization is needed). The game presented now is too small to see the effect, I suggest the authors run it on e.g. connect four. I believe the current experiments are simply not sufficient.


Finally, I think the measure of whether the exploration helps or not should ultimately be exploitability rather than just a uniform measure of bad decision over all the game states.



Limitations:
Addressed

Rating:
4

Confidence:
5

REVIEW 
Summary:
Inspired by the failure of KataGo against an amateur-level agent, the author proposes to use an additional randomization scheme to encourage the agent to explore the less experienced part of the decision tree. Such a randomization scheme allows the agent to randomly deviate from the planned policy, and then switch back to learn the correct value function. Empirically, they evaluate their method on Tic-Tac-Toe and justify the effectiveness of their method.


Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
1. The problem of exploring the less experienced part of the decision tree is interesting. 
2. The paper provides detailed related work. 


Weaknesses:
The main concern of this paper is the contribution is marginal. 

The proposed method is a minor modification (a randomization scheme) to the existing method. This is not to say the investigated problem itself is not important. But with such a minor modification, the paper needs to provide sufficient evidence to prove its effectiveness, either by theoretical analysis or a large body of experiments. However, I am afraid the paper provides neither. For the evaluation, the paper only evaluates the proposed method in a very simple scenario, Tic-Tac-Toe, which is way too easy. More experiments on more complicated scenarios are definitely needed.  

The writing is unprofessional and should be improved substantially. For example, references should not be included in the abstract. 



Limitations:
Yes

Rating:
3

Confidence:
4

REVIEW 
Summary:
To improve prediction accuracy, this paper divides the training phase's policy into exploration and normal policies. The proposed algorithm is tested in the game of Tic-Tac-Toe, and different noise strategies are introduced for experimentation.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper introduces a novel method for increasing policy exploration and experiments with different noise strategies for exploration.


Weaknesses:
1. The experiments lack persuasiveness as there is no comparison or theoretical analysis with advanced exploration algorithms.
    
2. The discrepancy between the mentioned ""poor"" predictions and the proposed exploration method is not fully addressed.
    
3. The setting of random time $t_{startNormal}$ is not clear.


Limitations:
The application of the proposed algorithm in more complex tasks and its effectiveness compared to algorithms with other exploration techniques remains to be studied.

Rating:
3

Confidence:
3

";0
THDGuhN7LA;"REVIEW 
Summary:
This paper studies a new task named semi-supervised few-shot object detection, where both of base and novel classes are supposed to be scarce.
The author first finds the vanilla supervised FRCN trained on base classes has a low recall on novel classes, and trained with extra unlabeled novel data can effectively improve the novel recall.
Then the author follows the SSOD framework Soft Teacher to do semi-supervised base training, where only partial base data is available. However, the original Soft Teacher has a low recall of small and ambiguous objects. Thus the author proposes a new proposal learning method to improve it.
Finally, the pre-trained model is then semi-supervised fine-tuned on a balance set comprised of both base and novel samples.
Experiments show Softer Teacher achieves a good GFSOD performance.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
* The idea is straightforward and has good soundness.
* The method has promising results in the GFSOD setting.

Weaknesses:
1. The proposed task of semi-supervised few-shot objects is similar to semi-supervised object detection. Since both base and novel classes are scarce, what's the meaning of splitting classes into base and novel? I can't see any practical significance in this task.

2. From my point of view, the setting proposed in the paper is closer to a semi-supervised than a few-shot object detection problem. Particularly, one of the key properties of few-shot learning is that the model does not know the novel classes in training, so it can adapt quickly to new classes with only a few examples in testing either using meta-learning (e.g., meta-RCNN) or small-#step finetuning (e.g., TFA). 
Another common sense in FSOD is that the novel classes are authentically rare, and we cannot find more images about that class, regardless of label or unlabeled.
Therefore, the proposed approach works best for semi-supervised object detection rather than few-shot object detection, and it is not fair to compare it with FSOD works.

3. The proposed method SoftER Teacher is an incremental improvement based on existing work SSOD Soft Teacher. The only improvement seems to be that the author appends a new loss to constrain outputs from the teacher and the student should be close to corresponding proposals, but the method is more likely to be only related to semi-supervised learning, it seems nothing about few-shot learning.

4. Section 3.1 is named ""What makes for Effective FSOD"", but 3.1 studies unlabeled data can improve the novel recall of the FSOD model. The title is not very accurate.

5. In line 252, the author argues, ""we are the first to incorporate external unlabeled data with few-shot fine-tuning"", I don't think it is a contribution or anything good.

6. There is no component analysis or ablation experiments to demonstrate the effectiveness of the proposed method. For example, the performance comparison of w/wo  the proposal learning loss.

7. The performance improvement is minor in the FSOD setting (not GFSOD). The novel performance is actually bad. The superior GFSOD performance may attribute to the strong baseline Softer Teacher on base performance.

8. The authors do not show any numbers related to the training resources (memory and time).

Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper focuses on Semi-Supervised Few-Shot Object Detection, where both base classes and novel classes have few labeled training set, along with abundant unlabeled data. For model architecture, the softer teacher is proposed to train with unlabeled data and a teacher-student framework. Experiments demonstrate strong performance using only 10% base labels.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The idea of introducing unlabeled data for few-shot object detection is interesting and has great value for real-world application. The idea of  reducing the number of labeled data for base classes in few-shot object detection is also interesting.

Weaknesses:
My major concerns are as below:

1. What is the sources of unlabeled images? Do the unlabeled images have both base-class and novel-class instances? In this way, it is no surprise that adding abundant images could improve the proposal recall and detection results of few-shot novel classes.

2. The training framework in figure 2 and Section 3.3 are not exactly the same. In Figure 2, the unlabeled images are used for both base-class pre-training and few-shot fine-tuning. But in Section 3.3, it seems that the second stage of few-shot fine-tuning do not use additional unlabeled image. Clarifications are need. If the second stage also use unlabeled images, can we merge the second stage into the first stage because both base classes and novel classes are few-shot? We do not need to have two stages for training in that case, and the problem becomes semi-supervised object detection and each class has very few labeled images. Thus, what is the difference between traditional semi-supervised object detection?

3. This work has improved overall performance of base and novel class. Although the performance of novel class improves compared some baseline model (e.g., Faster RCNN), but has far worse performance compared to the SOTA [1,2]. Does this mean that the additional unlabeled images only work well with base classes, but not for novel class? Using unlabeled image is perhaps a right way for semi-supervised object detection. But does using unlabeled images is the right way for few-shot object detection?

[1] Qiao, Limeng, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang. ""Defrcn: Decoupled faster r-cnn for few-shot object detection."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8681-8690. 2021.

[2] Kaul, Prannay, Weidi Xie, and Andrew Zisserman. ""Label, verify, correct: A simple few shot object detection method."" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14237-14247. 2022.

4. The Table 1 and Table 2 are not complete. Table 1 lacks the comparison of Faster R-CNN (Our Impl.) and Soft Teacher (Our Impl.) as in Table 2. Table 2 lacks the comparison of the latest method for few-shot object detection (e.g. LVC, DeFRCN) as in Table 1.

5. The Figure 3 (b) and (c) are very confusing. I can only find one red box. Does it mean that vanilla FRCN-base only have one proposal? This is weird.

6. What is the difference between soft teacher and softer teacher? In L201-L207, the author mentions that soft teacher has an aggressive threshold of 0.9 which is not good. How did the authors address this problem? I do not find the answer in the main text. L208-L225 seems to be a simple extension of soft teacher without big changes.




==========================================================================================

After reading author's rebuttal and other reviews, some of the concerns about technical details are clear. But the major concern about the significance of doing few-shot base/novel partition is still there. I would suggest the authors make the problem setting simpler to get broader impact.

Limitations:
Please see the weaknesses above

Rating:
4

Confidence:
5

REVIEW 
Summary:
This article has done a meaningful work, which is a object detection method that combines few-shot with semi-supervised learning. The author introduces a SoftER Teacher for semi-supervised object detection in few-shot scenarios. SoftER Teacher enhances the quality of region proposals to substantially boost semi-supervised FSOD. Compared with LVC, DeFRCN and other methods, the performance has been improved.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This task is very meaningful. As far as I know, the traditional few-shot object detection task is difficult to be directly applied to the industry, and the method combined with semi-supervised target detection will be a good solution. (Although the method in this paper is not the first to consider the combination of few-shot and semi-supervised.)
- It is good to see that the author provides the source code in the supplementary material, which provides a guarantee for the reproducibility of this article.
- The authors present rich experimental results in the article and supplementary material.

Weaknesses:
- To my acknowledgment, the current mainstream FSOD method is verified on MS COCO 2014, not MS COCO 2017. ""Consistent with the current literature on FSOD"" may be ambiguous.
- In Table 1, since the author did not report the results of LVC in 5-Shot, the experimental results of LVC are from the original article? But considering that our method has unlabeled data for additional testing, it seems unfair to compare the results with the LVC experimental setting.
- In addition to the comparison with the FSOD method, it would be better to add some comparisons with the SSOD method (under the Few-shot setting).
- In Table 1, I found that the results of the novel class seem to be relatively weak, what is the reason? Because the method in this paper utilizes additional data.

Limitations:
The authors describe the limitations of this paper in the supplementary material.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper approaches the task of Few-shot Object Detection (FSOD) from a semi-supervised perspective, where in addition to base classes data it uses unlabeled data during the base-pretraining phase, and then fine-tunes on the combination of base and available novel data using the best design choice of freezing appropriate layers (backbone, FPN and RPN) following the past works. The benefit of the approach comes with a higher bar on fully-supervised base class performance, which is attributed to training on additional unlabeled data. This higher performance then translates to a better overall (base + novel) classes performance in the fine-tuning phase, and establishes the effectiveness of semi-supervised learning on the FSOD task. The authors propose a SoftER Teacher approach which, in addition to the Soft Teacher loss, adds a consistency loss between teacher and student using at proposal-level. The authors show that this leads to better proposals (using recall) in low-data regimes.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The paper is well-written with extensive experiments and ablations, and the semi-supervised exposition in the FSOD setup is much appreciated with a potential for realistic low-shot setups
- The paper is well-positioned with respect to prior related works 


Weaknesses:

- Table 2 on VOC07 and lower performance of Novel classes in low-shot setup
    - Retentive R-CNN compared to SoftER exhibits a trend that its 1-shot performance is much higher that the proposed SoftER, and this trend gets reversed with more shots (such as 10-shot)
    - The authors acknowledges the phenomenon in lines 295-296, and in line 297 mentions that Retentive R-CNN “generally falls behind on novel class performance”. However, this isn’t true in low-data setup (1-shot).
    - This trend, however, doesn’t appear in the COCO dataset where novel class performance in 1-shot case is low for both Retentive R-CNN and the proposed SoftER approach
    - My question is:
        - Do the authors have any intuition for this behaviour?
        - The authors leverage COCO-20 and COCO unlabeled2017 as he unlabeled data source for VOC (Table 2) and COCO (Table 1) experiments. Do the authors think that the domain mismatch between VOC and COCO is reflected in low-shot novel class performance in the case of VOC (Table 2)?
        - In general, the proposed approach does better with relatively higher-shot regimes, which also appears in the claims made in the paper - such as Fig 1 (30-shot). But as a reader, there seems to be little explanation about low-shot regimes, which runs counter-intuitive since the approach uses additional unlabeled data compared to prior approaches, and is expected to perform well especially in low-data regimes

- Presence of novel classes in unlabeled data
    - Line 277-278 mentions the use of COCO-20 for VOC and COCO unlabeled2017 as the source of unlabeled data for VOC and COCO respectively
    - This makes the assumption that novel classes in the plots of VOC and COCO experiments are necessarily present in the unlabeled set
    - In general scenarios, such assumption may not hold true. Do the authors have some intuition of how the proposed approach would work if the percentage of novel classes in the chosen unlabeled set is low / absent?


### Minor concerns
- Figure 3a for more percentage of base labels
    - Not a head-to-head comparison between FRCN-Base and FRCN-Base + Unlabeled, since the latter assumes more data
    - Does the difference narrow with more percentage of base labels?

- Conflicting claims
    - Fig 1: exhibiting less than 7% in base degradation
    - line 54: exhibiting less than 9% in base forgetting


### Justification of the rating
My main concern is highlighted above. In general, explanations towards the above mentioned concern would help the readers


Limitations:
Yes

Rating:
6

Confidence:
4

";0
