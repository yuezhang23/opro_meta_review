id;text;label
2MRz5bSnan;"REVIEW 
Summary:
The authors proposed the permutation decision trees method, which uses Effort-To-Compress as the impurity measure to model the order dependencies of data instances, and extended the proposed permutation decision tree to a variant of random forest. They also did some experiments to compare the performance of the proposed methods with random forests. 

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The proposed structural impurity can actually capture the order dependencies of data instances, as shown in the examples in Table 1. 

Weaknesses:
The paper exhibits several weaknesses, which are outlined below:

1. Insufficient clarity regarding the chosen setting: The authors' intended focus appears to be on time series data; however, the task discussed pertains to multi-class classification, which is an i.i.d. setting. The authors are recommended to formalize the problem setup.
2. Inconsistent use of notation: The paper demonstrates inconsistencies in notation usage. For instance, the features presented in Table 3 are denoted as $f_{1}, f_{2}$, whereas in Figures 3-7, they are represented as $x_{0}, x_{1}$.
3. Unfair experimental setup and insignificant results: The experiment setup lacks fairness, and the obtained results do not exhibit statistical significance. In the only out-performing dataset, the random forest model employed only one tree, while the proposed permutation decision forest utilized five trees, indicating an apparent unfairness in the comparison. Furthermore, the hyperparameters' n_estimators vary across different datasets, which is deemed unreasonable. 

Limitations:
The authors adequately addressed the limitations in Section 4.

Rating:
2

Confidence:
4

REVIEW 
Summary:
In traditional decision tree algorithms such as CART and C4.5, impurity measures are used to split internal nodes. The paper proposes a decision tree induction method by using effort-to-compress (ETC) measure, which can capture order dependencies in the data. With ETC’s ability to capture order dependencies, permuting the data can result in different trees, thereby constructing a forest without the need for bagging. This proposed decision tree induction method can be used for datasets with temporal order. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper uses ETC as a new impurity measure for constructing decision trees. Since ETC is sensitive to the order of data points, the tree built using this measure may be well-suited for temporal datasets. And it also provides a different way for constructing diverse trees and thereby getting a forest. Overall, the paper is clearly written and easy to follow. 

Weaknesses:
- What about the bias and variance in the permutation decision forest? Random forest uses bagging and random feature selection to make trees in the forest uncorrelated, thereby reducing variance. But trees in the permutation forest are not uncorrelated. Using the ensemble of these correlated trees may not reduce variance. 

- In the toy example, some leaf paths are shown in different trees. I am wondering if there will be a significant number of duplicated leaf paths within the permutation forest. 

- Experiments only show the comparison between random forest and permutation tree forest in terms of F1-score. How about other evaluation metrics, e.g. misclassification loss? The results don’t show that the proposed method outperforms random forest. And there is no comparison between the performance of single trees, such as CART vs. ETC tree. 


Limitations:
The authors have addressed the limitations of the paper and propose future directions. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper ""Permutation Decision Trees using Structural Impurity"" introduces a novel split criterion for the training of decision trees that also takes the order of labels inside the training data into account. This way, to obtain a forest, one only needs to shuffle the data before training individual trees. Moreover, the novel split criterion supposedly works better for data that includes (temporal) dependencies, although der are no experiments to support this claim. 

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
- I think the idea of tackling non-iid data with a novel split criterion is nice, and Effort-To-Compress as an impurity measure seems like a good choice

Weaknesses:
- The experimental evaluation is very weak. The authors compare their method on 6 small real-world datasets and one artificial dataset and compare it only against Random Forests. Moreover, their method seems to be worse compared to RF. Hyperparameters are also incomparable, as the RF uses smaller trees than their method although it is well-known that RF benefits more from larger trees. In addition, the number of estimators changes for every experiment. There is no clear experimental protocol, and the authors do not use random repetitions and/or cross-validation but resort to a single train/test split. The experimental evaluation is hence borderline useless and can only be seen as a first test-experiment.  
- The paper contains limited valuable information. While the Effort-To-Compress (ETC) measure seems to be of central interest here, the authors do not present a formal mathematical explanation of it. They mention the NSRPS algorithm to compute ETC, but also do not explain it mathematically, and only offer a single example. A thorough mathematical explanation and the typical explanations of the notation (Model function f(x), samples X, labels Y, etc.) is missing entirely. 
- The authors deal with the case in which the order of samples is important. This is completely against the typical IID assumption we have in Machine Learning. Unfortunately, the authors neither discuss this (certainly interstring) difference in more detail nor do they really present any real-world example of this. 
- A dedicated Related Work section is missing, although there is plenty of space left in the paper. The authors decided to waste roughly two pages by printing different DTs, which does not add any new information to the paper. This space would have been used better to highlight related work or pinpoint the novelty of this work in more detail. 
- Eq. (1) and Tab. 4 do not fit the page width

Limitations:
The authors acknowledge that their method is worse compared to the state of the art and intend to perform more testing. As this paper presents a novel method I don't see any immediate negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper proposes a novel in Decision Tree literature splitting criteria based on Effort To Compress (ETC) gain. Use of this criteria is justified by a desire to work with data that doesn't conform to i.i.d. assumption about the generating distribution. There is an experiment on a synthetic data that shows that different decision trees are generated when different orderings of the data are used for training. A permutation voting forest is introduced, that allows using random permutations of full data to obtain multiple different decision trees for use in the final ensemble of trees. There is an evaluation of Permutation Voting Forest against regular random forests on multiple real world datasets that however show slightly lower results when using proposed method. 

Soundness:
2

Presentation:
2

Contribution:
4

Strengths:
- The paper opens a novel line of research about using Decision Trees for modeling data, that comes in a sequence and does not follow i.i.d. assumption. 
- There is a novel application of ETC measure as a splitting criteria in decision trees.
- A generalization of the proposed model: Permutation Decision Forest is introduced, that uses a novel idea of shuffling the data in the context of a splitting criteria that generates different trees for different permutations of the training data. 


Weaknesses:
- It is noted that usage of ETC allows to get rid of i.i.d. assumption. But this claim needs more thorough theoretical analysis. If we want to keep sequential structure of the data, the sequence still gets destroyed upon split: split does not split examples in a consecutive way; some examples may go to the left split, then some to the right, then again to the left part of the split and so on. So, new left and right sequences after the split will have completely different properties. Considering an example from introduction, where ETC is used: musical compositions. Splitting the musical composition according to some feature, like presence of some range of frequencies at a given moment, will result in an unpleasant music on both sides of the split, because instead of hearing half of the musical composition, we will hear a ""fractal"" - small parts of original sequence with small gaps inbetween, that got assigned to left or right parts of the split
- Related to the previous point: at the testing phase there is no ""memory"" in the model, and the model still predicts elements by looking at them one by one. So, shuffling the testing set will result in exactly same predictions. Can we say that the problem of non-i.i.d. distribution is solved, if the behavior on the testing set is equal to the behaviour of the i.i.d. models?
- Testing of regular Decision Trees with proposed splitting criteria on real data is needed (only the forests were tested on real data, but proposed forests work differently due to the proposed shuffling of the input data, so regular trees must be evaluated separately as well). It would be nice to both test on regular datasets (that are not sequentially ordered, like the datasets from section 3.2), and also to find at least some example real datasets where ordering is important, and where proposed model (regular permutation decision tree) is both practically and theoretically better than the baseline decision tree models.
- In section 3.2 a more thorough experimental design would be more convincing. (a) If we compare proposed model to the baseline, why hyperparameters are different for same dataset? If hyperparameters tuning was done, that should be thoroughly described. (b) Experiments should be run several times on different train-test splits and mean scores and standard deviations should be reported to allow fair comparison in the presense of noise.


Limitations:
Limitations are well described in the paper, which is good. Since there were identified significant limitations in terms of accuracy of the proposed permutation decision forests (that may also affect proposed single permutation decision trees), it would be more convincing to include additional experiments that will clarify the extent of such limitations right away without deferring them to the future work. 


Rating:
4

Confidence:
5

";0
wEiUGpcr0M;"REVIEW 
Summary:
In this manuscript, the authors have developed an interesting self-supervised learning model, by the incorporation of persistent homology into contrastive learning module. More specifically, a special topological distance based contrastive loss is proposed. The model is novel, and the results are very promising. However, I have some concerns about the persistent homology analysis part.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors have developed an interesting self-supervised learning model, by the incorporation of persistent homology into contrastive learning module. More specifically, a special topological distance based contrastive loss is proposed. The model is novel, and the results are very promising.

Weaknesses:
The PH model is not explained clearly. 

Limitations:
Missing many important related literature. The advantage of their filtration process is not clear.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Paper uses self-supervised learning tools for graph representation learning by facilitating topological data analysis (TDA) methods. In particular, for molecular representation learning, the authors use persistent homology outputs to improve the embeddings obtained by GNNs. They evaluated their model in molecular property prediction problem, and consistently got performance improvements.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
GNNs and TDA are both very successful and completely different methods in graph representation learning. In the past years, there are several approaches to integrate these two methods effectively. With this aim, the paper proposes a new way to use TDA output to improve node embeddings in GNNs by using contrastive learning ideas. The idea is novel and has a lot of room for improvement.

Molecular Representation Learning is a significant application area for graph representation learning. The authors applied their model in this domain, in particular, molecular property prediction. They obtain strong results on this important question.

The paper's experimental part and ML details are strong. The authors made an in-depth analysis of the model from various angles.

Weaknesses:
The experimental results (Table 4) do not show significant improvements in several cases. 

The results only report the performance of internal models. It would be nice to see the comparison with the SOTA results on these datasets. 

PH construction seems weak as it does not use clique complexes, and only uses nodes and edges in the filtration, i.e., the top dimension is set to be 1. This filtration are not commonly used in graphs as it reduces PH to only node and edge counting by using a simple Euler Characteristics argument. However, fortunately, this does not affect their performance in this setting since molecular graphs are planar and do not have loops of length 3, as all loops have length $\geq 5$. The authors should add a note for nonexperts that for molecular graphs, this trivial filtration setting is equivalent to the traditional clique complex setting for sublevel filtration because of the special structures of molecular graphs (no cycle of length 3). For TDL, PI is a good choice, but for TAE, it looks weird. To be used in such a loss function, there are better stable PH vectorizations, e.g., Silhouette, landscape.

Limitations:
There are various choices to be made in several places for the model. On one side, it gives flexibility to adapt the model to different settings, but on the other hand, it needs expertise in several domains for fine-tuning.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes two molecular self-supervised learning methods, which consists of fingerprint autoencoder and topological distance contrastive learning. The insight behind this paper is to utilize topological fingerprint as a supervision in self-supervised learning. Thus, the authors reconstruct the topological fingerprint of a given molecule with autoencoder and filter out similar molecules in negative views in contrastive learning based on the similarity in topological distance space. The experimental results show that their method improves previous baselines in various downstream tasks.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- The paper is well written and easy to understand.

- The experimental results are comprehensive; the authors considers several setups such as linear probing and fine-tuning.

Weaknesses:
- Lack of Novelty: Excluding similar molecules from negative sample set is already considered in [1]. Conceptually, the difference of TDL and [1] is that TDL utilizes PH and [1] utilizes ECFP fingerprint (I know that the loss of [1] is based on augmented molecules, but I think this does not make big difference). This limits the novelty of this paper.

- Table 1 does not support the effectiveness of proposed method: Correlating the distance in embedding space with the distance in corresponding PIs are not the main purpose of molecular representation learning. If PIs are indeed very important, then why should we use learned representation of proposed method? Can't we just utilize PIs as the molecular representation? In other words, Table 1 and Table 17 seem to contradict.

- Insufficient rationalization of the usage of PIs: In molecular domain, ECFP fingerprint is a widely applied molecular representation since it reflect the substructure-wise molecular information. Why should we use PIs in molecular representation learning?

- Flexibility of TDL: The authors insisted that TDL can be flexibly and efficient applied with any graph contrastive learning framework. However, any other two existing methods can be composed with each other to improve the performance. For example, ContextPred + GraphCL is possible and the flexibility is not the unique feature of TDL. 

- Table 4 seems weak: TDL (or TAE) combined with existing method does improve the overall performance. However, Mole-BERT and SEGA shows better performance than the proposed method.

----Sorry for confusion. I added the reference.

[1] Improving Molecular Contrastive Learning via Faulty Negative Mitigation and Decomposed Fragment Contrast, Wang et al., JCIM 2022

Limitations:
Yes. The authors addressed the limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes two approaches to leverage topological information (obtained from persistent homology) for molecular representation learning in a self-supervised setting. The first (TAE) uses an encoder-decoder architecture whose decoder aims to recover topological fingerprints. The second approach (TDL) consists of a contrastive loss based on the similarity between topological fingerprints. The latter is combined with existing contrastive learning methods. Experiments on linear probing and downstream prediction tasks show the efficacy of the proposals.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Ablation studies: There is a substantial number of experiments and ablation studies.
- I like the simplicity of the proposed approach. 
- Flexibility: TDL can be combined with most SSL approaches.

Weaknesses:
- Overall, I believe the paper provides limited insight to support the proposals. Also, it does not discuss which structural information the proposed approach captures but not existing methods. From a conceptual level, we know that 1-WL GNNs cannot capture information even from simple homology (e.g., number of independent cycles of a graph). Thus, TAE has inherent limits/failures. In other words, the topological information we loose after pushing a graph through a GNN (which would be captured by TDA) cannot be recovered from GNN embeddings.
- Results on downstream tasks: Based on Table 4, the gains from TDL look marginal. The gain is less than one standard deviation from the base model for many datasets.
- Incorporation of domain knowledge: The claim that the proposal allows for incorporating domain knowledge seems overstated. The basis for such a claim comes from the choice of the filtration function. However, it is unclear how different filtration functions affect the topological embeddings --- thus, domain experts cannot leverage their knowledge to choose the filtration functions.
- TAE vs. TDL...which one should we use? The paper says that ""TAE, which we developed for comparison purposes only..."" (line 283). I am unsure whether TAE should be introduced as a main contribution or as a baseline (in the experiments) for assessing the feasibility of learning the topological fingerprints with a simple architecture. 


Limitations:
The authors mention limitations in the main paper (section 5).

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper explores self supervised learning in the context of molecular representation, specifically based on persistent homology. The paper proposes an autoencoder to demonstrate the general representational power of PH and a contrastive-learning-based loss that can be applied to existing SSL approaches. The proposed approach is evaluated for molecular property predictions, showing improved representations and predictive power compared to baselines across different tasks. The claim is that the new loss function enhances baseline performance particularly with small datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and the idea is novel and interesting. 

Weaknesses:
- Given the technical nature of PH, and its origin in the domain of topological data analysis, a more mathematical foundation of the methods in the paper would be desired. 

Limitations:
The authors describe the limitations of their approach. 

Rating:
7

Confidence:
2

";1
ZfFR4d5gUM;"REVIEW 
Summary:
This paper studies the training of 2-layer neural networks and proposes a two-timescale limit/regime. In this 
limit/regime, the learning rate of the first layer is much smaller than the learning rate of the second layer. 
As a result, the training of the network can be viewed as training the first layer and performing linear regression 
over the first layer outputs, which is simpler/more structured than training both layers at the same rate. To 
demonstrate the usefulness of this strategy, this paper considers a toy model and shows that a 2-layer network can fit 
a certain family of 1D piecewise constant functions. The authors also empirically show that SGD can fail to fit this 
family of functions outside the two-timescale regime.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
* The presentation is clear.
* This paper not only contains results in the $\varepsilon \to 0$ limit, but also non-asymptotic results for small but 
  non-vanishing $\varepsilon$. The derivation in the $\eta \to 0, \varepsilon \to 0$ limit is clean (Sec. 4.2), and 
  the asymptotic-to-non-asymptotic parts are also easy to follow.
* This paper reminds me of a technique that is gaining popularity in the theory community: training the network for 
  one (large) step, freezing the first layer, and then performing linear regression in the second layer using the 
  features learned in the first layer (cf. [AAM22], [DLS22]). The limitation of this technique is that since the 
  first layer is fixed after the first layer, it lacks the ability to refine the learned features. I feel the 
  strategy introduced in this paper is a potential remedy to this problem, as here we also have the linear regression 
  part of the argument, but the first layer can be trained for multiple steps. 

[AAM22] Abbe, Emmanuel, Enric Boix Adsera, and Theodor Misiakiewicz. “The Merged-Staircase Property: A Necessary and Nearly Sufficient Condition for SGD Learning of Sparse Functions on Two-Layer Neural Networks.” In Proceedings of Thirty Fifth Conference on Learning Theory, 4782–4887. PMLR, 2022. https://proceedings.mlr.press/v178/abbe22a.html.

[DLS22] Damian, Alex, Jason D. Lee, and Mahdi Soltanolkotabi. “Neural Networks Can Learn Representations with Gradient Descent.” arXiv, June 30, 2022. http://arxiv.org/abs/2206.15144.

Weaknesses:
* Almost all things in this paper are 1D and somewhat tailored to this specific piecewise constant function class. 
  I wonder whether/how this can be generalized to higher dimensions, other network architectures, and more general 
  function classes. 
* It seems that the dynamics are still relatively local. That is, we need some neurons in each target interval at 
  initialization, which may not be reasonable when the dimension is high.
* The authors should probably add some discussion on the training-for-one-large-step-type technique (see the Strengths 
  part of the review). 

Limitations:
This is a theoretical work and, as far as I can see, has no potential negative societal impacts. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studied the problem of fitting a piecewise constant univariate function with a shallow neural network. Specifically, the authors consider a gradient flow with a time-scale difference between the dynamics of the first and the second layer weights. It is shown that the trained shallow network can be arbitrarily close to the target function in $\mathcal{L}_2$, as long as the weight update on the first layer is much slower than one on the second layer.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The two-time-scale regime seems novel in training neural networks. The results are well presented and their proofs are clearly explained.

Weaknesses:
Only a very special problem is studied. 

1. The target function is univariate, piece-wise constant. This is very restrictive. 

2. The loss is a population loss, thus the case of finite samples (fixed design or random samples) is not considered.

3. Network is quite different from those studied in other works. The activation function is a smoothed version of a stair function; the first layer is only parametrized by the bias at every neuron. 

With this many assumptions/restrictions, even though the main results are well presented and explained, it is hard to see whether those observations can give insights into the usefulness of implementing two-time-scale training for practical networks. The author did not discuss the limitations of these assumptions, nor did they show how two-time-scale training can be used in practice, even empirically.

Limitations:
See ""Weaknesses""

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, the authors considered the problem of learning piece-wise linear function in 1d using two-layer neural network. They considered gradient flow on mean-square loss with different learning rates for 2 layers (two-timescale). Specifically, the outer layer weights are moving much faster than the inner layer weights. The activation used in 2-layer network is similar to a rescale version of sigmoid activation. This paper shows that under proper choice of the parameters, GF converges to small loss within polynomial time in relevant parameters. Experiments are provided to support the results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	Understanding the training dynamics and convergence of neural networks is an important problem.
2.	The paper is overall easy to follow and clearly written. The proof sketch is given so that the reader can understand the main proof idea.
3.	The idea of this two-timescale/two different learning rates in analyzing the training dynamics for neural networks seems to be interesting.


Weaknesses:
1.	The problem considered is only in 1d and it would be interesting to see if the analysis could be generalized to multi-dimension.

Limitations:
The limitation is discussed in the paper. This is a theoretical work and therefore no foresee negative societal impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies the training dynamics of fitting a one hidden layer shallow network with heaviside activation to a piecewise ground truth function with one-dimensional input. It proves that gradient flow always recovers the ground truth in finite time with only mild over-parametrization.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written, and intuitions behind technical proofs are well-presented, and theoretical results are well-supported by numerical experiments. The theoretical result itself is a nice observation, despite being in the simple one-dimensional input case.

Weaknesses:
The paper only considers the one-dimensional case. Based on the proof techniques, it is not clear if it is extendable to high dimension, which is of ultimate interests in the deep learning theory community, since the derivation in section 4.2 would not hold any more. While it is understandable that such a result would be difficult to obtain in high dimension, the paper didn't present any experimental results in the high-dimensional case either.


Limitations:
N/A

Rating:
6

Confidence:
4

";1
bNNIf8F9OU;"REVIEW 
Summary:
In this work, the authors focus on improving the generalization ability of the top-K recommendation model by a proposed principled Adversarial InfoNCE loss (AdvInfoNCE). Existing contrastive learning based methods usually lack considering the tailored inductive bias (such as hard negatives and false negatives) and sufficient theoretical understanding for the generalization ability. The proposed AdvInfoNCE loss could adaptively explore and assign hardness (weight) for each negative instance in an adversarial fashion. The theoretical guarantees and experiments demonstrate the effectiveness of the proposed AdvInfoNCE loss.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The motivation that adaptively assigns hardness for each negative instance in an adversarial fashion to improve the generalization ability is justified.
2.	The theoretical proof demonstrates the generalization ability of the proposed AdvInfoNCE loss.
3.	The experiments on unbiased datasets and out-of-distribution settings demonstrate the effectiveness of the proposed AdvInfoNCE loss in terms of generalization ability.


Weaknesses:
1.	It would be better to add an intuitive example to show the adaptive learning process for hard negatives and false negatives in the min and max stages.
2.	In terms of adaptively learning hardness for each negative instance, it would be better to add some baselines focusing on mining hard negatives or false negatives. 
3.	In terms of the generalization ability, it would be better to add some baselines focusing on out-of-distribution. Besides, OOD experiments are also performed on popularity-based distribution shift scenarios, what’s the difference compared with debiasing experiments?


Limitations:
The authors adequately point out the limitations and there is no negative societal impact of their work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper is on collaborative filtering (CF) enhanced by contrastive learning (CL). The authors point out that the adoption of CL into CF is suboptimal due to challenges such as the issue of out-of-distribution, the risk of false negatives, and the nature of top-K evaluation. They also note that current CL-based CF methods lack consideration of the tailored inductive bias for CF and have limited theoretical understanding of their generalization ability.

To address these limitations, the authors propose a principled Adversarial InfoNCE loss for CF that focuses on mining hard negatives and distinguishing false negatives from the vast unlabeled user-item interactions. The proposed method is compared with several state-of-the-art contrastive learning-based CF methods on both unbiased and synthetic datasets. The experiments show that the proposed method outperforms the baselines in terms of accuracy and robustness, demonstrating its potential for improving the performance of recommender systems.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
i) The paper proposes a novel approach to adaptive contrastive learning in collaborative filtering by adopting an adversarial approach. The proposed Adversarial InfoNCE loss addresses the limitations of existing methods and allows for the fine-grained assignment of hardness to each negative user-item pair, which enhances the recommender's generalization ability and empowers top-K recommendations via informative negative sampling.

ii) The paper provides innovative theoretical insights into the benefits of adversarial hardness learning. It shows that the proposed hardness scores are correlated with the out-of-distribution problem of recommendation, and can thereby enhance the generalization ability of recommenders.

iii) The study of hardness gives an in-depth analysis on the learned hardness scores, which uncovers the importance of learning correct hardness for the massive negative samples without observations.

Weaknesses:
i) The proposed method can be viewed as an adaptive SSL method for recommendation [1-2]. Also, it can be a learnable negative sampling approach [3-4]. Literature review (baseline comparison would do better) should be done for these two very relevant research line.

ii) Though a brief training cost experiment is given in the appendix. I would expect more detailed efficiency experiments or analysis to support the claim that the proposed AdvInfoNCE can serve as a foundation loss for future CF researches, as the proposed approach utilizes the adversarial training method.

[1] Graph contrastive learning with adaptive augmentation

[2] Automated Self-Supervised Learning for Recommendation

[3] Personalized Ranking with Importance Sampling

[4] AHP: Learning to Negative Sample for Hyperedge Prediction

Limitations:
N/A

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper studies contrastive learning (CL) in collaborative filtering (CF) for top-k recommendation. In particular, it focuses on the CF-tailored challenges for CL, and then presents adversarial infoNCE (AdvInfoNCE) loss. This loss dynamically assigns hardness to negative instances and incorporates a fine-grained ranking criterion to improve the CF recommender’s generalization ability. Furthermore, this paper highlights theoretical properties of AdvInfoNCE. Experiments on both synthetic and real-world datasets are done to show the effectiveness of this loss, especially in out-of-distribution scenarios.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1.	The motivation of considering fine-grained hardness-aware ranking criteria is clear and reasonable. Technically, it is insightful and novel to transfer the distinguish between false and hard negatives into a margin learning problem.
2.	As proved in Sec 3.3, the adversarial training framework of AdvInfoNCE is natural and somehow equivalent to solve a distributionally robust optimization (DRO) problem. I appreciate this theoretical guarantee that can endow a AdvInfoNCE-trained CF models with better generalization ability.
3.	The experiments are done on four datasets with two CF base models, which are sufficient to demonstrate the effectiveness of the proposed loss. Moreover, the selected baselines are quite new, including some recent and strong works.
4.	This proposed loss seems to be a general loss that can be applied in general recommendation models.


Weaknesses:
1.	The hardness is denoted as $\delta$ in Sec 3.2, while related to $p(j|(u,i))$. This is not explained clearly. More clarifications are needed here.
2.	In Line 214, the limitation is stated as the ‘training instability’, which is not empirically shown in the experiments, such as indicated by ‘training loss variance’. It would be better to discuss more about this instability.
3.	Although the proof of Theorem 3.1 seems correct, the DL-divergence In Line 195 misses a minus sign. Please double check and fix it.


Limitations:
Please refer to the weaknesses and questions.

Rating:
8

Confidence:
4

REVIEW 
Summary:
Current losses for collaborative filtering struggle to handle the issue of unobserved user-item pairs.  Typically, the approach is to treat unseen pairs as negatives while seen pairs as positives, but this is somewhat problematic because unseen pairs could just be unobserved positives.  The authors propose an adversarial InfoNCE-based loss that claims to address this problem in collaborative filtering.  This loss works by minimizing the InfoNCE loss given that we have adversarially learned weights for the negative samples.  They give a theorem that shows this proposed loss can be interpreted as a distributionally robust optimization problem.  Finally, they give some empirical results showing the efficacy of their method over other CF baselines.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1.  This paper provides a method that appears to give solid gains across various collaborative filtering tasks.

2.  They do make an attempt to try to interpret what the \delta (adversarially learned parameters) in their method are doing.

3.  The graphs for the results and the pictures explaining the methods are good and helped me with understanding. 

4.  The authors try to tackle a hard problem: it is hard to think about how to best utilize unseen pairs in collaborative filtering due to their unseen nature.

5.  The method is fairly novel as a nontrivial extension to InfoNCE to the collaborative filtering setting via adversarial training.

Weaknesses:
1.  I don't understand the role of the adversarial variables in the algorithm.  In figure 2a there was some attempt at interpreting the values of the deltas, but it still does not make sense.  I hope the authors can explain the role of the variables better.

2.  I think the terminology of ""hard negative"" is confusing, because typically in the self-supervised learning literature people call negatives that are near the decision boundary ""hard negatives"".  However, in this paper hard negatives are the opposite: negatives that are far away from the positives pair.  I suggest rewriting the paper to make the message more clear.

3.  In general, the paper is hard to understand and has many grammatical errors.  The authors should fix this to make the paper easier to read.

4.  From what I can understand, the loss should simply make the deltas as large as possible (positive) to increase the loss value given that there are no constraints (aside from the number of epochs trained, I guess).  I have concerns about the usefulness and stability of this algorithm.

5.  In theorem 3.1, we assume that the deltas imply a probability distribution.  Is this true?  As we train do the deltas for a user i add up to |N_i|?  I'm not sure there is a constraint there enforcing this.  In that case I'm not sure the theory applies to the algorithm as-is.

Limitations:
Seems to be sufficient.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a principled AdvInfoNCE loss for CF methods to improve generalization ability. It utilizes a fine-grained hardness-aware ranking criterion to assign weights for unobserved user-item interactions. In this way, it can enable better distinction between different negative, thus mitigating the inductive bias in CF-based methods. It provides theoretical proof of the effectiveness of AdvInfoNCE loss and the experimental results compared with other popular loss used in recommenders look promising. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It is well-written and easy to follow.  
Good motivation of improving the generalizability of CF-based methods.  
It provides thereotical guarantees for the loss design and conducts comprehensive analysis on the effectiveness of its method.  
Experimental results compared with other popular functions adopted in CF models look promising.  
Code is open.  


Weaknesses:
Experiments can be more extensive. The results on MF and LightGCN look promising. But I think it would be more convincing if the authors can consider more CF-based backbones like MultVAE [1] and DGCF [2].  

[1] Liang et al. Variational Autoencoders for Collaborative Filtering. 2018 WWW.  
[2] Wang et al. Disentangled Graph Collaborative Filtering. 2020 SIGIR.  

Limitations:
More CF-based backbones can be considered.  

Rating:
7

Confidence:
4

";1
aMTiwdK3y8;"REVIEW 
Summary:
The paper proposes a method to reconstruct 4D hand (3D hand sequence) from a short RGB sequence with two types of Fourier Query Flow (pose flow and shape flow). In the Fourier Query Flow, the 3D trajectory of each point is transformed into 3 Fourier series along the time dimension and represented with the first few coefficients (3*(2*6+1) = 39 in this paper). Pose flow is generated with joint flow via LBS. The geometry is represented in the canonical space with a pretrained occupancy network and warped into the real space with the pose flow. Then shape flow adds small displacements to it. Experiments demonstrate that the proposed method outperforms existing methods and can produce continuous and smooth results.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality: The idea of represent 3D flow with Fourier series is novel and interesting. 

Quality: The presented results is of high-quality.

Clarity: The paper is well-written and easy to follow.

Significance: It seems that this method prevents the results from temporal jitters or abrupt motions, which I’ll discuss in Questions. And it’s very computational efficient. 


Weaknesses:
I do not see some major weakness. But I still have some questions about this method. I’ll leave them to the Question part. And it would be great if authors can add these discussions to the paper. 

A typo: line 228 t()^2 ?


Limitations:
I appreciate the authors’ discussions about limitations of the proposed approach in Sec. 5, which helps the understanding of the suitable scenarios.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper introduces FourierHandFlow - an implicit 4d representation for learning spatio-temporal hand shape deformations. 
The core idea is to introduce a coarse-to-fine implicit deformation model parameterized with a fixed Fourier basis to ensure smoothness and efficient inference. The coarse (pose / joint flow) part models the dynamics of the joints conditioned on images and pose predictions, and the fine part models the dynamics of the per-query point deformations on top of the joint flow, conditioned on image features. 
Experimental evaluation is conducted on InterHands2.6M and indicates that the proposed method outperforms recent baselines in terms of quality of shape reconstruction.


Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- Overall coarse-to-fine flow formulation generally makes sense, as most of the local per-point deformations would be strongly dependent on the joints. 
- Proposed formulation uses a fixed Fourier basis parameterisation of the flows is interesting, and bears multiple benefits:
  a) it allows for more efficient inference, since re-sampling points over temporal domain can be computed in closed form for different timesteps.
  b) it is guaranteed to lead to smooth trajectories by construction.
- Implicit 4D formulation leads to automatic learning of correspondences, which allows texture transfer.
- Quantitative results indicate that the proposed method outperforms multiple recent baselines (although there is not enough clarity on the evaluation protocol, see below).

Weaknesses:
1. The paper is at times hard to follow. 
- For example, when introducing the method (L136), authors do not really specify an exact form of the underlying representation, and vaguely refer to it taking a sequence of RGB frames as input. Does the overall method take any other sources of supervision or conditioning? 
- Similarly, from Section 3.1 it is not really possible to understand what is the ground truth used to pre-train the occupancy and LBS functions. Do you have geometry ground truth here? What is the resolution of this ground truth (a short look on InterHand2.6 suggests that geometry that is provided is coming from MANO)?
- After going through the experimental section I am still not sure I understand what is the ground truth geometry that this paper is comparing against. 

2. Motivation / quality.
- The quality of the resulting geometry seems to be of low resolution and does not contain any high-frequency details. 
- The main argument for using more complex machinery of implicit 4D representations over explicit mesh-based representations (L108) is the ability of those to capture high-resolution geometry details. 
- Yet, the resulting meshes seem to contain no high-resolution details, and examples e.g. in Figure 3 show that all the considered methods struggle at rough alignment with the images - which should be possible to solve with more robust keypoint / segmentation constraints.  
- The question thus arises if there is enough motivation for the development of the complex implicit machinery. If the resolution is the only limiting factor, why not upsample the mesh of MANO and fit it to ground truth scans, followed by a simple image2shape regression? 

3. Evaluation / baselines.
- There is no comparison to a parametric model MANO fitted to sparse constraints such as keypoints / segmentation. 

Limitations:
Authors discuss limitations and broader societal impact. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper introduces an implicit spatio-temporally continuous hand representation for RGB videos. Firstly, based on LEAP [25], the occupancy function and LBS weights are pretrained as priors for query points. Then two query flow representations are introduced to model the skeleton and the shape, respectively. The query flow representations are Fourier coefficients, which can be treated as a low-pass filter for 4D representations. Experiments on InterHand2.6M and RGB2Hands show the proposed method achieves accurate and efficient 4D predictions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed 4D implicit representation is highly efficient during both training and testing.
- The Fourier coefficients are effective to get smooth and continuous temporal dynamics.

Weaknesses:
- [Balance] This paper proposes using Fourier coefficients to model 4D hand and emphasizes this will guarantee smooth and continuous temporal dynamics. Especially, it learns coefficients for N=6 basis functions (Line 175). I am wondering how Fourier coefficients and N balance the accuracy, the efficiency and the smoothness. For example, will N=6 lead to over smooth? why not use a larger N? is it because of efficiency?

- [Optimisation] Unlike existing representations, the proposed representation is coefficients. I am wondering if this will raise any training or optimation issues. For example, will this representation require more epochs to converge or be harder to fit during training?

- [$\Psi$] For pose flow, there is an off-the-shelf pose estimator $\Psi$. I'm curious about the role this pose estimator plays. Is it like a condition that directly determine the final performance or an initial values that only reduce search space (Line 209) or a refinement (Tab.3)? Would it possible to use different $\Psi$ with varied performance and see the impact on the final results during training? Also, what will happen if $\Psi$ provides bad predictions during testing? What if we do not have $\Psi$?

- [Metric] It would be better to compare SOTA methods like [19,44,45] wrt other metrics like MPJPE, even if they focus on a single frame or are based on MANO.

Limitations:
Yeah.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes FourierHandFlow, a 4d hand pose and shape representation that inherently uses Fourier series as query flow representation. Given RGB sequence, a fixed number of Fourier series are learned to represent hand pose and shape. The authors use two types of flows to decompose pose and shape: pose flow (joint flow) and shape flow. Such decomposition makes it more efficient to reconstruct 4d hands. Experiments on Interhand2.6M and RGB2Hands datasets demonstrate its superiority over existing two hand estimation methods.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper is clearly written and well-motivated.  The compact Fourier series representation is proved to be effective for two hand 4d reconstruction in a smoother way. The video results are impressive. The quantitative results show large improvements. 

Weaknesses:
The main weakness is that, I am not very sure whether such representation could correctly model the surface details (or pose dependent deformation) of the hand because Fourier series based representation seem to generate over smoothed results due to its low dimension nature of shape flow. For example, at video 04:06 (frame 7101), the little finger of the left hand in “Alt. View 1” is too thin. 

Limitations:
Yes. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper introduces FOURIERHANDFLOW, which is a spatio-temporal continuous representation for the human hands. It combines a continuous 3D hand occupancy field with articulation-aware query flows represented as Fourier series along the temporal axis. These query flows are parameterized by coefficients learned from an input RGB sequence. Specifically, two types of Fourier query flows, namely pose flow and shape flow, are used to address the challenges of continuous and smooth 4D reconstruction, computational efficiency, and articulated shape modeling with correspondences.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- This paper provides a well-articulated description of the problems with existing implicit methods. It presents a clear logical flow and addresses specific challenges effectively. The results show particularly noticeable improvements for abrupt or jittery motions correction.
- Although Fourier series is not a novel concept, this paper extends it to the temporal dimension.

Weaknesses:
- It is undeniable that this work is closely related to the Fourier Occupancy Field [9]. The authors should consider adding comparative experiments with this work.
- This paper focuses on describing the acquisition of Fourier Query Flow in the method section but lacks a direct description of how the final pipeline for generating the 4D hand is constructed (e.g., how the pre-trained canonical field occupancy is utilized). A more comprehensive overview of this aspect should ideally be included in the first paragraph of the methodology section for better clarity.
- The pose flow and the shape flow are two important branches proposed in this paper, and the final flow is the sum of both. It would be helpful to provide clearer illustrations that show the trajectories generated by each branch separately, as well as the combined trajectory. This would enable readers to gain a clearer understanding of the method.

Limitations:
The authors have partially addressed the limitations of their work, though there is space for improvement (see the section Strengths And Weaknesses).

Rating:
5

Confidence:
4

";1
IGTbT9P1ti;"REVIEW 
Summary:
The SOTA approach in multi-modal learning employs contrastive loss to project embeddings from different modalities into a unified embedding space. However, models trained on different modal pairs result in disparate embedding spaces. This paper introduces a novel method to align distinct multi-modal embedding spaces by leveraging the overlapping modality they share. Additionally, it proposes several techniques to ensure a more robust alignment in the aligned embedding space. The experimental results presented in the paper demonstrate impressive performance across various tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: This paper introduces a novel extension to the existing CLIP approach, enhancing its capabilities by aligning distinct embedding spaces. It incorporates innovative ideas to achieve this alignment objective, contributing to the advancement of the field.
Clarity: The paper is well-structured and written in a clear style.

Weaknesses:
Necessity of Innovative Ideas: Although the paper introduces several innovative ideas, it would benefit from stronger evidence to support their necessity. Specifically, more clarification is needed regarding the significance of aligning the shared embedding space. What advantages does this method offer over labeling a dataset using the pretrained models utilized in this paper?
Lack of Experiment Descriptions: The experiment section would benefit from more detailed descriptions regarding the evaluation tasks and datasets used. 
Additional Baselines: To establish the significance of the proposed method, it is important to include appropriate baselines trained with the same amount of data as the proposed method in the experiments.

Limitations:
Yes

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper introduced a novel technique to learn multimodal contrastive representations (MCR) without paired data from the two modalities by using a third modality to bridge between the two existing modalities. The technique assumes the existence of a learned MCR between each existing modality and the third modality, and the proposed approach brings the 2 separate MCRs together by a learned projection layer trained with both inter and intra-MCR alignments. The paper used this technique to learn an MCR between audio and images, with only unpaired audio, image and text data and existing MCR between image and text (CLIP) and between audio and text (CLAP). Then the learned MCR between audio and image was used to zero-shot on 3 audio-visual tasks: audio-image retrieval, source-localization, and counterfactual audio-image recognition. The new method is able to achieve state-of-the-art performance on all 3 tasks zero-shot, out-performing MCRs learned directly with audio-image paired data. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1.The paper studies an important and challenging problem: learning a multimodal contrastive representation between two modalities without direct paired data.

2. The proposed approach is generally quite straightforward and easy to understand. The proposed approach also only adds a very small amount of additional parameters in the projection module, and requires limited additional training.

3. The proposed approach achieved state-of-the-art performance on three different audio-visual tasks zero-shot, outperforming MCRs learned directly with paired audio-visual data.

4. The paper conducted a comprehensive ablation study that examines and justifies every design choice in the methodology.

Weaknesses:
Although the proposed approach does not require paired data, it does require two pre-trained that happens to bridge each of the target modalities into a third one. While this happens to be the case for audio and image modalities (where CLIP and CLAP bridges both to text), it is unclear whether this approach could be applied to a wider range of modality settings, as all experiments in this paper focused on audio-visual MCR learning.

The experiments also only included cross-modal audio-visual tasks. It would also be interesting to evaluate how good are the learned audio-visual MCR on unimodal image or audio tasks (such as image classification), since MCR like CLIP had shown great performance as unimodal image representation in addition to cross-modal performance.

Limitations:
The authors did not address the limitations of the proposed method. The paper could be improved if the author discusses some limitations of their approach (such as applicability to different modality settings).

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents a Connecting Multi-modal Contrastive Representations (C-MCR) scheme that aligns the spaces of two already pre-trained MCR models by utilizing the overlapping modality between two, in a data- and training-efficient way. One interesting property of C-MCR is that aligning those two MCR spaces are feasible without the modality pairs to be aligned (e.g., image and audio) during training. To that end, two alignment objectives named Inter-MCR and Intra-MCR alignment and the Semantic Enhancement strategy are designed, while addressing the robust alignment and modality-gap issues. The Semantic Enhancement mechanism that consists of Inter-modality Semantic Consistency and Semantic Completion enables robust and comprehensive alignment. In addition, the two proposed objectives are the key to the semantic-enhanced inter- and intra-MCR connection in the C-MCR framework. As an instantiation of the C-MCR framework, the audio-visual-aligned contrastive representation is obtained using CLIP and CLAP models, and its effectiveness is verified by a series of experiments on various audio-visual tasks in a zero-shot manner. 


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The motivation for the C-MCR is clear and strong. Achieving the aligned MCR spaces without using the explicit modality pairs which can be expensive in some situations seems to be a clear advantage.

- The idea for aligning the MCR spaces including semantic completion mechanism and MCR alignment objectives is simple and straightforward, while it seems to be effective even lack of direct audio-visual pairs during training.

- The presentation of the work is quite clear and the paper is generally well written. The codes are included in the submission.

Weaknesses:
- It would be desirable to make analysis on whether the two projectors trained to indirectly align audio-visual modalities via the proposed C-MCR scheme can also improve the alignment between image and language / audio and language compared to simple CLIP or CLAP. If it could, such discovery can strengthen the contribution of this work.

- In this work, connecting only two MCR representations is explored. Discussion on whether the proposed scheme is seamlessly applied to aligning three or more MCR modalities would be helpful.

- Sensitivity analysis on some important hyper-parameters (e.g., temperature scaler $\tau$ and loss weight $\lambda$) is missing. How those sets of hyper-parameters are determined?

- For the projection layers, two layer MLPs with activations are adopted. Is the alignment ability affected by the depth of the projection layer?  Would like to see the performance changes when using single-layer MLP or MLPs with more than two.

Limitations:
Limitations and potential negative societal impact are addressed.


Rating:
6

Confidence:
3

REVIEW 
Summary:
- This paper proposes C-MCR, a multimodal contrastive framework that aligns multimodal representations of overlapping modality pairs to learn effective representations of the non-overlapping modality pairs. Specifically, the authors concentrate on learning the inherent semantics of visual-audio data solely through language-visual and language-audio pairs. The authors developed lightweight projectors, semantic enhancement techniques, and inter-intra-modality alignment objectives. Experiments are well designed on various downstream tasks in a zero-shot manner for C-MCR with different metrics, and C-MCR demonstrates superior performance compared to the chosen baselines. The authors further conduct ablation studies on the proposed components to evaluate the effectiveness of C-MCR.

---------------------------

My score is updated after reading the authors's rebuttal and the reviews from my peer reviewers.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The motivation to connect modality representations and reduce reliance on high-quality modality pairs is strong, and the proposed method is straightforward and clearly described.
- The authors have presented illustrative figures to explain their framework
- The proposed method demonstrates notable performance improvements over other baselines.
- Ablation studies are well-defined and provide clear insights into the effectiveness of the proposed modules

Weaknesses:
- Figure 1 is highly informative, but it may cause confusion when simply referred to as “As illustrated in Figure 1”. Adding more detailed text or subsection numbers to label the components would enhance the clarity for the readers.
- It would be more promising if the approach section could be generalized for additional modalities to demonstrate the extensibility and significance of the proposed methods in various applications involving multimodal data. 
- Related works could also include additional multimodal contrastive learning frameworks [a1, a2]
- The proposed modality pair alignment is very interesting, however, the idea of projectors and contrasting components has already been explored in the past.

[a1] ""Contrastive multiview coding."" Computer Vision–ECCV 2020

[a2] ""Geometric multimodal contrastive representation learning."" International Conference on Machine Learning. PMLR, 2022.”

Limitations:
- The authors could provide a more comprehensive discussion on the limitations of the proposed method and future works. For example, they could explore the possibilities of extending the method to accommodate additional modalities and the application in other domains. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on extending existing Multimodal Contrastive Representation (MCR) to more modalities without massive high-quality data pairs. To this end, they propose a training-efficient method for learning a new MCR without paired data. Specifical, they connect one existing MCR pre-trained on modality $(A, B)$ and one existing MCR pre-trained on modality $(B, C)$, to obtain the new MCR pre-trained on modality $(A, C)$. As a result, they connect the pre-trained CLIP and CLAD models to derive audio-visual data, which achieves significant retrieval performances on AVE and Flickr-SoundNet. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	**The writing is clear and well-motivated.** It is necessary to extend multimodal contrastive representation to more modalities without massive high-quality data pairs. 

2.	**Technically sound and easy to understand.** The semantic-enhanced inter- and intra-MCR connection method is flexible and training-efficient.

3.    **The results are impressive.** Without any paired audio-visual data, they achieve state-of-the-art performance on six datasets. More specifically, Tab.2 and Tab.3 show that C-MCR surpasses existing methods by a large margin. 

Weaknesses:
1.  **The main results are not sufficient**.  The current experimental results are only experiments on the visual-audio task. This is not enough to support the claim of connecting multi-modal contrastive representation.  This article should provide more modality alignment experiments, e.g. (3D point, image)->(text, image), (text, image)->(image, sketch/RGB-D).

2.  **The comparison experiments are not fair**. As shown in Table 2 and Table 3, a column should be added to the table to discuss which pre-trained models are used. It is unfair to only compare performance with different pre-trained model usage settings.

3.   **Lack of thorough analysis of the proposed method**. The inter-MCR and intra-MCR are needed for visualizing by T-SNE to understand the mechanism of modules. In addition, the intra-variance and inter-variance between different modalities should be measured w or w/o inter/intra-MCR.

4.  **The proposed method is trivial**. To some extent, there have existed similar methods [1][2][3][4] that align the semantic-enhanced embeddings across different modality spaces. Can you provide a more thorough comparison to illustrate the novelty of your work?


[1] Text-only Training for Image Captioning using Noise-injected CLIP. EMNLP 2023.

[2] I Can't Believe There's No Images! Learning Visual Tasks Using Only Language Data. Arxiv 2022.

[3] From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping. IJCAI 2023.

[4] DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training. ICLR 2023.


Limitations:
N/A

Rating:
5

Confidence:
4

";1
2NkGfA66Ne;"REVIEW 
Summary:
This paper introduces a simple and efficient method for general 3D segmentation based on SAM (a powerful 2D foundation model) and NeRF-style representation. Instead of building a 3D foundation model from scratch, SA3D uses two steps to lifts 2D SAM segmentation results to 3D in a more concise and efficient way. Based on a well-trained NeRF, a rendered reference view and the human input prompts are sent into SAM to get the first segmentation. Then the proposed mask inverse rendering and cross-view self-prompting strategy will help optimize a volume-based 3D mask field and propagate segmentation information to different views in an iterative and incremental manner. The comprehensive experiments prove the effectiveness of the designed pipeline.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1.	The proposed method is simple but effective. It provides a general interactive 3D object segmentation paradigm which do not rely on heavy pre-training.
2.	The method is pretty efficient. Given a pre-trained NeRF, the 3D segmentation can be completed within only minutes with the help of SAM.
3.	The experiments have covered variant datasets and comparison to SOTA methods, which are comprehensive and persuasive.
4.	The paper is well-written and easy to follow.


Weaknesses:
1.	I expect that segment “anything” in 3D means the method can segment all the things in a 3D scene, so that it is consistent with the purpose of SAM. But the proposed SA3D can only segment one object at each time.
2.	Furthermore, using prompts (such as points, scribbles, text) to achieve 2D segmentation of one target object is a long-studied subject. Besides SAM, there should be many alternative choices such as [1][2][3], which should have been evaluated for this 3D task. Or the irreplaceability of SAM needs to be explained.
3.	I wondering if the method requires complete observation of the target object in at least one image. What if the target object is not observed completely in any views? How will the choice of reference view affect the 3D segment results? Is there any necessary strategy to achieve best performance? The robustness of the method should be evaluated.

[1] Sofiiuk K, Petrov I A, Konushin A. Reviving iterative training with mask guidance for interactive segmentation[C]//2022 IEEE International Conference on Image Processing (ICIP). IEEE, 2022: 3141-3145.
[2] Liu Q, Xu Z, Bertasius G, et al. SimpleClick: Interactive image segmentation with simple vision transformers[J]. arXiv preprint arXiv:2210.11006, 2022.
[3] Chen X, Zhao Z, Zhang Y, et al. Focalclick: Towards practical interactive image segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 1300-1309.



Limitations:
Yes. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to lift 2D segmentations from foundation models such as SAM to 3D by iterating between SAM and NeRF, without re-training or re-defining either. Given a trained NeRF model, the model first renders a view, which is also processed by SAM given a user click. With the segmentation by SAM, the model optimizes a 3D segmentation volume such that it volume renders into a mask consistent with what SAM produced (“mask inverse rendering”). Next, the model projects this initial segmentation volume to other viewpoints, producing incomplete 2D masks. Finally, the model computes “good prompts” for SAM to complete these masks (“cross-view self prompting”). The model iterates between these steps until a complete segmentation volume has been produced.

Because of the generalization power of SAM, the model is able to segment almost anything in 3D, without requiring applying changes to SAM or NeRF, making itself a framework that can be applied to any 2D foundation models that we want to lift to 3D.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:

The proposed method is simple yet effective, following the recent trend of developing foundation models and/or open-vocabulary LLMs. More importantly, it also bridges the gap between powerful 2D models and 3D understanding as required by robots or autonomous vehicles. It is general and applicable to any 2D foundation models. The results are strong both qualitatively and quantitatively.



Weaknesses:
Since the framework is claimed to be (and I think it is) general and applicable to any 2D foundation models, the paper will be much stronger if the authors could demonstrate the use of this framework to lift another foundation model’s output into 3D. 

The paper will also benefit from showing some 3D shape results that are extracted from the 3D segmentation volume. Often, it’s the underlying 3D geometry that matters for, say, robotic manipulation. “RGB looking good” is separated than that.


Limitations:
Yes

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a method for segmenting a pre-trained NeRF by utilizing SAM. Given a pre-trained NeRF, it first asks users to provide prompts (e.g., some points) for a reference view. It then utilizes the SAM to generate a 2D segmentation for the reference view and utilizes the 2D mask to optimize a 3D segmentation mask. After that, it will iteratively update the 3D mask by (a) selecting a random review, (b) rendering the view, (c) utilizing the 3D mask to automatically generate prompts for the view, (d) utilizing prompts to query SAM, (e) utilizing SAM output to update 3D mask. The method proposes a self-promoting strategy to generate prompts given the 3D mask and an IoU-aware view rejection to ignore the bad prediction from SAM.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper proposes a novel method for segmenting a pre-trained NeRF by using SAM.
2. The paper proposes a self-promoting strategy for automatically generating prompts for SAM and a rejection strategy to ignore bad SAM predictions.
3. The authors conduct experiments on three datasets and provide some ablation studies.
4. The paper is easy to follow.

Weaknesses:
1. The method requires minutes for a single segmentation (e.g., of an object), which may greatly limit the usage of the method in many real-world applications (e.g., robotics manipulation).

2. Recently, there are also many NeRF/Point Cloud-based open-world (vocabulary) 3D segmentation (for both scene-level and part-level) methods[1-8] that leverage pre-trained 2D VLM (e.g., CLIP). However, the discussion and comparison with them is missing. It seems that many of these prior methods don't need per-instance optimization and can generate a 3D segmentation mask in just seconds. Please cite these papers and discuss the advantages of the proposed methods.

3. In Line 153, the paper states, ""Given an incomplete 2D rendered mask"". Why is the rendered mask always incomplete? Is it possible that some SAM predictions are wrong and include extra regions, which leads to an enlarged 3D segmentation mask? If this is possible, the self-prompting strategy will also generate a wrong prompt for SAM in the later steps. Please explain whether this case is possible and how the proposed method can handle the wrong SAM prediction (including extra regions).

4. The negative refinement term (Line 137) is unclear to me. Please explain in more detail about the motivation for this term.

5. Equation (7) is not clear to me. Could you explain in more detail? Also, it would be better to provide an ablation study to verify the necessity of the confidence decay step (Equation (7)). Can the self-prompting strategy still work without confidence decay?

6. For all tables, could you include runtime for both the proposed method and baseline methods?

7. It would be better to include evaluations on some standard 3D segmentation benchmarks (e.g., ScanNet and PartNet) to have extensive comparison with existing methods as well. 



[1] Kerr, Justin, et al. ""Lerf: Language embedded radiance fields."" arXiv preprint arXiv:2303.09553 (2023).

[2] Peng, Songyou, et al. ""Openscene: 3d scene understanding with open vocabularies."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[3] Ding, Runyu, et al. ""PLA: Language-Driven Open-Vocabulary 3D Scene Understanding."" arXiv preprint arXiv:2211.16312 (2022).

[4] Ha, Huy, and Shuran Song. ""Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models."" 6th 
Annual Conference on Robot Learning. 2022.

[5] Zhang, Junbo, Runpei Dong, and Kaisheng Ma. ""Clip-fo3d: Learning free open-world 3d scene representations from 2d dense clip."" arXiv preprint arXiv:2303.04748 (2023).

[6] Liu, Minghua, et al. ""Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[7] Yang, Jihan, et al. ""Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding."" arXiv preprint arXiv:2304.00962 (2023).

[8] Jatavallabhula, Krishna Murthy, et al. ""Conceptfusion: Open-set multimodal 3d mapping."" arXiv preprint arXiv:2302.07241 (2023).


Limitations:
Yes

Rating:
5

Confidence:
5

REVIEW 
Summary:
The authors propose a combination of the newly introduced Segment-Anything Model (SAM) with Nerf, yielding the  Segment Anything in 3D (SA3D) system.
SA3D cam take a 3D scene reconstructed by Nerf and based on a user prompt (e.g. a few keypoints) can carve out distinct 3D objects from the scene. This is shown to outperform previous state-of-the-art systems. 
They two novel bits to successfully combine the two approaches:
Firstly a loss function in order to induce a 3D mask field on a voxel grid based on the SAM outputs (Eq. 4) and secondly (and most importantly) a method to prompt SAM on another view based on the existing 3D mask view - allowing one to only prompt from one image and then perform ""prompt propagation"" to the rest.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Simple approach that should be easy to reproduce - the authors also share their code, which looks reasonably clean. 

Solid experimental improvements - I am convinced by these numbers that the method outperforms some of the latest state-of-the-art systes.

Good ablations.

Interesting -but a bit hand-wavy - results that SA3D can improve SAM; some more validation of this would be welcome.

Weaknesses:
- Overstatement: the authors are  using SAM in tandem with Nerf, to get 3D objects out of a scene, which is great - but practically this is similar to classic co-segmentation or the systems that they compare to, but a bit better because of piggy-backing on SAM. 
Stating in the abstract that  ""Our research offers a generic and efficient methodology to lift a 2D vision foundation model to 3D, as long as the 2D model can steadily address  promptable segmentation across multiple views."" suggests more than just this - one can start imagining extending the attention operations to 3D, supervising for depth/volumetric reconstruction etc, or more importantly having a foundation model for 3D, none of which is  the case based on what we have in the present paper. Technically the statement is not false - but implies more than what is actually happenning in the paper. 


- I could not find any discussion about the computational efficiency of the proposed algorithm. At the moment it's unclear what is the importance of this.

Minor:

- Unclear intuitively what the second term does in Eq. 5 from the way it's written. I think a more obvious rewrite is 
(lambda - 1) L_proj + lambda sum_r M(r) \propto L_proj + lambda/(lambda-1) sum_r M(r), 
and explain that the second term is just a regularization term on the optimized segmentation field.

- l. 280: ""we demonstrate limitations of SA3D in panoptic segmentation"" -> could not find any pointer to this in the paper.

Limitations:
Yes

Rating:
7

Confidence:
3

";1
YV1MYtj2AR;"REVIEW 
Summary:
This work presents an approach to train model-based RL methods such that it generalize to novel views on multiple RL benchmarks. The method leverages classical STN and frozen encoder to benefit the generalization performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The method is sound and simple with barely hyperparameters tuning

The design choices study such as different moving views and the insight are helpful for research community.

Weaknesses:
The writing can be largely improved on section 2 and 3. It is unclear what the model-based RL problem is, and how to define the view generalization, how to map the model to actual control/policy for evaluations, for audience that are not familiar with this line of work.

The problem is specific and the method novelty are limited.

Comparisons with baselines are unclear. At least some comparison with model-free RL and recent work [1] would be helpful.
[1] Multi-View Masked World Models for Visual Robotic Manipulation, Seo et al., 2023

Limitations:
See above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper mainly provides a training paradigm. Utilizing the dynamic transition model of the environment during the testing phase as a supervisory signal, STN is used to quickly finetune the mapping of observed potential states, resulting in better performance of the strategy mapping trained on a single view task on unseen test views. This work conducted comparative experiments on three types of tasks and four challenging views, achieving results that surpass existing methods. Ablation experiments were conducted to prove the effectiveness of each module setting.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The training paradigm provided in this article can effectively handle the generalization problem when view changes occur, and a thorough comparative experiment and ablation experiment have been conducted on the proposed model. The core idea of reconstructing mapping h using the environmental dynamic transfer model in the paper is effective.

Weaknesses:
1. The subscripts for o and a in Formula 2 are missing.
2. There are issues with the baseline selection of IDM+STN. Cheetah run alone cannot prove that IDM+STN is generally better than IDM. It is not a problem to conduct ablation experiments on the Cheetah run model alone, but this cannot be used as sufficient evidence for the superiority of IDM+STN over IDM and thus selecting it as the baseline.
3. Chapter 3 emphasizes the differences between Formula 2 and Formula 1, on one hand, due to the fixed parameters of Network d, and on the other hand due to the use of SAE in Network h. One of the core methods is to fix the parameters of d, which is intuitive to avoid changes in the target domain of h during the update process, resulting in the failure of the policy pi. However, the ablation experiment showed that the setting of d^* did not achieve a consistently effective effect.
4. The main approach is to use the dynamic transfer model of the environment under the new view as supervision to quickly train the changed h '. Can you obtain a generalized h directly based on o'?

Limitations:
The authors have discussed the limitations

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper addresses the novel problem of view generalization in reinforcement learning, where an RL agent is trained on an environment with a fixed view and then evaluated on a test environment having the exact same dynamics but observed from a different perspective. In order to address this issue, the authors introduce an innovative method that permits test-time adaptation to the new view. 
The authors integrate a learnable spatial transformer network (STN) into the feature extraction component of the agent. This augmented encoder is then fine-tuned at test time to generate features that enable a frozen latent dynamic model to predict future state representations. An essential aspect of the authors' approach is that this adaptation does not necessitate any reward signal, which is a key advantage.
Experimental results demonstrate a significant reduction in the generalization gap presented by a new view across several heterogeneous benchmarks.




Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The paper is well-structured and easy to follow, providing clear and detailed explanations that make  believe that I could readily reproduce the method based on the descriptions provided.  
The authors introduce a new problem in the field that could stimulate further research.  
The proposed method's strength lies in its test-time adaptation capability, which eliminates the need for any reward signal, making it highly applicable to real-world scenarios. This characteristic also suggests that the approach could be integrated into any model-based method, enhancing its universality.  
Another advantage is the apparent minimal interaction required with the testing environment, promoting efficiency.  
The method is evaluated across various environments, with the results showcasing impressive improvements compared to non-adaptive methods.  
The paper's ablation study effectively illustrates the contribution of each component of the method, highlighting the importance of every aspect in achieving the observed results.  Additionally, the annex offers excellent visualizations demonstrating the impact of the spatial transformer on the feature map, providing intuitive understanding of the method's mechanics.



Weaknesses:
Despite the many strengths of the paper, a few areas could benefit from further development and clarification.

1. It would be beneficial to see a comparison of the performance decrease relative to the original view. The lack of this data makes it challenging to truly gauge the significance of the generalization gap and the efficacy of the proposed method.

2. The scope of comparative studies is somewhat limited. The paper primarily compares with methods not designed for visual adaptation, which may not provide the most insightful comparison. It would have been advantageous to see how the proposed approach stacks up against other strategies specifically intended for visual adaptation.

3. An exploration of whether the proposed method can also benefit model-free algorithms such as Soft Actor-Critic (SAC) is missing. This could broaden the applicability of the findings and provide additional insights into the method's versatility.

4. It remains unclear why the proposed method underperforms with the Inverse Dynamics Model (IDM). A deeper exploration of this anomaly could bolster the robustness and reliability of the approach.

5. Finally, the similarity between the proposed method and the PAD method, which performs test-time domain adaptation in model-free RL, raises questions. The key differences are the use of a Spatial Transformer Network and the leveraging of the latent dynamics model of model-based methods instead of adding it as an auxiliary component. However, these differences, while noteworthy, do not necessarily represent a substantial departure from the PAD method. This similarity begs the question of whether the presented method offers significant novelty or if it is essentially an adjustment of existing approaches.

Limitations:
The authors mentioned a limitation of this work: the proposed method has not been tested in real-world scenarios, such as with physical robots, which may limit its immediate applicability.
Another potential limitation could be that the method has only been tested on model-based reinforcement learning (RL), whereas it would be relatively straightforward to evaluate its effectiveness in model-free RL.



Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on improving the generalization ability of visual DRL to adapt to unseen views. The authors propose a model-based policy adaptation approach that combines spatial transformer networks with a self-supervised dynamics prediction objective to address this problem. The effectiveness of the approach is evaluated through experiments on three commonly used benchmarks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- This paper is well-written and easy to follow. 
- The proposed method is well-motivated. 
- The authors try to solve a realistic and essential problem, i.e., view generalization, especially in real-world or sim-to-real robotic scenarios.

Weaknesses:
- While the problem setting is attractive, the technical contribution is insufficient as a full NeurIPS paper. It is a straightforward combination of multiple existing works, like STN. 
- There needs to be more than the current evaluation to demonstrate the superiority over existing works.

Limitations:
Not Applicable.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Adapting policy into new view setup is an important task in RL. This paper presents MoVie, Model-based policies for View generalization to achieve the fast view adaptation of model-based policy. Specifically, they combine spatial transformer networks into the encoder models and train it during test-time using dynamics prediction loss. Despite the simplicity, the experimental results show that the strong performance of MoVie over several benchmarks on various test set. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The proposed method is simple yet provide stable performance gain over various methods. Experimental validation is extensive enough. 
2. The paper is well written and easy to follow. 

Weaknesses:
1. Technical novelty and technical depth is limited. I might decrease scores if I found works that combine STN with test-time training. 

Besides, few discussions are made on the choice of test-time loss and entire architecture. For example, the experimental results suggest that the DM is better than IDM especially when combining test-time training, but the paper lacks discussion on why it is. Besides, I agree prediction loss is one of the natural loss to train STN, but I think it is also possible to extend the method to model-free methods by using some self-supervised loss function (like contrastive learning). 

2. I think the experimental validation is already comprehensive, but it lacks qualitative analysis of their representation before/after adaptation. 

3. No theoretical justification for the choice of test-time loss and or entire architecture. 


Limitations:
N/A

Rating:
6

Confidence:
4

";1
YwgA3avHrP;"REVIEW 
Summary:
This manuscript presents a clip-assisted semantic image segmentation method for surgical instruments. In terms of methodology, the proposed work can be viewed as an adaption of CLIPSeg to a domain-specific problem. Compared with CLIPSeg, a mixture of prompt strategy is used for augmenting text prompt information. Hard sample mining is also employed to further improve segmentation performance. The image segmentation network architecture has also been optimized. The proposed method is evaluated on two public surgical instrument segmentation datasets and is shown to outperform some fully-supervised and clip-assisted methods.

**Commented after rebuttal**: I would like to thank the authors, other reviewers, and the ACs. I have carefully read through the rebuttals, and the comments from other reviewers. The discussion with the authors is constructive. Most of my raised concerns have been properly addressed.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed method yields improved performance on two surgical instrument segmentation datasets, compared with some existing fully-supervised/clip-assisted methods.

Employing text prompt brings additional flexibility in terms of the categories of segmentation targets. This is a major advantage of clip-assisted segmentation over conventional fully-supervised segmentation.

The proposed method is introduced in sufficient detail and can be easy to follow.

Weaknesses:
The take-home information for readers may be limited/unclear. Despite that domain-specific adaptions on top of vanilla CLIPSeg leads to improved performance on surgical instrument segmentation, these contributions themselves may not be of sufficient interest to readers: hard sample mining, feature pyramid, generating multiple text prompts are already the common practice and should have been well-known for the community. The authors are encouraged to highlight their key take-home information to readers, and argue how the proposed work brings new knowledge.

In the abstract and introduction, the authors highlight improved flexibility when dealing with new segmentation targets of clip-assisted segmentation models. However, this point is not sufficiently evaluated and discussed in the experiments: how does CLIPSeg and CRIS that are also clip-driven work in this context?

The authors may also want to discuss/comment on a closely-related existing work [1].

1. CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection

Limitations:
Automated instrument image segmentation may affect the outcome of a surgery, which may lead to real-world impact to the clinicians and the patient. The authors are encouraged to discuss the potential impact of their work for patients, clinicians, device manufacturers, and the society. 

The authors are also encouraged to discuss if the proposed method exacerbate/mitigate potential risks and biases in image segmentation.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel text promptable surgical instrument segmentation approach to overcome challenges associated with the diversity and differentiation of surgical instruments by using the large CLIP model and a text promptable mask decoder. The experiments show the effectiveness of the proposed method on surgical instrument segmentation. However, I am very concerned with the novelty of the proposed modules due to the limited situations.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1, The paper is well-written and easy to understand.

2, The experiments show the effectiveness of the proposed modules.

Weaknesses:
1, There are too few novelties due to most of the proposed modules being explored in the traditional segmentation tasks.

2, The dataset is too limited to prove the effectiveness of the proposed modules. Furthermore, there is no specific module designed for surgical instrument segmentation.

3, Lacking the comparison of inference time.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a text prompt-based surgical instrument segmentation method, which is more scalable to handle the diverse targets in endoscopy videos.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- originality: this is the first work to use text prompt for surgical instrument segmentation
- quality: the performance of the proposed methods surpassed previous methods
- clarity: the paper is well organized
- significance: the method can handle unseen targets, which is desired in clinical scenarios.

Weaknesses:
- EndoVis challenge is organized every year. However, this paper validated the method on old datasets (18-19). Why not use the latest dataset and compare it to the challenge winning solutions? e.g., 21-22
http://opencas.dkfz.de/endovis/challenges/2022/

- Reference formats are not consistent.

Limitations:
A more comment task is to segment both instruments and tissues.  It would be great to validate the method in this setting. Here is a public dataset
https://www.kaggle.com/datasets/newslab/cholecseg8k

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel approach for surgical instrument segmentation in minimally invasive surgeries. By leveraging text prompts and vision-language models, the proposed method achieves improved segmentation performance. The approach shows promise for practical use in robotic-assisted surgery.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The present work contributes with an innovative and effective approach for text promptable surgical instrument segmentation in minimally invasive surgeries.

This paper presents a meticulous study of previous work, which is important in the development of the present work. Also, the technical aspects are clearly explained and have also been evaluated using the correct metrics.

Another strength of this paper is the introduction of a mixture of prompts mechanism. By leveraging multiple text prompts for each surgical instrument, the authors enhance the segmentation performance of their model.

The experimental evaluation of the proposed model on EndoVis2017 and EndoVis2018 datasets demonstrates its superior performance compared with other works and promising generalization capability.

In summary, the work is an interesting application of deep learning in the medical area, and it also has a remarkable novelty.

Weaknesses:
Regarding the ablation study, it would be good if the authors could explain why they chose 448x448 as the image size. Aren't some details lost using this size?

It would be good if the authors could give more details about the dataset, i.e. the average duration of each video for example. 

It would be very positive to also include more datasets, such as EndoVis2019, EndoVis2020 or EndoVis2021.

Limitations:
limitations are not mentioned, authors should include the limitations in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel idea of utilizing text prompts and vision-language models to make surgical instrument segmentation more flexible and robust to diversity. The proposed method and custom modules achieve strong results on two endoscopic datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The paper tackles an important problem in surgical instrument segmentation, which aims to enhance robot-assisted surgery systems. The key idea of using text prompts to improve generalization and adaptability to new instruments is novel.
2. The method is technically sound, leveraging recent advances in vision-language models like CLIP. The image and text encoder setup makes sense. The text promptable mask decoder uses attention and convolution schemes nicely for decoding.
3. Several custom modules are proposed to boost segmentation performance: 1) Mixture of prompts leverages multiple prompts effectively. 2) Hard instrument area reinforcement focuses on challenging regions.
4. Comprehensive experiments on two datasets demonstrate superior performance over state-of-the-art methods. The cross-dataset generalization results are promising. The ablation studies validate the efficacy of individual model components like multi-scale feature extraction, mixture of prompts, and hard area reinforcement.

Weaknesses:
1. The problem definition and goal can be further sharpened. How does text-based prompting specifically help with increasing instrument variety and subtle inter-class differences? This needs more elaboration upfront.
2. Some architectural details are unclear - like how exactly text features are integrated into the convolutional prompting scheme. More implementation specifics will help reproducibility.
3. The computational complexity and inference speed are not analyzed. This could be important for practical usage.
4. More in-depth experimentation on real-world surgical videos and systems would be preferred to further demonstrate applicability.

Limitations:
1. The method has only been evaluated on two datasets with limited surgical scenarios. Performance on more diverse real-world data is unclear.
2. It relies on high-quality textual prompts, which may not always be available or easy to construct in practice.
3. The requirement of retraining with new text prompts for new instruments reduces adaptability.

Rating:
6

Confidence:
4

";1
UFW67uduJd;"REVIEW 
Summary:
This paper presents a reconstruction-based method for multivariate time series anomaly detection. It is a memory-guided Transformer which contains the gated memory module. Because of the training instability in updating memory items incrementally when the items are
initialized randomly, the authors propose a two-phase training paradigm to ensure stable training. The method calculates anomaly scores considering the input and latent space. The method achieves state-of-the-art results on five benchmark datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The authors make a good explorations on leveraging memory module to multivariate time series anomaly detection task, and the proposed method can be effective solutions for the given task.
2. This paper contains enough experiment results and ablation study on five datasets to support its claims.
3. The proposed methodology is well presented, and most of the paper is well written.

Weaknesses:
1. The paper writing can still be improved. For example, the grammar error on line 88 ""there exist video representation learning"" -> ""there exists video representation learning"". Typo on line 234 ""32.56p%"". The citation in related work can be extended with author's name + 'et al.' rather than just a number. 
2. The font size in the figures (in introduction and experiment) is pretty small.
3. I saw that you split the training data into 80% for training and 20% for validation. Do other methods for comparison in the experiments share the same data split(ratio and files)？ How many times you repeated your experiments? Since I saw the results you reproduced for Anomaly Transformer are quite different from their own paper, is it fair to list all these methods to compare with your model?


Limitations:
Yes, the authors have addressed the potential negative societal impact of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose a memory-guided transformer with a reconstruction paradigm for multivariate time series anomaly detection. The time-series encoder, memory parameters of normal patterns, and projection heads are updated during training time in a two-step fashion. On test time, input queries are projected on the normal patterns learned base with a learned sparse linear projection. As a result, reconstructed anomalies tend to look like normal samples, amplifying the reconstruction loss and enabling detection. Experiments are run over common multivariate time-series datasets following previous works practices.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The proposed gated memory module seems simple, original, and efficient for detecting anomalies on multivariate time series based on transformer encodings and input sample reconstruction. 

* The authors provide thorough ablation studies and some insights into the importance of each part of their algorithm.

* The manuscript is overall easy to follow and well-written. Code is provided.

## Suggestions

The typical memory size is quite small, which I see as a positive point of this method. So, I suggest this could be emphasized in the main manuscript. I read the entire paper wondering what the memory size was and whether the trade-off memory-performance was good, just to be positively surprised in the appendix.

Weaknesses:
## Regarding novelty

1. Overall this work seems to be an incremental modification of [8], [24], and [40]. In particular, contribution #3 (line 77) does not seem very novel. I would like the authors to please discuss Equation (11) in this paper versus (6) in [40] in more detail.
2. How does the memory update stage is different from other memory gates, such as GRU, MRU, etc?

## Regarding state-of-the-art claim

1. Why not compare to MNDA [24], memAE [8], USAD[2] in Table 1? Why are A.T. numbers in Table 1 different from [40]? I would suggest adding error bars at least to these methods and yours to make the comparison fair. I know this was not done in these previous works, but claiming state-of-the-art without comparing confidence intervals in an empirical field is not a good practice.

## Regarding methodology

4. Not clear why not just have a memory on the input space and retrieve the closest samples instead of the proposed mechanism on the latent space?
5. Why is it so sensitive to k-means initialization? Is it computed with enough iterations of the K-means algorithm?
6. How does algorithm 1 outlines the two-phase training paradigm (line 175)? The first and second phase seems to be omitted.

## Other less important questions

7. How are the thresholds computed in figure 3?
8. What is the computation overhead when compared to A.T.? MEMTO seems simpler computationally. Could the authors discuss this?


Limitations:
The authors have discussed potential limitations and shortcomings of their algorithm and no special negative societal impact exclusive of their work is evident.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper used memory network to capture frequently present normal pattern in date set. While training, the memory are built in latent space in the autoencoder framework.  Along with reconstruction error, the distance between a representation with the closest memory unit is used to calculate anomaly score. If a data does not have representation close to memory then that is an anomaly. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Idea of keeping more than one memory for common patterns from normal data is Novel and makes more sense than existing methods of keeping memory of anomalies.  Experimental results were produced on a large number  of data sets.  

Weaknesses:
Literature surveys missed graph based anomaly detection techniques for multivariate time series like Deng, Ailin, and Bryan Hooi. ""Graph neural network-based anomaly detection in multivariate time series."" In Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 5, pp. 4027-4035. 2021.


It is required to have an ablation study in the main paper to bring our significance of memory gate and Kmeans clustering. 


Limitations:
Number of majority patterm in normal part need to be known before which is not possible during unsupervised task. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors of this paper focused on tackling the problem of over-generalization in reconstruction-based deep models and made a contribution to the field of multivariate time series anomaly detection. The main challenge they encountered was dealing with complex dependencies and inter-variable correlations within the data. To address this, they proposed a novel gated memory module approach. This module learns how much each memory item should be updated based on the input data, effectively capturing the prototypical features of normal patterns in the data. By doing so, it aims to overcome the inability of existing models to capture dynamic nonlinear temporal dependencies and complex inter-variable correlations.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
• The authors explained the importance of multivariate time series anomaly detection with the proper real-world examples
• Comprehensive comparison of their approach with traditional methods (OC-SVM, Deep-SVDD, LSTM-VAE, Anomaly Transformer etc.)
• Thorough explanation of the training hyper-parameters, dataset setting, algorithms and code
• Achieved state-of-the-art results on five real-world benchmark datasets

Weaknesses:
• Lack of theoretical proof: two-phase training approach and bi-dimensional deviation-based criterion need a detailed theoretical proof
• Limited discussion of computational efficiency: how gated memory module and two-phase train- ing affect the training time
• I couldn’t find any explicit explanations that how the LSD and ISD help the training. In the descriptions, LSD and ISD only appears in Table2 (and the sec3.4). If LSD+ ISD just help the anomaly score but doesn't help the training, this should be clearly justified. 
• Need more quantitative experiment about the improvement from gated memory seems limit. (Table 3)

Limitations:
Please refer to the Weaknesses and Questions.

Rating:
4

Confidence:
4

";1
37cADkATD0;"REVIEW 
Summary:
---
I have raised my score based on the answers and results provided by the authors during the rebuttal. 

---
This paper proposes an algorithm named ExpGen that can selectively exhibit a maximum entropy exploration behavior at test time by measuring epistemic uncertainty through ensemble of policies. In order to obtain a policy trained to maximize the entropy, the entropy is formulated as an intrinsic reward, where the sample based approximation to state distribution entropy is obtained using a trajectory of states are used as neighbors. The experimental results show that ExpGen is able to achieve highest score in two of the five ProcGen environments used in the paper, namely, Maze and Heist.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper proposes novel framework that combines the idea of switching strategies based on the epistemic uncertainty and leveraging the maximum entropy policy to bridge the gap between the performances during train and test time.

Weaknesses:
Some parts of the logic are unclear and there are few typos and mistakes in the paper. The experimental draws some what countering argument against the motivation of using the general framework instead of ones based on inductive biases, e.g., IDAAC. This naturally leads to combination of IDAAC and ExpGen, but is mentioned in the paper, but not experimented. Another weakness would be the memory and computational inefficiency, resulting from needing to train and inferencing ensemble of policies. Finally, the empirical studies does not draw a clear picture of why and how ExpGen works, i.e., most results are observed performances of the comprehensive algorithm, lacking a detailed ablation of each components of the algorithm.

Limitations:
Although limitations are included as part of the discussion, some clear limitation could be directly stated, which should help making a clear distinction of ExpGen to other algorithms. For example, comparing memory and computational efficiency against using designs like PPO with extrinsic and intrinsic reward instead of ExpGen with ensemble.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper is based upon the key insight, that maxEnt exploratory policies exhibit a much smaller generalization gap than usual reward seeking policies.
Previous work introduced the framework of epistemic POMDPs, where a random action is chosen until the uncertainty of the policy is low enough again (quantified by policy ensemble members that agree or disagree).
The paper improves upon the previous work by using the maxEnt exploratory policy instead of the random policy when policy ensemble members disagree.
This allows to substantially improve generalization performance on ProcGen tasks previous methods fail on (Maze and Heist).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Very well written introduction and related work.
- Well executed idea to improve weaknesses of previous state-of-the-art algorithm for specific generalization problems of the ProcGen suite.
- Interesting and well executed empirical investigation.

Weaknesses:
**Major Weaknesses**

- The usage of L0 norm instead of L2 norm in Eq. (8) is not justified, as to the best of our knowledge, the (log of the) k-NN distance does not approximate the entropy under this norm, but only under the euclidean (L2) norm. Therefore, maximising Eq. (8) might not yield a maxEnt policy.
- Evaluation protocol of main experiments is not clear (see questions).

**Minor Weaknesses**

- In line 72 / 73, it is stated that the exploration policy is GUARANTEED to generalize. The paper does not contain a formal proof, but rather an empirical investigation, therefore this claim should be down-toned to reflect this correctly.

**Remarks**

- Please check the citation style again, often \cite is used instead of \citep.
- Use \eqref to reference equations.
- Check notation (e.g. the true state as part of the history in line 128)

Limitations:
The authors openly discuss limitations of their approach for other environments of the ProcGen suite, which are better suited for different algorithms that incorporate invariances into the trained policy.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies zero-shot generalization in RL. They first make an interesting observation: that intrinsic novelty-based rewards corresponding to maximum entropy exploration exhibit a smaller generalization gap than extrinsic environment rewards on ProcGen games. This suggests that MaxEnt rewards are in some sense richer and harder to memorize, and the agent is less prone to overfitting to them and more likely to generalize its exploratory behavior to new levels. The MaxEnt reward here is implemented using a kNN based method, similar to ProtoRL of Yarats et al. 

Based on this observation, the paper proposes a new algorithm (ExpGen), which trains a MaxEnt exploratory policy (which, as noted above, should generalize its exploratory behavior well), as well as an ensemble of K exploitation policies using the usual extrinsic reward. At test time, if the ensemble agrees on an action (indicating good generalization), then this action is executed. If not (indicating poor generalization), the exploratory policy (which is assumed to generalize well) is executed instead for a certain number of steps, and the process is repeated. 

This algorithm is evaluated on the ProcGen benchmark, where it is compared to a number of other published methods (PPO, PLR, UCB-DrAC, PPG, IDAAC, LEEP). On two games (Maze and Heist), it significantly outperforms the other methods. However, it significantly underperforms in several others. 

Overall, this paper has several things going for it: the insight that MaxEnt reward has more favorable generalization properties is definitely interesting, and I think the ExpGen algorithm (or some variant of it) has potential. However, the experiments do not (yet) convincingly make a case for this algorithm: while it shows advantages in some games, it significantly underperforms in others and its aggregate performance does not seem favorable. This may be fixable — as the authors note, the invariances induced by other algorithms are orthogonal to the contributions of ExpGen, so I suspect the benefits could be combined. I would suggest combining the architectural modifications and auxiliary losses from IDAAC with ExpGen - if this indeed combines the best of both algorithms, the resulting method would be a lot more convincing. This is discussed in the paper, but not done, and in my opinion needs to be tested. 

Second, there are a number of presentation and/or methodological issues which also need to be addressed. Please see my comments in the Weaknesses section. 

I think that if all these issues can be addressed, then this would make a strong submission. I think the substance of this paper is good, but it probably requires another revision cycle to be polished enough for publication. 

**Post rebuttal update: the authors have addressed my two main concerns. They have shown that ExpGen can be combined with IDAAC to get robust and SOTA results across all ProcGen games, and they have shown that the baselines do not improve when given a larger sample budget. Based on this, and assuming the authors will also improve the presentation as promised, I think this is now a strong submission and have raised my score to a 7 (Accept).**


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper addresses an important problem - zero-shot generalization in RL is relatively understudied compared to the singleton MDP setting, but important for many realistic settings
- As mentioned above, the insights are original, and the algorithm (also original) follows nicely from them

Weaknesses:
- As mentioned above, the experiments in their current form are not convincing enough
- There is potentially an important methodological issue which needs clearing up, namely ExpGen appears to use many more samples than the other algorithms it is compared to due to the use of several policies. 
- The presentation could be improved

Limitations:
Yes. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work studies generalization on unseen similar tasks, in a zero-shot manner, and discusses how invariance based approach to overfitting might not work all the time. The algorithm proposed, called ExpGen, has one part that explores the space, while a ensemble of agents are trained to do the reward optimization, and it claims to achieve sota results on ProcGen.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well written, and it introduces the problem statement and the challenges faced by the current methods well.
2. There is a good discussion about the related work as well, which helps place the proposed method in a proper context.
3. The set of experiments and tables are clear and the authors have given proper pointers to architecture choices and hyperparameters.
4. The limitations section is also addressed very well. The work talks about the game of Dodgeball where all the methods suffer and some possible insights on things that could be looked into for this.
5. The idea looks promising and can help in adding a useful contribution to the community.

Weaknesses:
1. Adding training progress plots can show how the evaluation scores evolved. 
2. Figure 4 is very difficult to read. It can also benefit from the more useful captions.
3. The proposed method does not do well in all the games. Other than the invariance, is there anything else that might be a reason for this? And is there a study on improving on these games?
4. For some baselines, the scores from the respective papers seem to have been used. It is possible the methods missed a proper hyperparameter tuning. Clarifying this in the work  and doing a proper hyperparameter search and tuning for all methods equally will be helpful.

Limitations:
The authors do discuss the limitations in their work.

Rating:
4

Confidence:
3

";1
VIaw1XHb4G;"REVIEW 
Summary:
The paper proposes the Interactive Multi-Fidelity Learning (IMFL) framework to develop small domain-specific Large Language Models (LLMs) under limited annotation budgets. Specifically, IMFL balances low-fidelity LLM annotations and high-fidelity human annotations to maximize model performance. Experiments on four domain-specific tasks demonstrate that IMFL outperforms single fidelity annotations, offering a cost-effective solution for domain-specific LLM development.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper addresses the practical challenges of deploying large language models in domain-specific tasks, such as their scale and high annotation costs.

2.  This work considers the practical setting of addressing the challenges of deploying LLMs, hence is somehow realistic. 

3. The IMFL framework offers an effective solution for the cost-effective development of small domain-specific LLMs by leveraging multi-fidelity learning.

4. Experiments on financial and medical tasks provide empirical evidence of the superiority of IMFL over single fidelity annotations.

Weaknesses:
1. While the paper demonstrates superior performance compared to single fidelity annotations, it would be valuable to provide a more comprehensive comparison with alternative approaches or baselines to highlight the specific advantages of IMFL.

2. It would be interesting if the author can provide more rationale for the selection of the dataset. 

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of how to fine-tune a relatively small, domain-specific language model under the constraints of limited annotation resources. The authors propose an Interactive Multi-Fidelity Learning (IMFL) approach. This approach aims to optimize the performance of the fine-tuned language model through multiple rounds of human-model collaborative annotation and fine-tuning. In each round, the method applies an exploration-exploitation query strategy (such as diversity sampling, uncertainty sampling, etc.) to balance high-fidelity human annotations and low-fidelity model annotations. Moreover, the IMFL method also introduces prompt retrieval and variable batch sizes to make better use of the annotated samples. The authors test the proposed IMFL method on four datasets from the financial and medical domains.



Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- In general, I think the experimental design of this paper is sound and the paper is easy to follow and understand.
- I think the problem studied in this paper is interesting and highly meaningful. The authors offer a straightforward and practical strategy for optimizing the performance of the fine-tuned language model under limited human and computational resources. Notably, the designs of the variable batch size and the coordination of human and model annotations in each round are innovative.
- The authors validate their method on four different datasets, consistently outperforming the baseline models in all cases. Particularly, I appreciate the extensive experiments and analyses performed in Section 4 to verify the effectiveness of each component of their framework, along with practical suggestions for future practitioners.

Weaknesses:
One major weakness of this paper is its strong assumption that we already have a good pool of unlabeled data, and all the methods are experimented on these pools. However, in many real-world settings, we do not have such an immediately available unlabeled data pool. The quality of the unlabeled data pool might be crucial to the effectiveness of the proposed method. Without relevant experiments, we don't know if the method proposed will still work in real-world settings.


Limitations:
Yes, the authors have explained the limitations of their study in the paper.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This article is devoted to a new algorithm for development of small domain-speciﬁc LMs under limited annotation budgets. one of the main ideas of the algorithm is to use as data annotators a combination of a Human and LLM (Large language models). The authors also proposed two innovative developments in their algorithm: 1) prompt retrieval to improve LLM annotation, 2) variable batch size. The paper also presents a new conﬁdence-based query strategy based on applying k-means algorithm to embeddings of the sub-sampled unannotated data and using least conﬁdence for selected items. Of the strengths, I can emphasize the following: a new query strategy for selecting elements is presented, which showed an increase in comparison with other approaches, which introduces scientific novelty to the article. With the help of the tables given in the scientific work, the qualitative increase is shown by the approaches described in the article. Of the weaknesses , I can note that it is not described in detail what happens at the stage “Execute prompt retrieval from Ah” which is described in Algorithm 1. I would like to get a detailed description for ""prompt retrieval"", for those who are not familiar with this term. There are no tables for hyperparameters with which the models were trained. I would like to see additional statistics on datasets: average, minimum and maximum sentence length.

==== After rebuttal  
Thank you for the reply, I have decided raise my score to 6.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The article presents a new algorithm for learning of small domain-speciﬁc LMs under limited annotation budgets the effectiveness of which is confirmed by comparison with various baselines. A new query strategy for selecting elements is presented, the effectiveness of the new strategy is confirmed by experiments. Two innovative designs are proposed to improve the learning process (prompt retrieval to improve LLM annotation, variable batch size), these approaches may be useful in other similar tasks.

Weaknesses:
* It is not clear which measure was used in clustering - cosine or euclidean distance, a comparison is needed. There is no information about which model is used to get embeddings in Exploration-Exploitation Query Strategy. Why was Sentence-BERT used to get embedding in Design 1? It seems that you need to use domain-speciﬁc models to get them in both cases. Since we use embedding of proposals, we would like to compare with strategies based on them, for example from the article “Deep Deterministic Uncertainty: A Simple Baseline”. In the article, the authors claim that they selecting cluster centers to reduce intra-iteration redundancy, but after all, at the next iteration, the selected elements in the new subset may be close to those already selected. Why do we reduce redundancy only inside the iteration?
* There are no tables for hyperparameters of models.

Limitations:
-

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces an algorithm to fine-tune LMs for domain specific tasks under a certain budget constraint. They use a mix of human and a high-fidelity LM for annotations. They fix the annotation budget for each and introduce an algorithm to sample from the unannotated pool and distribute it between human labelers and LM annotator. They perform extensive comparisons and ablation studies and show the effectiveness of their approach.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The results section clearly demonstrate the effectiveness of their method under the conditions they considered. 
- The paper is well written and easy to follow. Claims are clearly stated, supported by empirical data.
- The results are well explored and discussed. Good intuitions provided.

Weaknesses:
- The authors considered very limited tasks. They do not consider the impact of their approach on more generative tasks.
- Their approach and conclusions strongly depends on the LM they used for annotations. Although this is specified in the limitation of their work, more powerful LMs will make the effect of their approach less profound.

Limitations:
yes

Rating:
6

Confidence:
4

";1
40L3viVWQN;"REVIEW 
Summary:
This paper introduces a meta-learning algorithm which compresses a training set $D$ of size $|D|$ into a smaller training set $T\subset D$ of size $|T|$. The algorithm can be defined for any *base learner*, i.e. any training strategy which outputs a trained hypothesis $h$ when given a training set. The set $T$ is selected in such a way that the performance of the base algorithm trained on $T$ and evaluated on $D$ is better than a predetermined threshold. Relying on some powerful recent results from [1], the authors show that one can obtain tight generalization bounds for the result of the algorithm expressed as a function of the size  ratio $|T|/|D|$: intuitively, if the model is able to compress the dataset $D$ into a much smaller dataset $|T|$ whilst maintaining the performance of the base learner trained on $T$, it must be that the sample size is already sufficient to train the model well. Thus, the results can be interpreted as a middle way between on the one hand  the traditional approach to generalization bounds via direct study of the function class searched by a given algorithm, and on the other hand,  the even simpler *test-set bounds*[2]. The function of the ratio $|T|/|D|$ which gives a bound on the probability of error is a complex implicitly defined function inherited from [1] with a very mild dependence on the failure probability $\delta$. 

The algorithm functions as follows: start with an initial hypothesis $h_0$, then find the element of the training set $D$ which is the least well explained by $h$ (for instance, the one with the largest loss function value), and include this element in the set $T$. Retrain the hypothesis based on $T$ and repeat the procedure until either every element of the set $D\setminus T$ has a loss function value below a given predetermined threshold. 

The proof of the generalization bounds is a reasonably direct consequence of Theorem A.4., which is a powerful result from [1] concerning compression schemes. The key is to show that the compression function defined by the algorithm (the function which compresses $D$ into $T$ satisfies the properties of *preference* and *non associativity*, which are proved in Lemmas A.5. and A.6 respectively, and that the failure event which is bounded in Theorem A.4, which is the *probability of change of compression* is implied by the failure of prediction event (this is shown in Lemma A.7.). A technical condition of non concentrated mass which is required to be satisfied for Theorem A.4 to hold, is also removed by a straightforward argument augmenting each datapoint by a continuous random variable which doesn't affect the loss function/order. It is also shown (Proposition A.9) that any compression scheme which outputs a single element and is preferent must correspond to a maximum function with respect to some well defined notion of order. 

Experiments on the MNIST dataset and on a synthetic regression dataset demonstrate that the proposed algorithm yields superior generalization performance and tighter bounds than the Pac-Bayesian baseline, and proide slightly inferior bounds compared to a test-set approach. It is still worth noting nonetheless that the test performance is still superior to the test set approach due to a better indirect exploitation of the whole training set in the compression step. 


==============Post-rebuttal==========

As seen in the comments below, my discussion with the authors and the extra material promised (especially the experiments in the rebuttal concerning support vectors) have increased my opinion of the paper, resulting in me raising my score to 6. I believe this paper is above the threshold and tackles very interesting questions, with the main downside being a relatively small amount of material and original proofs (which should not necessarily disqualify a paper for publication). 

=====


**References** 

[1] Marco C. Campi, Simone Garatti. Compression, Generalization and Learning. ArXiv 2023.
[2] Langford, J. and Shapire, R. Tutorial on practical prediction theory for classification. JMLR 2017.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. This is a very interesting direction, not just in terms of providing generalization bounds (which is the main aspect presented in the paper) but also in terms of potential practical interpretation: this is a simple algorithm that, in principle, allows one to compress a dataset into a much smaller one, allowing us to derive interpretable information about which datapoints are key to the training procedure. 

2. This is a novel application of a mathematical result from [1] into a machine learning context, and has a distinctly novel flavor compared to many existing generalization bounds. 

3. The paper is relatively well written and a pleasure to read. The proofs are clean despite the fact that they must have been a bit annoying to write: It is quite trivial to convince oneself that the results (lemmas A.5, A.6, A.7 and Proposition A.9.) hold, but not so much fun to write down explicitly.

Weaknesses:
1. The results are essentially straightforward applications of existing results from [1], there isn't really any non trivial difficulty that had to be vanquished in the proofs. In short, this paper isn't a lot of work from the authors. 

2. The experimental evaluation is still relatively preliminary, there is so much more to explore: Other datasets and architectures, better baselines, and more importantly, the implications in terms of interpretability. For instance, the following follow-up experiments could be performed: 

2.1 Compare to more baselines on more datasets (Cifar, other Pac Bayesian bounds). In footnote 7 on page 7, the authors claim that ""Pac-Bayesian approaches have been developed only for linear regression problems"". This is a highly doubtful or (at best misleading) statement. In addition, generalization bounds based on function class capacity could also be evaluated in the synthetic regression dataset. 

2.2 (most important) Investigate the interpretability of the results: for instance, it would be interesting and rather key to evaluate the method on a synthetic binary classification dataset with a simple kernel method. It seems that in an ideal situation, the set $T$ should eventually correspond to, or at least strongly overlap, the set of support vectors. Even in the case of MNIST, it would be very interesting to visualize the chosen datapoints and see if they have something qualitatively different from the non-chosen ones. 

3 A very big issue is that the algorithm, as presented, is not that applicable in practice. Indeed, to make the algorithm work in the examples considered, the authors needed to pretrain the model on a significant proportion of the training set. In particular, the algorithm cannot select which datapoints in the set used for pretraining are more important than others. Since the algorithms used rely on gradient descent and the authors interpret the call to the base learner as a single gradient step, the initialization is key. It remains to be seen whether the algorithm in its pure form, with a base learner that performs exact empirical risk minimization on $T$, can work in any practical scenario. At the very minimum, it should be checked whether training the network on $T$ from random initialization (rather than pretraining it to obtain $h_0$ and then continuing training with $T$, which is what is done here) yields comparable performance.  

3.2 (related to 3) It seems like the extension to the case where several datapoints are introduced into $T$ simultaneously (perhaps in a hierarchical way) would not be too much to ask in a first submission, since it is key to solving the problem of the overreliance on initialization. 

4. I feel like the description of the previous results could be more extensive, for the benefit of the reader. For instance, the following things could improve the paper's reach and interest to a broader audience: 

4.1 Explain the gist of the proof of theorem A.4., at least in terms of intuitive reason why the function $\Psi$ from line 187 appears. 

4.2.  Write down the result (theorem 3.3) from [2] which is used to evaluate the test set bounds and discuss it. 

4.3 In line 472, the sentence ""See also Section 4 in [1]"" appears to suggest that the connection between the probability of change of compression and the classification/regression error is already established in [1]. If that is the case, how much of the present paper is truly not covered or implied by the discussion in [1]? 

5. I feel like a more detailed description of existing literature on training set compression schemes and how they relate to the present method is needed. It is hard to believe that no such literature exists. 




**Minor comments/typos**

The concept of ""probability of change of compression"" is frequently used in the main paper (cf., e.g., line 341, but it is only defined in the appendix (455-457)

Line 28: ""the precision of available bounds is much problem dependent"" ===>  ""the precision of available bounds is highlyproblem dependent"" 

line 64: ""is laying the groundwork"" ==> ""lays the groundwork""

line 110 ""is add to""  ===> ""is added to"" 

Lines 114 and 115: ""enough appropriate"" ==> ""appropriate enough"" 

Line 175:  ""an hypothesis"" ==> ""a hypothesis"" 

Line 280: ""fits well the data"" ==> ""fits the data well"" 

lines 480 and 560: I would use ""remove the condition"" instead of ""release the condition"" 









**References** 

[1] Marco C. Campi, Simone Garatti. Compression, Generalization and Learning. ArXiv 2023.
[2] Langford, J. and Shapire, R. Tutorial on practical prediction theory for classification. JMLR 2017.

Limitations:
Mostly the reliance on intialization, the limited evaluation of interpretability and the coverage only of the case where a single element is added to the set $T$ at each iteration. See  ""weaknesses"" for more details"".

Assuming there is really no comparable result in the literature (i.e. no generalization bounds expressed in terms of the success of a compression method), this is still a **very interesting paper opening up a new direction**. However, the amount of content included in this first contribution is surgically small.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work elaborates on the recent breakthroughs of Campi&Garatti 2023, by exploiting compression theory results to design a novel meta-algorithm, namely the Pick-To-Learn (P2L) algorithm. This algorithm aims to compress the dataset to a smaller, truly impacting one, this notion of impact being defined through a hypothesis dependent order $\leq_h$. Authors provably show high-probability generalisation bounds for P2L and experimentally shows that P2L with gradient descent as subroutine yields better theoretical results and experimental performances than both the test-set approach and PAC-Bayes learning.


In conclusion, I am convinced by the P2L algorithm, and it could be enough for acceptance at this point. However, I remain doubtful about the experimental process, see the Questions part. 

**References**
Wu et al. 2022 : Split-kl and PAC-Bayes-split-kl Inequalities for Ternary Random Variables

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
I found the theoretical part of this work utterly interesting as it offers a breath of fresh air in the generalisation field (at least up to my knowledge).

Weaknesses:
 I have several concerns about the experimental setup (in particular the comparison with PAC-Bayes theory). See the 'Questions' part.

Limitations:
None.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, the authors present a novel framework called P2L, which aims to derive generalization guarantees for black-box supervised learning algorithms. P2L operates as a meta-algorithm that utilizes a learning algorithm to induce a compression scheme. The algorithm relies on two main components: a *criterion of appropriateness* and an *appropriateness threshold*. The criterion of appropriateness serves as a generalization of the loss function, measuring how well a hypothesis describes a given data point. The appropriateness threshold determines when the meta-algorithm terminates by ensuring that the hypothesis is appropriate for all unselected data points. At each step, the algorithm selects the example with the highest loss, according to the criterion of appropriateness, from the set of selected training data ($T$). This example, denoted as $\bar{z}$, is added to $T$, and the learning algorithm produces a hypothesis ($h$) based on $T$. Using this new hypothesis, a new $\bar{z}$ is selected, and the process iterates until h becomes appropriate for all examples in the original dataset ($D_S$). An important property of P2L is that running the algorithm on $T$ yields the same hypothesis as running it on the full dataset $D_S$. Consequently, $T$ effectively compresses $D_S$, similar in spirit to dataset distillation or identifying core examples within a dataset. To provide generalization guarantees, a theorem demonstrates that the cardinality of $T$ can be utilized to derive tight upper bounds on a suitable measure of statistical risk. The effectiveness of P2L is evaluated through experiments on binary MNIST and synthetic regression datasets. The results indicate that P2L outperforms the PAC-Bayes bound and performs competitively with using a hold-out set, all without requiring additional data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- To the best of my knowledge, the proposed algorithm is novel and quite elegant (computational complexity notwithstanding). It is another realization of the principle “compression is intelligence”. 
- I find the technical tools used interesting and believe that this framework can potentially lead to many future works.
- The resulting bounds are tighter than PAC-Bayes bounds.
- The presentation of the algorithm is very clean and easy to follow. Most technical concepts are explained carefully and illustrated with examples. This is quite rare for theory papers.
- The algorithm exhibits some very interesting properties that are potentially desirable for learning algorithms. (*""the misclassification on the test-set for P2L is constant across all prior/train portions""*)


Weaknesses:
- In contrast to many existing generalization bounds, P2L can only be used for risk certification, that is, it does not offer an immediate way to make the bound tighter (which may improve the performance of the model) nor does it provide an understanding of why particular algorithms or architecture work well. Of course, this is partially due to the fact that P2L is designed to be as general as possible (i.e., black-box) but depending on the goal, the latter is sometimes more important than the risk certification itself.
- The algorithm seems not particularly efficient which limits its practicality, especially for deep learning with very large datasets. If the model is trained from scratch at every iteration, the complexity could be quadratic in the number of data points (i.e., $N^2$). On the other hand, it seems more suited for applications where the dataset is small. Perhaps it's better to phrase the paper in that direction. If this is not the case, please correct me. The authors do already address this point in the conclusion but note that PAC-Bayes generalization bounds for deep learning often use Gaussian posterior which is easy to sample from and empirically the estimate concentrates quite fast. Furthermore, these bounds can be made deterministic (Nagarajan et al., 2018).
- Related to the previous point, the paper would benefit from more empirical evaluation such as those in (Lotfi et al., 2022) who provided the SOTA PAC-Bayes bounds for several benchmarks. The field of PAC-Bayes bounds for deep learning has made significant progress in the past couple of years and only having binary MNIST results makes it hard to judge the empirical value of the proposed algorithm.
- The paper is generally well-written and clear but there are several important clarifications needed (see questions).

**Reference**

- PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization. Lotfi et al., 2022
- Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. Nagarajan et al., 2018

Limitations:
See the weakness section.

Rating:
7

Confidence:
3

";1
pTCZWSDltG;"REVIEW 
Summary:
The submission #8302, entitled ""CorresNeRF: Image Correspondence Priors for Neural Radiance Fields"" proposes a novel set of losses to improve the quality of NeRF under challenging conditions. In particular, the developed strategy effectively deals with the problem of sparse images. To achieve these performances, the authors take advantage of an out-of-the-box matching strategy between pairs of images to enforce geometric constraints during the training of the implicit representation. In particular, two types of losses are proposed to improve the quality of the reconstruction, namely the reprojection loss and the depth loss.
An extensive series of experiments demonstrates the relevance of these extra losses incorporated into the training. Another advantage of the proposed strategy is that it can easily be integrated into most implicit reconstruction techniques.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The paper is well-written and straightforward
- The approach is simple and can easily be integrated into most NeRF-based approaches leading to improved results
- The proposed technique is very effective under sparse view constraints
- The computational overhead is very limited as matching strategies are often very fast

Weaknesses:
- The approach is very simple; the losses in themselves are not really new but demonstrate very effective results. The contributions appear to be limited, but the quality of the results might justify an acceptance. For this reason, I would like to express a mixed opinion regarding the acceptance of this work. Note that a relatively similar loss (on 3D structure obtained via correspondences) is applied in ""Structure-Aware NeRF without Posed Camera via Epipolar Constraint"" but with less success than in this manuscript #8302.
- CorressNeRF demonstrates good performance when few images are used, but it would be interesting to know the effect of these losses in more common scenarios.

Limitations:
I have very little to say about this paper as it is very clear and straightforward. I would like to kindly recommend additional experiments, as explained in the previous section. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents NeRF regularization method for few view NeRF.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Using the state-of-the-art image matcher to regularize NeRF training is novel.
2. The paper is well-written and clear.

Weaknesses:
1. The paper proposes employing the cutting-edge image matcher to enhance NeRF training. However, a similar idea was presented in Neuris [1], which also suggested using patch matching to optimize NeRF training. Intuitively, one could assume that a state-of-the-art image matcher would identify more precise correspondences than patch match, leading to superior results. However, considering that Neuris integrates additional monocular depths and surface normals, the effectiveness of combining these three methods remains uncertain. Therefore, the author appears to have overlooked an essential baseline, Neuris. It is recommended that the author carry out experimental work based on the Neuris setting rather than implementing Neuris in their own setup, which make the conclusion more convincing.

2. The pixel loss and depth loss appear to aim towards the same goal. The concept of using both has been previously explored in DSAC [2], but was later discarded in DSAC++ [3], deemed as unnecessary. While the author provides an ablation study to illustrate the effectiveness of the reprojection loss, its value remains questionable. This is primarily because, in multiview settings, the reprojection loss mirrors the depth loss, making its unique contribution uncertain.

3. Minor, The citation of UNISURF in Figure 5 seems to be wrong. 

[1] NeuRIS: Neural Reconstruction of Indoor Scenes Using Normal Priors
[2] DSAC-Differentiable RANSAC for Camera Localization
[3] Visual Camera Re-Localization from RGB and RGB-D Images Using DSAC

Limitations:
The image matcher sometimes fails when there are not enough overlap regions.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposed CorresNeRF, a method that leverages image correspondence priors to improve NeRF training on sparse input views. The correspodence matching is computed by off-the-shelf methods. The authors augue that the introduced inexpensive image correspodence priors can be used to supervise training of arbitrary NeRFs and lead to better performance / faster convergence taking sparse view inputs. Further, a robust correspondence loss is designed, including reprojection loss and depth loss baesd on correspondence priors. Overall, the method demonstrates superior reconstruction quality against baselines like VolSDF and NeuS, etc.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper's main contributions are summarized as follows:

- Introduction of image correspondence as a cost-effective prior to supervise the training of any NeRFs.
- Design of a pipeline to obtain robust correspondences from standard methods, including automatic augmentation and filtering.
- Introduce a robust correspondence loss incorporating reprojection loss and depth loss based on correspondence priors.
- Extensive experiments conducted on various baselines and datasets demonstrating the method's effectiveness across different types of neural implicit representations.

The authors conduct extensive experiments on various datasets, which demonstrate the effectiveness of their method. They find significant improvements in both novel view synthesis and surface reconstruction metrics. The proposed method outperforms other state-of-the-art sparse-view reconstruction methods and works well with various types of NeRF, including those with other priors.

Weaknesses:
There are some limitations that should be properly discussed:

- Dependence on the quality of image correspodence. The quality of the obtained image correspodence matching significantly impacts the effectiveness of the proposed approach. Less accurate correspondences can negatively affect the supervised training of NeRF, which might lead to suboptimal reconstruction results.
- Performance in non-sparse scenarios. The method is focused on the advantage of using CorresNeRF in sparse-view configurations, but it doesn't mention how this method would perform in dense-view configurations. It would be interesting to show such an ablation study to verify this.
- The main comparisons show VolSDF and NeuS results as baselines. However, a more adequate baseline would be SparseNeuS: 
SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views. ECCV 2022. 
- The method shows 3 input views on DTU/LLFF datasets. What happen if arbitrary number of input views are given? This is soemwhat related to the 2nd point above. But it would be nice to have such experiments to better evaluate the robustness of the proposed pipeline. 


Limitations:
Limitations are discussed in the above weakness section. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper introduces CorresNeRF, a method that leverages image correspondence priors to improve the performance of Neural Radiance Fields (NeRF) in scenarios with sparse input views. The authors propose a plug-and-play module that incorporates correspondence priors into the training process by adding loss terms on the reprojection error and depth error of the correspondence points. They develop an adaptive algorithm for augmenting and filtering the correspondence priors to enhance their quality. The proposed method is evaluated on novel view synthesis and surface reconstruction tasks using density-based and SDF-based neural implicit representations across different datasets.

The proposed CorresNeRF utilizes image correspondence priors to supervise the training of NeRF models. This approach addresses the challenge of sparse input views and enhances the performance of NeRF in reconstructing 3D geometries.
The authors propose an automatic augmentation and outlier removal process for improving the quality and robustness of the correspondence priors. This process enhances the dense correspondence estimation and mitigates the effects of inaccurate correspondences.
The paper formulates a correspondence loss that incorporates reprojection and depth errors based on the correspondence priors. This loss effectively guides the learning of implicit functions in NeRF models and improves their performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper demonstrates several strengths across different dimensions:

The paper introduces the concept of leveraging image correspondence priors to improve the performance of NeRF models in sparse-view scenarios. This novel approach addresses the challenge of reconstructing 3D geometries with limited input views and introduces the use of image correspondences as explicit supervision for learning implicit functions in NeRF. The combination of image correspondence priors and NeRF training is a creative and innovative approach that expands the capabilities of NeRF models.

The paper addresses a significant problem in the field of 3D reconstruction and view synthesis. Sparse-view scenarios are common in real-world applications, and improving the performance of NeRF models under such conditions has practical implications. The proposed method offers a practical and effective solution by leveraging image correspondence priors, which are readily obtainable and can be computed using standard methods. The experimental results demonstrate the superiority of the proposed approach over previous methods, highlighting its potential for advancing the state-of-the-art in novel view synthesis and surface reconstruction tasks.

The paper presents a well-designed methodology with clear objectives and a systematic evaluation process. The authors carefully consider the limitations of existing methods and propose solutions to overcome them. The proposed CorresNeRF method incorporates robust correspondence loss and automatic augmentation and filtering of correspondence priors, enhancing the quality and effectiveness of the training process. The experimental evaluation is thorough, encompassing various neural implicit representations and datasets, and the results demonstrate significant improvements in performance metrics.

The proposed method is described in a structured manner, with detailed explanations of the augmentation and filtering process, formulation of correspondence loss, and evaluation metrics. The figures and equations further enhance the clarity of the paper, aiding in the understanding of the concepts and techniques presented.


Weaknesses:
While the paper demonstrates several strengths, there are also a few areas where it could be improved:

Experimental Evaluation: The paper would benefit from a more detailed analysis of the computational efficiency and resource requirements of the CorresNeRF method. Providing insights into the computational demands and resource utilization of the approach would help readers understand the practical implications and scalability of the method.

Image Correspondences: While the paper introduces image correspondences as priors, it is important to acknowledge the potential challenges in estimating accurate and robust image correspondences, especially in scenarios with occluded or noisy images. Conducting a sensitivity analysis of correspondence accuracy would provide a clearer understanding of the method's performance under different conditions and shed light on its robustness and generalization capabilities.

Comparison with State-of-the-Art: The paper would benefit from a more comprehensive comparison with existing state-of-the-art methods for sparse-view reconstruction, such as MVSNeRF and GeoNeRF. Providing a thorough evaluation and comparison against these methods would help establish the superiority and novelty of the proposed CorresNeRF method.

Limitations:
The limitation is discussed.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper proposes an approach for sparse-view NeRF reconstruction by using image correspondences as a prior. NeRF under the sparse-view regime is overparameterized and under constrained hence requiring a prior to optimize. This paper proposes to use image correspondences that are extracted across the different views, in particular, they use DKMv3. They propose two additional loss functions based on the correspondences from the prior: the first uses the reprojection error by using the expected depth as predicted by the NeRF while the second uses a correspondence-depth based loss by finding the closest 3D points in space based on the correspondence from the prior. Experiments on novel view synthesis and surface reconstruction show the improvement of their proposed approach.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes to use image correspondences as a prior for sparse view nerf reconstruction which is intuitive and sound. Image correspondences as a prior is generalizable and hence a pretrained model can be used, namely DKMv3. They propose two simple, yet intuitive losses for their approach. Experiments show that the proposed method is able to perform better compared to existing baselines.

Weaknesses:
The effectiveness of the method is relying on the accurate prediction of the correspondences, and it is known that correspondences can be erroneous on texture less regions, illumination changes or wide-baseline cameras. On real scenes, these issues might arise more -- e.g. on sparse scannet images as used by existing benchmarks [53, 54]. The sparse view inputs here have wide-camera baselines as opposed to forward facing scenes in LLFF. It would be more convincing if the method can also perform reasonably in such settings.


Some references on sparse view NeRF:

**[53]** Dense Depth Priors for Neural Radiance Fields from Sparse Input Views, CVPR '22
**[54]** SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates, CVPR '23

Limitations:
The authors have included limitations in the main paper of the submission.

Rating:
5

Confidence:
4

";1
zW1uVN6Mbv;"REVIEW 
Summary:
The authors study the setting of unsupervised learning where observations belong to several domains, and we only observe the marginal distribution of each domain. A set of latent variables generates the observations, where a subset of latents are shared across domains. The authors provide the first identification results in this setting, assuming that the latents follow a linear SCM, and the observations are an injective linear transformation of the latents. With this model, identifying the (unobserved) joint distribution of the observations equates to identifying the latents. The mapping between the exogenous noises and the observations, as well as the distributions over the exogenous noises, are identified up to signed block permutation. With additional conditions, the authors also identify the causal graph for the latents up to a signed permutation consistent with the topological ordering of the latents. The authors validate their claims with a synthetic data experiment.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This is a strong paper that provides the first identifiability results on multiple-domain unsupervised learning where the joint distribution of the observed variables is unobserved. These results are impactful, since this problem setup is well-studied and has practical applications in single-cell biology. Existing approaches are significantly limited by their lack of identifiability, so this paper makes a valuable contribution.

The writing style is rigorous, and definitions, assumptions, and results are explained precisely.

Weaknesses:
This paper could be improved with more context on how they are extending existing identification results to achieve theirs. Currently, the authors mention which existing results are being used, but do not provide an intuitive description on why they need to be extended, and how they do so.

The notation could be improved. Capital letters are used to denote matrices, probability measures, vector- and scalar-valued random variables, sets of nodes, and sets of edges. It would improve readability if you used font styles (e.g. lower-case bold for vectors) to differentiate them.

Limitations:
The authors made it explicit that this work is primarily about identifiability, and less about scalable algorithms and evaluation on realistic datasets.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors address unpaired multi-domain causal representation. In detail, the authors learn the representations of the observed data from different domains that consist of causally. To achieve this, the authors consider the data generation process where the relationship between the latent variables is linear. Based on this generation process, they prove that the joint distribution of observed data and the shared causal structures of latent variables are identifiable.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors investigate the causal discovery with latent variables from different domains.

Weaknesses:
1.	There are several works about causal discovery with latent variables under linear and multi-domain case like [1], which also considers the shared causal structure among latent variables and provide identification guarantees. It is suggested that the authors should discuss these works.  
2.	Moreover, the authors discuss several works about domain translation between unpaired data and claim that none of these works have rigorous identifiability guarantees. However, Multi-domain image generation, image translation, and domain adaptation belong to the proposed setting, and [2][3] have addressed the multi-domain causal representation learning problem recently. And the authors do not consider these works. It is noted that [2][3] considers the multi-domain causal representation learning with nonlinear transformation, which seems to be more general than the proposed setting.  
3.	As for the identification of joint distributions, it is not clear why the identification of $l, B$, and $P$ can identify the joint distributions of observed data from multi-domains.   
4.	In section 3, the authors assume that the distribution of errors is non-Gaussian for the identification of linear ICA by not allowing asymmetric distribution. But some distribution like the Laplace distribution is symmetric and they can also satisfy the identification of linear ICA.
5.	According to this paper, the authors consider the structure of latent variables to be linear but flexible. In the simulation experiment, the authors only consider three shared latent variables, it is suggested that the authors should consider more latent variables and different structures.  
6.	Besides, it is suggested that the authors should consider more compared methods and employ other metrics like recall, and precisions to evaluate the performance of causal discovery.  

[1] Causal Discovery with Multi-Domain LiNGAM for Latent Factors  
[2] Multi-domain image generation and translation with identifiability guarantees  
[3] partial disentanglement for domain adaptation

Limitations:
Please refer weaknesses

Rating:
6

Confidence:
3

REVIEW 
Summary:
- The paper considers causal representation learning from unpaired multi-domain data, with latent variables both shared and specific to domains.
- Its key contribution is a new identifiability result for linear causal models with non-Gaussian noise, linear mixing function, and a number of other assumptions.
- In addition, the authors develop a practical representation learning algorithm for this setting and demonstrate it on toy data.

I've read the authors' rebuttal. They have addressed my concerns adequately.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Causal representation learning is an interesting, relevant, and mostly unsolved problem.
- The setting considered here (observational unpaired multi-domain data) is practical and well-motivated from single-cell biology applications.
- The identifiability result is, to the best of my knowledge, novel, and substantially different from existing results.
- As far as I can tell, it is also correct, though due to review overload I have not been able to check the proofs properly.
- The paper is extraordinarily well-written. The authors manage to be precise, yet still provide intuitive explanations.

Weaknesses:
- The contribution made here has only one real weakness, and that is the host of strong assumptions underlying the identifiability result: 1D causal variables, linear causal model, no causal effects from shared to domain-specific latents, non-symmetric error distributions, different error distributions for each variable, linear mixing function, full-rank mixing function, observed variables include sufficient ""partially pure children"", and the list goes on. To put it bluntly, this list makes me wonder if this identifiability result present progress on the road to algorithms that work in practice on interesting real-world datasets.
    - Of course, strong statements such as CRL identifiability require strong inputs, but these need not be in the form of model assumptions, they could also come from the data side. Perhaps it is a bit out of scope for this paper, but I would be curious if the availability of *interventional* data or some other form of auxiliary data would allow the relaxation of some of these model assumptions.
    - While the authors do a good job in providing an intuition for why these assumptions are needed, I would like to know if there are any real-world problems that satisfy them all. This is partially discussed for single-cell data and a few of these assumptions, but could the authors provide a more complete example that ideally satisfies all assumptions?
- There are no experiments to speak of, though I also don't think that all papers need experiments.

Limitations:
- The paper is very clear about the (many) assumptions in the theory. It also openly acknowledges the limitations of the experiments.
- I do not see any particular need for an extensive discussion of societal impacts.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work tackles the problem of learning the latent causal structure from multiple unpaired domains. Under a linear non-Gaussian condition, this work presents the identifiability guarantees for the joint distribution over the domains and the causal structure within the shared latent partition. Synthetic data experiments are presented to validate the theory.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The problem is well-motivated and timely. Unpaired data are prevalent in the wild, and this work provides a rigorous treatment as the initial step to leverage such data in a principled manner.
2. The paper is nicely written, and the theoretical analysis is clearly articulated with sufficient explanations.
3. The theoretical techniques are clearly explained. Connections and attributions to prior work are appropriately introduced, which aids the assessment of this paper’s technical contribution.

Weaknesses:
1. The linear assumptions: practical multi-domain (modal) data-generating processes are often highly nonlinear, e.g., images and text. The applicability of the linear assumption may not be as appealing.
2. The heterogeneous noise distributions: pairwise distinct exogenous distributions appear a strong assumption to me and can potentially oversimplify the technical challenge. I would be interested in learning about the necessity of such an assumption.

Limitations:
Please see the weakness section.

Rating:
6

Confidence:
3

";1
00EKYYu3fD;"REVIEW 
Summary:
This work investigates what constitutes a good latent space for generative models, and proposes a new training paradigm for generative models – DAE. Simply put, with DAE generative models are trained as Autoencoder in two stages. First, a relatively weak decoder is employed, whose purpose is to aid the encoder in learning meaningful representations. Second, the weak decoder is replaced with the “actual” decoder and training is continued. The second decoder is the one eventually evaluated as a generative model.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. The paper is very well written, easy and interesting to follow.
2. The questions of what constitutes a good latent space and how is one constructed are often overlooked. Most works follow longstanding paradigms of using predetermined distributions (e.g., Gaussian) or whatever is learned from an autoencoder. Posing these questions and formulating the setting, on its own, is impactful.
3. The proposed method DAE is empirically proven effective, and since it is rather simple to implement, I conjecture it might have a strong impact on future generative modeling works.


Weaknesses:
While the mathematical formulation in the paper looks sound to me, I think it does not benefit the paper, and significant portions of it could be moved into the appendix. As the authors acknowledge in the Discussion section, the formulation“ serves mainly illustrative purposes” and is “not proven mathematically”. I don’t see it as an issue, as much of Generative Modeling research (and ML in general) is empiric in nature. The proposed method, DAE, could be introduced as an empirically-supported design, while some of the mathematical formulation could be described to serve as intuition (e.g., Theorem 4.1) . However, dedicating over three pages to it seems excessive to me. In my opinion, most readers would greatly benefit if the experiments in Appendix B were present in the main paper instead of some of the mathematical formulation. 


Limitations:
Authors addressed limitations. One of them (non comprehensive evaluation) raises questions. I would appreciate a clarification on that.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents an approach with theoretical analysis to explore a more suitable latent distribution for generation. For this purpose, this paper proposes a novel distance between the latent and target distributions and tries to minimize it to obtain the optimal data-dependent latent distribution. In practice, a two-stage training strategy called Decoupled Autoencoder (DAE) is introduced to leverage a superior latent distribution to improve the generative performance. The experiments on VQGAN and DiT show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.The paper is well-structured and easy to follow.

2.The methodology is supported by theoretical analysis.

Weaknesses:
1.The proposed methodology is somehow similar to AE-OT-GAN [1], which also learns a latent distribution via an autoencoder and then utilizes the learned latent to train GAN. The authors should explicitly highlight the advantages of their approach over [1].

[1] AE-OT-GAN: Training GANs from Data Specific Latent Distribution. ECCV 2020. 

2.Some important related works are missed, particularly the line of works that explore improved latent sampling in GANs, e.g., [1] [2] [3] [4]. In special, AdvLatGAN [2] seeks to adjust and discover a more suitable latent distribution via adversarial learning in both the post-training sampling and the training, and it has already introduced the concept of optimal latent distribution noted $p_z^{op}$ in its theory.

[2] Improving Generative Adversarial Networks via Adversarial Learning in Latent Space. NeurIPS 2022.

[3] Discriminator optimal transport. NeurIPS 2019.

[4] Your gan is secretly an energy-based model and you should use discriminator-driven latent sampling. NeurIPS 2020.

3.The experimental results are insufficient. The comparison basically focuses on the performance gain beyond the vanilla backbones. Authors should compare the other efforts on improving the latent sampling/distribution, e.g., the previously mentioned works.

Limitations:
Please see above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper first proposes a new framework to analyze latent spaces in the context of generative models.
This framework takes inspiration from prior results about GANs, which allowed interpreting the min-max training objective as computing a distance between distributions to be minimized, to define a similar distance between the latent distribution and the data distribution.
From their analysis, they derive a simple two-step training for auto-encoders to learn better latents and reconstruction, in which they first train the encoder with a weak decoder to extract good latents, and then train a larger decoder to get better reconstructions.

They perform experiments in a simple toy case, and then with commonly used models such as GANs, VQGAN, and DiTs.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper is easy to follow
- It proposes a novel view on latent codes, including an explanation of some different properties between SSL and generative latent codes, and a theoretical framework to describe why a powerful encoder/decoder pair can't learn a good latent code.
- The proposed practical solution is simple and their experiments show that it can improve performances in a variety of generative settings.

Weaknesses:
- While the fresh view on latent codes is interesting, it doesn't provide any theoretical guarantees.
As far as I understand, the main conclusion is that in order to obtain a good latent code the encoder and decoder should not be too powerful but is not able to give an indication about how the correct balance. Note that in the context of Variational Autoencoders, a similar conclusion had been reached before: that a too powerful decoder is a good explanation for the phenomenon called *posterior collapse* that describes a state in which the latent code is completely uninformative [1].
It is good that it is formally extended to other auto-encoders, but unsurprising.

- Because of this lack of quantification of optimal complexity, it is quite unclear whether the positive results obtained in VQGAN and DiT settings are actually related to the theoretical conclusions or not. It could very well be completely unrelated and just confirmation bias.

- Related work present different generative models with loose links to the proposed method, but does not discuss any study about the latent spaces of generative models. In addition to links with posterior collapse in VAEs [1], I would also be very interested to read what the authors have to say about sparsity and disentanglement properties of VAE [2], and PCA directions in GAN space [3] among other things.
Moreover, I have to point out that contrary to the paragraph in related works, Mask Autoencoders have been explored for their generative capabilities [4].


--- 
[1] Fixing a Broken ELBO. ICML 2018.
Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A. Saurous, Kevin Murphy.

[2] Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations. ICML 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, Olivier Bachem

[3] GANSpace: Discovering Interpretable GAN Controls. NeurIPS 2020.
Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris

[4] MaskGIT: Masked Generative Image Transformer. CVPR 2022.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman

Limitations:

The authors are very open regarding the limitations of their submission.

Potential negative societal impacts of generative models are not. Widely acknowledge ones include amplification of biases, potential misuse for propagating fake information, and concerns about lack of attribution and copyright infringements.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes an asymmetric training scheme for auto-encoders that double as image generator. Based on analytical insights that the decoder should have less capacity than the encoder for the encoder to capture correctly the data distribution, they propose a first training cycle where a strong encoder and a weak decoder are trained jointly. This produces a latent distribution able to better capture the data distribution. Then, in a second stage crucial for the end application, the encoder is frozen and a strong decoder is trained using the ""good"" latent distribution. Experiments are carried out on faces datasets with VQGAN and class conditional imagenet with a diffusion model and show the proposed training scheme is promising.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The strengths of this paper are :
- It is a simple method, yet effective. As such, it should be easy to reproduce. The improvements are maybe questionable (no error bars, test against the training set - as is traditional in this domain), but given that they are reported with widely different architecture and on different dataset, there is more confidence than usual that it actually works.
- The analysis part is really nice. The linear example in particular sheds some light as to why under a constrained budget, the complexity of the encoder should exceed that of the decoder to properly minimize the projected distance (divergence) to the data distribution.

Weaknesses:
The main weakness is that most of the paper is about the analytical part, which is a bit handwavy (acknowledged in the conclusion), and lacks a good structure. The reading could be improved by having a clear outline of the work such that one does not wonder where the text is headed to after each section.

Limitations:
The conclusion is a list of limitations that is very honest.

Rating:
6

Confidence:
4

";1
TqW5PL1Poi;"REVIEW 
Summary:
This paper aims at improving reasoning with large language models (LLMs) by prompting them in a way that they parse the problem into a language that is understandable by a SAT solver, and then employ an off-the-shelf SAT solver to solve the problem. Empirical results on multiple datasets show the benefit of the proposed approach.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The idea of obviating the need for planning by employing a SAT solver is neat.
* Empirical results are strong.
* The approach works on various tasks/datasets.
* The analysis in Table 5 is quite interesting.


Weaknesses:
* I believe the benchmarks used in this work may slightly overestimate the performance of the proposed model and the approach may not work as well on more realistic use cases (see the limitations section for more detail)
* It is not clear to me how some bodies of work can fit within the parse-plan-execute framework described on Page 4. Could you comment on how the approaches such as [1, 2] that use LLMs as a tool within a reasoning algorithm can be described in this framework? How about decomposition-based approaches such as [3, 4]?
* [minor] The descriptions in Section 3 and, to some extent, Section 2 could be significantly shortened and some experimental details could be moved to the supplementary (e.g., decoding strategy, temperature, etc.). This makes room for a more elaborate description of different categories of related work (as opposed to putting everything into one category) and for moving some example failures from the appendix to the main text.

[1] LAMBADA: Backward Chaining for Automated Reasoning in Natural Language

[2] Selection-inference: Exploiting large language models for interpretable logical reasoning

[3] Decomposed prompting: A modular approach for solving complex tasks

[4] Least-to-most prompting enables complex reasoning in large language mod

Limitations:
Many of the current LLM reasoning datasets (including some of the benchmarks in this work) have been created by first generating a puzzle in a formal language and then turning it into text either using templates or using human annotators. Therefore, the performance of approaches that translate back to a formal language (including the current work) may be overestimated on these benchmarks. Could the authors comment on the applicability of their approach to more realistic applications in the following two cases as examples:


1- Consider a logical reasoning puzzle described below:

Fiona assassinated the mayor. If somebody killed the mayor, they must go to prison. Should Fiona go to prison?

My guess is that if you translate to a formal language, you will get something like this:

assassinated(Fiona, mayor)

killed(X, mayor) -> go(X, prison)

go(Fiona, prison)?

which makes the solver not be able to produce the correct answer.


2- Consider the mathematical puzzle below:

Fiona has 10 apples, 15 dragon fruits, and 11 coconuts. How many tropical fruits does Fiona have?

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper propose SATLM, which aims to solve the problem of planning error when using CoT and ProgramLM. Specifically, SATLM use a LLM to translate natural language problems into formal the language that is accepted by a solver and let the solver to do planning as well as calculation. The results on several reasoning datasets show the effectiveness of this method.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper propose to translate a problem into a formal form and then directly call a SAT solver, which is a good idea. The experiment results in Table 1 shows the effectiveness of the method.
2. The structure of the paper is clear and the paper is easy to follow. Figure 1 is informative enough to decribe the proposed method.
3. I like the ablation studies in this paper, which makes it clear which parts are important to the performance gain.

Weaknesses:
1. My main concern is the method may only perform well on simpler tasks such as GSM (I think it's GSM8K?) or Proofwriter. The question in  Proofwriter is nearly in the format of formal reasoning, which only requires line-by-line translation, which in turn makes the accuracy to be as high as 99.7%. On more complicated tasks such as MathQA and MATH, I think (1) the problems might not be able to expressed in first-order logic and (2) even they can, it's not easy to do the translation.
2. There are other works that combine symbolic solver with LLMs in a similar or different way. The authors have mentioned some in the related work part. However, none of them is empirically compared in this paper, and only listing the difference from them is not enough.

Limitations:
The paper has discussion of limitations in section 6, which I think is enough.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a framework to augment LLMs with symbolic solvers to compensate known flaws in LLMs' reasoning (e.g., planning and arithmetic). The main novelty of this paper is that LLMs is prompted to generate declarative specifications (rather programs) so that off-the-shelf automatic theorem provers like Z3 can be deployed to solve the symbolic problems.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Augmenting LLMs with symbolic solvers is a super valuable direction to explore. Upgrading the symbolic solver from a programming language interpreter to an automatic theorem prover surely will benefit LLMs further. This paper is well motivated, nicely structured and has compelling experiments.

Weaknesses:
The main weakness to me is that SatLM does not differentiate itself enough from the previous [Faithful CoT paper](https://github.com/veronica320/Faithful-COT) as a framework. Besides parsing sub-problems into a programming language like Python, Faithful CoT can also put those problems into Planning Domain Definition Language (PDDL), which is, in a sense, specification.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work looks at using an LLM to generate a declarative task specification from a natural language specification for reasoning tasks and leverage an automated SAT solver to solve the problem. They showcase that SATLM performs better than using Chain of thought or ProgramLM (which converts natural language into python programs) in various reasoning tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written, the experiments are thorough.

Weaknesses:
The main contribution of this work seems to be about using LLMs as a semantic parser (for mostly short-context problems) which essentially maps a problem from NL specification to a formal specification (SAT problem specification here). This is a well-known problem in NLP for which LLMs (particularly LLMs which are trained on text and code) are known to perform comparably to the supervised neural semantic parsers [1,2]. The results of this work essentially indicate whether LLMs can perform NL-to-SAT semantic parsing given that once the parsing is done correctly, the SAT solver solves it. This makes the comparison between SATLM and ProgLM/CoT a bit unfair as the core ability that has to be compared is the parsing ability and not the planning/execution ability and these are intertwined in the latter. Comparisons between SATLM and other semantic parsers (other LLMs as done in Section 4.5 and other neural task-specific semantic parsers) for NL-to-SAT semantic parsing has to be highlighted in my opinion. 

[1] Shin, R., & Van Durme, B. (2022, July). Few-Shot Semantic Parsing with Language Models Trained on Code. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 5417-5425).
[2] Zhuo, T. Y., Li, Z., Huang, Y., Li, Y. F., Wang, W., Haffari, G., & Shiri, F. (2023). On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. arXiv preprint arXiv:2301.12868.

Limitations:
I have provided the limitations of the work in the weaknesses section. The authors do address important limitations such as problems being less suitable to declarative prompting, limitations of SAT solvers when dealing with complex formulas and potential future work on re-prompting the LLM with the SAT solver feedback.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper describes a new approach to solving NL reasoning tasks using large language models (LLMs). Specifically, the key idea is to combine LLMs with SAT solving, where LLMs are only used in the first parsing step. The authors of the paper call this approach satisfiability-aided language modeling (SATLM). The authors also point out that by SAT, they mean all kinds of tools for automated reasoning, including traditional SAT solvers, SMT, and first-order theorem provers. The idea is that the declarative specification that is generated as output from prompting is closer and more accurate compared to more imperative problem formulation. The work is evaluated on several data sets with standard prompting, chain-of-thought (COT) prompting, and executor-augmented LLMs (PROGLM).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:

- The paper is relatively easy to read and strikes a good balance between formality and accessibility.

- The main idea of the paper is clear, and it seems to be a new direction of how to encode NL tasks using LLMs.

- The paper contains relevant related work and contains relevant benchmarks





Weaknesses:
- There is an interesting discussion on how the proposed approach can catch errors better than PROGLM (see e.g., the paragraph starting on line 166). This is good, but there is no discussion on how this kind of error can be handled. How easy is it to debug such errors? How can a user generate a fix or solution if such an error occurs? 

- The results are not always that easy to interpret. For instance, in table 2, if an answer is incorrect, does it then include both incorrect because of parse error, or just incorrect because of the SAT solver generating the wrong answer? This is much clearer in table 5.

- The level of detail of how the approach has actually been implemented could be improved. More specifically, how are the different tools chained together, are all steps automated etc.


Limitations:
There is a clear paragraph on the limitations of the approach at the end of the paper. 


Rating:
5

Confidence:
3

";1
JpU5YmMKx7;"REVIEW 
Summary:
This article expands the concept of transfer entropy to include attentive transfer entropy. Implementing attentive transfer entropy on connectomes of realistic biological networks demonstrated that this extension encompasses the transient coupling effect within the connectivity constraints of realistic biological networks. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper provides convincing empirical evidence that the attentive transfer entropy is particularly useful in understanding coupling effects of realistic biological networks. They achieve this through first conducting ablation study on toy models of simple connectivity (full connectivity?) and comparing the proposed attentive transfer entropy with multiple commonly used baselines. 



Weaknesses:
It is a bit confusing that the classifier with conventional attentional mechanism works almost as well as the ATEn in many scenarios (e.g., the Izh dynamics in C.elegans) and overall in Mouse connectome. In addition, it seems which dynamics being used to simulate the sequences may also influence the performance, but there was no discussion on that. 


Limitations:
This study employed predetermined dynamics, which is hardly realistic for functioning networks in the real world. The sensitivity to the choice of dynamics may hinder this method's potential utility for analyzing experimental results. 


Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposes ""attentive transfer entropy"" as a way to infer connectivity of real and synthetic coupling networks. The submission suggests extending a 'neural estimator of mutual information' to transfer entropy and adding ""attention"" to focus on moments in time where the influence is larger.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* higher performance than other methods this estimator is compared to.
* fewer data necessary than other methods this estimator is compared to.


Weaknesses:
* The introduction of attention appears to be ad hoc, a mathematical/information theoretic justification of the attention mechanisms seems to be missing. 
* The methods seems a direct extension of the Mutual information neural estimator, the only technical innovation seems to be the attention mechanism which seems to lacks theoretic grounding.
* The whole theory (including the analytical toy example) considers discrete time dynamics but the methods is then applied to inference of couplings in systems of coupled differential equations. Something seems to be missing here to link the two descriptions (continous time and discrete time).

Minor issues:
* Table 1 is a bit confusing. The second column is ""brain, brain cortex, neural, brain"". This seems rather unspecific. What is the actual brain region in the cat an macaque data? Why don't you use a full connectome, e.g. of a fly (https://www.science.org/doi/10.1126/science.add9330)
* Weird notation in first equation. Why is t and index, usually ODEs are written as d x(t) /dt =f(x(t)) and not d x_t /dt=f(x_t)


Limitations:
Limitations (mentioned above) are already addressed in the paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors extend the MiNE method [23], of using a neural network to estimate mutual information, to estimating transfer entropy between two coupled time series. When the coupling is weak on average due to being intermittent, they propose a differentiable attention mechanism that learns to transiently gate the transfer entropy calculation to only these intermittent moments. They test their method to identify coupling in networks created using simple neuron models connected as per connectivity datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-	The authors combine several new techniques to address an important issue of connectivity determination from neural recordings / time series.

Weaknesses:
-	The authors straightforwardly extend the theory of the neural estimation of mutual information to their neural estimation of transfer entropy without attention in Appendix A.. However, they do not say anything about their attention based method. Can it be shown that it is also somehow consistent / convergent?
-	It appears to me that all the ‘connectomes’ apart form the C.Elegans one are between macro brain regions. Yet these have been used to connect individual neuron models. Can the authors not use local connectivity of cortical neurons from say the Blue Brain project and deduce this connectivity?
-	Ultimately the method is only tested on simulated datasets even though the connectivity is based on macro-connectivity of animal brains. Numerous Ca-imaging or spike-train recordings of hundreds of neurons in various small regions of the brain are available. Yet the authors do not seem to have applied their method to these. Thus it calls into question whether their method is suitable for real-world data.


Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel method to reconstruct coupled networks from observational time series data, where the coupling effects between variables are sparse and weak. The authors propose a new attention mechanism called coupling attention is introduced to identify critical regions in time series where coupling effects momentarily emerge amidst dominant intrinsic dynamics. The authors design a differentiable objective function called Attentive Transfer Entropy (ATEn) to train the coupling attention model in an unsupervised, data-driven manner.



Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
ATEn is well motivated and the use of Mutual Information Neural Estimation as a component to both estimate and optimize attention parameters makes sense.

The concept of using attention to identify sparse coupling effects in time series appears original for this reviewer and the method leverages the use of neural networks well. The method could likely be extended in the future to other neural architectures like transformers, therefore this work is a good initial foray into further work and improvements.

Text is well written and clear and the ablation experiments make sense.



Weaknesses:
There needs to be more clarity in the experimental setup:

Specifically, the way overfitting is prevented in the proposed ATEn method. Like Mutual Information Neural Estimation method, parameters are optimized in Transfer Entropy, which can result in perfect estimates of the mutual information (due to overfitting). How have the authors addressed this?

What are the inputs to fθ and fφ and how are they different from the inputs to the attention and classifier models? Is there a feature space beyond the scalar timeseries of the neurons?



Limitations:
No obvious negative societal impacts. Refer to Weaknesses for constructive suggestions for improvement.

Rating:
7

Confidence:
3

";1
yIcCkMUCtL;"REVIEW 
Summary:
The authors study the covariate shift setting of nonparametric (kernel) methods
(Regularized Empirical Risk Minimization with optional importance weighing) with
an analysis which includes a wide array of losses and and two conditions on the
importance function. They establish sharp convergence which corroborate other
rates in literature. Additionally they provide experiments showing these rates
in practice.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Quality: The authors extend the results to a wide array of losses and two types of covariate shift problems which is nice.
- Clarity: The paper is well-written and notation makes it easy to follow


Weaknesses:
- There are quite a bit of terms which are unknown in practice and they need to
  estimate the importance function which limits the practical impact of the
  method.


Limitations:
Same as in Questions section. No need for societal impact limitation.

Rating:
6

Confidence:
1

REVIEW 
Summary:
This manuscript presents convergence rates for kernel methods under covariate shift. Results fit quite a general framework, including common classification and regression losses. Two approaches are analyzed: (i) a usual M-estimator and (ii) an importance-sampling-like M-estimator. It is shown theoretically and empirically that the latter outperform the former.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The analysis presented in this paper provides interesting theoretical results regarding learning under covariate shift, which is a contemporary topic. The manuscript is well organized; it explains clearly the problem, state the results while discussing the hypotheses and, at the end, illustrates the theoretical findings by a numerical experiment.
I would like to stress that discussions regarding hypotheses are opportune and corollaries provide intelligible results.
The take-home message, stating that the importance-sampling-like estimator is better that the naive one, is interesting and confirms practitioners’ intuition. 

Weaknesses:
Major remarks:
1) My main concern is about the novelty of the proofs: hypotheses (i) and (ii) look like straightforward tools to link expectations under the source distribution to the target distribution by linearity or Cauchy-Schwarz inequality. I had a very quick glance to the supplementary material and it confirmed this guess (although I admit that I may be wrong). I think that its important, in order to assess the contribution of the paper, that the authors explain the original derivations appearing in the proofs, with respect to techniques used for obtaining similar results without covariate shift (unfortunately, I have no reference in mind).
2) Another (minor) point is that Figure 1 does not seem to verify neither hypothesis (i) nor (ii) since $\phi(x)$ seems to explode when $x \to \infty$. If it is the case, it would be better to find another example (or at least to discuss this point). If it is not the case, it would be informative to explain it.

Some suggestions of improvement:
1)  $f^*$ is defined in Section 2.1, before the problem setting in Section 2.2. However, in practice, it corresponds to the optimal function under the target distribution, which is not clearly stated. I suggest to make it clear.
2) Although an informed reader understand definitions Line 113, it is not totally clear that expectations are conditioned by observed data. I suggest to had this information.
3) $D$ could be added after “Finite rank” in Table 1.
4) Line 264, it is not totally clear that “For the moment bounded case” correspond to Figure 3. I suggest to had it.

Typographical remarks:
1) Extra “the” Line 5.
2) “that” instead of “that is” Lines 126, 131 and 132.
3) In Theorems 1-3, $\delta_n$ should satisfy an inequality that involves $\delta$ instead of $\delta_n$.
4) There are $\phi(\textrm x)$ instead of $\phi(\textrm x)^2$ Lines 212 and 223.
5) Full points are missing in captions.

Limitations:
Limitations are not addressed.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper provides a unified analysis of convergence properties for different kernel-based estimators under covariate shift. The analysis covers different loss functions and is focused on standard and importance weighted empirical risk estimators. The former are specified in Eq. (1) and the latter in Eq. (3). 

The first assumption is pretty standard and requires a uniformly bounded kernel function. The second assumption enforces a locally strong convexity on the expected loss function relative to source and target marginal distributions (source available during training, target assumed to be shifted and available at test time). The assumptions that characterize the distribution shift are given on page 4 (lines 131 and 132): i) in the first case the importance weights are $\alpha$-uniformly bounded, ii) in the second case the second moment of the importance weight function is bounded. 

Theorem 1 gives convergence bounds relative to case i) under the assumptions above. Further assumption is made to give a more readable interpretation of the bound in Corollary 1 which ties the convergence rate to kernel spectrum decay.
Theorem 2 gives a similar convergence result in a more difficult case ii), again under the assumptions listed above.
Theorem 3 considers an estimator that uses importance weighted empirical risk estimator, with truncated importance weights to avoid issues with tail samples. It is for case ii) and bounded second moment of importance weights. The latter result indicates much tighter convergence rate than the one in Theorem 2 that considers standard estimator without importance weighting.
Empirical analysis illustrates the tightness of the bounds on synthetically generated learning tasks and a real-case study. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
While I have not checked the proofs, the theoretical part of the paper is its strongest point. It is also an interesting characterization of distribution shift carried into the bounds and would be interesting to see what other more granular specifications are possible for future studies. A relative comparison between Theorem 2 and 3 also illustrates the utility of truncated importance weighted estimator, which might be important for practical applications. 

Weaknesses:
Empirical study might be the weakest part of the paper but given its nature should be fine. It might also be interesting to see how relevant are the assumptions on distribution shift relative to practical applications and datasets. 

Limitations:
Adequately addressed

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the generalization guarantees of non-parameteric methods in RKHS under covariate shift.
Compared to previous work (Ma et al. AOS2023), the authors extend their results from the squared loss to general Lipchitz loss functions.
The derived results show that

- under the uniformly bounded case for the importantce ratio, the unweighted estimator achieves the optimal learning rates in the $L2(d PT)$ space, where $PT$ is the target distribution. 
- under the bounded second monment case, the above estimator is sub-optimal.
- under a truncated ratio, a sharp learning rate can be achieved.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- generalization analysis under covariate shift is derived from the squared loss to general loss functions
- Under the the uniformly bounded case and bounded second moment case for the importantce ratio, the results can recover the result under the squared loss
- the results are derived under the truncated case

Weaknesses:
- Extension from the squared loss to general Lipschitz loss functions is based on Assumption 2. More discussion on this assumption is required for specific loss functions. If the space is limited, the discussion can be deferred to the appendix.

- There are several parts unclear in the proof. For example,  in the proof of Lemma C.1.2, the notations $P_n$ and $P$ are undefined in Eq.(2), and more details are needed for the first inequality in Eq. (2). 

- The proof idea and structure is almost the same as (Ma et al. 2023). For example, there is no significant difference between the proof of Theorem 3 and Lemma 2 in (Ma et al. 2023). This is because, under Assumption 2 and Eq. (10), the results under Lipschitz loss functions can be well controlled. 




Limitations:
N/A

Rating:
5

Confidence:
4

";1
BEHlPdBZ2e;"REVIEW 
Summary:
The paper introduces Tensor3 Net, a message-passing neural network architecture designed for molecular systems representation. Tensor3 Net leverages rank-2 Cartesian tensor representations and O(3)-equivariance. The tensors are decomposed into rotation group irreducible representations, enabling separate processing of scalars, vectors, and tensors when necessary.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
The paper is well-written and the idea of building a O(3) equivariant model with Cartesian Tensor is valid, given the computation complexity of models such as e3nn, based on spherical harmonics. 

Weaknesses:
The one main weakness is that the author only model rank-2 tensors, which is relatively arbitrary. What it should be done is to introduce a network with general rank cartesian tensors (these will then be 3 x 3 x 3 x ... x 3 tensors). I see the value of adding the l=2 components in a cartesian way too marginal for publication at Neurips. 

Limitations:
The authors do not discuss the limitations of their approach. 

Rating:
2

Confidence:
4

REVIEW 
Summary:
The paper presents TensorNet, a novel O(3)-equivariant message-passing neural network for efficient representation of molecular systems in scientific research. This model utilizes Cartesian tensor atomic embeddings to simplify feature mixing via matrix product operations. By decomposing tensors into rotation group irreducible representations, it enables independent processing of scalars, vectors, and tensors when required. TensorNet outperforms higher-rank spherical tensor models in performance, utilizing fewer parameters, even with a single interaction layer for small molecule potential energies. Additionally, TensorNet can accurately predict vector and tensor molecular quantities on top of potential energies and forces, greatly reducing the model's computational cost. Therefore, TensorNet provides a promising framework for developing state-of-the-art equivariant models with enhanced efficiency and computational affordability.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper adeptly presents TensorNet, a new learning model that not only establishes state-of-the-art performance but does so with a remarkable reduction in the number of parameters utilized. This marks a significant leap in model efficiency without sacrificing performance quality, setting a new benchmark in the field.

2. The majority of the empirical outcomes display a marked enhancement over existing methods, with the implementation of TensorNet consistently yielding superior results. The experimental evidence provided substantiates the model's efficacy, reinforcing the robustness and applicability of this innovative approach in real-world scenarios.

Weaknesses:
The paper's exposition of the model architecture can be somewhat challenging to comprehend due to its complex nature. A potential improvement would be the inclusion of intuitive diagrams or visual aids within the main body of the text, not just in the Appendix. Simplified illustrations, possibly even a step-by-step visual guide, could greatly enhance the reader's understanding of the architecture and make the methodology more accessible to a broader audience.

Limitations:
N/A

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes TensorNet, an O(3)-equivariant neural network architecture for molecules. Using the decomposition of a 3x3 matrix into a scalar, vector, and matrix shown in Eq. (2), TensorNet efficiently computes the interaction of O(3)-equivariant features up to l=2, where l is the degree (frequency) of the O(3) representation. The performance is evaluated on several standard benchmarks, such as qm9, which show that TensorNet achieves equivalent or better performance than baselines. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. TensorNet is a novel method. I have never seen the construction of an equivariant net based on the decomposition (2).
2. The performance is evaluated with several different datasets and in terms of different metrics (e.g., prediction error, computation speed).
3. The performance is comparable to or better than existing approaches. 

Weaknesses:
1. The paper has room to improve in terms of presentation. I'm unfamiliar with the chemistry (molecule) domain, and some parts seem challenging to understand without expertise. For example, a vector r_ij is defined on line 175, but the mathematical definition is not described. Also, the meaning of the cutoff radius is unexplained. Another point is that there is no reference nor citation to Eq. (2), the core equation of this paper. These are not well known in the machine learning community, and it could be better to introduce them in plain words. 
2. The limitations of the proposed method are not explicitly discussed. One limitation is that TensorNet cannot capture higher degree (l>2) information of O(3). 
3. It is argued that some existing methods have a downside: ""the computation of tensor products in most of these models containing higher-rank tensors and pseudotensors can be expensive"" (lines 111-112). However, only one method (ET) is compared with the proposed method in terms of computational cost. Also, there is no theoretical evaluation of the complexity e.g., using big-O notation. 

Limitations:
Limitations are not explicitly addressed. No particular concern for potential negative societal impact. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes a cartesian tensor representation for efficient learning of molecular potentials. It enables the feature mixing process to be a simple matrix product operation. In addition, the matrix product operation is simplified by cost-effective decomposition techniques. Experimental results demonstrate that the proposed method can effectively reach a comparable performance with a much smaller number of parameters. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The proposed method is very technically solid and this reviewer also thinks the efficient computation of equivariant architectures becomes increasingly important; 

(2) The extension of torchMD-net with efficient decomposition techniques is reasonably motivated, just like low-rank decomposition techniques for large language models;

(3) Experimental settings are very solid, covering enough number of experimental settings, which fully support the effectiveness of the proposed method. 

Weaknesses:
(1) The presentation is not that easy to follow and the notation is a little bit complicated, which brings additional hardness for readers to understand the core idea. In addition, although the paper is more about mathematical techniques utilization, no figure illustration provided is still very tough for readers to quickly capture the core idea; 

(2) The proposed method is more like an application of efficient tensor decomposition techniques to the existing equivariant network architectures. Without molecular domain-specific insights somehow lowers the significance of the proposed method. 

Limitations:
NA. 

Rating:
5

Confidence:
4

";1
QRAS5wSgEy;"REVIEW 
Summary:
This paper consists of two mostly unrelated parts.  In the first part, the authors aim to detect whether two sets of samples are drawn from the same distribution.  To do this, they apply the unbiased estimate of the MMD, in the manner suggested by Sutherland et al., to produce a conformal p-value.  Then they propose a small variation on Sutherland's approach, which they call MMD-CC, that requires twice as many samples but supposedly reduces some variance (although no further explanation or analysis is given).  They apply these techniques to detecting distribution shift between CIFAR-10 and CIFAR-10.1 using a feature-generating network trained on an external dataset (ImageNet), similar to ""outlier exposure"" methods.

In the second part of the paper, they aim to detect whether a single sample is drawn from same distribution as a set of inliers or not.  For this, they propose the CADet method, which has no discernable relationship to MMD (3) or the unbiased MMD estimate in (4).  Rather, CADet is closer to the CSI approach from Tack et al. and the ICLR'21 approach of Sohn et al., in that it relies on contrastive learning and distributional shifts.  Because key parts of CADet are never explained by the authors, it's difficult to summarize further.  The authors test CADet on experiments where ImageNet is the inliers and other datsets are the outliers, but they never compare to the state-of-the-art CSI approach even though they write ""the closest work in the literature is CSI"" on line 86.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The combination of Sutherland's MMD with SimCLRv2 gives good empirical performance in detecting distributional shift between CIFAR-10 and CIFAR-10.1.

Weaknesses:
1. Claims about the existing literature are overblown and key citations are missing.  
a. On line 46, the authors claim that ""only a few works detect OOD samples from single inputs ... without requiring access to OOD samples to train or tune the method.""  But there are dozens, if not hundreds, of papers proposing such methods.  
b. Although the authors acknowledge that the CSI paper by Tack et al. is very close in spirit to CADet, they do not cite an even closer method in the ICLR'21 paper by Sohn et al.  
c. Although the authors frequently compute conformal p-values, they don't cite any relevant literature.
d. The authors do not cite previous work that uses MMD for OOD, such as Dong et al.'s ""Neural Mean Discrepancy for Efficient Out-of-Distribution Detection"".

2. Throughout the paper, the p-values are invalid.  In the best case, the conformal p-values computed by the authors hold only only marginally, i.e., in expectation over the validation data.  They do NOT hold for a fixed validation set.  For a detailed explanation, see Bates et al. ""Testing for Outliers with Conformal p-values"" or Magesh et al. ""Multiple testing framework for out-of-distribution detection"" or Angelopoulos et al. ""Conformal Prediction: A Gentle Introduction"".

3. In some cases, the p-values are invalid for more fundamental reasons.  In MMD-CC Algorithm 1, for example, the conformal p-values are computed as if n_perm was the size of an i.i.d. validation dataset.  But n_perm permutations are not independent and so the p-values computed in this way are invalid.  For example, there is nothing preventing the user from specifing a value of n_perm that is higher than the total number of possible permutations.  In fact, the MMD-CC method is entirely heuristic and comes with no analysis, which is a serious problem. 

4. The MMD-CC method is underwhelming at a practical level as well.  First, it requires twice as many samples as Sutherland's MMD.  Despite that limitation, it performs worse in Table 1.

5. For the CIFAR-10-vs-CIFAR-10.1 experiment, the proposed feature generators were trained using ImageNet (see line 160).  This is problematic for several reasons.  First, using an external datset for OOD detection is a form of outlier exposure that violates the main claim of the paper stated on line 45.  Second, the other methods under test do NOT use an external datset, making the comparison unfair.  Third, the use of an external dataset goes against the instructions on line 94, which explicitly state that SimCLRv2 is to be trained on ""in-distribution samples."" Fourth, alhough the authors wrote that ""a fair comparison is difficult,"" that's not true; they could have easily trained their feature generator on the CIFAR-10 inlier dataset and it would have avoided all these problems.

6. The authors claim a weakness of CSI is that it can't detect adversarial perturbations, but in doing so the authors are conflating two entirely different versions of the OOD problem.  The first version is where one is given a trained supervised classification network and asked to detect samples different from those used to train it.  In this problem, we start with a network and so adversarial perturbations are well defined.  The second version (solved by CSI) is where one is given an inlier distribution and are asked to detect samples different from it.  Here, there is no network, and thus no adversarial perturbations, and so it is unfair/illogical to claim that CSI is ""not well suited for adversarial detection."" CSI solves an entirely different problem.

7. The section ""detection of distribution shifts from a small number of samples"" hides the fact that a huge number of samples are required to train the feature generating network.  In fact, by training on an external dataset, the authors are implementing a form of ""outlier exposure"" that goes against the main requirement stated on line 45.

8. Critical information about the CADet method (the main contribution according to the title/abstract) is missing, making it impossible to fully evaluate the paper.  In particular, the similarity function ""s"" in (5) is never specified.  On line 195 it says ""self-supervised contrastive learning trains a similarity function,"" but that is incorrect: SSCL trains a feature generating network that maps an input ""x"" to a set of features ""z,"" as specified on line 99.  SSCL does NOT train a similarity function that maps two inputs ""x1"" and ""x2"" to a scalar.

9. The choice to linearly combine the m^in and m^out scores in (6) is heuristic, and probably highly suboptimal.  The framework of multiple hypothesis testing gives principled ways to combine scores.  For more on multiple hypothesis testing, see Candes' STATS 300C lecture notes.

10. The CSI method by Tack et al. is ""the closest work in the literature"" according to line 86. Yet the authors curiously omit it from all the numerical experiments, choosing much easier competitors. 

Limitations:
yes

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper studies the OOD detection of new classes and adversarial attacks with learned similarities via self-supervised contrastive learning in conjunction with MMD two-sample test. To enable MMD applicable for OOD detection on single samples, they improve the idea by using augmentations to create a set. Experimental results on new classes and adversarial examples detections show that their method can perform favorably against previous arts.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-written and well-organized.

2. The paper does a good job summarizing preliminary ideas and providing detailed settings reproducing the overall algorithm.

3. The paper provides helpful discussions regarding the efficiency of the method.

Weaknesses:
1. According to Table 3. it seems that the proposed method does not show superior performance against previous arts when using supervised training. Discussions regarding this matter could be included to better justify the method.

2. I'm not an expert on OOD detection. I'm wondering why there are no error bars in the reported results, given that different hyper-parameters could easily improve (or decrease) the performance, and the differences between the compared arts are not that significant reported in Table 3. 

3. Some hyper-parameters are empirically selected without proper justifications, especially those regarding augmentation.

Limitations:
Limitations have been discussed in the manuscript.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors propose to handle the OOD detection problem, exploring self-supervised contrastive learning to detect samples from previously unseen classes and adversarially perturbed samples. They use self-supervised contrastive learning with the maximum mean discrepancy (MMD) to test if two sets of samples originate from the same distribution. They propose CADet which takes advantage of the similarity of the contrastive representation/transformation of the same sample. The proposed approach is thoroughly tested in two scenarios: the scenario with previously unseen classes and the scenario with adversarially perturbed samples.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The problem of OOD detection is very important in a lot of aspects of machine learning, and it's a worthwhile problem to study and research.
+ The proposed approach, CADet, achieves good performance on the benchmarks.
+ The combination of MMD and self-supervised contrastive learning seems to be effective.

Weaknesses:
- The work is motivated by the claim that OOD samples will be encountered in the wild when the machine learning models are deployed in the wild, but this work is not tested in the wild, in a smart computing or cyber-physical system setting. This is a minor complaint as it is tested on (solely) datasets, but it is advised that the authors revise the paper to not include such claims.

- The work fails to be evaluated on ""fake"" OOD samples, or samples that might originate from a different distribution than the training samples but belong in the same classes. For example, the Webcam domain in the Office-31 dataset and the Amazon domain in the same dataset, have the same classes, but distribution-wise, the samples are different. Will CADet consider Webcam's samples of class A to be OOD from Amazon's samples of class A? 

- The Related Works section is too high-level: it's evident that the authors are aware of the recent advance in the field of OOD detection, but a more detailed explanation (one sentence or two) for (most of) the mentioned works would be desirable.

Limitations:
+ The paper proposes an effective approach (self-supervised CL + MMD), but there are other mainstream alternatives to MMD such as the KL divergence and Mahalanobis distance. Why do you pick MMD in particular, instead of KL-divergence and Mahalanobis distance (both of which can be adapted to perform on both instance-level and distribution-level)? Is there any comparison on why KL-divergence based and Mahalanobis distance based approaches aren't as effective?

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes Maximum Mean Discrepancy with Clean Calibration (MMD-CC), as an improvement of MMD when the number of samples is small. Moreover, it introduces CADet, a novel anomaly detector for both OOD and adversarial detection inspired by MMD. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In line with recent works, unsupervised anomaly detection in deep learning is a relevant research objective. The paper is well-written and proposes a novel approach for anomaly detection, improving existing works on Maximum Mean Discrepancy. Moreover, I appreciated the idea of improving MMD to overcome its limitation of requiring sets of test samples.

Weaknesses:
- Experimental setting. I did not understand why for benchmarking adversarial detection you kept ODIN [1] and discarded the Mahalanobis detector. To the best of my knowledge, ODIN is mainly designed for OOD detection, furthermore in [2] the Mahalanobis detector achieves better performance than both LID [3] and ODIN for adversarial detection. Moreover, no adaptive attack was tested [4].
- Strength of the results. In Table 3, CADet has a high variance of the results depending on the training strategy  (iNaturalist vs ImageNet-O) and is not the overall best performing. Moreover, only the AUROC was reported, while other commonly employed metrics are missing (FPR, AUPR). Regarding MMD-CC, it should be better clarified its significance given that it is significantly better only on Table 2, with PGD and small n_samples.
- Missing related works. Recently, other unsupervised anomaly detection (for both OOD and adversarial) algorithms have been proposed, such as [5]. A comparison with such method should be performed.
- Reproducibility. No code was released. Some hyperparameters are missing (e.g., number of neighbours for LID [3], perturbation size for Mahalanobis [2] and ODIN [1]). Given how challenging it is to evaluate and compare OOD detectors, this is an important weakness.
- Architecture scale. Given the existence of other unsupervised methods for anomaly detection ([5]) and the lack of convincing performance improvements over related methods, the computational limitations of CADet are a significant weakness.

[1] Liang, S., Li, Y., & Srikant, R. (2018). Enhancing the reliability of out-of-distribution image detection in neural networks. ICLR 2018.

[2] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. NeurIPS 2018.

[3] Ma, X., Li, B., Wang, Y., Erfani, S. M., Wijewickrema, S. N. R., Schoenebeck, G., Song, D., Houle, M. E., & Bailey, J. (2018). Characterizing adversarial subspaces using local intrinsic dimensionality. ICLR 2018.

[4] Tramer, Florian, et al. ""On adaptive attacks to adversarial example defenses."" Advances in neural information processing systems 33 (2020): 1633-1645.

[5] Raghuram, J., Chandrasekaran, V., Jha, S., & Banerjee, S. (2021). A General Framework For Detecting Anomalous Inputs to DNN Classifiers. Proceedings of the 38th International Conference on Machine Learning, 8764–8775. 


Limitations:
The authors addressed the main limitations of their method.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a novel method for Out-of-Distribution (OOD) detection called CADet (Contrastive Anomaly Detection). The authors leverage self-supervised contrastive learning and the maximum mean discrepancy two-sample test(MMD) to assess whether two sets of samples have been drawn from the same distribution. The method is designed to detect OOD samples from single inputs and performs well on both label-based and adversarial detection benchmarks, without requiring access to any OOD samples nor previous classes to train or tune the method.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The authors propose a novel method for OOD detection that does not require access to neither in distribution samples nor OOD samples to train or tune the method.
2. They use similarity functions learned by self-supervised contrastive learning with MMD to assess the distribution even with few shot images.
3. Paper shows the method outperforms current methods in adversarial detection tasks while performing well on label-based OOD detection.


Weaknesses:
I do not see major weaknesses for this paper as of now. I will be updating this section if need be later.

Limitations:
Authors have done a good job presenting the limitations of their paper.

Rating:
6

Confidence:
3

";1
os2BdbiGwX;"REVIEW 
Summary:
The paper proposes a method to combine deep mutual learning with BNN to diversify the weight distributions of each BNN networks in a pair or ensemble, to improve performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. AFAIK this is the first work combining mutual learning with BNN, so the authors can claim this point.
2. The paper is in general written clearly and easy to follow.
3. Experiments are adequate with ablation studies on individual features impact on diversity.

Weaknesses:
1. Some design choices are found to be ""empirically"" working well without too much discussion or hypothesis.
2. Would be interesting to see how the model performs for o.o.d test data, especially uncertainty performance.

Limitations:
None.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed a mutual learning approach to learn a pair of Bayesian Neural Network(BNN). The posterior of BNN is approximated by Variational Inference using a Gaussian distribution with a diagonal covariance matrix. To make the BNN learn different perspective of the data, the author proposed to increase the diversity in parameter space and intermediate feature space by adding the an estimate of distance between parameter distribution and fused feature distribution of two BNN models into the objective function. Empirically, the proposed method outperform existing mutual learning method and vanilla BNN model in terms of accuracy, negative log likelihood loss and expected calibration error. An ablation study is also provided to investigate the usefulness of each component. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well written and easy to follow. Increasing the diversity of parameter distribution and intermediate feature distribution of peer BNN models to boost performance is an interesting idea. Experiments and detailed ablation study demonstrate the effectiveness of proposed method.

Weaknesses:
1. It is mentioned in the abstract and introduction that the BNN model with variational inference may underperform deterministic model or BNN obtained by MCMC, the baseline only involves BNN model trained with(DML) or without(vanilla) mutual learning. Would the proposed method close the gap to some extent? Data augmentation, optimizer may all affect performance, so it is still helpful to include deterministic model results follow with same training setup. I would expect the BNN model to outperform deterministic model at least in NLL and ECE, and with the 50 ensemble, it can outperform the accuracy. 

2. Continue with last point, for MCMC method (e.g. in line 81 of the paper), I agree that traditional MCMC method(e.g. Metropolis Hasting) may not be feasible for large model, and memory storage can be an issue for MCMC method. But I don't think the stochastic gradient MCMC cited in line 81 would require prohibitive computational cost, it behaves like adding a noise to at each step of standard SGD training. 

3. The code is not provided so it may hurt the reproducibility of the paper.

Limitations:
The authors addressed the limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper titled addresses the challenge of improving the performance of Bayesian Neural Networks (BNNs) by leveraging the concept of mutual learning. BNNs provide a means for quantifying uncertainty in predictions through probability distributions of model parameters. However, BNNs often fall short in performance compared to their deterministic counterparts. The authors propose a novel approach that employs deep mutual learning to enhance the capabilities of BNNs.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Innovative Approach: The paper introduces a novel method that combines deep mutual learning with Bayesian Neural Networks. By promoting diversity in both network parameter distributions and feature distributions, the proposed approach enables peer networks to acquire distinct features, capturing different characteristics of the input data. This innovative technique enhances the effectiveness of mutual learning in BNNs.
2. Detailed algorithm description: The paper provides a thorough and detailed description of the proposed algorithm for improving the performance of Bayesian Neural Networks (BNNs) through deep mutual learning.
3. Comprehensive Experiments: The authors conduct extensive experiments to evaluate the proposed approach thoroughly. The experimental results are statistically sound and demonstrate significant improvements in classification accuracy, negative log-likelihood, and expected calibration error compared to traditional mutual learning methods for BNNs.

Weaknesses:
1. Limited variety in experimental validation: One weakness of the paper is that the proposed approach and its effectiveness are only verified through experiments conducted on Residual Neural Networks (ResNets). It would have been beneficial to include experiments on a diverse set of network architectures to demonstrate the approach's effectiveness across different model types and complexities. 
2. Lack of detailed explanation for temperature, α, and β: One weakness of the paper is the limited explanation provided for the temperature parameter (T), α, and β, which are crucial components of the proposed approach. These parameters play a significant role in controlling the diversity of network parameter distributions and feature distributions, but their specific effects and optimal values are not thoroughly discussed.
3. Weakness in the conclusion: The current conclusion merely restates the experimental results and does not highlight the broader implications of the proposed approach or its potential impact on the field.

Limitations:
The author should supplement more experiments to prove its effectiveness and strengthen the conclusion.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper focuses on improving the accuracy of BNNs by promoting diversity in both parameter space and feature space while training two peer BNNs with mutual learning between them. More specifically, they train two variational BNNs with a mean-field Gaussian variational loss for each along with a KL divergence term between the (temperature-scaled) predictive distributions of the two models, a Wasserstein distance term between the corresponding approximate posterior distributions across the two models (added as a softplus(-distance) term), and a KL divergence term between corresponding feature distributions. On the latter term, instead of directly maximizing the distance between corresponding feature distributions, they instead do so on ""fused feature distributions"". To do so, they use learned cross-attention to fuse the features from multiple feature levels in a model (two at a time). Then, they use the KL divergence between the distributions of the fused feature distributions of the two peer networks. To derive the distributions, they use the conditional probability density defined as $p_{i|j} = \frac{K(F'_i, F'_j)}{sum_{k=1, k \noteq i}^n K(F'_k, F'_j)}$, where $K(F'_a, F'_b)$ is a kernel function between two fused feature representations. Given those conditional probs, they compute a KL divergence term. Similar to the parameter space diversity term, they add this term to the loss as softplus(-divergence). The paper claims to be the first to propose maximizing the distance between feature distributions to promote diversity. In terms of experiments, the paper includes results for ResNet models on CIFAR-10/100 and ImageNet, measuring accuracy, NLL, and ECE as metrics, and comparing different approaches.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper does a great job of precisely articulating the modeling approach, and discussing the relevant background info. More specifically, the proposed approach of adding terms to promote diversity in parameter and feature space is clear and would be easy to reimplement.

Weaknesses:
My main concern is with the experiment section. More specifically, a few key details are unclear in the text, and importantly a deterministic baseline is missing that I believe should be present given the framing of the paper and relevant literature. Please see the Questions below. Given updates, I believe the paper would be great and I would gladly update my rating.

Limitations:
No limitations are included.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a novel method for enhancing the performance of Bayesian Neural Networks (BNNs) by employing deep mutual learning. The proposed approach aims to enhance the diversity of both network parameter distributions and feature distributions, encouraging individual networks to capture unique characteristics of the input data. The effectiveness of the proposed method is demonstrated on datasets, including CIFAR10, CIFAR100, and ImageNet.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed method improves performance and uncertainty estimation while reducing the expected calibration error (ECE).  The technical approach is novel as the method introduces mutual learning in the context of BNNs and first to propose maximizing the distance between feature distributions and parameter distributions. The paper includes large scale data experiments (ImageNet) and ablation studies to demonstrate the effectiveness of each technical contribution introduced in this paper.

Weaknesses:
The previous studies mentioned in the paper utilize alignments on feature maps [4] or predictions [38], rather than diversifying them. In contrast, the proposed method diversifies both feature distributions and parameter distributions which is an opposite approach to the previous works. Interestingly, both alignment-based and diversification methods improves performance over vanilla BNNs, as indicated in Table 1, 2, 3, and 5. However, the paper does not explicitly explain the reasons behind the performance improvements resulting from these contrasting approaches.

Given the observed contradicting results in the experiments, where the alignment-based method (DML [38]) also enhances the performance of BNNs, an important question arises: could combining alignment-based methods with parameter diversification further improve BNN performance? Alternatively, is it necessary to diversify both feature and parameter distributions to achieve significant improvements?

In the experiment section, the proposed method is only compared with [38] and not with [4]. 

Hyperparameters used for CIFAR experiments and ImageNet experiments are different. However, the paper does not describe details regarding the hyperparameter tuning or determination.

Limitations:
Limitations are shortly addressed in the supplementary.

Rating:
5

Confidence:
4

";1
UXtLrsG4Rf;"REVIEW 
Summary:
The paper proposes a method to learn low-dimensional continuous-time representations of network nodes, based on the collection of interaction events among them. More precisely, the events are in the form of $(i,j,t)$, where $(i,j)$ is the pair of nodes involved in the interaction event, and $t$ is the occurrence time. The proposed method first estimate the intensity function $\lambda_{i,j}(t)$ of events between each pair of nodes $(i,j)$ at every time instant $t$, then project the intensities of each node at time $t$ onto a learned lower dimensional subspace to obtain a representation. Theoretical results on the recovery error of the representation is provided. Numerical experiments using real data shows the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper proposes to estimate the representation of nodes using continuous-time events, which seems to be a novel type of data.

Weaknesses:
I find the presentation of the paper generally vague and hand-wavy. See the following.

1. The introduction is way too high-level. The authors should be more specific about the problem setting in this paper, for example, why we care about dynamic models, continuous-time event data, low-dimensional representation of nodes etc.

2. The related work is not specific. The authors should use a sentence to summarize the contribution of the mentioned papers and explain the difference from your work.

3. Lemma 1 is not correct. $\widehat U_d$ minimizes the residual sum of squares at $B$ chosen time instants, but not the integrated one. 

4. In Section 3, notation part, what is the difference between $\gg$ and $\gtrsim$? Also is the universal constant multiplicative or additive?

5. It's not clear what `$\approx$' means in Section 4. 

Limitations:
.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper presents a framework called Intensity Profile Projection (IPP) for continuous-time representation learning in dynamic networks. The authors aim to address the challenge of capturing temporal dynamics and evolving relationships in dynamic networks with both high statistical precision and interpretability. The model leverages the concept of intensity profiles, which encode the temporal changes and interactions between nodes in a network. The model provides a uniform error bound for learned node representations and preserves a novel ""temporal coherence"" property compared to existing baselines. Empirical results on real-world dynamic network datasets demonstrate that IPP outperforms existing methods in various tasks such, highlighting its ability to capture continuous-time representations and uncover temporal patterns in dynamic networks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper introduces the Intensity Profile Projection (IPP) framework, which offers a unique and innovative approach to continuous-time representation learning for dynamic networks. It introduces the concept of intensity profiles and effectively utilizes them to capture temporal dynamics. 

2. Theoretical analysis towards the model shows that the model can achieve high statistical precision and preserve interpretability in terms of """"temporal coherence"".

3. The paper is in general easy to follow.

Weaknesses:
1. Lack of comparison with state-of-the-art methods: Although the paper claims improved performance over existing methods, it does not provide a comprehensive comparison with some existing continuous models such as GraphODEs[1,2,3,4] which combines neuralODE with GNNs to model network evolution over time.

2. Scalability: The scalability of the IPP framework is not extensively discussed. It would be valuable to address the computational requirements and scalability limitations of the proposed approach, especially when dealing with large-scale dynamic networks.

3. The related work section is too short to provide a comprehensive background of the research topic.


[1] Huang, Zijie, Yizhou Sun, and Wei Wang. ""Learning continuous system dynamics from irregularly-sampled partial observations."" Advances in Neural Information Processing Systems 33 (2020): 16177-16187.

[2] Song Wen, Hao Wang, and Dimitris Metaxas. 2022. Social ODE: Multi-agent Trajectory Forecasting with Neural Ordinary Differential Equations. In Computer Vision–ECCV 2022: 17th European Conference.

[3]Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting system dynamics. In
401 ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 705–715, 2021.

[4] Zang, Chengxi, and Fei Wang. ""Neural dynamics on complex networks."" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 892-902. 2020.

Limitations:
The authors have discussed the limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
To represent the continuous dynamic network, authors provide the framework based on the intensity profile. First, the intensity between nodes is estimated, which produces the intensity profile. Low dimension reduction via SVD is applied on the intensity, and then each node embedding is obtained by the low dimensional subspace.
Author also provide various theoretical analysis about the error bound and the bias-various trade-off. Theoretical analysis as well as empirical analysis on the simulated data demonstrates that the proposed method capture structural preserving and temporally coherent properties. Case study on the real data is conducted to explain the outcome of the proposed framework qualitatively

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- Simple but powerful method is proposed
- Based on the mathematical model, theoretical bound is analyzed and explained.
- IPP can capture the behavior of a bifurcating block model.

Weaknesses:
- The proposed method is not novel enough. SVD decomposition is a very common technique for the reduction of dimensions, and it often suffers from the long-tailed singular values. 
- Comparison is too limited. The analysis has been made only for the simulated data with figures. More experiments as well as some qualitative results would be great to have.
- SVD decomposition does not prevent producing negative values at the reconstruction.
- The proposed projection space is very dependent on the fixed dataset. At least, how to leverage the given embeddings for predictions is not straightforward. Given this, the potential application value is not very clear.

Limitations:
Often, the meaning of each dimension from the SVD decomposition is not clear. This interpretability is not necessarily required for the representation, but this should be addressed when presenting the case study.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose an approach for learning time-varying node embeddings from continuous-time dynamic network data, which consist of a set of instantaneous timestamped relational events between nodes (e.g., messages from one social media user to another). Their proposed approach learns a projection that minimizes reconstruction error of the pairwise intensities between nodes and comes with theoretical guarantees on estimation error. They also show that their approach generates embeddings that both preserve network structure at a given time and is temporally coherent. They demonstrate strong empirical performance on simulated data compared to other dynamic network embeddings. Furthermore, they use their approach to analyze a real network data set on face-to-face interactions of primary school students, which is quite enlightening due to the interpretability of their model.

*After rebuttal:* The authors have clarified the one question I had about the meaning of ""inductive"" in their setting. I continue to strongly support the paper.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Proposed approach learns time-varying node embeddings from continuous-time networks with theoretical guarantees, which is among the first, if not the first, in the literature.
- Proposed embeddings can satisfy two good properties of structure preservation and temporal coherence.
- Very well written and organized paper that provides highlights of theoretical analysis in the main paper followed by details, including proofs, in the supplementary.

Weaknesses:
- There's a large body of related literature on probabilistic generative models for continuous-time networks using point process models such as Hawkes processes that should be discussed. Many of these models are based on stochastic block models or latent space models and are thus also learning node embeddings. See suggested references below.
- No quantitative evaluation. This is only a minor weakness in my opinion because I view the main contribution to be theoretical.

Typos and minor issues:
- Supplementary Section C heading: Visualsation -> Visualisation

References:
- Arastuie, M., Paul, S., & Xu, K. S. (2020). CHIP: A Hawkes process model for continuous-time networks with scalable and consistent estimation. In Advances in Neural Information Processing Systems 33 (pp. 16983-16996).
- Corneli, M., Latouche, P., & Rossi, F. (2018). Multiple change points detection and clustering in dynamic networks. Statistics and Computing, 28(5), 989-1007. doi:10.1007/s11222-017-9775-1
- Huang, Z., Soliman, H., Paul, S., & Xu, K. S. (2022). A mutually exciting latent space Hawkes process model for continuous-time networks. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (Vol. 180, pp. 863-873).
- Junuthula, R. R., Haghdan, M., Xu, K. S., & Devabhaktuni, V. K. (2019). The Block Point Process Model for continuous-time event-based dynamic networks. In Proceedings of the World Wide Web Conference (pp. 829-839).
- Matias, C., Rebafka, T., & Villers, F. (2018). A semiparametric extension of the stochastic block model for longitudinal networks. Biometrika, 105(3), 665-680. doi:10.1093/biomet/asy016
- Yang, J., Rao, V., & Neville, J. (2017). Decoupling homophily and reciprocity with latent space network models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.

Limitations:
Limitations are thoroughly discussed in Section 6. I commend the authors for being very forthcoming with these limitations. I don't view the limitations as weaknesses, because they are mostly limitations that apply to all unsupervised problems.

Rating:
8

Confidence:
4

";1
54hYifmQZU;"REVIEW 
Summary:
In this paper, the authors introduce a new regret metric, called Cost of Learning in Queueing (CLQ), to quantify the rate at which an optimal scheduling policy can be learned to minimize the time average queue lengths. The authors derive a lower bound to CLQ and show that an UCB-based policy comes close to achieving the lower bound. They also extend their policy to multiple queues in a network setting by combining the UCB policy with the celebrated Backpressure policy. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The dependence of the regret bound with respect to the slack parameter (epsilon) is optimal.
2. Bounding the CLQ metric in terms of satisficing regret is intriguing.


Weaknesses:
1.	The paper [34] considers an unnormalized version of the same metric proposed in this paper. However, the results presented in this paper give a potentially weaker bound (linearly increasing) than the result in [34]. To be more specific, Theorem 2 proves a constant (let’s denote it by $c$) upper bound to the CLQ metric. Plugging this upper bound into Eq (3) yields the following linear bound on the queue length regret:

   $\sum_{t=1}^T \mathbb{E}\left[(Q(t, \pi)) -  Q(t, \pi^*) \right]\leq cT, ~ \forall T \geq 1 $

But it is already known from [34, Theorem 2, 3] that there exist simple dynamic policies under which the queue-length regret (i.e., the LHS of the above equation) can be bounded by a constant. Clearly, the CLQ metric fails to capture this strong result and paints an overly pessimistic picture. It is also not clear if the result presented in this paper strictly and quantitatively improves upon [34], even for smaller horizon lengths. 

2.	The proposed algorithm directly uses UCB, and its analysis does not present any new technical insights.  


Limitations:
This is a theoretical work and does not seem to have any potentially negative societal impact.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a new metric to quantify the cost of learning in queueing networks. This notion is required to capture the differences between holding costs in queues and, say costs accumulated in a bandit setting; the latter having a monotonicity property (in expectation). The authors then derive a worst-case lower bound for this metric, and propose UCB based algorithms that come `close' to the lower bound in the order sense. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The CLQ metric proposed here is meaningful, and quite appropriate for queueing systems. Prior work uses a standard regret metric, which may not be very appropriate given that queue tend to regenerate.

2. The UCB-based analysis appears quite novel; the authors have to bound the CLQ differently in the initial learning phase and subsequently; this issue does not arise in standard UCB analyses for bandits.

3. The analysis extends to queueing networks.

Weaknesses:
No significant weaknesses here. I would have liked to see some more exposition in certain places, including a description of the algorithms, and a comparison between the lower and upper bounds derived here. But I can see that the authors have done the best they could to tell the story within the space constraints.

I do have some (minor) suggestions though.

1. I think it is worth highlighting around Theorem 1 that the bound derived is not instance-dependent, but worst-case in nature.

2. The following sentence on Line 270 on Page 7 ""Therefore, in expectation, the queue under .... never empties."" was unclear to me.


Limitations:
Not applicable.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies a problem that involves both learning with queueing.
In the simple setting, there is a single queue served by multiple
servers. However, the service rate at each server is unknown and needs
to be learned. Intuitively, the combination of the learning policy and
the scheduling policy will impact the queue length dynamics of the
system.  Prior work mostly focuses on the queue length performance in
the late stage of the system. However, due to the nature of queueing
systems, this late-stage performance is relatively invariant to the
learning policy, and thus the late-stage metric does not adequately
capture the impact of learning. In contrast, the first contribution of
this paper is to propose a new metric, called CLQ (Cost-of-Learning).
CLQ takes the difference between the time-averaged queue length of a
given policy and that of the optimal, and then takes the maximum over time. 
This maximization over the entire time-horizon allows CLQ to capture the
early-stage dynamics of the system, where the impact of learning is more
obvious.  Then, the authors provide both lower bounds and achievable
bounds for this new metrics, which differ only by a logarithmic factor
in the number of servers.  Finally, the results are also extended to
more general multi-queue multi-server systems.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The new notion of CLQ captures the impact of learning more accurately
than existing studies, by including the early-stage dynamics of the
queue. This is a significant contribution. 

2. The lower-bound and achievable-bound are nice and differ only by a
logarithmic factor.

3. The results are extended to general multi-queue multi-server systems. 

4. The proof idea based on satisfying regret is also very interesting.

Weaknesses:
I don't find major weaknesses.

Limitations:
I do not find discussions on limitations or potential negative societal
impact.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors consider online queuing systems in a discrete time setting. They study settings with single class queue and multi-class queues, and they propose to consider a metric CLQ that serves as a conservative measure on how the queue length(s) could grow across every time point in a horizon. The authors propose a natural UCB algorithm, and demonstrate that it has a near optimal CLQ in a single queue setting. The authors also derive similar bounds in a queueing network setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The analysis is quite novel. In addition, the definition of the CLQ and its analysis is new to the best of my knowledge. In particular, the consideration of the satisficing regret is quite interesting. 
- The authors achieve a nearly tight result in the case of one queue. Overall, the technical results are solid.
- The notion of CLQ seems adequate as an alternate metric for online queueing systems, but I still have some question (see Question 2 in ``Question'')

Weaknesses:
1. The discussions on queueing networks could be benefited from more details. While the authors stated that they will provide additional examples on settings modeled by their queueing networks model in Appendix A, Appendix A does not seem to contain much details. To overcome this weakness, the authors should provide a detail account on how 
 
  - the model on the bipartite queueing system in [11] (Line 187), 
  - the model on the multi-server system in [39] (Line 190),

are modeled by the queueing network formulation used by the authors. In particular, it will be crucial for the authors to highlight what are the individual element in the instance tuple (Line 170) in these models. While there could be quite a fair bit of details, I believe that the authors could include them in the Appendix A so that useful details are provided without violating the page limit.

2. There is no simulation to showcase the results. 

3. The notion of CLQ seems not to tell us the long term behavior of a policy, since it is taking a worst case over all horizon lengths $1, 2, ....$ (See Question 2)

Limitations:
There is no potential societal impact to my best knowledge. This is a theoretical paper.

Rating:
6

Confidence:
4

";1
m11TbsaQQI;"REVIEW 
Summary:
{Summary}
This paper proposes a stochastic cubic regularization type algorithm for hyperparameter optimization that does not depend on hyper-gradients. Theoretical analysis shows that the proposed method can converge to approximate second order stationary points with lower sample complexity than that of first-order optimization methods, which can only find first-order stationary points. Experiments demonstrate the effectiveness of the proposed method using both synthetic and real data.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
{Strengths}
1. This paper is clearly written and provides a new algorithm to solve nonconvex bilevel hyperparameter optimization problems (with stochastic relaxation).  
2. In this setting, solving the cubic subproblem takes little time, as the cubic problem dimension is usually very small.
3. Theoretical analysis shows that the proposed cubic algorithm achieves lower sample complexity than that of first-order methods.



Weaknesses:
{Weakness and Questions}
1. The algorithm design is a direct application of cubic regularization, and therefore is not novel.
2. It is not clear what is the technical novelty in the convergence analysis compare to the analysis of inexact cubic regularization. The author mentions that the constructed $g$ and $B$ are not unbiased estimators of the gradient and Hessian. How does this challenge and affect the technical proof?
3. There are existing works on finding second-order stationary points of bi-level optimization, e.g., ``Efficiently Escaping Saddle Points in Bilevel Optimization''. Please discuss and compare with it.
4.  In the experiments, many curves are piece-wise flat (constant). Does that mean the hyperparameters do not change in those iterations?

Limitations:
see the previous section.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a cubic regularization scheme for the outer loop optimization of bilevel optimization problems. The cubic regularization is very appealing since it can ""avoid saddle points"". Theoretically, authors extend the convergence results of the cubic regularization to inexact gradients to make it work for bilevel optimization. Empirically, authors show that their method achieves better performance on bilevel optimization tasks. Interestingly, they investigate the eigenvalues of the hessian.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The idea is very interesting and tackles the very important and hard problem of finding a proper outer procedure for bilevel optimization. Avoiding saddle points seems to be a very appealing property!

Weaknesses:
However, it feels like the writing of the paper could be polished (there are unmerged commit line 264 in the supplementary material.). A lot of statements are not properly backed. I am not sure to properly understand the interest of the proposed method.

Limitations:
NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of hyper-parameter optimization in the context of machine learning. Through stochastic relaxation, the problem can be formulated as bilevel optimization, and the authors propose to use cubic regularization to solve the optimization problem. It is shown that under some regularization conditions on the loss function and hyper-parameter distribution, the algorithm efficiently converges to an approximate second-order stationary point. Furthermore, experiments are conducted to verify the effectiveness of the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and quite easy to read. The authors explain the motivations, method and results in a very clear way.
2. Extensive experiments are conducted to demonstrate the wide applicability of the proposed method. All experiment settings are explained in details.

Weaknesses:
While the authors provide theoretical guarantees for the proposed method, it seems that these guarantees can be directly derived from known results on the convergence of cubic regularization methods. At the same time, because the problem considered is bilevel, some assumption seems less natural. For example, the Lipschitz-Hessian assumption on $J(\theta)$ is quite difficult to verify in practice, since the expression of $J$ itself contains a minimization problem. I wonder if it is possible to obtain convergence results under more 'fundamental' assumptions.

Limitations:
The authors have adequately addressed the limitations and potential societal impact of their work.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper proposes a new optimization based technique for hyper-parameter tunings using Adaptive Cubic Regularized Newton method (ARC) based on stochastic relaxation. The author highlights the limitations of the existing hyper-parameter optimization algorithms.They show that their suggested method achieves better convergence guarantees as existing methods only prove convergence to first order methods where their work also achieves second order optimality conditions. The authors also run some numerical experiments to show that the algorithm outperforms existing methods in terms of faster convergence. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* A weakness of existing methods is clearly identified and solved.
* The originality lies in being the first to use the second order method (ARC) in a bi-level optimization problem for hyper-parameter tuning.
* The paper is well written and the result can serve as the base of future work in  using second order methods for hyper-parameter tunings.
* Some minimal numerical results are shown


Weaknesses:
* The major contribution of this work should be explicitly stated in a section called “Our contributions”.
* The authors use ' the curse of dimensionality' as an example of a disadvantage for existing methods; however, when explaining their proposed approach and arguing that even they have to compute the full Hessian, this will not be an expensive operation since hyper-parameters are often low dimensional. This contradicts with the disadvantage mentioned earlier.
* The authors didn’t compare their results against the global optimal (if exists) set of hyper-parameters in their numerical experiments. 
* In Algorithm.1, the authors didn’t explain how to optimize the lower-level objective in (1)
* In Algorithm.1, the authors didn’t explain if ARC is being solved using an iterative approach or a factorization based approach when solving the subproblems.
* In their analysis, the authors are not taking into account the cost of solving the ARC subproblems.
* For the synthetic data experiments, the authors didn’t use any reference and they didn’t explain why they didn't use real data for the problem of filtering useful features.
* The details of the experiment parts are not enough so that people can reproduce the results. For example, the authors don't state what machine learning models they used.
* The proof is a bit tricky and it has some typos and errors.

Limitations:
Exposed by the authors.

Rating:
6

Confidence:
4

";1
XKeSauhUdJ;"REVIEW 
Summary:
This paper describes an application of diffusion models (Sohl- Dickstein et al., 2015; Ho et al., 2020) to text editing. Methodologically, this work differs from previous text diffusion (Li et al., 2022) by leveraging insights on glyph encoder and OCR detector. Empirically, this work advances the state of the art for text editing by scaling these methods to larger datasets. The paper also proposes to use self-supervised training to train the diffusion model and further explore diffusion guidance.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This is the work on diffusion LMs that shows results on a text editing baseline and seems to have positive results in terms of the metrics used.
2. The writing is clear and the motivations seem sound.

Weaknesses:
1. The main weakness is the novelty. The core idea of this paper, i.e. latent diffusion, has been demonstrated to be successful in many generation tasks. Thus it is not surprising that it works on scene text editing. Most of the techniques used in the paper have been proposed perviously.
2. The author did not provide any details regarding the position control module, thus the ablation study of this part is not convincing.
3. The authors did not evaluate a variety of evaluation measures that prior work has done such as SSIM, MSE, PSNR, and many more. These metrics should be computed to a get better idea of the quality and diversity of the output. Please see these this paper for the description of these metrics: “ Krishnan P, Kovvuri R, Pang G, et al. Textstylebrush: transfer of text aesthetics from a single example[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.”.
4. There are missing comparisions such as Krishnan et al 2023's TextStyleBrush[1] and Ji’s 2023’s DiffSTE [2]. 
5. The model rely on a pretrained OCR encoder which just seems like an arbitrary choice. An ablation should be provided with different pretrained encoders to understand the impact of this choice.

[1] Krishnan P, Kovvuri R, Pang G, et al. Textstylebrush: transfer of text aesthetics from a single example[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.
[2] Ji, Jiabao, et al. ""Improving Diffusion Models for Scene Text Editing with Dual Encoders."" arXiv preprint arXiv:2304.05568 (2023).

Limitations:
Beyond the weaknesses I listed, the authors were good at addressing several limitations of this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, the authors present DiffUTE, a universal self-supervised text editing diffusion model for language-guided image editing. They address the limitations of existing diffusion models by focusing on rendering accurate text and text style during image generation. DiffUTE incorporates modifications to the network structure, allowing it to handle multilingual character drawing using glyph and position information. Furthermore, a self-supervised learning framework leverages a large amount of web data to enhance the model's representation ability. The experimental results showcase the impressive performance of DiffUTE, demonstrating its ability to achieve high-fidelity and controllable editing on diverse real-world images. Overall, this paper presents a significant advancement in language-guided image editing and offers a promising approach for rendering realistic and customizable text in generated images.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The problem addressed in this paper is a realistic problem that current diffusion models struggle to handle effectively.

- The incorporation of LLM  into the inference process is a compelling and intriguing approach.

Weaknesses:
- The paper claims significantly better results than other baselines in Table 1. However, it would be helpful to clarify if there are other baselines that have not been adequately considered.

- A simple baseline is missing. Have the authors considered directly replacing the ""source text"" with the ""target text"" and calculating the FID (Fréchet Inception Distance)?

- In Table 1, the results for SD1-FT and SD2-FT appear to be poor. It would be valuable to explain the main differences between your method and these baselines.

- There is limited mention of other methods that fine-tune the encoder-decoder. How important is this step? Additionally, could you provide details on the difference in parameter numbers shown in Table 1?

- The discussion regarding self-guidance is absent, despite the proposal of a self-supervised approach for achieving text editing in diffusion. The related papers are:

- Self-Guided Diffusion Models
- Why Are Conditional Generative Models Better Than Unconditional Ones?
- Visual Chain-of-Thought Diffusion Models

Limitations:
Yes

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors propose a method of fine-tuning Stable Diffusion to modify words in images, while maintaining the original font style and the background region.
Specifically, they first fine-tune the VAE with text images from several datasets.
Then, utilizing an off-the-shelf OCR detector, they randomly mask out one text box, and tune the denoising network asking it to fill the region with original text that is given as the condition in cross-attention layers.
With this simple and intuitive method, they achieve improved performance in various evaluations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- the authors tackle a meaningful task
- the proposed method is simple and easy to reproduce
- the proposed method demonstrates improved performance on various evaluation metrics

Weaknesses:
- would only work for texts that can be detected by off-the-shelf OCR detectors
- there are missing details on some parts of the method

Limitations:
please refer to the Weaknesses section

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes DiffUTE for general text editing. 

DiffUTE utilizes Stable Diffusion model with several specific model designs, progressive training strategy, positional and glyph guidance, and a self-supervised training framework. 

Equipped with these designs, DiffUTE achieves remarkable results compared to other baselines on several public datasets. 
Moreover, the authors also provide a chat-based interface which enables an easier manipulation for the users. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality, motivation and significance:
- The paper shades an interesting perspective to edit text using pre-trained Stable Diffusion model. Two motivations raised in Line 29 and Line 32 are intuitive. 
- The interaction module is interesting and easy to use. 

Technical approach: 
- Finetuning VAE with a progressive training strategy  (PTT) with different image sizes in different stages is a good choice to overcome blurry outputs. As shown in Table 2, with PTT, DiffUTE has a noticeable gain. 
- The insight into generating fine-grained texts makes sense. With positional and glyph guidance, DiffUTE generates texts with natural shapes.
- Proposed self-supervised training strategy is straight-forward and useful. It also reduces the need of human annotations. 

Clarity: the paper offers a smooth writing and is easy to follow.  

Weaknesses:
- The motivation of using diffusion models v.s. GANs is not clearly stated. Why would the authors prefer to use diffusion model (e.g., Stable Diffusion)? 
- The paper lacks some failure case analysis. For example, DiffUTE relies on pretrained OCR detector. What if the OCR detector compromises in some cases? 

Limitations:
The authors emit some ethical discussions in the paper. For example, the authors should discuss the misusage of the technique for misinformation spread. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces DiffUTE, an innovative diffusion-based text editing framework designed to seamlessly fill in missing words in an image with user-specified text. By employing a self-supervised training framework, the model effectively learns from an extensive collection of synthetic data pairs, enabling it to infer accurate text styles and generate images that seamlessly incorporate the desired text. Experimental results showcase remarkable qualitative text editing performance from both the model's precision in both text and style accuracy. Additionally, quantitative analysis shows that the proposed method surpasses the performance of baseline approaches.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
+ The proposed method exhibits impressive editing performance, as demonstrated through extensive experiments. It displays a strong ability to accurately infer text styles and generate corresponding images.

+ Leveraging LLM, the model offers broad applicability across many possible application scenarios.

+ The paper is organized clearly and is easy to read.

Weaknesses:
- Quantitative metrics for style: Although the paper effectively showcases the model's ability to generate text that is stylistically consistent with the rest of the images, it does not provide a quantitative analysis or specific metrics to support this claim.
- Alternative diffusion-based baselines: While ControlNet is a powerful diffusion-based editing framework, it is not specifically designed for text editing tasks. Another recent diffusion-based editing approach, DiffSTE[1], shares similarities with this work as it focuses on specialized text editing and exhibits commendable performance. How does the proposed method compare to DiffSTE in terms of performance?

[1] Improving Diffusion Models for Scene Text Editing with Dual Encoders.

Limitations:
This paper clearly discusses the limitations.

Rating:
6

Confidence:
4

";1
CXPUg86A1D;"REVIEW 
Summary:
This paper introduces the Semantic Pyramid AutoEncoder (SPAE) for enabling frozen Large Language Models (LLMs) to perform understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts raw pixels to lexical tokens extracted from the LLM's vocabulary, allowing the LLM to perform multimodal tasks. The proposed method is evaluated using in-context learning experiments with frozen PaLM 2 and GPT-3.5, demonstrating success in image understanding and generation tasks. The main contributions include the first successful method using a frozen language model for image content generation, a new SPAE tokenizer producing interpretable representations, a new progressive prompting method for long cross-modal sequences, and evaluation on visual understanding and generation tasks.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
1. Compared to LQAE, the method presented in this paper demonstrates superior performance in image reconstruction.

2. By interacting with large language models such as GPT-3.5 and PaLM-2, this paper effectively utilizes the in-context learning capabilities of these models to generate corresponding images, offering an intriguing research direction.

Weaknesses:
1. This paper's experiments are limited to 128x128 resolution, significantly lower than the mainstream 256x256 clarity.

2. To achieve comparable performance with VQGAN, a larger latent space (# tokens) is necessary.

3. Is it possible to attain the same performance as VQGAN + frozen codebook + semantic guidance and an expanded latent space (# tokens)? The need for a pyramid structure comes into play here. Whether the pyramid structure is necessary requires experimental support.

4. The VAE proposed in this paper employs perceptual loss and utilizes a VGG network with supervised pre-training on ImageNet during the training process. In contrast, other methods like LQAE do not use perceptual loss. A strategy similar to ViT-VQGAN should be adopted here, using a version without perceptual loss for training on the comprehension task.

5. There is a lack of model variants in the ablation experiments on the understanding tasks.

6. In lines 203-204, it is mentioned that ""We train with a batch size of 256 for 450k steps, which takes 1.4k TPUv3-hours."" This training cost is larger than that of both VQGAN and RQ-VAE. It would be interesting to investigate the impact of the training cost on the performance of the VAE proposed in this paper.

Limitations:
addressed

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes Semantic Pyramid AutoEncoder (SPAE) which enables frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos through in-context learning. The authors first introduce SPAE, a pyramid tokenizer that produces interpretable representations of semantic concepts and fine-grained details. In addition, the authors also propose a new progressive prompting method that facilitates the in-context generation of long cross-modal sequences. The proposed method outperforms SoTA in few-shot image classification accuracy by a large margin.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed approach does not require further tuning of LLM. It is thus very portable.
- The paper is well-written and the approach, motivations, and experiments are well-derived and clear.
- The paper presents an interesting algorithm to leverage LLMs to address non-linguistic tasks, such as how to combine pretrained models and how well it would perform, even without tuning LLM and its codebook.
- Impressive few-shot results on image classification.


Weaknesses:
Although this paper has shown how a frozen LLM can generate image content or realize visual understanding tasks. The tasks the authors choose to evaluate in this paper are still limited (only few-shot image classification on ImageNet). 
Can the authors provide more quantitative experiments to validate the SPAE’s generalization to various tasks (e.g., caption, VQA…)? Besides, there are only qualitative examples of image-to-image generation, which only prove that SPAE “can” generate image content, not that it can generate well.

Limitations:
The authors have described the limitations of this work. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a novel multi-modal generation method named SPAE which leverages semantic autoencoder and in-context denoising for semantic reconstruction. The SPAE converts raw pixels into interpretable lexical tokens from the LLM's vocabulary, capturing both semantic and fine-grained visual details. The paper conducts in-context learning experiments using frozen PaLM-2 and GPT 3.5 on a diverse set of image understanding and generation tasks for showing the effectiveness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
● Semantic Pyramid AutoEncoder (SPAE) is introduced for enabling frozen LLM to perform both understanding and generation tasks involving non-linguistic modalities, such as images or videos.
● Different from previous work, SPAE produces interpretable representations of semantic concepts and fine-grained details in the form of multilingual linguistic tokens by leveraging frozen LLM.
● The proposed method outperforms the best-published few-shot image classification accuracy by an absolute 25% under the same in-context setting.

Weaknesses:
1. The 5-shot evaluation on ImageNet classification is somewhat unfair. It is not clear about the performance with GPT-3.5 under this setting. The paper only reports the results of PaLM-2.
2. The hyperparameters in the paper are quite tricky (e.g. threshold for each layer, the weight of the loss).  Besides, the effect of dynamic weight used in the final objective is not ablated. 
3. Since SPAE is only trained on ImageNet, it is not unknown whether this method scales well on large-scale datasets (e.g. Laion-400M, Laion-2B, COYO-700M, etc).

Limitations:
The paper demonstrates the generation under the low-resolution setting. The higher resolution (e.g. 1024x1024, 2048x2048) remains unexplored. Can you provide some comparisons with diffusion, such as high resolution and complex scenes?

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces SPAE, a method that aligns visual representation with a fixed LLM representation. SPAE effectively captures semantics and visual fine-grained textures for a range of cross-modality tasks. The paper showcases good and solid results in few-shot learning and reconstruction/generation tasks.






Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well written and well motivated.
- The proposed idea is interesting, cross-modality with codebook is not a novel idea, but using the fixed pre-trained LLM as dictionary is an interesting idea and seems working well.
- The results are solid and convincing.

Weaknesses:
- The quantitive comparisons are not fair comparisons; as the language model sued in SPAE is much bigger and trained on much more data than the baselines. One interesting baseline is to replace the heavy LLM to smaller LMs and discuss the impact.
- Lack of comparisons to other works on generation tasks. It would be good to include comparison to other generative models with same prompt.

Limitations:
None

Rating:
6

Confidence:
3

";1
cRzt1umRNx;"REVIEW 
Summary:
EDIT: Having read everything here. I am increasing my score slightly. However, I still think the paper is not clearly explained. It is unclear why you first introduce the method without the feature map. It seems like there are two different versions of the method and it is not clear which is which. I am also confused by the experimental set up for datasets such as CORA. Do you first embed the graphs into a manifold? 

Additionally, I think that requiring a closed form expression for geodesics is somewhat strong and limits the applicability.


The authors claim to attempt RESNET structures to hyperbolic manifolds

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper is applicable to a wide variety of manifolds

Weaknesses:
The proposed methods seem to not really leverage the manifolds intrinsice geometry and depend entirely on the embedding on the manifold into ambient space $R^D$. 

Indeed $n_i$ is defined on all of $R^D$. This means that there is no gauranteed that there would be any notion of consistence if $M$ was embedded into $R^D$ and $R^{D'}$ two different ways (where $D'$ may or may not equal $D$). This seems to be a major limitation of this that is not properly discussed. at a bare minumum, there should be some level of invariance to, e.g., Rotations and Translations of the manifold in $R^D$ after its been embedded. 

Additionally, much of the paper is hard to understand such as the construction of the feature maps, which appears to take place in local coordinate systems which will not be consistent across the manifold.

Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper extends the well known residual network which are usually applied to Euclidean data to a variant defined on manifold. The main novelty is to replace the ""addition / plus"" operation in Euclidean space to exponential operation. Specifically, given an input point on a certain manifold (e.g., hyperbolic or SPD), it first learns a vector in the tangent space of the given input point through a neural network layer, and then maps the learned vector back to the manifold using exponential. By utilizing pushforward and pullback operations, the proposed method can transform input between different manifolds with different dimensions. Experiments are conducted on hyperbolic and SPD spaces to demonstrate the superior performance of the proposed method over HNN and SPDNet.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper proposed to extend residual network from Euclidean space to non-linear manifold by replacing the conventional addition/plus operation with manifold exponential operation. The theoretical part is sound and the experiments are effective in supporting the proposed method.

Weaknesses:
The biggest weakness, as also mentioned in by the authors at the end of the paper, is whether the proposed method is only applicable to hyperbolic and SPD matrix? Is it possible to apply this residual network to other non-linear manifolds that have closed-form exponential manifolds? Furthermore, Is it possible to apply this residual network to other non-linear manifolds that do $\textbf{not}$ have closed-form exponential manifolds? If so, please list such manifolds.

Limitations:
The authors mentioned the limitations of the proposed method in Line 357 - 359.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper generalizes the ResNet layer to non-euclidean geometries by replacing the Euclidean sum with the exponential map. The theory is general and applies to any smooth metric. They propose a way to parametrize a vector field on the manifold which is more geometrically principled than the trivial vector field embedding. Empirically they show improvement in performance on hyperbolic datasets and PSD spaces compared to some baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper theory is general and applies to any smooth manifold metric. In ResNet a neural network produces a vector field and the output of the ResNet is the input plus such vector field. The proposed generalization, assuming access to a vector field on the manifold, defines the output of the ResNet as the exponential map of the input in the direction of such a vector field. This is consistent with the Euclidean case.

A vector field in Euclidean space can simply be the output of a neural network, while on a manifold more care is required. Using an embedded vector field is straightforward, but, as correctly pointed out by the authors, it is not very principled geometrically. The authors propose a computationally tractable and geometrically principled way of defining a parametric vector field on a manifold. The idea is to make use of a collection of $\mathbb{R}$ valued functions (obtained as a projection on hyperplanes or similar) to define, through push forward and pullback, a vector field on the manifold.

Extensive specific examples of such a collection of functions are given for the hyperbolic and PSD manifolds. Experiments are also performed in these cases, and the proposed approach appears to outperform the current state of the art.

In the general manifold case (Appendix B.5), the idea of projection on pseudo-hyperplane is appealing and well-argued. And the further generalization to “hyper-disks” allows proper formal extension also to the non-geodetically-complete manifolds.

Weaknesses:
The definition of the vector field (Appendix B.4) is not sufficiently formal and contains mistakes. Specifically, the author assumes access to a smooth function $f:M\rightarrow \mathbb{R}^k$, a so-called “feature map”. The differential $D_x f$ is then a linear map from the tangent space in $x$, $T_x M$ to the tangent space in $f(x)$, $T_{f(x)} R^k = R^k$. Observing that the dual of $R^k$ is isomorphic to $R^k$ itself, the pullback of the differential $(D_x f)^*$ can be seen as a map from $R^k$ to $(T_x M)^*$, for every $x\in M$. This function is evaluated in $f(x)\in R^k$ such that the vector field (as defined in line 215) is a map
$$ l_f: x \rightarrow (D_x f)^*(f(x)) $$ which, as we saw, take values in $(T_x M)^*$ and NOT on $T_x M$, as line 215 is saying.

We would like to see this inconsistency explained. Is this based on the observation that both $(T_x M)^*$ and $T_x M$ are isomorphic to $\mathbb{R}^{dim(M)}$?

And also, can you reason about the choice of evaluating the pullback of the differential (line 215) in $f(x)$? Is this somehow principled?

The equivalence with Euclidean ResNet shown in Appendix D is a proper extension in the case of an embedded vector field, but it is rather difficult to follow in the case of the feature map. Specifically, Proposition 1 is trivially proved for an embedded vector field, but the same argument should also apply to the case of a feature-map-induced vector field. It would be helpful if the discussion on $g_{w,b}$ could e.g. focus on the case of axis-aligned planes (i.e. each $w$ should be an element of a standard basis and $b = 0$), such that the differential $D_x f$ reduces to an identity. We found this to be significantly more intuitive.

In the general manifold case (Appendix B.5), the idea of projection on pseudo-hyperplane is, although well explained, not at all investigated. First, it is not clear how to practically implement such projections of these pseudo-hyperplanes (not in the geodesically complete case, and even worse in the general case). Second, there are no experiments regarding general manifolds, and there is also no dissertation on the increased computational complexity with respect to hyperbolic and PSD cases. This reduces “contribution 3” in the statement in lines 72-76.

*Minor:*
* The related work section argues that the proposed construction is different from a neural ODE (which also generalizes ResNets), but honestly, we found this argument to be incomplete. It does seem like the proposed construction is a neural ODE.
* Line 126, we found it unclear which ""specific structure"" is being exploited in methods using Frechet means (these averages apply on practically all manifolds).

*Regarding the score:*
We are willing to increase the score if these concerns are appropriately discussed in the rebuttal.

Limitations:
The parting ""Limitations"" paragraph discusses a relevant assumption. However, we feel that a discussion of the tractability of (projections onto) ""pseudo-hyperplanes"" (Eq. 17 in the appendix) is lacking.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes an extension of standard ResNets called Riemannian Residual Neural Networks. The extension is done based on Riemannian manifolds as discussed in Equation (2). Some numerical results on node classification problems are presented in section 5 to show the improvements of the proposed generalization of ResNets.

-- Post-rebuttal Review Update --
 
I thank the authors for the detailed responses to my comments. I find the responses satisfactory and raise my score to 6.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1- The idea of Riemannian ResNets sounds interesting and as the numerical results suggest could help improve the performance of ResNet models in applications where the chosen Riemannian geometry suits the dataset.

Weaknesses:
1- The paper's presentation remains abstract in the main body, and I do not find the current presentation accessible enough to deep learning practitioners. For example, Section 3 spends about 2.5 pages explaining Riemannian geometry but does not discuss a concrete example where the exponential map and vector fields can be discussed. The examples in section 4.2.1 appear late in the draft and also do not derive the expression for the exponential map that appears in Riemannian ResNets.

2- Since the paper has not discussed the algorithmic steps of training and evaluating a Riemannian ResNet, it is not that easy to see how the network can be trained for non-Euclidean Riemannian geometries. I suggest adding one or two algorithms to the draft to discuss the steps of training a Riemannian ResNet for the cases discussed in Section 4.2.1.

Limitations:
Please see my previous responses.

Rating:
6

Confidence:
2

";1
jOuPR9IH00;"REVIEW 
Summary:
This paper considers variance-weighted least-squared regression for offline RL with general function approximation. Under a uniform data coverage assumption, they show that the proposed algorithm obtains a sub-optimality bound that scales with the $D^2$-divergence of the offline data set, the positive lower-bounded constant of the uniform data coverage, and the complexity of the function class. Their bound obtains the right order when realized in the linear case. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- clear presentation (though some parts can be improved further -- see Weaknesses)
- the obtained result is new and relevant to the offline RL community 


Weaknesses:
- The main weakness is that the uniform data coverage assumption is very strong. In the linear case, this assumption is equivalent to that the behavior policy is exploratory overall dimensions of the linear feature. A question for the authors is that in such a case, why would we even need pessimism?  Pessimism is used when the data coverage is partial thus we become pessimistic about uncertain actions. But when the coverage is uniform, it can eliminate the need for pessimism and we can simply use greedy algorithms. I understand that without such a uniform data coverage assumption, it seems difficult to get a reliable estimation of the variance of the transition kernel and it would be interesting to get rid of this assumption. But if we could not get rid of it yet, the very least expectation is that we need to explain this assumption further, especially regarding where pessimism is really needed with this assumption. 


- Writing can be improved further. For example, the $D^2$-divergence and the definition of the bonus function (Def 4) can be explained and motivated further. The current presentation of these concepts are not very helpful 

- Some claims might be potentially misleading. It's not comfortable to view the proposed algorithm as computationally efficient even in the oracle sense. Specifically, the construction of the bonus function in Definition 4.1 is far from being computationally efficient since it is essentially a constrained optimization over the version space. That said, it is nowhere more computationally efficient than version-space-based algorithms such as the ""Bellman-consistent pessimism"" of Xie et al. 

- Though the main result is new, it appears expected given the already-developed machinery in Argawal et al. 2022 and Xiong et al. 2022. What are the technical challenges in the current problem that the existing techniques cannot resolve? 

- Some minor: PNLSVI is never introduced before used 

Limitations:
Yes 

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper studies offline RL with non-linear function approximation. The paper is mainly motivated as existing sample complexity guarantees on offline RL algorithms with general function approximation yield suboptimal dependency on the function class complexity, e.g. when the bounds are translated to the linear case. The paper proposes an oracle-efficient algorithm that achieves minimax optimal problem-dependent regret when the bounds are specialized to the linear case. The paper also introduces a new coverage definition.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper appears to be technically sound with some new ideas in the algorithm design and formulation of dataset coverage.
- The approach achieve minimax optimal rate in non-linear function approximation, when bounds are converted to linear.

Weaknesses:
- The main weakness is that the proposed approach either requires uniform coverage or non-linear bonus oracle. The non-linear bonus oracle is a strong requirement and in effect, simply removes the difficulties related to pessimism in offline RL. On the other hand, the uniform coverage assumption is too strong and thus, it is unfair to compare its efficiency to pessimistic offline RL algorithms.
- A clear comparison to prior work is not presented. In particular, there are multiple axes of comparison, such as dependency on $\epsilon$, dependency on function classes, data coverage requirement, type of oracle, computational efficiency/tractability, realizability assumptions, etc. It is difficult to clearly evaluate the results in this paper without such comparisons. For instance, it will be helpful to have a table as well as translating the bounds of the other algorithms into linear case to see in detail. Additionally, there are several pessimistic offline RL algorithms with general function approximation that only require optimization oracles instead of the more difficult bonus oracle, and no comparison with those papers are presented:

Cheng et al. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR

Rashidinejad et al. ""Optimal conservative offline rl with general function approximation via augmented lagrangian."" arXiv preprint arXiv:2211.00716 (2022).

Ozdaglar et al. Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation. arXiv preprint arXiv:2212.13861

Zhu et al. Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a pessimistic nonlinear least-squares value iteration algorithm to tackle the offline reinforcement learning problem. The main motivation of the paper is to propose an algorithm that are both computationally efficient and minimax optimal w.r.t. the complexity of nonlinear function class. The proposed pessimism-based algorithm strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation and is oracle efficient. Also, the proposed algorithm is proven to be optimal w.r.t. the function class complexity, closing the gap originated from the previous work on differentiable function approximation.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
1) The proposed algorithm is proven to be optimal w.r.t. the complexity of nonlinear function class, closing the gap from the previous work on the differentiable function class and generalizes it to the wider nonlinear function class.
 2) The proposed algorithm is computationally efficient if there exist the efficient oracles for both regression minimization and bonus function optimization/searching.

Weaknesses:
1) The paper's presentation needs some work. For example, the terminology definition is not consistent. The D^2 divergence definition in Definition 3.2 is not consistent with the later terminology of D_F in line 239. The language itself needs some work too. For example, lots of places where it needs 'an', but 'a' is used and vice versa. Please define RL before using it in the abstract. There are also some ambiguities in the definitions that needs clarification in the Question section. 
2) The paper's claimed contribution is a bit exaggerated. Although the proposed algorithm does not need the computationally heavy optimization as previous works in planning phase, it transfers the main computation burden to the Oracle to find the satisfied bonus function, which seems to be a very time-consuming task. It also applies to the claim of being the first statistically optimal algorithm for nonlinear offline RL. Being able to get optimal result in the reduced linear function class does not necessarily mean it's optimal in the broader nonlinear class.
3) Although the considered class is the nonlinear one and general than the previously considered linear or differentiable class, the techniques used in the analysis are nothing new in my opinion, except re-defining the metrics in the nonlinear function class and connect the results together along with additional assumptions.
Overall, I think the paper is well motivated, but given the presentation and the insignificant contribution, it's not ready to be published. 

Limitations:
N/A

Rating:
3

Confidence:
3

";0
yloVae273c;"REVIEW 
Summary:
This paper studies offline reinforcement learning (RL) with linear function approximation and partial data coverage. The authors propose a primal-dual optimization method based on the linear programming (LP) formulation of RL. They prove a $O(\epsilon^{-4})$ sample complexity in both discounted setting and average-reward setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The algorithm proposed in this paper only requires near-minimal dataset coverage assumption, which is important in offline RL.
2.	The paper also considers average-reward offline RL, which is often neglected by literature.
3.	I like the table for comparison to previous work, which makes the presentation more clear (although I think there is some missing important literature, which I will mention in the weakness section).
4.	The proposed algorithm is both computationally and sample efficient.

Weaknesses:
1.	The first concern is the ‘linear function approximation’ setting, which is restricted. Actually, the main motivation that this paper studies function approximation beyond tabular settings is large state (or action) spaces in practice. However, in real settings, the linear function approximation assumption hardly ever holds. Even in Table 1, many algorithms in previous work apply to general function approximation, which further makes the setting studied in this paper restricted.
2.	Algorithm 1 in this paper achieves a $O(\epsilon^{-4})$ sample complexity. This is in terms of expectation (as shown in Theorem 3.2) instead of high probability. The previous results that the authors are comparing to are high probability bound (e.g., [1,2]), so it would be more comparable if the authors could also show a $O(\epsilon^{-4})$ sample complexity bound under high probability. Also, since the previous work studies general function approximation while this paper studies only linear function approximation, it is hard to say that a $O(\epsilon^{-4})$ sample complexity bound in linear function approximation setting is better than a $O(\epsilon^{-5})$ bound in general function approximation setting. Moreover, [3] achieves the near-optimal sample complexity $O(\epsilon^{-2})$ with near-identical settings of [1,2]. (So I disagree with the statement that ‘It is very important to notice that no practical algorithm for this setting so far, including ours, can match the minimax optimal sample complexity rate of $O(\epsilon^{-2})$’. Therefore, a $O(\epsilon^{-4})$ in linear function approximation is not that attractive compared to previous work.
3.	The authors use an LP formulation of offline RL. I think it would be better to compare to other work using LP formulation, e.g. [4,5], where [4] is computational and sample efficient under partial data coverage and general function approximation, and [5] achieves near-optimal sample complexity under similar settings.
4.	The authors compare the computational complexity. However, it is not that direct to compare an $O(n)$ complexity in linear settings to a $O(n^{7/5})$ complexity in general settings. If the authors really want to demonstrate that their algorithm has better computational complexity, it would be better to do some simulations in the same environment (even in some toy examples).
5.	Another advantage that the authors claim is that their algorithm could be adapted to average-reward setting. However, neither did the authors emphasize and explain the importance and challenges of average-reward settings, nor discuss why (or whether) previous work could not be adapted to average-reward settings. I suggest the authors discuss this a bit more.

**References**

[1] Xie, T., Cheng, C. A., Jiang, N., Mineiro, P., & Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34, 6683-6694.

[2] Cheng, C. A., Xie, T., Jiang, N., & Agarwal, A. (2022, June). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR.

[3] Zhu, H., Rashidinejad, P., & Jiao, J. (2023). Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

[4] Zhan, W., Huang, B., Huang, A., Jiang, N., & Lee, J. (2022, June). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory (pp. 2730-2775). PMLR.

[5] Rashidinejad, P., Zhu, H., Yang, K., Russell, S., & Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studied offline RL in linear MDP setting, where the transition and reward have low-rank structures and the feature $\phi$ is known. The authors formulated the problem in a primal-dual way and proposed a gradient-based algorithm. They provided convergence guarantees, which only requires coverage over optimal policy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper writing is clear and easy to follow.

The discussion and comparison with previous works is very detailed.

The algorithm is computationally efficient. The algorithm design has some interesting points, especially the reparameterization design to avoid knowlegde of $\Lambda^{-1}$ and updates for variables $v$ and $u$.

The coverage assumption seems weaker than previous literatures.

Weaknesses:
I didn't see too much technical novelty in the method and proof.

The setting is linear MDP, which is kind of restrictive.

Convergance rate is kind of far away from optimal.

Limitations:
N.A.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed an primal-dual framework for offline reinforcement learning in linear MDP Contrary to the more common case of finite horizon, they considered the case of infinite horizon with discounted reward. They reduced the problem of offline reinforcement learning to a problem about solving the saddle-point of a Lagrange form. They designed an algorithm which uses stochastic gradient-based optimization to solve the saddle point. They provide a sample complexity of O(\eps^-4) for both cases of discounted MDP and averaged-reward MDP, and their algorithm is also computational efficient. 

To summarize, the formulation of offline RL into a linear programming problem is very interesting. The proof seems very solid, and I like the comparison for the concentrability constant in the last discussion section. The comparison for the constant C is thorough and very good. 

However, I still have some questions about some details in the main text.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The formulation of offline reinforcement learning to a linear programming problem is very good.
2. The algorithm is clearly motivated by solving the saddle points of a Lagrange form. The algorithm itself is simple and computationally efficient, with a guaranteed sample complexity for both discounted MDP and averaged-reward MDP.
3. They proposed a new concentrability constant C and compare it to other constants appearing in other literatures about offline RL. I think the understanding of the relationship of these concentrability constant is basically correct and very clearly expressed.
4. The proof seems very solid and the result in averaged-reward case is new.

Weaknesses:
1. I have some question about your comparison to previous results. Your main references are Cheng et al and Xie et al. 

1.1 For Xie et al, the Theorem 3.2 in https://proceedings.neurips.cc/paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf implies that their sample complexity is O(1/\eps^2) when applied in linear function approximation. This result is based on assumption3 in their paper. This assumption naturally holds in your paper since you consider linear MDP and they consider the case of 'linear function approximation' (for their difference, see point 2). So it is natural for you to compare your sample complexity to this result, not the O(1/\eps^5) one. [notice that, their algorithm in section 3 is computationally inefficient]

1.2 In Theorem 4.1 in Xie's paper, their sample complexity is O(1/\eps^5) when applied on general function approximation, and O(1/\eps^3) when reduced to linear function approximation case (see paragraph 'Dependence oon T'). Again, their assumption for linear function approximation holds in your case. **This algorithm, however, is computationally efficient.** So you should also compare with this alg with  O(1/\eps^3) sample complexity.

1.3 In Cheng's paper, in theorem 5, their sample complexity seems to be O(1/\eps^3), not O(1/\eps^5). I wonder how you derive their sample complexity in Table one.

1.4 I am not sure how you get the O(n^{7/5}) computational complexity for Xie's paper. Could you derive it in more detail?

2. I think in many places in your paper, you confuse the two terms: linear MDP and linear function approximation. Your case is called linear MDP instead of linear function approximation, so I suggest you changing the wrong terms. For reference, both Xie's paper and Cheng's paper consider the 'linear function approximation' case.

Limitations:
/

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies offline reinforcement learning with linear function approximation. They propose a primal-dual algorithm, formulating linear RL into a minimax problem and solving it with gradient descent-ascent. Sample complexity analysis is provided for infinite-horizon discounted and average-reward MDPs, where the rate is $O(\frac{1}{\epsilon^4})$ for both settings.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is primal-dual and thus easy to implement in practice.
2. The paper provides rigid theoretical analysis.

Weaknesses:
1.  The newly defined coverage ratio $C_{\phi,c}$ is a little strange when $c\neq \frac{1}{2}$. For example, when we choose $c=1$ and thus we don't need the knowledge of $\Lambda$, the coverage ratio $C_{\phi,1}=\sum_{x,a}(\frac{\mu^*(x,a)}{\mu_B(x,a)})^2$. Then when $\mu^*=\mu_B$, $C_{\phi,1}$ will become $|X||A|$. However, in the literature, when the behavior policy is the same as the optimal policy, the coverage is typically 1. The authors claim that we can estimate the $\Lambda$ via the offline dataset so that we can choose $c=\frac{1}{2}$, but do not provide any theoretical analysis about this point. I will be more convinced if the authors can give more rigid proofs for this method.
2. The sample complexity is worse than the typical rate $\frac{1}{\epsilon^2}$.

Limitations:
None.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of offline reinforcement learning (RL) for linear Markov Decision Processes (MDPs) under the infinite-horizon discounted and average-reward settings. The authors propose a primal-dual optimization method based on the linear programming formulation of RL, which allows for efficient learning of near-optimal policies from a fixed dataset of transitions under partial coverage. The proposed algorithms improve the sample complexity compared to previous methods from $O(\epsilon^{-5})$ to $O(\epsilon^{-4})$ under the discounted setting and provide the first line of result in the average-reward setting with realizable linear function approximation and partial coverage.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 The proposed algorithm improves existing algorithms in both statistical efficiency and computational efficiency under the discounted reward setting with the linear function approximation (we note the baseline may handle problems beyond the linear MDPs).

2 The algorithms presented in this paper do not explicitly leverage the principle of pessimism, but focus on the linear programming formulation of MDP, and rely on a new reparametrization trick extended from the tabular case. The technique itself seems to be novel to me.

3 The algorithms present the first line of work for the offline average-reward MDP.  

4 The paper is easy to follow, with a thorough comparison with existing work that clearly positions the results in the literature.

Weaknesses:
1 I am confused about the requirement of $\Lambda$ to be invertible (line 140) as this seems to be very closely related to the uniform coverage condition where we assume that the smallest eigenvalue of $\Lambda$ is lower bounded from zero. I am wondering what is the key difference between them. Can you elaborate on this with some intuitions or examples?

2 The authors discuss the relationship between the coverage condition considered in this paper and that of [1] and show that the coverage condition is a low-variance version of the standard feature coverage ratio if $c=1/2$. However, in this case, the algorithm explicitly uses $\Lambda$, while the PEVI proposed in [1] does not. In contrast, $c=1$ leads to a worse bound but we do not need the knowledge of $\Lambda$. Could you provide a more detailed characterization or example to illustrate the difference between these two cases?


typo: line 328, $\epsilon^2 \to \epsilon^{-2}$

[1] is pessimism provably efficient for offline rl

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors investigate offline RL in linear MDPs and introduce a novel LP-based method. They assert that their proposed approach achieves the lowest sample complexity of $O(1/\epsilon^4)$ among computationally efficient algorithms. In comparison, existing computationally efficient algorithms can achieve $O(1/\epsilon^5)$. Additionally, the author's theory can be extended to the average reward setting.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* To the best of the author’s knowledge, in offline linear MDPs, the result in the average-reward setting is novel. 
* The LP formulation in linear MDPs is worthwhile to investigate 


Weaknesses:
* I am uncertain about whether it is appropriate to claim that existing offline RL algorithms in linear MDPs achieve $O(1/\epsilon^5)$. It appears that [38] may have better sample complexity. In Table 1 of the manuscript, the author mentions that [38] cannot handle the discounted setting. However, extending from the finite-horizon to the discounted infinite-horizon setting is relatively straightforward. Hence, this comparison may not be entirely fair. If [38] indeed has better sample complexity, it significantly impacts the author's contribution. Thus, I currently rate the paper with a score of 4.

* I am not entirely certain about the significance of the extension to the average reward case.

* Presently, I cannot determine whether the reason [9] and [36] cannot handle the average reward case is due to the algorithms or their analysis. If this limitation arises from their analysis, their algorithm has the potential to be superior as it can handle more general MDPs.

Limitations:
They discussed. 

Rating:
4

Confidence:
2

";0
gx20B4ItIw;"REVIEW 
Summary:
This work investigates the emergent communication framework for reasoning rules. That is, unlike prior studies that focus on communication about perceived low-level contexts, this paper proposes a cognition-oriented environment to encourage agents to reason and communicate about high level-rules. To this end, it introduces a new interesting and unbiased benchmark, rule-RAVEN. This benchmark, as opposed to the original one (I-RAVEN) avoids overfitting and pushes the agents to have an actual communication protocol.

The authors show, with different experiments, that agents are able to succeed in the reasoning tasks and develop a compositional and semantically stable language.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The authors introduce a well-thought benchmark that could be beneficial for future works to analyze the content of emergent languages. This benchmark, which is a modification of I-RAVEN, forces agents to develop an actual communication protocol. They perform the needed ablation to show its benefit compared to I-RAVEN.

Furthermore, this paper is well-written, and a detailed description of the setting, and hyper-parameters are provided (on top of the code).

Weaknesses:
The main weakness of this work is its motivation. As stated in the paper, the goal of the emergent communication framework is to:
- either study the origin of the human languages
and/or
- develop intelligent communicating artificial agents

It is unclear what this work's position is. If the former, is there a theory that our language emerged to communicate about a high-level reasoning task? If so, can this line of work be clarified in the paper? If the goal is to develop communicating agents, communicating about visual inputs is more practical for human-agent interactions.

Limitations:
.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper takes the ever-popular Lewis signalling game for emergent
communication and studies experiments on rule-focused communication as opposed
to the perception-focused communication of prior work.  In particular, it uses
a modified version of Raven's progressive matrices to formulate a signalling
game directly on attribute-value vectors which requires pattern
recognition/reasoning to complete.  The authors find that agents can learn to
communicate when using a two-stage curriculum which essentially pretrains the
speaker for more stable communication at the beginning of the communication
stage.  The experiments demonstrate that the resulting emergent language
correlates better with the underlying rules of observations rather than the
individual observations themselves.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
## Originality
- `[major]` Addresses the signalling game from a new perspective, i.e.,
  reasoning instead of perception.
- `[minor]` Introduces a new dataset.
## Quality
- `[major]` Presents good variety of empirical evaluation with clear results
- `[minor]` Presents different levels and senses of ""generalization"".
## Clarity
- `[minor]` Details for implementation are presented without being overwhelming
## Significance
*See Originality.*


Weaknesses:
The paper is relatively complete, but what keeps my rating from being higher is
that there is relatively sparse comparison with prior work.  Such comparison
would better contextualize the results and increase its significance.  For
further details, see the *Questions* section of the review.


Limitations:
N/A.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposed a new environment along with the training framework for emergent communication of abstract rules. They designed a context generation pipeline rule-RAVEN to avoid overfitting and a two-stage curriculum training method for more stable convergence. They evaluated the emerged language from the perspectives of generalization and transfer learning.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper proposed a new research angle of abstract rule reasoning for emergent communication. The context requires the agent to go beyond the low-level perceptual features and communicate more abstract rules. 
2. The candidate pool is smartly designed to motivate agents to extract rules from the context.
3. A suite of comprehensive evaluations is designed to measure the generalization of the emerged languages. 


Weaknesses:
1. Structural requirements of the new benchmark may need to be further explained: I am not clear about why the rules must be unambiguous. From my understanding, though the multiple rules can be applied to the current context, as long as the agents can communicate either of the rules, the receiver should capture the correct candidate? Though Figure 4 demonstrates the receiver can select the candidate correctly without the sender’s message trained with rule-RAVEN, further experiments/explanations are still needed to show that:

    a. The communication training benefits from the structural or functional requirement or both.

    b. The language that emerged using the I-RAVEN dataset is not/less generalizable/compositional/transferable. 

2. Sender’s rule reasoning and perception encoding are entangled. In the first stage, both $g^S$ and $f^S$ are trained. Though the training data is not shown in the communication stage, it will still introduce structural information because of the term $\mathbb{1}(r_i, m_i)$. 

    a. does that require the length of the messages to equal the size of the rules? 


Limitations:
1. Though the goal of the task is to emerge the language for abstract rules, it will also be interesting to know whether the receiver can learn to induce rules after the communication (instead of applying rules during the communication, not required experiments). Similar to ETL, you can test the accuracy of the reasoning problem on the communicated receiver without further training.
2. As the author mentioned, the current context input is a structured symbol. It will strongly encourage compositional language. It will be interesting to know how agents can emerge languages in a raw pixel input.
3. Can the emerged languages generalize to contexts with different attributes?


Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel setting for abstract reasoning (i.e., RAVEN problems) by proposing a speaker-listener framework for communicating higher-level abstract rules. The authors propose an unbiased dataset (rule-RAVEN) to overcome overfitting in the original RAVEN-family datasets (I-RAVEN), and propose a two-stage curriculum agent training method for successful communication. Experiments have shown the efficacy of the curriculum training for solving the rule-RAVEN and out-of-distribution generalization.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- I like the idea of both: i). introducing communicative game settings to abstract reasoning tasks, and ii). see how higher-level relational abstractions (instead of low-level perceptual features) emerge in communicative games. The limited capacity communication channel formulation can lead to emergent abstractions for problem-solving, including more powerful representations for abstract reasoning and concept learning. Previous attempts in drawing are good cases but not complex enough to depict the importance of abstraction and emergent language. This preliminary trial on RAVEN tests sets a suitable problem formulation for emergent communication in abstract reasoning.

- The rule-RAVEN dataset effectively mitigates the existing bias in the I-RAVEN dataset, making the speaker-listener communication valid.

- The paper is well-written and easy to read. The flow of writing in section 5 is also appropriate for addressing potential concerns for readers.


Weaknesses:
Although I like the task settings in this paper, the experiment and proposed methods appear to have some weaknesses. I list them as follows:

- The communicative formulation is very similar to Mu & Goodman, 2021. It seems this work (communicative RAVEN) is a special case of generalization, shifting from learning object-centric, attribute-level concepts (e.g., shape red or blue) to learning relational concepts (number-increasing). Authors should address more comparisons to these existing formulations.

- The evaluation for emergent language is still quite limited. For example, can you probe the learned language to see if it can be linearly projected to some algebraic representations for relational concepts (e.g., the ""number increasing"" concept can be described as a multiplication matrix in Zhang et al., 2022) or just explicitly manipulate them and see if they have some language-like syntax or compositionality emerged.

- The use of symbolic RAVEN and two-stage curriculum training (with the first stage supervised learned) made me doubt the applicability of this communicative method to more complex or real-world tasks. For example, Mu & Goodman, 2021 used a real-world dataset, pixel input, and end-to-end training.

refs:
1. Mu, J., & Goodman, N. (2021). Emergent communication of generalizations. Advances in Neural Information Processing Systems, 34, 17994-18007.

2. Zhang, C., Xie, S., Jia, B., Wu, Y. N., Zhu, S. C., & Zhu, Y. (2022, October). Learning algebraic representation for systematic generalization in abstract reasoning. In European Conference on Computer Vision (pp. 692-709). Cham: Springer Nature Switzerland.

Limitations:
The authors did not address the limitations.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes an emergent communication game over abstract visual concepts, inspired by Raven's progressive matrices tests. The basic idea is to evaluate neural speakers and listeners on a communication game, where the speaker sees a collection of images encoding some abstract rule (e.g. ""number of objects in the image is increasing""); the speaker must then generate a message that allows a listener to complete an unseen sequence. The authors show that agents trained to play this game indeed seem to learn to communicate the abstract rules for which they are trained for, as measured by intrinsic measures of language compositionality and ease of transfer to harder tasks.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- This is an interesting dataset and interesting problem in emergent communication which may be useful to the community. It indeed explores more abstract visual concepts than in existing work (though note that novelty over the existing EC literature is overclaimed; see Weaknesses).
- Careful controls for dataset difficulty (ensuring one distinct feature that can be used to solve each task; ensuring ""hard negative"" rules) show the authors' care to making sure this is a well-constructed dataset, including an analysis of to what extent existing 
- Interesting experimental analysis shows that models seem to be (to some extent) communicating abstract rules, rather than superficial input features.

Weaknesses:
- Only a synthetic dataset consisting of clean symbolic inputs is evaluated. One could imagine more realistic settings requiring communication of rules at least over synthetic visual inputs, if not more realistic visual concepts. Similarly, there is no exploration of downstream transfer to other tasks that perhaps don't involve emergent communication, e.g. instruction following or visual reasoning. While this does not preclude publication, there's not a lot one can gain from this paper as it relates to actual realistic ML tasks. If the paper were to be rejected, IMO it would likely be because the experiments are just a little too synthetic/marginal to be useful to the broader NeurIPS community.
- The claim that existing work in EC does not at all care about expresing abstract generalizations or rules is a bit overblown. Separating inputs given to the student and teacher, so as to facilitate communication of abstract concepts, was introduced as early as Lazaridou (2017), recurs in Choi et al., Kiela et al., etc. Mu and Goodman (2022) also propose generalizations over abstract visual concepts involving multiple visual inputs, which is very similar to the task presented here. I do think the present work makes some interesting contributions over the existing literature, in that it is even more abstract, but the relation to existing work needs to be made more clear. Many of these papers are not discussed in detail and simply bucketed as  ""forcing agents to descrie low-level features of images"" (L31-32) which I believe is false. Section 2 Emergent Communication also completely neglects to discuss such efforts in the EC community.
- I think it's important for footnote 1 to be made more clear in the text, i.e. that this is not a grounded communication game over real images, despite many of the introductory figures seemingly suggesting this.

## Minor

- Title of paper and title on OpenReview do not match
- spaces between text and citations would be ideal
- It'd be interesting to see how pretraning agents on such visual reasoning communication tasks might improve performance on downstream visual reasoning tasks such as ARC (Chollet et al., ?)
- The description of the paragraphs in L119 and L128 as ""structural"" and ""functional"" requirements is a little confusing and nonstandard to me—it's not clear what structural and functional mean here. It might be appropriate for example to refer to the ""functional requirement"" as sampling ""hard negative distractors"", as is used in the terminology for contrastive learning for example. In other words, distractors should be sampled carefully so as to represent close but not quite correct rules that force the speaker and listener to communicate precisely the right rules.
- L171 ""directly from the sketch"" -> ""directly from scratch""?

Limitations:
yes

Rating:
6

Confidence:
4

";1
AWpWaub6nf;"REVIEW 
Summary:
This work establishes a general convergence analysis for a heterogeneous federated learning (FL) algorithm that trains a shared global model using a sequence of time-varying and client-dependent local models. In particular, this work establishes sufficient conditions for the convergence of such a heterogeneous FL algorithm to the neighborhood of a stationary point of the standard FL. Based on the theoretical results, the authors propose practical suggestions for designing heterogeneous FL algorithms and conduct thorough experiments to support their claims.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Overall, this paper is well-written. It is the first to provide a general convergence result to the neighborhood of a stationary point for the standard federated learning (FL) framework, specifically for a heterogeneous FL algorithm that trains a shared global model through a sequence of time-varying and client-dependent local models. This result provides a convergence guarantee for several previously proposed heterogeneous FL algorithms. The optimality gap in the result depends on two important parameters: the minimum coverage index and the model reduction noise. These parameters offer valuable insights for designing practical heterogeneous FL algorithms. The experimental results align well with the theoretical findings presented in the paper.

Weaknesses:
The last term on the right-hand side of the inequality in Theorem 1 (and similarly for Theorem 2) corresponds to the average of the norms of the global parameters encountered during the optimization process. This term is not necessarily small in a straightforward manner.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies federated learning with heterogeneous client models and non-iid client data. By assuming that the client models are pruned versions of a common global model and using the notion of minimum covering index, the paper provides the convergence of FedAvg under client model pruning. Theoretical results show that pruning techniques that more evenly update the parameters and result in smaller model distortion are preferable to aggressive pruning. Such result is also verified through the numerical results on MNIST and Cifar-10, and Cifar-100 datasets.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Novelty: this paper provides a first convergence analysis to FedAvg with model pruning. The theoretical proof is sound and clear. 

Clarity: the paper thoroughly discusses the convergence result in theorems 1 and 2 and verifies the results through numerical experiments.

Weaknesses:
1. Term $E\Vert \theta_q\Vert^2$ in the bound. This term appears in all theorems and lemmas in the main paper. However, it is unclear how large this term can be. Neither theoretical discussion nor numerical justification of this term is provided. This undermines the strength of the theoretical results.

2. The connection between the theorem to the general convergence result of FedAvg under the same assumption. When $\delta = 0, \Gamma_{min} = T$, the convergence should recover the rate of standard FedAvg. However, such a discussion is missing, weakening the paper's clarity.

3. The connection between the proof of pruning and lossy compression. Assumption 2 takes the standard assumption of model compression in compressed FL (e.g., [19, 21, 22]). Yet there is a critical difference in the minimum covering index. The connection and difference between the proof technique should be discussed.

Limitations:
The authors have discussed the limitation of the paper, that partial client participation is not considered, and how the theoretical result guides the design of an optimal model pruning strategy is also limited.



Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper focuses on the cross-device federated learning setting. The authors introduce a general theoretical framework for analyzing FedAvg with masks on local pruned models. This analysis is particularly valuable in establishing the convergence of federated schemes when the global model is distributed across multiple edge clients. Furthermore, the paper includes numerical illustrations of their algorithm, providing practical insights on its performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper is well-written and easy to understand, but with few points that might be improved (see ""Questions"" part).

* The paper conducts a thorough theoretical investigation on federated learning with reduced-size models. Specifically, the authors provide novel tools to study the convergence of FL pruned models.

Weaknesses:
* Although interesting, some parts may seem to have been rushed. For example, assumptions and lemmas should be referred with ref{}. Further, I also spotted a few errors on the supplement, even though I haven't read the supplementary in detail. It's important to review and correct these typos.


Limitations:
The authors did not address the potential negative societal implications of their research, but this does not seem critical for this particular theoretical and numerical study.

Rating:
5

Confidence:
2

REVIEW 
Summary:
In this work, the authors provide a general theoretical framework to analyze the convergence of Federated training schemes conducted over local models of heterogeneous network structures. Such structures can usually be obtained through different model reduction methods, such as model pruning/sparsification or model extraction. The proposed framework introduces the minimum covering index concept to conduct the analysis, representing the number of local models concurrently updating the same set of parameter indices. The paper is well written, but a couple of clarifications are necessary.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Very well-written paper with clear objectives and contributions.

Generalized framework to encapsulate different model reduction algorithms.

The introduction of the minimum coverage index concept and its interplay with model reduction noise can lead to very promising and interesting insights.

Weaknesses:
Further elaboration is needed on terms and concepts used during the theoretical analysis.

Empirical evaluation needs improvement.

Limitations:
No.

Rating:
6

Confidence:
4

";1
pefAAzu8an;"REVIEW 
Summary:
The paper investigate the performance of a specific type of architecture, RNNs coupled with hypernetworks, in meta-reinforcement learning. Meta-RL aims to address the sample inefficiency of RL algorithms by learning to perform few-shot learning when given a distribution of related tasks for meta-training.

The authors note that while specialized and complex meta-RL methods have been proposed in the literature, recent work suggests that using an off-the-shelf sequential model, such as an RNN, trained in an end-to-end manner can serve as a strong baseline. However, the supporting evidence for this claim has been limited. The paper presents an extensive empirical investigation to address this gap. While RNNs can achieve strong performance in meta-RL, the study finds that the use of hypernetworks is crucial in maximizing their potential. Interestingly, when combined with hypernetworks, the simpler recurrent baselines outperform existing specialized methods and establish themselves as state-of-the-art (SOTA) on standard meta-RL benchmarks. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
S1: The paper is extremely clear

S2: The authors make sure to tune each methods, which is something I do not see often enough 

S3: Coupling RNNs and hypernetworks is an idea that is novel in the field of meta-RL, and the attempts at understanding why they might be outperforming other baselines are interesting

Weaknesses:
W1: While the hyperparameter tuning was fairly done in terms of computation budget, it is to me not clear this strategy is the most relevant. I can imagine that many of the complex methods have many more hyperparameters to tune than the simple methods that you propose, and an excessively small search grid could unfavorably disadvantage the former. 

W2: The empirical investigation is, as noted by the authors, quite limited. In that regard, more empirical evidence from different benchmarks would make the paper stronger.

Limitations:
The limitations I can think of have been addressed by the authors.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper explores how to approach the problem of meta-reinforcement learning by using hypernetworks. In particular, the authors propose to employ an RL2-like scheme, where instead of producing a vector $\phi$, the recurrent model outputs a set of neural network weights. They dub the method RNN+HN. They introduce a set of task inference-based baselines with and without the usage of hypernetworks. Subsequently, they show that the simple RNN+HN approach outperforms more sophisticated baselines in gridworld environments and in Mujoco. Finally, they perform an ablation study by using RL2 with state-conditioning.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Overall, I find the paper interesting and the results quite convincing. However, the authors' claims are very strong -- stating that the proposed method is state-of-the-art in the wide field of meta RL. Although the experiments do show that the method works quite well, I do not think it is enough to support this bold claim. As such, for now, I am going with ""borderline reject"", but I would be inclined to increase the score if the authors either extended the empirical evaluation or toned down the claims presented in the paper.

Strengths:
- The paper introduces a nice, conceptually simple idea that provides substantial improvements. Parameterizing the policy using a neural network with weights generated by a hypernetwork is an elegant idea.
- The empirical results are in general quite good, the RNN+HN method outperforms most of the baselines in many cases.
- The paper is well written.
- The appendix includes additional ablation studies and more baselines.

Weaknesses:
- The critical weakness of the paper is the mismatch between the claims and the empirical evaluation. Namely, the authors state multiple times that their method is state-of-the-art in the field of meta-RL, but the experiments do not support that claim sufficiently:
    - There are not enough external baseline methods. The baselines the authors use are inspired by existing methods but do not correspond exactly to what has been proposed in the literature previously (e.g. TI-naive, TI). Additionally, the authors omit existing established works such as MAML-based algorithms and PEARL. As they say, they ""exclude the latter methods since estimation of a policy gradient in policy-gradient approaches requires more data than in our benchmarks"". That is, in general, a reasonable assumption, but seems too strict when claiming state-of-the-art.
    - The authors support their claim by saying that their method outperforms previous SOTA [1, 2], but I'm not convinced that these previous works are still SOTA as of now. Some of the recently published works also show very good results [3, 4].
    - Currently, the authors only consider two sets of environments: gridworlds and Mujoco continuous control. I think the empirical evaluation should be extended to include more complex environments such as Meta-World, RLBench, Atari, or DeepMind Alchemy. I think that environments with high-dimensional state spaces (e.g. images) would be interesting to test the limits of the proposed method.
    - There are some environments where RNN+HN falls behind (e.g. Walker, Cheetah-Vel). This is not a grave problem by itself, but it makes the problem of having relatively few benchmarks even more problematic -- how will this method scale up to other environments?
    - Why the RNN baseline only appears in a single environment in Figure 8 and Figure 9? I think it's an important baseline that should be included.

[1] Zintgraf, Luisa, et al. ""Varibad: A very good method for bayes-adaptive deep rl via meta-learning."" arXiv preprint arXiv:1910.08348 (2019). \
[2] Beck, Jacob, et al. ""Hypernetworks in meta-reinforcement learning."" Conference on Robot Learning. PMLR, 2023. \
[3] Melo, Luckeciano C. ""Transformers are meta-reinforcement learners."" international conference on machine learning. PMLR, 2022. \
[4] Chalvidal, Mathieu, Thomas Serre, and Rufin VanRullen. ""Meta-Reinforcement Learning with Self-Modifying Networks."" Advances in Neural Information Processing Systems 35 (2022): 7838-7851.

Limitations:
The author discuss limitations sufficiently.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates the performance of recurrent neural networks in meta-RL. They suggested that a current neural network can achieve strong performance with hypernetwork. They compared this method with numerous baselines on several meta-RL benchmarks and found that the recurrent baselines along with hypernetwork could achieve SOTA performance.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
This paper introduced a novel assertion that a recurrent method, when combined with a hypernetwork, can achieve SOTA performance in meta-reinforcement learning. The authors substantiated this claim with rigorous empirical experiments demonstrating superior performance. Furthermore, they conducted an analysis elucidating the reasons behind the method's impressive performance. The paper is well-written and clearly structured, which makes it easily comprehensible for the readers.

Weaknesses:
1. The authors could enhance the comprehensiveness of the study by testing their algorithm on a wider range of environments.

2. The final analysis segment of the paper would benefit from further development to unequivocally ascertain why the proposed method can achieve state-of-the-art performance.

3. The paper lacks ablation studies for the different settings of hypernetwork component, leaving its individual contribution to the overall results unclear.

4. The format of the reference is wrong

Limitations:
The authors have discussed the limitations of this paper in the last section.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The manuscript proposes to modify a common meta-RL baseline, in which a meta-learned RNN provides the policy network with an encoding of the trajectory. The proposed recurrent hypernetwork instead lets the RNN predict the weights and biases of the policy network. This is shown to be a strong model in comparison with several variations of RNN-based and task-inference meta-RL approaches.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
I think the proposed approach is novel and the idea is described in sufficient detail.
The experimentation affords equal computational resources for hyperparameter optimization of all models. The method seems to perform better in 6 out of 7 considered environments.
The ablation studies adequately address some questions that I thought of while reading the paper.

Weaknesses:
1. The implementation of the RNN and RNN+HN models could've been described in a bit more detail. It is for example not clear what the meta parameters $\theta$ correspond to in the RNN baseline, since line 108 says that $f$ and $\pi$ use distinct parameters from $\theta$. Is this sentence wrong and $\theta$ are the RNN's parameters? I could also not find all hyperparameters of the architectures, such as the type and layer sizes of the RNNs.
1. For the strong claim that recurrent hypernetworks are SOTA in (all of) meta-RL, the presented experiments seem rather limited. What about experiments on RLBench or Meta-World? Also, there are many more categories of meta-RL methods to be considered than RNN-based and task-inference methods. The manuscript can benefit greatly from a clear justification of this strong claim.
1. I could not find an indication of how exactly the performance curves were generated, e.g. how many seeds per model, what exactly does the shaded area convey, etc.

Limitations:
The authors mention that they cannot guarantee that the proposed method will improve over every baseline nor over every environment. This does not feel like a careful look at model details and how they might affect adaptation to different task variations. One general criticism of many meta-RL methods is that they are evaluated on very narrow task distributions. Maybe the authors could focus on that. The recent meta-RL survey by Beck et al. [1] might give some ideas for discussion.

## References
1. Beck, Jacob, et al. ""A survey of meta-reinforcement learning."" arXiv preprint arXiv:2301.08028 (2023).

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper is an empirical investigation that tests the performance of different variants of three methods for meta-RL: RNN that provides task information implicitly from previous trajectories on the same MDP, TI that trains a task representation with VAE manner, and VI that is the baseline Varibad [1]. On mujoco environment, the paper finds that RNN with output being a hypernetwork that outputs the parameter for the policy works the best, and thus re-establish [2] RNN+hypernetwork as the state-of-the-art method for meta-RL. 

**References:**

[1] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. In International Conference on Learning Representation (ICLR), 2020.

[2] Jacob Beck, Matthew Jackson, Risto Vuorio, and Shimon Whiteson. Hypernetworks in meta-reinforcement learning. In CoRL, 2022.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Many variants are tested in the environment. There are 12 variants tested in the paper, which gives a thorough analysis on the effect of using hypernetwork for each method; in addition, the effect of ""conditioning on the state twice"" is also considered, which makes the conclusion that hypernetwork is crucial more rigorous.

2. The figures appended in the paper does a well job in conveying the architecture of each method.

Weaknesses:
**1. The experiment results are not convincing enough.** 

a) The method proposed (line 184) in the paper, RNN+hypernetwork, is already proposed in prior work [1]. More specifically, RNN alone is equivalent to RL2 [3] as line 106 suggests, and RL2+hypernetwork is already tested in [1], which is inferior to Varibad [2]+hypernetwork. Thus, the key contribution of this paper is not proposing new method, but to overthrow the previous conclusion that Varibad+hypernetwork is better. However, this paper only tests gridworld and mujoco environments, and on both environments RNN+HN is only marginally better than VI+HN (which in [1] also has multiple methods that works similarly well, indicating that mujoco alone is not a strong benchmark). Most importantly, the metaworld (ML1 and ML10) environment, which is the decisive evidence that Varibad+hypernetwork is better than RL2+hypernetwork, is missing in this paper. With such result missing, it is hard for the readers to be convinced that the result is the other way around from that in [1].  

b) The baselines, though with many variants, do not cover enough areas for the claim of the state-of-the-art. Reviewers of [1] have already pointed out that the state-of-the-art status of Varibad [2], even before the presence of [1], is questionable. Also, while it is true that the meta-RL method can be briefly summarized in policy gradient, implicit task representation and task inference as the paper suggests (line 61-63), in Table 2 of [4] there are many branches within each of the direction. For example, what if transformer instead of RNN is used [5]? Is policy gradient method necessarily worse than the other two branches? For a state-of-the-art method, those methods also need to be considered.

**2. The delivery of the paper's idea can be improved.**

a)  There is no motivation stated about why hypernetwork is used. The paper only states that ""we present the key insight that the use of a hypernetwork architecture is crucial ..."" (line 36-37) and ""hypernetwork has never been widely evaluated in meta-RL"" (line 81-82). However, there is no intuitive explanation about why hypernetwork is useful for meta-RL and should be considered in the first place. While there is explanation in [1] about preventing degeneration of multi-tasking, the paper should be self-contained and inform the readers about why hypernetwork, the most important component in the finally proposed RNN+hypernetwork method, is considered for meta-RL.

b) The method section can be modified to convey the idea more clearly. Currently, it is hard to identify which method is proposed (line 184) by the paper, and it is unclear for the first-time readers about why multiple methods are listed in the method section (and most of them is not proposed by this paper), instead of the usual paradigm where each key component is listed in a subsection. Since the methods are not novelly proposed by this paper, they can be put into experiment setup section (with more emphasis that this is an empirical study paper rather than conventional paper that proposes a new method, e.g., add a contribution summary at the end of introduction section); or, a summary could be added in the front of the method section, to tell the readers that all those methods listed below are tested in the experiment section, and in each section we discuss one type of method tested. A table summary would also be very helpful.

c) There is no detail of the environment settings except gridworld. What is the cheetah-dir environment? What are the definitions of state, action and reward, and how are the MDPs in meta-learning differs? What reward can be considered expert-level performance? While they can be found in prior work, the paper should be self-contained and the settings should be attached in the appendix to help the reader form a better intuition about the environment.

**3. Other minor problems:**

a) Some experiment results are missing, for example, RNN in Fig. 8b, 8c, 9b, 9c and 9d;

b) Fig. 10 is never referred to in the paper; reference to Fig. 10 should be added in ""latent gradients"" paragraph.

c) The x-axis of walker environment is not aligned with other mujoco environments.

d) The name, VariBad, should be mentioned in VI and VI+HN to more clearly show the connection of the method tested to existing baselines.

**References:**

[1] J. Beck et al. Hypernetworks in meta-reinforcement learning. In CoRL, 2022.

[2] L. Zintgraf et al. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. In ICLR, 2020.

[3] Y. Duan et al. $RL^2$: Fast reinforcement learning via slow reinforcement learning. In arXiv, 2016.

[4] J. Beck et al. A survey of meta-reinforcement learning. In arXiv, 2023.

[5] L. Melo. Transformers are Meta-Reinforcement Learners. In ICML, 2022.

Limitations:
There is an independent limitation section in the paper, which I think generally discusses an important limitation of the work. Though, as suggested in the weakness section, I think currently the limitation is not sufficiently mitigated. There is no negative societal impact discussed in the work, which I would encourage the readers to add; though the work is still far from application, automated control itself brings potential challenge, such as job loss, to the human society.

Rating:
4

Confidence:
3

";1
Fkckkr3ya8;"REVIEW 
Summary:
This paper studies the compositional reasoning capability of Transformer models on three kinds of tasks, namely multi-digit multiplication, logic grid puzzles, and a dynamic programming problem. The compositional complexity of a given task, or the number of reasoning steps, can be easily controlled by varying lengths of inputs. This paper formulates the reasoning steps as a computation graph and uses characteristics like lengths and widths of the computation graph to qualify the level of complexity. For experimental evaluation, this work evaluates several Transformer models (i.e., GPT3, ChatGPT, GPT-4) using zero-shot, few-shot, and finetuning techniques. This paper shows that in zero-shot and few-shot settings, all three Transformer models deteriorate sharply from nearly perfect to nearly zero when increasing compositional complexity. Furthermore, finetuning GPT-3 with or without scratchpads does not help GPT-3 generalize to unseen and more complicated task instances. To gain a deeper understanding of these failures, this work finds that 1) Transformer models are likely to give partially correct predictions on outputs that can be determined by a small set of input features, and 2) Transformer models are likely to rely on computation patterns seen in training data and generalize poorly when they are not sufficient for OOD tests. Finally, with reasonable assumptions, this work gives a theoretical justification for why any models fail to generalize longer compositional reasoning tasks. Informally, as long as every single step is not perfect, the error will happen for a sufficiently long reasoning chain. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- This work carefully selects three representative compositional tasks and introduces computation graphs to systematically quantify the level of complexity. 
- State-of-the-art transformer models like GPT-3, ChatGPT, and GPT-4 are used in the experimental evaluations. The observed limits are likely to be universal for existing transformer models. 
- Some new insights in analyzing the successes and failures of Transformer models are presented. Specifically, relative information gain seems a likely reason that Transformer models tend to predict partially correct answers; and linearized subgraph matching suggests Transformer models tend to capture spurious patterns in the training.
- Four interesting categories of errors are proposed and can effectively characterize the mistaken behaviors of Transformer models when the compositional complexity increases. The restoration error is particularly interesting, as it captures an often ignored problem -- the model could give correct answers but because of wrong reasons. 
- Some simple but formal analyses are proposed to illustrate the theoretical limits of Transformer models. 


Weaknesses:
- It is well-known that large language models like GPT-3, ChatGPT, and GPT-4 behave poorly on complicated reasoning and planning tasks.  So, most results of this work are somewhat expected.

- Only three tasks are evaluated, and only one specific algorithm is chosen for each task. Although the chosen algorithm seems straightforward, it is possible that the particular algorithm may not be appropriate for Transformer models to learn. It does not rule out that the Transformer models may be good at learning other less intuitive algorithms for the evaluated tasks. 

- Finetuning is only performed on GPT-3. It is understandable that finetuning larger models like ChatGPT or GPT4 is computationally prohibitive for a small research group. But, from a scientific point of view, the observations on GPT-3 may not generalize to ChatGPT or GPT4, and the emergent capabilities of larger language models may overcome the limits observed on GPT-3.

- The claim of subsection 3.2.2 is a bit subjective. Linearized subgraph matching is a means proposed in this work to better understand Transformer models. It does shine some light on why Transformer models fail for the chosen tasks, however, whether that's indeed Transformers' internal dynamics is not quite clear. 

- The theoretical justification seems oversimplified. It has nothing to do with the Transformer models. 

Limitations:
The authors discussed potential limitations, and there is no (obvious) negative societal impact of this work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work studies the ability of LLMs for compositional tasks. They propose a formalization called computation graphs. With this formalization, they further propose three different tasks: multi-digit multiplication, Einstein’s puzzle, and a dynamic programming problem. They test multiple LLMs on these three tasks with different training and inference settings. By varying the depth and width of the graph, they show that LLMs do not do well when the scale of the problem is large. In their further analysis, they point out two problems of LLM models: relying on single-step shortcuts and relying on similar training subgraphs. They also conduct an error classification and notice many hints for memorization (e.g., restoration error). Finally, this work presents a theoretical analysis showing the effect of error propagation.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1.	This work presents a relatively comprehensive and detailed evaluation of the compositional abilities for LLMs. The empirical results contain multiple training settings, and results w/wo scratchpads. 
2.	The three tasks designed in this work can be valuable for future work on compositional tasks.
3.	The error analyses in this work are detailed and insightful. I really like the error classification analysis, which shows a more detailed description about LLM behaviors, and provide concrete and quantitative evidence for memorization, restoration error and error propagation.


Weaknesses:
1.	While the theoretical derivation is interesting, I find the takeaways and the perspective to be slightly underwhelming. The main takeaway seems to be error propagation lead to bad results when the problem scale is sufficiently large. This is true, but this is also true for everything, and for both machines and humans. To me, the more interesting problem lies in that why even n is not very large, compositional problems still seem to be relatively hard. Additionally, I think the theoretical derivation applies to sampling but not to greedy decoding. 
2.	The idea of using a computation graph to control and analyze the difficulty of composition is not novel, see related works in [1][2][3]. While I do feel that the approach and objective of this paper are different enough compared to previous works, there should still be a discussion about these works.
3.	Minor presentation issue: Some experimental setup details are hard to find. For example, I cannot find how large the finetuning datasets are. And some of the other important details (e.g., how you split the train/test for finetuning experiments) are in the appendix. I understand the space is tight in the main paper, but I would encourage the authors to add some details to the main paper so that the results can be interpreted more easily.



[1] Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, and Ryan Cotterell. Benchmarking Compositionality with Formal Languages. In COLING 2022.

[2] Keysers, Daniel, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev et al. Measuring Compositional Generalization: A Comprehensive Method on Realistic Data. In ICLR 2020.

[3] Ben Bogin, Shivanshu Gupta, and Jonathan Berant. Unobserved Local Structures Make Compositional Generalization Hard. In EMNLP 2022.


Limitations:
This work provided a limitation section in Sec. 8 in the main paper and a societal impact statement in Appendix E.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper contributes to the literature on analyzing the limitations of Transformer models in compositional tasks by presenting an analysis based on computation graphs. Based on these computation graphs, some interesting new observations are made, which I believe are worth publishing.

I found the paper a great read and very interesting. I have many comments/criticisms/questions below, but I would like to clarify that these are not attempts to attack the content, and more aiming for constructive criticisms for trying to make the paper more clear and situated in the literature, as I think the work is very solid.

Some typos/comments:
- Part of this work has already been studied in the past, and I think the authors should do a better job in differentiating the new insights from previously known parts. For example, I think it'd be great to discuss the new findings given: ""Making transformers solve compositional tasks"" (2021), ""The devil is in the detail: Simple tricks improve systematic generalization of transformers"" (2021), and ""Grokking: Generalization beyond Overfitting on Small Algorithmic Datasets"" (2022)
- Page 2: ""This substantial gap suggests that systematic problem-solving capabilities do not emerge from maximum likelihood training"" -> See the Grokking paper I mentioned above, as they show that maybe systematic problem-solving abilities can improve when training beyond the overfitting threshold.
- Section 3: As discussed in ""Making transformers solve..."" mentioned above, even if ""vanilla"" Transformers really struggle to solve compositional tasks, slight modifications to the architecture show significant gains. So, when only evaluating variants of the GPT architecture, it seems like conclusions are limited to GPT-like models, and not to ""Transformers"" in general, as small architectural decisions can have big impact in these types of tasks. For example GPT models being decoder-only models, they employ causal attention, where as encoder-decoder models would be able to deploy bidirectional attention to the input problem, which can give very different inductive biases. Carefully clarifying the extent of the conclusions would be better.
- Section 3.1: ""suggesting that systematic problem-solving capabilities do not emerge via exhaustive training on task-specific data."" -> Again, I'd like to bring up the Grokking paper results, and I wonder if the authors could comment on the implications for their results here.
- Section 3.1: Although results are very interesting, I think the authors over-generalize the conclusions from experiments with one particular model pretrained in one particular way. I think it might be good to constrain conclusions of results to decoder-only-GPT-style-pretrained-Transformer models.
- Propositon 4.1: this has very little to do with Transformers, but with any method that approximates steps. And btw, this proposition applies to humans as well, as the probability that a human makes a long calculation correct decreases with the length of the calculation due to the growing probability that at least one of the steps is wrong. So, again, I think that clarifying the extent of the conclusions in the paper needs some work, whereas above I was pointing out that experiments show failure cases for GPT-style models and you are overgeneralizing to Transformers, here you are showing a limitation of sequential prediction models, and too narrowly just blaming Transformers. Moreover, I see this proposition very related to the classic error compounding in the literature of imitation learning (e.g., see ""Efficient Reductions for Imitation Learning"", 2010), and usual solutions, e.g. Dagger, rely on training models to recover from mistakes, mitigating the problem. It'd be great to comment on the relation if the authors see it appropriate.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Even if the weaknesses of Transformers in compositional tasks have been studied many times in the recent past, this paper brings an interesting new perspective based on computation graphs that allows for a more finegrained analysis than previous work.

Weaknesses:
Perhaps the two main weaknesses (fixable) are that (1) conclusions seem to be overgeneralizing to the whole class of Transformer models, when only one instance of them was used for evaluation, and (2) it seems that some of the most relevant pieces of work in the literature were not discussed, making it harder to see what are the new pieces of knowledge that this paper is bringing compared to what was already known. Both of them fixable with small modifications to the manuscript though.

Limitations:
As acknowledged by the authors in their own section, the main limitation is that experiments were conducted with just GPT models, and it's unclear if we can generalize from those, given these are all the same Transformer architecture, and pretrained in a particular way.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper develops empirical and theoretical studies on the limitations of Transformers for solving compositional tasks. It defines computation graphs and quantifies compositional complexity with the graph metrics and uses relative information gain to predict surface patterns learned by Transformer. It conducts extensive experiments on three representative compositional tasks and finds that Transformers rely on pattern matching and exhibits poor generalization in compositionality. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
This paper conducts extensive experiments and profound analysis. The idea of quantifying compositional complexity with computational graphs is inspiring.

Weaknesses:
The empirical findings and the driven conclusions are quite trivial. 

Limitations:
The paper lacks methodologies or instructions for improving the model based on the obtained empirical findings and driven conclusions. 

Rating:
6

Confidence:
3

";1
hh6azymUaE;"REVIEW 
Summary:
This paper proposes the quadratic gradient for privacy-preserving logistic regression. Such gradient is used together with Nesterov’s accelerated gradient (NAG) and Adagrad on Homomorphic Encryption techniques.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper tackles the privacy-preserving (in the sense of encryption) logistic regression. The paper is clear with introduction and motivation. The algorithm is easy-to-follow and well-supported by experiments.

Weaknesses:
Overall, it is hard to understand the privacy concerns in this work as there are many privacy-preserving techniques available. Without an example or experiment of privacy attack, the motivation of using HE in the first place is not backed up. In addition, only [12] is compared in the empirical results. While the new method seems promising, the lack of other baselines makes it hard to understand the limitations and benefits of the new algorithm.

Limitations:
NA.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper proposes a second-order version of Nesterov's accelerated gradient (NAG) descent and Adagrad for logistic regression by incorporating an approximation to the Hessian. The authors call this the ""quadratic gradient"". Specifically, a diagonal approximation to the Hessian for logistic regression is proposed. Some empirical results are shown to illustrate the benefit of the proposed method over vanilla NAG and Adagrad.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The proposed approximation of the diagonal Hessian may be interesting for NAG.

Weaknesses:
Unfortunately, this paper has several weaknesses.

**1.** **Limited novelty**: The proposed approximation to the Hessian in Section 3.2 seems like a trivial and incremental extension of the idea of reference [4] discussed in Section 3.1. 

**2.** **Lack of clarity**: The proposed methods are unclear to me and the presentation needs to be heavily improved.

* The enhanced NAG method described in line 150 is unclear to me -- what is $G$ here and is $\alpha_t$ the step-size here? Moreover, Algorithm 1 seems different from the discussions in Section 3.3. What are $\alpha_0$ and $\alpha_1$? They don't look like the quantity $\alpha_t$ introduced in Section 3.3. Why is $\alpha_1$ chosen to be $0.5(1 + \sqrt{1 + 4 \alpha_0^2})$? I don’t understand lines 31 and 37 in Algorithm 1 and what are $\gamma$ and $\eta$ here? What is the role of $W$ in lines 34 and 35 – it is not being used at all. In summary, the enhanced NAG method/Algorithm 1 has been presented poorly and I don't understand the method at all. 

* What is the enhanced Adagrad algorithm? The two equations after line 154 which are supposed to explain the enhancement are not very clear. What is the difference between $G^{(t)}$ and $g^{(t)}$ in these equations? Is $G = \tilde{B}^{-1} g$ here? There is no algorithm summarizing it like Algorithm 1 for enhanced NAG. Also, suddenly for Adagrad, the authors have a negative sign in front of the gradient which corresponds to minimizing the function whereas for NAG and the previous discussions in the paper, maximizing the function has been considered. Please stick to either minimization or maximization for consistency.

**3.** **Premise of enhanced Adagrad:** One way to interpret Adagrad is that it tries to maintain a diagonal approximation of the Hessian inverse and applies it to the gradients (a.k.a. preconditioning). So I'm not sure why applying a second approximation of the Hessian inverse on the *already preconditioned* gradients makes sense intuitively. Additionally, the authors themselves point out that enhanced Adagrad cannot be applied to general optimization problems (line 182) due to ""learning-rate explosion"". Then why introduce this method at all?

**4.** **Setup and experiments**: The datasets on which experiments are performed are not standard benchmarking datasets in the ML community and appear to have very few features (looking at Table 2). There are no test set statistics provided. I'd be more convinced if the authors showed empirical results in a *standard logistic regression setup without any kind of encryption* (which frankly seems irrelevant to me in this paper) on benchmarking ML datasets.

----

*Some general comments*: The introduction on logistic regression can be compressed. It is standard to consider the *negative* log-likelihood objective and apply gradient *descent* to minimize it. A couple of small typos -- in line 96, I guess it should be ""$\bar{h}_{k i}$ is the $k^\text{th}$ element in the $i^\text{th}$ row of the Hessian"" and in line 216, it should be ""public"".



Limitations:
Not in too much detail but as I mentioned in Weaknesses, the authors point out that enhanced Adagrad cannot be applied to general optimization problems. No foreseeable negative societal impact.

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper proposed a new approach to improve the gradient used by first-order optimization methods in logistic regression by utilizing a constant bound to the Hessian matrix. The authors demonstrate how to use their method under a fully Homomorphic-Encryption scenario. They test their method on many real-world datasets under non-private settings and Homomorphic-Encryption settings.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper is written clearly.
2. The experiments are all using real-world datasets which have strong practical implications.

Weaknesses:
1. The `quadratic gradient` method is not very new. As the authors have mentioned in Section 3.1 (Line 95), most parts of the method were proposed by Bonte and Vercauteren. I understand that the missing non-negative restriction is important for using the convergence results by Böhning and Lindsay (Line 92). However, using the absolute value is rather an straightforward solution. 
* A more interesting and critical question remaining to be answered is why this proposed `quadratic gradient` method is faster as the authors claimed in the conclusion (Line 238). 
* Another problem with this method is its usage being restricted in logistic regression: The authors provided a choice of $\bar{H}$ for logistic regression, while it may be very hard to generalize it to other problems, especially neural network training. 
2. The experiments have not shown much advantage of using the `quadratic gradient` method. In Table 1 and Table 2, the accuracy and AUC of the proposed method are almost always lower than the compared baseline method [12]. I understand that the learning time is reduced, but it was not a main problem in [12] as shown in the tables, and we are not sure if there is a tradeoff between the learning time and the accuracy.
* The description of the experiment details in Section 5 is very short. The authors suggest the readers refer to [12]. I think it would be better to have the details in supplementary and a discussion of the weaknesses of [12] in these experiment settings, along with why the proposed method solves those weaknesses.

Minor weaknesses:
1. The maximum likelihood estimation (MLE) is commonly referring to the estimate for $\beta$. The value of the loss function is the negative log-likelihood. That said, the y-axis of the figures could be corrected. Also, the objective function is usually the mean of the loss for each data point, not the sum.
2. Typo: Line 216, pulbic -> public.
---
I have read the rebuttal which answered my questions but did not fully address my concerns on the weakness.

Limitations:
N.A.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes a new gradient method that can be efficiently used under homomorphic encryption. The proposed method replaces the gradient $g$ by an approximation of $H^{-1}g$, where $H$ is the Hessian. This approximation is done using a specific diagonal matrix, that speeds up convergence while being possible to use under homomorphic encryption.


Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper proposes a new method for privacy-preserving logistic regression under FHE, that achieves reasonable results with less computation than existing methods.
2. The proposed method is quite versatile as it can be applied to a variety of optimization algorithms.


Weaknesses:
1. Experimental results are not convincing. Contrary to the paper's claims, the proposed method often performs way worse than existing ones (especially on iDASH, Edunburgh and pcs). Datasets are also very small, and therefore do not account for how the method scales with the dimension. Given this is an empirical paper, it seems a bit insufficient.
2. The fixed-Hessian method seems to be closer to preconditioning (see e.g. [1]), where gradients are linearly transformed before being used, than to a proper second order method.
3. The proposed method does not seem to be too novel. In particular, the paper refers to [2] (which itself refers to a paper with the same title as this manuscript), which proposes a very similar method.

[1] Preconditioned Stochastic Gradient Descent, Xi-Lin Li 2015.
[2] Quadratic Gradient: Uniting gradient algorithm and newton method as one, by Chiang, 2022.


Limitations:
Limitations are not discussed in the paper. In particular, experiments only consider a specific setting, with a choice of parameters that seems arbitrary (e.g., 3 vs. 7 iterations), and some claims are not supported with evidence.


Rating:
2

Confidence:
4

";0
XRTxIBs2eu;"REVIEW 
Summary:
The authors present a new long-range transformer architecture by incorporating SSMs. This novel model outperforms several established baselines, such as Transformer XL, Block Recurrent Transformer, and Sliding Window Transformer, in terms of cost-effectiveness trade-off for tasks involving long-document or code modeling.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1) Well-motivated. Long-range modeling is becoming increasingly important for LLM community.
2) Good results on language modeling (PG19, arXiv, Github)
3) The writing is clear and effectively conveys the ideas and findings.
4) The model design is intuitive and well-reasoned. The inclusion of both local full attention and linear components to handle long sequences is a sensible approach.

Weaknesses:
1) The scale is too small. For language modeling, based on the success of LLM, we always expect good scalability. This paper only conduct experiments up to 380M params, which are far from many emergent abilities threshold. When scaling up, many inductive bias would become useless[1]. 
2) More insightful experiments beyond language modeling are required. For instance, as the authors mentioned in the limitation section, what about the results on Long Range Arena? 
3) Any case study about how this model captures long-range dependency? Why this model is indeed better? It seems that putting one efficient attention layer with linear or sub-linear complexity before self-attention should work similarly.
4) Some strong baselines like CoLT5[2] are missing.

[1] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?
[2] CoLT5: Faster Long-Range Transformers with Conditional Computation

I enjoyed reading this idea but I believe the missed experiments above, especially (1), would be highly desirable. Without a set of experiments about scaling the model up, I cannot agree this paper is useful enough.


Limitations:
See weakness

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper focuses on combining two efficient techniques for long-range modeling: state-space models (global contextualization) and block-recurrent transformers (local contextualization). In particular, they propose two different approaches, the first uses SSMs to output contexts for multiple heads (multi-head), and the second concatenates the last entries from the previous window to form a combined context state (multi-filter). Evaluation is performed on three language modeling datasets that outperform block-recurrent transformers in perplexity and is much faster when compared layer-wise. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
S1. Exploring different ways to combine SSMs and block-recurrent models to improve efficiency is a compelling direction.  SSMs offer a parallelizable way to capture long-term information and avoid sequential computation in block-recurrent models. The results of this study should be of interest to researchers that study architectures that capture local and global information.

S2. The evaluation even though it focuses mainly on comparison with block-recurrent models and SSMs on three language modeling tasks, it is thoroughly described and well-executed. 

Weaknesses:
W1. Even though the paper is mainly empirically driven, the delivery lacks a comprehensive and diverse set of evaluations to demonstrate the effectiveness and limitations of the method.  

W2. Experiments are targetting language modeling on three tasks but there is no experiment that measures the long-range capabilities of the model. There are several long-context classification benchmarks that the authors can use in addition to language modeling: LRA [1], MuLD [2], and CAB [3]. 

W3. The method design makes specific assumptions about the hardware to be employed and bases its evaluation on it; e.g. efficiency comparisons are made per layer. It's not explored to what extent the benefit remains when comparing training time/speed vs quality for the whole model and evaluation is performed on typical accelerators. That reduces the practical impact in my view. 

W4. A study regarding the behavior of the model when increasing the model size is missing. Scaling aspects are important to consider when making claims about outperforming transformers. 

[1] https://arxiv.org/pdf/2011.04006.pdf
[2] https://arxiv.org/pdf/2202.07362.pdf 
[3] https://arxiv.org/pdf/2210.07661.pdf
 

Limitations:
Discussion about the limitations of the proposed method would be useful, I'd suggest talking about scaling behavior, performance on conventional accelerators, and generalizability to long-context tasks. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
State space models (SSMs) perform well on modeling long-range dependencies with good efficiency scaling, but on language modeling, transformers still outperforms SSMs. This paper tries to combine the best of both worlds and proposes a hybrid model, Block-State Transformer, which combine SSMs’ capacity on long range modeling and Transformer’s ability on modeling local context. The input sequence is split to multiple smaller segments. For each segment, transformer layer will do a self attention on this token embeddings and cross attention to the output of SSMs. Their experiments show that on language modeling, their approach achieve reasonable speedup with comparable performance to Transformers. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed combination of SSMs and transformers allow the model to exploit advantages of two powerful methods while avoiding their drawbacks. 
2. The SSMs used in the proposed method can be swapped to different SSMs making it possible to enjoy the advancement on SSMs field. 
3. The proposed method give similar performance on language modeling compared to Transformers. 


Weaknesses:
1. There was already existing work on SSMs that achieve similar performance on language modeling (https://arxiv.org/abs/2212.14052) compared to Transformers. The authors should include a discussion and comparison to the relevant work. 
2. The evaluations are performed on language modeling for 4096 length sequences. On this setting, there are a lot of strong transformer baselines with efficient self-attention designs. It would be good if the authors can provide an empirical comparison with these baselines. 
3. Perplexity is only one indicator of how the language models performance. To get more precise understanding of performance, it would be good to include a comparison on downstream tasks. 
4. Code is not available. 


Limitations:
Yes. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes block-state transformers, a method to combine state space models with transformers for language modeling. The paper evaluates block-state transformers on PG19 and arxiv math and finds promising results.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Combining state space models and Transformers is an interesting idea worth exploring. The presentation of the paper is clear. The explanation of state space models, which can be quite complex, is very clear. The evaluation hits the right notes in terms of the major questions to ask.

Weaknesses:
The evaluation is missing many recent works combining state space models and attention in various ways. The claim that SSMs do not match Transformers on language has not been true for a while. Most of these methods were released significantly before the NeurIPS deadline and are critical to compare against for evaluation.

* Mega [1] combines attention and state space models.
* BiGS [2] is a new SSM-based architecture that matches Transformers in language.
* H3 [3] combines SSMs and attention in alternate layers.
* Hyena [4] removes attention completely and replaces it with a convolution-based layer (similar to an SSM).

Confusingly, many of these works are cited in the paper - and ideas from the papers are used extensively in the methods proposed (e.g., ""BST:{SH,MF}"" uses the structure from H3 and Hyena without comparing against those architectures as baselines). Using the ideas from these papers without comparing against them makes it difficult to understand how this method compares against previous methods and where the performance improvement comes from.

Performance is also hard to evaluate compared to standard models such as Transformers (GPT-Neo [5], Pythia suite [6]). TransformerXL is an older model that is not trained as well as modern Transformer-based LLMs.

[1] https://arxiv.org/abs/2209.10655
[2] https://arxiv.org/abs/2212.10544
[3] https://arxiv.org/abs/2212.14052
[4] https://arxiv.org/abs/2302.10866
[5] https://github.com/EleutherAI/gpt-neo
[6] https://github.com/EleutherAI/pythia

Limitations:
The paper would be stronger with more discussion of limitations.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The authors present a novel architectural framework called the Block-State Transformer (BST), which integrates State Space models and Block-Recurrent Transformers to create a competitive autoregressive language model capable of effectively processing lengthy sequences. The input sequence is passed through a State Space model like S4, and the output of which is later in Block-recurrent Transformer as a replacement of the recurrent state vectors. To obtain the final output, the input embeddings are divided into fixed-sized windows and processed in parallel by a series of Block-Recurrent Transformers. Due to the usage of the S4 output as recurrent state vectors within the Block-Recurrent Transformers, the absence of recurrence allows for parallel computation. The authors propose three distinct integration modes, which differ in terms of how the S4 output is integrated within the recurrent state vectors. To evaluate the performance of BST, the authors compare it against four baseline models: Transformer-XL, Slide, Block-Recurrent Transformer, and a hybrid Gated State Space model. The comparison is conducted across three diverse datasets, namely PG19, arXiv Math, and GitHub. BST demonstrates slight perplexity improvements in the PG19 and GitHub datasets. Additionally, the authors present ablation studies on various parameters, including SSM layer placement, the number of SMM layers, and the state dimensionality of SMM.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ **Strong Presented Results**: Authors presented results on competitive benchmarks across reasonable prior baselines such as Transformer-XL, Slide, Block-Recurrent Transformers etc. and do outperform them across several tasks.

+ **Computational efficiency**: The proposed method is able to provide a huge improvement in terms of computational efficiency over models like Block-Recurrent Transformers by parallelization.

+ **Interesting combination of prior ideas**: By using the parallelizable nature of the SSMs the authors were able to introduce parallelization to Block-Recurrent Transformers thereby achieving computation efficiency. 

Weaknesses:
- **Incomplete Related works**: The authors' treatment of related works, particularly in the context of models combining Transformers and S4 appears to be lacking. It would have been beneficial for the authors to provide a more comprehensive discussion on existing models that incorporate both Transformers and S4. This would have allowed for a deeper exploration of the advancements and limitations of these models, highlighting the unique contributions of the proposed Block-State Transformer (BST). In addition to that, there could be more details on SSM development in related works since S4, S5, and S4D were mentioned in the paper later.

- **Missing Preliminaries on S4/S5** : A more detailed description of the S4 models within the method section would have been beneficial, particularly regarding the computation of the kernel since the complexity of the S4 model is the major part of the BST model complexity. Specifically, the computation of the kernel is not trivial if you want to keep overall L log(L) complexity and it relies on the form of the A and B matrices. A more comprehensive exposition regarding the computational aspects of the S4 models is deemed necessary for a thorough understanding of the subject matter.

- **Additional benchmarking** : As the authors themselves admit in the limitations section, there are further results required, especially on well benchmarked domains such as the Long Range Arena and also other long-term datasets to provide convincing evidence of BST performance. 

Limitations:
NA

Rating:
6

Confidence:
4

";1
qUlpDjYnsp;"REVIEW 
Summary:
This paper proposes Wave-GD, a score-based generative model, to generate graphs with high fidelity. By capturing the dependency between nodes and edges at multiple resolutions in the spectral space, it claims it overcomes the over-smoothing problem and achieves real-like frequency characteristics of nodes and edges.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow.
2. The paper's motivation is clear, and the paper has shown some limitations that previous models may have.
3. The idea of the paper is simple and straightforward.

Weaknesses:
1. The experiment is not sufficient to support the proposed method.

(1) lack of baselines: some baselines that utilize spectral information/graph characteristics are missing. It would be great to see the author compare them.[1,2,3]

(2) lack of more robust graph datasets: Ego-small and Community-small graphs are too simple, I think over-smoothing can't be a major issue for graphs in such scales. And how the ""multi-resolution"" should be defined in such small graphs? I suggest the authors run experiments on more complex graphs such as Planar graphs, SBM graphs, and large networks if possible with hundreds of nodes. 

(3) I suggest the author also include DiGress in Figure 5 -- since it also utilizes spectral information.

(4) Another comment in Figure 5: it's not clear whether the performance drop when increasing #GNN layers is really due to the over-smoothing problem, more investigation should be conduct to further prove this phenomenom. 

2. While the author propose to also diffuse on the edge weights obtained from SGWT, there are also other ways to define the edge importance (e.g., edge conductance). Justiifcation should be provided why SGWT is the chosen over others.

[1] Luo, Tianze, Zhanfeng Mo, and Sinno Jialin Pan. ""Fast Graph Generative Model via Spectral Diffusion."" arXiv preprint arXiv:2211.08892 (2022).

[2] Martinkus, Karolis, et al. ""Spectre: Spectral conditioning helps to overcome the expressivity limits of one-shot graph generators."" International Conference on Machine Learning. PMLR, 2022.

[3] Chen, Xiaohui, et al. ""Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling."" arXiv preprint arXiv:2305.04111 (2023).

Limitations:
1. The result on molecule generation is not that expressive.

2. The contribution may be insignificant: there are many GNN design that may alleviate the over-smoothing problem. Any baseline model with a better score networks may overcome this limitation easily. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this work the authors tackle the problem of graph generation learning where the goal is to learn the key features of a set of graphs and be able to generate graphs with similar properties. To that extend, the authors extend the GDSS (Jo 2022) method through an additional loss term (Eq. 7). This loss term encourages the employed GNNs to learn to reconstruct spectrally modified matrices A^s_i in addition to the normal adjacency matrices. The spectrally modified matrices are obtained through an SVD/PCA like approach where some spectral properties of the adjacency matrix are accentuated. This spectral accentuation is learnable as part of the training procedure.
The authors evaluate their approach on two real world and two synthetic datasets also used in previous studies. In terms of MMD (Mean maximum deviation) their approach frequently outperforms other methods employed for the task of graph generation learning. On molecule data, the employed procedure still ranks amount the best.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The authors provide an interesting extension to an existing approach (GDSS), which allows to better learn the scales that are important for a specific set of graphs. The greatly increased stability during training compared to GDSS (Figure 3) seems promising. The paper contains extensive comparison to other methods. The presented approach is on par/outperforms these other methods on the presented datasets. Most computational overhead is in the training phase, inference is as fast as GDSS.

Weaknesses:
Overall the core weakness of the paper is its presentation, which has a lot of room for improvements and weak support for the main claims of the paper (last point).

The presentation of the SDE learning (lines 155-179) is not understandable without reading the GDSS paper. I think it could be drastically improved by highlighting the differences/ improvements in comparison to GDSS rather than repeating all the definitions/equations.

The description of Figure 2a) could be much improved as it is not well understandable without reading the rest of the paper first.

Some of the equations and notation does not further the main cause of the paper: Section 3.1 beyond the first paragraph can almost entirely be cut. The introduced transformation $X^{s_i}$ on the edge signal X is not really used.

Lemma 1 seems to be misplaced as it introduced (lines 138ff) quite far away from where it is used (lines 190ff). Also it seems Lemma 1 is likely not a new result.

The bold highlights in Table 1 are not correct, in column “orbit”, three other methods outperform the presented approach but are not highlighted in bold. Also in lines 289f the text mismatches the table.

It is also unclear from the text how much hyperparameter tuning was done to achieve the results presented.

Last but not least, the experimental section shows, from an applicant's point of view, that the proposed changes lead to empirical performance increase on the graph generation task. On the other hand the experimental section does not well support whether the introduced changes actually had the desired effect of learning different “scales” better. 

It might as well be, that the introduced changes simply increase training robustness. More on this in the “Questions” section. It seems that one would need additional support for the main claim.

Limitations:
see questions above

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces a graph generative approach that leverages diffusion models and wavelet theory. The key concept revolves around utilizing the wavelet transform of the adjacency matrix across various scales, and learning a joint backward diffusion process that remains valid at all considered scales simultaneously. Consequently, the proposed approach exhibits a multi-resolution characteristic. The proposed approach is evaluated in the graph generation task using four benchmarks and is compared against autoregressive and one-shot approaches from previous works.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The adaptation of diffusion models to the graph generation field is a timely and intriguing topic, considering the challenges posed by the discrete nature of graph data.

The experimental evaluation provides substantial support for the claims made in the paper. The proposed approach outperforms recent methods on three benchmark in the graph generation task and achieves comparable results in molecule generation task.
The experimental setup and the metric considered for the evaluation are clearly presented.

Weaknesses:
Having to choose the parameter *J* without any insight or guidance for different datasets can be a drawback in practical applications. I would suggest to further investigate the relationship between performance and *J* among different datasets, along with the impact of graph statistics on the optimal *J*. Also, it is not clear which how many scales have been used to obtain the results reported in Tables 1-3.

The time complexity of the proposed approach, as indicated in Table 3, could present challenges in certain settings. Further discussion or potential mitigation strategies for addressing this issue would be beneficial.

Clarity could be improved in certain sections. For instance, Figure 1's purpose is unclear to me, and the role of spectral coherence between nodes and edges in the proposed approach needs better explanation. Providing a clear definition of the scale (s) domain and kernel function (k) in the preliminary sections would also help readers understand the concepts more quickly.

Limitations:
The authors adequately addressed the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper claims the node feature and graph topology are not coherent in most previous generative graph models and high-frequency signals in node features and graph topology may neglect during the generation process. Therefore, they propose a Wavelet graph diffusion model (Wave-GD) with score-based diffusion. Specifically, it uses different graph wavelet bases to get graph signals in different frequency ranges. The overall model diffuses node features, the original graph, and the adjacency matrix constructed by graph wavelet bases. To improve the coherence between node features and graph topology, the score-based models are based on graph multi-head attention layers, which take the product of node features and adjacency matrices including the original one and ones learned by different bases, which alleviates the gap between node features and graph structures. Performance on three small synthetic datasets and one real-world molecular dataset show that the proposed model can generate graphs that are not only realistic graphs in shape but also obey the chemical rules in high fidelity.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The novel part is the paper considers high frequency and discovers the coherence between node features and graph structures in diffusion models but still uses a simple dot product to solve it.
2.	The empirical results on real-world molecular datasets show the effectiveness in terms of generating realistic graphs with high fidelity and high novelty in a relatively fast time. 
3.	The model enjoys high flexibility regarding different tasks where they need frequency graph signals at different scales. 


Weaknesses:
1.	""nodes and edges"" in line 26 is misleading, it would be better if you mention ""node"" means ""node features"" and ""edges"" means ""graph structures"" in the introduction and then use ""node"" and ""edge"" for simplicity.
2.	Multi-resolution and coherence are two major claims in your paper, but it is not verified that coherence improves performance. 
3.	the proposed model is limited to small graphs due to the decomposition in spectral graph wavelet bases.

Limitations:
Yes. It may produce molecules that may harm the body.

Rating:
6

Confidence:
3

";1
DFaGf3O7jf;"REVIEW 
Summary:
This paper tackles updating the knowledge in LMs, focusing on allowing LMs to make new inferences consistent with the updated facts. To do this, the authors propose using the LM itself (or a teacher) to generate natural continuations for the ""updated/new entity"" definition. These continuations are used to update the LM. The update is conducted using a KL divergence loss between the LM conditioned on the definition and the LM that doesn't see the definition. The results show superiority to baselines in updates and in preserving old knowledge.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper seems like an excellent contribution. It's well motivated, well presented, and the key idea is simple, novel, and effective. The evaluation is convincing.

Weaknesses:
The method, like many others, is relatively opaque in terms of what it teaches the models and why/how it works precisely. However, it's well motivated and the analysis in Sec 7 begins to shed a little bit of light into this. More work is needed on that front, but I think it's fair to assume this will lie beyond the scope of this paper.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of injecting new entity knowledge in LLMs, such that these knowledge can be propagated and utilized when LLMs make inference on related queries. The paper proposes a context distillation method that consists of two steps to inject entity knowledge in a definition sentence: 1) Use a LLM to generate a set of continuations (a.k.a transfer set) for the definition sentence. 2) Fine-tune a student model such that its output distribution without conditioning on the definition sentence is close to the output distribution of a teacher model that conditions on the definition sentence.
They conduct experiments on two datasets about entity knowledge and show that the proposed method outperforms several baselines including standard fine-tuning and previous knowledge editing methods.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper studies an important question of knowledge injection and propagation of injected knowledge. The proposed method is novel in this context.
2. Some of the conducted analyses are insightful, such as the NLL with/without definition sentence for analyzing the supervision from the teacher model.

Weaknesses:
1. On Entity Inferences dataset, the conclusion that the proposed method improves the model ability to make inference using the injected knowledge is suspected. The reported performance improvement might due to the overlap between the generated transfer set and the probe sentence in the evaluation set. Without reporting (1) the level of overlap, and (2) a baseline that simply fine-tunes on the transfer set, the possibility of this overlap cannot be ruled out.
2. How does the method perform compared to a baseline that simply prepends the transfer set to the query?

Limitations:
Limitations are discussed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a method to propagate knowledge update to LMs via training a student model through context distillation, such that the LM can make inference on an entity even though the relative context/knowledge of the entity is not given. The framework involves two steps: 1) create a transfer set that contains the knowledge that the student model will be learning from; 2) compute the distribution of the transfer set tokens for both the teacher model (while given context, i.e. a definitional sentence) and the student model and update the student model's parameters by minimizing the KL divergence of the two distributions. The paper evaluates the student model with two sets, Entity Inferences and ECBD to show that the knowledge has successfully propagated.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper is more efficient with multi-entity editing and achieves competitive performance on the two evaluation set in terms of propagation success (accuracy and decrease in perplexity) while causing little impact on specificity.

Weaknesses:
It seems like the paper is more focusing on new knowledge ingestion, either in Entity Inference (synthetic entities) or ECBD (introducing new entities after 2022). While this is an important aspect, a harder task is to update existing knowledge in the old model. One dimension could be temporal shifts, e.g. after a new election, population/economic changes (potentially resulting changes in superlative statements), factual changing official announcement (e.g. solar system has 9 planets before 2006 and Pluto was downgraded to dwarf planet in 2006 - solar system has 8 planets now). It is unclear whether the model can adapt to the new facts while maintain low specificity.

Another baseline is to try prompting the LLMs with new knowledge and see how it propagates. If the existing LLMs can handle such knowledge updates well, it may be hard to justify why we need to train a separate student model.

Limitations:
As mentioned by the authors, this work mainly uses relatively small size LMs for experiments and its generalizability to LLMs is unknown. While it may apply to LLM trainers/creators to adapt this method to update their models, it does not extend to end users/organizations of the LLMs who want to ingest or update knowledge, e.g. from specific domains or confidential sources, potentially through local edits.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper propose a context distillation-based approach that can both impart knowledge about entities and propagate that knowledge to enable broader inferences. This approach consists of two stages: transfer set generation and distillation on the transfer set. In the first stage, a transfer set is generated by prompting a language model to generate a continuation from the entity definition. In the second stage, the model parameters are updated so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set.
The authors' experiments demonstrate that this approach is more effective in propagating knowledge updates compared to fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. A straightforward motivation that conditioning on information about the entity can lead to lower perplexities.

2. The authors' method of generating a transfer set by prompting an LM to generate a continuation from the entity definition is a unique contribution to the field.

3. The authors compare their method with other knowledge injection methods, including fine-tuning, and demonstrate the superiority of their approach. They also conduct an in-depth analysis of the types of continuations needed in the transfer set.

4. The authors' method provides a scalable and effective way to update the knowledge of LMs.


Weaknesses:
1. As the authors concede in Section'Limitations', their proposed methodology has yet to be substantiated on models of a larger scale. For instance, LLaMA-65B may present a fitting candidate for such validation.

2. The experiments in the paper focus on a specific type of knowledge update: adding definitions for entities. It's unclear how well this method would work for other types of knowledge updates, such as knowledge revision.

3. The results of Finetuning on transfer set (full) are not shown in Table 2.

4. Writing content issues.
  (1) It would be clearer to add arrows in the table to show whether the larger or smaller values are better.
  (2) What are Finetuning on definition (full) and Finetuning on definition (last only)?


Limitations:
Yes.

Rating:
6

Confidence:
3

";1
7eW6NzSE4g;"REVIEW 
Summary:
This paper proposes an imitation learning method IMC that can model multi-modal behaviors. IMC avoids the mode-averaging issue with an objective similar to reverse-KL. To cover all modes in the dataset, IMC further introduces a mixture model with multiple components, each focusing on different data distribution it specializes to. The mixture model is optimized with the EM algorithm. The authors provide extensive experiments over simulated control environments and demonstrate IMC's superior modeling abilities.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. IMC is a well-motivated method for multi-modal density estimation and provides an elegant reverse KL-based solution.
2. The experiments in low-dimensional control environments are extensive. IMC is compared against major generative models (with maximum likelihood objective) and addresses the mode-averaging issue better.
3. The visualization of learned trajectories clearly demonstrates the learned modes of different methods and the natural mode-ignorance ability of IMC.
 

Weaknesses:
The presentation in the experiment section could be improved. Currently, the subsections for different environments repeat mostly the same conclusions that IMC achieves high success rates and covers diverse modes. I think it is better to emphasize anything special for different environments or different baselines.


Limitations:
The authors have discussed the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors study a method, Information Maximizing Curriculum (IMC), that performs behavioral cloning by having the model selectively choose a learned weighing of the demonstration data for which the model is best at predicting (via minimizing the reverse KL divergence). To avoid the mode-seeking behavior of this approach, the authors extend the method to make use of K such model components, leading to a mixture of experts (MoE) approach, whereby each component selectively models the distribution in this way, while maximizing their joint information projection over the dataset (via simultaneously maximizing the entropy of the MoE distribution). Experiments in two robotics  simulators show it outperforms several other baselines based on generative models and basic MoE methods trained via expectation-maximization and backpropagation.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is overall well written. The method and its motivations are clearly communicated.
- The authors compare to several important baselines, spanning generative models, energy-based models, and MoE methods, and show strong performance improvements.

Weaknesses:
The main weakness I see is that this work seems almost identical to Li, et al (2023), which proposes largely the same method. Moreover, this prior work looks at a very similar set of environments as this paper. While the authors cite this prior work, they do not directly compare to it. Given the extremely close similarity between the two methods, it seems important to compare to this work to justify the contribution in this paper, which in a sense, is an extension of the method in Li, et al (2023) by incorporating neural networks.

I see two main ways to show improvement over this prior work: The authors can either (1) show their method outperforms the approach in Li, et al (2023) on the tasks studied, or (2) Demonstrate clearly how IMC can scale to environments in which the method of Li, et al (2023) cannot, thereby clearly justifying the strengths of this extension. I imagine neither of these aims is too difficult, but such a comparison seems sorely missing, given the strong similarity between the two works.

Lastly, I find it odd that the authors couch their method as “curriculum learning,” ignore the field of active learning altogether, and proceed to imply that their method is novel in not requiring an a priori difficulty metric for each datapoint. The authors should relate their method to active learning, which is a mature field of study, and most active learning methods can be viewed as “curriculum learning” without any a priori notion of task difficulty.

### References

Maximilian Xiling Li, Onur Celik, Philipp Becker, Denis Blessing, Rudolf Lioutikov, and Gerhard Neumann. Curriculum-based imitation of versatile skills. arXiv preprint arXiv:2304.05171,
2023.

Limitations:
This work only focuses on fairly simple continuous control tasks. Scaling this to more complex imitation learning settings (e.g. from pixels or controlling a much more complex action space) as well as discrete control settings would add confidence in the utility of this approach. Relatedly, demonstrating success on these more challenging settings would be a great way to highlight why IMC is an important improvement over the Li, et al (2023) work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a curriculum based approach for imitation learning. Overall, the imitation learning problem is posed as a conditional density estimation problem. Given the multi-modal nature of underlying data, this paper proposes to learn a curriculum based mixture of expert policy. Intuitively, each expert is only responsible for learning a subset of the training data and learns the policy for this subset using reverse KL. Experimentally, the proposed approach is verified in 4 different environments and compared against both common and recently proposed approaches.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper presents an interesting and grounded approach for learning from multimodal data distribution. The overall idea of using a curriculum to weight data samples based on how well they are represented under the expert policy seems to be generally useful. The approach is shown to work in diverse settings and seems competitive with recent results.

Weaknesses:
*Baseline results:* Overall, the proposed approach provides very little significant advantage over simpler baselines. For instance, in Figure 4 and Figure 5 we can see that for most tasks (Pusing, Tennis, Kitchen), a diffusion model based approach (DDPM) is highly competitive to the proposed approach (success rate difference is less than 0.05) across all tasks. Only in the Obstacle avoidance task, does DDPM approach suffer although CVAE still performs quite well (unclear why DDPM performs poorly here). 

*Benchmark tasks:* While the paper tries to evaluate the proposed approach on multiple datasets, most of the tasks/datasets are not commonly used across multimodal tasks (only Franka-Kitchen is the commonly used dataset). Given the large set of recent works in this area it would be much better to evaluate on tasks which other recent works evaluate. For instance, most recent works evaluate on RoboMimic dataset [1] and the block pushing task from IBC [2]. Both of these task suites have human demonstrations available and since the underlying data distribution is highly important for these proposed approaches, reusing datasets from previously proposed approaches will allow for a much fairer comparison.

[1] Robomimic: https://github.com/ARISE-Initiative/robomimic
[2] Florence et al. Implicit Behavior Cloning

--- Post Rebuttal ---

I don't see any issues with the paper the provided code also looks reasonable and reproduces the results so I would update my recommendation to Weak Accept.

Limitations:
Limitations are discussed

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes a learning protocol that learns multiple policies (""skills""), distribution over skills, and a per-skill priority over experience buffer. This is achieved by maximizing a variational lower bound of a certain averaged regularized KL distance. Namely, the objective is a sum of two terms. The first term is a KL distance between the per-skill priority over previously seen actions and a policy, both conditioned on the skill and previously seen observation, and averaged over them. A second term is an entropy of the joint distribution over experience and skills, which acts as a regularizer, forcing it to be as diverse as possible.

In the experiment section, the skill policies share a common backbone and output a mean of an isotropic Gaussian distribution. The paper tests the method over several tasks, such as Obstacle Avoidance, Block Pushing, Franka Kitchen, or Table Tennis. The approach compares (mostly) favorably against seven baseline methods.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a simple objective to train multiple policies at once, such that together they cover multiple modes in the dataset. The results show that the method (mostly) performs better than the considered baselines.

Weaknesses:
The empirical part could be improved. Namely:
* Performance metrics should be defined in the main body of the paper:
	* There could be a dedicated section for that purpose. This would improve the exposition and allow more discussion about the method in Sections 5.1-5.4.
	* This could also make reading the results (e.g., Table 1) more accessible. For instance, the definition of entropy varies across environments, as shown in Appendix C, which makes the numbers incomparable and possibly draw wrong conclusions.
	* The discussion on the range of entropy values should be moved from the Appendix to the main body. It provides grounding for the numbers in Table 1.
	* Additional place for this and a more in-depth discussion of the results could be achieved by moving Section 2.1 and Figure 1 to the Appendix (it is a well know property of KL). Similarly, some parts of Section 3 could be moved to the Appendix.
* The paper does not provide numbers on how the algorithm mixes between skills, e.g., the entropy of the distribution over components ($p(z)$).
* It seems that each environment required a different setup of the method. This is a limiting factor for the method. The paper, however, does not guide the potential user of the method in choosing the relevant hyperparameters.  For example, estimating how many modes are in the data can be a non-trivial task, and consequently, setting the number of components ($N_z$) selected for each experiment may be non-trivial. The paper has no information regarding the actual values of $N_z$ chosen for each experiment.
* Descriptions in Sections 5.1-5.4 are mainly technical and deal with the setup. The discussion about the results of IMC is limited to one or two sentences that do not add more information than Table 1 or Figure 6. A reader would expect that most of such a section would provide real insights about the method (e.g., what skills were learned, how they were acquired during training, where all modes were discovered, how a distribution that mixes skills looks, etc.).
* Figure 6: Shouldn't we expect that the performance is an increasing function of the number of components (the more modes covered, the higher the objective)?
* There is no mention of what values $\eta$ were chosen.
* Other issues:
	* Table 1: The description is not self-explanatory. What is the selected number of components? What versions of entropy are used? What is the setup of experiments? Etc.
	* Sections 5.1, 5.2, and 5.4 refer to Table 13, which is not present in the paper. Most likely, it should refer to Table 1.
	* Figure 6 precedes Figure 5.

The quality of the technical part of the paper could be improved and more clearly agitated. In particular, 
* Section 2.2: 
	* There is no definition of $\mathcal O$, $\mathcal A$, $p$, $z$.
	* Is $z$ a continuous or discrete random variable? From the context of the following sections, it would seem that the latter.
* Section 3.1:
	* It seems more clear to write $p(o, a)$ instead of $p(\mathcal D)$.
	* In equation (2), there should be $\mathcal H(p(\mathcal D))$ (or better $\mathcal H(p(o,a))$, see the item above). Additionally $\mathbb E_{p(\mathcal D)}$ could be more transparent if written as $\mathbb E_{o,a\sim p(\mathcal D)}$.
	* The formulas in lines 80, 85, and 86 should be better justified. There is only a cryptic comment in line 81 about optimization in an alternating fashion, which is not justified, nor a suitable reference is given.
	* The description in lines 87-93 is relatively informal and does not refer to the formulas in lines 80, 85, and 86. Additionally, the text uses colloquial terms such as ""representational capacity of the policy"", ""capacity [..] is exhausted"", or alludes to the convergence of curriculum, which was not proved.
	* There is no definition of $\mathcal D_n$ (lines 85, 86, 119, 120, 124, 129, 130, etc.). Why not just write $(o_n, a_n)$?
	* In the proof of Proposition 3.1 (Appendix A.1.), the formulas read $p^*(o)$ where the Proposition refers to $p^*(z)$. A similar comment refers to other proofs in Appendix A.
* Section 3.2
	* There is no description of the objective $J(\psi)$ and its lower bound, showing how the individual terms promote desirable behavior. 
	* In equation (3), it seems that the entropy term should be equal to $\mathcal H(p(o, a, z))$; otherwise, equation (4) seems not to be valid.
	* It seems that in equation (4) and in the definition of $R_z$, there should be $q(z|o,a)$ (in place of $q(z|o)$ and $q(z|\mathcal D)$, respectively). Additionally, it would make sense to define $R_z$ as a function of $(o, a)$ (instead of $\mathcal D$).
	* Similar comments apply to Sections 3.3-3.5.

Edit: After the Authors' rebuttal, I have raised the score (4->6).

Limitations:
The paper includes a brief limitations section. What could also be mentioned is that the method requires setting several important hyperparameters (see the review), some of which require non-trivial knowledge about data. The method also was tested on continuous tasks, so a natural question (limitation or future research) would be to ask about performance on discrete domains, such as combinatorial puzzles like chess or video games like Atari benchmark.

Rating:
6

Confidence:
4

";1
dybrsuNAB9;"REVIEW 
Summary:
Previous scene flow estimation methods require complicated coarse-to-fine or recurrent architectures as a multi-stage refinement. In contrast, this paper proposes a simpler single-scale one-shot global matching to address the problem. To this end, this paper decomposes the feature extraction step via a hybrid local-global-cross transformer architecture. Extensive experiments show that the proposed method achieves SOTA performance on multiple scene flow estimation benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	This paper proposes Global Matching Scene Flow (GMSF) and achieves state-of-the-art performance on multiple scene flow estimation benchmarks.
2.	The proposed pipeline is simple and effective.
3.	The authors have provided the code in the submission.

Weaknesses:
1 Some related studies have been neglected. Please compare with the previous study [cite1] in experiments. Besides, previous studies [cite2-3] need to be cited and discussed.

[cite1] Lang I, Aiger D, Cole F, et al. SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow[J]. arXiv e-prints, 2022: arXiv: 2211.14020.  
[cite2] Li X, Kaesemodel Pontes J, Lucey S. Neural scene flow prior[J]. Advances in Neural Information Processing Systems, 2021, 34: 7838-7851.  
[cite3] Li R., Zhang C., Lin G., Wang Z., Shen C.: Rigidflow: Self-supervised scene flow learning on point clouds by local rigidity prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), IEEE Computer
Society, Los Alamitos, CA, USA (June 2022), pp. 16959–16968.  

2 Compared methods are not consistent in Table 2 and Table 3. Please clarify the reason.

3 The motivation of Eq. (11) is unclear. Please further clarify the reason why this term can be viewed as a smoothing procedure. 

4 Ablation study is not enough. I suggest the authors conduct an ablation study to Eq. (13). Specifically, two version models need to be compared, i.e., model A trained with the first term and model B trained with Eq. (13).

5 The authors need to compare the FLOPs, GPU memory, and run-time of recent scene flow methods. Although the proposed method is simple and effective, the computational cost needs to be compared.

6 It seems that tokenization is too complex and redundant. Specifically, Table 5 shows that DGCGG+PT achieves almost the same performance as MLP+PT. Therefore, I think only using PT is enough, and I suggest the authors conduct experiments to report the performance of GMSF with only PT as the tokenization process. In this way, GMSF would become more efficient.

7 Whenever any abbreviations appear for the first time, it requires a full form. For example, GMSF and LiDAR.


Limitations:
N/A

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper presents GMSF, a transformer-based method that matches dense features to estimate the scene flow from point clouds. The proposed method uses a transformer-based architecture that matches two point clouds and calculates the scene flow leveraging the cross- and self-attention modules. The paper presents experiments on FlyingThings3D and KITTI Scene Flow datasets where the proposed method consistently improves the benchmarks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
In sum, I think the solution using cross- and self-attention mechanisms are an interesting application to solve the scene-flow problem. Although, this solution has been applied to sparse feature matching (see LoFTR), I think it is also good to see it works for the scene-flow problem. Here are the details of the strengths:

S1. I find the solution an interesting application of cross- and self-attention mechanisms to solve the scene flow problem. Though in general I think the novelty is not that high since the overall idea has been used for feature matching in LoFTR CVPR 2021 [36].

S2. The clarity of the narrative is quite good. The description of the architecture and attention modules is clear. Also, the description of the losses make sense and are easy to understand. Thus, I think the paper can be reproducible.

Weaknesses:
Overall, I think while the paper shows another application of attention to compute 3D scene flow, I think the paper is lacking more thorough experiments and ablations. Here are the details:

W1. Missing ablations. First, the paper is not showing the performance impact of the parameters of the KNN component shown in Figure 2.I am sure this is also crucial since this allows the encoder to grab local information. How to set the KNN parameters? What is the effect of this parameter in the final performance? Second. What is the behavior of setting $\lambda$ to a different value? Why was $\lambda=0.9$ and what is the performance of the method when varying this parameter?

W2. Is the KITTI Scene Flow dataset challenging enough? Since this is a dataset for autonomous driving, the motion of the vehicle is mainly planar and thus limiting the motion degrees of freedoms (only 1 for rotation, and mostly one 1 for translations, since the car moves linearly most of the time). I think this greatly simplifies the complexity of finding correspondence of any type (e.g., scene flow) in these autonomous driving datasets.

W3. Lastly, I am concerned about the novelty of the approach. I think previous works have shown that attention mechanisms are useful for matching tasks in vision, and thus I struggle to find novel components or ideas in the paper. I think the paper should discuss more in depth the novelties of the paper more in detail.

----
Post Rebuttal

After reading the rebuttal and discussion with the authors, my concerns have been addressed and I will increase my rating.

Limitations:
I think limitations are stated adequately.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work aims to address the task of scene flow estimation for 3D point clouds. The authors propose a hybrid architecture based on local-global-cross transformers. Given as input a source and a target point cloud, first, the local transformer extracts geometric features within a patch, then the global transformer analyzes each point cloud individually using self-attention to capture the overall context, and finally the cross transformer exchanges information between the point clouds to generate the final feature representation for each point. The scene flow is predicted by performing pointwise matching with the cross similarity matrix, while occlusions are handled through a self-similarity matrix applied to the predicted scene flow. To evaluate the effectiveness of their approach, the authors conduct experiments on two benchmarks for scene flow estimation, demonstrating better performance compared to existing methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
-  This work introduces a straightforward architecture for scene flow estimation, utilizing transformers. Without bells and whistles, the authors show that the proposed network can produce discriminative per-point features that can be robustly matched for scene flow computation.  
-  The authors conduct extensive experiments on the FlyingThings3D and KITTI Scene Flow benchmarks in different preprocessing and occlusion settings. The results suggest that the proposed method has significant performance improvement on FlyingThings3D, while also performing competitively on KITTI Scene Flow when compared to existing methods for generalization test.  
-  The paper is overall well written and easy to follow.

Weaknesses:
-  One concern for this work is its limited technical contributions: the hybrid network uses the well-established transformer architecture, while the scene flow is estimated by a common probabilistic point matching approach. One seemingly interesting proposal is the occlusion handling with the self-similarity matrix. However, it lacks in-depth explanations for why it helps with occlusion handling, and its ablation study is also missing in Sec. 4.5.
-  In the generalization test on KITTI-S (Tab. 3), the proposed method exhibits a performance gap compared to PT-FlowNet [8]. More detailed analysis and explanation would be helpful for gaining a better understanding of this discrepancy.
- For the ablation study in Tab. 4, it is unclear whether the performance saturates with eight global-cross transformer layers, and whether more layers would be beneficial or not. To provide a comprehensive assessment, it would be good to include comparisons of runtime and memory usage, as the backbone is built upon transformers, which may not scale well with more points, and the paper mentions that input point clouds are resampled to 8K points.
-  Minor: L267, “Although we don’t” => Note that we do not


Limitations:
The authors discussed limitations at the end of the paper.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a hybrid local-global-cross transformer scene flow estimation model, achieving the state-of-the-art results on FlyingThings3D and KITTI Scene Flow datasets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This work has a mature model design based on Transformers, achieving superior results on the Flyingthings3D and KITTI Scene flow datasets.

2. The structure and writing of this paper do not have major issues.

Weaknesses:
1. Scene flow estimation has been developed for many years, and the autonomous driving industry has already introduced Occupancy and Flow Prediction techniques. Researchers should not be limited to designing a toy model solely to maximize the benchmarks on FlyingThings3D and KITTI Scene Flow. These two datasets have dense point clouds and clear correspondences, which are far from practical applications. For this paper, I hope the authors can conduct more experiments on the Waymo motion data (https://waymo.com/open/data/motion/).

2. Undoubtedly, Transformers will bring more powerful feature modeling capabilities and higher latency. How does the latency of this method compare to competitors? Please add this item to the main experimental table.

Limitations:
The limitations has been discussed.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper proposes GMSF for scene flow estimation from point clouds.
As far as the authors are aware, GMSF is the first to address scene flow estimation with global matching - GMSF is formulated as a single-scale one-shot global matching.
GMSF incorporates a novel local-global-cross transformer architecture to extract high-quality feature representation, to finally compute the scene flow between point clouds via global matching. 
GMSF outperforms existing methods on F3D_c, F3D_o, F3D_s and KITTI_o, while performing competitively on KITTI_s. 
The ablative results show that increasing the number of global-cross transformer layers - thus increasing the capacity - is beneficial to the performance, and that the presence of local information is crucial for the performance of GMSF.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The paper is overall well written and easy to follow.

* The proposed architecture, and the motivation behind it, is intuitive and novel. While the architectural novelty itself is not entirely new (e.g., the sequence of local and global attention has first been introduced by SuperGlue[1] and LoFTR[2] for 2D image matching, and has been applied to 3D point cloud registration through methods including CoFiNet[3] and GeoTransformer[4]), its application to the task of scene flow seems conceptually new.

* Strong performances on the standard benchmarks of scene flow estimation. 

[1] PE Sarlin et al., SuperGlue: Learning Feature Matching with Graph Neural Networks, CVPR 2020
[2] J Sun et al., LoFTR: Detector-Free Local Feature Matching with Transformers, CVPR 2021
[3] H Yu et al., CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud Registration, NeurIPS 2021
[4] Z Qin et al., Geometric Transformer for Fast and Robust Point Cloud Registration, CVPR 2022

Weaknesses:
* Insufficient ablative experiments. What if the number of transformer layers increase to a number higher than 8? The given results show that the performance improves gracefully with the number of layers, and it naturally leaves the question to 'until how much'. Also, what if the global transformer and cross transformers are decoupled, such that they can have varying number of layers? 

* Lack of latency and computation analyses. The authors emphasize that GMSF is a single-scale, one-shot method; then how does it compare to existing methods in terms of latency and computation (FLOPs, memory)? 

* Lack of analysis. How does incorporating global-cross transformer layers improve the performance, and how does incorporating **more** layers **further** improve the performance? This has been partially answered by Table 4, but a visual comparison / analysis would be more convincing.

* Lack of mention of 3D point cloud registration methods. While the task at hand is different, the architectural design and motivation is closely related to 3D point cloud registration, which I believe is therefore worth mentioning in the related work section. Also, it might be a bit of an overstatement to mention that GMSF is the 'first' to address scene flow estimation with global matching, as scene flow estimation and point cloud registration are seemingly closely related tasks .

Limitations:
The authors have addressed the limitations of GMSF in the paper. 

Rating:
5

Confidence:
5

";1
GhNCFtLSsy;"REVIEW 
Summary:
First, I must emphasize that I was new to the RL community before this NeurIPS. This is the first RL paper I have reviewed. I have been struggling to understand this paper. I tried my best. I hope the AC, reviewers, and authors may think me helpful.

This paper studies how to transfer a pre-learned RL system to new tasks using successor features and the optional keyboard. Compared with previous methods, new things are as follows: 1) the Q-value is no longer linear about the task encoding, here, the authors use a network $g_{\theta}$ to predict the weight of SF features based of task encoding and state representation. 2) The SR approximation is modeled as the expectation of a discrete distribution, the authors claim this can help stabilize the training. 3) The learning objective is modified. The new objective consists of three parts a) a term to decrease the prediction error of reward using $\phi$, b) a term to decrease the prediction error of reward using the difference of $\psi$, and c) a term to maximize the possibility of current $\psi(a_t,s_t,w)$ to occur using the twohot operation. 4) A new transfer scheme that learns a new task encoder and state function at transfer time, where the task encoding is the weighted sum of a Bernoulli random variable and is aware of the time. The proposed method achieves better performance than USFA and MSFA in 3D playroom dataset, showing a better success rate. 



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
# Novelty
Above all, I may not be able to judge the novelty of an RL paper. So a comment based on my experience in my familiar domain.

1. The new assumption of SF and Q-Value: It is good to introduce $g_{\theta}$, but the Q-value is still a linear combination of the SF. So I think this is still somewhat narrow. 
2. New SF approximation: In my opinion, the key advantage is that the distribution distributes the difference between $\psi(a_t,s_t,w)$ and $\psi(a_{t+1},s_{t+1},w)$ to $m$ different points. So each point only needs to change modestly when the time progress, and the similar features among different times can be better stored by those points of high density among times. Correct me if I am wrong here. This seems a good point, but the authors lack enough explanation here.
3. New Learning Objective: I don't understand why the authors omit $\mathcal{L}_{\psi}$ in training, the new objective seems to have no term to align the difference of $\psi$ and the cumulant $\phi$. The authors also do not provide enough explanation to why this objective is designed, nor any ablations to explain this.I have a hard time understanding this part.
4. New transfer scheme: This seems a novel point. The authors again use distributions to model the task encoding. This form may be of stronger approximation ability.


# Clarity
1. I like the Summary of Challenges in Fig.8, it helps quickly figure out the clues of this work.
2. The authors give enough reference to key concepts, this helps me a lot in reviewing this paper.



# Significance
Good in the showed case, but narrow in the scenario.

Weaknesses:








# Clarity:
One of my major concerns is the clarity of this work. It seems like it was done in a hurry. The notations are used carelessly, usually not unified. I can see the same symbols with different superscripts or subscripts, or arguments. The components of the proposed method lack enough explanation of why they are helpful. Those make me restless, can't sleep overnights and recall them in dreams. I suspect several places use wrong notations, correct me if I am wrong. Some issues of clarity are as follows:
1. Line 112 you miss an ""of"".
2. It is $r_t^{\tau}$ or $r_t$ in the second eq. of Eq (4)?
3. I recommend to write Eq. (7) as $\mathrm{Twohot}(y_t^{\psi_k})\log p_{\psi_k}^T$ for clarity.
4. Not every fig of this paper has an error bar.
5. Notations are a bit confusing and frequently unexplained. I have a hard time understanding them.
6. What does $\theta^o$ mean in Eq. (6\&7)?
7. Righthand side of Eq. (7) seems not a scalar?
8. Line 213, what does the loss function mean? Why do you design it? Also, is the state function also trained by it?

# Significance
1. The improvements are not significant.
2. Environment is a bit narrow. Why only in 3D cases?
3. The authors only study one dataset, making the result not strong enough.
4. Limited ablation studies. No ablation about the loss, the new task encoding scheme, or the new SF approximation. The showed two ablations, in my opinion, are about minor issues.


Limitations:
Limited scenarios. The computation burden seems to be increased compared with previous methods. The networks can be harder to train due to complicated designs. Computing approximations from sampling of distributions can also heavy the burden of computation both in training and inference.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The author(s) present a novel and efficient approach to knowledge transfer in long-horizon tasks, aiming to minimize interactions with the environment (less frames required in training). Their proposed method leverages Categorical Successor Feature Approximator (CSFA) within an enhanced deep OK network, resulting in several tasks (e.g., Find, Place).

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Proposed  a dynamic transfer query that enables sharing among task-encoding and SF-approximators;
- Implemented this methodology in long-horizon tasks.


Weaknesses:
1. The authors did not clearly position their contribution in relation to other approaches commonly used in long-horizon tasks, such as hierarchical approaches, hindsight approaches and meta-learning approach [1-3].
2. While the overall results of the experiments are significant (e.g., Figure 3 improve 0.2 ~ 0.8 in success rate), the description and documentation of some experiments might be unclear or insufficient. 1) USFA or MSFA might require fewer computational resources compared to MSFA, as their calculations appear to be simpler (line 246). However, without further detailed information, it is difficult to make a definitive judgment. I suggest that the authors include memory and computational load comparisons among these methods. 2) The author's comparisons are focused on MTRL, but there is a lack of comparison with other approaches mentioned in references [1-3]. It would be beneficial to include comparisons with other relevant approaches mentioned in reference [3].
3. From the paper, it is unclear why MSFA is considered sharable or suitable for fitting different tasks. How it compare with other pre-training encodings or other method representations.
4. Some typos, e.g., ""Transfer tasks are conjunctions of known known tasks"", line (289)

[1] Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors, NeurIPS 2020

[2] Learning to Reach Goals via Iterated Supervised Learning, NeurIPS 2020

[3] Skill-based Meta-Reinforcement Learning, ICLR 2022

Limitations:
- This paper could benefit from an expanded evaluation of the transfer learning quality beyond just the success rate metric, e.g. adding visualizations of the baseline task encoding and dynamic query encoding to show the benefit of dynamic query is scalable.
- This study lacks of discussion about the upper bound of the number of tasks it can effectively handle during sharing coding. The author primarily tests the approach on two main tasks (""find"" and ""place"") with several sub-tasks or steps required to complete each main task. However, it remains unclear what the upper limit number of main task and sub-tasks?

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper focuses on the Option Keyboard (OK) task which was recently proposed as a method for transferring behavioral knowledge across tasks. They propose a new method of Deep Option Keyboard (Deep OK), which enables transfer with discovered state features and task encodings. To enable discovery, they further propose the Categorical Successor Feature Approximator (CSFA), a novel learning algorithm for estimating SFs while jointly discovering state features and task encodings. With Deep OK and CSFA, this paper achieves the first demonstration of transfer with SFs in a challenging 3D environment where all the necessary representations are discovered. The experiment results show that CSFA can discover SFs compatible with SFGPI and Deep OK achieves strong jumpstart performance for transfer to combinations of training tasks.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	Transfering with SFGPI in a large-scale multi-task setting, discovering cumulants and task encodings, while sharing a task encoder and SF approximator across tasks gives some insights to the community and seems novel.
2.	The paper’s writing is generally clear and the figures were helpful for understanding.
3.	The problem formulation is clear and the approach is well-reasoned.
4.	The experiments are sufficient and convincing to some extent.


Weaknesses:
1.  Though SFGPI is dynamic and can share a task encoder and SF approximator across tasks, its generalization ability is not clear and no more discussion can be found in the paper. Maybe a more general method that sampling queries from the task encoding space that is more fairly concentrated could be designed and at least provide some discussions.
2. In Figure 3, CSFA discovers representations compatible with GPI across short and long task horizons. The longer horizon is defined by just one “Place Near” task. Will CSFA outperform MSFA and USFA on other general tasks?
3. Deep Ok doesn’t reach an optimal performance but uses far fewer frames compared with Distral.


Limitations:
See the weakness and question sections.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work focuses on attempting something which no previous work has done before;  learning task encodings, cumulants and successor features (SFs). The author(s) proposed a solution based on learning these three functions jointly. They considered a model that learns the SFs as probability mass function using the two-hot representations inspired from MuZero. They named this model as the Categorical Successor Feature Approximator (CSFA). With the learned SFs, the authors will then be able to perform Generalised Policy Iteration (GPI) to obtain the policy for the relevant task at evaluation phase. In order to find a better policy for this task, the authors also introduced a method named Deep Option Keyboard (DOK), which relies on a parameterised function to output coefficients which are then used as preference vectors. The models are evaluated in a 3D environment, using pixel observations. The tasks included going to certain locations and collecting and then dropping objects at different locations. In particular, DOK is used to study the effects of transfer in sparse reward and long horizon tasks. The proposed model is able to perform better than the baselines such as Universal Successor Features and Modular Successor Feature Approximators.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
I find the work interesting as this is an attempt at a challenging problem of learning task encodings, cumulants and successor features jointly. The figures, in particular, figure 2, is clear in aiding the reader to understand the architecture of their model. The writing overall was mostly clear and easy to read. The experiment's descriptions are concise and helped me to understand the setup a lot even though I am not familiar with the playroom environment. The authors also performed ablation studies which showed that the key to their model is learning the SFs using a probability mass function, rather than the canonical expected values.



Weaknesses:
Although there are no major weaknesses in my opinion, I do have some points for thought.

Firstly,I am not sure it is a good idea to call the mechanism for learning the preferences, “Deep Option Keyboard” when it seems that there are no options involved. 

Secondly, it would also perhaps be a good idea to see how this model compares with the standard Q-learning algorithm, such as Rainbow or even just a distributional RL model since the proposed model uses a probability mass function to get an idea how much of the transfer benefits one could achieve from the utilization of the successor features.

Thirdly, there was some mention of static vs dynamic query. Perhaps it will be helpful to give a clear definition of what it means by static and dynamic query. 


Limitations:
None

Rating:
8

Confidence:
4

";1
wNpsGwixjG;"REVIEW 
Summary:
The authors design robustness-aware quantization (RAQ) to speed up the noise estimation network by leveraging the robustness of early-stage diffusion models. Specifically, the authors found that the quality of generated images is less affected by the early-stage. Therefore, they reduce the bitwidth of activations for the early-stage, and maintain high-bit activations for the later-stage. Experiments show that the proposed method can speed up early computations while maintaining generation quality.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The idea of the paper is simple yet effective, promoting the application of Post-Training Quantization (PTQ) in the diffusion model.
2. The analyses in the paper are extensive. The authors demonstrate the early-stage robustness through entropy transition across steps (Fig. 2) and noise injection. 
3. The paper is well-organized and easy to read. 
4. The authors also provide the code for results reproduction, showing the solidness of the work.

Weaknesses:
1. In Tab. 1, RAQ only sets different bitwidths in five intervals, which is inconsistent with Algorithm 1, which sets different bitwidths in each step. Some explanation is needed.
2. The paper only reduces the activation bitwidth on the base of Q-diffusion. However, compared with LDM-4, the FID of Q-diffusion increased by 1.21 about LSUN-Bedrooms (256x256) in Tab. 1. Therefore, the activation bitwidth should not be reduced only. It is better to further apply RAQ to weight bidwidth to obtain a better trade-off between performance and efficiency.
3. The authors propose RAQ to accelerate the early-stage computation. However, the running time, FLOPs, and model size are not provided to demonstrate the effectiveness of the proposed method.

Limitations:
The authors do not discuss its limitations or potential negative societal in separate section. It is better if the authors add some discussion.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes to quantize diffusion models to a different extent along the iterative process for image generation. The main motivation of the proposed approach is that diffusion model is robust to input distortion at early stages (i.e. noisy stages) of the iterative process. Therefore, the proposed approach starts with a 4-bit quantization, and gradually increase activation bits along the iterations. Experiments show that the proposed approach achieves improved performance with the same effective bitwidth.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper has a good motivation. The empirical experiments show that it is legitimate to apply different rates of quantization at different stages of different diffusion process.

2. The proposed method effectively improves the performance with a reduced bitwidth, as shown in Table 1.

3. The idea is simple and is easy to follow.  

Weaknesses:
1. From Table 1, the bitwidth for each timestep is model-specific. That means optimization has to be done for each model. It would be good to have analysis on the robustness of the bitwidth selection.

2.  As one of the main objectives is to improve the sampling efficiency, the comparison of runtime should be included. This is important for readers to understand the improvement brought by the proposed method.

Limitations:
Please consider discussing the limitations and potential societal impact of the proposed approach in the paper. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents robustness-aware quantization (RAQ), a novel strategy to use mixed precisions for activations when quantizing diffusion models. The authors found that inaccurate computation during the early stages of the reverse diffusion process has minimal impact on the quality of generated images, and propose to use low-bit activations for the early reverse diffusion process while maintaining high-bit activations for the later stages. Experiments have been conducted for both unconditional and conditional generation using latent diffusion and stable diffusion on various datasets.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper is well-structured and presents a clear motivation for leveraging the robustness of early-stage diffusion models to use lower-bit activations at those time steps to further improve the computation efficiency.
- Experimental results show that the proposed method can use lower precisions for early-stage computation without sacrificing the quality of the generated images. 
- The experiments with stable diffusion indicate the effectiveness of the proposed methods on text-to-image applications.

Weaknesses:
My biggest concern with the proposed RAQ approach is its practicality. The method suggests using low-bit activations for the early denoising process and high-bit activations for the later stages. However, the paper does not provide sufficient arguments on how this varying precision can be efficiently implemented and how much additional benefits it can bring compared to the simple W4A8 cases. In real-world applications, changing activation precisions could introduce complexities in designing and implementing corresponding kernels for different stages of the process, as the weight precisions need to be always upcasted to the activation precisions when performing the compute on conventional GPUs (e.g. the compute will always be WyAy for WxAy precisions, where x=4 and y>=4 for the settings discussed in the paper). Consequently, this could limit the practical utility and impact of the RAQ approach. An analysis of the theoretical speed up or memory saving should be done to show that changing activation precisions for early stages can indeed bring substantial improvements in compute efficiency (so the extra efforts for kernels implementation can be justified), and providing some additional simple experimental results will be preferred.

Limitations:
The authors adequately addressed the limitations.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The author initially notes that errors in the early stages of the reverse diffusion process result in minimal disturbance to the final generated image. As a solution, they suggest employing low-bit activations for the initial reverse diffusion process while preserving high-bit activations for the subsequent stages, in conjunction with PTQ.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The idea is clear and easy to understand

- The proposed RAQ method outperforms the other methods such as Q-diffusion


Weaknesses:
- Could you the authors explain how is the entropy calculated and why higher randomness in the pixel values will cause the images blurrier?


- Cpmparison to other methods. The authors mentioned two PTQ methods PTQ4DM and Q-diffusion, but only provide quantitative and qualitative comparison to baseline and Q-diffusion.


- In section 3.2, it seems obvious that add the same amound of noise to a noisier image will have less influence than to a less noiser image?  

- And the authors did not explain why in figure 3, the performance on two different dataset are so different.

Limitations:
See weaknesses.

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this submission, the authors propose a novel approach to speed up the noise estimation network by leveraging the robustness of early-stage diffusion models. Specifically, they present an algorithm to modify the quantization bit width according to the diffusion step. The proposed method shows positive results in reducing activation bits below 8 bits.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
•	The writing of this manuscript is easy to follow, and the illustrations are clear.

•	This work is well-motivated. Based on the analysis, the authors provide insights on the different roles that different diffusion steps play and show the room to improve the PTQ process by treating early and later steps differently.

•	The experiments show positive results of the proposed method.


Weaknesses:
•	The real-world benefits of reducing activation bits. With advanced samplers, the sampling steps of diffusion models are significantly reduced, e.g., to 50 steps or lower. Thus, the gain achieved through low bit width calculation in the early steps may be marginal in real-world evaluation. On the other hand, bit width is usually a power of two. To my knowledge, some execution cores are designed to process 8-bit-only or 4-bit-only data. Irregular bit widths like 6 bits are treated as standard bit widths by padding zeros. Thus, the benefits of reducing to irregular bit widths (e.g., 6 bits) instead of standard bit widths (e.g., 4 bits) are questionable from the perspective of hardware. The authors are encouraged to provide real-world evidence of the benefits of RTQ or a discussion of the above concerns.

•	The choice of FID threshold. In the RTQ algorithm, the choice of FID threshold is critical since it determines the final bit width dictionary and thus the quantization gain. How do you set this hyperparameter for a new dataset?


Limitations:
The choice of FID threshold in the RTQ algorithm is unclear.

Rating:
4

Confidence:
4

";1
KTRwpWCMsC;"REVIEW 
Summary:
This paper proposes a novel method for estimating uncertainty in prediction of time series. Modern Hopfield Networks is used as a method of measuring uncertainty based on the existence of similar historical data. This paper presents its construction method and shows that it works for multiple data and forecasting methods

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The authors have clarified the issues with CP for time series and proposed a method to improve this point by applying MHNs, and experiments have shown very good results.

Weaknesses:
My four points of concern are as follows:
- This paper consists only of a method proposal and an experimental evaluation, and does not include theoretical effects. In this structure, is an evaluation on four data sets sufficient to demonstrate the effectiveness of the proposed method? 
- The task setting of this paper seems to be related to Gaussian processes and Bayesian optimization as well. This point seems to be a natural one for many people, and while it may not be directly comparable, would it be appropriate to make no mention of it?
- The proposed method incorporates MHNs, a function that calculates the reliability of forecasts from past information, into CP. The results shown in the experiment indicate the validity of the CP issue and the effectiveness of the improvement direction. However, the experiments did not provide justification for introducing HMNs to solve the problem.
- Identification of issues and presentation of solutions is an important contribution, but it is a combination of existing methods, and I feel it is weak in terms of novelty.

Limitations:
The authors do not explicitly address Limitation. On the other hand, the study is oriented toward reducing the social impact on the limitations of conventional methods and does not promote adverse effects.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The work presents HopCPT, a conformal prediction algorithm for Modern Hopfield Networks in the setting of time series forecasting. The work uses a novel (attention-like) weighted quantile approach to leverage similar time steps from the past, improving both coverage and sharpness compared to SOTA CP methods. The proof mainly extends from  (Foygel Barber et al., 2022) and (Xu and Xie 2022). The method is shown to perform well on both synthetic and real world datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Novel and sound CP algorithm on a widely-used architecture for forecasting. 
- Excellent efficiency performance on time series datasets, with through comparisons to existing methods. 
- I also appreciate the use of proper scoring rules for evaluation (Winkler score). 

Weaknesses:
- From the paper alone, it's difficult to tell what exactly are the theoretical guarantees of your algorithm. While the authors did refer to Appendix B, I think it is important to briefly describe the assumptions and state the main bound (B.8 and B.9) of your theoretical analysis in the paper. 

Limitations:
- It might be good to stress that the guarantee is marginal and asymptotic, which can be achieved by rather simple algorithms (see Gibbs & Candes 2021, 2022). This is of course due to the properties of time series data, not a weakness of the approach.



Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper deals with providing conformal prediction (CP) sets for the time series prediction problem or, more generally, the cases where the exchangeability assumption is violated. Similar to the prior work of Foygel Barber et al. (2022), they also try to assign weights to the points in the calibration set with higher (lower) values for those who are coming from a more (less) similar region (aka regimes) as the test point. For such a weighting mechanism, they use Modern Hopfield Networks (MHN) that are capable of finding regions similar to the point for which a prediction set is sought. Using numerical experiments, they showed the superiority of their proposed framework.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is indeed well-structured, nicely written, and easy to comprehend. 
The extensive amount of numerical experiments supports their claim about the (empirical) superiority of their proposed framework over the vast range of competitors.

Weaknesses:
The contribution of this paper seems to be relatively marginal. Indeed, the authors introduced a new way of weighting the non-conformity scores (i.e., reforming the distribution of non-conformity scores) on top of the NexSCP framework proposed by Foygel Barber et al. (2022) while adopting the structure proposed by Xu & Xie (2022a) for interval constructions.  

While the numerical results show their framework's (empirical) superiority in terms of efficiency and coverage, one should keep in mind that it adds relatively high computational complexity to the model. This is, of course, also reported in Appendix A.6.

Limitations:
The authors mention a limitation of the proposed framework: the need for large data for HopCPT to perform well. However, there is still room for elaboration on other limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a conformal prediction approach for black box time series forecasting models. The method is based on learning to predict the model’s errors using a small neural network structured similarly to the softmax computation in an attention mechanism. The prediction interval of the proposed method utilizes the softmax scores in the error sampling procedure, drawing each error with the probability given by the softmax score. Both in synthetic and benchmark experiments, the proposed method outperforms other CP approaches in terms of PI-width and coverage gap. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* The manuscript is extremely well structured and very easy to follow. The authors provided an excellent overview of the conformal prediction task and relevant work. 
* The experiments provide a thorough insight into how the proposed method functions in a synthetic and more realistic setting. Figure 2 contrasts the proposed with competing methods in a comprehensible and straightforward way. 
* In addition to a solid theoretical motivation, the proposed method performs outstandingly and convincingly on a wide range of prediction models and several data sets. 


Weaknesses:
* Without a background on Hopfield networks, the method seems an application of the attention idea to conformal prediction. It would be helpful to lay out the similarities and differences between MHNs and the attention mechanism. 
* I am missing a section on the limitations of the proposed method. For instance, how does the method scale to very long time series? 


Limitations:
While there is no dedicated limitation section, the authors provide “negative results” in the appendix. A concise paragraph about the method’s limitations would be appreciated. 

Rating:
7

Confidence:
4

";1
g9gjpFOiO4;"REVIEW 
Summary:
Edit: Updating score from 6 t o7 based on the discussions.

Extracting useful representations from unlabelled data for label efficient training of medical image segmentation task is a widely studied problem. This work approaches learning useful representations using contrastive learning (CL) within a semi-supervised setting. Strategies for obtaining variance reducing pixel partitions for CL are presented, along with theoretical analysis that show their variance reduction properties. The variance reduction estimation is also used to improve training stability and convergence. Comprehensive experiments on multiple medical imaging and computer vision datasets are performed showing strong performance improvements compared to other SSL methods. The authors show their method is label efficient across all the datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Focusing on variance reduction guarantees to extract contrastive samples for pixel level training is a strong contribution of this work. 

* The theoretical analyses showing their unbiasedness and use of variance reduction techniques to improve training stability can have important applications in related domains like self-supervised learning.

* The  experimental evaluation is extensive, with strong performance improvements on multiple datasets.

* Contrastive loss landscape visualisation in Fig 3 is insightful; that the contrastive learning holds across datasets is quite convincing.

Weaknesses:
* **Robustness**: This work makes two main claims about the usefulness of their CL framework. Firstly, and convincingly so, about label efficiency. There are several places in the paper where model robustness is alluded to, or strong claims made, without any evidence. This could be because the authors view robustness simply to be good performance across multiple datasets? If so, this should be clarified. Currently, the claims about model robustness are misleading. See [1,2,3] for different robustness analyses of deep neural networks.

* **Assumption in Th. 3.2**: The guarantees of $Var[H_{SG}] < Var[H_{NS}]$ only holds if different $P_m$ do not have the same expected value over the aggregation function h(x;p). How is this ensured? In medical images, there are scenarios where the differences between classes are small in both intensity and feature spaces? How does the variance reduction guarantees hold in such situations? 

* **Aggregation functions**: Aren't the aggregation functions some type of a distance measure? And why is it expensive to compute these on dense pixel grids (L213)? 

* **Main method in Appendix**: While I appreciate all the details presented in this paper, moving the main method to the Appendix is not a good idea. Several of the important details are in the Appendix and does not serve the purpose of what Appendices are supposed to be.

* **Literature overview**: I was curious as to why the authors refrained from discussing self-supervised representation learning both when motivating the work, and also in their general discourse. 

[1] Bastani, Osbert, et al. ""Measuring neural net robustness with constraints."" Advances in neural information processing systems 29 (2016).

[2] Carlini, Nicholas, and David Wagner. ""Towards evaluating the robustness of neural networks."" 2017 ieee symposium on security and privacy (sp). Ieee, 2017.

[3] Singh, Gagandeep, et al. ""Fast and effective robustness certification."" Advances in neural information processing systems 31 (2018).

Limitations:
Authors do not discuss any limitations of their work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors present ARCO, a novel semi-supervised contrastive learning framework that employs a stratified group sampling strategy (i.e. **SG** and **SAG**) to compute gradient estimators with reduced variance, thereby enhancing representation learning in dense contrastive learning. By improving dense contrastive learning, ARCO addresses issues related to class imbalancedness and enhances the performance of semi-supervised segmentation, particularly in scenarios with long-tail distributed anatomical classes. The authors provide theoretical evidence demonstrating the effectiveness of the proposed sampling techniques in reducing the variance of the aggregation function, specifically the contrastive loss. The efficacy of the proposed sampling technique is validated on eight 2D/3D benchmark datasets with different label settings, further reinforcing its effectiveness and practical applicability.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:

The motivations are clear, and the method is reasonable. This study shows me a new insight/direction to consider medical image segmentation. The following are my detailed comments.

---

**Empirical contribution**

Pixel/voxel-wise sampling constitutes a critical facet of contrastive learning at the pixel/voxel level. With the aid of variance-reduction estimation, the authors proffer two pragmatic approaches - Stratified Group (SG) and Stratified-Antithetic Group (SAG), tailored for pixel/voxel-level segmentation tasks with exceedingly scarce labels.  

* The authors introduce a novel framework termed ARCO (strAtifed gRoup COntrastive learning) devised for multi-class segmentation tasks. This framework appears to be both intriguing and efficacious. The authors undertake a rigorous validation of the proposed methodologies across eight benchmark datasets, encompassing three 2D medical image segmentation, two 3D medical image segmentation, and three semantic segmentation benchmarks.  

* The empirical results, both quantitative and qualitative, attest to the efficacy of the proposed model across all label ratios and datasets. For instance, the model demonstrates a marked enhancement in segmentation accuracy (up to 4.1% absolute improvements in Dice coefficient) on the challenging multi-class MMWHS dataset under a 1% label setting.  

* The authors conduct comprehensive ablation studies to substantiate that the proposed mechanisms merit consideration. These studies encompass eight benchmark datasets, diverse network architectures, and varying label ratios to validate the efficacy, model-agnostic nature, and label efficiency of the proposed methodology.  

* Lastly, the proposed methodologies are not only facile to implement but also boast of universal applicability. For instance, they can be seamlessly integrated into any scenario necessitating pixel/voxel sampling.  The paper presents a robust and versatile framework for pixel/voxel-level contrastive learning, which is empirically validated through extensive experiments and ablation studies. The methodologies are characterized by ease of implementation and broad applicability, making them a valuable contribution to the field of image segmentation.

---
**Theoretical contribution**

* The proposed methodologies, SG (Stratified Group) and SAG (Stratified-Antithetic Group), have exhibited remarkable efficacy in the experimental study. Consequently, it is intriguing to ascertain whether theoretical insights can elucidate this enhancement in performance. To this end, the paper furnishes a cogent theoretical analysis of the methodologies, revealing that the variance-reduction attribute of the two sampling methods is instrumental to their performance.

* First and foremost, in Section 3.3, the paper meticulously delineates the SG and SAG sampling methodologies through lucid mathematical equations. SG is executed by segregating pixels into mutually exclusive groups, followed by uniform sampling of a specified number of pixels from each group. SAG, which is predicated on SG, imposes an additional constraint of symmetry among the sampled pixels within each group.

* Subsequently, the sampled pixels are amalgamated through an aggregation function, which acts as an estimator for the target quantity. In the realm of image segmentation, this quantity could be, for instance, the contrastive loss function. It is posited that an optimal balance must be struck in the sample size; an overly diminutive sample size may fail to encapsulate the salient information from the underlying image, whereas an excessively large sample size would entail high computational complexity. Thus, the ideal scenario would be for SG to capture the crux of the image information through a relatively modest sample size. The paper demonstrates that SG possesses this attribute by establishing that it achieves reduced variance in comparison to the naïve sampling method (i.e., uniform random sampling from all pixels). Specifically, Theorem 3.2 establishes that SG is an unbiased sampling methodology, with a variance that does not exceed that of naïve sampling. More precisely, the variance of SG can be decomposed into the variance of naïve sampling minus a non-negative term. This non-negative term is conjectured to be almost certainly greater than zero, as it would be zero only if the expectation of the aggregation function within each group is identical to the expectation over the entire image, which is highly improbable. Thus, Theorem 3.2 suggests that SG is likely to consistently outperform naïve sampling.

* Figure 5 reveals that SG/SAG exhibits marginally expedited training convergence compared to naïve sampling. This observation is theoretically substantiated in the concluding paragraph of Section 3, which is commendable.

* In my assessment, the theoretical analysis presented in the paper is inextricably linked to the empirical component and provides a persuasive rationale for the empirical performance augmentation of SG vis-à-vis naïve sampling, thereby bringing the empirical narrative full circle.
---

**To sum up**

**1. Clarity** 

The manuscript is eloquently composed, proffering a lucid and cogent progression of information. The authors adeptly elucidate the procedural framework, facilitating the readers' comprehension of the proposed methodologies, namely, the two instance sampling methods - Stratified Group (SG) and Stratified-Antithetic Group (SAG). In Section 3.3, the SG and SAG sampling methods are meticulously delineated through precise mathematical formulations. The discourse within the Methodology section (Section 3) furnishes an exhaustive exposition of the innovations introduced by the study, adeptly accentuating the distinct contributions of the research to the scholarly domain.

**2. Novelty** 

This work is the first work, empirically and theoritically, to validate the variance-reduction approach within the context of pixel/voxel-level contrastive learning for semi-supervised medical image segmentation, particularly in scenarios characterized by a paucity of labels.

**3. Experimental Comprehensiveness** 

The authors undertake a comprehensive suite of experiments encompassing eight medical datasets, which include both 2D and 3D modalities, as well as semantic segmentation benchmarks. Additionally, a diverse array of contrastive learning frameworks and varying label ratios are employed to rigorously assess the efficacy, model-agnostic properties, and label efficiency of the proposed methodology. This extensive experimental evaluation substantiates the robustness and versatility of the technique in the domain of medical image segmentation.

**4. Theoretical Implication** 

The authors furnish a cogent and meticulously articulated theoretical analysis of the proposed approach, elucidating the underlying principles with clarity and precision. This analytical exposition contributes to a deeper understanding of the methodology's foundations and its implications.






Weaknesses:
The proposed method is very interesting. There is no obvious weakness in the proposed ARCO. However, I do have a few questions. See the following section.


Limitations:
The authors have addressed the potential broader impact in clinical scenarios.


Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes two new sampling strategies, SG and SAG, for contrastive learning in semi-supervised frameworks. Compared with randomly sampling pixels for contrastive learning, pixels are grouped into several subsets, and then pixels are sampled from each subset. The proposed method is proven to reduce the variance of sampled pixels. Solid experiments are conducted to support the above claim.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Solid experiments and good performance.
2. Simple yet effective method that benefits contrastive learning for semi-supervised frameworks.
2. Sound theoretical analysis.

Weaknesses:
1. It would be better to also compare NS with the proposed SG/SAG in natural image datasets.
2. What is the application scenario of SAG? SG seems to have better performance and stability than SAG.
3. Line 222-223, what is the meaning of ""p is orthogonal to p'""?

Limitations:
The authors have included a discussion on the potential negative societal impact.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a sampling strategy to improve contrastive-learning-based medical image segmentation performance with limited labeled data and training stability. The sampling strategy is used in conjunction with a previously-published contrastive semi-supervised training strategy. The main contributions include this sampling strategy, some theoretical support that the strategy should improve training stability, and experiments evaluating the method.

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
- The baseline comparison experiments are thorough.
- The authors address a real applied problem (that most medical image segmentation datasets have limited data) with a new method and theoretical support, resulting in a well-rounded paper. 


Weaknesses:
Major weaknesses
- The paper’s writing is a major limitation. The prose is difficult to understand, which limits the entire paper—it is hard to clearly understand the motivation, the proposed method, the contributions, or the benefits. The work would benefit from more rounds of grammatical revision; it is hard to parse what the author is trying to communicate a lot of the time.
- Partly due to the writing issues, it is unclear how the proposed sampling strategy differs from previous dense contrastive learning-based image segmentation strategies; it seems that the methodological contributions here are minor, if existent.
- The method is complex, consisting of two backbones (connected via EMA), global/local instance discrimination losses, augmentations, supervised losses, nearest neighbor loss, global contrastive loss, unsupervised loss… with all of these components, a very thorough ablation section is needed to understand how much the novel component (a pixel-level sampling strategy) is contributing to performance. The existing ablation section is not so thorough. As a result, the paper does not contribute much understanding about the strengths/weaknesses of different components of this complex pipeline.

Minor weaknesses 
- MONA is not a well-known training strategy; it would be useful to provide a longer overview on what MONA does and how the proposed approach differs. This discussion could go in an appendix.
- The same concept is often referred to using different words: model “convergence,” “robustness,” and “stability.” I’m not sure if you’re always talking about the same concept, or if you are using different words to refer to the same idea. If the latter, it helps the reader to always use the same word.
- This is a minor stylistic note, but the use of bold and italics is often distracting.


Limitations:
Limitations and impacts adequately addressed.

Rating:
4

Confidence:
4

";1
haniyY7zm1;"REVIEW 
Summary:
The paper provides theoretical results characterizing the generalization capabilities of methods based on Wasserstein distributionally robust approaches. In particular, the results presented extend the conditions under which the performance guarantees are not affected by the curse of dimensionality and are applicable for general classes of models.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper shows that the usage of Wasserstein radius of the order 1/sqrt(n) can provide generalization bounds in situations more general than those considered in existing works (linear models). In addition, the results presented also cover regularized versions of WDRO. The more general results are obtained using a novel type of proof based on a concentration bound for the dual problem, which is of independent interest.

Weaknesses:
The paper contribution with respect to the state of the art needs to be better described. In particular, the extension to non-linear models of the scaling 1/sqrt(n). 

The problematic dimension-dependent scaling arises in Wasserstein methods while other techniques based on robust risk minimization have been shown to provide performance guarantees with the scaling 1/sqrt(n). It would be good if the authors describe this fact and the related work. 

If I am not mistaken, examples 3.6 and 3.7 correspond to cases for which the right scaling was already proven in previous works. In order to better assess the paper's contribution, it would be good if the authors discuss interesting examples for which the paper provides the right scaling while existing results cannot. 


Limitations:
The paper adequately describes the limitations of the methods proposed, mostly in terms of the specific assumptions needed for the results to hold.


Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents generalization bound for Wasserstein DRO and entropic regularized Wasserstein DRO (or called Sinkhorn DRO in Wang et al.) formulations. Those generalization bounds do not suffer from the curse of dimensionality. The theoreical analysis is also supported by two examples in Section 3.4. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The theoretical analysis is interesting from two aspects. First, the authors reveal that the radius selection of WDRO to make the empirical robust loss  dominate the true loss does not suffer from the curse of dimensionality. The analysis follows different techniques from existing literature such as Gao et al, Blanchet et. al, etc. Second, the technique is general enough so that it also applies to entropic regularized Wasserstein DRO (or called Sinkhorn DRO in Wang et al.) formulations. This is the first work that investigates the statistical properties of such formulations.
- The authors also present two examples in machine learning to demonstrate the technique assumption holds and the proposed theoretical analysis applies.

Weaknesses:
- The writing of this paper could be potentially improved:
 1. There should be a comma in Eq.(1), or equation between line 83-84, or Eq. (4), or equation between line 194-195, or equation between line 301-302. 
 2. There should be a period in Eq. (10).
 3. The contribution and related work part in the introduction section should be separated.
 4. It would be a little bit confusing to first introduce KL-divergence regularized WDRO risk in Eq.(5-6) and then introduce it corresponds to the Sinkhorn ambiguity set in line 194-195. The authors should put them together in Section 2.2
 5. The notation could be potentially improved. For example, in Eq. (7) the authors use $\hat{\mathcal{R}}$ to refer to the risk based on empirical distribution $P_n$. I would suggest replace the notation $P_n$ with $\hat{P}_n$ for consistency. Further, in Eq. (7) I think the authors are meaning $\rho$ should at least scale in the order of $\sqrt{(1+\log(1/\delta))/n}$, then why not write $\Omega(\sqrt{(1+\log(1/\delta))/n})\le \rho$ instead of $O(\sqrt{(1+\log(1/\delta))/n})\le \rho$? The same applies for equation between line 199-200.

- It is great that the authors present statistical analysis for entropic regularized Wasserstein DRO. I would suggest the authors add some explanation or numerical example to demonstrate the benefit of introducing entropic regularization. Will it bring extra benefits than standard WDRO?

- The analysis is limited to quadratic cost function, which could be restrictive. From my own trial and reading, I think the major difficulty for generalization is that, it is difficult to apply Laplace approximation technique for general p-th power of norm function. In other words, it is difficult to obtain the p-th power of norm counterpart of Lemma A.3 and Lemma G.1.If so, I suggest the authors add explanation for the difficulty of extension.

- Some literature is missing. For example, readers may wonder why consider adding entropic regularization to WDRO problem and what is the applications? I suggest the authors make the following revisions:
   1. update reference [J. Wang, R. Gao, and Y. Xie. Sinkhorn distributionally robust optimization. arXiv preprint arXiv:2109.11926, 2021] as [J. Wang, R. Gao, and Y. Xie. Sinkhorn distributionally robust optimization. arXiv preprint arXiv:2109.11926, 2023]. In the updated version, the authors demonstrate that people can find $\delta$-optimal solution to general entropic regularization WDRO problem with complexity $\tilde{O}(1/\delta^2)$. So one major benefit of adding entropic regularization is the computational tractability;
  2. add several application papers brought by entropic regularization WDRO in literature review:
     (i) Dapogny, Charles, et al. ""Entropy-regularized Wasserstein distributionally robust shape and topology optimization."" Structural and Multidisciplinary Optimization 66.3 (2023): 42.
     (ii) Song, Jun, et al. ""Provably Convergent Policy Optimization via Metric-aware Trust Region Methods."" arXiv preprint arXiv:2306.14133 (2023).
     (iii) Wang, Jie, and Yao Xie. ""A data-driven approach to robust hypothesis testing using sinkhorn uncertainty sets."" 2022 IEEE International Symposium on Information Theory (ISIT). IEEE, 2022.
     (iv) Wang, Jie, et al. ""Improving sepsis prediction model generalization with optimal transport."" Machine Learning for Health. PMLR, 2022.

Limitations:
N/A

Rating:
7

Confidence:
5

REVIEW 
Summary:
This work proves generalization guarantees for Wasserstein DRO models that only require the radius of order $O(n^{-1/2})$ under mild assumptions for general classes of models. This provides concentration results that do not suffer from the curse of dimensionality.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The theoretical contribution is the main strength. The empirical concentration of Wasserstein distance suffers from the curse of dimensionality, and this paper is able to prove the results (under some assumptions) that do not have this curse of dimensionality issue and provide statistical guarantees on the performance of WDRO solutions.

Weaknesses:
There is no significant weakness in this paper. Nevertheless, I think adding some discussion or examples for which the assumptions and thus the results in this paper do not hold can be beneficial; it can show failure cases and may also motivate future directions for the extension.

Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper provides generalization guarantees of Wasserstein DRO for a general class of functions, in which the radius scales as $1/\sqrt{n}$ and does not suffer from the curse of dimensionality. Moreover, these guarantees hold for any distribution in the neigbourhood of the true distribution, so that they still apply when the distribution shifts at testing time. The results in this paper hold for both constrained and regularized version of Wasserstein DRO. The authors also provide a proof sketch that explains the main ideas and techniques used in the proof, and apply their results to logistic and linear regression.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. This paper provides novel generalization guarantees such that the robustness radius does not suffer from the curse of dimensionality. To the best of my knowledge, the results in this paper are novel and make a non-trivial contribution to the DRO community. Moreover, the authors consider the regularized version of Wasserstein DRO and provide similar guarantees as well.

2. Most parts of the paper are well-written. The necessary backgrounds are clearly explained, and theorems are accompanied with detailed explanations of related definitions and concepts.

Weaknesses:
In Section 3.4, the authors considers logistic and linear regression as applications of their theorems. It would be better if more complicated and popular parametric models can be included in this section to justify the main assumptions.

Limitations:
This paper does not have potential negative societal impact.

Rating:
7

Confidence:
2

";1
iGmDQn4CRj;"REVIEW 
Summary:
The authors show that simply tuning standard hyperparameters provides state-of-the-art performance on a wide variety of class-imbalanced datasets, which may be surprising and give an impact to the community: We have to re-think the experimental settings for performance evaluation on imbalanced datasets. 

The authors show that simply tuning existing components of DNNs, such as the batch size, data augmentation, architecture size, pre-training, optimizer, and label smoothing, can achieve state-of-the-art performance without any specialized loss functions or samplers. 
Specifically, (1) imbalanced data prefers small batch sizes, (2) the data augmentation strategies that achieve the best performance on balanced datasets yield inferior performance on imbalanced datasets, (3) large models that do not overfit to balanced datasets strongly overfit to imbalanced datasets of the same size, (4) adding an self-supervised loss during training can improve the performance on imbalanced datasets, (5) the SAM optimizer improves minority accuracy, and (6) label smoothing prevents the overfitting to minority classes.

Thorough experiments are performed on six image datasets, including natural image, medical, and remote sensing datasets as well as two tabular datasets. The models are a variety of CNNs, XGboost, and SVM. The authors run five seeds for each evaluation and report the mean and one standard error. The performance improvement compared with state-of-the-art (e.g., [Zhou+, CVPR2023, http://home.ustc.edu.cn/~zzp1994/2023068462.pdf]) is significant (especially on CIFAR-10 and -100).

The authors conclude that, even though they attain the state-of-the-art, existing methods designed for web-scraped natural image classification benchmarks do not always yield improvements on other real-world problems. If we are to reliably compare methods for class imbalance, we need a more diverse benchmark suite, i.e., there may not a universally good model for imbalanced datasets.

The paper is well-written and easy to follow. I enjoyed reading the paper. Contribution is clear. Experimental settings are well-explained. Distinctions from previous works are clearly stated. Overall, the contribution and impact of the paper is significant compared with other papers that have been published in top conferences.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The authors show that simply tuning standard hyperparameters provides state-of-the-art performance on a wide variety of class-imbalanced datasets, which may be surprising and give an impact to the community: We have to re-think the experimental settings for performance evaluation on imbalanced datasets.

- Experimental results are convincing. Thorough experiments are performed on six image datasets, including natural image, medical, and remote sensing datasets as well as two tabular datasets, which are sufficient compared with other contemporary papers published top conferences. The models are a variety of CNNs, XGboost, and SVM. The authors run five seeds for each evaluation and report the mean and one standard error. The performance improvement compared with state-of-the-art (e.g., [Zhou+, CVPR2023, http://home.ustc.edu.cn/~zzp1994/2023068462.pdf]) is significant (especially on CIFAR-10 and -100).

- The paper is well-written and easy to follow. Contribution is clear. Experimental settings are well-explained. Distinctions from previous works are clearly stated. Overall, the contribution and impact of the paper is significant compared with other papers that have been published in top conferences.

- There are some papers that claims that supervised contrastive learning is effective for imbalanced learning, e.g., [Wang+, CVPR2021, https://arxiv.org/abs/2103.14267]. There is also a seminal paper that analyze self-supervised learning and class-imbalance [44], in which Liu+ claim that pre-trained self-supervised representations are more robust to imbalance than supervised representations. To my understanding, the authors' approach is somewhat similar to (but not exactly the same as) these two, i.e., adding self-supervised loss to supervised training and mitigating the pre-training phase (Joint-SSL). The authors show that this approach is effective on imbalanced datasets, which is a consistent result with the two references above, although Joint-SSL only is not always helpful or the improvement is marginal (Tables 1 & 2).

Weaknesses:
- In this paper, hyperparameter tuning is shown to be important for performance on imbalanced datasets. THEREFORE, I would like to see ALL the hyperparameter settings and tuning method of ALL the experiment. I would like to see the code to reproduce the experimental results.

Limitations:
- As stated by the authors, the optimal learning configuration depends on the task, and thus it may be unclear how universal the know-how obtained in this paper is.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper studies the long-tail recognition problem and the impact of existing components of standard deep learning pipelines on the generalization performance, such as the batch size, data augmentation, architecture size, pre-training, optimizer, and label smoothing. They find that simply tuning those components can achieve state-of-the-art performance without any specialized loss functions or samplers.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Rethinking the effect of existing components in long-tailed recognition is a very important research topic.
- It is interesting to show that the minority class performance decreases at some point with the increase of the model size for imbalanced data.
- Extensive experiments are conducted on small-scale datasets.

Weaknesses:
- The batch size experiment in Fig. 1 is not very convincing. What is the training method used? Is it ERM? Does this result hold across different methods? If it is ERM, I wonder what if you do cRT afterwards, will the same conclusion (data with a high degree of class imbalance tends to benefit from smaller batch sizes) hold?
- For data augmentation, it is also not clear what method is evaluated. Besides, data augmentation improves more on the minority classes is not new knowledge IMHO. There are even papers designing augmentations that strengthen the minor classes specifically, e.g. MFW [1] and TFE [2]. Also, to what extent does the claim ""AutoAugment emerges as the most effective for imbalanced data"" hold? Does it hold across different datasets, architectures, and training methods? From Fig.7, it is very convincing if AutoAugment outperforms TrivialAugment significantly since the error bar is not reported. 
- Model architecture: I'm not sure that balanced and imbalanced accuracy are ""virtually uncorrelated"" is well supported as the performance difference between methods in Fig.3 right is very small. How stable are the results in Fig.3 right? 
- What was the performance of ERM without the ""tuned routines""? What is the performance improvement of each component in the refined ERM (batch size and data augmentation)? I think showing this ablation can be very helpful in understanding the paper.

[1] Procrustean training for imbalanced deep learning, ICCV 2021

[2] Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning, CVPR 2022

Limitations:
- Since ImageNet has also been used as a standard dataset for long-tailed recognition for quite long time, I wondered whether the authors had any results on that to verify if their findings scale beyond CIFAR-10/100.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents different approaches to enhance the performance of neural network classifiers over imbalanced datasets. Unlike most research focusing on specialized loss functions or resampling techniques, this study promises that state-of-the-art performance is achievable over the neural classifiers by simply tuning existing components of standard deep learning pipelines. Section three suggests that the study is evaluated over six image datasets and two datasets from UCI Machine Learning Repository, validated over ResNets and WideResNet classifiers, and compared with nine standard baselines. The reported evaluation measures include overall test accuracy and minority and majority class accuracy over the 20% of classes with the smallest and highest number of samples. The key observations from the study are -- (a) small batches perform better for the imbalanced data, (b) the role of augmentation as a regularizer is undeniable, but the performance over the minority class is sensitive to the chosen augmentation policy, (c) the larger networks are more susceptible to overfitting minority class in the case of imbalanced data, (d) models pre-trained on larger datasets tends to perform well in this case, (e) the integration of self-supervised loss with the supervised learning referred to as Joint-SSL performs well, (f) Sharpness-Aware Minimization (SAM-A) pulls decision boundaries away from minority samples, (g) whereas standard training routines overfits, and (h) applying more smoothing to minority class examples than majority class examples prevents overfitting on minority samples. Furthermore, integrating these findings with Joint-SSL and SAM-A atop M2m (SOTA) establishes new state-of-the-art performance across all (class-imbalanced) benchmark datasets, CIFAR-10, CIFAR-100, and CINIC-10. Some other important observations made in the study are (i) SGD optimizer performed uniformly better on imbalanced data, (ii) training on data that is more balanced than the testing distribution did not improve representation learning by preventing overfitting to minority samples, (iii) a low correlation was observed between the performance over the web-scrapped (benchmark) datasets and real-world datasets, and (iv) a correlation was observed between neural collapse and low test accuracy. Overall, this study emphasizes that simply tuning standard training routines can significantly improve performance in the case of imbalanced datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This study approaches the significant and challenging problem of imbalanced learning.
2. The paper does extensive study and finds that state-of-the-art performance is achievable by finetuning the different components of existing deep learning pipelines. 
3. It is interesting to investigate the imbalance problems using the proposed solutions, which are easy to understand.

Weaknesses:
1. The paper has bold statements at the beginning (resolving class imbalance in general) but has yet to explore the class imbalance issue in textual datasets. 
2. The evaluation metrics used to assess the proposed solution revolve around accuracy.
3. Comparison with SMOTE is not reported but is mentioned as a baseline.
4. The dataset statistics, reflecting the nature of the imbalance in the dataset, need to be included.

Limitations:
1. The paper does not propose the class imbalance issue in general.
2. The paper references other research work for experimental setup in many instances, which could have been avoided.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors study the problem of class imbalance. To be specific, they first investigate the effects of different hyper-parameters & design choices in an imbalanced setting. Moreover, the authors use such optimized settings with existing methods to show that significant improvements can be obtained.

After the rebuttal:
The authors addressed my concerns with detailed explanations and additional experiments. With those, I believe the paper provides useful insights for people working on class imbalance, therefore, I vouch for the paper's acceptance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Easy to follow text.
2. Class imbalance is an important problem and therefore, analyses that offer more insights are valuable.

Weaknesses:
1. The most important issue I see with the paper is that its contribution is limited. 

1.1. Out of the 6 Lessons (take-away points) in Introduction, Lessons 2-6 are already known in the literature. Only the analysis on batchsize is novel (as far as I know) and the results are intriguing.

1.2. The application of the tuned setting with the other methods does not offer any contributions. 

2. More challenging datasets such as ImageNet-LT, iNaturalist are missing.

3. Regarding Lesson 1:

3.1. Lesson 1 (batchsize): The paper does not provide any intuition as to why batchsize has such an effect in an imbalanced setting.

3.2. Lesson 1: It would be worthwhile to see the same analysis with different architectures because, as we see in Figure 3, different architectures exhibit different behaviors under imbalance.

4. If I may, I suggest the authors to focus only on batchsize and provide solid & theoretical insights about why/how it affects.

Limitations:
None.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The authors suggest tackle the class imbalance issue from a hyperparameter optimization perspective. Throughout an extensive empirical study, they raise questions about the behavior of well-established techniques for balanced data under long-tail data distribution.  The synergy of the resulting prescriptions is also checked on several datasets

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* This paper introduces a new approach to tackle the class imbalance issue by optimizing several hyperparameters. The paper is clearly written,  The claims are  well-stated and sufficiently  supported empirically in most of the cases. 
* The authors tried to sketch intuitive explanations for some 'unexpected' outcomes 
* The authors reported the performance of several combinations of their micro-receipes and show strong results on several datasets.

Weaknesses:
* The content of this paper is clearly going beyond the maximum allowed number of pages.  To not violate the rules, the authors decided to move very relevent parts of the paper to the appendix which makes the paper less readable. Nevertheless, I insist that this action does not compromise the clarity. I would have been nicer to cut the less important components and limit the scope of the paper to the most important hyperparameters.    
* The novelty of this paper is limited some 'findings' are valid for the balanced setup as well (pretraining, SSL) or just trivial (AutoAugment) 
* For a purily empirical paper, it is important to diversify the architectures in order to draw more valuable conclusions. I would have loved to see the performance of a transformer based model under different contraints


Limitations:
none

Rating:
4

Confidence:
5

";1
CSbGXyCswu;"REVIEW 
Summary:
This paper proposes *FINE-GRAINED RLHF*: an extension to (the now popular) *reinforcement learning from human feedback* (RLHF) that allows incorporating more granular human feedback into the language model (LM) training process. Typically, RLHF leverages reward/preference models that are trained to output a *scalar* reward for the whole generated sequence, and this *reward sparsity* can hinder improvements for long-sequence generation. Furthermore, scalar rewards do not offer insights into which parts of the sequence were problematic or displayed undesired behaviors.

To address this, the using *fine-grained reward models*: for each ""category"" of feedback of interest, a separate reward model is trained. Furthermore, these models generate feedback at lower granularity (for example, at sentence-level). The policy LM can then be optimized with respect *weighted* sum of the rewards at each timestep, using standard RL algorithms like PPO.

The authors explore the impact on two language generation tasks: (1) detoxification and (2) long-form question answering.
For detoxification, the authors find that (with a reward based on an external API) training with rewards provided for every sentence yields better performance when compared to a holistic sequence-level reward. However this evaluation is fully automatic.
For long-form question answering (QA), the authors introduce a novel dataset, QA-Feedback, accompanied with fine-grained human feedback on initial policy LM. The feedback comes in three error categories at different levels of granularity: (a) irrelevance, repetition or incoherence (sub-sentence level), (b) incorrect or unverifiable facts (sentence level), and (c) incomplete information (whole sequence level). They collect this feedback through crowd-workers. They also collect *preference-based* feedback, for comparison with (hollistic) RLHF.
By training separate reward models for each category, the authors show that fine-grained RLHF outperforms the traditional RLHF and supervised baselines, both in automatic and human evaluation. Interestingly, by carefully adjusting the weights of different reward models, the authors show LM behaviors can be fine-tuned to optimize different combinations of desired behavior, and outperform. The authors also explore the reward model accuracy and even compare against a ChatGPT baseline.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I really enjoyed reading this paper. It explores a very prominent topic recently (learning from human feedback), it is well written and it does an very extensive analysis (at least for one of the tasks it tackles). “Reward sparsity” is a well known problem in RL, and recent works in RLHF for LLMs just kinda ignores it. Their proposed fine-grained RLHF is seems like a good approach to tackle it, makes intuitive sense and seems easy to implement. Their findings surrounding controllability could also have significant implications for (RLHF’d) LLMs. They also introduce a novel dataset for QA with fine-grained feedback.

Weaknesses:
The main weakness of this paper was that only a single LLM was tried: while I believe that their findings will generalize to other LLMs, an experimental validation of this would make this paper even stronger. Even trying re-using the reward models trained for the initial GPT-2 policy on some other LLaMA would already be quite a bonus. 

Also a couple of other minor ones:

- I think task 1 (detoxification) in general is not a good setup: in uses closed source APIs for model-based (from a relatively weak one) PPL as evaluation. the simulated “sentence-level” reward is also not very elegant. I would ideally suggest trying to replace it with another simple, controllable generation task and preferably using human-eval. But ye most of these are covered by task 2
- The multi-reward aspect of the algorithm makes RLHF now a sort of *multi-task* optimization problem, and bring with the problem of how to weight the different categories. The discussion on the trade-off in (4.5) is interesting and helpful, but I think more could be said on how to pick a weighting for a general case where we just want a single “quality”. Also a very simple pareto analysis (probably trying more weightings of the reward) and some plots showing the final trade-off (ideally in terms of human reward?) would add more technical insight

Limitations:
See above

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies how fine-grained RLHF improve over holistic preference RLHF.
The study is conducted on two tasks and fine-trained reward comes from sub-sentence, sentence, and full sequence level annotations on different feedback types (toxicity, relevancy, factuality and completeness).
Study show that fine-grained reward improved over holistic preference and demonstrated LM behaviors can be customized using combinations of feedback types.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is clearly written with good visual aid, has sufficient details on datasets, reward model training, descriptions on experiment hyperparameters and frameworks.
- The paper demonstrates benefits of fine-grained reward model on two tasks, with various reward density and reward types. It is convincing to see  it works across very different problem settings.
- There is clear and concise analysis on reward model and human rating agreement to understand the quality of reward model.
- Experiment sections contains relevant ablations that supports main results and analysis of how different types of reward model works together.

Weaknesses:
- (minor) in the detoxification analysis, I'm not fully convinced that the gain comes from reward, it is possible that the API has better accuracy annotating shorter sequence. Some additional analysis similar to your reward model human agreement analysis on task2 could help explain it better. 
- I didn't find the mention of reward model size which I assume would be the same as the value model of t5-base which is smaller than the policy model. In previous works there is advantages of using larger than policy network value/reward function. It might be important to show what is the effect of reward/value model size on fine-grained reward vs holistic reward.
- (major) on task2, there lacks analysis comparing F.G.RLHF vs Pref.RLHF on holistic preference eval. It would be a lot more convincing if there is gain in that eval. Otherwise it could be argued that these different approaches are optimizing toward very different signals.
- the comparison ChatGPT using ROUGE score: it is a reference-based score that might not correlated well with human preference.  Using ROUGE score itself to compare these systems are not sufficient or even detrimental. Your eval with rel/fact/comp and the holistic preference are much better metrics.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work improves the reinforcement learning from human feedback (RLHF) with more fine-grained reward signals.
For the feedback density, the authors suggest to reward the policy model at sub-sequence level.
The experiments on the detoxification task shows that the fine-grained rewards can outperform the holistic sequence level ones.
For the feedback diversity, they set three reward categories: 1) **rel.**, irrelevance, repetition, and incoherence, 2) **fact.**, incorrect or unverifiable facts, and 3) **comp.** information completeness.
The first two signals, i.e., rel. and fact., are with the sub-sentence and sentence level density, and the comp. is for the full sequence.
A new QA-FEEDBACK dataset with fine-grained error annotations and pair-wise preference ranks (for the baselien preference-based RLHF) is constucted for the training and evaluation.
Compared with the preference-based RLHF, the suggested fine-grained one behaves better on all three error types in both human and automatic evaluation.
This framework also demonstrates the promising ability of customizing LM behaviors by adjusting the weights of different reward models.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. This work suggests a new perspective to RLHF for LM, where fine-grined and denser reward signals can benefit LMs. 
2. The proposed fine-grained RLHF framework is simple and effective, validated via thorough experiments and analysis.
3. A new dataset is contributed for the research community, which is carefully created.
4. The paper is well-written and easy to follow.

Weaknesses:
1. The models used in this paper are relatively limited in size, only including large and base models. I'm curious how your method would scale to larger models, such as the more commonly used 13B and 30B models, and what kind of performance it would achieve. 
2. LM customization is interesting, but lacks human evaluation. I suppose that incorporating human evaluation could help to further understand this.

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in different error categories. Through experiments on detoxification and long-form question answering, the authors demonstrate how learning with this FINE-GRAINED reward function leads to improved performance, supported by both automatic and human evaluation. The paper's contributions include a new framework for fine-grained reward learning, a new dataset for long-form question answering, and experiments demonstrating the effectiveness of the proposed approach. Though, the proposed method still has some limitations in terms of computational cost, algorithm complexity, and task-binding. In all, it has some contributions to the RLHF field.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The paper introduces a new framework for fine-grained reward learning, which is a novel approach to language model training. The authors also provide a new dataset for long-form question answering, which can be used for future research in this area.
- The paper is well-written and clearly presents the proposed framework and experimental results. The authors provide detailed descriptions of their methodology and evaluation metrics, which enhances the quality of the paper.
- The proposed framework has the potential to improve the performance of language models by providing more fine-grained feedback during training.
- The idea is simple but reveals the potential of RLHF at finer grains.

Weaknesses:
- I know that data labeling is cumbersome and expensive, but the 3,853 training data collected in this work is still a bit small, not sure if this is enough to draw the conclusions mentioned in this work.
- The proposed method is a bit complicated, it will introduce more computational cost in RLHF, and it will introduce more hyper-parameters for fine-tuning.
- The design of T_i^k is not fully explored, which may have a big impact on the performance of the proposed method.

Limitations:
- The training dataset is not large enough (especially for the Long-Form QA task), which limits the verification of a conjecture: As the size of training data increases, the effect of the FG-RLHF proposed in this paper will weaken.
- The proposed method is expensive, not only in the additional computational cost of the reward model, but also in the different deﬁnitions of ﬁne-grained feedback in terms of the feedback types and the density level of each feedback type.

Rating:
6

Confidence:
4

";1
vO04AzsB49;"REVIEW 
Summary:
This paper considers the problem of offline imitation learning with supplementary data with optimality not guaranteed. The paper gives theoretical analysis on the performance gap bound between expert policy and learner's policy for behavior cloning (BC) on expert data only and naively using BC over the union of expert and supplementary data. Based on the analysis, the paper proposes a provably better method than BC, which is ISW-BC. ISW-BC uses importance sampling (with a lower threshold) between state-action pairs to ""correct"" the learning from non-expert state-action occupancy onto expert state-action occupancy. In several mujoco and Atari environments, ISW-BC works comparably well or better than the SOTA methods, DemoDICE and DWBC. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
**1. Good writing that is easy to follow and clearly conveys the idea.** The paper is a math-heavy one with many pages of theoretical proofs; however, the core result is well-summarized by Tab. 1, the theorems, and Eq. 3-5. Besides, for readers that are interested in theoretical results with function approximators, the theorems are kindly summarized in Sec. E in the appendix, leaving the long proof details in the next section. The limitations, broader impact and computational resource are all well-discussed. 

**2. Simple but effective idea.** The idea of ISW-BC proposed by the paper is very simple; it only requires separate training of a discriminator and an actor, which is quite easy to implement. However, such method is theoretically guaranteed to be better than BC and is indeed better than many baselines in multiple environments.

**3. Solid theoretical and practical results.** The theoretical results clearly shows that how bad could it be when we are treating non-expert data as expert data in behavior cloning (BC), and how the quality (value function) of the non-expert data affects the result. Based on this, the author proposes ISW-BC, a method that is theoretically proved to be better, and indeed achieves superior performance over multiple baselines (DWBC, DemoDICE) on multiple environments (Atari, mujoco, and even non-RL tasks).









Weaknesses:
**1. The proposed method, ISW-BC, might be non-robust to stochastic environment.** Consider a simple tabular MDP with five states $s_{begin}, s_1, s_2, s_{success}, s_{fail}$; 

the agent always begin at $s_{begin}$, and there is only one action for $s_{begin}$, which has 50% probability of leading to $s_1$ and 50% probability of leading to $s_2$; 

there is only one action for $s_1$ that 100% leads to $s_{success}$, and two actions for $s_2$ that 100% leads to $s_{success}$ and $s_{fail}$ respectively;

 $s_{success}$ is the success state that terminates the episode with $+1$ reward; $s_{fail}$ terminates the episode with $-1$ reward.

Now, consider the scenario where we only have one expert trajectory (1-shot is a common case) that goes from $s_{begin}$ to $s_1$ and finally $s_{success}$. The supplementary data acts uniformly random. By definition, the discriminator now gives a close-to-zero ratio for $d_h^E/d_h^U$ with any history that involves $s_2$ (let us ignore the numerical stability issue for now because there are engineering solutions). DICE methods have an equivalence of value function (which is the Lagrange dual function), which can guide the agent back from $s_2$ to $s_{success}$. ISW-BC, however, append no or very little weight to supplementary data on $s_2$, and, because expert has never experienced $s_2$, does not know what to do on $s_2$.

**2. It strikes me as a little strange how we ""justify"" the use of weight in imitation with imperfect data from theoretical analysis**, because the story of the paper seems to be improve over NBCU (later proved to be empirically better than DICE/DWBC), but the NBCU analyzed in the paper is too unintuitive to ever work; it is natural, rather than with theoretical analysis for one to know that (s)he cannot treat non-expert data that is arbitrarily bad as expert ones. **(Despite of this, I am still convinced that the theoretical analysis on NBCU is a notable contribution.)**

**3. Other minor problems:**

a) Besides the empirical advance on offline IL mentioned in the paper, there are more theoretical advances in offline IL (more specifically, the unification of offline IL and RL), which are MAHALO [1] and ReCOIL [2]. I encourage the author to briefly discuss them in the related work section.   

b) The color of the curve for each method should be unified throughout the paper. For example, the color of ISW-BC in Fig.9 and Fig.10 in the appendix are not unified and might mislead the readers.

  

**References:**

[1] A. Li et al. MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. In ICML, 2023.

[2] H. S. Sikchi et al. Imitation from Arbitrary Experience: A Dual Unification of Reinforcement and Imitation Learning Methods. In ArXiv, 2023.

Limitations:
**Limitations:** The authors have discussed the theoretical limitations in line 280 and line 746-750. I think, however, there are more limitations that the authors could consider to add, and concentrated in a single limitation section:

1) non-robustness to stochasticity (elaborated in point 1 of the weakness section);

2) the assumption that $d^U$ covers $d^E$, which is also a weakness that DICE possesses but still a concern in practical use.

Note these limitations do not necessarily mean that the work is not valuable enough for the conference; however, they do pose concern for readers who want to apply ISW-BC in the future.

**Potential Negative Societal Impact:** The paper does a good job in discussing the broader impact at the beginning of the supplementary material.



Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper provides derivations of the imitation gap, the gap in performance between the trained agent and expert who provided the data, when traditional behavioural cloning (BC) is used. The specific setting assumes there is plentiful of supplementary data to train the BC agent on, but since this supplementary data may (and probably is) of poor quality (i.e., not as good as expert demonstrations), traditional BC produces less than optimal agents. The paper proposes a importance-sampling correction by training a discriminator on the dataset (to distinguish the high quality and low quality demonstrations), and then train BC using the importance-sampling correction. The results indicate the proposed method improves over BC and existing methods, while not reducing original BC performance in settings with high amount of expert data.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Theoretical backing and derivation of the method.
- Both theoretical and empirical improvements over baselines, and proposed method is more applicable than baselines (e.g., no natural extension of DemoDICE to image recognition task, as it lacks rewards).
- Empirical results in three different settings (MuJoCo, Atari and object recognition).
- Method is simple to implement. With a good baseline code base shared, I could see other people adapting this method and trying it out.

Weaknesses:
- Proposed method has rather small/noisy improvements in terms of metrics in the experiments.
	- In ""noisy expert"" setting, the proposed method is clearly better than the baselines. However this setting seems rather unrealistic (proper trajectories from a policy but actions are random). A more realistic scenario would be rollouts from a poorly trained policy, or a random agent.
- Not a very novel setting (as evident by the number of baselines) and the solution, while well executed, is a combination of existing works in somewhat simple way.
- Training discriminators may be problematic (which is a shared difficulty with baselines). Authors note this in the Appendix for the Atari experiments.
- (Minor) No code available, but paper lists references to libraries and datasets used. Nevertheless, replicating the results as presented in the paper will be near-impossible, given the earlier works in ML field. I urge authors to share the code, even if it is ""messy"", so that others can build on the contributions of this work.

Limitations:
Authors have broader impact section, and correctly report the limited societal impact. Authors list some of the limitations of the method in Appendix, e.g., difficulties regarding training discriminator in the Atari domain.

## Rebuttal acknowledgement

I have read authors' rebuttal and new results which did address my concerns, and I updated my score from 6 to 7 (before discussion period closed).

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of offline imitation learning (IL) with a supplementary dataset, which can address the scarce expert data issue in pure IL. In this setting, the challenge is that the supplementary dataset may have out-of-distribution samples. This paper considers the classical method Behavioral Cloning (BC) and its variants, and proves their imitation gap bounds in offline IL with a supplementary dataset. The theoretical results show that the naïve BC on union dataset (NBCU) method suffers a non-vanishing gap, and thus may be worse than BC which only learns from the expert dataset. To address this issue, the authors propose the method Importance-sampling weighted BC (ISW-BC), which can select in-distribution samples in supplementary dataset. They prove that ISW-BC can eliminate the gap in NBCU. The experimental results also show that ISW-BC outperforms existing methods on a variety of tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper conducts a systematic theoretical study of offline IL with a supplementary dataset. The developed theory closes the gap between theory and practice and lays a foundation for further studies of this problem.

2. This paper proposes a simple and effective method ISW-BC. The authors validate that ISW-BC can address the distribution shift issue in both theory and practice, which makes advances over existing methods.

Weaknesses:
1. This paper is a bit dense to read. I believe that this paper would benefit from providing more intuitions and proof sketch for the theoretical results. Besides, the authors should give more analysis of the experimental results, which can give the reader an intuitive idea about how and where the proposed algorithm improves upon existing methods.

Limitations:
The authors have discussed the limitations and broader impacts of this paper in the conclusion part and appendix.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In the paper, the authors focus on imitation learning (IL) when working with supplementary imperfect data. They conduct a thorough theoretical analysis to understand the limitations of IL under various dataset compositions. The authors' theoretical analysis provides insights into the bounds and constraints of IL when dealing with different types of datasets.

To address this problem, they propose a novel method called importance-sampling behavior cloning ISW-BC. The proposed method is designed to mitigate the issues associated with imperfect data in IL. This technique leverages importance sampling to assign appropriate weights to different samples, thereby effectively reducing the impact of imperfections in the training data.

To validate the effectiveness of their approach, the authors conduct extensive evaluations on a diverse set of tasks. The results indicate that the proposed method outperforms the current state-of-the-art techniques in most cases. This suggests that the importance-sampling behavior cloning method is a promising solution for tackling the problem of imitation learning with supplementary imperfect data.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
One strength of the paper is the authors' meticulous exploration of the various theoretical bounds that arise when working with imperfect data within the framework of BC. By dissecting these limitations, they provide a deep understanding of the challenges faced in practice, enabling researchers and practitioners to make more informed decisions when applying BC to real-world datasets.

Furthermore, the authors introduce a novel method based on importance sampling, which offers a clear and intuitive approach for addressing the imperfections in the data. 

In addition to their theoretical contributions, the authors demonstrate the practical relevance of their proposed method by conducting a thorough analysis on diverse tasks. This empirical evaluation validates the effectiveness of their approach across various application domains, further strengthening the paper's findings.

Weaknesses:
The authors' analysis lacks consideration of alternative methods that can effectively learn from imperfect data. [1]

In order for the method to be applied, a dataset of labeled expert demonstrations is required. In many practical applications, we do not have access to this information.

[1]Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations,Daniel S. Brown, Wonjoon Goo and Scott Niekum, CoRL 2019

Limitations:
See Weaknesses

Rating:
6

Confidence:
3

";1
ljgM3vNqfQ;"REVIEW 
Summary:
This paper proposes an unsupervised time series anomaly detection algorithm called NPSR (Nominality score conditioned TSAD by Point/Sequential Reconstruction), which combines both point-based and sequence-based reconstruction models. Specifically, it proposes a nominality score which is the ratio of a contextual deviation (or in-distribution deviation) to the total deviation (which is assumed to be the sum of the in-distribution deviation and the out-of-distribution deviation). The contextual deviation and the in-distribution deviation is computed by using the sequence-based and point-based reconstruction models. Based on the nominality score and anomaly score (computed using point-based reconstruction model) computed in the neighborhood, the induced anomaly score is further proposed by considering the temporal relationship. Some theoretical results of the proposed algorithm are provided. Experiments on several benchmark time series anomaly detection datasets are performed to demonstrate the performance of the proposed algorithm in comparison with several state-of-the-art algorithms.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.	This paper studies an important and interesting problem, i.e., how to detect anomalies in time series data without label data. 
2.	The proposed algorithm combines both point-based and sequence-based reconstruction models, which achieves quite good performance on several time series anomaly detection benchmark datasets. 
3.	Some theoretical results are provided for the proposed algorithm.
4.	The paper is generally well written and the presentation is clear.

Weaknesses:
1.	Overall, this paper proposes a heuristic-based unsupervised time series anomaly detection algorithm. The overall pipeline in Algorithm looks Ok to me, but from experiments it seems the most important part is the point-based reconstruction. In other words, the rest of the proposed algorithm may be simplified. The ablation studies in Table 3 also partially confirms it.
2.	Flawed experiments. The F1 scores reported in Table 2 for other algorithms, e.g., Anomaly Transformer, are not consistent with results reported in the literature, such as [33]. More justifications are required. 
3.	The theoretical results look sound, but may not be useful in practice. In other words, can you justify the value of these results in real-world time series anomaly detection applications?

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
TSAD techniques are targeted towards detecting either point or contextual anomalies, but often struggle to adequately capture both simultaneously. In this work, the authors propose a novel reconstruction-based AD technique that introduces the notion of a nominality score and a subsequent induced anomaly score. The method achieves a better trade-off between detecting point and contextual anomalies than competing techniques, and outperforms them over a variety of benchmark multivariate datasets. 

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The problem statement is interesting and relevant, and I appreciate the use of visualisations such as in Figure 1, that convey the difficulty of the task.
- I appreciated that the model has several components that could be tweaked, such as the choice of soft or hard gate function, and the ablation study does a good job in highlighting their individual contributions. I would however have liked to see more discussion or synthetic examples that demonstrate whether there are certain properties of the monitored data that can guide the selection (beyond empirical comparison).

Weaknesses:
- I found the presentation and clarity to be quite poor overall, as a result of which the contributions are sometimes difficult to follow properly.  Section 2.2 introduces a lot of similar notations that are difficult to keep track of – a figure could be very helpful here for conveying the differences between the various time series and deviations.
- The authors rely on results reported in earlier papers as performance measures for several competing techniques. While I appreciate that it may be time-consuming to re-implement and re-run all experiments, I do worry about potential inconsistencies in the experimental set-up that may involuntarily skew the comparison. The authors mention how if the results from the original paper are unavailable, *“we search for the highest reported values among other publications.”* I can see this as introducing several inconsistencies, especially since the source of the result isn’t reported alongside the performance figures presented in the table.
- I would have liked there to be a dedicated *Related Work* section in the main paper in order to better understand how the work fits alongside other reconstruction-based techniques.  

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed a novel idea to handle the time-series anomaly detection problem by calculating the ""nominality score""(concerns on this name are in the question part and I'll keep using this name in the following review) and the induced anomaly score. And the F1 score can be mathematically proved to improve using the proposed scores. The authors then constructed the point-based and sequence-based reconstruction models to estimate the anomaly score and experiments on several datasets demonstrate the superior performances.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The proposed method is novel and provides a new angle to address the anomaly detection problem. Without directly considering how ""abnormal"" a point is, this work start with thinking what is ""normal"" and induce anomality score based on the nominality score. 
2. The authors provided both theoretical and experimental supports for the proposed methods which are reasonable.
3. The writing of the paper is clear and easy to follow.


Weaknesses:
1. A major concern is on the expectation of improvement by using $\hat A(\cdot ;g_{\theta_2})$. While the Claim 1 and Claim 2 as well as the proofs gives a guarantee that using this method won't result in a worse F1 score, it says nothing about the gap with/without the method. Intuitively, the expected improvement may be related to $\alpha$ in Eq. (2), $d$, the function, the number of data, and other factors. The actual case could be that even if the $\alpha$ is larger than 1 by a margin (say \alpha=1.5, 2, or 5), the expected number of $N(t)\leq \theta$ for abnormal points are still a small portion of the whole dataset, making the proposed method useless in practical use. The analysis of a bound or expectation is missing.
2. The correctness of the method relies on several hypotheses, typically: 1) distribution of abnormal points $\Delta x_{t,a}^p$ has a larger variance than that of normal points $\Delta x_{t, n}^p$. This is used to guarantee that nominality score of normal points are larger than abnormal points; 2) the threshold is well selected so that the nominality score of all the normal points (with $y_t=0$) passes the threshold, while a significant number of abnormal points cannot pass the threshold. While the hypotheses are reasonable theoretically, I wonder whether they still hold when the scores are estimated and calculated based on the outputs from neural networks.

Limitations:
No apparent limitations and negative societal impact observed by the reviewer.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Basically, this paper aims to consider anomalousness of data points both from a point perspective, which is independent of the temporal relationships in the data, and contextual perspective, which reflects temporal relationships in the data. The paper derives an induced anomaly score that considers both.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper addresses an important problem of considering both point and contextual anomalies. The paper provides good theoretical background for their work and convincing experimental results, showing particular anomalies that their method finds that are not found by methods that look for either point-based or contextual anomalies.

Weaknesses:
The datasets on which testing is performed have rather high anomaly rates. The authors should experiment with some datasets with much lower anomaly rates, perhaps by leaving out some anomalies in the datasets that they use.

Post rebuttal comment:
Thanks to the authors for the responses. While the anomaly rates that the authors use are worth testing, it is quite common to use lower anomaly rates, and such lower rates would constitute a better test of the performance of the new algorithm. With regard to removing anomalies from datasets, one can remove individual time series that are identified as anomalous.

Limitations:
No limitations are described on the work. Societal impact material is not relevant here. However, a description of future work should be provided based on any patterns in the errors that the presented algorithm makes.

---

Post rebuttal addition:
Thanks to the authors for pointing out the discussions on limitations in the supplementary material. However, such material is critical to understanding the nature of the contribution, and so needs to be in the main paper.

Rating:
6

Confidence:
4

";1
7nXaoclHed;"REVIEW 
Summary:
This paper studies the problem of constructing a spectral clustering oracle with sublinear pre-processing and query time complexity. The paper introduces a new algorithm which improves on previous methods with respect to the running time at the expense of a slightly worse approximation guarantee. In contrast with previous methods, the new algorithm is more practical as demonstrated by experiments on synthetic data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
On the theoretical side, this paper introduces a new sublinear-time clustering oracle with guarantees which improve on the previous state of the art in terms of running time. The new proof techniques are introduced in quite a natural way and the intuition is clearly explained. An important contribution of this paper is that the proposed algorithm is practical and admits an implementation.

Weaknesses:
The theoretical improvements over the previous algorithms are quite small (in particular, the improvement over [30]), however I don't view this as a major weakness given that the new algorithm is more practical.

The statement of Theorem 1 is quite difficult to follow. For example, the parameter $\xi$ doesn't seem to be introduced or explained intuitively.

The experimental evaluation is quite limited - it does not include comparison with any other method, and does not report the running time of the algorithm. If the algorithm cannot be compared with that of [30] or [14], consider stating why this is the case in section 4.

Limitations:
Limitations are adequately addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies oracles for spectral graph clustering, i.e., local algorithms that answer membership queries for single nodes in a spectral clustering of a graph. There is a line of research on testing cluster structure in degree-bounded graphs, and recently, the learning version of the problem studied in this paper has become popular. Besides a result for robust clustering oracles by Peng, this works is closely related to a paper by Gluch et al. Compared to the work of Gluch et al., this submission improves the preprocessing time to $O(n^{1/2 + O(\epsilon / \phi^2)})$, which is better by a factor of approximately $2^{poly(k/e)}$ at the expsense of requiring a conductance gap of approximately $\Omega(1/poly(k))$, which is worse by approximately a $poly(k)/\log(k)$ factor, and a misclassification error of $O(poly(k) \epsilon)$, which is worse by approximately a $k / \log(k)$ factor. The misclassification error is the fraction of vertices that are assigned to the wrong cluster (compared to the ground truth clustering). The query time of the two algorithms is roughly the same. In a nutshell, the result in this submission trades an additional polynomial dependency in conductance gap and misclassification error against the removal of an exponential dependency in preprocessing time. The authors experimentally confirm the misclassification error and query complexity proven in their theorems.

This work builds up on the dot product oracle introduced by Gluch et al. The algorithm in the latter work estimates the means of the clusters (in an embedding space) and uses the dot product oracle to estimate the closest cluster center (mean) for a query node. The exponential preprocessing time arises from the former part. In the present submission, the authors propose an algorithm that doesn't estimate the cluster means, but compares the dot product between node embedding directly. Intuitively, if two nodes belong to the same cluster, they have a large dot product with their cluster mean, and so they should also have a large dot product with each other. This modification also results in the aforementioned trade off, i.e., the increased misclassification error and the stronger requirement on cluster separation.

The question answered by this paper arises naturally from the work of Gluch et al.: Do we need to compute the cluster means explicitly? While the answer may not be surprising (no, but there is a trade off), it requires some work to actually prove this, as the formal argument is not simple and obvious. The intention of the experiments is not clear to me, as there is no comparison with other algorithms, or insights how the theoretical algorithm needs to be modified and tuned for applications.

Rebuttal: Rating changed from weak accept to accept due to authors' rebuttal responses.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The question explored in this paper is natural.
* The paper confirms an intuitive concept of spectral embeddings for clustering.

Weaknesses:
* The result in this paper is not very surprising or better than one would expect. It seems more like a reasonable trade off.
* The experiments seem currently not very useful.

Limitations:
-

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a spectral clustering oracle with sublinear pre-processing time and query time. The query is in the form of $(G, x)$ where $G$ is a graph with underlying clusters and $x \in V$ is a vertex. The goal is to (1) construct the oracle efficiently, (2) report which cluster vertex $x$ belongs to efficiently.

Comparing to the previous work, the main contribution is improvement on the pre-processing time, which reduces exponential to polynomial on $O(k/\varepsilon)$, but blows up the misclassification error from $\log k$ to $\text{poly}(k)$, also slightly relaxing an assumption on the gap between inner and outer conductance.  The query time is asym-same. 

The main technique is to replace the exhaustive search for each sampled vertex to decide a vertex $x$ belongs to a cluster with center $\mu$, with estimating the inner product of their spectral embeddings. It is proved the magnitude of the inner product roughly shows if two vertices belong to the same cluster.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written and well organized.
- The result is neat, the proposed algorithms are more easy to implement comparing to previous ones.
- I think for a theory paper, having experiments is always a plus. However, there is a mentality that either do it well or just don't do it. The experiments can be improved or at least clarified better.

Weaknesses:
- The major concern is that the contribution of the main result is quite limited. Yes, $O(k/\varepsilon)$ is an important factor, but it still in the $\tilde{\Omega}(\sqrt{n})$ regime, not to mention compromise on others. I actually like the robustness result better.
- On the experiments, if the authors want to keep and improve the section, I would suggest:
  - (1) Clarify the evaluation. I believe the theoretical result on the error is the number of query instances? Then the current report does not look like so, is it the fraction?
  - (2) The issue of query complexity is not mentioned before. It is out of blue. Explain why you do it.
  - (3) Add the robustness experiments.

Limitations:
See above.

My reason to give the current assessment is mainly on the technical novelty limit.

Rating:
6

Confidence:
4

";1
4VAF3d5jNg;"REVIEW 
Summary:
This paper presents an adaptive label-efficient forecasting technique for online binary prediction with expert advice. The proposed approach implements a label querying probability that is a function of the observed scenario, rather than based on pessimistic conditions. This enables the method to adapt, i.e., have lower label complexity, to benign environments while remaining robust to adversarial ones, unlike prior approaches in this label-efficient forecasting. Sharp analyses of the regret and label complexity and results on synthetic scenarios are provided in support of the method.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The paper is very well-written and organized.
* The introduced algorithm is novel and intuitive. The regret and label complexity analyses of the approach seem sound.
* The method remedies the shortcoming (lack of adaptivity) of prior label-efficient prediction approaches. This enables it to query fewer labels in benign scenarios while remaining robust to adversarial ones.
* The authors present empirical evaluations that demonstrate the effectiveness of the method.


Weaknesses:
* The method only applies to binary prediction tasks with zero-one loss.
* Additional details on prior work on label-efficient prediction would be helpful in contextualizing the benefit (adaptivity) of the proposed approach.


Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper considers a binary prediction game on $0-1$ loss, it proposed efficient sampling scheme via an modification of the exponentiated weight forecaster, which selectively acquire labels $y_t$ based on $Ber(q_t)$, where the design of $q_t$ is correlated to the disagreement among experts’ predictions at each round for the exponentiated weight forecaster. 

Ultimately, the proposed algorithms achieves the regret of the exponentiated weight forecaster $O(\ln N \sqrt{ n} )$ over time horizon $n$ and $N$ number of experts, the design labelling acquisition parameter $q_t$ resultant to a labelling complexity of $O( \frac{\sqrt{n}}{\Delta^2} )$, where $0 < \Delta \le \mathbb{E} [\ell_{t,i} - \ell_{t,i^{\ast}}], \forall t \in [n], i \neq i^{\ast}$ represents the lower bound expected loss gap comparing to the optimal expert with index $i^{\ast}$, which also signifies the difficulty of identifying the optimal experts.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and easy to follow. It first introduces the intuition of sampling parameter $q_t$ if there is a perfect expert, then extended to general case. The paper also provided an graphical illustration on the lower bound of $q_t$ which matches with expectation.

The result is novel comparing to previous result in two folds. (1) There is no assumption on how $y_t$ is generated. The proposed algorithm is able to attain the $O(\ln N \sqrt{n})$ regret without less labels. (2) In contrast to previous work on sampling by disagreement, the label complexity can be quantified as $O(\frac{\sqrt{n}}{ \Delta^2})$. 

$q_t$ is easy to compute, numerical experiment with respect to time horizon $n$ shows the expected regret and expected number of labels which matches with theoretical results. Experiment with respect to number of label (number of weights update) shows labelling efficient algorithm proposed in the paper matches the minimax rate in active learning asymptotically.




Weaknesses:
It seems that the assumption $0 > \Delta \ge \mathbb{E} [\ell_{t,i} - \ell_{t,i^{\ast}}], \forall t \in [n], i \neq i^{\ast}$ is required only for bounding the labelling complexity, ( in order to track how $q_{t+1}$ evolves in line 493), without this assumption the regret still holds. 

I am a bit concern such assumption is very strong, it generally asks the best expert $i^{\ast}$ is wining over every other expert at every round. In addition, the assumption is $\ell_{t,i} \in \{ 0, 1\}$, if we are at a specific iteration $t$ where $\ell_{t,i^{\ast}} = 1$, then what values  can $\ell_{t,i}, \forall i \neq i^{\ast}$ take in order to satisfy the condition for a strictly positive $\Delta$? 

Whether labelling complexity can be bounded without this assumption?


Limitations:
yes

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposes an interesting novel approach to prediction with expert advise. In the standard prediction with expert advise setup, the learner receives experts' predictions, commits to its own and then sees the true outcome as produces by the (possibly adversarial) nature. Suppose that obtaining the true outcome is costly; do we really need to do this all the time? Clearly, if all (or most) experts agreed on the same prediction, the value of the true outcome for adjusting our trust in them is negligible; in the standard weight-based algorithms with multiplicative update the contribution of this round will simply be eliminated by normalisation.

The paper takes an algorithm from Cesa-Bianchi and Lugosi (exponential weighting with fixed $\eta$) and shows that its regret bound stands as it is if the true outcome is requested with certain probability.

It is then shown that the expectation of the number of outcomes actually requested is upper bounded by $3\eta T + O(\log\log T)$, so for small $\eta$ there is linear improvement in the number of requested outcomes. If $\eta$ is chosen to minimise the regret bound using prior knowledge of $T$, we take $\eta\propto 1/\sqrt{T}$ and thus the bound reduces to $O(\sqrt{T})$. This results holds under some conditions on experts' behaviour  though and they seem restrictive.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I think this is an interesting take on the well-known problem and should be published.

Weaknesses:
No obvious weaknesses.

Limitations:
Yes

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper investigates the PEA problem in the context of online binary classification where the cost of obtaining labels for streaming data is high, necessitating selective label collection adaptively. To this end, the authors introduce a carefully designed label collection strategy based on the classical Hedge algorithm. The resultant label-efficient forecaster has a best-of-both-worlds theoretical guarantee. The authors further demonstrate that regret of their label-efficient forecaster asymptotically reaches the minimax rates of pool-based active learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The paper is well-structured and easy to comprehend.
* The theoretical analysis provided in the paper appears to be solid and sound.

Weaknesses:
* The primary conclusion of the paper, Theorem 3, relies on a core assumption that there exists a unique optimal expert whose expected loss in each round surpasses all other experts by a specific margin $\Delta$. Is this assumption too strong? Are there real-world application scenarios that satisfy such an assumption? (To my knowledge, this assumption has only been used in the COLT'14 paper: A Second-order Bound with Excess Losses).

* Does this specific PEA setting investigated in the paper relates to the bandit setting? Both are concerned with identifying the optimal expert (arm). How then do the setting and techniques used in this paper differ or relate to those in bandit scenarios? Does the problem studied in this paper present novel challenges in comparison to the bandit setting?


* The paper states that the online prediction setting is similar to streaming active learning. If this is the case, should the numerical experiment compare the regret convergence rate of the proposed algorithm with that of the streaming active learning algorithm?


* Some writing aspects could be improved:

  * In line 227, the symbol $\tilde l$ is undefined.
  * In line 264, the regret bound (7) is termed ""pessimistic"" without explanation.

Limitations:
The paper should further clarify the method and theory part, see Weaknesses above.

Rating:
6

Confidence:
3

";1
BkQM8huiIc;"REVIEW 
Summary:
The paper presents experiments conducted with real mice, that were put in varying conditions of social stress. In particular, pairs of mice where put together and their behaviour was observed. The attacking mouse was classified as dominant and winner. Initially, loosing mice were move, whereas winning mice stayed. Mice were euthanised and their brains were analysed for brain activity, based on some marker which has been shown to be an indicator.

The results were recoded and various models where fitted to the data in order to explain when mice attacked or defended, based on their believe of the opposing mouse. The data is based on more than 100 mice.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The idea of the paper is appealing. Using data of real animal behaviour and try to model it with a game theoretic approach. The sample size is large, compared to other animal studies. The math and results are presented well.

Weaknesses:
There are several issues with this paper.

The biggest issue that I have with this paper is that more than 100 mice where ethanised without any discussion of documentation of ethical standards and considerations.

Furthermore, the mice are placed purposely in stressful conditions and it is not clear how the collected data is a good reflection of their beliefs. As an example. Winning mice remain in the environment, while loosing mice are moved to another environment. Yet, attacking is considered winning. As a mouse, not moved to another enviroment, defending could be the more natural behaviour if a new animal is placed in the environment. A male lion defends it's place while the youg male attacks to take it's place. The classification of attacking = winning is not well argumented, which questions the underlying assumption.

The discussion of the broader impact is mainly restating the experiemnts and methods. There is no real discussion what can be learned from fitting the models to the data.

Limitations:
The authors state ""A detailed discussion of our results including the strengths and limitations of our approach is provided in Appendix A"". I expect the discussion of the limitations in the main paper.

The title of the paper is ""A normative theory of social conflict"". I don't see various models fitted but I can't see a normative theory that is derived from the experiments.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper aims at explaining the underlying principles of social conflict with the Theory of Mind modeling. The authors collected the behavioral data of 100+ mice in agnostic contact events, and fit the parameters of the Bayesian inference model. The experiments show that animals' actions are consistent with modeling both the first-level belief of strengths and the second-level belief of the opponents. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The model comparison considers a comprehensive suite of contributing factors. The model’s effectiveness is clearly demonstrated by contrasting different control groups. The success regression of the Bayesian model to the mice data shows that both the first and second-level beliefs are employed for decision-making.  


Weaknesses:
This paper is difficult for me to make a judgment on. The modeling and the ablation studies are sound. However, the Bayesian belief update framework is already well-studied and there is no new algorithm-wise contribution. The model fitting showing both the first and second-level ToM are used in multi-agent decision-making is not new discovery as well. I’m open to discussion with other reviewers and authors about how to posit this paper and its contribution to Neurips. 


Limitations:
see weaknesses

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper models social conflict in mice using a game theoretic Bayesian theory-of-mind model. They find that the mice's actions are most consistent with the ToM model where agents (mice) maintain primary beliefs and secondary beliefs about the beliefs of their opponents. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper models an interesting dataset of mice in social conflict which includes whole-brain neural data. They do many comparisons between different models and ablation studies in their experiments.

Weaknesses:
I saw two main weaknesses in the paper. The first is that the paper needs more explanation of the significance of the results. The introduction is combined with the related work, and in particular, the ""introduction"" part is only one paragraph while there are four paragraphs describing related work. I couldn't understand what the specific contributions of this paper were compared to previous results. I also wasn't sure about the ""so what?"" of the paper. I think the authors could move the ablation studies in Sec 3.1.1 to a appendix and expand more about the contributions of their results. The description of the model and how it is fit also takes up a lot of space, some of this could probably be moved to an appendix.

The second weakness is in the empirical evaluation of experiments. I'm confused why the models are evaluated by the change in negative log likelihood on the test set. If I'm understanding currectly the actions and outcomes are discrete (either attack/defend or win/lose).. meaning the ground-truth probability of an action/outcome at a given time is either 1 or 0. So, evaluating the models on log likelihood means that models that make more extreme predictions are preferred. Log likelihood is used to fit the model, but in testing, we just care about the predictive accuracy. Why not just evaluate on predictive accuracy? It would be more interpretable too.

The authors evaluate many models, compute many significance tests, and control for FDR because of the multiple corrections. However, they only seem to report the results for the *best* models: ""Our comparisons were iterative: they lasted until a model under consideration outperformed all the other models. Below, we report the results for the final round of these comparisons."" There aren't enough details in the paper to understand how exactly they controlled for FDR. Did they only control for FDR for the best model comparisons shown in Fig 4? It seems that there may be a _selective inference_ problem going on here. Generally, it makes sense to report a table in the appendix with all hypothesis tests that were run, the resulting p-values, and their corresopnding q-values. I would ask that the authors add this in their revision for clarity.

Since the main result seems to be that the 1-ToM does better than 0-ToM (at least that's what is highlighted in the abstract), why not also test 2-ToM? Is it possible to recover a 0-ToM model using the 1-ToM model class they've defined? In that case, one would expect that 1-ToM fits the data better (but perhaps doesn't generalize as well). If the authors compared to 2-ToM, it would provide stronger evidence that 1-ToM is the correct model (as opposed to simply having lower log likelihood because of having the flexibility to use more parameters.

I may have misunderstood some of the experimental evaluation, and if so, I am willing to change my mind on the above points.

Limitations:
The strengths and limitations are relegated to the appendix. As described in ""weaknesses"", I don't think there is enough contextualizing for the motivation and takeaways of the paper. I think the authors should move the limitations/strengths to the main text and move some of the ablation studies/method description to the appendix instead.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper uses behavioral and neural data in mice to develop a game-theoretic and Bayesian theory-of-mind based model of social conflict. Mice are assumed to choose whether to attack or defend optimally (the game-theoretic component) based on their beliefs (and beliefs about opponent's beliefs; the BToM component). The authors show that mice behavior is best explained by the BToM model, rather than simpler models (e.g. pure RL over attack/defend actions) or ablated versions of their model. Finally, neural correlates of the model are identified.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper is ambitious, covering both a theoretical model and extensive empirical validation. Based on my limited understanding, it proposes and provides evidence for the role of ToM in a fundamental social conflict paradigm. If the general novelty of the approach is in fact high, I agree this could form an interesting basis for future work.
- The model itself is interesting and seems well-justified (although it took a few reads to understand the structure, see weaknesses). 
- The science (experimental paradigm, model comparisons and ablations) seems sound. I appreciated the comparisons to, e.g., simpler R-W reinforcement, which justify the ToM-based approach.

Weaknesses:
- Presentation. I found the paper difficult to understand: explanations of the experimental paradigm, model, and neural analysis were all compact and somewhat cryptic. Relationship to prior literature-- especially to contextualize the findings, such as neural correlates-- was terse, and the discussion itself was relegated to the appendix. This made it difficult to understand the contribution.
- The discussion of neural findings (both in the main text and appendix) was particularly brief. I'm vaguely aware of neural imaging of ToM elsewhere (e.g. in human gameplay [1,2] or pedagogy [3]) but these connections aren't discussed. Obviously these studies are done in people not mice, but given the authors' stated ambition to be relevant to human behavior, I think it'd be helpful to sketch out how these findings relate to imaging work there. 
- This is more of a note to the authors than a weakness of the scientific contribution itself, but I'm not sure NeurIPS is the best venue for this work. There's a lot of ground to cover here  for 9 pages.  Similarly, I'm not sure how relevant the findings will be for the majority of the NeurIPS audience. I was personally unfamiliar with the experimental paradigms and background literature, so I had trouble judging its significance. A neuro journal with a more expansive format and familiar audience might be a better venue.

[1] https://www.pnas.org/doi/10.1073/pnas.0807721106
[2] https://www.pnas.org/doi/10.1073/pnas.0711099105
[3] https://www.pnas.org/doi/abs/10.1073/pnas.2215015120

Limitations:
No concerns. Review board approval was obtained for animal experiments. 

Rating:
6

Confidence:
1

REVIEW 
Summary:
This work provides a normative framework for reasoning about the strength and, ultimately, the chance of winning in a social conflict in mice. The analysis supporting the practicality of the framework includes both behavioral and neural data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The topic of the paper, i.e., using AI models for the analysis of behavioral and neural data in animals during social interactions, is really interesting. It also covers a variety of topics and models relevant to the work. Furthermore, it used different types of data (behavioral and neural) in the analysis.

Weaknesses:
My main concern about the paper is their claim about the existence of Theory of Mind (ToM) in mice. This is highly controversial in psychology, even for non-human primates, and such a claim needs a lot of control experiments.  Moreover, reasoning about strength is different from mind. In addition, I am not fully convinced that level-1 performs significantly better than level-0 as the cost of parameters and deeper reasoning is not considered. Presentation of the results in likelihood, as opposed to sth more human interpretable such as accuracy, makes it hard to evaluate the fits. Even if the results hold, I think the authors should use another term such as joint reasoning about the opponent, or ""1-ToM like"" framework, with a detailed explanation about the model mimics 1-ToM models in humans and do not necessarily mean the existence of ToM in mice.

Limitations:
yes

Rating:
6

Confidence:
4

";1
mkve1raJUc;"REVIEW 
Summary:
This paper studies outlier-robust location estimation for symmetric(-like) distributions. For ""semi-product"" distributions, the authors show that one can achieve $O(\epsilon \sqrt{\log 1/\epsilon})$ asymptotic error with a polynomial number of samples (and time), and $O(\epsilon)$ asymptotic error when given quasipolynomially many samples (and time poly in the sample set size). For elliptical distributions, the authors show that as long as the scatter matrix has $\Omega(\log d)$ effective rank (hiding $\epsilon$-dependence here), then in quasipolynomial time we can yield asymptotic error $O(\epsilon \sqrt{\log 1/\epsilon})$. Crucially, for the last result, the scatter matrix is not known to the algorithm.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Prior works in (algorithmic) robust statistics get ""stuck"" at the $\sqrt{\epsilon}$ asymptotic error, when the covariance of the distribution is not known, even when the underlying distribution is guaranteed to have higher moments or might even be sub-Gaussian. This paper identifies semi-product distributions and elliptical distributions as special classes for which further progress can be made. The author(s) adapts the filtering framework, and proposes using variants of the Huber loss as score as opposed to the basic quadratic score in the filtering step. This allows the author(s) to beat the $\sqrt{\epsilon}$ error even when covariance/scatter matrices are unknown to the algorithm.

Weaknesses:
I find some parts of the writing clarity can be improved. In particular, I'm still a bit confused about how the guarantees of this work compare with prior works, re: knowledge assumptions on the algorithms. Some of the technical claims are also a bit over-sold (unless I misread or am misunderstanding). See the ""Questions"" section for more details.

Another weakness, for me, is the motivation: the practical relevance of semi-product and elliptical distributions seems to be rooted in mathematical finance. However, at least from the cited works, the relevance of these distributions appears to more or less be ""because we can write down theorems"". I hope the authors will consider adding a more self-contained discussion on why we should care about these distributions; I think it will make the paper more convincing.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper considers sample complexity of (robust) mean estimation possibly without moments. Two typical examples are product distributions and elliptical distributions. The main technical contribution of this paper is to adapt the filtering techniques to the setting with less restrict moment assumptions. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The results of the paper are interesting and new; while the idea may not be. The basic argument relies on filtering, which was previously proposed by [DKK+17] and [DKK+19]. The authors of the paper make a good observation on how the idea of filtering (and coupling) can be used with less restrictive moment assumptions (but it finally replaced by some concentration). The paper is well written, and I enjoyed every minute reading it. 

Weaknesses:
As mentioned, the main concern is that the authors replace the moment conditions with ""concentration"", i.e. $P(|\eta| \le \rho) > \frac{1}{100}$ and $P(R \le \sqrt{2d}) \ge \frac{1}{100}$. Moreover, from technique viewpoint, the main components, filtering and coupling (identifiability), are not novel. The time complexity is not explicit (though polynomial). Also I think the paper is lack of (synthetic or real) experiments.




Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the robust mean estimation problem without any moment assumptions. Instead, they consider a class of symmetric distributions, that is, semi-product and elliptical distributions. They develop a method based on Huber loss and the classic filtering technique that can achieve the same error rate as if the underlining distributions are sub-Gaussian. Their sample complexities are nearly optimal (with additional log factors).

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The idea to use Huber loss and develop a similar result for the classic filtering method is very smart. I think this idea can be generalized to other settings and can be of independent interest.

- The paper is well-structured. The techniques part clearly provides the motivation and is very readable.

Weaknesses:
- The title is a little bit overclaimed. This paper studies the robust mean estimation problem without moments, but with the constraint that the distribution should be symmetric. I think it would be better to reflect this constraint in the title. 

- The notation style is not consistent. Based on my understanding, the authors use bold font for random variables. However, in some cases, like line 95, some symbols are not in bold font. 

- A quick question: I can understand that Huber loss has many fantastic properties. Compared with l2-loss, it is more robust against outliers; compared with l1-loss, it is differentiable everywhere. However, l1-loss is only not differentiable at 0. Hence, my question is can we replace the Huber loss with l1-loss? If not, can you briefly mention what is the difficulty?

Limitations:
Please see the weakness part.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This work studies robust mean estimation for symmetric distributions. There may possibly no moments (not even the first moment, hence the term location instead of mean), so previous efficient algorithms that rely on strong distributional assumptions may not work on the heavy-tailed setting. 

The main result is that similar statistical guarantees as the Gaussian setting can be obtained for $\rho$-semi-product distributions, where a semi-product distribution is a set of distributions that covers slightly more than that of product distributions. For instance, semi-product distributions include elliptical distributions, which is not a product distribution. For the case of known covariance matrix (or scatter matrix for elliptical), the guarantee obtained nearly matches best-known guarantees for Gaussians. For the unknown case, they obtain an error guarantee of $O(\epsilon^{1-\frac{1}{2k}})$ using $\tilde{O}(d^k)$ samples.

The key approach is to generalize the filtering idea for robust mean estimation with a Huber loss instead of a quadratic loss in order to be able to handle distributions without second moments. Once the Huber loss is properly incorporated into the filtering technique, the necessary certificate for filtering can be obtained algorithmically without using sum-of-squares proofs for symmetric distributions.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
- This work studies a setting which (partially*) generalizes previous works to handle heavy-tailed distributions in which first or second moments may not exist. Here, they incorporate Huber loss into the common filtering technique that has been used in much of the recent robust learning literature. This in itself is novel. Furthermore, by exploiting symmetry, the filtering technique is made more algorithmically feasible without relying SoS approaches.
- Through studying heavy-tailed symmetric distributions, such as elliptical distributions, in the case of unknown covariance, the paper obtains error bounds of $o(\sqrt{\epsilon})$ that were not known for general subgaussian distributions. 
- The main result obtains strong guarantees that nearly match that of Gaussians.

*refer to Weakness 1

Weaknesses:
1. Only a minor weakness: though the abstact motivates the work by stating that previous efficient estimation algorithms assume strong distributional conditions, the symmetry distributional assumption seems also possibly strong. While it is able to incorporate heavy-tail distributions, it also limits itself in generality (as ever-slightly altered Gaussians are not symmetric but have strong concentrations to exploit).
2. While I enjoyed the content of the paper, he main body seems abruptly cut off without a conclusion or final algorithmic overview. The presesntation would be greatly improved with a final algorithmic description along with a retrospective conclusion.

Limitations:
No limtation addressed.

Rating:
7

Confidence:
4

";1
VEpU9rFaQr;"REVIEW 
Summary:
While humans and machines oftentimes make differing decisions, it's unclear whether humans make these decisions based on extra factors or information unavailable to machines. To understand this situation, the paper proposes a statistical test to determine if expert predictions are independent of the labels, when accounting for the input features. This idea indicates whether humans rely on different information, and in a sense add additional value unseen by a model. To evaluate this, the paper analyzes doctor predictions in a hospital admitance system, and find that doctors tend to use additional information.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. Statistical approach is well motivated and clearly demonstrates how to test for humans relying on extra information
2. Test allows for flexibility due to choice of L, allowing for different statistical properties
3. Method is evaluated on a real-world hospital dataset, and the connection between the evaluation and the methodology is clear 


Weaknesses:
1. Method relies on dataset containing pairs that are close in input space, yet distinct in feature space; such a situation might not be generalizable
2. Evaluation is only done on one real-world dataset; a controlled evaluation of the test would give better insights into how the test performs and the impact of different parameters


Limitations:
Authors discuss limitations fairly thoroughly.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a statistical framework for measuring whether, in the context of algorithmic predictions, human experts incorporate valuable information in their decision making that is unknowable to the algorithm. The authors formalize this question as a simple hypothesis test: “are human expert predictions independent from the outcome variable, when conditioned on the feature vector”.  The proposed statistical test for “presence of human expertise” is straightforward, drawing on prior literature on testing conditional independence, and provides interpretable p-values. The paper then uses this framework to analyze real-world medical data from an emergency department of a large hospital, showing that physicians do in fact incorporate information above and beyond that captured by a standard algorithmic screening tool.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The main strengths of this paper are in it’s simplicity, clear exposition, and scoping of a well-defined problem that it tries to solve. The authors’ give an elegant framework for formally defining the problem of measuring valuable human expertise and their proposed test is quite intuitive. The paper presents all its ideas in a concise manner and also discusses its limitations quite candidly. 

I also like the thoroughness of the experiment conducted by the authors—it seems well executed and the results are compelling evidence for the validity of their statistical test.

Weaknesses:
Some weaknesses of this work:

- From an algorithmic/technical standpoint, this paper uses a straightforward notion of conditional independence to define the problem, and a simple binned conditional independence test to solve it. This simplicity is not a bad thing, but it is worth noting that the main contribution of this work doesn’t present a novel technique or technical insight.
- The results and experiments of this paper would likely not extend to a high-dimensional setting. Their current experiment uses discrete, scalar features. The authors discuss this in their limitations. A concrete example: how would a test like this work for, say, radiology images, where the human predictive distribution is likely not smooth w.r.t the $\ell_2$ metric.
- I understand it’s not an easy task, but this paper would be much stronger if there were additional experiments where this technique was employed to understand the interplay between human and algorithmic decision making.

Limitations:
Yes, good discussion of limitations and prior work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a method for determining whether a human expert is usefully using outside information that a model does not incorporate in order to make decisions. The goal is to test whether complementarity, humans working with models, is possible for a given task. The paper sets forth an algorithm, ExpertTest, provides some theoretical guarantees, and uses emergency room admissions as a case study for the technique. In the case study, the method indicates that doctors are making use of external information that is not captured in a commonly used risk score. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main strength of this paper is the novel problem that it seeks to solve. Understanding whether and how humans can add their expertise on top of automated decision systems is an important goal, and it is often understudied. It is a creative approach to a crucial problem. 


Weaknesses:
The primary weakness of the paper, in my opinion, is that the problem setup focuses on variables, or features, rather than also considering the functional form for the prediction itself. Fundamentally, if we are comparing the performance of something like the Glasgow-Blandford score (GBS) to humans, how do we know that the human is not using the same exact input features $X$ as the GBS, but just has a better way of mapping those $X$ to the prediction we are interested in? Why does $U$ have to exist at all for the human to be lending their “expertise” to the problem? That is, let’s say the GBS output is $\tilde{Y} = \tilde{f}(X)$, and the human output is $\hat{Y} = \hat{f}(X)$. If $\hat{f}$ is closer to the actual generating function in the ground truth than $\tilde{f}$, then the human could be “adding expertise” without actually using additional information as defined in this paper. Perhaps the human’s training would help them map these variables better than the GBS algorithm does. If I am missing something here, please correct me, but it’s very unclear to me why the method proposed leads to the conclusion that the expert is using additional features to make a decision. 


Limitations:
I think the paper should more carefully address the assumptions in the initial problem setup, specifically what I lay out in the weaknesses section. What model of human expertise is being considered when the assumption is that any additional information is encapsulated in the $U$ variables?


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a hypothesis-testing approach to identify whether a set of predictions made by a human expert uses additional information that is conditionally independent from the input covariates. The paper provides theoretical guarantees for the test in a general setting and asymptotically. The proposed test is then applied to real-world data with emergency room physicians.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper itself is written in a way that is easy to follow along, building the motivation and proposed work in a step-by-step manner.
- The paper proposes a test that is a novel application of conditional independence work in the statistics literature for a novel use case. 
- The paper evaluates the theoretical test on a real-world use case.


Weaknesses:
- Can the test help improve decision outcomes? Typically, the primary goal in human-AI decision-making is to achieve complementarity (e.g., as discussed in [1]), particularly by leveraging the complementary skills of human and AI. Because the test does not account for human and AI prediction accuracies, it is difficult to say whether performing such a test has any implications on complementarity. Another relevant cite is [2].

[1] Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. Bansal et al. CHI 2021.

[2] A Unifying Framework for Combining Complementary Strengths of Humans and ML toward Better Predictive Decision-Making. Rastogi et al. EAAMO 2022.

- Experimental validation: While it is great that the authors perform experiments on real-world data, it would be ideal to also verify the behavior of the test using synthetic where the differences between human and AI can be more carefully controlled to establish how sensitive the test is to these similarities and potentially the effect on the choice of L.


Limitations:
Yes

Rating:
7

Confidence:
4

";1
Eewh7sl0Xj;"REVIEW 
Summary:
The present paper offers a Toeplitz matrix architecture which can handle sequence modeling. The architecture comes in two flavors. The first flavor is a fast version that is most useful for bi-directional tasks. It speeds up previous Toeplitz networks by using an interpolation and low rank approximation scheme in its setup. The second is a Fourier based model that appears to offer advantages in causal tasks. Benefits are shown in terms of the speed of training and some marginal benefits in performance.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The main strength of this paper is a noticeable speed up in an alternative architecture to transformers. I am a fan of papers that look at ways to speed up sequence modeling. The Curren paper presents a nice idea. More specifically:
- The approximations to the TNN appear to be effective in experiments and result in a faster network
- Approximations do not seem to deteriorate performance and may offer some added performance boosts
- The changes to the architecture are grounded in some theory 


Weaknesses:

**Disclaimer**: I am not an NLP expert and am more focused on the theory side. I have significant concerns about the theory in this paper, but feel the experiments and techniques are solid enough to potentially overcome that issue. Furthermore, none of the theorems are crucial to the crux of the paper, and if any are wrong, they can be removed. For this reason, I placed a borderline accept rating for now, but I believe this will need to be confirmed by people who are closer to the experimental side of the literature and can assess the experiments in a more rigorous fashion. 

\
Broader comments:
- Most of my larger concerns revolve around the theory and proofs in this project listed below.
- Reading through many times, I could not understand what was gained in the “causal training” proposal in section 3.1. First, it seems the parameters are changed to live completely in the Fourier regime. This change was made to obtain “an alternate causal speedup"", but I don’t see where that speedup arises. Following the steps, the main change seems to take the algorithm to do an FFT on the $n$-dimensional space resulting in runtime of $O(n \log n)$ which is worse than before. Also, changing the algorithm to work in Fourier space introduces a different implicit bias that I’m not sure is desired. For example, the $\lambda$ parameter controlling the decay is not controlled here. I also have many questions about this approach which I’ve left below.
- Experiments in table 1 don’t appear to offer much improvement especially considering the added parameters. I would also ask the authors to include citations to the models or results compared to in this table so it is easier to see what is being compared to. 

\
Theory comments:
- Theorem about ReLU MLPs being $d$-piecewise linear has assumptions missing or is just wrong. If the authors are implying that any ReLU network from $\mathbb{R} \to \mathbb{R}^d$ has $d$ pieces or contiguous linear regions, this is clearly wrong. MLPs are universal approximators so this is clearly false. If the authors are saying that this only holds for an MLP with a single hidden layer of width at most $d$ then this may be correct. But I don’t see this assumption made anywhere.
- Theorem 2 has a few confusing elements from my end. First, it is hard to parse. There are many variables and factors like condition number that it is hard to know the scaling of. Second, the bound doesn’t appear to be all that good. The error grows at least linear in $n$ and depends on other factors like singular values or the nystrom error that may also be badly bounded. I suppose the authors would argue that it is exponentially small in the degree of interpolation $N$, but this degree would have to grow at least logarithmically in $n$ to counteract the $n$ factor. This would result in a runtime essentially equivalent to just doing FFTs on the whole space. Third, the error in interpolation is an unusual thing to even bound in my opinion. The weights are updated with this interpolation taken into account. In other words, the algorithm learns a weight matrix with parameters contained in this interpolation. 
- Definition 2 and 3 present the discrete time Fourier transform, but in practice only the DFT of the matrix form is ever used. The resulting statements, regardless of their correctness, do not seem to apply to the setting in practice. Unless I am missing something.
- Related to the above, I cannot see why Theorem 3 and 4 are correct. Similar to my previous statements, MLPs are universal approximations so they can output any possible function. How can that statement hold true? 

\
Small:
- Simply having a % label on the y axis of Fig 1B is confusing. Percentage relative to what? Also, what does 20% speed-up mean; i.e. that it ran in 20% less time? Simply having this number on the first page can be confusing without the context added.
- Fig 1A and 1B also appear to be different size fonts.
- Line 150: I think the dense-case runtime is only a factor of $r$ worse so $O(nr  + r \log r)$ and not $O(nr^2 + r \log r)$ unless I’m missing something.
- For someone outside of the NLP community, section 3.1 needed more motivation and formality. Some details about what “causal masking”, “causal kernel”, and the sequential nature of the data would be helpful.
- To follow easier, it would be good to define what the role of N is in section 4.1 (i.e., number of interpolating points)

\
Formatting:
- Hyperref links seem to be broken
- Line 189: sentence is a run-on and hard to follow

\
Finally, to add ideas not for criticism, but instead for completing the paper and offering new ideas, there is a wealth of literature on related techniques that could be useful here, or at the very least cited. For example, there are sparse Fourier transforms that can offer speed-ups beyond the $O(n \log n)$ that the paper aims to improve on, e.g., [1]. Since matrices are low-rank and/or sparse, this could be a more direct way to get the speed-ups desired. Second, there are a number of papers on optimizing structured matrices like Toeplitz matrices, e.g. [2]. In the sequence modeling specifically, there have been a lot of papers on unitary networks for example [3], one paper which actually uses low rank approximations in its implementation [4]. Third, low rank approximations have also been used to speed up other architectures like conv-nets, [5-6]


\
**References:** \
[1] Hassanieh, Haitham, et al. ""Simple and practical algorithm for sparse Fourier transform."" Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2012.\
[2] Kochurov, Max, Rasul Karimov, and Serge Kozlukov. ""Geoopt: Riemannian optimization in pytorch."" arXiv preprint arXiv:2005.02819 (2020).\
[3] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. ""Unitary evolution recurrent neural networks."" International conference on machine learning. PMLR, 2016.\
[4] Kiani, Bobak, et al. ""projUNN: efficient method for training deep networks with unitary matrices."" arXiv preprint arXiv:2203.05483 (2022).\
[5] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\
[6] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.



Limitations:
There is a very brief discussion of limitations in the conclusion, though I feel this could be expanded. I would also appreciate some context for this work in relation to other works in NLP and how it fits into the broader landscape of NLP architectures. For someone like me not in the community, this would be useful to better understand its limitations from a practical perspective.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors of the paper propose two modifications of a recently published alternative to attention mechanism, Toeplitz Neural Operator (TNO), which constitutes the most important part of Toeplitz Neural Networks. The application of TNO is the multiplication of the input sequence by a Toeplitz matrix. Parameters of this Toeplitz matrix are given by a lightweight feed-forward network, called Relative Position Encoder (RPE). The first proposed modification, called SKI-TNN, represents a learned Toeplitz matrix as a sum of a sparse and low-rank matrix. Thus, its multiplication by a vector has the complexity of O(nr^2 + r log r) instead of O(n log n), where r is the rank of a second summand. However, this modification can speed up only the task of bidirectional modeling. In order to speed up the causal modeling (such as autoregressive language modeling), authors view the TNO as an application of kernel to a vector. Then, they train the RPE to model the real part of the Fourier transform of this kernel. The imaginary part is then computed via the Hilbert transform of the real part. This modification does not change the asymptotic complexity, but achieves empirical speed up.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The article explores an important topic of speeding up the token mixing part of a general sequence modeling pipeline. Nowadays, this topic is highly relevant because of its applications in the field of NLP. The article builds off of a very recent paper [1].
2. The article creatively combines together a large body of previous work. It uses the ideas of TNN, SKI, FFT, Hilbert transform, Nyström approximation, fast causal masking, etc. 
3. The experimental results show that the proposed modifications do indeed speed up the original TNN.
4. Overall, the presentation style is mathematically strict and to the point. The formulae in section 3.2.1 and in Appendix are sufficiently well-explained. Both modifications proposed in the paper are succinctly defined in Algorithm 1 and Algorithm 2. This helps the reader significantly to understand the main ideas.
5. In section 3.2.1, the authors specified not only the theoretical complexity of their modification, but also the practical limitations they meet when implementing it, and specified the practical complexity as well as theoretical.
6. The theory on the smoothness in Fourier Domain is supported with experimental visualizations
[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023.


Weaknesses:
1. The main claim of the article is the speedup achieved by the proposed modifications. The only results supporting this claim are Fig. 1 and some percentages in the text (in section 5.1). Fig. 1 shows the performance on specific tasks from the LRA benchmark. Firstly, it is not clear for which task the baseline (TNN)  is evaluated. Secondly, the choice of the tasks shown on the graph is questionable. The hardest task from the LRA benchmark, Pathfinder-X, is not shown. As for the speedups mentioned in the text: it would be better to put them all into a separate table. Moreover, it would be interesting to see a speed comparison in the form of a table similar to Table 5 from the TNN article.
2. In section 4.2, theoretical results on the choice of activations are presented. Several possible improvement ideas. The ablation study with experimental results for different activation types would be of interest. Moreover, the graphs showing the decay rate for randomly initialized networks might be improved. It would be better to leave only the lowest and highest lines and show the average line in between. Also, if you compare the rate of convergence to some baseline rate (e.g. exponential), plotting it would be appropriate. In addition, it seems to be not quite fair to compare the decay rates of trained and untrained networks.
3. While the overall presentation style is to the point, as mentioned earlier, it would help the reader if the abstract, introduction and related work were more general. Both abstract and introduction may be hard to read for an unprepared reader, as they contain too much mathematical details and not enough motivation. Moreover, some paragraphs of the introduction repeat the abstract almost word for word, while rephrasing would help the reader to understand the ideas more deeply. The related work section should describe either the ideas of mentioned papers or their connection to your paper more clearly.
4. The section 3.2.2 is a bit obscure. “Inverse time warp” is not a common term, and it is not described in the section.


Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents several techniques to speed up Toeplitz neural networks (TNNs). In particular, TNNs use convolution of length n (sequence length of the input) and so scales as O(n log n), and TNNs have many calls to the MLP that generate relative positional encoding (RPE) and decay bias. To reduce the time of convolution, for bi-directional modeling the paper proposes to approximate the Toeplitz matrix as a sum of a short convolution and a low-rank matrix, which results in O(n + r log r) complexity where r is the rank of the approximation. For uni-directional model (e.g. auto-regressive modeling), the paper proposes to parameterize the convolution directly in frequency domain and uses the Hilbert transform to obtain the imaginary part from the real part of the filter to ensure causality. The approximation error is then analyzed. Validation on language model (Wikitext-103) and long-range benchmark (LRA) show that the approximation lead to some speedup (10-15%) and the quality stays around the same.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The idea of using asymmetric Nystrom to approximate the Toeplitz matrix is quite clever. This allows a decomposition into a sparse and a low-rank component, which leads to asymptotically faster algorithm in the case of bi-directional modeling.

2. While uni-directional modeling prevents the Nystrom technique due to causal masking, parameterizing the filters directly in the frequency domain is able to overcome this challenge. While this is not asymptotically faster, it avoids one inverse FFT per layer and leads to some speedup.

Weaknesses:
1. Unclear what problem the paper is trying to address, and how it is motivated.
The intro starts out with Toeplitz neural networks, and the paper aims to make it faster. However, it's not clear why we want to make these faster, and what we would enable if we make these faster. Are they being used in very large-scale tasks? Are they being scaled to very long sequences?
While the technical contributions are solid, it's not clear to me why the paper chose to tackle this problem.

2. Unclear what the technical challenges are. 
- The paper mention that they want to avoid O(n log n) computation. But in practice O(n log n) isn't very slow, especially on GPUs. FFTs are pretty much bounded by memory bandwidth, and they take only 2-3 times as long as any pointwise operation. So if the goal is to speed up TNNs, then it makes more sense to have an efficient implementation, rather that using algorithms that faster asymptotically (O(n + r log r)) but is slower than a hardware-friendly algorithm (line 150, where using matmul with O(n r^2 + r log r) is faster). 
- The paper mentioned ""many calls to the RPE"". Why is this a problem? Showing a profile of how much each operation is taking will be much more convincing. That would motivate the approaches in the paper much better.
Without knowing how long each operations in TNNs are taking, how do we know that we're solving the right problem?

3. Lack of detailed speed benchmark. Given the goal is to speed up TNNs, I would have expected one of the main results to be speed benchmarks, across different sequence lengths, on different devices, to show the tradeoff. In the main paper, speed is only reported in Figure 1b, which is end-to-end speed for a particular sequence length (512).
How do we know that we're close to the maximum speed on these devices (GPU)? Or are we still far from optimal? When we speed up convolution and RPE, what is the remaining bottlenecks.
Having these would make the paper stronger.

At sequence length 512, an optimized implementation of attention (e.g. FlashAttention) is likely faster than FFT and the method in this paper. This is my impression as the Hyena paper [1] reports that TNNs are not faster than FlashAttention until sequence length > 4k.

[1] Hyena Hierarchy: Towards Larger Convolutional Language Models. Poli et al. 2023.

Limitations:
Not necessary.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes to reduce the computational complexity of Toeplitz neural networks. TNNs are a new form of network for sequence modeling that reduces space complexity of the attention matrix to allow for longer sequences.TNN model consists of a stack of Gated Toeplitz Units (GTU) that includes TNO (Toeplitz Neural Operator) that does token mixing with relative positioning. Then, GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO). 
The paper addresses the TNNs efficiency limitations: 1) super-linear computational complexity 2) many calls to the RPE: for each layer, one call per relative position. Thus, the paper proposes to reduce both the complexity of sequence modeling and of the relative positional encoder. The work proposes solutions by means of both the Structured Kernel Interpolation (SKI) [2] and working with frequency domains.
It does so through:
–	Approximating Toeplitz matrix using low-rank approximation and replacing the RPE MLP with linear interpolation and using Structured Kernel Interpolation.
(for O(n) complexity, that is use linear interpolation over a small set of inducing points to avoid the MLP entirely   -using an inverse time warp to handle extrapolation to time points not observed during training)
–	Causal training, SKI does not bring benefits, so instead they eliminate explicit decay bias by working in the frequency domain, using Hilbert transform (to force causality) and also use some smoothness
–	 For the bidirectional case, they eliminate the FFT applied to the kernels.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The work includes a number of solutions to improve TNN speed-up (addressing RPE MLP, the FFT, and the decay bias).

RPE is a neural network to obtain relative position embedding to obtain entries in Toeplitz matrices. These entries could be evaluated with stationary non-SPD kernel which is a good idea.

So first decomposing Toeplitz matrix and then using interpolation for MLP is a comprehensive pipeline.

The theory part makes the arguments more sound, and the explanations in supplementary materials are fairly abundant.

The experiments on LRA show good predictive performance on long range data and on wikitext some speed-ups.


Weaknesses:
The major paper of the paper talks about the SKI to accelerate the TNNs but in experiments SKI is only shown in the LRA experiment and does worse than both TNN and FD-TNN

As mentioned in the paper, doing sparse-dense multiplication in practice can be slower than dense-dense matrix multiplication (but that is only part of the potential speed-up)


Limitations:
Yes.

Rating:
5

Confidence:
3

";0
UvBwXdL95b;"REVIEW 
Summary:
This paper solves an interesting problem — joint pose and NeRF optimization on in-the-wild image collections. Unlike prior works such as BARF, this work aims at handling unconstrained images with varying illumination and transient occluders. To tackle this problem, this work incorporates learnable camera parameters, depth prior, and semantic features from DINO as supervision into the NeRF-W framework. Experiments demonstrate that the proposed method outperforms BARF and its variants on unstructured Internet images.    

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.  This work identifies an interesting and important research problem — joint pose and NeRF optimization on in-the-wild images. Prior works on the joint pose and NeRF optimization are limited to controlled settings. 
2. The proposed method is intuitive and combines the NeRF-W framework and several components (e.g., depth prior, semantic features, etc.) from other works (such as NoPe-NeRF).  

Weaknesses:
1. The main contribution of this paper is the problem setting — pose and NeRF estimation in the wild. The proposed method combines NeRF-W and BARF, which has limited technical novelty. 
2. The presentation may be improved. Figure 1 and Figure 2 have not been referred to and explained in the main text. 
3. Missing references:  Meng et al. GNeRF: GAN-based Neural Radiance Field without Posed Camera. ICCV 2021

Limitations:
I did not find discussions on the limitations of this paper. It would be great to discuss the limitations and analyze the possible reasons and future works.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper tackles NeRF training from in-the-wild Internet photos without pre-computed camera poses. The main idea is to leverage (self-supervised) image features and a carefully designed optimization strategy, together with ideas from existing work, including modeling transient regions and using mono-depth supervision. These components work in combination to avoid local minima and enable joint optimization of both pose and NeRF from scratch.

Experiments show that this joint optimization pipeline successfully recovers camera poses in several in-the-wild scenes and leads to good NVS results comparable to those with COLMAP preprocessing, whereas the previous work of BARF and its variants all fail.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
### S1 - Technically sound approach to a challenging problem
- The task of estimating camera poses on raw in-the-wild Internet photos using a gradient-descent-based optimization is challenging, due to noisy correspondences and local minima.
- The proposed method incorporates many good ideas from existing work and works well on a number challenging scenes.
-- For instance, the use of self-supervised image features for registering semantic correspondences via rendering;
-- The mechanism to model transient objects using learned opacity. This paper further adapts the original volumetric opacity in NeRF-W to per-image opacity, which is claimed to be more effective.
-- The use of depth supervision from pre-trained models.

### S2 - Promising results
- The paper demonstrates good results on a number of challenging scenes using Internet photos, where existing methods clearly fails.

Weaknesses:
### W1 - Complicated pipeline
- The resulting pipeline is complicated, involving many components, eg, 6 MLPs in total, and easily becomes confusing. It took me several passes back and forth to understand the exact implementation.
- It also requires a heavily crafted training schedule, gradually activating and deactivating some of the components.
- A critical concern on such a pipeline is its robustness across various scenes. Would one need to fight against all the hyperparameters and the training schedule when training on other scenes. I strongly suggest the authors also present results on other standard datasets (W3).

### W2 - Unclear motivation and potential redundancy of some technical designs
- It is unclear to me why the per-image ""candidate embeddings"" can help with the pose optimization. It seems the motivation is to allow some of the per-image variations (multi-view inconsistencies) to be factored into these per-image embeddings. The paper presents ablation results which shows numerical benefits of this component, but why is it useful for pose estimation?
- Are they still useful in general in the case of a multi-view static scene without transient entities?
- Isn't this the job of the separate mechanism for handling transient regions?
- Also, it is confusing to me why the transient confidence weights $\mathcal{W}_i^{\text{depth}}$ are calculated from the candidate densities $\sigma^{(c)}$, rather than from the transient opacities $\alpha^{(\tau)}$. Are there some redundancy between the two?

### W3 - Only on one dataset
- The paper only presents results on one dataset, consisting of Internet photo collections of 4 scenes, which the method is specifically tailored to.
- However, the paper claims to solve general ""unposed NeRF"". I strongly recommend the authors to also test it on standard multi-view datasets, eg DTU, CO3D etc, to assess the robustness of this complicated pipeline.

### (minor) W4 - Do intrinsics matter?
- How are the camera intrinsics obtained? From COLMAP? If so, this would undermine the value of avoiding cumbersome COLMAP preprocessing.
- Or are they estimated or assumed to be some value for all images?

### Other comments
- There are a few existing work that leverages DINO for pose estimation of objects, which the authors should consider referencing, eg: Zero-Shot Category-Level Object Pose Estimation [1], LASSIE [2], MagicPony [3].
- Line 125: when the ""candidate embeddings"" are first introduced, it was not immediately obvious to me they are per-image embedding vectors. Also, are they jointly optimized? The term ""candidate head"" is also quite obscure to me.
- Fig 2: the blue arrow in the middle connecting $\hat{\mathbf{F}}_i^{(c)}$ to $\theta_4$ is slightly inaccurate, as the $\hat{\mathbf{F}}_i^{(c)}$ is the result of ray integration, whereas the input to $\theta_4$ should be the raw per-point feature $\hat{\mathbf{f}}_i$, if I understand correctly.
- What is the computational overheads compared to vanilla NeRF?

### References:
- [1] Zero-Shot Category-Level Object Pose Estimation. ECCV 2022.
- [2] LASSIE: Learning Articulated Shape from Sparse Image Ensemble via 3D Part Discovery. NeurIPS 2022.
- [3] MagicPony: Learning Articulated 3D Animals in the Wild. CVPR 2023.

Limitations:
The authors included a brief paragraph on the limitation of the image features. I would expect some discussion on the robustness of the proposed pipeline.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper produces a novel approach for optimisation of pose in NeRF scenarios. The core novelty is the addition of a candidate head that improves network stability when the images poses are not yet converged, along with some other tweaks like a transiency inference head or a feature field. 

Edit: I have read the rebuttal, and given the scores from other reviewers and I would like to keep my score.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The idea is novel, interesting and timely.
* The results show good improvements over the current s-o-t-a.
* The experiments section is thorough enough to demonstrate the qualities of the proposed approach. 

Overall, while my negatives might sound long and would seen to outweigh the positives, I think it brings a potentially valuable contribution to the community, so I very much support the acceptance of the paper. My main worry is about the somewhat confusing and short explanation of the core contribution of the “candidate” heads, which should be expanded and improved.

Weaknesses:
* The paper is somewhat confusing to read at times. For example “unconstrained images” are introduced and used without introducing the term. The meaning does become later in the paper, but it would be good to introduce things earlier on. Similarly, I do not see much point for Figure 1, as features have been used for this type of matching for years.
* Even though the literature review section of the paper is quite good, I think the paper does anchor things a bit too much on BARF, and ignores other works such as NoPe-NERF / GARF / etc in the comparisons. I do understand that some of these works were arxiv at the time of the submission (but are publications now), so this should not penalise the paper too much. That being said, it’d be nice if some of these comparisons could be added.
* I also found some of the notation difficult to follow initially as I found not find any outline of it’s meaning. An example is the (c) in line 126 which is not explained. 
* Looking at the core contribution of the paper (i) the intuition (i.e. the “roughly speaking … “ part at line 125+) could be expanded and (ii) the size of the embedding should have been ablated in the results section.
* The results section could have been expanded e.g. with (i) the ablation noted above, (ii) extra ablations where, e.g., the feature matching part is turned off.

Limitations:
* To some extent, but the paper could benefit from a more clear failure case section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel method for optimizing NeRF without a pose-prior and on in-the-wild image collections containing transient occluders and varying lightings. The main contributions are four fold. Firstly, the authors propose a candidate head for NeRFs that uses image-level representations via a learned embedding for compensating inaccurate poses during the early optimization. Secondly, learning a view-independent feature field based on DINO features as intermediate representation increases the robustness of the joint optimization wrt Varying lightings, weather and time. To reduce the impact of transient occluders the authors suggest using a separate network that predicts occluder in 2D image level based on the feature maps. Additionally, to achieve higher accuracy in the geometry the authors apply monocular depth supervision on regions without occluders. Experiments are conducted on four scenes of the Phototourism dataset with initial camera poses set to the identity.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1) The paper is very nicely presented and easy to read.
2) The paper tackles the challenging and relevant problem of in-the-wild NeRF reconstruction without given posen. The contribution is clear and solves different subproblems that emerge in this domain, e.g. transient occluders.
3) Most parts are well motivated, the methodology is technical sound and experimentally justified e.g.:
 - Candidate head sounds plausible and improves pose optimization and image quality significantly, see Figure 4 and Table 3.
 - Feature field optimization seems to help for in-the-wild images, see Table 3.
 - Depth supervision improves performance, see Table 3 and Figure 5.
4) The experimental section contains a comparison to BARF, adapted variants with additional supervision cues and numbers for NeRF-in-the-wild. It appears to be a plausible choice and supports the contribution of the paper.
5) The authors provide code in the supplementary materials. 


Weaknesses:
1) The candidate head is introduced to output color and density, equation 4, however it is later used to predict features, see Figure 2. This appears to be misleading for the readers in the beginning.
2) One of the main limitations of NeRF based methods is the optimization time and an analysis is missing on that. It would be good to discuss the optimization time for the method and the baselines. To get a sense if the time and computational effort is comparable to COLMAP + NeRF-W and others.


Limitations:
Limitations are discussed in the supplementary.
I'd suggest that the authors discuss the overall optimization time as a general limitation of the method, if applicable. There are state-of-the-art NeRF architecture, such as Instant-NGP that facilitate optimization in a few minutes instead of hour or days, which might be a good follow up.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a joint camera pose and NeRF optimisation method that can handle transient scenes, including moving objects and various light conditions, by integrating NeRF-W, BARF, NoPe-NeRF, and DINO-based feature-metric loss in a sophisticated way. 

The method is evaluated on four scenes in the Phototourism dataset, showing good performance compared with (modified) BARF baselines.

---
**After rebuttal**: I have read authors' rebuttal and it addresses my concerns.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It’s a challenging task to estimate camera poses in scenes with moving objects and different light conditions, especially in a NeRF setup. This method proposes to make the pipeline more robust by considering
* Moving objects and lighting conditions similar to NeRF-W;
* Pose optimisation similar to BARF;
* Monocular depth un-distortion similar to NoPe-NeRF; and 
* DINO-based Feature-metric.

In short, the proposed method is novel and leads to promising results. It’s also a plus that code is also provided in supplementary.

Weaknesses:
I think a primary concern is how robust the pose estimation is in more scenes. The method is only evaluated in 4 scenes. Is it possible to have an experiment, especially for pose estimation successful rate on more scenes?

I understand that it takes a long time for NeRF to reach the best rendering quality, but we should be able to tell if the pose optimisation is successful far before NeRF converges. For example, we can consider pose estimation successful if rotation error is lower than $\theta$ degrees in $x$ epochs. In this way, we can see the pose estimation success rate in many more scenes.

Limitations:
Yes

Rating:
7

Confidence:
4

";1
JVzeOYEx6d;"REVIEW 
Summary:
The paper presents a human preference model for text-to-image generation and a method to enhance text-to-image models using this preference model. To achieve this, the authors develop a human preference annotation pipeline and create a dataset consisting of generated images and human ratings. The proposed model is trained to predict human preference rankings, and experimental results indicate that it aligns better with human preferences than existing automatic measures. Additionally, the paper introduces a learning method to fine-tune a diffusion model using the human preference model. The experimental result demonstrates that the text-to-image generation model, when adjusted with the proposed method, is preferred by human annotators.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper's clear contribution is the human preference dataset, which features high-quality annotations from a professional data annotation company.
- The experiment suggests that ImageReward outperforms popular measures like FID and CLIP scores in evaluating text-to-image generation.
- By training the text-to-image model using the human preference model, the authors achieve improved performance in both automatic and human evaluations.

Weaknesses:
- The paper exceeds the page limit. The authors should carefully follow formatting instructions and revise the manuscript accordingly.
- The description of ImageReward training lacks detail, which may make reproduction difficult.
- Information regarding the human evaluation of experiments in Section 4.2 is missing.
- Certain aspects are unclear. For example, the intention of Figure 7 is unclear for me. What image generation problems are demonstrated in the examples? The numbers in Table 2 are unexplained. What agreement measure is used?

Limitations:
The limitations section addresses three main issues:
- Annotation scale, diversity, and quality
- The heuristic nature of RM training
- The lack of theoretical background for training the diffusion model with RM feedback
The authors provide meaningful suggestions for future research directions.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This study presents ImageReward, a general-purpose text-to-image human preference reward model. They have collected 137k expert preference dataset, which contain a lot of rating and ranking annotation.  Additionally, the authors propose Reward Feedback Learning (ReFL), a direct tuning algorithm designed to optimize diffusion models. The performance of ImageReward surpasses that of existing models and metrics.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The paper is very well presented with clear paper writing and good demonstration.
2. The idea is very novel, which explores directly using the human preference as the supervision signals to tune the pretrained text-to-image generation models
3. they also collect a large-scale high-quality human preference dataset, which can inspire many future works. 

Weaknesses:
BLIP, being outdated, falls short in generating accurate and comprehensive image descriptions. An alternative approach is to employ more recent models like MiniGPT-4 or LLaVa, which have the potential to produce superior reward scores. These advanced vision-language models not only comprehend the objects within the image but also grasp the emotional and artistic aspects. It would be beneficial if the authors could include a comparative analysis involving these models to further strengthen their findings.


Limitations:
it is well discussed in the paper

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper explores human preferences and introduces an ImageReward mechanism, which can be employed for evaluating text-to-image generation. The study further enhances the performance of existing text-to-image generation models through a Reward Feedback Learning (ReFL) approach. The experiments primarily focus on the alignment between the proposed ImageReward and human preference/judgment, which is annotated by real individuals.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem addressed in this paper is vital as it aims to judge the alignment between text-to-image generation models and human preferences during both training and evaluation. The proposed ImageReward model could effectively guide both the training and evaluation processes of text-to-image generation models.

Weaknesses:
There are some concerns about experiments including the considered generative models and the results of the correlations with human preferences/judgments. Please refer to more details in the following.

Limitations:
The authors discuss the limitations and broader impact in the last two sections of the paper and propose potential solutions and outline future directions for further research and development.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper introduces ImageReward, a new dataset of human preference over generated images given a text prompt. Human preference for images is rated across three dimensionalities: text alignment, image fidelity, and harmlessness. Using the dataset, they train a reward model to score the generated image and text prompt pair. The reward model consists of a small trainable MLP head over BLIP text-image features. The paper further proposes a baseline method of fine-tuning the generative diffusion model using the score model to increase the generated image alignment with human preferences. The fine-tuning loss is a weighted sum of standard diffusion loss and the one from the scoring model (only at the predicted images at lower timesteps of diffusion).

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The annotated dataset of human preference is one of the first datasets of this kind and scale. This will help in further research in both text-to-image model evaluation and improving the generations with higher human preference. 
   
Both the reward scoring model and fine-tuning method based on the score model are shown to work on par of better than existing baselines.  

The paper consists of extensive analysis and details regarding the dataset annotation, scoring model, and its comparison to recent methods. 


Weaknesses:
1. One limitation of the dataset might be that it becomes less relevant as the generative models improve. Given that the dataset only consists of generated images with their corresponding text prompt and not a plausible ground truth image for the prompt with the best possible human preference score. 

2. Only fine-tuning the model on 0-10 timesteps with the scoring model doesn't seem optimal. Specifically, in cases of object omission, the layout has already been decided in the initial stages of diffusion; thus, the reward score guidance at later stages might not be effective. Is there any ablation or analysis regarding what metrics among fidelity and text alignment improve the most? 

3. In Eq2, phi is implemented as a ReLU function, as mentioned in line 260. Probably this should be ReLU over the negative of the score function. Because the higher the score, the better. Or is my understanding incorrect? 

4. It would be great to expand on the evaluation setup and metrics, which are sometimes not very clear.

    (a) In line 122, are the 100 real user test prompts different than the ImageReward dataset used to train the scoring model? Similarly, 466 and 371 prompts in Table 3. 

    (b) How is the ""filter"" evaluation metric calculated in Table 3? Does this calculate the number of times the model didn't select the worst image in top-k?
    
    (c) In Table 4, it's unclear how the evaluation numbers are reported. Does it denote the #winrate for each method out of total N samples (N being the sum of the column), or is it a binary comparison of each method vs the baseline? How many generated images per prompt over the 466/77 prompts were used for the evaluation? 

    (d) Some of the baseline, e.g., reward weighted fine-tuning method, performs worse than the baseline in Table 4. Is there any analysis regarding that? 


Limitations:
yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper aims to improve text-to-image (T2I) from human preference feedback. They first collect a human rating dataset to train their human preference model, ImageReward. With the reward model, they further optimize a pre-trained T2I via the proposed Reward Feedback Learning (ReFL). The experimental results indicate that their ImageReward is more robust than the widely-used CLIP-Score, and ReFL-optimized T2I also performs better than baselines.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
+ This paper is well-written and easy to follow.
+ The collected dataset is valuable for the V+L community as well as the trained ImageReward, which can help various visual generation tasks (not only T2I).
+ The proposed ReFL pipeline can keep improving the T2I model. Both automatic metrics and human evaluation support the superior performance of their framework. 
+ They provide lots of qualitative examples and detailed discussion in the supplementary.

Weaknesses:
+ The novelty can be an issue since the human preference-trained reward model and feedback learning are already introduced in large language modeling (LLM). It looks like they just apply the same pipeline from LLM to T2I.
+ It is not easy to collect large-scale human preferences for reward model training. Is there a more efficient way to build ImageReward instead of fully relying on human annotations?
+ A detailed analysis of ImageReward should be considered. For example, how many human preference pairs can lead to how well ImageReward and then how well the final optimized T2I model is.

Limitations:
Since their pipeline relies on the collected labels, the human annotation can contain ethnic issues, which they also discuss in Appendix B.

Rating:
7

Confidence:
4

";1
EoDpq18R30;"REVIEW 
Summary:
The paper first introduces two desirable properties for network embedding. The discrimination property expresses how well the model is able to distinguish node pairs that are similar from those which are dissimilar. The monotonicity property expresses how the similarity scores obtained from the embedding follow the same order as the probability of a pair in the original dataset. The authors first define these two properties, argue why they are important for network embedding methods, and illustrate them with an example.
Then, they consider a general framework for network embedding algorithm.

They present a standard expression of the likelihood of such models, discuss the elaboration of positive and negative samples sets, and derive elementary results of such models regarding the monotonicity and discrimination properties. They show both can be satisfied in an ideal case where infinite data is available (possibility results), and how both cannot be simultaneously satisfied for finite dataset (impossibility results). Doing so, they derive an optimal expression for the similarity between pairs of nodes, and how the positive sampling distribution should (negatively) correlate with the negative sampling distribution in order to satisfy monotonicity. Finally, they analyse the expected gap between empirical optimal solutions (finite dataset) and theoretical optimal solutions (infinite dataset).

They discuss the implication of these demonstrations. In particular, they detail some caveats of state-of-the-art sampling strategies in light of their findings.

Finally, they design a simple embedding procedure (SENSEI) that focuses on satisfying the discrimination and monotonicity properties to some extent. They show on 4 real-world datsets how their approach outperforms existing methods both in terms of link prediction (related to the discrimination property) and node recommendation (related to the monotonicity property).

They conclude the paper with a short overview of existing works on network embedding.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem discussed is general; many previous works could be studied in the light of these findings
- Writing is clear and sections are well organized.
- Experiments are well carried out, with ablation tests, numerous baselines and four real-world datasets.
- The discussion of previous works limits in the light of some findings (Thm.2) is interesting and insightful.


Weaknesses:
- Illustrations could be improved. The first line of Fig.1 is not very helpful, gray bars are not explicitly defined, and the symbolic representation of p(u|v) is unclear. I believe simpler visualisations are doable. Also, figures should take the entire width of the page, in my opinion, instead of floating in the middle of the text.
- There are several mentions of ""large"" and ""small"" values of p and s. While it helps get a sense of what is discussed, I believe these should be better defined, as they are key words in the demonstrations. In line 233, the authors introduce the ""intermediate"" values with a rigorous definition; I believe discussing this above in the text might help to reach an explicit definition of ""large"" and ""small"".
- All demonstrations are in the appendix. I understand that available space is an issue for large mathematical formulas, but proofs for Th.1 and Prop.1, that are key in the text, are short enough to be included in the main text. In their absence, the main paper cannot be considered as self-sufficient. 

Limitations:
In the words of the authors, with which I agree: 
""The limitations of our paper lie in that the theoretical analysis is conducted on proximity scores and the proposed SENSEI model is designed for plain (non-attributed) networks rather than attributed networks. The key to generalize the proposed properties/SENSEI model to attributed networks is a new definition of the “proximity” (i.e., node pair similarity). Currently, the proximity refers to the topological aspect of the node pair similarity.""
These limitations lay the ground for broader future theoretical works on the topic.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This work targets on network embedding under different strategies. The authors first theoretically demonstrate the dilemma in existing network embedding algorithms, especially the sampling-based framework, that the sampling-based embedding performance can have an error gap due to the competing sampling strategies.  Then, they propose two properties, i.e., discrimination and monotonicity, that should be obeyed, and eventually, under this perspective, they propose a novel network embedding approach called SENSEI to perform the (plain) network embedding. The SENSEI includes two steps, where the first step samples nodes with intermediate distribution following the first category of sampling strategies to satisfy the discrimination property, while the second step follows the second category of sampling strategies to partially satisfy the monotonicity property. Experiments on several datasets show the effectiveness of the proposed approach.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
+ This work has a clear motivation to devise a new reconciling strategy to satisfy the discrimination and monotonicity properties. 
+ The theoretical analysis is sound and details the significance of the two steps. Corresponding discussions clearly tell the advantages of the proposed embedding approach.
+ The performance of SENSEI is not incremental in both link prediction and node recommendation tasks. Though the work mainly performs on plain networks, the authors still give the results on multi-layer networks.
+ The proposed approach seems not to be difficult to reproduce.


Weaknesses:
- This work can only be effective on sampling-based embedding algorithms, though I think it is valuable for this field.
- According to the results of Table 2, the promotion of step 2 is incremental. However, will step 2 harm the performance of the link prediction task? (See my second question in `Questions’) If so, the necessity of reconciling the competing sampling strategies is not very strong.


Limitations:
Yes. The authors claim that the attribute could have helped promote the approach.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Graph embedding methods typically involve negative sampling, where a set of nodes are sampled as positive samples (usually nodes of close proximity, such as edges) and a set of nodes are sampled as negative samples. While early work tended to randomly sample negatives from non-edges, more recent work has adopted different approaches for negative sampling. The authors identify two main classes of methods for negative sampling in the literature. Their main contribution is identifying two properties that they claim any good set of node embeddings should satisfy: discrimination and monotonicity. They then demonstrate that these two properties cannot be simultaneously satisfied with a finite number of samples. They propose the SENSEI method that satisfies discrimination and a *partial* monotonicity property and demonstrate strong empirical performance.

*After rebuttal:* The authors have clarified my concerns, and I continue to support the paper. I also did not see anything from the other reviews that would negatively affect my opinion.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- Provides an explanation unifying different negative sampling strategies for graph embeddings based on different desirable properties. This is a novel way of thinking about the graph embedding problem.
- Interesting theoretical results with practical applicability: a possibility theorem to satisfy the two desirable properties in with infinite samples along with an *impossibility* theorem to satisfy the two desirable properties in practice with finite samples.
- New SENSEI algorithm that satisfies a weaker version of the two desirable properties and shows strong empirical performance.

Weaknesses:
- Generality of results is questionable. The proposed properties of discrimination and monotonicity are very much tied to two assumed tasks: link prediction and node recommendation. If one is only interested in one of these tasks (or perhaps a different task altogether), then the trade off between these two properties may not apply.
- Discussion of limitations should be moved to the main paper.


Typos and minor issues:
- Figure 1 appears very early on in the paper, before the detailed discussion on the first and second category of methods on page 6. I did not understand it until reading page 6. I suggest the authors place a shorter summary of the first and second categories in the figure caption or earlier in the paper around the first time Figure 1 is used.

Limitations:
Limitations are discussed, but not in much detail and only in the supplement. Such a discussion should be moved to the main paper, with a longer discussion in the supplement if necessary.

Rating:
8

Confidence:
4

";1
6kRQTPEVip;"REVIEW 
Summary:
The paper proposes a set of methods for modeling chemical reactions that involve radicals during the reaction process. The authors first introduce the current landscape of chemical reaction datasets based primarily on the USPTO and discusses the USPTO shortcomings in terms of reaction interpretability and its inability to showcase reaction involving multiple steps. Next the authors briefly describe the RMechDB dataset, which does contain pathways with radicals, followed by a description of their methods. The methods are based on OrbChain, which provides a standardized way of describing chemical reactions with radical and arrow pathways. After that, the authors introduce their predictive methods, including two-step prediction, plausibility ranking, contrastive learning with a reaction hypergraph and text-based sequence to sequence models. The authors conduct experiments for all the aforementioned methods to further understand their capability in accurately describing radical reaction pathways on the RMechDB dataset. The performances of the different methods vary across different settings of the conducted experiments. The authors then provide a pathway search example, further description of their package and a conclusion. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper provides has the following strengths:
* Originality: The papers provides a new perspective on chemical reaction modeling that involves radicals and is also more interpretable for classically trained chemists. 
* Quality: The paper describes and analyzes four different and relevant methods for the radical modeling problem and provides clear motivations for their importance.
* Clarity: The paper motivates the problem they address quite and describe the necessary background.
* Significance: Expanding the capabilities of machine learning models to provide more interpretable reaction models with more steps in the reaction process could have significant impact on various chemistry related problems.

Weaknesses:
The paper could be further improved:
* Providing a clearer description of the context of the results related to original problem the authors motivated. How well do the described methods provide more interpretability to chemical reaction modeling? How do the metrics the authors measure relate to that original premise? [quality, clarity, siginificance]
* The authors only briefly describe pathway search, but provide little context for what their results mean. What does a recovery rate of 60% imply? How does the reaction tree look like and how interpretable is it? [clarity]
* The authors refer the reader to the appendix very often, which I think contains a lot of significant information needed to fully understand the experimental results. I recommend putting more of that information in the main paper. [quality, clarity significance]
* The authors only provide a brief description of the RMechDB dataset and its unclear if that paper had any modeling methods the authors could compare their proposed methods to. Further clarification on this would be helpful. [clarity]

Limitations:
The authors do not provide a detailed discussion on limitations. A discussion on limitations would make the paper stronger.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors provide a reaction predictor system that provides an accurate and interpretable prediction of radication reactions. Due to the lack of training data, there is a dearth of reaction predictors for radication reactions. The authors present 3 deep-learning-based approaches. The first approach is a two-step process that identifies possible reactive sites and then ranks the reactive site pairs. The second approach uses a contrastive learning approach to identify the most reactive site pairs. Finally, the authors also show a transformer-based approach to perform sequence-to-sequence translation from products to reactants. In the two-step, OrbChain approach, the authors present a GNN-based approach to identify reactive sites and a siamese network-based approach to rank the plausible reactive sites. Multiple reaction representations to perform plausibility ranking. The contrastive learning approach also uses a GNN and both a custom atom pair representation and a hypergraph representation are evaluated. Finally, a pre-trained MolGPT on USPTO dataset is used as well. The authors find that the graph-based methods outperform MolGPT and the contrastive learning methods yield the most accurate results. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The authors compare multiple models to show the efficacy of different types of models such as GNNs and text-based Transformers for reaction prediction
- The authors also use multiple representations and model architectures for a very thorough evaluation of the proposed reaction 


Weaknesses:
- It is not clear how or which of the three algorithms described is used in RMechRP. 
- The presentation of the paper could be improved. There are 3 approaches described with multiple models and representations for some approaches. A short summary of the findings and comparisons or a visualization of the approaches could significantly improve the presentation


Limitations:
- Limited comparison as the authors don’t include a related works section so it is difficult to contextualize the scope of their current work to the field.
- Is there a reason the MolGPT model could not be trained on the new RMechDB dataset for evaluation rather than only fine-tuning?

Rating:
6

Confidence:
4

REVIEW 
Summary:
- a new model is described for prediction of radical chemical reactions
- the model is trained on a dedicated database of radical reactions for atmospheric chemistry, an important application
- several, reasonable baselines are evaluated

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- reasonable, state of the art ML modelling (contrastive learning, attention GNNs, reasonable reaction representations inspired from molecular orbitals, building on previous work by Baldi's group)
- reasonable strong baselines (transformers)
- compelling results
- important application

Weaknesses:
- other baselines, like MEGAN https://pubs.acs.org/doi/abs/10.1021/acs.jcim.1c00537 or https://www.nature.com/articles/s42256-022-00526-z could be considered 


### Related work 
Several references in the introduction are not correct:

The Cao & Kipf MolGAN paper should be removed, because it does not deal with chemical reactions.
similarly, the Rogers et al ECFP does not deal with reaction prediction, and should be removed in the intro.

On the other hand, the Segler et al paper should be cited as an ML paper.

The ELECTRO paper by Bradshaw et al should be added. https://arxiv.org/abs/1805.10970

contrastive learning to distinguish between plausible and implausible reactions has already been used in https://www.nature.com/articles/nature25978 (called in-scope filter there), which should be referenced as well

Limitations:
n/a

Rating:
7

Confidence:
5

REVIEW 
Summary:
Authors present two models that predicts radical chemistry reactions. The first model 'OrbChain' is comprised of two components 1) one GNN model for predicting pairs of reacting atoms/groups, 2) a model which ranks the plausibility of these pairs. The second model is a a fine-tuned Rxn-Hypergraph model, adapted for the task of predicting radical mechanism by using an atom classifier model.



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ Selects an interesting problem domain, specifically radical based chemistry.
+ Authors plan to open source the Radical Mechanistic Prediction model and release software for easier use.
+ Compares against relevant baselines, such as fingerprint representations, MolecularTransformer.

Weaknesses:
The presentation of results could be more clear. In particular:
- It would be helpful to have a clear statement of the key contributions provided by this work. The list of desiderata provided at the end of section 2 are important, but I believe that these properties have already been provided by previous models, especially references [20, 21] for radical reactions.
- If I understand correctly, OrbChain is the name of the two part model, but components of the model are still used for the second modeling approach using the fine tuned Rxn-Hypergraph model.
- The table formatting makes the results somewhat difficult to parse, it would be helpful to have more spacing between the caption and the table, and for the 
- Table 3, where there is one column with 'AP \n Morgan2 \n TT' and it wasn't immediately obvious that these are different molecular descriptors.
-  For Figure 3, I believe the reaction type should be 'Homolysis' rather than homolyze. 
- For Figure 3, It would be helpful to have a sense of the number of reactions in each class to better compare the relative performance by the model between reaction classes.
- There are several typos in the manuscript, e.g. a missing close parenthesis in lines 48-49 of page 2, 'weather' instead of 'whether' on line 188 on page 5, some tense mismatches. Please review for grammar errors.


In Section 2, authors state that 'None of the currrent reaction predictors can offer ... chemical interpretability, pathway interpretability, or balanced  atom mapping'. There are actually several models that provide interpretability for reaction mechanisms/reaction type. In addition to the works on radical mechanism prediction cited by the authors as references 20 and 21:

- In https://arxiv.org/pdf/1805.10970.pdf, Bradshaw et al. predict electron pair pushing mechansims with a generative model.
- The MolecularTransformer model has also been shown to provide atom mapping by visualizing attention weights (https://arxiv.org/pdf/2012.06051.pdf, Figure 2)./

For text based models such as Molecular Transformer, could you quantify the percentage of reaction predictions that suffer from a 'balance problem'? It isn't clear to me that this is a big issue with MolecularTransformer or other text based models.






Limitations:
I do not identify any negative societal implications.

By my understanding, the presented model is intended to be limited for only radical based reactions, as it is trained on this domain of reactions, and not for other types of chemical reactions.

Rating:
6

Confidence:
4

";1
M03sZkmJXN;"REVIEW 
Summary:
The paper addresses the challenge of integrating protein sequence and structure information to improve protein representations. The authors propose CoupleNet, a network that utilizes graph convolutions to model the relationships between protein sequences and structures. The approach involves constructing two types of graphs to model sequential features and structural geometries and performing convolutions on nodes and edges simultaneously. The proposed approach outperforms state-of-the-art methods on various protein-related tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) Comprehensive Protein Features: CoupleNet incorporates multiple levels of features in proteins, including residue identities, positions, and geometric representations. By considering both primary (sequence) and tertiary (structure) information, the model captures different aspects of proteins, leading to more informative and rich representations. The authors have demonstrated good work in engineering the features of their model. By carefully considering both primary (sequence) and tertiary (structure) information, the model has succeeded in capturing diverse aspects of proteins, leading to more informative and robust representations. 

2) Experimental Results: The paper presents experimental results on a range of protein-related tasks, including protein fold classification, function prediction, and domain prediction. The results demonstrate that CoupleNet outperforms state-of-the-art methods by large margins. The comprehensive evaluation and superior performance validate the effectiveness of the proposed model.

Weaknesses:
1) Limited Novelty: While the proposed approach in CoupleNet is interesting and effective, it does not offer a significant departure from existing methods. Similar approaches, such as GearNet, have previously explored this concept by integrating the radius and sequence information as different edge types within a single graph. Therefore, the idea of constructing two separate graphs in CoupleNet, while slightly different in implementation, does not present a significant departure from existing methods.

2) A weakness of the paper is the lack of a detailed explanation regarding why the proposed model, CoupleNet, performs better than the state-of-the-art methods. While the experimental results demonstrate superior performance, the authors do not provide a thorough analysis or insights into the specific aspects of the model architecture or design choices that contribute to its improved performance. Without a clear explanation of the underlying factors that make the model more effective, it becomes challenging for readers to fully understand and interpret the advantages of CoupleNet over existing approaches.

3) Lack of Code Release: One notable weakness of the paper is the absence of code release. The lack of code availability hinders the reproducibility and transparency of the research.


[1] Zhang, Zuobai, et al. ""Protein representation learning by geometric structure pretraining.

Limitations:
yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a representation learning framework using GNN for protein datasets. A key contribution is combining sequence and structure information using the proposed GNN. The learned representations achieve better performance on downstream tasks of protein fold classification and function prediction.


The sequence+structure methods have been previously investigated in several works, as included in the baseline. I don’t see the motivation behind using a “sequence-structure graph” that differentiates the novelty of CoupleNet. I suggest updating the abstract and introduction by mentioning the limitations of existing “ sequence+structure” approaches and then specifying how CoupleNet addresses that. Could discuss the drawbacks of existing “feature fusion” methods.

I think the primary area of this paper should be applications. The construction of a joint sequence-structure graph is specific to modelling proteins, and the rest of the GNN operations are standard. The structure graph is based on Ingraham et al., and the sequence graph is based on trRosetta et al.. The message-passing scheme is based on ComENet. I don’t see any methodological novelty.
Moreover, empirical results are reported without proper discussion on application to protein problems. The empirical results of baselines are reported based on the article [15]. Without error bars or cross-validation (commonly done for fold prediction and function prediction), it is hard to make any conclusions. At this point, I lean towards rejection.

########
Post Rebuttal
########

I have read the author's rebuttal. Overall, the authors did a great job in responding to the comments, and I have accordingly adjusted my score, recommending accepting the paper.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Learning protein representations that combine sequence and structure information is relevant to the community. Using GNNs is a good demonstration of DNNs' applications to biological problems like protein function prediction.

A new joint sequence-structure 3D graph that can model complex interdependencies for learning representations that achieve good improvement over existing approaches.

Weaknesses:
The introduction doesn't provide sufficient motivation behind the proposed method. It is unclear what the paper means by ""...methods cannot deeply integrate the information…."" What is ""deep integration""? The proposed method itself uses a type of message-passing mechanism, so what is the issue with message-passing cited in the article [8]? 

The paper should first identify the issues in existing ""feature combining"" methods. Then motivate how the proposed CoupleNet addresses that. For instance, could take an example of a specific protein where existing methods perform poorly and then discuss the complex interdependence between structure and sequence for that particular example that needs appropriate modelling for improving the performance of downstream tasks such as fold prediction. 

I would suggest adding a paragraph that discusses the importance of sequence-level information and another on structure-level information for tasks such as fold classification or function prediction. Discuss the importance of local/non-local residue contact information. Then establish how encoding for such a multitude of information can be helpful for downstream applications. I would suggest taking a look at non-homologous proteins. Could discuss the availability of large-scale high-resolution structure data and how deep GNNs can leverage that.


Line 94-99: No clear explanation of the benefits of complete message passing. Why is it essential to consider ""global"" completeness? What is the downside of ""local"" completeness? Why not cite SphereNet [a] here? Which notion of ""completeness"" does this paper build on?
It is implicit from the equation, but that simply is restating from the ComENet paper. The paper should be well explained, avoiding room for any such ambiguity.

This paper needs to explain better the need for global ""completeness"", which can be done by discussing the importance of conformers like structure. Discuss the equivalence of 3D graphs under SE(3) transformations for protein structures and why that's important to be considered for representation learning.

In my understanding, the classes in the fold classification task tend to be unbalanced. I suggest authors report per class accuracy.

[a] Liu, Yi, et al. ""Spherical message passing for 3d graph networks."" arXiv preprint arXiv:2102.05013 (2021).

There is no comparison of time-space complexity.

Limitations:
Although a sentence is added on limitation, ”. A limitation is that the detailed inter-relationships between sequence and structures remain to be explored and uncovered”. This sentence is just vague and provides no meaningful information. The paper consistently discusses “deeply co-model sequence and structures together”. So what could not be “deeply modelled” and needs more exploration? 

The sequence and structure of information have been combined in several existing works. The main issue is that the paper doesn’t establish clear motivation for combining the two using a combined graph. Moreover, the need to consider “completeness.”

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors proposed CoupleNet to co-model the protein sequences and structures. CoupleNet separately builds a sequence-based graph and a radius graph for message passing. It achieved state-of-the-art performance on several datasets compared with recent baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed CoupleNet utilized much biological knowledge to incorporate useful geometric information like backbone torsion angles and inter-residual torsion angles. This information may help the model better capture the structural information.

2. The experiments demonstrated the SOTA performance over a wide range of baselines. Ablation studies demonstrated the effectiveness of ""coupling"" the sequence and structural information.

3. The protein figures clearly and concisely defined the geometric features used in the paper.

Weaknesses:
1. The construction of two separate graphs (one for the sequence and one for the structure) is not very innovative. In fact, GearNet (already cited by the authors) already used this formulation and the description of the two graph construction in this paper is very similar to GearNet.

2. Though the backbone structure can be completely determined by the descriptor, the residue information is lost in the proposed model. Failure to capture this information may affect the performance on downstream tasks.

3. There is some confusion regarding the graph construction that can be clarified. See the following questions.

Limitations:
Limitations and potential negative societal impacts were properly addressed in the manuscript.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a network called CoupleNet that combines protein sequence and structure information to generate informative protein representations. The network utilizes multiple levels of features, including residue identities and positions for sequences, as well as geometric representations for tertiary structures. It constructs two types of graphs to model sequential features and structural geometries, and performs convolution on nodes and edges simultaneously to obtain superior embeddings. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods on various protein-related tasks. The paper highlights the significance of complete structural representations in learning protein embeddings and suggests further exploration of the inter-relationships between sequence and structures.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The paper is clearly written and easy to follow. Related work is thoroughly discussed, situating the context and contributions of the proposed method relative to prior studies.

* The experimental results show substantial gains over previous state-of-the-art baselines on benchmark datasets for protein structure prediction. The proposed method achieves these results with a relatively small training dataset, whereas some baselines utilize much larger resources such as the AlphaFold database. This indicates the model may be more data-efficient and able to elicit more from limited information.

Weaknesses:
* The relationships between protein sequence and structure, especially as they relate to function, are not deeply explored or discussed. For tasks like protein function prediction that aim to determine the utility or effects of a protein, understanding the connection between its sequence, structure, and biological role is critical.

* The proposed method achieves promising results for these functional tasks through an end-to-end modeling approach, but additional analysis interpreting what the model has learned about sequence-structure-function relationships would strengthen scientific validity. Has the model captured complex, nuanced relationships, or is performance driven more by statistical associations in the training data? Discussion of these relationships and how the model may be representing them would address concerns about the depth of knowledge actually obtained.

* For full reproducibility and scientific validity, additional details on the experimental setup, hyperparameter selections, and sensitivity analyses are needed. e.g. The variance or confidence intervals of reported results should be provided to determine their reliability and sensitivity to stochastic effects. Point estimates alone do not indicate the variability across trials or uncertainty in conclusions.The effects of different random seeds on performance should be analyzed to confirm results do not depend highly on a single seed selection. Sensitivity to initialization is an important consideration, especially for complex neural networks. Hyperparameter choices require further explanation and analysis of the effects of varying key values such as layer sizes, attention heads, learning rates, loss trade-offs, etc. The initial values selected may bias conclusions if performance is highly sensitive to these hyperparameters. Exploring this sensitivity would reinforce the results do not depend entirely on the specific choices made.

Limitations:
N/A

Rating:
6

Confidence:
3

";0
h3lTrt4Ftb;"REVIEW 
Summary:
In transformers architecture a word sequence is first represented by a sequence of word embeddings. This sequence of vector is then iteratively transformed by the successive transformer layers of the model. This paper proposes to characterize the trajectories taken by word embedding sequences. Features are introduced to better understand what happens during these transformations: the curvature along one dimension is estimated by looking the angle of the ""curve"" (the arcos) and trying to make correlation with ""surprisal"".


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It is important to understand what is going on inside transformers and what kind of transformation are learnt by the model. Looking at the trajectories is clearly a good idea. This notion of curvature tries to characterize the surprisal observed in the data based on a intuitive assumption. Some experimental results show that the curvature can indeed capture something of the iterative transformations.



Weaknesses:
While the starting idea is nice, the submission needs to be improved. Many important points remain unclear. Here is a list in reading order (more or less).

- Concerning the dataset: UD is a multilingual dataset, what is the language ? Why selecting only very short sentences ?
- For the models used, it is really messy. Are the models retrained from scratch ? finetuned ? All along the paper, we never know.
- On the same topic, what does ""untrained model""  mean ? Is it randomly initialized ? Only pre-trained ? Why it is a good basis for comparison ? 
- This a bit similar for evaluation data: you could define the different datasets once and make clear reference afterwards. 
- The decoding strategy maybe importance for some measurement. Maybe the greedy choice is not the best.
- The definition of surprisal could be given and related to the perplexity/NLL which can be the optimization criterion.
- At the end the statistical observation are not so impressive, or I did not clearly understand. The claim are not so clear at the end. It is difficult to really conclude that the prediction aims at  linearizing the input through a series of transformations. 

- You could use latex reference with section numbers (see eg line 130).
- The colors used for some figures (3 and 5) are really difficult to distinguish even on a color printed version, or with the pdf.
- The figure captions are very long ! 


As a conclusion, I really liked the starting idea of the paper and I think that it deserves further improvement before submission.


Limitations:
No

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper hypothesizes that the deep, casually-masked transformer models learn to predict by linearizing representational trajectories. This hypothesis is rooted in observations from the neuroscience literature. The hypothesis is tested through experiments that probe:

   (1) the degree to which representation curvature decreases with network depth,
   (2) the relationship between curvature and model performance,
   (3) the curvature of representations of model-generated text,
   (4) the relationship between text surprisal (entropy?) and curvature.

Curvature is defined in the sense of pairwise cosine-similarity between adjacent representations, averaged across sequences of text.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The straightening hypothesis is interesting, and the experiments convince me that transformers do exhibit straightening behavior. The experiments appear to be generally well executed (but see my questions below). Experiments #1 and #2 in particular clearly establish the pattern of increased straightening as a function of model depth, model size, and optimization steps.

It seems possible that this observation of straightening could be important and exciting to the neuroscience community, but I do not have the right background to make that judgement.

Weaknesses:
It is not clear to me why the straightening hypothesis is important. Accepting that LLM's do indeed straighten trajectories, what should I do with this knowledge? The conclusion gestures at the possibilities with respect to interpretability of models and revealing ""when and how they could fail and suggest ways to make models more efficient and robust."" But the connection between straightening and these broader goals (which are undoubtedly of relevance to a broader NeurIPS community) are are not clear to me.

I am very open to an argument that more clearly makes the case straightening is important: either from the perspective of its importance to the neuroscience community, or for its potential significance to the broader machine learning community (as hinted at in the conclusion).

Limitations:
As the authors acknowledge, the stronger hypothesis--that straightening is a consequence of the predictive loss--is not supported by the experiments in this paper. This would require experiments involving, e.g., MLM or classification tasks to observe whether straightening also appears in these settings.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This work investigates whether large language models learn to ""straighten"" the word-by-word representation of sentence as it passes through the model layers. The word-by-word curvature of the sequence embeddings is defined as the angle between two consecutive word embeddings (i.e. arccos of the cosine similarity of consecutive word embeddings from a particular layer). The idea is that a ""straighter"" trajectory would enable generalization via extrapolation. This work tests a number of models from the GPT-2 family of various sizes and shows that the word-by-word sequence curvature decreases from the early to middle layers (relative to the first layer of the model), and then increases towards the later layers.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Well written, clear, and concise manuscript
- Investigates a topical question that will be of interest to many in the NeurIPS audience

Weaknesses:
W1. Several times throughout the manuscript (including in the abstract and title), it is claimed that the results that larger models that have better next-word-prediction also have less curved trajectories in the early-to-mid layers suggests that models learn straighter trajectories in order to predict better. There is no evidence in this manuscript to support this claim. There is only evidence of correlation between the two, and not of causation. These claims need to be dialed way down and qualified. There can be other causes that lead to both straighter trajectories and better prediction performance. For example, the finding in the later part of the paper that sentence surprisal is correlated with curvature can be exactly this cause: it is possible that the most likely next word is the one that leads to the most ""straight"" trajectory. Therefore, a model which learns to predict the most likely next word (i.e. a language model) and achieves a good performance, will also have a straight trajectory. The interesting question is why the most likely next word would lead to a straight trajectory, but that is not answered by the current work.

W2. The manuscript heavily leans on a hypothesis developed by previous work (Henaff et al. 2018/9) but it is not clear how the curvature measure defined in the current work is related to the one developed by previous work. This needs to be clarified.

W3. A few possible confounders for the results. See Questions below.

Limitations:
Please discuss the fact that there can be other causes for the results you observe (see Weakness 1)

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work examines the an hypothesis regarding neural trajectory straightening as a mechanism, by which neural language models achieve next word prediction. Specifically, this hypothesis connects between the objective of next word prediction and extrapolation to the embedding of the next word in neural representation space. They define layer curvature based on prior work and find that (1) autoregressive LMs consistently reduced their curvature from early to middle layers, and this effect was only observed for trained models; ,odel size and training dataset size affected the model’s ability to reduce curvature; model-generated sentences exhibited lower curvature compared to natural human-generated sentences; average curvature correlated with average sentence surprisal in the middle layers of the model.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This paper explores an interesting hypothesis regarding the connection between an internal geometric property of Transformer based LMs and their performance. If this was not examined in prior work, I find the questions posed in all 4 experiments novel and interesting, and the results non-trivial. In particular, I liked the thoroughness in testing both different model sizes and different training set sizes for the same model size, that did a good job in removing an important confounder in my opinion (though not enough discussion and experimentation regarding the point in 1B tokens training set that broke the trend)


Weaknesses:
I find that this paper can be strengthened from several different angles: 

- Discussion of prior work: I am not familiar with literature on geometric interpretability of language models. This paper is on this exact topic but does not convey sufficient background on related work.
- Question scope: the focus on one specific geometric measure limits this paper’s strength, to me it seems that several related measures can be examined. Alternatively, though it’s intuitive, I find the focus on this specific geometric measure as not sufficiently motivated. 
- Depth of investigation: For each experiment, only the basic setup was ran and often there was not sufficient discussion on the outcome or follow up experimentation (eg, what happens in the second half of the network? Why did the 1B token experiment not show the same trend as 1M, 10M, 100M?)
- Several experimental design choices were not sufficiently motivated (Why only one dataset of 8,408 sentences? Why constrain sentences to be between 6 and 19 words long, and to not contain abbreviations or uncommon words?)
- writing and presentation. There were several clumsy sentence phrasings (eg, first intro sentence) and some typos (eg, mid sentence capitalization line 121). More importantly, some core quantities were not adequately presented (eg, no formula given for the employed 3-gram surprisal metric), and the figures were generally pretty hard to decipher (eg, What does figure 4A mean? I didn’t understand what quantity is referred to by the title: “The predictions of the representation straightening hypothesis”. What do the axes of this plot correspond to?). As mentioned above, I found the results sections not written well enough, often reiterating the premise and the intuition and not conveying and discussing the actual experimental outcome clearly enough. 

Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper provides evidence that autoregressive language models - specifically the GPT family straighten the internal trajectory of word sequences, making them more linear, in order to better predict next words. They show that trained models decrease sequence curvature across layers, larger models straighten more, model-generated sentences are straighter, and curvature correlates with unpredictability.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper introduces computational evidence for the trajectory straightening hypothesis using a simple and intuitive curvature metric and backs it up with various experiments. It is also well-written and raises exciting questions about the working/interpretability of these models.

Weaknesses:
The paper did not perform any ablation studies to see what is causing the trajectory straightening. Removing different components of the transformer like the feed-forward layer, and seeing the effects on straightening may lead to some more insights. It is not clear if straightening depends on the transformer architecture specifically or also occurs in other model architectures like LSTMs or vanilla RNNs Do similar dynamics occur in MLPs?

Limitations:
The authors have a sufficient limitations section.

Rating:
6

Confidence:
3

";1
cm53OBkctM;"REVIEW 
Summary:
This paper focuses on an online learning setting for Markov Decision Processes (MDPs) with a countably infinite number of states. It adopts a Bayesian learning perspective, assuming that the parameters of the MDP follow a prior distribution over a known parameter space. The paper proposes a Thompson-sampling-like approach to solve the MDP in an online fashion. This approach assumes access to an optimal policy oracle, where the parameters of the MDP are provided as inputs, and it also relies on specific assumptions about the features of the parameter space.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The model investigated in this paper exhibits a high degree of generality, and the results presented contribute significantly to the field of theoretical reinforcement learning by offering near-optimal algorithms for MDPs without a bounded state space. The inherent complexity of the problem necessitates intricate proofs, and although I haven't examined the complete proof in detail, I have confidence in the correctness of the underlying intuitions. The proof combines Lyapunov analysis with the proof presented in [38] for Bayesian learning in an MDP with a bounded state space, thus offering a potentially valuable contribution for future research. Additionally, the simulations conducted in the paper, which demonstrate the scaling of the algorithm's regret, are appreciated for providing empirical evidence supporting the algorithm's performance.

Weaknesses:
The results of the paper rely on a set of assumptions that may be difficult to verify. Of particular concern is Assumption 3, which assumes stability of the optimal algorithm under one set of parameters for the MDP, even when considering another set of parameters. Establishing this property for more general systems can be challenging and requires careful calibration of the parameter space and policy space.

The algorithm presented in the paper is heavily dependent on access to an oracle capable of solving the optimal policy, which itself is a complex problem for general queueing systems. This reliance on an oracle can limit the practical applicability of the algorithm.

The algorithm necessitates returning to state 0 (line 14) at the end of each episode. This requirement could result in an exponential dependence on the maximum queue length, potentially rendering the algorithm less relevant for practical implementation. As a result, the claim of practicality made in line 345 may not be adequately supported.

In relation to the previous point, the paper obscures many constants within the theoretical results. These constants, associated with the system's dimension and ergodicity, could play a crucial role in determining practical performance and should be given more attention and consideration.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors study Bayesian learning of the problem of optimal control
of a family of discrete-time countable state-space MDPs governed by an unknown parameter $\theta$ from a general parameter space $\Theta$ with each MDP evolving on a common countably-infinite state space $X$ and finite action space $A$.
As the setting is Bayesian, they assume that the model is governed by an unknown
parameter $\theta_\star \in \Theta $ generated from a fixed and known prior distribution. 
The learning goal: Bayesian regret minimization where the value function is the infinite-horizon average cost, and the regret is measured with respect to best policy in $\Pi$.
They prove a $\sqrt{TA}$ regret bound, up to poly-logarithmic factors, but the dependency in the complexity of the function class is unclear to me. 

Disclaimer: I am not much familiar with this area of RL literature and hence might not understand the results correctly. Also, I could not fully verify the correctness of the presented results.
I gave that assessment  as the paper was very hard to follow for an unfamiliar reader.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:

1.	I think the results are a nice contribution to the Bayesian  RL community. 
2.	The two examples presented contribute to the richness of the paper. 
3.	The presented bounds seem reasonable.
4.	The adaption of Thompson sampling to countable state spaces might be useful in other RL settings.


Weaknesses:

1.	Abstract is quite long and parts of it seems like copy-paste of the introduction.
2.	Writing requires improvement: (1) lightening the contribution compared to existing literature, (2) notation is hard to follow and makes proof reading hard, (3) complexity of $\Pi$ or the dimension $d$ should appear in the presented bounds, even if the dependency on them is logarithmic, those parameters are not negligible. 
3.	Need to mention the dependency in complexity of the function class is in the regret bound. Is it logarithmic for finite $\Pi$?  Is the regret dependent on the covering number? I would be happy if the authors could clarify that point.


Limitations:
N/A.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper present an adaptation of TSDE to parametric MDPs with unbounded state space.

The regret is sqrt{T} which is good but that is  under strong ergodic assumptions and lower order terms can harm the behavior of the algorithm for small values of T.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is sound technically. (I quickly checked the proofs)

- The problem of learning unbounded MDP is a natural and important question to address.


Weaknesses:
1. The strong conditions (especially  Ass 4) are not  necessary and sufficient conditions for existence of an optimal policy, nor for the existence of a solution of the Bellman equation.

2. The queueing examples are not really convincing: The optimal service rates can be computed efficiently by just estimating the arrival rate and solving the optimal control problem for the expected sojourn time. Also, numerically,  the growing rate of the regret gets worse as the arrival rate increases. 



Limitations:
I cannot see any limitations

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors consider the average reward Markov decision process framework with countable state spaces. The considered objective is to perform closed-loop optimal control for a family of MDPs parameterized in a compact space; this is a particularly interesting setting as the cost function is not assumed to be bounded. The authors propose a Thompson Sampling based algorithm and analyze its regret under suitable assumptions. They are able to show a finite time $\sqrt{|\mathcal{A}| T}$ bayesian regret bound. The authors also show the practical significance of their algorithm by an empirical application to queuing models.  


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper is very relevant to the community as it opens the way towards studying RL in continuous spaces without the assumptions of bounded reward functions. Therefore tightening the connection of reinforcement learning theory and optimal control.

- Although certain assumptions are somehow stringent, it is nice that the paper is able to convert an infinite horizon setting to a somewhat episodic setting. The latter usually allow for simpler analyses.

- The application to a queuing model is also appreciated, a nice change from the standard RL benchmarks. 

- I would like to clarify that I did not go through the theoretical proofs and can therefore not comment on their correctness apart from the general intuition that such results should be possible given appropriate assumptions on stability and dynamics.


Weaknesses:
- The assumptions seem somewhat stringent. For example, the finite support of transitions can be challenged even in the specific example of queuing models when large amounts of arrivals are possible. More importantly, assumption 3 seems quite limiting, can the authors elaborate on what this implies for stability? I am not very knowledgeable in optimal control but it seems that you are assuming stability of all policies? 

- The paper is sometimes very dense and not straightforward to follow. For example, lines 173 to 192 requires prior knowledge of several papers and definitions, e.g. Poisson equation’s link to the problem at hand/ forcing function and other similar passages in the text. I would advise the authors to add the relevant definitions and Lemmas at least in the appendix to make the paper self contained.

- The paper fails to cite many RL works in the continuous spaces setting. Namely, the entire line of function approximation in RL, for example: 1) “Frequentist Regret Bounds for Randomized Least-Squares Value Iteration” by Zanette et al. which provides a algorithm based on Thompson Sampling as well, and many other works (see references therein) in the model-free paradigm 2) “Bilinear Exponential Family of MDPs: Frequentist Regret Bound with Tractable Exploration and Planning” which also provides similar algorithms for MDPs that seem to include the queuing models presented here, see references therein for model-based approaches to RL in continuous spaces.  


Limitations:
The proposed work and algorithms is theoretical and does direct societal impacts. The possible theoretical limitations are detailed to some extent in the paper and to be addressed in future work.

Rating:
7

Confidence:
3

";1
ftPoVcm821;"REVIEW 
Summary:
This work presents a recursive method to summarize demonstrations into programs through LLM.  The idea is interesting in that it uses spec as the bottleneck to connect complex demonstrations and complex robot task code, encoded and decoded through chain of thoughts. The method is evaluated on three different benchmarks involving table-top manipulation, novel kitchen text tasks, and EpicKitchen. The method outperforms naive language-to-code baselines and can generalize to longer-horizon tasks as well as learning user intents.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The problem is challenging in that demonstrations and codes are both complex: demonstrations have lots of details and multimodality, yet codes are abstract and need to follow strict requirements. 
2. The method is sound: it uses divide and conquer to tackle some limitations of the current LLM. 
3. The new benchmark can be interesting to researchers that want to attempt the high-level planning problem in kitchen tasks.
4. The figures and the pseudo codes are helpful
5. The method generalize to longer-horizon tasks as well as learning user intents.


Weaknesses:
1. Have several concerns on the evaluation metrics 
2. Need more details on discussing the tabletop benchmark, the EpicKitchen experiment and the new proposed benchmark. Looks like epic kitchen is closer to diverse raw data such as Youtube and yet the new kitchen simulator and the table-top tasks has more predicates as well as low-level relationships


Limitations:
See above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents demo2code, a framework that takes as input user's language instructions as well as demonstrations, and outputs synthesized code for completing the tasks. It first iteratively summarizes given demonstrations to a compact task specification, then reasons by incorporating user preferences etc, and lastly output expanded execution code. The method is evaluated on a range of tasks, including table top manipulation, a simple cooking simulator, and real-world epic kitchen dataset.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The framework is novel in that it proposes to summarize demonstrations and language instructions using LLM, which is then used for action generation via code synthesis.
- the recursive and hierachical way of summarizing demonstrations and generating code is reasonable
- the idea that using LLM to reason about user preferences makes sense

Weaknesses:
- Upon reading the introduction i was excited to see how the approach is able to handle both instruction and demonstration: the latter usually comes in a visual space, but then i realize the authors made a big assumption that they can query the simulator to get state-based demonstration. This assumption presents a few issues: 1) such privileged information hinders the application in realistic settings. In fact, the author had to manually densely label the epic kitchen dataset, which leads to the question of how this can be used for real-world settings 2) even in simulation, it's not straightforawrd to obtain these state information. for example, as opposed to `op-top`, relations such as `in(obj, microwave)` is hard to obtain easily. Also, binary spatial relations loses dense geometric informations. 3) if access to step-by-step low-level state is assumed, and the LLM can summarize and generate step-by-step specifications, why not directly use task and motion planning(TAMP) to solve the task?
- how does the oracle spec2code works? does it use TAMP? if yes, what's the advantage of demo2code over it?
- the experiment which uses epickitchen, but need additional manual annotation, loses the point of evaluating on real-world dataset

Limitations:
- assumption on access to privileged low-level state and relations
- additional annotation on real-world data
- other limitations are discussed in the last section

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Demo2Code, a new method for generating code given a natural language description and demonstrations of the task. Demo2Code recursively summarizes demonstrations using a language model (LM) to create a task specification. The task specification is concatenated to the description and then recursively synthesized into code using a LM. Demo2Code is shown to outperform previous SoTA on an object manipulation environment, an author-designed cooking simulator, and the EPIC Kitchens dataset. Finally, qualitative results are shown demonstrating OOD generalization, grounding, and understanding of user preferences. 



Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
*Originality:* a new long-range sequential decision-making benchmark, Robotouille, was developed that focuses more on high-level actions rather than manipulation and navigation. This may be useful for future work studying agents that learn from task specification.

*Quality:* the experiments are comprehensive and cover a range of tasks and domains, suggesting that the method is robust to distributions of tasks. Furthermore, the results show a marked improvement over the previous SoTA on all the tasks, which likely demonstrates the efficacy of leveraging demonstrations.

*Clarity:* the paper is well-written and the figures are self-explanatory.

*Significance:* generating code from demonstrations is an important step towards developing agents that can efficiently interact with humans. Moreover, this work demonstrates a working end-to-end setup from natural language demonstrations to code, likely encouraging future work in this area.

Weaknesses:
There are two implicit assumptions of the method that are not evaluated and may make the method difficult to use in practice. I believe that these assumptions may be hard to overcome, so I am leaning towards giving a 6.5 but am rounding up because of the high-quality execution.

1. *The method assumes that demonstrations are complete descriptions of each state in the trajectory.* I would imagine that in many real-world environments (which is the setting that such a method would likely be deployed in), obtaining a complete description of each state in the trajectory is noisy. Some actions may be occluded or unable to be clearly delineated. Moreover, it may not even be clear a priori how to canonically parse actions from video or a natural language description of the environment. Would it be possible to develop an ablation where some of the actions are noisily parsed or even entirely omitted from the demonstration? I suspect that GPT-3.5 may not be able to handle the perturbation, but GPT-4 may be able to.

2. *The method assumes that the task description is well-specified.* Humans can often provide demonstrations and descriptions of the task that are under-specified or misspecified. For example, when booking airline tickets, a human may forget to describe their preference for red-eye flights and there may not be enough demonstrations to determine their underlying preferences [1]. In its current form, the method appears unequipped to handle such cases and it is unclear what solution it would generate. As mentioned in the work, one possible remedy is to provide feedback to the LM, but it is unclear how successful such an approach would be.

[1]  Lin, J., Fried, D., Klein, D., & Dragan, A. (2022). Inferring Rewards from Language in Context. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 8546–8560). Association for Computational Linguistics.



Limitations:
In general the limitations are well-addressed. The following two limitations were also addressed as weaknesses above: 
* Demonstrations are complete descriptions of low-level actions. In practice it seems difficult to obtain complete descriptions of all actions and state of an environment, as actions are often noisy and difficult to clearly delineate. E.g., given some video of a human washing the dishes, it would be unclear how to appropriately parse the action space (in the paper it was done by hand). 
* The natural language description of the task may be misspecified by the human. This seems like a fundamental limitation of the current method, and might make the generated code incorrect or misspecified.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors propose an LLM-based completion framework to translate natural language instructions, in addition to transcribed state sequences of demonstrations (as PDDL (or other strips-like) predicates), into code for executing the task with a robot.

The method is based on recursively summarizing the demonstrations into a 'specification', and then recursively expanding the 'specification' into python(?) code. The recusrsion stops in the first phase when no further summarization is possible, and in the code expansion loop, when no function is undefined.

The authors demonstrate this is three different domains with upto 10 distinct high level actions occuring in each demonstration.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Multimodal learning from demonstrations plus language instructions is an important problem that will enable consuming a wider modality of data sources to generate robot/agent programs and behavior. Using the summarization/translation capabilities of LLMs will likely be a key component of such a system.

Weaknesses:
**Methodology**: The paper does not define what a task specification is concretely, nor does it define what it means for the demonstrations to be adequately summarized, in particular in Alg 1 the function is_summarized() is not defined, nor is its operationalization defined anywhere in the text of the paper. Specification traditionally refers to a formal statement whose semantics are well known and the satisfaction of the specification with respect to an output is computationally well-defined and consistent. By not having a concrete definition of specification, the evaluations set up in the latter part of the paper suffer from lack of diversity and a lack of quantification of task difficulty.

**Code as output language**: This work relies on the output code being interpretable, and this involves providing a set of primitive parametric functions to the LLM to use. The part of sensing the environment, and acting using perception feedback is something that is already programmed into the primitives. Coming up with an adequate and a competent set of parameterized primitives is challenging, and in this case is entirely the responsibility of the system designer.

**Claims of generalization** Tthe generalization shown is only generalizing towards named entity substitutions, not to complex control flows and temporal specifications. Specifically, all the tasks shown here are a sequence of subgoals where performing any subgoal out of order will not preclude the agent from completing the subsequent actions. Further, there is no reactivity in the task specifications, and there are no avoidance tasks. All of these task specifications notions are very common in robotics and planning problems that this system has not been evaluated on. Refer to [1] for a survey on robotics mission types (also relevant to symbolic planning). All the tasks here are limited to the visit or sequenced visit type. Further while the submission claims that demo2code can generalize for complex long-horizon tasks, the maximum task length is quite smaller than state of the art for symbolic planning. Automatically translating textual domain descriptions to a formal domain description followed by the use of automated planners has already shown more reliable performance on harder problems [5]. Further there is quite a bit of evidence that LLMs cannot plan beyond the simplest of domains [6], and this line of research is unacknowledged in the submission.

**Issues on evaluations of learning from demonstrations:** Generalization from demonstrations and language is a tricky subject. Usually demonstrations and language contain complementary sources of information. Therefore none of the system behaviors are incorrect in Figure 5, the core issue is that inductive learning is by definition an ill-posed problem, and many approaches to inductive learning have relied on Bayesian inference in the past. [2],[3]. Specifically, the issue of where to place the purple block (fig 5a) given the language description is underspecified, and the system had to forcibly ground the placement to any of the valid options to generate a trajectory. Committing to a valid assignment as was done by the lang2code model is one approach, and asking for resolution of referential ambiguity is another approach [4]. One might argue that learning to overconstrain the output based on a single demonstration is an example of overspecification.



[1] - Menghi, C., Tsigkanos, C., Pelliccione, P., Ghezzi, C. and Berger, T., 2019. Specification patterns for robotic missions. IEEE Transactions on Software Engineering, 47(10), pp.2208-2224.

[2] - Tenenbaum, J.B., 1999. A Bayesian framework for concept learning (Doctoral dissertation, Massachusetts Institute of Technology).

[3] - Shah, A., Kamath, P., Shah, J.A. and Li, S., 2018. Bayesian inference of temporal task specifications from demonstrations. Advances in Neural Information Processing Systems, 31.

[4] - Williams, Tom, Rafael C. Núñez, Gordon Briggs, Matthias Scheutz, Kamal Premaratne, and Manohar N. Murthi. ""A dempster-shafer theoretic approach to understanding indirect speech acts."" In Advances in Artificial Intelligence--IBERAMIA 2014: 14th Ibero-American Conference on AI, Santiago de Chile, Chile, November 24-27, 2014, Proceedings 14, pp. 141-153. Springer International Publishing, 2014.

[5] - Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J. and Stone, P., 2023. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.

[6] - Valmeekam, K., Olmo, A., Sreedharan, S. and Kambhampati, S., 2022. Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). arXiv preprint arXiv:2206.10498.

Limitations:
Please see above

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a method that can take both demo and language in and teach LLM to perform new tasks. The idea makes sense and the algorithm is easy to understand and works very well. Evaluation results and ablations show improvement over existing works.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and the idea is clear and easy to understand.

Weaknesses:
It's probably better to define what ""task"" is. For example, the authors claim ""Demo2Code can generalize across complex, long-horizon tasks."" However, for real world robotics tasks, I would imagine there will be a lot of corner cases and the code needs to handle it and therefore it won't generalize to those tasks.

Limitations:
As the author said, the ability of this framework is limited by the capability of LLM.

Rating:
7

Confidence:
3

";1
9BV9dMhRjt;"REVIEW 
Summary:
This paper considers the problem of estimating _persistence intensity (resp. density) functions_, which are topological summaries arising when considering multiples realizations $\mu_1,\dots,\mu_n$ of persistence diagrams---which are counting measures supported on an open-half-plane $\Omega$. Namely, (Chazal and Divol, 2019) proved that $\frac{1}{n} \sum_{i=1}^n \mu_i$ converges toward a limit object $E[\mu]$ called an _expected persistence diagram_ which, under mild assumptions, admits a density $p$ wrt the Lebesgue measure on $\Omega$, called a persistence _intensity_ function. Note that $p$ may not integrate to $1$ (the $\mu_i$s are *not* probability measures, only Radon measures). 

It is natural to wonder how fast one can estimate $E[\mu]$ given an i.i.d. sample $\mu_1,\dots,\mu_n$. Given that $E[\mu]$ has a smooth density $p$, while the $(\mu_i)$ are discrete, it is tempting to adopt a kernel based approach (i.e. considering the convolution of the $\mu_i$ by a kernel $K_h$, with $h>0$ being the bandwidth), yielding an estimator $\hat{p} = \hat{p}_{h,n}$ of $p$. This is the purpose of the current paper which shows that, in brief, assuming $p$ is $s$-Hölder,

$$\| E[\hat{p}] - p \|_\infty \leq O(h^s), \qquad \text{(bias)},$$

and (with high probability)

$$ \| \hat{p} - E[\hat{p}] \|_{\infty , h} \leq O(n^{-\frac{1}{2}}h^{-1}), \qquad \text{(variance)},$$

where $\| \cdot \|_{\infty,h}$ denote the sup-norm but only accounting for points in $\Omega$ that are at distance $> 2h$ from the diagonal $\partial \Omega = \{ (t,t), t \in \mathbb{R} \}$. 

As an alternative, they also study the persistence _density_ function, which is substantially similar to the above, but considering the quantity $\frac{1}{n} \sum_{i=1}^n \frac{\mu_i}{\mu_i(\Omega)}$ as a starting point, hence the limit object is a _probability_ distribution. This first normalisation step allows the authors to obtain similar results as those mentioned above, but stronger in the sense that they do not need to ignore points close to $\partial \Omega$. 

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
## Clarity

The paper is fairly well written, though it may be hard to understand for the reader that is not familiar with statistical topological data analysis problems. Theorems are precisely stated, and, with few exceptions, mathematical quantities are well-defined.  

## Originality and Significance

The authors provides quantitative results for the convergence of kernel-based density estimator for expected PD, which is new to the best of my knowledge. The introduction of the persistence _density_ may also be worth of interest, if further motivated (see below). 

## Quality

This is a competent paper in terms of the results provided (which seem correct as far as I can tell). However, the motivation behind the type of results themselves is arguably questionable. 

Weaknesses:
There are several points which fail to convince me and prevent me from supporting this paper yet. 

## 1. The use of the sup-norm. 

The paper provides results in the sup-norm on $\Omega$. It is important to stress that this norm does not account for the peculiar role played by the diagonal in the geometry of persistence diagrams, in contrast with the standard $\mathrm{OT}_p$ metric. The authors justify this by an inequality of the form $\mathrm{OT}_p^p(\cdot, \cdot) \leq C | \cdot - \cdot|$, 
meaning that being small in sup-norm is (strictly, as proved by the authors) more demanding that being close in $\mathrm{OT}_p$ metric. [Edit: fix the rendering by removing the infty symbol; the norm in the rhs is the sup-norm.]

Sure, it means that the rate obtained for the sup-norm implies the same rate for the $\mathrm{OT}_p$ metric (up to the role played by the exponent), but it also means that the task is _harder_, and that this norm does not induce the same topology as the natural metrics on persistence diagrams. Recall (as suggested in $\ell$167-171) that accounting for the diagonal is not simply a trick to compare measures with different total masses, but also has an algebraic meaning (from which we get the mentioned stability results). The sup-norm induces a topology that fails to capture the fact that one can compare diagrams ""downweighting points close to the diagonal"". 

To me, this is what prevents, for the persistence _intensity_ function, to obtained ""global"" sup bound, and get only bounds valid $2h$-away from the diagonal. The sup-norm cannot handle the noise properly. 

In addition, because performing estimation in sup-norm is _harder_ than estimation in $\mathrm{OT}_p$-metric, one only get a convergence rate of $\frac{s}{2(s+1)}$, which is natural for the sup-norm, but quite _slow_ for the $\mathrm{OT}_p$ metric. Indeed, (DIvol and Lacombe, 2021) prove that the empirical expected persistence diagram (i.e. without any convolution by a kernel involved) converges toward the persistence _intensity_ function at (the faster) parametric rate $O(n^{-\frac{1}{2}})$. Of course, the latter result considers the weaker (but more natural) metric $\mathrm{OT}_p$, but as long as there is no very strong motivation to compare persistence intensity functions using a sup-norm, it is reasonable to wonder why should we struggle to obtain slower convergence rates. 

## 2. Motivations behind the persistence _density_ function

As (interestingly) observed by the authors, statistical estimation improves when considering the normalized persistence _density_ function. However, here as well, I fail to be fully convinced by the proposed motivation, namely ""the normalized persistence measure may be desirable when the number of points (...) is not of direct interest but their spatial distribution is"" ($\ell$80-81). 

I do not agree with this claim because this normalization typically discard points away from the diagonal, asymptotically. For, consider a $N$-sample on a sphere + some tubular noise, and the Vietoris-Rips filtration on it. Then (with high probability), the corresponding (random) persistence diagram in $H_2$ (degree-$2$ homology) will have one point away from the diagonal, and a bunch of points close to the diagonal accounting for the noise---so does the corresponding persistence _intensity_ function. As $N$ increases, the points accounting for the noise get closer and closer to the diagonal, but also more abundant. As such, if one normalize the persistence measure by its mass/number of points, the bump/point accounting for the ""robust"" topological information will asymptotically be erased. In particular, this normalized persistence measure is not continuous (for, say, the vague topology) with respect to the Hausdorff distance, a central property satisfied by the Vietoris-Rips filtration. (Note : this is what we can observe in Figure 3 vs Figure 2). 

Therefore, (i) it is not surprising that one obtains stronger (this time) results with this weaker representation and (ii) it is not clear to me why would one actually consider this representation at is losses its topological interpretation, as far as I can tell. 

## 3. About the experiments

The numerical illustrations have all been deferred to the supplementary material. 

To me, the (main body of the) paper should be self-contained, in sense that one should not _have to_ look at the supplementary material to understand it at high level. Proofs, complementary results, _complementary_ experimental report can be deferred to the supplementary material, but having a **Numerical illustration** section without any numerical illustration, mostly saying ""look at the supplementary material"", is not serving the paper. Right now, the paper can be considered as experiment-less, and while numerical illustrations are not mandatory, this clearly does not support the paper. 

Note that I looked at the experiments nonetheless. While they are fairly interesting, they do not bring more motivation to support the paper (with, e.g., a ML experiment where using the persistence _density_ function is much better than using the more standard persistence intensity function).  

## 4. Complementary minor comments
- I think that there is a typo in the definition of $\Omega_\ell$, which is (I think) inconsistent with the description made below ($\ell$67) and its use in section 3. 
- More references could have been cited through the paper, e.g., when listing different linear representations ($\ell$129-131), it may be nice to credit their respective authors.  Similarly, a more precise comparison with related work (mostly Divol's line of work with Polonik, Chazal and then Lacombe), would be helpful to understand what is the paper contribution and how it differs from these works. 


Limitations:
I do not see any negative societal impact _specific_ to this work. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
The paper proves several theoretical inequalities involving the optimal transport distance, intensity, and density functions on a plane triangle with a non-standard boundary motivated by persistent homology.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper rigorously proves in appendix C six theorems and three corollaries from section 3. All definitions, statements, and proofs are written in great detail. Also, the paper is well-written overall.

Weaknesses:
Starting already from section 2 about the background, it seems that the term ""persistence"" is not really needed because there is no connection with real data. 

Borel sets, measures, densities, and other concepts of classical probability theory can be considered on a plane triangle without the ""persistent"" adjective. 

Hence it is strange to read lines 97-99 saying that ""measure and probability are not yet standard concepts in the practice and theory of TDA. As a result, they have not been thoroughly investigated"" because measure and probability are standard concepts in probability theory for nearly 100 years. 

The conclusions reveal the main theoretical weaknesses in lines 328 and 332: ""Our main focus is on the estimation of the persistence intensity function [CD19, CWRW15]."" More explicitly, line 106-109 accepts that ""[CD19] provided explicit expressions for p and p˜  ... We will refer to the functions p and p˜ as the persistence intensity and the persistence density functions, respectively. We remark that the notion of a persistence intensity function was originally put forward by [CWRW15]."" 

Limitations:
Though the paper doesn't include the required keyword ""limitation"", the limitations appear in Assumptions 3.2, 3.3, 3.4. For instance, Assumption 3.4 essentially requires that there is not too much ""little noise"". 

In a simple case of the sublevel persistence of a scalar function, this function can be perturbed only by introducing a ""bounded amount"" of pairs of adjacent local maxima and minima. More exactly, the persistence diagram allows only a bounded sum of ""noisy artefacts"" near the diagonal.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work develops a set of methods and theories for statistical inference for TDA based on samples of persistence diagrams:  
a. The work focuses on the estimation of the persistence intensity function. The work also proposes the novel persistence density function, which is the normalized version of the persistence intensity function.
b. The work present a class of kernel-based estimators based on an iid sample of persistence diagrams and derive estimation rates in the supremum norm, which is stronger than the optimal transport distance norm.
c. The work obtains uniform consistency rates of estimating linear representations of persistence diagrams, including Betti numbers, persistent surfaces, persistent silhouttes and weighted Gaussian kernels.
d. The work presents several theorems, theorem 3.1 compares the L^\infty norm and the optimal transport distance in terms of controlling the estimation error; theorem 3.5, 3.6 show the kernel estimation error bound for the persistent density function and the persistent intensity function; theorem 3.8, 3.9 show the estimation error bounds for the linear representations. 

These theoretic results are fundamental and important, they lead to novel direction for statistical inference for TDA based on random persistent diagrams.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work is very solid, and gives rigorous mathematical proofs. The formulations of key concepts, main theorems are clear and rigorous, the mathematical deductions for lemmas, theorems, corollaries are thorough and in detail. The theoretic results are convincing and impressive.

Weaknesses:
The work is highly theoretical, the heavy mathematical deductions are abstract. It will be more helpful for readers to digest if the authors further explain the motivations, the main proof approaches, the interpretations of the theorem, the potential direct applications of the results. More specifically,

1. It will be helpful for readers to better understand the article to give a table of symbols, list the major symbols and their meanings;
2. It will be helpful to give some figure to illustrate the concepts, such as persistent diagram;
3. Some math symbols and operators can be further explained, such as:
a. The two symbols in line 145 are hard to differentiate, especially on a laptop screen, maybe the authors can emphasize the shuttle differences, or use different symbols;
b. The formula in line 165, \|x-y\|_2^q needs more explanation
c. The formula in line 247 in the supplementary, the operator Proj_{\partial\Omega} needs more explanation






Limitations:
This work is theoretical, it mainly focuses on theoretical deductions. The limitations are not adequately addressed. It will be helpful if the limitations in practical applications are further discussed. 
1. In reality, how difficult to satisfy all the assumptions listed in the paper ?
2. If the point cloud include several homology generators with similar birth and death times, may the current approach mix them and cause confusion ?
3. In theorem 3.6, from samples close to the diagonal, can we get more precise estimation ?


Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper tackles the problem of estimating the persistence intensity function, describing the distribution of rando persistence diagrams, and proposes a variant called persistence probability function that integrates to one.  The paper starts with a theoretical analysis of the estimation error bound of the intensity function using the OT measure and the L-infinite norm, showing that the latter allows the definition of stricter bounds. The paper also proposes a method to estimate the persistence intensity function and the persistence probability function under the assumption of i.i.d. samples using a kernel density estimation approach. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Persistent diagrams are an important tool for characterizing topological structures (e.g. surfaces and graphs). Being able to estimate with high accuracy the distribution of such structures could indeed be beneficial to their analysis, with applications also to the learning domain.

The paper, at least from a not-so-expert reader like I am, seems very rigorous in the theoretical analysis and provides in the sup. mat. all the proofs of the introduced theorems.



Weaknesses:
The main weakness (if we want to call it so) of the paper is that it is not easy to read by nonexperts of the specific topic. It is quite dense and mostly mathematical and does not provide many intuitive explanations of why some properties could be important from a practical perspective.
In general, I would have appreciated a more gentle introduction to the problem and a wider introduction/literature review on the practical application/advantages of persistent intensity functions.


Limitations:
I don’t foresee any particular negative societal impact. A discussion about the practical applicability of the method would be interesting.

Rating:
5

Confidence:
2

";0
RJq9bVEf6N;"REVIEW 
Summary:
The paper addresses  activity reasoning, where the task is to detect human activities given visual inputs. 
To achiever compositional generalization for activity understanding, the paper propose a new symbolic system that is capable of relational reasoning.
In the experiments, the proposed system improved naive CLIP models showing its strong reasoning capability.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall, the paper addresses an important problem for integrating symbolic reasoning with neural perception models.  The paper demonstrates an instantiation for such an integration by providing formal definitions.  The paper conveys its idea with a good presentation using clear visualizations. The obtained results are interesting. 
These contributions are beneficial for the community to build agents that can perform complex reasoning and understand the world better beyond simple visual perception.

Weaknesses:
My major concerns regarding this paper are listed as follows.
- The notation and semantics of the proposed method are unclear.  The rule is denoted using the logical entailment $\models$. Is this different from implication $\rightarrow$, which is commonly used for logic?  In Tab. 2 in the appendix, $\rightarrow$ is used.  Since the definition of logical entailment is not provided in the paper, it is unclear how the rules can be understood as formulae. The implication and entailment articulate different concepts.
- Related to the first point, it is not clear how the proposed symbolic system is related to other symbolic reasoners, e.g. standard first-order logic? What are the pros and cons?
- The paper mainly addresses activity reasoning, but it is not well-described in the paper. Even though some visualizations are provided in the paper,  it is not easy to follow the arguments for readers who are not familiar with this topic. It would be beneficial to provide some simple examples throughout the paper.
- Related to the previous point, the empirical setting is not easy to follow. E.g. on line 232 , it says, ""We follow the setting of HAKE [20] to integrate the reasoning ..."", however, this explanation is not self-contained. The experimental settings should be articulated more clearer in the paper.

- Other minor issues:
	- Definition 5 is not clear. What are activity semantic and ongoing symbols? 

Limitations:
The authors listed some limitations in the paper.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This work is in the area of human visual activity reasoning and tries to improve System-2 (logic, deliberative reasoning) integrated with System-1 (that deals with intuitivity, associativity, and correlation) to improve explainability, generalization and efficiency which is demonstrated through empirical evaluation. This work attempts to improve over existing methods which use limited symbols, and are therefore is insufficient to cover complex patterns of activities for visual activity understanding. They do this by proposing a new system with broad-coverage symbols and rational rules.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
-This work attempts to solve the lack of compositional generalization (generalized understanding of visual concepts, their relationships, and novel combinations) by creating a system of symbols with broad semantic coverage and rules that are rational and unambiguous. E.g., an existing work contains the following rule:
Hand hold ^ Feet close with |= human ride a boat. 
This rule does not cover the complex patterns of human activities.

- This work improves the existing methods by using an inexpensive annotation method by utilizing LLMs (OpenAI GPT3.5 API). This method is able to generate diverse symbols and rules quickly as shown in their symbolic system experiment. For the semantic coverage of the huge image dataset (the target domain), they replace it (the target domain) with the semantic coverage of language models.

Weaknesses:
The core message of the paper appears to be that more symbols with greater coverage leads to improved performance.  Figure 4 provides a high-level view of how these symbols are generated from LLM output, and Figure 6 notionally shows how they integrate with visual processing, but technical details are missing or unclear. For instance, how are initial symbols ""extended into rules satisfying entailment""?  In what format are symbols and rules stored?  What is the method for integrating the symbolic reasoning system into the overall visual processing pipeline?

Their method uses symbol-rule loop to generate new rules whereby they use a known symbol to generate a new symbol and connect it with the rules. The author(s) do not present examples to test the robustness of this method.

Limitations:
One limitation regarding symbolic representations of continuous activities is mentioned.  The authors have not addressed negative societal biases, and I can think of cases where some societal biases might arise.  Activity-based reasoning is at its core a kind of ""stereotype""-based reasoning: ""Balls are round.  Balls roll.  Apples are usually round.  Apples probably roll."" In the domain of common objects, the risks of such stereotype-based reasoning are probably minimal, although if other objects with potentially harmful affordances (e.g., firearms or other weapons) are used without noting that those behaviors are harmful and should be avoided, this presents potential for abuse.  Can things like race, gender, or social class be represented symbolically? Should they?  Would symbolic representation of these bias system outputs in the presence of ""coded"" objects like firearms?  Beyond universal basic behaviors, who decides what the symbolic representations of them are? Without attending to these questions and more, there is a risk of attributing ""typical"" or ""stereotypical"" behaviors to certain individuals or groups of people, and risk generalizing them to other individuals, and hence reproducing patterns of bias and discrimination.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the use of a symbolic system for understanding human activity. The author argues that a good symbolic system for human activity understanding should possess a diverse range of symbols that accurately identify the human activity, and the rules within the symbolic system should be logically correct. The paper proposes the utilization of LLMs to guarantee these two properties and suggests building a comprehensive pipeline incorporating both LLMs and VLMs. Experiments demonstrate that the proposed system outperforms the baseline model, which solely employs VLMs, on multiple datasets.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and easy to follow.
- The paper identifies two ideal properties of a good symbolic system: semantic coverage and logical entailment, which is very reasonable.
- Utilizing the commonsense knowledge of LLMs to achieve these two properties is also promising.
The experiments have demonstrated the effectiveness of the model on multiple datasets.

Weaknesses:
- Some crucial details of the method are still missing:
    - Regarding symbol initialization, it is not explicitly mentioned how the prompt in Appendix 3.3 (Line 79) is generated. It would be helpful to clarify whether it is manually written or generated using a template, and if so, what is the template.
    - There are two versions of prompts used for rule extension in the main paper (Line 139) and the appendix (Line 84). In the main paper, a single known symbol is used, while in the appendix, all known symbols are used in the prompt. Additionally, in the appendix, the first sentence contains specific information related to the airplane case: ""In a picture, **there is an airplane**"". How this particular prompt is generated?
    - According to Algorithm 1, symbols that contribute to a rule passing the entailment check should be removed from the candidate symbol list (Line 13). However, in Figure 4, after the 1→6→7→8 rule is generated, there are still rules such as 2→1 and 3→1→9 being generated. It would be useful to provide an explanation for this discrepancy.
- According to Table 1, some generated symbols are more complex than the target activity, such as ""wave goodbye to loved ones,"" and some do not follow the format of ""<The person's hands/arms/hip/legs/feet> <verb> <object>"", like ""airline staff checking the boarding pass"". Given the complexity of the generated symbols, VLMs might face challenges in recognizing them in the images. Although experiments have shown that the current pipeline performs better than directly using VLMs on multiple datasets, there is no guarantee that VLMs can easily recognize the generated symbols.
- While the author proposes two ideal properties that should be satisfied, the experiments only focus on the final accuracy of the models on human activity understanding tasks. The accuracy heavily relies on the performance of the VLMs employed. There is no evaluation of how well LLMs ensure these two properties. This makes it unclear what the contribution and bottleneck of the proposed system are. One potential experiment the authors could consider is to have a subset of the dataset and evaluate the pipeline with VLMs replaced by human judgment. But other experiments are also appreciated.

Limitations:
The authors have a paragraph discussing the limitations of the proposed method in the last section.

Rating:
5

Confidence:
2

REVIEW 
Summary:
In this paper, a symbolic-System-2 submodule is integrated with a System-1 module to perform human activity recognition. The work follows the intuitive idea, where a visual prediction module extracts visual symbols and a symbolic reasoning module performs logical reasoning to recognize activities. For the visual symbol extraction module, the work applies a general VLM, to predict the truth value of a predicate in a logic clause. The logical system then performs fuzzy logic to predict the final outcome of the rule, ie, conclusion. The logical system is constructed from LLM, without expensive human annotation in existing works. In experiments, it is shown that the model is better than existing baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ The work leverages and extends the intuitive idea of System-1 and System-2 combination. The integration is important compared to System-1 only pure predictive framework and it's really encouraging to see more and more works integrating these two systems in typical vision tasks.
+ Compared to existing works, the method leverages LLM for knowledge-base extraction. While the cost saving is from LLM, rather than some nice problem structure design, I do appreciate the efforts to construct formal logic base from natural language systems and make it work.


Weaknesses:
I'm not an expert in logical system design so I will refrain from commenting on the system design part on logics. However, I do see some general problems regarding this work.

For one, designing logical system specifically for a task risks overfitting. While the performance is improved on a specific task, I do note that the slight increase may not be worth the generality from models like CLIP. Besides, the logic system is pretty complicated. And I do not think the system, or the system construction method could be easily adapted for other tasks. On the other hand, such a system is also only demonstrated in a single task. I doubt if the method could be more general. 

I'm also interested in seeing more detailed ablation study. In particular, whether it is the System-1 part of the System-2 part that more adversely impacts performance. Would it be possible to assume perfect System-1 and test the System-2 performance or vise versa? Besides, while in general the idea should work. There are many subtleties in telling the truth value of statements, eg, the leg is ``close'' to something. How should such subtleties be addressed? 

Those being said, I admit the inherent conflict between logical system that are delicate, explainable, robust, and generalizable and deep learning system that are general but fragile. I do not see these points as weaknesses but rather interested in hearing how the authors see them.

For my final rating, I'd like to sync with fellow reviewers.

Limitations:
Yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of visual activity understanding and proposes a way to leverage large visual and language models to i) propose symbols from an image, ii) derive new symbols via logic entailment and perform classification. The results show that the augmented system with symbolic reasoning capabilities performs slightly better than the equivalent large models without the reasoning system. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper proposes an interesting way to leverage large visual and language models to derive a set of symbols relevant to a particular picture and the action being executed in it. Constructing an ontology based on the compiled knowledge represented in an LLM seems to be a powerful approach to enable task specific higher level reasoning.

Weaknesses:
One of the key improvements over other related work is claimed to be the broad semantic coverage of the derived symbols and the generated rules. However, given the statistical nature of the used large models it is not clear how accurate these symbols and rules actually are. A deeper analysis of the generated ontology is definitely needed in order to assess the applicability of the proposed method.

Limitations:
The core of the proposed work is based on utilising pre-trained large models which require vast amounts of data and computational resources and so the proposed method is as efficient as described only by assuming access to such large models.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes to increase coverage and generalization of logic-based methods to model 'system-2' or deliberative reasoning for the task of human activity prediction in images. Specifically, the authors incorporate an LLM (GPT3.5) to obtain a larger number of symbols and inference rules related to human activity prediction and further also utilize an LLM to check logical entailment of generated rules (based on which noisy rules are filtered). Their proposed symbolic system operates upon a VLM which is used for symbol extraction from the image, based on which rules are activated with a fuzzy-logic computation (to obtain a probability score for the inference chain). 

Authors show that adding their method on top of two VLMS (CLIP and BLIP2), enhances their performance on prominent activity recognition benchmarks and also show visualizations of activated rules and predictions besides ablation studies of their method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The usage of LLMs to broaden semantic coverage of symbolic/logic-based methods by discovering relevant premises and rules and also for checking/approximating logical entailment is relatively novel. 

2. The methodology and formalism of their approach is well described, with appropriate examples and figures/tables to help reader understand the approach.

3. Results on multiple prominent benchmarks (including HAKE, HICO and Stanford-40) for human activity prediction show that the method can benefit both zero-shot (for BLIP2 and CLIP) and finetuned (for CLIP) performances of prominent VLMs. Ablation results also highlight contributions of individual method components/variations.

Weaknesses:
1. The symbolic system's application draws on using a VLM multiple times to extract the probability of individual symbols. It thus seems that the computation will linearly scale with number of symbols, which can be a significant drawback due to the incurred computation cost.  (VLMs such as BLIP2 by themselves have very large computation cost, hence even increasing that 100x for 100 symbols is a very significant additional compute). Further, extending this to further domains beyond human-activity recognition would require many more symbols, and ultimately possibly not be scalable. 

2. Evaluation baselines only consider results of the backbone VLMs currently. There are multiple recent works in related direction of human-object interaction and activity prediction in zero- or few-shot scenarios which have not been mentioned or used as baselines (e.g. Weakly-Supervised HOI detection (ICLR 2023); RLIP (NeurIPS 2022)). Currently only RelVIT is mentioned in supplemental. Authors should either compare with such methods or mention why these recent methods are not used/suitable for comparison. 

3. Further, since LLMs are employed for much of the 'system-2' reasoning, authors should consider reporting results of alternative methods using VLMs + LLMs directly without any logical system (e.g. VLM generates image caption, based on which LLM predicts possible activity when prompted for each activity in the label space -- similar to PICa in  ""An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"" AAAI 2022). These results could better highlight the contribution of the proposed logical system and that performance increments are not due to just LLM-VLM combination.

Less important:
1. Failure cases could be illustrated for a small sample of the questions to better highlight which component is currently leading to errors (e.g. is it the VLM not producing accurate symbols or is it a noisy rule, etc)

Limitations:
Potential negative societal impact is mentioned to be minimal.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel method for visual activity understanding using an enriched symbolic system. The authors focus on enhancing the integration of intuitive (System-1) and logical (System-2) reasoning processes in AI systems. They argue that the current System-2 reasoning methods suffer from limitations such as a dearth of symbols and limited, ambiguous rules, inhibiting the system's ability to capture the complex patterns of activities and limiting its explainability, generalization, and data efficiency.

In response to these shortcomings, the authors introduce a new symbolic system enriched with a broader set of symbols and more rational rules. This system, underpinned by commonsense knowledge, aims to foster a more comprehensive understanding of human activities visually. To support their work, they leverage the advancements in LLMs and techniques like the symbol-rule loop and entailment check. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The concept of developing a new symbolic system that combines aspects of intuitive (System-1) and logical (System-2) reasoning is indeed a creative approach. However, the idea of incorporating commonsense knowledge into AI reasoning is not entirely novel, as there have been previous works exploring similar notions.

The authors put forward a detailed methodology and apply recent advancements such as Large Language Models (LLMs) in their approach. 

While the paper is mostly well-written, certain parts, especially the description of the symbolic system and its instantiation, could benefit from more elaboration or clearer explanations.

Although the paper targets a significant area in AI - the integration of System-2 reasoning into visual activity understanding - it's unclear how much practical impact this specific method would have on the broader field. The lack of comparison with other methods, especially in various real-world applications, makes it difficult to evaluate the actual significance of their work.


Weaknesses:
The paper's evaluation approach, while it encompasses multiple datasets, does fall short in providing a comprehensive set of comparisons with current state-of-the-art methods. The selected baselines do not reflect the full breadth of existing approaches, which undermines the claim of superiority of the proposed method. It would be valuable to include comparisons with more recent vision-language models, such as LLaVA, MiniGPT-4, and LLaMA Adapter, to provide a more meaningful assessment of the proposed system's performance.

The paper does spend a substantial portion of its content introducing the method background. Shifting sections 3.1-3.2 to the appendix could help maintain focus on the main contribution of the paper, while still providing the necessary context for interested readers.

The explanation of the core methodology is another area that requires improvement. It is not adequately detailed in the introduction, with key aspects of the proposed method encapsulated in a single sentence (lines 57-58). The paper would benefit from a more thorough and step-by-step explanation of the modeling method in the early part of the paper, such as the introduction, which would help readers better understand the unique features and operations of the proposed system.

Limitations:
The authors adequately addressed the limitations.

Rating:
4

Confidence:
4

";1
I9GNrInbdf;"REVIEW 
Summary:
The paper answers two important questions for the diffusion modelling community: 

(1) How to interpret the encoding given by the ODE transport map of a diffusion model - it is proven to be an optimal transport map.

(2) How can this ODE type integration be translated to discrete diffusion models - a low stochasticity sampler is derived by again drawing on optimal transport ideas.

The low stochasticity sampler is interpreted on 2d datasets.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
I like this paper as it very satisfyingly answers the above two important research questions in a very neat way using optimal transport.
Regarding the continuous state space results, the paper has made a significant contribution to the understanding of the 'latent space' induced by a diffusion model integrated using the probability flow ODE. This result has been conjectured before and this paper has now proven the optimal transport interpretation of the flow for a realistic set of assumptions.

Regarding the discrete state space results, the paper has made another significant contribution by formulating what an equivalent 'ODE' integration scheme could be for discrete state spaces, overcoming the challenge imposed by the discrete nature preventing a deterministic map from always being found. Formulating the problem as an optimal transport one seems a very natural way to overcome this. The intuition behind the derived optimal transport reverse rate is also very natural being a bias to only ever move to higher probability states and never flowing backward in such a way as to maintain the same marginals. This is really neat. I went through the proofs for propositions 5-7 and they seem correct to me.

The significance of this paper for the community will be large. For the discrete state spaces I can envision multiple works investigating this new reverse process formulation, for example, given that less transitions occur during sampling, it may be possible to achieve better sampling speed just as ODEs are easier to integrate in continuous state spaces.

Weaknesses:
The paper is weak on the experimental side but I believe the strength of the theoretical insights outweigh this.

The paper has not provided any experiments to demonstrate the results on continuous state spaces however I believe these would mostly be similar to Khrulkov et al. The strength of the theoretical results stands alone. Note that I have not the expertise in this area to check the proofs.

The discrete state space experiments are toy but do very clearly present the idea. It is a shame that the authors did not include a practically relevant data type e.g. text as it would have been very interesting to investigate 'discrete interpolations' using this reduced stochasticity sampler. The paper would greatly benefit from an experiment of this type.

The presentation of the paper is poor in places although the overall flow of the paper is good.

In eq(16) you should define what a neighborhood is properly.
On line 178 you should define what you mean by mutual flow.

Typos:

eq (2) dt is in the brackets as well for drift

line 72 x_{t+t} should be x_{t+1}

again line 74 k+t should be k+1

eq 17 extra bracket

eq (26) s=0

line 174 I assume you mean the sum is to K not S?

Edit after rebuttal: I have read the author response and the new experiments alleviated my main concern and I have raised my score to 7.

Limitations:
It is unclear how the method scales to high dimensional / large number of state size datasets.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Building upon the concept of probability flows, the paper establishes a clear link between diffusion processes and optimal transport. It first shows that the continuous probability flow corresponds to a Monge map for every finite-length time interval. It then states and proves analogous statements in the context of continuous-time discrete space (CTDS) models. These results follow the intuition that diffusion should be asymmetric to avoid _mutual flows_ between states. Finally, the authors design a practical sampling method that substantially reduces the variance of end positions of particles, given their initial status.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper presents a well-executed analysis of diffusion processes from the perspective of probability flows. It modifies a previous result by Khrulkov et al. [29] to make it applicable to the case of Brownian motion with initial distribution given by Eq. 17, i.e., that of a delta train induced by samples.
To analyze the discrete case, the authors revise the (reversed) heat diffusion process by preventing moves to states with lower probability. This procedure, reminiscent of well-known sampling techniques, leads to easy-to-analyze forward (Eq. 27) and backward transition rates. The authors show that the resulting process displaces mass optimally with respect to the L1 cost.


Weaknesses:
The notation used throughout the paper is at times hard to parse: Several similar symbols are used concurrently but there is no concise overview of their meaning (and differences). This makes the presentation less intuitive and requires more effort from the reader. In particular, the interpretation of “infinitesimal transport” would benefit from a revision aimed at better disambiguating between processes $\hat{Y}$ and $\tilde{Y}$, and between generators $A$ and $\hat{A}$. Also, the expanded state notation (i.e., $i_1, …, i_K$) is never needed in the main text and could be dropped altogether.
It would also be good to include additional derivations (in the appendix), such as the one of the reverse-time generator $R$ (eq. 29), which are not entirely trivial.

**Experiments**

The authors include several experiments on toy datasets that showcase the effectiveness of the proposed sampling strategy in reducing the variance. They nicely illustrate the theoretical statement which links the process generated by $Q$ to an optimal transport map. However, they do not describe any practical scenarios in which this feature could be necessary or desirable. In particular, reduced variance in sampling seems to imply that slight biases in the initial distribution of particles could lead to unfaithful reconstructions of the other marginal, i.e., by creating “holes” that the optimal transport map fails to fill.
I am also skeptical of the choice of experiments. Why is the current selection restricted to synthetic and low-dimensional settings? Did you run out of time or is the method's applicability limited?

**Miscellaneous**

To ease the description of sampling processes it would be helpful to use the differential (or discrete increment) of time flowing backward, e.g., in Eq. 2.

Further, some **typos**:

- In Table 1, given that the MMD value for the SDDM simulation on the checkerboard dataset has the wrong sign.
- In line 72: $(x_{t+t})$.
- In Eq. 19: missing a ½ factor?
- In the Discussion section.


Limitations:
The practical relevance of the proposed sampling framework is not discussed. It is unclear whether the reduced variance achieved by it could be of use in real-world scenarios and justify the (admittedly) inferior quality of marginal reconstruction quality of DPF.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors present a theoretical way to define the probability flow for continuous-time diffusion models with discrete state-space, a notion which is largely used for continuous state-space diffusion models, but does not generalize to the discrete setting. They bring several contributions:

- In the continuous setting, they show that the probability flow (under finite time horizon) of the Ornstein-Uhlenbeck process, starting from a finite collection of samples, is the optimal Monge map with respect to the quadratic cost, thus proving the conjecture of [1].

- Then, they extend the notion of probability flow in the discrete setting, when the noising process $P_D$ is chosen as the discrete analogue of the Brownian motion. To do so, they consider a modified version of the transition rate of $P_D$ (with its reverse generator still tractable) and show that it generates a Kantorovitch plan between $P_D(t)$ and $P_D(t+s)$ under a certain cost function, for $t>0$, $0<s<\delta_t$. Relying on the correspondence between optimal transport and probability flow that they highlight in the continuous case, they define the process hence generated as the discrete probability flow of the original process. 
 
- Relying on the score-based framework in continuous time developed by [2], they train two discrete diffusion models, one with the true noising process $P_D$, the other one with the modified noising process generated by $Q$, called Discrete Probability Flow (DPF). They show experimentally that DPF decreases the uncertainty occurring in the sampling phase, without hurting the quality of the generated samples, and give illustration of how DPF empirically satisfies optimal transport in this setting.

[1] Understanding ddpm latent codes through optimal transport, Khrulkov et al., 2022.

[2] Score-based Continuous-time Discrete Diffusion Models, Sun et al., 2022.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Strong theoretical result in the continuous setting. Really good contribution in the field of discrete state space diffusion models, to bridge the gap between diffusion model and optimal transport.

- Paper very well-written, easy to follow.

Weaknesses:
The scalability of the experiments.

Limitations:
The notion of probability flow is only defined for the noising process corresponding to the Brownian motion.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper presents a formulation of discrete probability flow using tools of Optimal Transport. First establishing conditions under which continuous probability flows are OT maps, the authors establish a version for discrete case. The authors then define a discrete probability flow using the connection to OT. Experiments compare the proposed method with SDDM showing that the proposed method yields comparable quality with reduced uncertainty. 


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper develops discrete probability flows using optimal transport with applications to diffusion modeling. Probability flows, OT, and diffusion models are all topics of great interest in the literature, which is a strength of the paper. I enjoyed the clear exposition of detailed math. The results were also clearly presented an accurately described. Overall, I found the paper to be a quite enjoyable and interesting read. 

Weaknesses:
I see no major weaknesses of the paper. Always, one can ask for more simulations and comparisons, and that would strengthen the paper. However, the work as presented is already quite strong and interesting, and I do not see further simulations or experiments to be necessary for the author's intended contribution. 

Detailed comments: 
- Line 102 ""This holds under the 2..."" It is a bit unclear what is meant. The equation 14 holds? The interpretation from 13->14? Also, under what assumptions? 
- Line 105: ""using a newly defined metric"" what is this metric? 
- Line 107, a bit more detail about the neighborhood and eq 16 seems helpful. 
- Line 121: ""This paper"" --> ""We"" would be clearer. (Also ""This paper"" in other places.)
- I don't believe SDDM is defined anywhere. It is also a bit surprising to have not discussed in detail SDDM since that is the main comparison. 

Limitations:
N/A

Rating:
8

Confidence:
4

";1
jfsjKBDB1z;"REVIEW 
Summary:
This paper proposes a training free framework for semi-supervised VOS. Here, the features of objects from a pretrained ViT are represented as vMF, where the objects across frames are associated to perform propagation from initial masks. The results are better than previous self-supervised methods that trained on unlabeled videos.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea of associating objects in vMF space for SVOS is interesting, which is a new insight different from the previous contrastive learning based self-supervised methods. The paper is well organized and sufficient comparison visual results are given. 

Weaknesses:
-The claimed limitations in SVOS is not true especially for the large memory footprint. Actually, some lightweight designs have been proven in VOS like mobilevos[1*] and AOT[2*]. Here, [1*] focuses a lightweight for real-time vos and [2*] tackles the memory storage issue in STM and the repeated inference for each object id at testing. It's true that the heuristic association method gets rid of the usage of complicated decoders where temporal correspondence and mask generation are performed. However, the large memory is not only from the network parameters but the computational costs. For now, the authors did't give convincing results to demonstrate this contribution like comparisons in terms of params, gflops or fps.
-For the temporal coherent, is it possible to extend to model both inter-object contrast and intra-object consistency, so the method can directly predict multiple instances at once.
--Although the method is training free, many hyperparameters need to be tuned during testing. Figure 5 also demonstrates that the change of hyperparameters would impact the final performance. 

[1*] MobileVOS: Real-Time Video Object Segmentation Contrastive Learning meets Knowledge Distillation, cvpr23.
[2*] Associating Objects with Transformers for Video Object Segmentation, neurips 21.

Limitations:
The authors discussed the limitations of their methods.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The method tries to combine classical technique for SVOS task namely vMF and CRFs with advance Deep Learning based ViT representations. By doing so, the proposed method eliminates the training requirements on video data. The authors perform comprehensive experimentation to evaluate their approach.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and easy to follow. Authors have done a thorough job in providing comprehensive experimental evaluation for their technique. The major strength of this approach is it does not require video training data.

Weaknesses:
- The performance of the model decreases when it encounters the unseen examples as can be observed from the table 2. If the proposed model does not require training then why would the performance on unseen examples is lower?
-  I'm not sure I understand what Fig. 3 represents? Is it a comparison of memory utilization by proposed method and baselines that require 1, 5, 21 reference frames? 
- The claim of low-memory footprint is unclear to me? is the claim made with respect to other baselines? If so, by how much delta is the proposed method better?

Limitations:
As the authors are using pre-trained VITs to extract the features. All the limitations of VITs carry forward to this approach.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel method for semi-supervised video object segmentation. The method combines pre-trained deep features from still images with streaming-data clustering techniques to model the object and the background as dynamic ensembles of von Mises-Fisher mixtures. The method does not require any additional training or fine-tuning on videos, and has a low memory footprint by storing only cluster-level information. The method also incorporates spatial coherence, outlier rejection, and convolutional conditional random fields to improve the segmentation quality. The paper demonstrates that the method achieves state-of-the-art results on two challenging benchmarks: DAVIS-2017 and YouTube-VOS 2018.




Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper shows that using pre-trained features from still images can eliminate the need for costly and supervised training on videos, while using streaming-data clustering can adapt to temporal changes and reduce memory consumption.
1. The idea of using vMF distribution to model the changes of features in stream data is interesting. 
2. The memory-conscious strategy opens possibilities for processing long videos, while the current solution is limited to videos with only about one hundred frames.

Weaknesses:
1. Almost all of the methods compared in this work target correspondence learning, which can not only handle VOS but also pose/object tracking. Thus it is reasonable that this work specifically tailored for VOS will achieve SOTA performance.
2. Though no training video is required, the backbone has to be pre-trained on million static images which is much larger than and has more diverse scenes than YouTube-VOS or Kinetics.
3. The improvement of performance on YouTube-VOS 2018 is not as considerable as that on DAVIS17 val. Moreover, it is even inferior to [1] on YouTube-VOS 2018 (i.e., 71.5 v.s. 72.4) with a stronger backbone (i.e., XCiT-small v.s. ResNet-50).
4. How about the inference speed of the proposed method compared to existing work? Will multiple vMF mixture models impose a tremendous burden on inference?
5. Please check the bib carefully, e.g. [7] and [8] are the same.

[1]. Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation. CVPR23

Limitations:
The limitation has been well discussed in Supplemental Material.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper tackles the semi-supervised video object segmentation problem. It presents a method that relies on clustering features from a pre-trained ViT model. The presented method has a low memory footprint and does not need any additional training. It shows SOTA results on DAVIS-2017 and YouTube-VOS 2018 validation datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The presented algorithm is the first to show a low memory footprint and indeed can scale easily. 

The paper presents a comprehensive study of the different components of the algorithm and an interesting ablation that shows the necessity of the different components. 

The results of the paper are both visually and numerically impressive. 

Weaknesses:
While the algorithm in the paper results in impressive results, it is hard to follow all the notations and the optimization objectives, mostly due to the nested indexing. While I was able to follow it eventually, I suggest rewriting it (one possibility is to start with a simple 2 classes foreground and background case -  and extend it later to N classes).

One element that is not ablated in this paper, is the quality of the pre-trained model and the features that are used. To show that this algorithm can improve and produce better results in the future, one should show how the performance of SVOS improves when ""better"" features are given to it. e.g. does a ViT model that was pre-trained on more data result in better downstream performance? Does a larger model improve the SVOS? Does the algorithm improve with higher resolution features? All of these questions are important for understanding if this algorithm will survive the ""test of time"". 

Limitations:
The limitations of the paper are discussed and strongly relate to the choice of the pre-trained model and its corresponding features, and might be addressed by using a model pre-trained on videos.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper essentially shows, that a combination of classic techniques such as stream-data clustering, using an EM-algorithm and dynamic updates, in combination with strong pre-trained features, allows to achieve state-of-the-art performance on two standard video segmentation datasets (Davis 2017 and YouTube-VOS-2018). 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Clearly achieving good performance with a combination of classic techniques, chosen well for the task at hand, is interesting to report and know about. Performance is strong on DAVIS (probably the dataset where most parameters are set) and somewhat less impressive on YouTube - but still sota

Weaknesses:
There is essentially no novelty in the paper - except to propose a well-chosen combination of classic techniques. 

Limitations:
ok for me

Rating:
4

Confidence:
4

";1
eGoE9CVRPc;"REVIEW 
Summary:
This paper presents a new model, the Regressor-Guided Multiple Instance Learning network (RGMIL), which addresses the challenge of providing discriminative instance-level representation in multi-classification MIL scenarios. It introduces an aggregator, the Regressor-Guided Pooling (RGP), that enhances the instance-level performance of Multiple Instance Learning (MIL) tasks. RGMIL outperforms existing methods in various datasets and demonstrates the potential for instance-level predictions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.  The paper presents a novel model, the Regressor-Guided Multiple Instance Learning network (RGMIL), which brings a fresh perspective to the field of multi-classification MIL scenarios. 
2. The paper is well-structured and the problem and proposed solution are explained clearly. 
3. The experiments are well-designed and the promising results effectively support the authors' claims.
4. The paper is inspiring for future research, including potential applications for multi-classification tasks such as the UCF-Crime dataset.

Weaknesses:
1. The paper lacks a clear definition and purpose for the metrics used in Table 4. Additionally, a thorough analysis of the results presented in the table is missing.

Limitations:
See the weaknesses.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents a new approach for Multiple Instance Learning (MIL) called Regressor-Guided MIL (RGMIL). The contribution of RGMIL is a new aggregation approach Regressor-Guided Pooling (RGP). In this approach, the aggregation part of the MIL model is split into N branches (for N + 1 classes, assuming class 0 is a negative class), where each branch is responsible for classifying a single class. The model is applied to a range of binary MIL problems (MUSK1, MUSK2, FOX, TIGER, ELEPHANT, Webpages, 20NewsGroups, Messidor, and UCSB Breast), and two max-based multi-class problems: MMNIST (proposed in this work) and UNBC shoulder pain estimation. The model mostly outperforms existing methods, and analysis is conducted as to why RGP is an effective pooling strategy. 

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
**Originality**
1. The RGB pooling is an original approach, particularly with the double pass through the regressor.
2. Section 4.3 is also a novel investigation into the bottlenecks of MIL model training and can help understand why certain models succeed/fail.

**Quality**
1. The evaluation covers a range of datasets and shows strong results, outperforming most other methods in most cases.
2. An ablation study is used to show how performance changes when training on different sizes bags but only classifying single instance bags.

**Significance**
1. Given the strong performance of RGMIL, it would appear useful in MIL settings that use max-based problems, i.e., traditional binary MIL or problems like pain estimation.

Weaknesses:
**Originality**
1. I believe some elements of the proposed method are already well known concepts, e.g., equation 4 appears to be one hot encoding, and equation 8 is a softmax. Simplifying these in the methodology and focusing on the novel parts of the method will help understanding. 

**Quality**
1. Some elements of the approach are lacking justification (see questions below).
2. The MMNIST dataset is proposed but then only evaluated in an Instance-level setting. A strong evaluation could involve looking at the performance when the training and test bag sizes are the same (i.e., 10/10, 16/16 etc.) as well as the instance-level setting.

**Clarity**
1. I found the paper very hard to follow and it took a considerable effort to unpick what the proposed method was actually doing.
2. It is not clear quite how the method works, particularly with regards to training. In Section 3.2, I am puzzled by the statement *""Parameters in both the RGP and regressor will NOT be involved in gradient descent""*. Please see questions below.
2. The work mentions how CNNs work and how humans would approach a MIL problem as justification for their approach. This makes the narrative harder to follow and detracts from the design of the novel pooling mechanism. I think this comparison would be best placed in the appendix, with more space allocated to discussing the model architecture and explaining how it works.
3. Figure and Table captions often contain little information, e.g., Table 2 does not state which dataset the model is trained on nor what the different columns means, Figure 2 should have y-axis labels, etc.
4. Some of the mathematical notation should be revisited to aid understanding. For example, is it necessary to have $M$ as the total number of instances and then $t$ as the number of instances per bag? Equations 5 through 9 jump through different notations, e.g., $fs$ for representations, then $H$, then $F_i$.
5. I would suggest another thorough proof-read to remove grammatical errors that make the work hard to follow.

**Significance**
1. My understanding is that this approach is only applicable to multi-class problems that have some max-based ordering of classes, e.g., in pain estimation, if branch 1 and 2 are both positive, then the prediction is class 2. I don't know how this would work for general multi-class problems (see questions below), which limits its significance.
2. I disagree with the final paragraph of Section 2.2 (comparing t < c and t > c). Instance-level performance is important for interpretability in both cases - just because there are a greater number of instances per bag does not mean instance-level performance becomes less important. 



Limitations:
1. The work does not address the limitations of the novel method or suggest any areas for future work.
2. The main limitation of the work is that it is very hard to follow and difficult to understand how the novel method works. This is frustrating as the results appear promising.
3. One potential limitation, discussed in the questions above, is how this could be applied to multi-class datasets that do not follow some form of max-based assumption.

Due to the difficult in understanding the approach and the lack of clarity in the method, I am leaning towards rejection as I don't think the method is reproducible from what is presented. However, there is certainly promise in the approach and the results appear strong.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper tries to overcome the drawback of low instance-level prediction performance  in  the existing Multiple Instance Learning (MIL) models. According to the authors, the low instance-level performance is because the existing techniques focus mostly on analyzing the relationship between instances and aggregating them while ignoring learning of effective instance-level representation. To this end, this paper proposes a novel aggregator function called Regressor-Guided Pooling (RGP) that directly learns discriminative instance-level representation by mimicking human inference process. An extensive evaluation is performed using the multiple MIL datasets along with the pain estimation datasets to validate the effectiveness of the proposed aggregator function. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper has made a novel contribution to MIL field by identifying and addressing  key limitations of the proposed MIL techniques. The significance of the novel contribution is demonstrated through non-trivial performance gain  over other competitive baselines.
2. The motivation for introducing the Regressor-Guided Multiple Instance Learning (RGMIL) framework is very novel, natural, and intuitive.
3. This paper deals with the multi-classification scenario under the MIL setting which is unique and different from most of the MIL works that focus mostly on the binary-classification scenario. This unique scenario also helped to enhance the novelty of this paper.
4. The extensive evaluation is conducted considering multiple MIL datasets. In addition to the quantitative result, the authors have done great job explaining why the proosed technique works and how it avoids the limitations of the existing MIL techniques. 

Weaknesses:
1. Most of the real-world video anomaly datasets are also used as a crucial testbed for the evaluation of MIL models [1, 2, 3].  Specifically, under video anomaly detection task, the MIL model is trained with video-level annotations (without having explicit access to the frame-level annotation). During the prediction process, the trained MIL model is used to perform frame-level prediction. The evaluation of the proposed MIL model on video-anomaly detection task is missing. I wonder how effective the proposed technique is in terms of making frame-level predictions. Also, in the related work section, the authors may need to explain how MIL techniques aimed to solve the video anomaly detection task are different from their work. 
2. Some of the figures deserve better treatment. For example, the caption in Figure 1 is not very descriptive. Further,  modules in the figures (with texts) are difficult to read and understand. 
3. The intuition behind Equation 6 is difficult to understand and is not very clear. 


References:
[1]. Sultani et al. “Real-world Anomaly Detection in Surveillance Videos”. CVPR2018.
[2]. Sapkota et al. “Distributionally Robust Optimization for Deep Kernel Multiple Instance Learning”. AISTATS2021.
[3]. Tian et al. “Weakly-Supervised Video Anomaly Detection With Robust Temporal Feature Magnitude Learning”. ICCV2021.


Limitations:
See weaknesses

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces _Regressor-guided MIL network (RGMIL)_ as a solution to address the challenges in Multiple instance learning, particularly in the multi-class classification scenario for pain-estimation. RGMIL introduces a novel aggregator, Regressor-Guided Pooling (RGP) in place of currently widely used attention-based aggregators. The design of RGP is based on simulating the correct inference process of humans when facing similar problems. The experiments show RGP out-performing evaluation benchmarks in MIL and works similar to supervised models for pain estimation.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The aggregator introduced in this work is grounded in prior knowledge of Multiple Instance Learning task construction. As a result, rather than opting for a complex black box approach, the design remains simplistic.This work aligns with recent research trends that emphasize leveraging simple yet valuable prior knowledge and inductive biases to design more effective solutions.
- Regressor-Guided Pooling (RGP), outperforms all MIL evaluation benchmarks including SOTA by a large margin (Table 1) and also works on-par with supervised models for pain estimation (Table 4). This compelling result clearly highlights the potential of the introduced method.
- The code available has a well-written README and looks easy to reproduce.

Weaknesses:
- **Missing dataset details**: The details of dataset(s) construction for MIL in pain-estimation setting is missing. One of the contributions mentioned in this work is RGMIL being the first weakly supervised deep model for pain estimation, so I feel it is important for this work to also focus on pain estimation datasets and benchmarks.
    - Firstly, it is not clearly mentioned whether the results presented in section 4.5 is for UNBC pain dataset. The way the dataset is constructed for the MIL setting is also not presented which makes it difficult to understand if the dataset is different or the same as supervised setting. 
    - Secondly, the result is presented on only a single dataset. Given that the contributions of the work also focuses on being the first weakly supervised model for pain estimation, it is important to benchmark on more datasets (for example BioVid dataset presented in [25]).


- **Issues in paper structure and writing:** The paper lacks a cohesive flow, making it difficult to follow. The writing style is inconsistent both within the sections and throughout the entire document. This paper needs major restructuring and rewriting to be easy to follow and understand.
    - The main paper contains many non-essential details and lacks conciseness. Few suggestions improve it: the assumptions section (3.1) should be condensed into smaller paragraphs, with more extensive details provided in the supplementary material. Similarly, sections 4.2, 4.3, and 4.4 are verbose and could benefit by retaining crucial details in the main paper and moving the remaining content to the supplementary.
    - In its current stage, some important contributions are left behind in supplementary (A1, A2) which supports the method’s effectiveness.
    - Figure and table captions (except Table 1) are not self-sufficient and needs referencing to the text for understanding. Authors should revise the captions for better stand-alone clarity.
    - Figure 1(b) is not clearly visible or understandable.


- **Missing important citations and explanations:** There are important statements without citations or explanations, based on which the paper makes assumptions for model design (lines 143-144) and presents results on a different dataset (lines 230-231).
    - line 143-144: 'Experience has shown that when we add the aggregator ⇢ to the backbone + regressor model for MIL problems, the instance-level performance often decreases significantly.' I would suggest the authors to cite works that show that adding aggregator decreases instance-level performance. If it is by observation across paper, the authors should present this in a table or a figure to give strong basis for assumptions.
    - line 230-231: 'Considering multiple factors, the UNBC pain dataset isn’t really a clear and flexible enough choice to present a demonstration.' What are the multiple factors here is not clear and I would suggest the authors to explain clearly.



- **Little to no focus on second contribution:** The paper's focus on the first contribution is appreciated, but it leaves little room to highlight the second contribution, leading to confusion in understanding the relevance of pain estimation in this work. It was referenced multiple times but only given a limited space (10 lines: 326-336) in the overall paper. To improve clarity, the authors can choose to implement either of these two suggestions:
    - Prioritize RGP as the main contribution and dedicate the paper to thoroughly highlighting its significance, while briefly mentioning pain estimation as a valuable benefit of the method only shortly. This also relieves the authors of having to benchmark any other datasets while fully highlighting the method and its effectiveness.
    - Condense some verbose sections, to be more concise, and move extra details to supplementary. This ensures that the authors have more space to present the benchmark results on pain estimation.

Limitations:
The authors have not mentioned any possible limitations or potential negative societal impact of this work. I would suggest the authors to include some details in settings where this method may not be useful. 

Rating:
5

Confidence:
3

";1
1osmdAfD4P;"REVIEW 
Summary:
The paper considers an online learning problem between a learner and adversary. The learner chooses action $x_t$ each round, and the state $h_t$ evolves according to the dynamics $h_t = Ah_{t-1} + Bx_t$. The oblivious adversary commits to a loss function $f_t$ each round. The learner suffers cumulative loss $\sum f_t(h_t)$. This paper uses FTRL to solve this problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper studies online learning with a linear control component. The framework captures a setting where past decisions affect future loss. The paper presented theoretical results with upper and lower bounds. 

The paper showed that the difficulty of this problem is captured by the so-called $p$-effective memory, which essentially quantifies the memory(less) property of the operator $A$. 

Weaknesses:
While the problem proposed in this paper is seemingly new, it does seem to shed much new algorithmic ideas and insights. 



Limitations:
The authors addressed limitations and directions for future work. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on online convex optimization with memory, a topic with increasing attention recently. Traditional framework assumes that the current environment is only affected by the decisions of a limited past, while this work considers that the current environment is affected by \emph{all} previous decisions. Specifically, the authors generalize the existing framework by studying the sequences of decisions in a typical sequence space. The authors then define the notion of memory capacity with the help of a bounded weighted norm in such sequence space and achieve new policy regret bounds. The authors verify its applicability by reducing several problems into the proposed framework, including variants of online linear control and performative prediction. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1)	The motivation is meaningful. It is definitely important in online learning literature to capture the historical impact in the sequence. 

(2)	The solution is simple, and the proof is clear and correct. 

Weaknesses:
(1)	My first concern comes from the novelty of the proposed framework. Although it enables an infinite memory length, the impact of history is modelled as typical linear operators, and positive results are obtained only when the operator behaves like a geometric combination of past decisions, which has been studied in some areas like linear control and reinforcement learning. Could the authors provide more special cases (in sec. 2.3), or elaborate more (in the paragraph just above def 2.3), to show that the proposed framework does give some new intuition to this problem? 

(2)	The technical contribution seems insufficient. The results mostly follows that of the traditional proofs in (Anava et al., 2015), with the operations on a finite sequence replaced by linear operators in functional space of sequences. Could the authors highlight the technical contributions in the proofs of their theory? For example, the explanation of (theorem 3.4) is quite clear and intuitive, is there something worth highlighted in the main results of the upper bounds?


Limitations:
Please see the comments above.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a problem called online convex optimization (OCO) with unbound memory, which generalizes an existing problem called OCO with memory. To address this problem, the authors propose an algorithm called FTRL and analyze its regret. Moreover, the authors demonstrate that this new problem and their algorithm have several applications.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1) The problem of online convex optimization (OCO) with unbound memory is more general than the existing problem called OCO with memory.
2) The proposed algorithm enjoys a regret bound and has some applications.
3) It seems that the results in this paper can simplify the regret anlysis for the problem of online linear control.

Weaknesses:
1) The extension from OCO with memory to OCO with unbound memory seems to be straightforward. Moreover, the algorithm for OCO with unbound memory and the corresponding analysis are very similar to existing algorithms for OCO with memory and their analysis. So, to some extent, the novelty of this paper is limited.
2) Although OCO with unbound memory seems to be more challenging, the authors only consider the history determined by fixed linear operators, i.e., $A$ and $B$ in the paper. Moreover, the authors do not explain why they only consider this case.
3) In the experiments, the authors simply set the step size of the proposed algorithm and the existing algorithms as $1/\\sqrt{T}$, instead of tuning it according to their theoretical guarantees. In this way, it is not clear whether the improvement in experiments is consistent with their theoretical guarantees.

Limitations:
There does not exist a potential negative societal impact.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper studies a generalization of online convex optimization (OCO) with memory. The setting allows the current stage cost to depend on all past decisions via a discrete-time linear dynamical system. The authors proposed a follow-the-regularized-leader algorithm that can achieve a sublinear static regret against any fixed action. They also showed a lower bound that matches the regret upper bound in the order of horizon $T$ and Lipschitz constants. The authors discussed two applications of their results to online control and online performative prediction.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Theory for online (convex) optimization with unbounded memory is important in the field of learning for control. While this problem is intractable in general, it is good to see results that formally define the “effective memory” and study the corresponding upper/lower bounds.

Weaknesses:
My major concern about this work is about the problem setting: I believe the setting proposed here is a special case of online control with adversarial disturbances (e.g., [Agarwal et al., 2019b]). Specifically, the history $h_t$ corresponds to the state and the decision $x_t$ corresponds to the control input. The only difference might be the benchmark: While online control compares against the best DAC policy, this work compares with a fixed action. But the DAC policy class can easily contain any fixed action if we add a dummy dimension with entry 1 in the (adversarial) disturbances. I hope the authors can correct me if my understanding is wrong, and a discussion in the revision may be helpful.

Since the proposed problem setting can be reduced to (or maybe equivalent to) online control with adversarial disturbances, one should evaluate the results in this work by comparing with not only [Agarwal et al., 2019b], but also more recent works on online control like [Minasyan et al., 2022], [Chen et al., 2022], and [Lin et al., 2022]. To the best of my knowledge, existing results can handle much more complicated settings that involves time-varying dynamics, unknown dynamics, or even nonlinear dynamics. They considered stronger benchmarks like adaptive regret or dynamic regret. I encourage the authors to do a more detailed literature review about online control and clarify the significance of the main results.

Besides, I also have a concern about the time complexity of the proposed algorithm. It seems that the time/memory complexity of constructing $\tilde{f}_t$ from past $f_1, \cdots, f_t$ grows linearly with respect to time $t$. However, existing online control algorithms like the one in [Agarwal et al., 2019b] only requires $log(T)$ time/memory for each decision. Thus, I am not sure how practical the proposed FTRL algorithm is.

[Minasyan et al., 2022]: https://arxiv.org/pdf/2202.07890.pdf
[Chen et al., 2022]: https://arxiv.org/pdf/2110.07807.pdf
[Lin et al., 2022]: https://arxiv.org/pdf/2210.12320v1.pdf


Limitations:
The authors discussed about some future directions in Section 5.

Rating:
6

Confidence:
3

";1
Pk9CdOZYRA;"REVIEW 
Summary:
The paper presents an “optimal” preconditioning for the Langevin diffusion by analytically maximizing the expected squared jumped distance. The authors have identified the optimal preconditioning as an inverse Fisher information covariance matrix. They apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive an efficient adaptive MCMC scheme that learns the preconditioning from the gradient history produced during the algorithm's execution. The authors show through experiments that their proposed algorithm can outperform other standard methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper introduces an innovative approach with the use of the inverse Fisher information covariance matrix as a preconditioner for MALA; to my knowledge this has not been previously proposed. The preconditioner is shown to maximize the expected squared jump distance, which could potentially improve the convergence of the algorithm.

The adaptive learning of the preconditioning from the history of gradients generated during the algorithm execution is a useful idea.

The experiments presented demonstrate the algorithm’s promise.


Weaknesses:
The paper lacks theoretical justification to support the empirical findings, which might raise questions about the universality and optimal applicability of the proposed method.

Limitations:
Yes.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors study a preconditioning matrix for the Langevin diffusion. It is given by maximizing the expected squared jumped distance. It turns out that the preconditioning is an inverse Fisher information covariance matrix for the target distribution. The authors apply the MALA scheme with this preconditioner to compute the MCMC algorithms. It is shown that in the Gaussian target distribution, this preconditioner performs as the Newton-type method. Numerical examples demonstrate the effectiveness of this method. The authors compared it with different methods, such as the position-dependent Riemannian manifold MALA sampler. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The authors present an important example of preconditioners, namely Fisher information matrix for Langevin dynamics. 
It is motivated by the Gaussian target distribution or the expected squared jumped distance. The method is similar to Newton's method in the Gaussian case.  

2. The numerical examples demonstrate the method's effectiveness for the Gaussian target distributions. 



Weaknesses:
1. The computation of the inverse of Fisher-information may not be simple. The authors propose some modification methods to approximate it. Some computational complexity analysis is needed. 

2. There is no theory supporting the convergence speed for non-Gaussian settings. Suppose the target distribution is a mixed Gaussian or a distribution that is non-log concave. Can the authors provide some numerical examples for these non-Gaussian target distributions to test the algorithm's performance?  It would be curious to know how Fisher information preconditioners behave. 

3.  There is some related literature in this direction. One picks different choices of preconditioners motivated by the Gaussian target distributions.  

Alfredo Garbuno-Inigo, et al. Interacting Langevin Diffusions: Gradient Structure And Ensemble Kalman Sampler, SIAM Journal on applied dynamical sysetm, 2019. 
Yifei Wang, et.al. Projected Wasserstein gradient descent for high-dimensional Bayesian inference, SIAM/ASA Journal on Uncertainty Quantification, 2022.
Yifei Wang, et.al. Accelerated Information Gradient Flow, Journal of Scientific computing, 2021


Limitations:
There are no limitations. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
Preconditioned Metropolis-adjusted Langevin Algorithm is considered. Optimality of the preconditioning matrix, with respect to expected squared jump distance, is found to be proportional to an object related to the inverse Fisher information. In contrast to the Fisher, the expectation is taken with respect to model parameters, giving an object that is not position dependent (in contrast to related Riemannian manifold MALA, and RMHMC). An online algorithm, based on classical tools from Kalman filtering, is given for efficiently computing the inverse of a Monte Carlo approximation to the Fisher Information, leading to a novel adaptive preconditioned MALA method. Debiasing in the transient phase is discussed, and a Rao-Blackwellization scheme is given to reduce variance of the estimator. Experiments are conducted on a range of standard testbed tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper provides a well principled approach to preconditioning MALA that shows improvements over both adaptive and Riemannian MCMC methods. In particular, it provides a quadratic approach to a Fisher-informed preconditioner that does not require cubic inversion/decomposition at every step, instead only needing quadratic updates. This approach outperforms the more expensive mMALA on nontrivial tasks.

The paper is communicated in an accessible way, and the experimental section covers a reasonable range of problems for samplers of this type.

Weaknesses:
The main body of the text claims that the Rao-Blackwellized version is significantly better, but this is not reflected in the numerics in Appendix E. This is not a weakness of the method itself, but appears misleading. When appendix E is mentioned, maybe ""detailed comparison between the proposed methods"" is clearer than ""detailed results"".

There are numerous grammatical errors and awkward phrasings, and the document would benefit from another read through with this in mind.

Limitations:
Limitations are addressed in weaknesses/questions. There is little view for negative societal impact in this type of work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This is a nice paper that develops a preconditioner for improved sampling of complex target distributions using gradient history.  The algorithm is well presented and is placed in context of other recent proposals from the literature.   There are several numerical experiments presented on small to medium sized examples that support the idea.  

A nice feature is that the paper is very readable which will enhance the uptake of the idea.

The numerical experimentation is good but not exhaustive, nonetheless convincing to this reader.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Reasonable overview of literature.   Careful presentation.  Readable style.  

Weaknesses:
Limited experimentation in high dimensional settings.     Limited ablation study.



Limitations:
The authors miss the chance to introduce the work by indicating the importance of sampling in large scale data science in providing improved robustness and the potential for uncertainty quantification.  I would suggest that the introduction be modified to develop this theme.   By its very nature, this work has potential to strengthen the foundations of AI and this should at least be mentioned.

Rating:
8

Confidence:
4

";1
G14N38AjpU;"REVIEW 
Summary:
This paper proposes to search for the optimal Transformer-based architecture for the knowledge tracing task. The authors have effectively constructed a search space that captures both global and local context modeling to accurately capture students' forgetting behavior. To further improve the efficiency of the model, a selective hierarchical input module has been designed to automate feature selection, eliminating the need for manual selection. By training the supernet and adopting the EA search, the authors have achieved superb results in two education datasets, EdNet and RAIEd2020.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper presents a clear motivation for the study, emphasizing the novel approach of utilizing NAS techniques to solve problems in KT, which is worth exploring and bridges the gap between KT and NAS
- The paper is well-structured and easy to follow, making it accessible to readers.
- The design of the search space for KT is reasonable and effective. And the experimental results seem convincing.

Weaknesses:
- The search cost should be reported, including the supernet training time and the evolutionary search time.
- The parameters and FLOPs of all architectures in Table 2 should be reported to provide a comprehensive understanding of the effectiveness.
- As an expert in NAS, I appreciate the authors' efforts in customizing an effective search space for KT and conducting the search for better architectures, which is a reasonable approach. However, from my perspective, it's difficult to judge the improvement shown in Table 2 without considering the change in parameters and FLOPs, which are necessary for a comprehensive evaluation.

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper modifies the Transformer architecture and employs Neural Architecture Search (NAS) to tackle knowledge tracing tasks

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Writing is good and the method looks sophisticated.

Weaknesses:
As I am not specialized in the field of knowledge tracing, I cannot accurately assess the novelty of this work or the adequacy of the experiments. However, as a general ML/NLP researcher, I can offer some intuition:

1. If the architecture in the Transformer is modified, then the pre-trained model is unavailable. Would it be necessary to pre-train the model from scratch.

2. NAS has been extensively researched. Although this paper appears to be the first to apply NAS specifically to the knowledge tracing problem, it is not clear why NAS is considered the most suitable technique for this task. For instance, have the authors considered alternative approaches such as mask-based methods [1]? Providing a comparison or justification would strengthen the argument for utilizing NAS in this context.

[1]: Continual Training of Language Models for Few-Shot Learning. EMNLP 2022


Limitations:
I don't see the discussion of limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents neural architecture search (NAS) for transformer in the context of educational application, specifically knowledge tracing. The authors employed two level NAS:
one at a local level to encorporate forgetting behavior and architecture serach at global level for exercise and response related embedding. The authors evaluated proposed approach
on two large datasets and compared with eight SOTA KT approaches.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. Local path for forgetting behavior encoding in KT
2. NAS for KT.

Weaknesses:
Comparison with other neural architecture search methods is not discussed. Please see Reference [1] for NAS experiment settings.
The technical contribution of the paper seems rather limited. 
How does the proposed NAS generalize to other transformer-based KTs, such as SAKT and AKT? 

References.
1. Ding M, Lian X, Yang L, Wang P, Jin X, Lu Z, Luo P. Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021 (pp. 2982-2992).

Limitations:
Yes, the trade-off between the time complexity of NAS is discussed in the Supplementary.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper introduces neural architecture search (NAS) for Transformer in the field of Knowledge Tracing (KT) for the first time. The authors propose a Transformer architecture that combines local and global paths, as well as a search space, to address the issue of students’ forgetting behavior in the knowledge tracing field. In this process, they propose a hierarchical fusion method that selects and fuses embeddings from various features. The proposed model demonstrates outstanding performance in experiments conducted on two large knowledge tracing datasets.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The attempt to apply Transformer NAS in the field of knowledge tracing is innovative. Moreover, the search space they introduced to apply Transformer NAS in the KT field is convincingly related to students' forgetting behavior.

- To reduce the high computational cost of evolutionary algorithms, the method used by the authors efficiently decreased the computational cost to a competitive level.


Weaknesses:
- Hierarchical fusion is excessively large architectures. The authors suffered from high computational costs and had to limit the number of feature candidates (Appendix 2, Limitation Discussion). While the proposed fusion method is intriguing, it would be beneficial to conduct more experiments on datasets with a large number of features, datasets with limited training data, or specifically focusing on the hierarchical fusion method.

Limitations:
- The authors well summarized the limitations of the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The architecture of a transformer model for a knowledge tracing task is searched for using evolutionary computation. The found solutions perform well.



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- ANN design is difficult 
- KT is important
- Simple NAS method

Weaknesses:
- Interpretability of the solution, i.e. ANN
- Limited discussion regarding computational cost. E.g. how much improvement for the additional search
- Readability of only acronyms introduced, e.g. LFA, PFA, KTM
- Figure 1 is non-intuitive
- Limited discussion regarding transfer of the solutions to other KT datasets

Limitations:
 none.

Rating:
6

Confidence:
3

";1
oSYjkJKHZx;"REVIEW 
Summary:
In this paper, the authors provide theoretical analysis on convergence and downstream performance of self supervised representation learning (SSL) approaches using tools from low-rank matrix completion. 

In particular, (i) they relate an eigenproblem objective to SSL methods, (ii) find that SSL methods perform a conjunction of laplacian embedding and low rank matrix completion, (iii) relate SSL augmentations to the number of observed entries required in matrix completion and (iv) provide some experimentation around incoherence as it relates to downstream performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Writing and theoretical exposition is clear
- Conceptually the trace maximization formulation is elegant. Specifically, the authors provide a broad framework through this line of reasoning to relate commonly used SSL objectives.
- The results on incoherence are well-aligned to empirical findings on projection head vs backbone representations, provides a potential explanation for this phenomena 

Weaknesses:
- Additional experimentation on performance across the trace maximization approach for small scale datasets would be helpful
- Further experimentation for the incoherence results in Figure 2 would be helpful, for instance how does incoherence compare across different SSL methods  (i.e. SimCLR, BarlowTwins, etc)

Limitations:
- It is unclear how the proposed trace maximization scales in runtime with $N$ and the number of view augmentations
- Some of the assumptions only hold for the self-supervised case and not the supervised contrastive learning case (i.e. CLIP) such as positive anchor backbone representations being close to one another. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
Self-supervised learning methods can effectively leverage limited signals to converge towards meaningful representations, but how is it made possible? This paper tries to give a response. This paper establishes a connection between SSL and a matrix completion problem by showing that these are Lagrangian dual of each other. This further implies that optimizing the SSL objsective simultaneously entails reconstructing the kernel matrix. This leads to some theoretical findings, including:

- The trace maximization formulation entails several popular SSL methods: SimCLR, BarlowTwins, VICReg.
- A less incoherent matrix is easier for matrix recovery, which explains why typical SSL methods rely on the representations (the incoherence is low) rather than the embeddings (the incoherence is high).

Based on the theoretical insights, this paper proposes a trace maximization objective for SSL (eq. 4). Following are some findings from experiments:

- The trace maximization objective is on par with existing SSL methods.
- There is a negative correlation trend between the incoherence and the number of layers in the projection head.
- The experiment findings support the hypothesis in proposition 4.3 — that incoherence indeed plays an important role in explaining the use of the backbone outputs.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- This paper proposes a novel, matrix completion formulation for SSL that can entail several popular SSL methods.
- The analysis of the matrix completion formulation provides insights about various parts of SSL.
- Empirically, the trace maximization objective leads to performances on par with other approaches.

Weaknesses:
I have some minor points about the usage of terminologies — please refer to the comment section below.

Limitations:
I do not see negative potential societal impacts of this work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper aims to provide a theoretical understanding of the recent successes of self-supervised learning methods by leveraging tools like Laplacian-based dimensionality reduction methods and low-rank matrix completion. The authors introduce an eigen-problem objective for spectral embeddings from graphs, which is used to interpret modern self-supervised learning methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Using an eigenproblem objective for spectral embeddings derived from graph augmentations, the authors explain the workings of contemporary self-supervised learning methods. This offers a fresh lens to understand self-supervised representation learning.
2. The paper further shows that self-supervised learning techniques can concurrently execute Laplacian-based nonlinear dimensionality reduction and low-rank matrix completion. This dual functionality further explains the success of self-supervised learning methods.


Weaknesses:
The approach presented in the paper is significantly dependent on the incoherence between the outputs of the backbone and the projection head.

Limitations:
The paper needs more numerical experiments. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors observe that self-supervised learning (SSL) attracts growing attention and that, by now, numerous corresponding loss functions have been proposed. They systemize these from the point of view of Laplace operators (on Riemannian manifolds) and low-rank matrix approximation. Indeed, for SimCLR, BarlowTwins, and VICReg they show that these learn eigenfunctions of a Laplacian. They also demonstrate that models trained w.r.t. related trace maximization objectives reach performances that are on par with those resulting from modern SSL techniques.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper is a blast from the past in the best possible sense. More rigorous theoretical underpinnings of recent self-supervised learning models are still very much lacking and this paper makes considerable progress in this regard and shows a connection to low-rank-matrix completion tasks. This contribution is theoretically rigorous and technically sound and solid. It also shows that “well known” trace maximization objectives lead to model which reach performances that are on par with those resulting from modern SSL techniques.

Weaknesses:
A minor point of criticism is that several interesting experimental findings are deferred to the supplementary material.

Limitations:
Given the scope of the paper (bridging a gap between modern learning techniches and classically ""well known"" models), I can't see any practical limitations. Neither are there any concerns regarding negative societal impact. 

Rating:
8

Confidence:
4

";1
8HzOyg1ngp;"REVIEW 
Summary:
The paper proposes a novel subgame resolving algorithm in Extensive-Form games called GS2, which will only sample a portion of subgames and dramatically reduce computation complexity.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The new subgame resolving algorithm GS2 has a theoretical guarantee, as all previous work did.
- The paper has sufficient experiments and also gets great performance in the Guandan game.

Weaknesses:
No code was released. Since subgame refinement is really an engineering topic, if possible, I think releasing the code will greatly benefit the whole community to do improvements in the future. Otherwise it will be really hard to reproduce the work since there are too many details in implementing algorithms.

Limitations:
Yes the authors have addressed the limitations adequately.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes GS2, a method to overcome the problem of large information states in subgame solving in imperfect-information games. Theoretical results and experimental evaluation on GuanDan are presented, with impressive results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main idea of the paper, to use a diversity function to filter the set of states, is nice. The practical results are particularly impressive: experiments on medium-sized games seem to clearly indicate that the diversity function has a positive impact, and the experiments on GuanDan seem to suggest that the bot is state-of-the-art.

Weaknesses:
I was a reviewer on an earlier version of this paper, during which time I had a robust discussion with the paper authors. The present submission is much improved and has addressed most of the concerns I had with the previous version. I have only a few lingering questions and minor concerns from that discussion, mostly concerning the experiments; see below.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The contributions of the paper are twofold:
- On the theoretical hand, the paper introduces a bound on the total increase in exploitability when refining the strategy in a subgame that consider only some of the possible infosets of the adversary in a stochastic way. 
	- This bound is based on considering that the exploitability of the refined strategy may increase (in the worst case) due to updating in the worst possible way the strategy for some of the opponent's infosets ($\delta$ term in the bound). This extra exploitability is then weighted for the probability of not sampling such an adversary infoset ($1-\omega$) term and for the probability of having reached that infoset in game ($\pi$ term)
- This theoretical bound is then used to design of the *Generative Subgame Solving (GS2)* algorithm. GS2 samples some of the nodes $h$ in the current infoset of the refining player. To sample the different $h$ that will belong to the subgame that will be refined, a *diversity-based generation function* is used. This function is an heuristics that selects the histories such that the distribution of counterfactual values at the infosets of the opponent at the root of the GS2 subgame are representative of the distribution that there would have been in the 1-KLSS. This allows to avoid to refine the strategy considering too pessimistic or too optimistic ""cuts"" of the original subgame, even if there is no formal proof that this reduces exploitability

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- Interesting and novel technical approach, which allows to interpolate between the unsafe solving from Ganzfried and Sandholm to the safe approach of 1-KLSS
- Formalism used is in line with previous works in the field
- Both medium and larger scale experiments. This allows to verify both the correctness and the scalability of the proposed approach
- clear structure of the paper and clear descriptions of the adopted solutions
- an example of how the technique would be applied on a game is presented, further clarifying the concepts

Weaknesses:
- Some of the claims presented by the paper are misleading or poorly justified:
	- The bound presented in Proposition 4.1 bounds the possible increase in exploitability from the blueprint to the refined strategy as the worst case scenario in which the refined strategy is maximally losing in case the opponent decides to switch to play only to that infoset.
	- Lines 235-240 claim that GS2 is more suitable for situations in which $\delta$ is already low, differently to traditional unsafe solving techniques. This means that the blueprint is of low quality, and therefore GS2 cannot make things too much worse in terms of exploitability wrt the original (already bad) blueprint. I don't get why such an argument would not apply as it to any other subgame solving technique
	- Line 290-291: ""[GS2] also refines the blueprint by selectively focusing on the most relevant portions of the game tree"". The introduced diversity generated function only considers the adversary's infosets as counterfactual values. There is no clear connection to how the sampled histories should be the **most relevant** from a strategy refinement perspective
- The large scale experimental setting presented feels disconnected from the techniques presented by the paper:
	- As indicated in Appendix C.2: *Although standard GS2 need to generate the full second-order knowledge infosets, we only consider the nodes in the first-order knowledge infoset.* I interpreted this as the fact that the set of histories $\{h \in S_{top}: \exists I_2: h \in I_2 \land  \exists h' \in I_1, I_2\}$ is not added to the constructed subgame. My opinion is that this approximation is really important to the point that the resulting algorithm should be clearly distinguished from GS2 as presented in the previous parts of the paper. This because while GS2 is a ""partial cut"" version of 1-KLSS, not adding all nodes in $I_2$ makes the technique possibly much more unsafe, and more similar to a ""partial cut"" version of the unsafe abstraction techniques from Ganzfried and Sandholm.
- GS2 is an unsafe solving technique: no cases in which such an unsafeness becomes evident are presented, leaving open the question of when such a technique may fail

Limitations:
N/A

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents a generative subgame solving framework that can scale to games with a large amount of hidden information. One of the key ideas behind the generative framework is to prioritize exploration based on diversity. The paper evaluates on small-sized tabular games and a large poker-like game called GuanDan.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
Overall, I have a positive opinion of the paper, with a few reservations (see below). In terms of strengths, I appreciated experiments on a real game. The structure and organization is also appropriate, with only minor clarity issues. While the method appears to be mostly a heuristic, it seems to be working pretty well in practice, and for that reason it seems worthy of discussion at the conference.

Weaknesses:
I find that the paper could be improved by expanding the discussion along the following directions:
- I think more should be done to ablate the choice of diversity-based generating function. The method proposed fundamentally boils down to sampling possible compatible histories in the infoset by means of a ""diversity-based generation function"". Many choices of prioritization could be imagined, and it would be important to have some form of data points about what tends to work and what doesn't. Also, data supporting the need of using prioritization as opposed to not prioritizing as all would be nice to have. 
- The discussion around KLSS as introduced by Zhang and Sandholm (L37) seems to suggest that it is a safe method, as it is said ""This approach enables the safe refinement of strategies"". However, KLSS is typically used in an unsafe way in practice.

I also think that the paper should discuss the following relevant related literature:
- Approaches used in the game of Hanabi. While common-interest in nature, the techniques that have been proposed in Hanabi have an overlap with the material of the paper. For example, the work on Learned Belief Search should be discussed.
- Approaches used in the game of Bridge, such as joint policy search, also seem relevant. 

Other comments:
- I feel like calling the games of section 5.1 ""medium"" is rather generous. The games only have a few hundreds sequences per player and can, for example, be solved exactly using the simplex algorithm via the sequence-form.
- L113, ""one cannot assume rationality of the chance player"". I found this obscure.

In conclusion, while I find the underlying idea rather straightforward, I think the strengths outweigh the weaknesses despite the limited evaluation. However, I think the paper would substantially benefit from adding more data regarding the choice of prioritization functions, and expanding the discussion regarding related literature.

Limitations:
The authors adequately addressed the limitations. 

Rating:
6

Confidence:
4

";1
R6KJN1AUAR;"REVIEW 
Summary:
This paper develops a theory to show how an additive decoder may be able to disentangle an image composed of several components. The proposed theory also shows how an additive decoder may produce novel images, possibly providing insights about the process performed by generative models.

Besides the theory, paper also performs some experiments showing that additive decoders may have certain advantages in performing those tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Paper is well written.

The topic and the experiments are interesting.

It has a broad and practical view. Literature review is relatively good.

The proposed theory might turn into a useful contribution for the research community.

Weaknesses:
Experiments are interesting but limited and disconnected from existing experiments in the literature, in my view.

-----------------

There are a few publications that are relevant but not cited:

–Li, N., Raza, M.A., Hu, W., Sun, Z. and Fisher, R, Object-centric representation learning with generative spatial-temporal factorization, NeurIPS 2021.

–Yoon, J., Wu, Y.F., Bae, H. and Ahn, S., An investigation into pre-training object-centric representations for reinforcement learning, ICML 2023.

Both of the above papers have experiments on images that may be decomposed in the authors’ additive scenario and possibly be used as a baseline for comparison.

-----------------

“Reasonableness”, mentioned under section 3.2, seems to be an unclear definition underlying a significant portion of the theory. It appears that authors attempt to define the reasonableness, yet, it is not clear what is the difference between $Z^{test}$ and $Z^{train}$. Given the bijective assumption, the difference between $Z^{test}$ and $Z^{train}$ should refer to a specific region in the domain and range of the function. Yet, it is not clear what that difference is.

I understand it is hard to define the limits of the underlying manifold of relevant images - that is exactly the heart of the difficulty in developing useful theory for deep learning. Could authors expand on their “reasonable” assumption? Perhaps identifying the boundaries of the “reasonable” manifold is hard, but it may be helpful to describe and contrast what is unreasonable. Perhaps providing a discussion and citing some previous studies on the underlying manifold of images would be helpful as well, e.g.:
 
Cohen, U., Chung, S., Lee, D.D. and Sompolinsky, H., 2020. Separability and geometry of object manifolds in deep neural networks. Nature Communications, 11(1), p.746.


-----------------

There is an extrapolation study and a dataset called VAEC from the paper below.

Webb, T., Dulberg, Z., Frankland, S., Petrov, A., O’Reilly, R. and Cohen, J., Learning representations that support extrapolation. ICML 2020.

The images in the VAEC dataset are designed as an extrapolation task. Do authors think this task can fit into their framework?


-----------------

It seems that the notation D (for the Jacobian) is only defined in the appendix under Table 2. Since the notation is used in the main body of the paper, it would be useful to define it there. If instead of D, $\nabla$ was used for the Jacobian, I would have inferred what authors mean by it. However, I was not sure about D until I found it in the appendix.

-----------------

For assumption 2, it may be better to use “linearly independent” instead of “independent”.

Moreover, it might be better to explain in words that: this assumption is requiring the … matrices, to have full column rank.

Overall, the notion formalized in assumption 2 seems strange to me. Are authors familiar with the notion of curvature for functions and manifolds?

Is there any precedence for the notion of nonlinearity defined under assumption 2 for any class of functions? I am not sure how authors’ notion of nonlinearity relates to known notions of nonlinearity/curvature, for example, the notions of curvature in differential geometry. Why should assumption 2 be satisfied over the entire manifold?

In its current form, assumption 2 stacks the first and second order derivatives together and then requires that the stacked matrix to have full column rank. It is not clear to me why stacking of these matrices is necessary. If the unrolled second derivatives have full column rank, would it be necessary for the first derivatives to have full column rank? If each of the first and second order derivatives, individually have full column rank, would that be sufficient?


-----------------

This is not a weakness of the paper, but perhaps worth mentioning. It is common in the literature to use x as the input to a function/model, and use y or z as the output of a function/model. However, this paper uses z to denote the inputs and x to denote the output. This may sometimes be a bit confusing. But that is the authors’ choice, and this is just feedback.



Limitations:
I did not see a discussion on limitations.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper analyses the statistical identifiability of latent variables in an autoencoder with a so-called additive decoder. It is shown that under this class of decoders, the blocks of latent dimensions associated with the additive decoder can be identified. This result is further related to the ability of a model to extrapolate.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(These may be subject to change depending on the answers to my questions below)

1. The paper is generally very well written in terms of giving intuitions behind the presented math (see the counter view in Weakness 1 below).
2. The paper touches on an important topic (identifiability), and the focus on additive decoders is both interesting, relevant, and novel.
3. The paper does a nice job of providing proof sketches, which is helpful since space constraints prevent the authors from including actual proofs in the main text.
4. The paper does a very nice job of connecting assumptions and results to existing work in various branches of the literature. This is very helpful.
5. Finally, I want to emphasize that the theoretical findings are both novel and interesting.


Weaknesses:
(These may be subject to change depending on the answers to my questions below)

1. The mathematics is often phrased sufficiently convoluted that the phrased intuitions are required (see Strength 1 above). E.g. I found definition 3 to be nearly unreadable, and I could not verify if the clearly phrased intuitions (lines 177-178) actually describe the math (I trust that it does, but it's a problem that it is so difficult to verify).
2. The 'additive decoder' construction seems quite similar to mixture models for which decades of work exist regarding identifiability. I was surprised to not see this link even briefly touched upon.
3. I found the extrapolation part of the paper to be less convincing than the identifiability part. Bluntly put, I got lost in the many assumptions made (Corollary 3 holds under the assumptions of Theorem 2, which hold under the assumptions of Theorem 1 which holds under Assumption 1) that I was unable to tell which were the important assumptions for the particular corollary. Thus, I lost my intuition, and the following discussion (Lines 381-304) seems rather speculative. Fere I struggle to determine what's what.

Limitations:
There is no need for a discussion regarding societal impact, etc., in a purely theoretical paper such as the present.

I wish the paper had had a greater discussion regarding the many assumptions made throughout the paper. Such a discussion would be akin to a ""limitations"" section often found in more empirical/methodological papers, and I don't see why a theoretical paper should not openly be discussing the limitations of the analysis (i.e. if the assumptions are appropriate).

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents the identification theory for the additive mixing function. Specifically, they transfer the existing nonlinear ICA conditions from the distribution (i.e., sufficient variability) to the nonlinear mixing function (i.e., sufficient nonlinearity). Under this model, they make the connection to extrapolation for generative models. Synthetic data experiments are designed to demonstrate their arguments.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper is well written — both the assumptions and the implications are adequately discussed and thus easy to understand.
2. The block-wise identification result is novel as a result of the sufficient nonlinearity condition inspired by the sufficient variability condition in prior work.
3. The connection to the extrapolation is interesting and yields a valuable understanding of current large models.

Weaknesses:
1. Some key assumptions, although discussed, are still evasive in their restrictiveness. The most notable is the sufficient nonlinearity assumption, which appears very restrictive. How to enforce this for the estimation model is challenging.
2. The primary assumption, namely additivity, can be very stringent. It is hard to believe this would hold for any realistic data-generating process. This also somehow trivializes the significance of the extrapolation part. I would also like to learn about the relation to recent work [1].
3. The experimental results lack detailed explanation. I struggle to make sense of the visualizations: what are the colored shades mean in Figure 4, and what does the color mean for the dots? Are the generating processes identical in the additive and the non-additive cases, i.e., does the only difference lie in the estimation model?

[1]. https://arxiv.org/abs/2305.14229

Limitations:
Please see the weakness section.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper extends the recently popular approach of constraining the nonlinear function to achieve identifiable disentanglement. Here the idea is that $\mathbf{f}$ is additive i.e. made of constituent functions that operate independently on non-overlapping partitions of the latent function. Identifiable disentanglement is achieved in this situation with very mild conditions on the latent distribution. These additive decoders can be seen as rudimentary version of the decoders used in object-centric representation learning and thus help explain their generalization performance. In particular, the paper shows that by exploring the full cartesian product of the latent symbols the model can generate images that are out of the training images' support (called cartesian-product extrapolation).

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- very well written paper with clear and intuitive examples despite the technical topic
- a novel way in which we can understand disentanglement by restricting the nonlinearity from the point of view of OCRL is a nice new angle to this increasingly popular field
- by making assumption about the structure of $f$ the authors are able make very mild assumptions about the distribution of the latent factors which is in contrast to the much more distribution
- additive decoders and the relevant results here provide a nice simple baseline model upon which future works can build more realistic ocrl models
- extrapolation guarantees is a nice addition, something previous works have been missing, and is something that hopefully will be adopted by the community (though it's unclear how that could be done; see below)
- assumption of sufficient nonlinearity is an insightful result and its connection to previous literature is nicely illustrated (albeit in the appendix)

Weaknesses:
Since the latent variables have only very mild restrictions, the price is paid by having fairly strong restriction on the 'mixing' function i.e. block-wise additivity (likely problematic in many realistic situations such as images with occlusion) + requirements on nonlinearity. This is likely useful for OCRL (e.g. scene mixtures) but in general may be very restrictive in the more general nonlinear ICA/disentanglement and also abstracts away from the desired goal of traditional nonlinear ICA where the aim is to separate out sources from 'heavily mixed' signals. 

Furthermore, the additivity also leads to a slightly problematic level identifiability results in the sense that each partition-block $z_b$ is identified only up to arbitrary invertible nonlinear transformation (equation 8. & 9. and $v_B$ specifically) -- this can be interpreted as there being a nonlinear ICA / mixing problem completely *unsolved* and thus unidentified for each block. So even though we are no longer left with the generic unidentifiability problem of $x = f(z)$, we are still left with complete unidentifiability in each block $B$, $ x_B = f_B (z_B)$. It feels like the term ""partition-respecting permutation"" and ""B-disentanglement"" are thus very specific to this approach and not really corresponding to the commonly accepted definitions of disentanglement in literature. Indeed the authors write ""Thus, B-disentanglement means that the blocks of latent dimensions zB are disentangled from one 177 another, but that variables within a given block might remain entangled.""

Related to this, for OCRL the model is quite simple, as admitted by the authors, and while they indeed may provide a good baseline (as mentioned above) it is hard to be sure whether the theorems fully explain their performance especially given these concerns about block-wise unidentifiability.
 
It would have been nice to see more extensive experiments and especially on more complex data.

Limitations:
There is some good discussion of limitations in the paper e.g. ""additive decoders make intuitive sense for OCRL, they are not expressive enough to represent the “masked decoders” typically used in"", ""Additionally, this  parameter sharing across f (B) enables modern methods to have a variable number of objects across  samples, an important practical point our theory does not cover."" Appendix also has nice examples of what happens when some assumptions are violated.



Rating:
7

Confidence:
3

REVIEW 
Summary:
Motivated by the problem of disentanglement in generative models, this paper proposes a novel decoder architecture, so-called additive decoders, based on the addition of block-wise latent variables, where blocks of latents correspond to semantic factors.

An extrapolation property of their decoder is demonstrated, which essentially consists of forming new products of latent blocks unseen at train time.

A theoretical analysis of their decoder is carried out. Several new definitions are made, as well as few necessary assumptions. Two theorems are presented on ""local disentanglement"" and ""global disentanglement"".

An empirical investigation based on synthetic image data, consisting of two balls randomly placed, is carried out, showing a case where their decoder improves upon a baseline case.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper addresses the problem of disentanglement in an interesting way: through blocks of latent variables which contribute additively to the overall decoding. This is a natural and reasonable idea.

The motivation for this paper is good: restrict the decoder to be additive to address the problem of latent variable identifiability.

The proof technique appears to build on the well-established results (Hyvärinen, AISTATS 2019).


Weaknesses:
The paper is quite technical. This makes it challenging to ensure all technical details are correct. I did not find any errors.

The authors write that this work may help explain the ""creativity"" of mainstream generative models like DALLE-2 and Stale Diffusion. It is not so clear to me that their analysis will be helpful.

Regarding Assumption 2, which the authors write is ""key"" for Theorem 2, I am not clear on how realistic this assumption is in practice. I understand that it is motivated by similar assumptions made in the ICA literature. However I come away with the feeling that it may not have much practical relevance. The authors give a toy numerical example in Example 3, which is helpful. But for instance, has this assumption been verified for the additive decoders used in the Experiments (Section 4)?

The proposed additive decoder cannot handle occlusions, as noted by the authors in Section 4, and discussed in the appendix. This is a limitation, since real images may have occlusions. 

Only small-scale synthetic data are included. It doesn't appear to me that training on larger scale data is feasible, but more discussion of this would be helpful. It's reasonable to develop theory on simple cases, perhaps even necessary in this cases, but we should be clear whether scaling the results can be expected.


Limitations:
Unfortunately I did not find any discussion of limitations in the main text. 

I don't think this paper requires a discussion on societal impact.


Rating:
7

Confidence:
3

";1
sQBGVw5qH9;"REVIEW 
Summary:
This paper proposes an approach to enhance the controllability of generative image models through the fusion of multiple modality signals.
The proposed model is a strict generalization of ControlNet which is able to incorporate geometric constraints into the generation, including pose, edges, and segmentation map.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper describes a very practical generalization of a SOTA approach to controllable generative models. It is well motivated, provides a clear description of the architecture, and both quantitative and qualitative results that validate the approach.

Weaknesses:
None. The paper is very clear on the benefits and limitations of the approach, and provides a practical improvement over a SOTA approach that is relevant to the research community and downstream users of the technology.

Limitations:
Limitations adequately described.
It is could be a good idea to include a sample of random results (not cherry picked) in the appendix in order to provide a sense of typical failure modes.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper studies text-conditional diffusion models and proposes a pipeline for image generation with multi-modal control signals. The pipeline contains three new ingredients. Firstly, the paper introduces gControlNet, a generalized version of ControlNet, that can take different conditions (such as sketch, segmentation and pose maps) simultaneously as input without increasing the ControlNet size linearly with respect to the number of conditions. Secondly, the paper proposes to inject features produced by gControlNet into the backbone model via normalization, instead of a simple sum as used in the original ControlNet. Thirdly, the paper introduces spatial sampling that aims to harmonize spatial arrangements from both text prompt and image controls. Experiments show that the proposed method achieves on-par quantitative performance compared with ControlNet and T2I-Adapter, and improved visual quality especially with multiple modalities.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. ControlNet proves to be a powerful approach for controllable text-to-image generation, while its original form requires fine-tuning a ControlNet network for each condition, which is resource consuming. This paper proposes a method to jointly train a shared network that can deal with different types of control input, which may extend the applicability of ControlNet in many cases.

2. The paper has done extensive quantitative evaluations to compare the proposed method with previous work. For generative models quantitative metrics may not be able to measure the actual quality of generated images, but it is important to study what are the good metrics and how to design better ones. This paper makes an effort towards the direction.

Weaknesses:
1. The gControlNet is a crucial component in the proposed pipeline, but the paper does not discuss details of the downsampling network M, either in main text or Appendix. On the other hand, it seems the controllable normalization and spatial guidance do not show significant improvement over their counterparts in previous work. In Figure 6 gControlNet without ControlNorm has much worse image quality, but is it due to insufficient training in this case (as the original ControlNet does not use ControlNorm and performs well)? As for spatial guidance sampling, the paper shows the effect of retaining background, but not placing objects to the correct location. The latter may not be an issue since sketch, segmentation and pose maps contain spatial information already. This seems to weaken the role of spatial sampling, which requires manual design that is not present in paper.

2. The paper uses Stable Diffusion 2.1 as backbone for training gControlNet, while ControlNet and T2I-Adapter used Stable Diffusion 1.5 as backbone. Therefore it may be unfair to directly compare gControlNet with ControlNet and T2I-Adapter. Also it seems that gControlNet does not show much qualitative improvement over ControlNet in single condition cases.

Limitations:
The paper discusses limitations of the proposed method.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This study introduces ChatIR, a chat-based image retrieval system that engages in a conversation with the user to clarify their search intent and retrieve the desired image from a large corpus. The system leverages Large Language Models to generate follow-up questions to an initial image description and achieves a success rate of over 78% after 5 dialogue rounds, compared to 75% when questions are asked by humans and 64% for a single shot text-to-image retrieval.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The strength of this submission lies in its clear and impactful contribution, which proposes a pipeline that combines multiple modalities such as edge, pose, segmentation mask, and more. The authors effectively communicate the significance of this research contribution, highlighting the potential of integrating diverse modalities to enhance the proposed method's performance. Additionally, the submission features a visually appealing and clear illustrative depiction of the method, which effectively aids in explaining the approach. The selection of an attractive and informative illustration further enhances the clarity and appeal of the paper.


Weaknesses:
One notable weakness of the submission is the lack of sufficient explanation regarding the experimental results. While the paper goes into detail in explaining the proposed method, it falls short in providing a thorough analysis and interpretation of the results. In Table 1, there are inconsistencies in the trends of metric scores between the proposed method and the two baselines, yet the authors do not adequately explain these discrepancies. It is crucial to understand why the proposed method may not perform as well in certain metrics, whether it is due to the limitations of the metrics themselves or issues with the method's performance. Merely providing cherry-picked demonstrations is insufficient and does not substitute for a comprehensive analysis of the results. It is essential to address these gaps in the paper by providing a detailed explanation and interpretation of the observed trends and discrepancies in the experimental results.


Limitations:
The authors have included discussions of the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents Cocktail, a novel pipeline for multi-modal and spatially-refined control in text-conditional diffusion models. The authors address the challenge of ambiguous descriptions in linguistic representations by incorporating additional control signals. They propose three main components: gControlNet, ControlNorm, and a spatial guidance sampling method.

gControlNet is a hyper-network designed to align and infuse control signals from different modalities into the pre-trained diffusion model. It can accommodate flexible modality signals and allows for the simultaneous reception of any combination of modalities or the fusion of multiple modalities. This capability eliminates the need for manual intervention and equilibrates the disparities between modalities, making the system more flexible and capable of seamlessly supporting multiple control inputs.

ControlNorm is a controllable normalization method that optimizes the utilization of information within branched networks. It decouples control signals, allowing for better representation of both semantic and spatial aspects. By preserving semantic information while conveying spatial information, ControlNorm overcomes the limitations of previous methods that either ignored semantic information or led to the loss of it through normalization. The proposed method effectively interprets conditional information and demonstrates its interpretative capability through generated images.

Additionally, the paper introduces a spatial guidance sampling method that ensures precise control over the generative process. By modifying the attention map, the method incorporates control signals into the backbone network, preventing the generation of undesired objects outside the specified regions. This approach enables the generation of high-quality images that closely align with the input conditions.

Overall, the proposed Cocktail pipeline addresses the challenges of ambiguous descriptions in text-conditional diffusion models and achieves multi-modal and spatially-refined control. It offers a novel approach to integrating control signals from various modalities, optimizing their utilization, and generating high-quality images with fidelity to external signals.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper introduces a novel pipeline called Cocktail that enables multi-modal and spatially-refined control in text-conditional diffusion models, addressing the challenge of ambiguous descriptions in linguistic representations. This is a significant contribution to the field as it tackles an important problem in generative models and expands the capabilities of text-guided image synthesis.

- The proposed pipeline consists of three well-defined components: gControlNet, ControlNorm, and a spatial guidance sampling method. Each component is carefully designed to address specific challenges and effectively integrate control signals from different modalities. The approach is comprehensive and demonstrates a systematic solution to achieve high-quality synthesis and fidelity to multiple external signals.

- The paper provides extensive experimental results and evaluation metrics, comparing the proposed Cocktail pipeline with state-of-the-art methods. The results consistently show that Cocktail outperforms existing approaches in text-guided image-to-image translation across multiple modalities. This empirical evidence demonstrates the effectiveness and superiority of the proposed method.

- The paper emphasizes the practicality and efficiency of the Cocktail pipeline by demonstrating its ability to accomplish multi-modal control within a single model. This not only simplifies the model architecture but also reduces the computational overhead associated with multiple branch networks. The approach is both technically sound and resource-efficient, making it highly applicable in real-world scenarios.

Weaknesses:
- The authors dont specify how they control for potential misuse of their model. A model like this if open sourced, can have wide ethical impacts with people using it for unethical means like creating deep fakes and modifying images which could potentially be unethical. This should be addressed by the authors.
- The paper does not provide a thorough discussion or analysis of the computational complexity or efficiency of the Cocktail pipeline. Considering the potential computational overhead of incorporating multiple modalities and the fusion process, it would be valuable to address the computational requirements and scalability of the proposed method
- The evaluation section could be expanded to include a more comprehensive analysis of the results. While the paper mentions various evaluation metrics, it would be valuable to discuss the limitations of these metrics and provide additional qualitative analysis or user studies to further validate the effectiveness of the proposed method.
- Although the paper compares the proposed Cocktail pipeline with state-of-the-art methods, it does not thoroughly discuss the limitations and failure cases of the proposed method. Understanding the shortcomings and potential failure modes is crucial for assessing the robustness and generalizability of the proposed approach.
- Implementation details seem sparse and might hinder reproducibility for the paper

Limitations:
- The authors dont specify how they control for potential misuse of their model. A model like this if open sourced, can have wide ethical impacts with people using it for unethical means like creating deep fakes and modifying images which could potentially be unethical. This should be addressed by the authors.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Cocktail, a pipeline to mix various modalities into one embedding. The model is based on a variant of ControlNet and infuses control signals from disparate modalities into the pre-trained diffusion model. It is also equipped with a sampling approach named spatial guidance sampling that constructs the cross-attention weights based on the spatial location. The model can perform text-to-image generation conditioned on multiple modalities and outperforms controlnet in some metrics.


Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
Both the proposed controllable normalization and spatial sampling method are novel and have not been previously explored in the context of controllable generation.


Weaknesses:
The model does not consistently outperform the two baselines: ControlNet and T2I-Adapter, in text-guided image-to-image translation.  The author claimed the model can benefit from mixed training of multiple modalities (line 213-215) but the proposed model shows a mixed result in various metrics across different tasks. Additionally, there is no quantitative comparison in multi-modality conditioning to existing baselines. In spatial guidance, no qualitative metrics showing its effect were given as well. Overall, it is unclear what are the advantages of the proposed method over simple baselines such as controlnet. 


Limitations:
Yes

Rating:
3

Confidence:
3

";1
wFuemocyHZ;"REVIEW 
Summary:
This paper proposes analyzes SDE and ODE-based samplers for diffusion models. Based on the analysis, this paper introduces a new solver, Restart, for sampling from diffusion models. The effectiveness of Restart is demonstrated on various unconditional and conditional generation tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written.
- The proposed method shows better performance than EDM at moderate NFE regions.

Weaknesses:
I am willing to raise the score by 1 or 2 points if the authors address my concerns satisfactorily.

**Weakness 1 : Ambiguity regarding Theorems 1 and 2.**
- For Theorem 1, if we set $[t_{\min},t_{\max}] = [0,T]$, the terms for contracted errors $TV(p_T^{ODE_\theta},p_T)$ and $TV(p_T^{SDE_\theta},p_T)$ vanish because $p_T^{ODE_\theta}$, $p_T^{SDE_\theta}$, and $p_T$ are all identically Gaussian distributions. Then, we end up with terms depending on $\delta$, $\epsilon_{approx}$, and $t_{\max} - t_{\min}$ only, so Theorem 1 does not provide any insight into how ODE and SDE have distinct ""winning regions"", as illustrated in Figure 1 (b). The proof and claim for Theorem 1 should be reformulated such that even with $[t_{\min},t_{\max}] = [0,T]$, Theorem 1 explains how ODE and SDE have winning regions.
- Likewise, I think Theorem 2 also should be proven for the entire interval $[0,T]$, so we can directly compare the errors for SDEs, ODEs, and Restart.

**Weakness 2 : (Possibly) weak performance on the small NFE regime.**
- How does Restart perform in the small NFE regime (NFE $\leq 30$)? In Figure 3, the figure cuts off just before Restart and ODE intersect in the small NFE regime. This seems to contradict the claim that Restart combines the best of both ODE and SDE. Moreover, given the large size of SOTA diffusion models, it is crucial that diffusion samplers work well in the small NFE regime as well.
-  How does Restart compare to recent fast samplers such as [1], [2], [3] in the small NFE regime?

[1] DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps, NeurIPS, 2022.

[2] Fast Sampling of Diffusion Models with Exponential Integrator, ICLR, 2023.

[3] Denoising MCMC for Accelerating Diffusion-Based Generative Models, ICML, 2023.

Limitations:
Discussed in Section 6.

Rating:
5

Confidence:
4

REVIEW 
Summary:
ODE-based samplers plateau in performance while SDE-based samplers deliver higher sample quality. The paper attributes this difference to discretization errors and accumulated errors. Based on these, the authors propose a sampling algorithm called Restart which alternates between the forward diffusion process and backward ODE.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The authors provide a theoretical explanation of the phenomenon that ODE samplers outperform SDE samplers in the small NFE regime but fall short in the large NFE regime.

2. The experimental results on image generation tasks validate the effectiveness of the method.

Weaknesses:
The proposed method relies on several hyperparameters (e.g. $S_{noise}, N_{restart}, i, K_i, t_{min, i}, t_{max, i}$), and the hyperparameters differ in different tasks. It would be hard to effectively tune these parameters in real application.


Limitations:
The author addressed the limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The papers propose a new sampling method for diffusion models, termed Restart Sampling. The authors first theoretically analyze the error propagation in diffusion models for stochastic and deterministic samplers under Wasserstein-1 distance and show that ODE samplers have a lower-discretization error but SDE samplers contract the initial distribution error as we run more steps. This agrees with the intuition and the experimental findings that support that ODE samplers are better for low NFEs but their performance flattens for more NFEs. Based on this analysis, the authors propose a method that tries to achieve the best of both worlds: it contracts the initial error with more steps and achieves the same discretization error as the ODE samplers. The implementation of the new method is very straightforward: one runs the ODE sampler and every K steps reverts back to some prior diffusion time using the forward model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The authors study a relevant problem in diffusion models. The proposed solution is simple, effective and novel. The theoretical results motivate the method and show clearly the differences between the SDE and the ODE samplers. The presentation of the paper is excellent. The authors show many experimental results, starting from toy models and going all the way to state-of-the-art text-to-image diffusion models. I think the paper and the method is of interest to the community and the audience of NeurIPS.

Weaknesses:
The error propagation of diffusion models has been studied before. The results that I am aware of are from the papers ""Sampling is as easy as learning the score"", ""Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers"" and ""The probability flow ODE is provably fast"". The first studies the error propagation of the SDE sampling method and the latter two the propagation of errors for deterministic samplers. It would be beneficial to compare with these works, highlight potential differences in the approach and the final results, etc.

Also, apart from the Stochastic and the ODE samplers, there is a whole family of samplers that satisfy the same Fokker-Planck equations and hence give the same marginals, e.g. see the work ""Fast Sampling of Diffusion Models with Exponential Integrator"" and also some of the samplers used in the ""Elucidating the Design Space of Diffusion-Based Generative Models"" (EDM) paper. It would be interesting to compare theoretically and experimentally to these samplers.

Another concern I have is that the evaluation is only as thorough as it should have been and it is only done for relative high NFEs. Since evaluating the performance of a trained model is relatively easy, I would expect a more thorough benchmarking. If the performance of Restart sampler breaks for low NFEs, it is useful to know it and acknowledge it in the paper.



Limitations:
The authors adequately addressed the limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
By analyzing of the trade-off between good sample quality and sampling time of both ODE and SDE-based generative models, a restart sampling strategy is proposed by this paper to combine the advantages of ODE and SDE sampling methods. The author proves two theorems that estimate the upper bound on the total error measured by the Wasserstein distance between generated and data distributions of ODE, SDE, and restart sampling methods respectively. It is illustrated that the total error can be decomposed into two parts: additional sampling error generated by discretization error and contracted error generated by the accumulated total error from previous sampling steps. Moreover, it is proved that ODE-based samplers have smaller additional sampling errors and SDE-based samplers have smaller contracted errors. So Comparing the three upper bounds on the total error, it can be proved theoretically that restart sampling yields a smaller total error because its additional sampling error and contracted error are both small. Finally, a range of experiments are done by authors which shows empirically that: 1. The total error of the restart sampler is indeed smaller than that of others. 2. Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. 3. Restart sampler better balances text-image alignment/visual quality versus diversity than previous samplers.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper is written with meaningful motivation and a clear structure. The restart sampling method proposed by this paper is innovative, simple, and effective.
2. The reasonableness and effectiveness of the resampling method are proved both theoretically and experimentally.
3. The upper bounds on the total error of the three sampling methods give us an intuitive understanding of the advantages and disadvantages of the three sampling methods.

Weaknesses:
1. The effectiveness of restart sampling method on high-resolution image synthesis is not confirmed. For example, a comparison of sampling speed and accuracy on ImageNet 128*128, 512*512 should be added.
2. The paper experiments on the sensitivity analysis of the number of restart iterations K, but there is no experiment on the sensitivity of another hyperparameter: the position and length of the restart interval.
3. As pointed out in the paper, the contracted error further diminishes exponentially with the number of repetitions K though the additional error increases linearly with K. Figure 4 illustrates this trade-off phenomenon, too. So it may be hard to find a suitable K to make the restart algorithm work for different datasets or tasks globally, making it difficult to apply. 

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposed a sampler that balances sampling speed and quality by adding noises and restarting the process. They provide theoretical analysis to show a better upper bound of this method compared to original ODE and SDE samplers. Experiments are done to verify their claims. 

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. Authors identify the main cause of different performances of SDE and ODE in different regimes. And by taking advantage of the contraction by adding noises, they balance the speed and quality of the sampler.

2. The theoretical analysis is clear and well-written. 

3. The experiments are thorough with a good explanation of the choices of hyperparameters. 

4. The experiments show good results for the proposed method.

Weaknesses:
See questions.

Limitations:
yes

Rating:
6

Confidence:
3

";1
AKAMNDe2Sw;"REVIEW 
Summary:
The authors empirically observed that the adversarial data lies on the flat local maxima yields enhanced attack transferability. Inspired by this observation, this paper proposes a regularization to help the gradient-based attacks to find the adversarial data at the flat local maxima. This regularization penalizes the gradient norm around the adversarial data and can be efficiently computed via the finite difference method. The empirical results validate the effectiveness of the proposed method in improving attack transferability.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The motivation of the proposed method is clear. The observation in Figure 1 is interesting and inspiring.

2. The authors utilized the finite difference method to make the computation more efficient.

3. It seems that the proposed method can significantly improve attack transferability. 



Weaknesses:
1. The proposed method is not theoretically motivated, which degrades its soundness. The authors claimed the optimization of perturbation equals the model training process using an analogy. However, it lacks supportive theoretical results to support this claim. Since the aforementioned claim is not solidly proven, the reason for the adversarial data at flat local maxima yielding better transferability seems unclear.

2. The paper does not provide the standard variance of the reported results to validate their significance. 

3. The paper lacks some empirical and theoretical analyses of the effectiveness of the finite difference method in speeding up optimization. I think the optimization of the gradient norm is very important for the proposed method. Therefore, the effect of the acceleration method is worth studying.


Limitations:
The motivation of the proposed method lacks theoretical support.



Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a method called Penalizing Gradient Norm (PGN) to improve the transferability of adversarial perturbations. The method is motivated by the observation that encouraging the flatness of the local landscape for adversarial examples can lead to better transferability, and thus PGN regulates the process of gradient-based adversarial attack algorithms by penalizing the magnitude of the loss gradient with respect to the input. Since such a regularization process requires the input Hessian which is a computationally expensive process, PGN utilizes the finite difference method to approximate the Hessian matrix. Experiments on the ImageNet-compatible dataset demonstrate that the proposed method can improve the transferability of untargeted attacks in comparison to other baseline methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Originality: The proposed method is original and intuitive. 

Clarity: The general structure of the paper is very clear: moving from validating an assumption to proposing an algorithm, and finally evaluating the proposed method with empirical results. 

Significance: The proposed method addresses a practical security concern of deep learning models. The proposed method improves the transferability of the adversarial perturbations compared to existing gradient-based methods. Extensive empirical evaluations were performed to demonstrate the efficacy of the proposed method.

Weaknesses:
Reverse adversarial perturbation (RAP) is a closely related work that encourages adversarial examples to be located at a region with low loss values. To demonstrate the novelty and significance of the proposed method, the paper needs a detailed discussion of the differences and similarities compared to RAP.

One of the major contributions claimed by the paper is the empirical validation that ""adversarial examples located in flat regions have good transferability"", and it is mainly covered in Sec. 3.2.
Putting aside the significance of the contribution, the authors should be very careful about the claims and statements in Sec 3.2. The assumption and the followed empirical validation both suffer from the lack of rigor and thus weaken the significance of the contributions. Please see the Questions section for additional discussions.

Some technical details in Sec 3.3 require clarifications.

Limitations:
I suggest the author include a brief discussion of the limitation of the proposed work.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper aims to boost adversarial transferability by using the Penelizing Gradient Norm, which can restrict adversarial examples located in flat regions. The writing is good, and it is easy to read. The experiments demonstrate that the proposed method achieves good results. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The motivation is clear, and the writing is well.

-  The analysis in Sec.3.2 is interesting, which can verify the assumption. 

- The proposed method is simple but effective. 



Weaknesses:
1. In Theorem 1, the authors briefly introduce the finite difference method, which is fundamental for efficiently approximating a second-order Hessian matrix. Although this is an interesting solution, it is better to verify this approximation in experiments if Eq. 6 is a good approximate solution to the objective function in Eq. 4. On one hand, I think the results of solving Eq. 4 directly should be reported, which can show that the approximated solution will not affect the performance. On the other hand, the running time and complexity analysis are also considered, which can show that this key design of PGN actually works well. 

2. In Table 1, we first observe that PGN is a good solution for boosting transferability. But, we can also observe that the source model IncRes-v2 can achieve the best average score. Does this mean that the loss surface of this model is in a more smooth region? 

3. I understand this paper focuses on boosting adversarial transferability. However, for black-box attacks, query-based adversarial attacks are also widely studied. Therefore, I am interested in if the Penelizing Gradient Norm is a generally method for black-box attacks, not limited to transfer attacks.



Limitations:
The proposed method is similar to the previous work [a].

[a] Penalizing gradient norm for efficiently improving generalization in deep learning. ICML 2022.

Rating:
5

Confidence:
5

REVIEW 
Summary:
In this work, the authors first assume and empirically validate that adversarial examples at flat local minima tend to have better adversarial transferability. Based on this finding, they introduce a regularizer on the gradients in the neighborhood of the input sample to achieve flat local minima. To make the attack more computationally efficient, they propose Penalizing Gradient Norm (PGN) attack, which approximates the second-order Hessian matrix by interpolating two Jacobian matrixes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It is the first work that empirically validates that adversarial examples at flat local minima have better adversarial transferability.

The approximation on the second-order Hessian matrix is reasonable with theoretical support.

The proposed method is simple yet effective. Extensive experiments have shown that PGN can significantly boost adversarial transferability compared with existing methods.

The visualization in Figure 2 validates that PGN can achieve better flat local minima than existing attacks, which further supports their motivation.

Weaknesses:
Results on vision transformers, such as ViT, Swin, etc. would better be included.

Evaluations on more defense methods, such as randomized smoothing and denoising should be conducted.

Limitations:
No.

Rating:
7

Confidence:
4

";1
fU9U7OYxfE;"REVIEW 
Summary:
This paper studies asynchronous proportional response dynamics (PRD) in linear Fisher markets under adversarial scheduling. The authors proposed an associated game with specific player utilities which admits an exact potential function. Then, the authors show that the set of pure NE of the associated game is the same as the set of market equilibrium bids, which is also the same as the set of maximizers of the potential function. Next, the authors show that the best response dynamics (BRD), where a single player is activated in each round, converges to the equilibrium prices. In terms of PRD with subsets of active players in each round, the authors discussed its connection with BRD and showed that PRD strictly increases the potential function value unless there is no update in bids. Finally, they show that a “generic” linear Fisher market (i.e., no multiplicative equality/degeneracy) exhibits unique equilibrium bids. With the above developments, and through mathematical analysis arguments, the authors prove the main theorem: if the market is generic, and if players are activated in subsets (arbitrarily, but each at least once in every $T$ rounds), then PRD converges to the unique market equilibrium.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The authors presented many interesting results that will likely be key foundational results for future research in game and market dynamics.
- These results are presented clearly with highly informative proof sketches and clear connections to other results.

Weaknesses:
None that I could think of. See **Questions**.

Limitations:
N.A.

More details:
- This work do not have (hidden, unstated) limitations. All claims in the abstract have been addressed in the work.
- This work is on theoretical properties of (variants of) well-konwn market dynamics and do not have immediate or potential negative societal impacts.

Rating:
8

Confidence:
4

REVIEW 
Summary:
In this paper, the authors examined Proportional Response Dynamics (PRD) in linear Fisher markets in a setting where participants act asynchronously. In particular, they considered a setting where at each step, an adversary selects a subset of players to update their bids. The paper showed that in the generic case, if each bidder individually uses the PRD update rule when included in the selected group, then the entire dynamic converges to the market equilibrium. As part of their proof, they have also established other properties such as uniqueness of the market equilibrium and the convergence of best-response dynamics in an associated game.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. This paper studies an interesting setup of the linear Fisher market where the activation can be asynchronous. 
2. The theoretical results of this paper appear quite sound. The authors adopted novel proof techniques; in particular, they established an important connection between the associated game and the original game, which helped prove the convergence of asynchronous PRD.


Weaknesses:
1. One thing that appears missing from the current paper is the motivation behind studying the linear Fisher market with asynchronous PRD. It is essential to provide a rationale for studying this particular setting and justify why it is important. Since the authors only allow an intermediate level of allowed asynchrony, it becomes even more crucial to justify why this specific setting is worth investigating.
2. The organization of the paper is unclear, especially regarding the relationship between the results presented in Sections 3-5 and the proof of Theorem 1. I find it a bit difficult to follow the flow and understand which results contribute to proving Theorem 1 and which are significant on their own. Additionally, the role of Section 4 is unclear to me in the overall discussion of the paper (it seems to me that this section investigate some property of the associated game; but I don't see how it contributes to other results).
3. The definition of the ""generic case"" needs more clarity. It would be helpful to provide more explanation as to why this assumption is necessary and what it signifies in the context of the paper.
4. The authors mentioned that the convergence of PRD under the full asynchrony model remains unclear. It would be beneficial to specify the main challenges associated with achieving convergence in this model. Additionally, it'd be good if the authors could elaborate on their conjecture that convergence occurs if information delays are bounded.
5. If the convergence of PRD is shown by the potential function of the associated game being strictly increasing, is it true that the convergence could be arbitrarily slow? This might be related to the second open question (i.e., no speed of convergence result is provided). 


Limitations:
See weaknesses.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper studies the convergence of Proportional Response Dynamics in linear Fisher Markets.
Fisher Markets are markets consisting of m divisible goods that should be shared among n agents with a linear utility function on items. The market not only must to decide the allocation, but it must also assign a payment to each agent for the fraction of each good she receives. We want that all items are sold (market clearing), that no agent spends more than her own budget (budget feasibility), and we are willing to allocate items so that there is no alternative market clearing and budget feasible allocation that an agent would prefer to the returned allocation (equilibrium).

It is known that a market equilibrium exists and can be computed in polynomial time by a centralized algorithm. Moreover there is a decentralized algorithm (known as tatonnement dynamics) that enable agents to quickly converge to the equilibrium, even if an adversary can choose at each time step which non-empty subset of agents would apply the dynamics update rule (subject to some fairness constraint).

In this work instead it is considered another dynamics (proportional response dynamics) that has been proved to converge when all agents update at each time step, but it was unknown whether this convergence property holds even against the above described adversary. This paper addresses this problem, by providing a positive answer.

The result is proved by observing that PRD for linear Fisher markets are equivalent to best response dynamics for a suitable potential game.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The paper is well written and presentation is very well-articulated and clear.
While the result re-uses some ideas previously established, it built an original framework to prove the main theorems on top of these ideas.

Weaknesses:
My first doubt about this paper is the relevance of this paper for this venue. While Fisher markets have been a very successful topic in economical and game theoretic literature, and computational aspects related to these markets have been of interest for theoretical computer science, these markets have not attracted very much the attention of AI community, maybe for the scarce practical applications (this can be seen also from the reference list of the paper). The paper does not make any effort to justify the study of these market within this community, neither it provides relevant applications.

My second doubt is about the relevance of these results. Why should we interested in proportional response dynamics if there is already a distributed dynamics that is known to converge to equilibrium even in an adversarial setting, that does it quickly, and that is a quite natural dynamics? Is there some motivation behind PRD that is not common also to tatonnement? The paper cites about similarity to a learning approach. Why this cannot be said also about tatonnement? And why should we interested to PRD convergence without bound on convergence time whenever we know that another dynamics converges quickly?

Based on this last comment it would be interesting to see in the experimental session a comparison about convergence of PRD and convergence of tatonnement, that is instead absent.

Limitations:
Partially

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of convergence in Proportional Response Dynamics (PRD) in linear Fisher markets when participants update in the dynamics under adversarial scheduling, i.e. an adversary specifies which subset of agents update their dynamics in a given round, subject to the constraint that each agent must be activated once every $T$ rounds at least. By leveraging auxiliary games and potential functions, it is shown that these asynchronous dynamics converge in generic settings; moreover, this analysis and connections also show that other natural dynamics also converge.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work makes substantial progress on an open problem on the convergence of adversarially scheduled PRD. En route to this result, this work also derives implications for other dynamics by exploiting new structural properties of auxiliary games they consider for the analysis. In general, this paper is quite well-written and the arguments in the main text are well-explained.

Weaknesses:
While this work makes substantial progress, it feels like attaining rates of convergence should be achievable under some adaptation of the (seemingly new) arguments provided here; however, the current analysis relies on compactness-style arguments so do not seem directly amenable to answering this question.

Limitations:
N/A

Rating:
7

Confidence:
1

REVIEW 
Summary:
Summary: The paper studies the Fisher market model, where there is a set of m sellers and n buyers. Each seller brings a unit of a divisible commodity for sale and each buyer brings a budget. The vendors value the money while the buyers value the commodities (goods).  A substantial amount of research has been done to develop methods for computing market equilibria and to understand the computational complexity of this problem. 

This paper makes progress on understanding a well known type of dynamics called proportional response dynamics, where the market starts in some initial configuration and evolves over time. The agents modify their bids based on past engagements with other agents and former market state(s). Specifically, in proportional response dynamics, each agent adjusts their bids relative to the utility value of the goods in  the preceding round. Previous studies demonstrated that proportional response dynamics gravitate towards market equilibria in diverse scenarios; these dynamics are also interpretable. 

Synchronous dynamics, where the players adapt their strategies with the same speed were studied extensively in the past work, but the more realistic asynchronous setting is less well understood (and before this paper not understood at all for the family of proportional response dynamics). The contribution of the paper is to study asynchronous proportional response dynamics in a very general setting where at each step a subset of players, adversarially chosen, update their strategies, with the constraint that each player has some minimum frequency of responding.

The main results of the paper are:

(1) For generic linear Fisher markets, proportional response dynamics with adversarial activation asynchrony, where each player is activated at least once every T steps, converge to the unique market equilibrium.

(2) The paper also finds a connection between the Fisher market and an associated game, such that the set of market equilibria of the Fisher market are the same as the set of Nash equilibria of the associated game and, furthermore, the same with the set of points that maximize a potential function \Phi (for which synchronous proportional response was shown in prior work to behave as mirror descent on this function). The paper shows that every proportional response step by any subset of players increases the potential function \Phi.

Evaluation: The paper makes several nice and significant contributions in a fundamental setting, contributing to the development of a theory of markets with learning agents. Such dynamics capture settings such as the stock market, which evolve over time. The paper raises intriguing questions for further study, such as how far asynchrony can be pushed, quantifying the rate of convergence depending on the extent of asynchrony, and investigating the correspondence identified in the paper between the proportional response dynamics and the associated game.

Various comments:

Page 2, line 60: “intermdiate” level of asynchrony -> intermediate

Page 6, line 236: ""\Phi is it’s potential"" -> its potential

Page 7, Lemma 1: ""for all"" should be capitalized -> For all

General suggestion -- some long inequalities or inequalities with fractions of sums are inlined (see e.g. page 5 line 205 in the main text, or page 3 in the appendix line 92 or page 5 in the appendix) and they are harder to read for this reason, it would be nicer to use displaymath or another environment like that. There are also some long paragraphs (e.g. when introducing the Fisher market) which could be divided into several shorter paragraphs for improved readability.



Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
+ The paper makes several nice and significant contributions in a fundamental setting. 
+ The paper is clearly written and of interest to researchers working in the space of games and learning.
+ Raises good questions for future work, such as how far asynchrony can be pushed, quantifying the rate of convergence depending on the extent of asynchrony, and investigating the correspondence identified in the paper between the proportional response dynamics and the associated game.


Weaknesses:
- Rate of convergence is not shown.

Limitations:
The authors have acknowledged the limitations of the paper and explicitly stated the open problems remaining to be solved, including analyzing the rate of convergence, studying even stronger degree of asynchrony, and so forth.

Rating:
7

Confidence:
4

";1
ghIBaprxsV;"REVIEW 
Summary:
The paper introduces a hierarchical semi-implicit variational inference that stacks multiple semi-implicit layers to construct a flexible generative distribution. The paper introduces a sequence of distributions that interpolate between the target and a base distribution. Each pair of intermediate inference and target distributions are matched by optimizing a SIVI bound. The method is applied in the diffusion model for sampling acceleration. 

##Post-rebuttal

Thanks for the authors' response. Most of my questions have been addressed and I will maintain my current score.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The idea of using semi-implicit distribution for the progressive approximation is interesting and intuitive. 

- The empirical results show HSIVI improves the distribution approximation and sample quality significantly on several tasks. 

- Though the modeling and training of HSIVI consists of many components, the paper explains the ideas concisely and clearly.

Weaknesses:
- The validity of the training procedure needs more concrete explanations or theoretical results

- The empirical study needs more evidence for the improvement of acceleration.

- Some closely related works need more discussions of similarity and improvement. 

Please see Questions for more details.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces hierarchical semi-implicit variational inference (HISIVI) as an extension of semi-implicit variational inference (SIVI). HISIVI incorporates interpolating distributions between prior and target data to train conditional layers progressively, resulting in accelerated sampling in diffusion models.

Soundness:
4

Presentation:
3

Contribution:
1

Strengths:
The paper is well-structured and easy to follow.

Weaknesses:
- The novelty of HISIVI compared to related work is not clearly demonstrated. It is recommended to include more related works and comparison: cascaded diffusion models[1], diffusion schrodinger bridge[2], flow matching[3], or rectified flow[4], etc.
- The experiment section is relatively weak. Comparisons could strengthen the evaluation (aforementioned references and more [5],[6], etc.)

[1] Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M. and Salimans, T., 2022. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1), pp.2249-2281.
[2] De Bortoli, V., Thornton, J., Heng, J. and Doucet, A., 2021. Diffusion Schrödinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34, pp.17695-17709.
[3] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M. and Le, M., 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747.
[4] Liu, X., Gong, C. and Liu, Q., 2022. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003.
[5] Xiao, Z., Kreis, K. and Vahdat, A., 2021. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804.
[6] Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K. and Anandkumar, A., 2022. Fast sampling of diffusion models via operator learning. arXiv preprint arXiv:2211.13449.

Limitations:
This work only deals with the geometric interpolation of distributions without providing justification. Exploring the impact of choice of interpolation on generation would enhance the quality of this work.

Rating:
3

Confidence:
2

REVIEW 
Summary:
Semi-implicit variational inference increases the expressiveness of variational posteriors but introduces intractabilities to their inference. This work adds to the semi-implicit variational inference (SIVI) work of Yu and Zhang (https://openreview.net/forum?id=sd90a2ytrt) which uses an alternative objective (the Fisher divergence) that, combined with the hierarchical nature of semi-implicit variational distributions, can be approximated as the solution of a mini-max optimization problem. This work takes inspiration from simulated annealing and denoising-diffusion models by proposing to expand the single-layer semi-implicit variational family into multiple-layer variants by specifying hierarchical auxiliary distributions to guide the semi-implicit variational distribution towards the target distribution. The authors denote this method Hierarchical Semi-Implicit Variational Inference (HSIVI). The auxiliary distributions are specified either by assuming the marginal densities of a geometric interpolation between a target and base distribution are available analytically or by using pre-trained scores from a diffusion bridge. The main contribution to this paper is stated in the author's claim that ""when used for diffusion model acceleration, we show that HSIVI can produce high quality samples comparable to or better than the existing fast diffusion model based samplers with small number of function evaluations on various datasets."" This claim is tested empirically on the CIFAR-10 (32x32) and CelebA (64x64) datasets with results showing comparable or better FID scores to existing fast diffusion model samplers with few (between 5 and 15) function evaluations. As well as the main contribution, an algorithm for training is provided by noting that the conditional layers can be trained sequentially until convergence by exploiting the hierarchical structure of the semi-implicit distribution. Also, an efficient parameterization of the neural networks is provided to make the algorithm computationally feasible.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
Quality

The work is a novel combination of well-known, peer-reviewed techniques including simulated annealing and SIVI. It is clear how this work differs from the work of Yu and Zhang (https://openreview.net/forum?id=sd90a2ytrt) using the hierarchical semi-implicit variational posteriors, and the value of this contribution is demonstrated on a multi-modal problem in Figure 2. The methods used seem to work well judging by the experimental results. 

Clarity

The submission is well-organized and clearly written.

Weaknesses:
Originality

There is limited technical novelty in this paper as the technical content on the work is almost entirely covered by the work of Yu and Zhang (https://openreview.net/forum?id=sd90a2ytrt) that rewrites the Fisher divergence leading to a form that does not require computing the score of the hiearchical variational posterior. The paper extends this by sequentially score matching to noised versions of the target distribution, which have been obtained analytically or approximated from pretrained denoising diffusion models. The main novelty in this work is instead an experimental one, showing that HSIVI can produce high quality samples comparable to or better than the existing fast diffusion model based samplers with a small number of function evaluations on two datasets by comparing FID scores (CIFAR-10 (32x32) and CelebA (64x64)) and one dataset by comparing samples visually (MNIST). The experiments were made computationally feasible by a trick to parameterize the neural networks that is detailed in Proposition 1.

Quality of results

Relatively simple experiments are provided that on one hand demonstrate the contribution well, but on the other hand do not cover a very diverse range of cases. No empirical failure case is provided for a complicated distribution that requires many function evaluations to sample from, which may provide an interesting avenue for future work.

On the reproducibility of the results, whilst the experimental setups are well detailed, no code is provided with the paper, nor indication it will become available upon publication. The reviewer would like to see that the code is made available since this is crucial for reproducibility. Furthermore, there may not be enough information to replicate the experiments. For example, as far as I can see, there is no discussion on the parameterization/values of the positive weighting function, beta(t) for joint HSIVI-SM training on CIFAR-10 and MNIST, which makes it impossible to reproduce without any code provided.


Significance

The work's significance would be improved if it addressed why we should expect improvements in sampling efficiency over denoising-diffusion sampling and where these improvements can be derived from (the particular form of variational family, or something else?).

Limitations:
The limitation discussion picks up on the important points, but it would be good to know if higher dimensional problems were attempted, or problems with more parameters (such as the ""huge VP deep continuous-time model (Song et al., 2020b) that has more channels and layers""), and did the authors run into memory issues that were discussed in Appendix F?

An ablation of the variational family for the conditional layers (as described in question 3) comparing directly to the denoising-diffusion sampling remains unexplored, and would be useful to see.


Rating:
6

Confidence:
3

REVIEW 
Summary:
Authors propose a hierarchical semi-implicit variational inference framework by extending the existing SIVI. Authors showed that the proposed HSIVI, given pre-trained score networks, can be used to accelerate the sampling process of diffusion models with the score matching objective. The numerical results show more enhanced performance in faithful modeling of complex distributions and diffusion model acceleration.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is fairly well-written, clearly motivated and generally addresses an important problem. The extension of SIVI to a hierarchical model, although not very novel, is intuitive and makes sense. I haven't checked the proofs but the overall approach seems to hold up.

Weaknesses:
- Extending SIVI to a hierarchical model, by itself, is not very novel. However, the application to diffusion acceleration is interesting. 
- The experiments and presented numerical results could be improved. (check next section for more)

Limitations:
The limitations to some extent have been discussed but more discussion (specially around memory usage) is needed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors extend semi-implicit variational inference to have multiple layers of latent variables, vastly increasing the. expressiveness of the model. A nice formulation of the asymptotic lower bound plus a training scheme is introduced as well.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is an extension that on the surface seems straight forward. But the authors demonstrate that this is far from the case and introduce a novel formulation of the semi-vi ELBO.

The paper was also written well and easy to follow. The experiments section was great too. 

Weaknesses:
My only qualm is the flow between the first half of the paper and the second half where the focus is on diffusion models. Previously, the authors argued that training using a sequence of distributions would lead to better marginal distributions. But for speeding up sampling of diffusion models, it seems like training the joint distribution leads to impressive results. This opens up the question of whether the sequence of distributions is even needed at akk.

Limitations:
It seems like the main text is missing a limitations section.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a new method called Hierarchical Semi-Implicit Variational Inference (HSIVI) that enhances the expressiveness of Semi-Implicit Variational Inference (SIVI) on complex target distributions. HSIVI works by applying SIVI to a hierarchy of latent variables, and using the variational distribution of the previous step as the implicit prior for the later step. The authors apply HSIVI for sampling from diffusion models, learned on a variety of synthetic and real-world datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novelty: The paper presents a new method called Hierarchical Semi-Implicit Variational Inference (HSIVI) that enhances the expressiveness of Semi-Implicit Variational Inference (SIVI) on complicated target distributions. This is a novel approach that has not been explored before, to the best of my knowledge.

2. Effectiveness: The authors demonstrate the effectiveness of HSIVI on a variety of synthetic and real-world datasets, including accelerating the sampling process of diffusion models. The results show that HSIVI outperforms existing methods in terms of accuracy and efficiency.

3. Clarity: The paper provides a detailed explanation of the HSIVI method, including its mathematical formulation and implementation. The authors also provide clear and concise descriptions of the experiments and results.


Weaknesses:
The main weakness of this work is that the method seem fairly complicated to implement in a practical setting. The method is presented in a general way, with an algorithm box. However, later another objective function is presented with some parameter sharing. Then, in the larger-scale experiments, HSIVI-SM is trained by fune-tuning a larger model. Either the method is not extremely stable / usable, or there are too many details but those are necessary for the experiment to be setup properly? (in the latter case, perhaps one experiment should go to the supplements?).

Limitations:
Yes

Rating:
6

Confidence:
2

";1
UZlAjSnmvB;"REVIEW 
Summary:
This paper present a new method for ensembling generative retrieval and embedding-based dense retrieval. Prior generative retrieval work that solely relies on a generation model to generate the retrieval document, but it is difficult to scale to large corpus. In contrast, this paper uses the generation model to predict candidate document clusters, but still adopt a dense retriever to produce the final document rankings, where the document from different clusters are weighted according the generation model's output.  

The authors first evaluated this method on MS MARCO. The proposed method show improvements over a dense retriever T5-ANCE, and also good support to corpus update. The authors also tested this method in a production ad system where improvements were also observed over their production dense retrieval model (a 4-layer transformer dual-encoder).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper proposes a simple idea to ensemble generative retrieval model and embedding-based dense retrieval.  In the proposed method, the generative retrieval model is essentially predicting document clusters from a query which are then used for reweighting or pruning the dense retrieval results. This aproach is a lot more practical and scalable compared to existing generative retrieval approaches that need to generate the exact document identifier. 
- Experiments show promising results.
- Paper is well written.

Weaknesses:
- The authors did not compare to any published neural retrieval baselines on MS MARCO, as T5-ANCE and NCI are both their own implementations. I'm wondering how does the method work with SOTA dense retrievers such as RocketQA v2 or coCondensor, whose results seem stronger than T5-ANCE on MS MARCO. 
- It would be nice to add another dataset, such as Natural Questions.
- It is unclear to me what is the advantage of RQ compared to other hierarchical clustering approaches. It would be nice to add some discussion and ablation studies. 
- There are some confusions around T5-ANCE. First of all, T5-ANCE does not seem to exactly follow the ANCE recipe which uses dynamic indexing for hard negative mining. Second, it is unclear the model size and hyperparameters for this model. 
- I think is is an over-claim to say ""For the first time, we demonstrate that a novel-designed generation-based model is able to handle
 a large corpus with millions of documents, reaching high recall performance and low serving latency at the same time."" From my understanding, this method mostly rely on dense retrieval and generation-based model is only providing cluster-level weights. To justify this claim, the author should report the performance without ensembling with dense retrieval.

Limitations:
No

Rating:
6

Confidence:
5

REVIEW 
Summary:
In this study, the authors present a new approach called Model-enhanced Vector Index (MEVI) that combines ideas from both autoregressive sequence-to-sequence models for indexing and dense retrieval models (twin-tower architectures), and includes Residual Quantization. This approach offers significant advantages and proves to be highly effective in real-world applications. MEVI demonstrates nice performance gains in terms of achieving both high recall accuracy and faster retrieval speed, even when dealing with large-scale corpora.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The method introduced combines two of the recent approaches for document retrieval - dense retrieval and the generative neural indexing. It showcases the power of these together and is extensively evaluated and experimented, both in an offline manner as usual, but also online in real-word scenarios (in commercial advertising). They also include a nice latency evaluation.
* The paper is well-written and easy to follow in general.


Weaknesses:
Major comments:
* MSMARCO is the only benchmark which was evaluated in this work. I’d love to see if the method generalizes well across more datasets. This is required in order to convince readers to favour this method.
* While an online A/B testing experiment appreciated, there is no guarantee about the quality of such kind of testing, while controlled crowdsourcing does provide an infrastructure for filtering out bad annotations.
* L152: I am not convinced that the tree structure is problematic because of decoding time. It can actually alleviate the decoding if using controlled decoding instead of plain beam search. Also, the hierarchical nature of the structure of embeddings proposed in this work is not that different (as stated in L167).
* The authors report results using MRR@10 Recall@50 Recall@1000 but it woul’d be interesting to see both more fine-grained retrieval results (Recall@1) following previous works, as well as a figure which states how the method results progress as k is being increased.
The training process of the twin-tower model and the sequence-to-sequence model are distinct which may be very problematic. But I do acknowledge this was stated in the limitations section.


Minor comments:
* Please elaborate more both in the contributions part in the introduction section about and in the introduction itself more about the proposed MEVI method. After reading this, I didn’t have any clue about the method which should be stated much early in the text.
* Related work section - please state how your method differs from other recent works.
* L146: insert space after T5-ANCE. Similar comment for the paragraph starting in 149 and the rest of the paper.
* L157-166: The mathematical notations lack clarity in terms of explicitly defining the dimensions of the vectors/matrices and what they represent.
* Table 4: please include detailed caption so the table can be understood standalone. The same applies for the rest of the tables.


Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work introduces residual quantisation codebook into k-mean clustering to generate semantic IDs. These semantic IDs are used to provide initial cluster-level ranking and prune the corpus into document subset with thousand documents. The interpolation between the dual-encoder similarity and a derived cluster score is used as final ranking. The experiments are on msmarco passage dataset.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- An novel idea is implemented and examined to exploit the semantic ids from generative retrievers to introduce clusters into dual-encoder
- The writing is easy to follow and the method is insightful
- Promising results are reported on large in-house dataset Ms Marco and on an online system

Weaknesses:
- The cluster-based retrieval has been long studied and this paper should be put into context better, e.g., [1, 2] . Besides, is this the first work to introduce clusters into dual-encoder retriever?
- The effectiveness improvement in Table 1 comes from the fine-grained clustering information. Does such boost still exist when using state-of-the-art dual-encoder retrievers, like rocketqav2 and GTR, where the document similarity is with higher quality? Actually, in Table 1, it can be seen that use of HNSW could boos the recall considerably. Also, the results of the online system are also on top of weak dual-encoder models (four layers). Thus, it is beneficial to better understand when and where the clustering information could help. 
- For the dynamic update of documents, is this the first work that investigates this problem? What are the existing methods tackling this problem and how they design the experiments? The results from Table 2 look promising, but I am not sure how convincing it is to support that claims of dynamic updates.
- As an ablation results, Table 3 is hard to read, and the configuration differences among rows are more than one out of three (subgroup, bits, top-k), and it is hard to answer by increasing subgroup or k it helps.
- Some formatting issues: a space is required between the text and their follow-up citation, like line 150


1. https://dl.acm.org/doi/pdf/10.1145/253495.253524
2. https://arxiv.org/abs/2008.00150

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes to improve dense neural IR models by adding a RQ structure (hierarchical clustering based on residuals at each step) before the ANN search. The structure is used to construct a semantic identifier string for each document. The authors thus use a generative approach to retrieval combined with a dense model.

This approach has two advantages, firstly to have a faster retrieval (since the ANN search is restricted to a subset of the document), secondly to have a better retrieval (by exploiting a cluster-based score). Experiments conducted on MS Marco (passages) and a proprietary advertisement dataset show improvements over T5-ANCE.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well written and technical details ensuring reproduction are given.

The approach is promising as it both allows for more effective and efficient retrieval - which is always a challenge in IR. The proposed approach, based on RQ clusters, is original and combines the strength of generative and dense models in IR.  

It also shows that adding documents is not as problematic as for other generative approaches (but however the performance is worse than T5-ANCE in that case, for a higher latency).

Weaknesses:
- Comparison with state of the art models (A2R/Adversarial Retriever-Ranker for dense models, SPLADE for sparse ones) is missing - it would have been good to see how with better dense models the proposed approach evolves.

- With respect to generative models, several approaches have better results than NCI nowadays (e.g. Ultron) or the more recent “Learning to tokenise” (SIGIR’23). At least Ultron should be reported (ArXiv 2022) for comparison purposes.

- The authors have chosen to use a proprietary dataset (section 4.7) for the second evaluation. It would have been much wiser to use publicly available ones as nothing can be checked on this one (e.g. BEIR) - reporting proprietary results in the appendix would have been appropriate.

Limitations:
The limitation section contradicts a bit the initial claims (“… the capacity of the sequence-to-sequence model is still insufficient to cope with large-scale corpus within acceptable inference latency …”), but correctly lists the different limitations of the paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This research paper introduces a deep text retrieval model that possesses the capability to effectively manage a large corpus comprising millions of documents. The model achieves remarkable recall performance while maintaining relatively low latency. Moreover, in addition to surpassing the performance of existing methods in a document retrieval benchmark, the paper also showcases the successful integration of deep text retrieval models into a commercial advertising system, demonstrating their practical value in an industrial setting.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1.	S1: They are supposed to be the first to demonstrate the successful implementation of deep text retrieval models in the practical application.
2.	S2: They investigate and test the effectiveness of Residual Quantization within the deep text retrieval model.
3.	S3: The experiment results on MSMARCO dataset display the effectiveness of the proposed method.


Weaknesses:
W1: They conduct experiments on a single dataset for their study. However, it is recommended that their model undergo experiments on commonly used datasets such as Natural Question, which is utilized in both DSI and NCI research. In addition, they point out that the existing methods are limited by unacceptable serving latency but they do not conduct such experiments to prove this. Since their method use Residual 
Quantization (RQ) as their clustering method, this method may be more time-consuming than other clustering methods during training.


Limitations:
The authors pointed out the limitations of their model including not jointly learning between twin-tower model and seq2seq model, and their method is still unacceptable inference latency in the large-scale corpus. They had provided potential solutions to address the above issues.

Rating:
4

Confidence:
3

";1
A6EquH0enk;"REVIEW 
Summary:
The authors consider the task of properly modeling heteroscedastic observation noise with Bayesian Neural Networks (BNNs).
They identify several shortcomings in prior work concerning variance modeling and propose to rely on parameterization
via natural gradients instead. Together with a Laplace approximation to model epistemic uncertainty and a closed-form
posterior predictive likelihood approximation the proposed approach is evaluated on three sets of experiments. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper considers a well-defined task, identifies prior weaknesses, and offers a principled solution.
- The paper is well written and can be understood easily by a reader
- The experimental setups cover a wide range of domains

Weaknesses:
- The individual steps, i.e., switching to natural parameters, reliance on Laplace approximations, and approximate
    closed-form marginalizations for the posterior predictive are mostly minor adaptations from prior work
- Section 5.2 reads more like an afterthought that is added to the paper to have an additional experiment. 
    - The setup description is mostly left to the prior work which introduced the experiment.
    - The evaluation of the results is similarly vague. E.g., Heteroscedastic VI (PP) has a huge variance in Figure 1, Exp 1,
whereas it suffers no such problems in the individual replications. The Laplace approach of the proposed method 
not only performs a little bit worse than the others in setting two (survival-screen-A375), but struggles a lot,
whereas the MAP approach does not suffer from this problem. 


## Minor 
- None of the tables do follow the NeurIPS style guide. Captions should be placed above a table, and tables should not contain vertical lines (fig 2)


## Typos
- Fig 2 right and Table 1/2 switch the se notation (from $\pm$ to brackets)
- Between Table 1 and 2 the formatting of the method column switches from right-aligned to left-aligned

Limitations:
Limitations of the model are discussed, but the societal impact is not. Given the theoretical nature of the paper,
I do not consider this lack of discussion to be a problem.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper extends the technique of the Laplace Approximation to incorporate heteroscedastic aleatoric uncertainty. The proposed technique achieves this by exploiting the natural parameters of the Gaussian likelihood. Main motivation is the coupling of the mean and the input dependent variance in the Gaussian likelihood. Using the natural parameters, the paper suggests the heteroscedastic Gaussian log-likelihood, linearized Laplace Approximation, and marginal likelihood for obtaining the prior precision term. Two standard benchmarks including UCI and one self-made benchmarks, improvements over the previous methods are demonstrated.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
In my opinion, the followings are the strengths of this paper:

- Presentation of the paper is generally clear to see the contributions of the paper. I appreciated especially the section 2, which carefully addresses the problem at hand. 

- The contribution of the paper is relevant to the current Bayesian Deep Learning community. For example, extension of the Laplace Approximation to consider input dependent aleatoric uncertainty is meaningful. To do so, the paper provides long stretch from finding the problem in standard formulation of the Gaussian likelihood, to showing the adaptations on linearized Laplace Approximation, Gaussian log likelihood, predictive distribution and marginal log likelihood.

- Experimental results show good performance.

Weaknesses:
The followings might be the weaknesses of the paper (all major):

- The experiments only focus on regression problem, which is limited in scope for a paper on Bayesian Deep Learning.

The paper mainly focuses on the regression problems, e.g., all the experiments as well as the methodologies (section 2 and section 5). One way to address this point could be certain changes in the title. Would it be possible to include the term ""regression"" in the title?

- Certain parts of the paper need revision in presentation.

1. Many technical terms are introduced without explaining them well. Major examples are in the introduction 2nd paragraph, e.g., feasible generalized least squares, natural parameters, standard FGLS, etc. 

2. The logic flow in the introduction may not be very kind to the reader. In particular, I had difficult time reading the 3rd paragraph. After reading the paper, I could grasp the concept well, but my feeling is that it is diving too much into the details early on.

3. I left minor comments later in the review.

- The baselines are restricted to the area of heteroscedastic aleatoric uncertainty estimation, which may be limited for broader audience. The scale of the experiments are also limited to toy-ish benchmarks. One option would be to use the uncertainty-baselines benchmarks.

In all the experiments, the baselines are rather restricted. While these choices validate the main point of the paper, I think the paper could improve by also competing against generic state of the art in uncertainty estimation. Would it be possible to include atleast MC-dropout, deep ensemble, and maybe stochastic HMC? (with homoscedastic aleatoric terms?) In this way, one could examine the importance of heteroscedastic aleatoric uncertainty estimation.

- Related work section is missing in the paper, which makes it harder to locate this work within the state-of-the-art.

To back up, there is no related work section in the main body of the paper. This makes it difficult to locate this work within the current state of the art. Would it be possible to include them? (one could shorten other parts of the paper). Some important areas are the usage of natural parameters of the Gaussian distribution, and more detailed treatment on aleatoric uncertainty, e.g., calibration methods, combination of both model and data uncertainty, etc. To name only few, highly relevant works seem to be: (1) Natural-Parameter Networks: A Class of Probabilistic Neural Networks (one of the works that uses natural parameters in more general context) and (2) estimating model uncertainty of neural networks in sparse information form (one of the works that uses natural parameters of Gaussian distribution for Laplace Approximation). Why the authors build on regularization based aleatoric uncertainty estimation method should also be mentioned.

Limitations:
The limitations sections exist.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This article focuses on refining the techniques used to manage two types of uncertainties in complex regression tasks utilizing deep neural networks. The uncertainties, known as aleatoric (arising from inherent randomness in the data) and epistemic (originating from the model's limitations), are critical to address for robust and accurate predictions. The authors propose an innovative approach using the natural parameterization of the Gaussian likelihood to overcome the gradient scaling issue commonly encountered in traditional methodologies. Additionally, they introduce an efficient Laplace approximation that enhances heteroscedastic neural networks, providing epistemic uncertainties, and facilitating automatic regularization through empirical Bayes. This method outperforms earlier strategies in heteroscedastic regression, demonstrating scalability and obviating the need for hyperparameter tuning. Empirical validation on diverse datasets, including a new image dataset, UCI regression, and CRISPR-Cas13, yielded superior performance, suggesting potential applicability to other real-world datasets in the future.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- They are addressing very important shortcomings in one of the best algorithms for quantifying uncertainty using neural networks.
- Their method seems mathematically solid and is supported by extensive experiments.
- The code is available, and this work seems to be reproducible.
- Their method is thoroughly compared with the rival algorithms.
- Compared to rival methods, this method has less hyperparameters. 



Weaknesses:
I cannot spot any weaknesses in this paper.

Limitations:
The limitations of this work are discussed properly.  

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces a new method for fitting heteroscedastic regression models with neural networks. In contrast to previous work that highlights the deficiencies of MLE and indirectly hint at regularization, this paper suggests using natural gradients during training and combines Bayesian ideas for regularization and to handle uncertainty. This modeling approach accounts for both aleatoric and epistemic uncertainty through the posterior predictive distribution. Bayesian inference can be computationally expensive, so they utilize a Laplace approximation to the posterior and simplifying factorizations on the Hessian. Natural gradients avoid some of the issues with gradients that occur when training with other parameterizations ($\mathcal{N}(\mu, \sigma^2)$). Empirically this method achieves strong results on real-world data for regression tasks as well as on image data.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Problem is well-motivated, and the distinctions between this method and existing ones are clear
- Experiments show solid results and extend the method to a setting with image data as opposed to only tabular data
- Combines ideas from approximate Bayesian inference with a parameterization commonly used for Gaussian processes 

Weaknesses:
- typo line 98: ""the mean respectively the covariance matrix"" --> ""the mean and covariance matrix, respectively""
- Structurally, it was odd to see Problem 2.1 presented so high up in the paper
- I would appreciate more discussion on the differences between the natural MAP vs natural Laplace methods. In particular, the differences in the interpretations of the two models (what sorts of uncertainties they account for) and when to use which

Limitations:
There was not much discussion of this, but that is appropriate for the nature of this work.

Rating:
7

Confidence:
4

";1
BqZ70BEtuW;"REVIEW 
Summary:
The manuscript proposes a new method for dense anomaly detection in industrial applications. The proposed method first extracts dense features with a frozen pretrained model. Then, a normalizing flow learns the distribution of extracted features at every spatial location. While the flow parameters are shared across the spatial locations, the parameters of flow prior are independently computed by an additional network h conditioned on the input image. In particular, the network h outputs variances of flow priors while means remain fixed. During the training, the input samples are augmented with synthetic negatives. The optimization objective is formed of the likelihood maximization for inlier features,  regularization of network h outputs, and discrimination between inliers and synthetic negatives. During the inference, the method detects anomalies according to the aggregated likelihood of features across different resolutions. The developed method achieves competitive results on two considered benchmarks.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
S1. The manuscript is well-written and easy to follow.

S2. Modelling different normalizing flow prior at every spatial location appears to be novel.

S3. The proposed method achieves performance gains over related works.

Weaknesses:
W1. The difference between CFlow-AD [19] and the proposed SANFlow i) is modelling independent flow priors for each spatial location and ii) mapping anomalous and non-anomalous features to flow priors with different means. These contributions may be rather incremental.

W2. Intuition on why estimating both mean and variance is inferior to estimating variance only (L197) should be comprehensively elaborated beyond L323-328. Especially since predicting parameters of flow prior is the main contribution of the manuscript.

W3. Missing related works which augment datasets with synthetic negatives [a,b,c,d].

[a] Shu Kong, Deva Ramanan: OpenGAN: Open-Set Recognition via Open Data Generation. ICCV 2021.

[b] Matej Grcic, Petra Bevandic, Sinisa Segvic:
Dense Open-set Recognition with Synthetic Outliers Generated by Real NVP.  VISAPP 2021.

[c] Victor Besnier, Andrei Bursuc, David Picard, Alexandre Briot: Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation. ICCV 2021.

[d] Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin:
Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. ICLR 2018

Limitations:
The manuscript adequately addressed the limitations of the method. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a novel NF-based framework for anomaly detection (AD) and anomaly localization (AL) task. The proposed Semantic-Aware Normalizing Flow (SANFlow) framework map the distributions of normal data to different distributions at each location in the given image instead of mapping a distribution of whole image and makes distributions of abnormal data distant from normal data. The proposed method achieves the state-of-the-art in both AD and AL.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
[1. The proposed SANFlow model addresses the problem of learning difficulty in anomaly detection, which is conventional in NF-based methods. This innovation makes full use of semantic information and enhances the accuracy of anomaly detection.
2. This paper provides a detailed description of the proposed SANFlow model. The methodology is well-developed and supported by theoretical analysis.
3. The experimental results on the Mvtec-AD demonstrate the superiority of SANFlow.

Weaknesses:
1. This paper lacks the implementation details of the multi-scale FN, making it difficult to reproduce the study. And no code is provided to reproduce the proposed method.
2. This paper needs ablation studies about the margin between normal and abnormal means.
3. More experiments should be conducted. The paper lacks experiments on other standard AD datasets like CIFAR10.

Limitations:
Yes.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper propose a NF-based framework to model the normal feature distribution as Gaussian distributions with zero mean but different variances, and the abnormal features as distinct Gaussian distribution. The proposed model demonstrates relatively good performances in both anomaly detection and localization.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The writing of this paper is clear and concise. It's easy to follow and understand each component of the proposed model. The motivation to model various distributions according to different regoins are reasonable.

Weaknesses:
1. The novalty of this paper seems to be increamental. It's based on / combanition of the reference papers [34] [22] [19] [48].
2. The incorporation of semantic information seems to be naive. All the pixels have the same base distributions following the inverse
Gamma distribution as IG(·|α, β).  I wonder if the semantic information can be incorparated into the corresponding base distributions (The same regoin share the same base distribution).
3.  I don't understand why you fix µi to be 0 for normal regions and 1 for abnormal regions. As the normal pixels have various characteristics, maybe corresponding to various µi, at least fot different regoins.
4.  s(zi) maybe helpful for computing anomaly score. I'd like to see the correlations between s(zi)  and  GT location, which can reflect the gap between simulated training  anomalies and real testing anomalies.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents an enhanced flow-based anomaly detection method by utilizing density estimation on normalizing flow (NF). The proposed approach involves predicting the inverse Gamma distribution using a statistics prediction network, which facilitates dynamic training of normalized flow distribution. Additionally, the training process benefits from the introduction of anomaly feature distribution and incorporates data augmentation techniques to enhance performance. By learning to map diverse normal features and abnormal features to distinct base distributions, the proposed NF framework enhances its capability for density estimation. The experimental results show that the improved density modeling achieved by the proposed framework, resulting in enhanced anomaly detection performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ In contrast to previous flow-based anomaly detection methods that transform the distribution of all features into a single distribution, this work introduces a statistical prediction network to guide the training of different semantic features according to their respective distributions.
+ By incorporating the data augmentation, both normal and anomaly distribution are predicted and employed to derive to the anomaly score. The qualitive results demonstrate that the localization performance of the proposed method is improved by data augmentation.


Weaknesses:
- The paper does not provide the detection performance of the PRO metric. The flow-based method tends to produce oversized defect regions, particularly for small-sized defects. Therefore, it is crucial to evaluate the location performance using the PRO metric.
- For image based anomaly detection, several recent AD methods achieves promising the image level and pixel AUROC on MVTEC AD, which make it difficult to identify the advantage while comparison. Therefore, it is necessary to compare the performance on other datasets, e.g. VisA. In VisA, images exhibit complex structures, objects placed in sporadic locations, and different objects, which could differentiate the detection performance of different methods.
- The paper utilizes a self-defined data augmentation method to synthesize anomalies. However, several existing data augmentation methods, such as Draem, nsa, and cutpaste, have been proposed for anomaly detection. It is important to consider whether the choice of data augmentation method impacts the detection performance.


Limitations:
Authors have addressed the limitations of the method.

Rating:
5

Confidence:
5

";1
tmxjuIFSEc;"REVIEW 
Summary:
This paper proposes a single-round participant contribution evaluation method for FL. The novel and interesting part is using the sample embedding similarity between client data and (server) validation data to indicate contribution, thus avoiding the time-consuming model retraining step.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Very useful problem
2. Interesting idea with using embedding instead of retraining models for contribution evaluation.

Weaknesses:
1. No theoretical analysis of why the embedding similarity can lead to an excellent approximation to the original Shapley-based contribution evaluation with re-training. Since embedding similarity-based contribution evaluation is quite different from original retraining-based contribution evaluation, such a theoretical analysis is a must; only empirical analysis cannot verify that the proposed contribution evaluation can always be good.

2. Experiment is too limited. More datasets, more parties, and malicious behaviors of parties should all be counted to ensure that the contribution obtained by the proposed method is robust.

3. The method seems to only work for classification. How to extend it to regression tasks?

Limitations:
Not enough. More discussions on when the proposed model can work well should be given.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduced a novel method to evaluate the participant contribution under the setting of federated learning. In the beginning, the author raised two challenges of recent works related to the participant contribution of FL: Multi-round training and dataset dependency, which are actually about the communication and computational costs in FL. The author derived the SPACE model to address these two challenges. Specifically, to address the high communication cost issue, the author introduced a module named Federated Knowledge Amalgamation, which enables a single round of communication. Then the author introduced Prototype-based Model Evaluation to reduce the evaluation complexity since this method only needs similarities comparison without iterating through a validation set. In addition, in the contribution part, the author rectified the utility function with a logistic function to address the rationality violation issue. Last, the author evaluated participant contributions to the SPACE model on two datasets compared with four baselines. Furthermore, the author experimented with SPACE on the client reweighting and selection problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The author raised two challenges of participant contribution evaluation in FL, and introduces corresponding techniques to address them. 
2. The author explained the reason and function of each module in detail. 
3. The author provides a theoretical value of communication costs and computational costs of baselines and derived model.


Weaknesses:
1. The author should pay attention to the writing style. For example, more focus on key points, like how to reduce computational complexity. Otherwise, it is hard to follow.  
2. The author should provide more detail about the theoretical analysis on communication and computational cost. For example, how the design has the advantage to reduce complexity and appropriate for GPU?
3. In section 4.3, the author pointed out a problem that applying model performance as utility function may violate rationality. And the author add a activation as solution. Could you explain why you choose this activation and why this design can avoid violations?
4. The focus in this paper is to reduce complexity, so the author should provide results of the computational cost and communication cost in experiment part.


Limitations:
The author can provide more details of the theoretical complexity analysis. In addition, a more comprehensive experiments should be conducted, such as including model complexity, ablation study and case study.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies to evaluate the contribution of each client during federated training efficiently. It proposes a framework named SPACE, which trains a student model in the server with one communication round to measure the similarity between local datasets and the validation dataset in server. Finally, extensive experiments are conducted to prove the efficiency of SPACE. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and organized.
- The proposed solution is feasible and efficient. 
- Extensive experiments are conducted to evaluate the proposed framework. 

Weaknesses:
- The assumption of SPACE is too strong. (See Question 1 and 2)
- The proposed contribution estimation framework is decoupled from federated training. (See question 3)
- The dataset and client number in the experiment are too small. (See Question 4)

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a novel approach named Single-round Participants Amalgamation for Contribution Evaluation (SPACE) for efficiently evaluating the contribution of participants in Federated Learning (FL). Accurately evaluating participant contribution has been a challenge in current FL, especially considering cost and scalability. Current methods mostly use the Shapley value, a measure from cooperative game theory, but calculating it is computationally expensive. SPACE, on the other hand, combines two new components - Federated Knowledge Amalgamation and Prototype-based Model Evaluation - to reduce evaluation effort. Federated Knowledge Amalgamation distills information from all local models into the server model in just one round of communication, saving time. Prototype-based Model Evaluation compares server and client prototype similarities, effectively eliminating the dependency on the size of the validation set.

Additionally, SPACE modifies the utility function using a logistic function to better reflect user satisfaction, enhancing the utility function's rationality in real-world settings. Experimental results show that SPACE outperforms current methods in terms of both running time and Pearson's Correlation Coefficient, a measure of correlation strength. The efficacy of SPACE has been tested in various applications, including client reweighting and selection, and consistently demonstrates exceptional performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- SPACE, appears to be a novel and innovative approach to evaluating the contribution of participants in Federated Learning. The introduction of components like Federated Knowledge Amalgamation and meta-learning using prototypes could potentially provide new avenues for research.
- The method aims to reduce the time and computational resources required for evaluating participant contributions by eliminating dependence on the size of the validation set and enabling evaluation within a single communication round. This efficiency is particularly valuable in Federated Learning, where minimizing communication is crucial due to privacy and resource concerns.

Weaknesses:
- Although not specific to this one paper, a recurring trend in federated learning conducts experiments that may overfit to specific use cases. In particular, most papers demonstrate the efficacy of their methods using datasets limited to Federated MNIST and non-iid versions of CIFAR (in this case with only 10 classes). Although these experiments might demonstrate proof of concept, adaptability and effectiveness of the method across a broader range of datasets and scenarios remains unclear. As such, I believe additional experiments with more extensive datasets, or an additional discussion on how this method can be applied to real-world scenarios might help.

Limitations:
See weaknesses above

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper studies the client contribution evaluation problem under federated learning settings. The goal is to achieve more computational and communicational efficient contribution evaluation. The paper proposes Federated Knowledge Amalgamation and Prototype-based Model Evaluation technique for the goal. Federated Knowledge Amalgamation treats client models as teacher models that together train the server model which serves as the student model. This training approach avoids the typical multi-round training of federated learning frameworks, thus reducing the communication costs. Prototype-based Model Evaluation derives the contribution of a client by computing and comparing the prototypes of the client models and the server model, thus reducing both the computation and communication costs of contribution evaluation. Experimental results on MNIST and CIFAR10 datasets confirm the effectiveness of the proposal. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper studies participant contribution evaluation in federated learning which is an important and trending topic. 

2. The proposed techniques are intuitive and are effective as shown by experiments on two benchmark datasets.

3. The paper is well written and easy to follow overall.

Weaknesses:
While the paper proposes an interesting and empirically effective technique for improving the efficiency of participant contribution evaluation in federated learning, there is no theoretical guarantee on the effectiveness of the proposed technique in terms of measuring the true participant contribution.

Minor presentation issues: 
Missing whitespace: ""Pandey et al.[34]"", ""Zhang et al.[64]""


Limitations:
As discussed in the paper, the proposed technique relies on ""a proper and reliable validation set with accurately labeled data, which can be challenging to achieve in real-world scenarios"".

Rating:
6

Confidence:
4

";1
HwWkIwzzKF;"REVIEW 
Summary:
This work studies the CBwK problem beyond the worst case scenario, presenting two results regarding the worst-case locations and log rate respectively. The proposed algorithm utilizes a re-solving heuristic that achieves $O(1)$ under full-information and $O(\log(T))$ regret under partial-information with some regularity conditions. The worst-case guarantee is also presented.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This is a well written paper. The key ideas and main contributions are clearly presented. The proofs seems complete and easy to follow. 

Weaknesses:
This reviewer has doubts on the exact contribution of this paper. 
Firstly, the problem studied (CBwK) is well-established in the literature. And the setting that the authors choose might be not so realistic and sort of marginal. For example, one major concern from my understanding is that the budget is ""soft"", since the budget constraint is only required in expectation, and in summation. However, in reality, ""hard"" constraints should be considered prevalently, where the constraint should not be compensated, and the expectation might also be unnecessary. The assumption of null action might not be met in reality as well.

Secondly, the algorithm and the re-solving heuristic are from literature. There's something new in the proofs since extending from BwK to CBwK requires a more complicated estimation.

To add to these, there are existing work on constrained reinforcement learning, e.g. [1] that considers similar setting. Since RL is typically considered more general a setting than CB, the results there should also be at least comparable.

Lastly, the assumption for full-information seems overly strong, and the results does not seem very pertinent to the main claims. Maybe it could be better moved to the appendix to make the contribution more clear.

[1] Sobhan Miryoosefi, Chi Jin, A Simple Reward-free Approach to Constrained Reinforcement Learning

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This works considers a general setup of Contextual Bandits with Knapsacks (CBwK). The authors identify sufficient conditions under which constant of logarithmic regrets are possible (while previous authors were mainly dealing with $\sqrt{T}$ worst-case regrets). The studied algorithm is rather intuitive and is based on a sequential refinement of the underlying best static LP problem. 



Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. Identification of the conditions for $o(\sqrt{T})$ regret is a great contribution.
2. The algorithm does not need to know whether the conditions are satisfied---it is adaptive.
3. The main body exposition is rather clear, but more details would be appreciated.
4. The approach is intuitive and computationally tractable.
5. Overall I liked the proofs, they are a bit hard to read due to heavy notation, but once the reader is comfortable, the presentation is rather good.

Weaknesses:
Here are the weaknesses in no particular order

1. Bounds are in expectation (which is fine per-se, but from my experience the results are often formulated with high probability)
2. Little to no comments on the actual stopping time of the algorithm, is it much smaller than T? (I only found this information in the appendix)
3. O(1) is great, but what kind of 1 it involves is not present in the main body.
4. Regret is not the only part of the story: e.g., Agrawal and Devour 2016 allow o(1) budgets, instead of constant budgets. In this case comparing to their result is not really fair. 
5. While computationally tractable, the algorithm is not very efficient, one needs to solve a potentially huge LP problem at each step. 




Limitations:
I was not aware of the work of Chen et al 2022, but upon skimming it looks very similar to the current submission (tools and techniques-wise). A broader discussion would be appreciated.

While being honest and correct, the extension to the continuous case, is not very satisfactory (but maybe it is a matter of taste).



Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies stochastic contextual Bandits with Knapsacks. The authors provide an algorithm that guarantees regret smaller than the worst-case $\tilde O(\sqrt{T})$ in non-degenerate instances. In particular, they provides an algorithm that achieves $\tilde O(1)$ regret when the fluid LP has a unique and non-degenerate solution. Moreover, in the worst-case the algorithm maintains $\tilde O(\sqrt{T})$ regret.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The main paper is clear and well written. The problem under study is interesting. The technical results are not straightforward.

Weaknesses:
Most of the results in the paper are based on the existence of an unique and non-degenerate solution (Assumption 3.1). The formal definition of this assumption is missing. Moreover, while this assumption is commonplace in the linear programming literature, it is not clear whether it makes sanse in the bandit with knapsack problem. Finally, previous work suggests that large regret is unavailable in many standard cases.

The main paper does not give a clear intuition of the techniques used to prove the theoretical results and the appendix is difficult to follows. Moreover, the appendix is heavily based on results in other papers, e.g., Chen et al. [2022]. This makes the proofs hard to follow. Moreover, it is not clear which are the main technical contributions of the paper, and to what extend the results follow directly from the application of known techniques.

Limitations:
Yes

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of contextual bandits with knapsacks and provides an algorithm that goes beyond worst-case (i.e., provide logarithmic regret) under the mild condition of unique optimal solution and non-degenerate solution. This also simultaneously enjoys an optimal worst-case regret bound when the conditions aren't met. The paper also shows a Omega(sqrt(T)) lower-bound when the instance has a unique optimal solution and a degenerate solution.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The paper considers a significant generalization of the results in Sankararaman and Slivkins 2021, in that it extends the logarithmic regret from two resources and best-arm optimality to arbitrary resources and unique best solution.

+ The paper also proves that the new algorithm derived from resolving the program enjoys worst-case regret bounds that are optimal (in the interesting regime of B ~ O(T)).

+ The results also applies to full policy-based contextual bandit problems, as opposed to prior work with logarithmic regret bounds which only applies to the linear contextual bandits setting.

Weaknesses:
- The algorithm needs to (a) solve a program at each round (can this be removed?) unlike UCB type algorithms where the program needs to be solved only when the ucb of any single quantity changes by a constant factor and (b) even in a single round the run-time of solving the program is unclear (I might be wrong, please correct me). Although the proposed results are interesting, this seems like a major downside from the algorithm front.

- [Some rewording of comparison with prior work] The proposed algorithm is very similar to that of [Flajolet and Jaillet, 2015] (the paper mentions this, although the wording could be more generous to the prior work). Likewise, the paper mentions that the setting considered in Sankararaman and Slivkins '2021 is ""and
almost surely excludes all problem instances"" which is not technically true. Note that the prior work paper uses the same argument of perturbing LPs in the case of d=2. Thus, the key difference is extending to d > 2 and not about the number of instances. 

- Although the lower-bound presented in this paper is interesting, I think some of the lower bounds are implied from the instances presented in Sankararaman and Slivkins '2021.

---

EDIT: 

I still don't know how to implment this algorithm apart from the trivial way of having one arm per (context, arm) pair. This is important, since then the algorithm is trivial, and the results then likely follow from prior works, in particular [1] which implements this for the k-armed bandit setting. Thus, the paper needs to explain clearly why this is better than that trivial reduction, and as a result, the bounds are significantly better. I believe multiple reviewers get at this same point, and i am not able to see a  convincing argument. Please make sure to incorporate this detailed comparision in the next version of the paper.


[1] Logarithmic regret bounds for Bandits with Knapsacks - Arthur Flajolet, Patrick Jaillet

Limitations:
This is a mathematical paper and no societal impact.

Rating:
4

Confidence:
4

";0
bHS7qjLOAy;"REVIEW 
Summary:
This paper develops a Riemannian Laplace approximation, which is a Laplace approximation that takes into account the Riemannian geometry of the loss surface. The contributions of this paper are as follows: i) showing that such a loss-aware Laplace approximation is better able to capture the true posterior (and predictive); ii) presenting the Riemannian geometry framework for the Laplace approximation, with the Hessian in both the normal and tangential space; iii) a practical algorithm for efficiently integrating the required ODE; iv) experimental evidence on several commonly used datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is for the most part very well written, it is for the most part easy to follow (main idea and background). And the authors give several examples (including figures) to help the readers further.
The experimental section is decent, having both a toy example that helps gain intuition and a quantitative evaluation on several standard datasets, where the method is benchmarked against vanilla Laplace approximation and the MAP estimate. I think for the purpose of this paper, the various alternative approximate inference algorithms would not be necessary to compare to, as this is a direct extension of Laplace.

Weaknesses:
Even though the paper is generally very well written, I did, however, struggle with some parts of the main section. As a reviewer who is very familiar with Bayesian neural networks and the Laplace approximation but less so with Riemannian geometry, I would prefer to have a more in-depth background section on Riemannian geometry. I think the Laplace approximation section can be shortened if space is needed (e.g. tricks of the trade and strengths and weaknesses could be shortened).

The main weakness is the scalability of the method (see limitations). Solving the ODE takes a very long time. According to Fig. (4), this is in the order of tens of seconds for a mini-batch. Even for very small neural networks. The models used in the experimental section are tiny, e.g. single hidden layer networks with only 50 hidden units. For the CNN, the authors mention that they use 2 conv layers, but I didn't find the kernel size.

It is also not clear to me if and which approximation the authors use for the Hessian. Can the complexity be reduced e.g. with a diagonal Hessian approximation? How would this compare to a diagonal or Kronecker-Factorised approximation of the standard LA?

Limitations:
The method has been applied to very small neural networks and the authors argue that this is because the method is computationally very expensive. While the authors acknowledge this in the limitations, it potentially limits the applicability to realistic neural network model sizes. It would be important to know more precisely what the computational complexity is (also as a function of the model size). Furthermore, I would like to see a comparison in wall clock time for the entire approximation compared to standard Laplace approximation, not only for different mini-batch sizes, but also for different model sizes, including large model sizes.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents a Laplace approximation for Bayesian neural networks that adapts the covariance to the local geometry of the loss, effectively overcoming the quadratic approximation of the loss. The authors report competitive performance with the standard Laplace approximation (both Monte Carlo sampled and linearized) on regression and MNIST-scale classification problems as well as a reduced reliance on tuning the precision of the prior.

The approach is explained clearly and makes a lot of sense (at least with my rather superficial understanding of Riemannian geometry), it is applicable in more general probabilistic models and I would expect it to lead to various follow-up works. While methodologically this is a very nice paper, I feel like it is let down by the empirical evaluation. The method is only tested on UCI and MNIST-scale datasets, which are hardly relevant for deep learning these days. The authors mention the computational cost of their method, but only discuss the reasons superficially without providing exact benchmark figures to give a sense of where the main bottlenecks arise. Given the apparent computational limitations of the method, an experiment with a non-NN model could have strengthened the paper.

All things considered, the strong methodological contribution outweighs the unconvincing empirical evaluation for me, so I would lean towards acceptance, although I wish could have given the paper a much higher rating.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* The core idea makes a lot of sense, is applicable beyond inference in neural networks and seems to work well for the experiments that are considered. It has the potential to address the rather restrictive approximation of a quadratic loss in the Laplace approximation.
* I am confident that the paper will inspire various pieces of follow-up work.
* The paper is well-structured and -written.
* Effective use of illustrative examples throughout.

Weaknesses:
* Only small scale problems are considered in the experiments
* This is exacerbated by lack of analysis of the computational cost. It is not really clear to me what specifically is preventing the method from being applied to larger networks and datasets (even something like CIFAR with ResNets would have been great). The discussion mentions scaling issues w.r.t. number of datapoints and parameters, but the relative behavior is not benchmarked at all and it is also not clear to me how much time the ODE solver actually spends e.g. calculating the Hessian-vector products vs computations independent of that. Given that this paper opens up a new direction for research, I think there should be much clearer pointers as to where exactly the current bottlenecks and limitations are and where improvements can realistically be achieved.
* There is no experiment demonstrating the efficacy of the approach on a non-NN probabilistic model. Given the apparent computational cost of the method, this would have seemed like a rather natural experiment to include and neural networks don’t seem like a particularly good fit for the approach.
* The explanation of why the method would work better with a mini-batch rather than the full dataset (section 3.3/fig 4) is not exactly clear and seems rather hand-wavy.
* Lack of HMC ground-truth baselines for the regression problems

Limitations:
Only discussed superficially and in a quite hand-wavy manner, I would have wanted to see concrete benchmarks and an analysis of how the compute time evolves with increasing dataset size and number of parameters respectively as well as some evidence from the literature that a tailor-made ODE solver could indeed allow the method to take the step from small conv nets to more modern architectures.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a novel Laplace Approximation for Bayesian Neural Networks. A key insight is to examine the local loss landscape with a Riemannian metric, which is determined by the gradient of the log posterior. Using this metric and an exponential map, a Laplace Approximation technique is developed to draw posterior samples that fall into regimes with low negative log posterior. The paper also develops a sampling method, which relies on the 2nd order ODE solver. Several experiments are conducted. When compared to the standard Laplace Approximation, evidences are provided to illustrate the improvements. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- the contribution provided by this work is original and novel to the best of my knowledge.

- the paper is polished well generally. Despite that the materials are developed on differential geometry, intuitions are relatively provided well.

- Laplace Approximation has been increasingly popular in recent years. Such extensions to incorporate Riemannian geometry could be relevant to the Bayesian Deep Learning community.

Weaknesses:
One complaint about the paper is that, without referring to the appendix, it is difficult to comprehend the material fully. For example, in section 3.3, I wish that the connection between an ODE and Riemannian metric is difficult to understand directly. Differential geometry is not often thought in engineering courses at many universities. It may make sense to recap the essential concept in the main paper. How the method could be used for linearized Laplace Approximation is made very short.

Another point for improvement is the choice of the baselines. It would make sense to include a deep ensemble and MC-dropout as a minimum. This could show how far the proposed Laplace Approximation can compete with popular methods in practice.

In the experiments, the paper could improve on analyzing the computational complexity, in comparison to the standard Laplace Approximation. DaxBerger et al 2021 claim that the major benefit of Laplace Approximation is on simplicity, it would be great to project this paper's method more on the plateau of quality of uncertainty Vs computational complexity. While it is great for research, I think the methods based on differential geometry may have certain drawbacks among practitioners. The paper could be more convincing by providing the gains in uncertainty, but additional overhead due to the added complexity of the pipeline.

Limitations:
There is a limitation section at the end of the paper.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The Laplace approximation offers a practical posterior but is limited due to the symmetry of the weight space it is parameterised in. The method proposed to improve posterior quality by adapting the posterior shape through a Riemannian metric that is determined by the log-posterior gradient. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Practically

The community is interested in using Laplace approximations for Bayesian posterior estimation due to its simplicity and wide applicability. The proposed paper deals with improving the quality of such approximations, which would benefit many.

* Methodologically

Since the quality of the Laplace approximation is inherent to the space it is parameterised in, it is very interesting to see how Riemmanian geometry of the loss landscapes provides a better understanding of the approximations and provides avenues to improve posterior quality.

* Results

It is very promising to see that the Riemannian Laplace approximation allows for good posterior fits even when no linearization or prior tuning is being used.

Weaknesses:
Overall, I think the promise of the paper of using the Riemmannian geometry of the loss surface to improve the posterior quality of Laplace approximation is very promising. My concerns mainly lie in the practicality of the approach in terms of scaling the method to larger models (deeper networks and models with more parameters) and larger datasets. Since the paper presents the method in the context of Bayesian deep learning, the scalability of the method is important.

- Scalability to deeper networks
The posterior samples from 'POSTERIOR - TWO LAYERS MODEL' of Fig. D.4 seem off. It would be good if the authors addresses how the method would perform for more complex model classes. Does the method break down in this case, or could this potentially be mitigated?

- Scalability to models with more parameters

As mentioned in the paper, there is a computational cost associated with the growing dimensionality of the parameter space because the number of necessary solver steps increases. I am worried that the method can not be applied to larger deeper NNs as the solutions that are found by ODE solver in practice for larger dimensions will not be of sufficient quality. Since deep neural networks typically consist of many more parameters than the models, this seems like a very big limitation.

- Scalability to larger datasets

The linear scaling in the number of data points.

- Quantitative results and comparisons

The method only considers very small models (e.g. single or 2 layer NNs) and small toy problem datasets. This small data regime would allow computing of (close to) exact posteriors, which would allow better quantitative assessment of the posterior found by Riemmanian Laplace approximation. Furthermore, it would be interesting to see larger model and data regimes.

Limitations:
As mentioned, I think being able to adapt the quality of Laplace approximations by considering geometrical aspects of the loss landscape is very interesting. My concerns are mainly in practicality of the approach.

Rating:
5

Confidence:
3

";1
q0sdoFIfNg;"REVIEW 
Summary:
The paper proposes the SPQR loss to regularize the independence of the Q-network ensemble. The authors apply the random matrix theory and a spiked random model to derive the KL loss between Wigner’s semicircle distribution and the empirical spectral density of eigenvalues. The authors show that the independence could be enforced by minimizing this KL divergence loss with theoretical guarantees by satisfying the testing hypothesis. Extensive experiments on both the online and offline RL tasks and several baselines demonstrate that SPQR could improve the performance of the current ensemble RL algorithms promisingly by increasing of independence of the ensemble networks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.	This paper is well-organized and well-written.
2.	The proposed SPQR is with a simple format and shows a promising improvement over the ensemble RL methods such as SAC-ens and REDQ.
3.	The authors build the SPQR loss with mathematical tools and provide the theistical guarantee to ensure the independence of the ensembled Q-networks.
4.	Experiments on both the online and offline RL tasks are sufficient to validate the generality of the proposed SPQR and that it can improve the performance of ensemble RL methods.


Weaknesses:
1.	Detailed explanations between the used math tools and the proposed method are missing, making the understanding hard for the readers. For example, the connection between the random matrix theory and a spiked random model is not explained well. What is the purpose of the use of a spiked random model in SPQR?
2.	There lack details of in Algorithm 1. For example, how to construct the symmetric Q-matrix is not shown. The detailed version of the core algorithm is given in the appendix, which is strange.
3.	The quality of the figures needs to be improved. For example, the text size is too small.
4.	There are many typos.

a)	In Line 73, “tp” should be “to”.

b)	In Line 81, “SPQR analysis Q-ensemble independence …”.

c)	In Line 113, “Q-learning, We plot”.

d)	In Line 206, “Proof of Theorem 4.1 are …”.


Limitations:
NA.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper deals with the problem of alleviating overestimation bias in RL, using ensembles Q-functions. The authors argue that previous methods do not provide a theoretical guarantee of the independence of the members of the ensemble. To provide this they propose an approach based on random matrix theory, which, in practice, can be implemented as a regularization loss. 

The approach is evaluated in a number of settings for RL in tasks in MuJoCo, D4RL Gym, Franka Kitchen and Antmaze. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:

+ The approach has a strong, non-trivial theoretical foundation.
+ Experimental results show a significant improvement over the baselines. 

Weaknesses:
- Throughout the paper, there are many grammatical errors and awkward formulations. Some of these might hinder understanding, such as on page 5: 
""high correlation occurs, which cannot be benefited by the ensemble method"". I assume it this case what the authors mean is ""benefit the ensemble method"". 
Many of these could be easily fixed with a grammar checker. 

- The text on figures 5 and 6 (and to a lesser degree, figure 4) are unreadably small. Graphs should be redrawn in such a way that the text is still readable at a normal printing size. 

Limitations:
No separate limitations section found in the paper. 

The theoretical nature of the paper does not raise ethical concerns. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a new regularization loss that improves the independence of Q ensemble, thus improving the performance on online and offline DRL settings. The authors first point out that previous works with Q ensemble either rely on assumptions that are inaccurate in practice, or rely on heuristics to improve diversity of networks and no theoretical support. The authors provide theoretical results that support the proposed method, and also conducted empirical experiments, where the proposed method is applied to multiple recent online and offline algorithms, and tested on the MuJoCo online and d4rl offline benchmarks. On a high level, the regularization (Spiked Wishart Q-ensemble independence reuglarization (SPQR)) encourages the Q ensemble distribution to be closer to an ideal independent ensemble, resulting in a more diversified Q prediction values, lower bias and better performance. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
**originality**
- The theoretical results and the proposed algorithm, the findings can be seen as novel
- Learning with Q ensembles has been studied in many previous works, but as the authors pointed out, they either rely on assumptions that are inaccurate in practice, or rely on heuristics, so the novelty is quite good here. 

**quality**
- Overall the presentation is good, writing is clear

**Clarity**
- The motivation, connection to related works are all quite clear, and ample technical details are given, the authors also explained that they try to be fair in the comparisons and use the same hyperparameters. And the authors explained computation time, performance improvement, implementation difficulty quite clearly. 

**significance**
- the analysis is nice, which shows that the proposed method seems to achieve the desired improved independence. 
- empirical results showing consistent performance improvement over the baseline, big improvement in some settings, slight improvements in others, but overall quite consistent. Consistent improvement over recent sota algorithms such as REDQ with small change in the code and low computation cost and no excessive fine-tuning is quite impressive.  
- the theoretical part is interesting and quite important 

Weaknesses:
I don't have very major concerns, one thing that can be fixed is the text in all your figures, especially those in the main paper are just too small, please make the fontsize of legend, as well as axis labels and ticks and any other text in the figures bigger so they are easier to read. 

Minor issue: 
- line 81: SPQR analysis -> analyzes ?
- Figure 3 caption can you add a short sentence on what does beta do? It is not explained until a later section and I got confused when reading to Figure 3. 

Limitations:
The proposed method seems to be quite general and can be applied to other methods based on Q ensembles. However can be good to have some discussion on limitations in the paper. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work proposes a spiked Wishart Q-ensemble independence regularization (SPQR) to improve the independence of ensembling in Q-learning. SPQR encourages the ensemble to be closer to an ideal independent ensemble by penalizing the KL divergence between the eigenvalue distribution of the current ensemble and an ideal one.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is easy to follow. The paper provides nice evidence of lack of independence in Q-ensemble training in current methods. The regularization to increase the diversity is well motivated, and seems simple and practical. The empirical evaluation is comprehensive, though more baselines would make them more compelling.

Weaknesses:
There is quite a bit of prior work on ensembling in deep RL, for example bootstrapped DQN [1] and MSG [2], which are not discussed. In particular, [2] provides theoretical and empirical support for constructing independent ensembles. A discussion and comparison with MSG is warranted given the emphasis on constructing independent ensembles.

While the emphasis of work is on improving ensembling methods in deep RL, recent methods have been able to forego ensembling while maintaining or improving performance over REDQ, for example DroQ [3] or RLPD [4]. It would be good to contextualize the improvements from SPQR by adding comparisons with these methods.

More comparisons with offline RL methods would help too, for example, IQL.

[1] Deep Exploration via Bootstrapped DQN. Osband et al. 
[2] Why So Pessimistic? Estimating Uncertainties for Offline RL through Ensembles, and Why Their Independence Matters. Ghasemipour et al. NeurIPS, 2022.
[3] Dropout Q-Functions for Doubly Efficient Reinforcement Learning. Hiraoka et al.
[4] Efficient Online Reinforcement Learning with Offline Data. Ball et al.


Limitations:
While I do not see potential negative societal impact, a discussion on limitations is missing from the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
Mitigating overestimation bias is crucial for deep reinforcement learning. Existing works about the ensemble techniques for Q-learning have been explored to leverage the diversity of multiple Q-functions. The authors argue that there has been no attempt to ensure ensemble independence from a theoretical standpoint. The authors introduce a regularization loss for Q-ensemble independence, based on random matrix theory, called Spiked Wishart Q-ensemble Independence Regularization (SPQR). The authors incorporate SPQR into online and offline ensemble Q-learning algorithms. Experimental results show that SPQR surpasses baseline algorithms in both online and offline RL benchmarks, demonstrating its effectiveness in addressing overestimation bias and improving performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Significance and Originality: The problem and viewpoint that the paper studies - how to address overestimation bias/out-of-distribution error in RL is an important problem, and there have been a number of works investigating how to tackle this problem based on ensemble learning. However, most of the methods assume the bias follows a uniform and independent distribution, which may not hold in practice. This paper aims to improve previous ensemble methods from this perspective with a theoretical guarantee of improved Q-ensemble independence. The viewpoint from the random matrix theory seems novel, and the authors also propose a practical and tractable implementation for the method, which makes it possible to be applied in standard high-dimensional tasks.

- Quality: The authors have done a comprehensive experiments to evaluate the methods in different setupds, and the proposed SPQR methods provides improvements in different aspects.

- Clarity: The paper is also well-written and easy to follow.



Weaknesses:
- It is claimed in the paper that the assumption that most previous works rely on may not be true (the i.i.d. assumption about the bias). Could the authors demonstrate this effect is generally invalid in most of the environments? It would be better to convince the readers (since previous methods also have great performance although this assumption may not hold in practice).

- Results in Figure 5 about the validation for the independence analysis are interesting. Does this generally hold in different tasks?

- The improvement in standard D4RL locomotion tasks is somewhat marginal (4% improvement) considering increasing computation costs. 

- The results in Figure 6 about the performance of SAC-Ens is very low. Could authors better explain this result?

Limitations:
No.

Rating:
6

Confidence:
3

";1
DEqiM9CmmZ;"REVIEW 
Summary:
This paper introduces the ANQ (Approximate Nearest Neighbor Q-Learning) framework, which aims to provide explainability in reinforcement learning models. ANQ combines neural networks for high performance and memory-based structures for explainability, offering a promising solution for domains like autonomous driving, quantitative trading, and healthcare. The paper discusses the challenges of explainability in reinforcement learning and how ANQ addresses them. It also presents the Sim-Encoder contrastive learning used in ANQ's state representation and provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. Overall, the paper presents a novel approach to reinforcement learning that balances performance and transparency, making it suitable for real-world applications. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Some strengths of this paper include:  

1. Novelty: The ANQ framework is a novel approach to reinforcement learning that combines neural networks and memory-based structures to provide explainability in decision-making processes. This approach is different from traditional reinforcement learning methods that focus solely on performance.  

2. Explainability: The paper addresses the challenge of explainability in reinforcement learning models, which is crucial for real-world applications where transparency and trustworthiness are essential. ANQ's ""data is policy"" design principle ensures that the model's decisions are explainable and interpretable.  

3. Sim-Encoder contrastive learning: The paper introduces the Sim-Encoder contrastive learning approach for state representation, which demonstrates its effectiveness in memory retrieval learning tasks. This approach enhances ANQ's performance and explainability.  

4. Evaluations: The paper provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. The results show that ANQ outperforms traditional reinforcement learning methods while maintaining explainability.  

5. Real-world applications: The paper highlights the potential of ANQ in domains like autonomous driving, quantitative trading, and healthcare, making it suitable for real-world applications. 

Weaknesses:
Some potential weaknesses of this paper include:  

1. Despite the advantage of interpretability, the performance of this framework is still far from that of SOTA RL algorithm. I think it would be better if this framework could have similar performance to SOTA RL method. 

2. Lack of real-world case studies: Although the paper highlights the potential of ANQ in various domains, it does not provide specific real-world case studies or examples to demonstrate the practical application and effectiveness of the framework. Including such case studies would strengthen the paper's claims and provide more concrete evidence of ANQ's utility in real-world scenarios. 

3. No comparison was made with other interpretable reinforcement learning algorithms. For example, some Neuro-Symbolic Search methods. 

3. The presentation of this paper could be better, for example, Figure 3 could be larger and clearer. 

Limitations:
Incorporation of latest techniques: The paper mentions that ANQ has not yet incorporated the latest techniques, such as maximum entropy learning from SAC and other contrastive learning methods for representation learning. While the paper acknowledges that these refinements will be addressed in future work, the absence of these techniques in the current implementation may limit the overall performance and effectiveness of ANQ.  

Rating:
5

Confidence:
2

REVIEW 
Summary:
The submission creates a framework called Approximate Nearest Neighbor Q-Learning (ANQ). ANQ uses a sim-encoder contrastive learning and approximate nearest neighbor search to find which states are similar. Utilizing this approach, they can use it to find similar states in aiding for the decision the framework has made. They showcase their performance by performing experiments on MuJoCo with continuous control tasks. The exact environments experimented were Walker2d, Ant, HalfCheetah, and Hopper. There is also an ablation study conducted to show the benefit of the sim-encoder and contrastive learning that aids this framework. They provide a component called Explainable Action to show why it executed a particular action based on the state.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Significance:
Good ablation experiments to showcase the benefit of the sim-encoder. The results make it convincing that for AQL, the sim-encoder is quite beneficial. Plus you did this ablation among 4 environments.

Originality:
The explainable action is an interesting piece that can provide a good impact. By searching for similar states and can provide the explanations as to why the decision was made.

Weaknesses:
Clarity:
Confusion, in Section 4.1, you mention that the algorithms included were SAC, PPO, and TRPO, then in the next paragraph you mention A2C. Consider in the first paragraph to mention the exact algorithms you compare because in the next paragraph, you mention an algorithm that was not discussed exactly in the previous paragraph. In Figure 3, you show TD3-1M, ARS-75M which were not discussed so please in Section 4.1 to denote exactly.

With the figures, please provide more with the caption like a summarization or a sentence to showcase why it is important. It can help the reader if they have not read the parts within the main text.

Significance:
The approach can be on par with one or two deep RL approaches. Consider to improve the performance to have bigger impact. Usually for methods that are explainable there is a drop in performance so others may be hesitant to use it due to the performance drop.



Limitations:
They do address the performance drop compared to the deep RL models. No negative societal impacts.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes a memory-based Q-learning algorithm aimed at better explainability. The method extends the prior work ""Episodic Control"" and enables learning with continuous action space.

This work provides 2 main contributions:
1. a one-step-away contrastive-learning objective to learn embeddings from states.
2. modified policy evaluation and improvement rules to account for the continuous action space.

The authors show empirical results to support their design choices:
1. The proposed algorithm is able to achieve some meaningful learning in 4 continuous control tasks.
2. The question-answering example shows that the method is able to find the nearest-neighbor states and their Q values to explain a chosen action.
3. The ablation study on the embedding module shows that it is necessary to learn dynamics-aware embedding.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The proposed method can learn in environments with continuous state and action spaces, making it ubiquitously applicable to real-world applications.
2. The algorithm and experiment settings are clear.
3. The ablation study on the embedding module justifies the contrastive learning objective.

Weaknesses:
1. The main weakness of this work is in the experiment results. In the Mujoco tasks, the performance hovers around the weakest baseline among all compared methods. For Ant, HalfCheetah, and Hopper, the policies also appear to have converged to suboptimal ones. This could have been caused by insufficient exploration.

2. The algorithm is only demonstrated in state-based environments, whereas the prior work ""Episodic Control"" can work with image observations. Contrastive learning has been shown to be useful for learning good feature extractors for images. The results would have been much more convincing if they were from vision-based tasks.

3. The notations are not fully explained. For example, $k$ and $e_t$ both exist in the dataset but the authors say that they use embeddings as keys. Also, $R$ in Equation 1 is not introduced.

4. Finding out the nearest neighbors and printing out their Q values is not a convincing way to explain the chosen actions because the Q values are computed in expectation. One can arguably explain a neural policy in a similar way, by sampling a few different actions and printing out their Q values.

Limitations:
The authors are upfront about the limitation in task performance and provide viable options for improvement. I would also encourage the authors to try their method on image-based tasks.

Rating:
3

Confidence:
4

REVIEW 
Summary:
Instead of using deep neural networks to approximate Q functions as it's done in deep RL methods, the paper investigates the potential of using nearest neighbor methods to approximate Q functions. They used contrast learning with a Sim encoder. They argue that such a method is more explainable than the ones with deep neural nets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The illustration is the proposed method is clear.

Weaknesses:
* The proposed nearest neighbor might be useful and efficient for low-dimensional domains like Mujoco. I doubt its effectiveness when it goes to high-dimensional domains like video games;

* I don't see why the nearest neighbor method is more explainable;

* The proposed method lacks proper baselines;

* The overall presentation needs improvements.

Limitations:
NA

Rating:
3

Confidence:
4

";0
nJFJcgjnGo;"REVIEW 
Summary:
This paper proposes a benchmark for evaluating model robustness against structural distributional shifts. The authors propose three different metrics to split the datasets for the evaluation. In the experiments, the authors evaluate different models under the proposed distributional shifts and show that they can be quite challenging for existing graph models.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper targets an understudied problem of evaluating model robustness and uncertainty estimation under structural distribution shift.

2. It presents a novel idea of using different evaluation metrics to create data splits for structural distribution shifts.

3. The authors evaluate different methods based on the proposed metrics and show some empirical findings.

Weaknesses:
1. The authors consider less important nodes are OOD nodes. However, real-world graphs follow the power-law distribution and only have a small number of nodes that have a high degree/node importance. So the proposed metric is kind of counterintuitive to me.

2. The experimental setup, particularly the equal splitting of in-distribution (ID) and out-of-distribution (OOD) data, raises concerns about the generalizability of the findings. Real-world scenarios typically involve a smaller proportion of OOD data, and investigating varying ratios of ID to OOD data would enhance the practical relevance of the evaluation.

3. Since the authors argue the existing methods only focus on feature distribution shift, while this paper only focuses on the structural distribution shift. It would be better to consider them together rather than separately evaluate them, as they often occur simultaneously and have interdependencies.

4. While the proposed metrics for creating data splits are new, the authors could provide more detailed justification and analysis for their choices. It would be beneficial to explore the sensitivity of the evaluation results to different metric configurations.

5. I have a concern that should this paper be submitted to the datasets and benchmark datasets since this paper doesn't focus on technical contribution. 

To strengthen the paper, it would be beneficial to address these additional points and provide more comprehensive justifications, analyses, and experiments.

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose three domain selections, namely, popularity, locality, and density to equip node prediction datasets with structural distribution shifts. Extensive experiments are conducted to compare present OOD generalization methods and OOD detection methods.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
S1: The proposed distribution shifts are novel.

S2: The paper is well written and easy-to-follow.

S3: The evaluations are clear and sufficient.

Weaknesses:
W1: The title is misleading. Since this benchmark only focuses on node prediction tasks, the title should reflect it. As we know, structural distribution shifts are different in graph-level and node-level tasks.

W2: The chosen shifts are somehow limited because they are not extracted from real-world scenarios.

W3: Line 39: ""Also, the splitting strategies in GOOD are mainly based on the node features and do not take into account the graph structure"" is somehow misleading. I suggest the authors to clarify that ""GOOD mainly focuses on node feature shifts in **node-level tasks**"".


Limitations:
Broader impacts are expected to be discussed.
The licenses of the datasets and the code (GOOD) are expected to be mentioned.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper provides a benchmark solution for producing diverse and meaningful distributional shifts from existing graph datasets. The experiments conducted demonstrate that the proposed distributional shifts can be difficult for existing graph models, and surprisingly, simpler models often outperform more sophisticated ones. The experiments also reveal a trade-off between the quality of learned representations for the base classification task and the ability to distinguish nodes from different distributions using these representations.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed data-splitting approach is quite interesting. The overall presentation is quite easy to follow.  
Based on the provided data-splitting strategy, most graph/node classification methods may not achieve a good classification performance and uncertainty quantification quality. It is also nice to see the authors conduct many empirical studies to validate the idea and compare the difference between various baselines. 

Weaknesses:
1. The technical novelty is somewhat limited. It seems like the technical contribution is based on existing techniques (PageRank, PPR, ...). I would like to see more discussions of the rationale/motivation for choosing the specific metric. For example, the PageRank is selected because of the need to quantify the node popularity, but other node centrality-based metrics can also be used to quantify the node popularity.   
2. The dataset size is also quite small. It's not promising the experience learned from these small networks can be extended to million-scale networks.
3. Some important baselines (e.g., GOOD: A Graph Out-of-Distribution Benchmark) are not included in the main context. 

I feel like this paper should be more suitable for the dataset and benchmark track.

Limitations:
 I am not seeing limitations of this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a general approach for inducing diverse distributional shifts based on graph structure and evaluates the robustness and uncertainty of graph models under these shifts. The authors define several types of distributional shifts based on graph characteristics, such as popularity and locality, and show that these shifts can be quite challenging for existing graph models. They also find that simple models often outperform more sophisticated methods on these challenging shifts. Additionally, the authors explore the trade-offs between the quality of learned representations for the base classification task and the ability to separate nodes under structural distributional shift. Overall, the paper's contributions include a novel approach for creating diverse and challenging distributional shifts for graph datasets, a thorough evaluation of the proposed shifts, and insights into the trade-offs between representation quality and shift detection.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: The paper's approach for inducing diverse distributional shifts based on graph structure is novel and fills a gap in the existing literature, which has mainly focused on node features. The authors' proposed shifts are also motivated by real-world scenarios and are synthetically generated, making them a valuable resource for evaluating the robustness and uncertainty of graph models.

Quality: The paper's methodology is rigorous and well-designed, with clear explanations of the proposed shifts and the evaluation metrics used. The authors also provide extensive experimental results that demonstrate the effectiveness of their approach and highlight the challenges that arise when evaluating graph models under distributional shifts.

Clarity: The paper is well-written and easy to follow, with clear explanations of the proposed shifts and the evaluation methodology. The authors also provide helpful visualizations and examples to illustrate their points.

Significance: The paper's contributions are significant and have implications for the development of more robust and reliable decision-making systems based on machine learning. The authors' approach for inducing diverse distributional shifts based on graph structure can be applied to any dataset, making it a valuable resource for researchers and practitioners working on graph learning problems. The insights into the trade-offs between representation quality and shift detection are also important for understanding the limitations of existing graph models and developing more effective ones. Overall, the paper's contributions have the potential to advance the field of graph learning and improve the reliability of machine learning systems.

Weaknesses:
Limited scope: The paper focuses solely on node-level problems of graph learning and does not consider other types of graph problems, such as link prediction or graph classification. This limited scope may restrict the generalizability of the proposed approach and its applicability to other types of graph problems.

Synthetic shifts: While the authors' proposed shifts are motivated by real-world scenarios, they are synthetically generated, which may limit their ability to capture the full complexity of real distributional shifts. The paper acknowledges this limitation, but it is still worth noting that the proposed shifts may not fully reflect the challenges that arise in real-world scenarios.

Limited comparison to existing methods: While the paper provides extensive experimental results that demonstrate the effectiveness of the proposed approach, it does not compare the proposed approach to other existing methods for evaluating graph models under distributional shifts. This limits the ability to assess the relative strengths and weaknesses of the proposed approach compared to other approaches.

Limitations:
The paper briefly acknowledges some of the limitations of the proposed approach, such as the fact that the synthetic shifts may not fully reflect the complexity of real-world distributional shifts. However, the paper does not provide a detailed discussion of the potential negative societal impact of the work.

Given the technical nature of the paper, it is possible that the authors did not see a direct connection between their work and potential negative societal impacts. However, it is always important for authors to consider the broader implications of their work, especially in fields like machine learning where there is a growing awareness of the potential risks and harms associated with these technologies.

In future work, the authors could consider providing a more detailed discussion of the potential societal impacts of their work, including any ethical or social considerations that may arise from the use of their approach. This could help to ensure that the work is being developed and applied in a responsible and ethical manner.

Rating:
7

Confidence:
5

";1
XsZ5YebcCz;"REVIEW 
Summary:
The paper investigates the problem of policy constraints in offline reinforcement learning (RL) settings, and finds the phenomenon that milder constraints on policies during training can lead to better performances at inference tests. The proposed component MCEP can be added on existing algorithms including TD3BC and AWAC. Experiments on D4RL dataset show improved performances over vanilla TD3BC and AWAC.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The strength of policy constraints in offline RL is an important problem. It is a novel perspective to separate the policies for value estimation and inference with different constraint levels. I would say the proposition that milder constraints can improve policy inference performances is an interesting problem. The experiments are thorough with necessary ablation studies, and the results indeed show some improvement by using the proposed constraining method.

Weaknesses:
I think one major critique of the paper is: the most essential discovery that milder constraints may be required for test-time inference is mostly from experimental evaluations. The observations are not even consistent for that only 6 out of 9 shows this pattern. This is not strong evidence showing that milder constraints are necessarily always better. Some theoretical analysis or at least insights about this observation can be provided to make it more convincing.

Another critique is that although the experiments show some improvement by using MCEP on TD3BC and AWAC and over some baselines like CQL and IQL. These are not the SOTA results on these offline datasets, there exists better algorithms proposed by the time of NeurIPS submission that should be aware of:

[1] Hansen-Estruch, Philippe, et al. ""Idql: Implicit q-learning as an actor-critic method with diffusion policies."" arXiv preprint arXiv:2304.10573 (2023).

[2] Garg, Divyansh, et al. ""Extreme Q-Learning: MaxEnt RL without Entropy."" arXiv preprint arXiv:2301.02328 (2023).

[3]  Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. ""Diffusion policies as an expressive policy class for offline reinforcement learning."" arXiv preprint arXiv:2208.06193 (2022).

The paper writing can be further improved.

Limitations:
The limitations of the current method are discussed, that the evaluation policies in MCEP may not be consistent with the value function and can lead to unstable value estimation if used in policy evaluation.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Mildly Constrained Evaluation Policy (MCEP) for offline reinforcement learning to address the issue of excessively restrictive constraints for action selection during test time inference. MCEP uses a more constrained target policy for value estimation and another less restrictive policy for performance evaluation. Empirical results demonstrate the effectiveness of MCEP.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
MCEP is easy to implement and can be plugged into many policy constraint offline RL methods.

Weaknesses:
Since $\pi_e$ does not participate in the policy evaluation, I think line 7 of Algorithm 1 can be removed and $\pi_e$ can be extracted from Q after actor critic learning to save computational cost. The contribution of MCEP is only to extract a less restrictive policy after RL learning, which is somewhat limited.

The overall idea of the paper is quite simple. However, the notations and descriptions are a bit confusing. For example, the notations in Algorithm 1 lack a clear definition ($\psi, \phi, \tilde \pi, \pi^e, \tilde w, w^e, \mathcal L(.,.)$). And $\psi$ and $\phi$ in line 6 and 7 of Algorithm 1 are reversed, since Q evaluation in Equation 2 is associated with $\phi$. 

No theory supports MCEP in the paper.

Limitations:
More hyperparameters need to be tuned for MCEP compared with the original algorithm.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This work addresses the issue of excessive policy constraints in stabilizing value estimation within the offline RL paradigm. A separate target policy is used solely for evaluation and stabilizing value estimation, which is more constrained than the ""evaluation policy."" The evaluation policy does not participate in policy evaluation and is improved by the value function estimates, with the level of constraint adjusted by the weight of the term.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Major points:
- This procedure can be easily integrated into offline RL algorithms that utilize policy constraints, and empirical results apply it to TD3+BC and AWAC. The empirical findings demonstrate promising improvements, with baselines encompassing the standard suite of state-of-the-art offline RL algorithms.
- The paper is well-written, easy to comprehend, and thoughtfully structured. 
- The idea itself is intuitive, and the toy experiments convincingly demonstrate that over-constraint poses a significant issue. Figure 2 clearly illustrates the adverse effects of over-constraint, with the policy performing poorly in low state value regions of the maze.
- The ablation studies are extensive and demonstrate the method's effectiveness.

I believe this simple yet intuitive method is worth presenting to the broader offline RL community. I believe this work should be accepted.

Weaknesses:
Major points:
- While the results show promise, they do not indicate substantial improvements across many environments, and there is some inconsistency observed. The method shows a decrease in performance in the medium-expert D4RL tasks and the pen task.

Limitations:
Yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
Offline reinforcement learning (RL) methods frequently involve a policy constraint to mitigate error propagation when learning the Q function. Generally, a single constraint strength is used throughout training. This paper proposes instead to use different constraint strengths for learning the target policy, which is only used for learning the Q function, and the evaluation policy, which is the final policy returned by the algorithm. In particular, a stronger constraint is needed to ensure stability when training the target policy, but weakening the constraint for the evaluation policy can lead to better performance.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
* The idea is fairly general and can be instantiated with various RL algorithms, as shown in the paper.
* The experimental results provide insight into the role of the constraint and the tradeoff between stability and performance.
* Conceptually, the approach allows for a continuum of algorithms between one-step RL and standard actor-critic methods.
* The paper is clearly written and understandable.

Weaknesses:
* The algorithm introduces an additional hyperparameter that requires tuning, which is already a challenge in offline RL.
* The paper found that “in 6 out of the 9 tasks, the $\alpha$ for better inference performance is higher than the $\alpha$ that enables safe Q estimates”. While 6/9 is a majority, this is not convincing evidence that weakening the constraints is always helpful.

Limitations:
Yes, limitations are addressed.

Rating:
6

Confidence:
5

";0
eW233GDOpm;"REVIEW 
Summary:
The authors propose a simple method to perceive the length of an LLM response by asking the LLM. Then, the authors propose to groups queries with similar response lengths into micro-batches, which are then allocated to different GPU nodes and processed in parallel . The authors show empirical gain in terms of throughput. This approach is also orthogonal to other inference acceleration approaches. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed method is simple and exact and does not sacrifice response quality while achieving speedup. 

2. The authors show impressive speedup in terms of throughput.

3. The speed up the author achieve can be applied on top of other inference acceleration techniques.

Weaknesses:
1. One fundamental weakness of the paper lies in the assumption that requests can be reordered, which may not hold in production. The author did not show any fairness metric which may help readers understand how their method affect each individual request in practice.

2. The author did not show any fine-grained ablation studies examining how often and how often the requests have been reordered and how it affects inference latency.

I would happily raise my rating if the authors can present more thorough ablation studies. 



Limitations:
The authors adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
>**Rebuttal:** The provided details satisfy my concerns. I think this paper should be accepted after applying the agreed changes.

>**TL;DR:** The paper presents a new technique to reduce the inference time of LLMs under intensive usage. This is an important problem that can reduce wasteful computations. However, the paper is missing some key comparisons and the experimental methodology is lacking justifications. Addressing my concerns and questions would improve my score.

This paper proposes a new to reduce the inference time of LLMs under intensive usage. The technique save wasteful computations by predicting the response length and aggregating similar predicted lengths together. The paper presents an inference pipeline, which is composed of response length prediction, failure collection and recomputation (FCR), and variable batch size (VBS).

Experimental results on real-world instruction datasets using the Vicuna-7B model demonstrated an 86% improvement in throughput without sacrificing performance quality. The datasets include the Instruction-in-Wild and Alpaca. The proposed technique is compared to previous works and outperforms them on both datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* **S.1.** The proposed technique can reduce inference time of LLMs and gain a 86% performance improvement.
* **S.2.** The paper tackles an important problem of reducing the LLM inference time and wasteful computations.
* **S.3.** The proposed technique outperforms previous compared works on two datasets.
* **S.4.** Reproduction code is provided as part of the submission.

Weaknesses:
* **W.1.** Th paper lacks comparison to existing LLM inference works such as [1][2].
* **W.2.** Some the key technique attributes are not well justified. For example, the target length prediction is four times the actual length. No experiments or ablations are provided to justify the ""four"". Another example is the FCR mechanism, which immediately stops the generation process when the maximum predicted length has been surpassed. There might cases where the generation process might only need a few more tokens to complete the task.
* **W.3.** The proposed technique relies on very short inputs in order to be effective. This is rarely the case for chat-bot, which require support for multi-turn conversations and are given as the examples for LLM usage. Furthermore, the proposed technique relies on high LLm usage to create batches of similar predicted lengths. This high usage is typically found in chat-bots.


[1] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. ""Orca: A distributed serving system for {Transformer-Based} generative models."" In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 521-538. 2022.

[2] Pope, Reiner, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. ""Efficiently scaling transformer inference."" Proceedings of Machine Learning and Systems 5 (2023).

Limitations:
The limitations of the proposed technique are described throughout the paper. The limitations include overhead of length prediction and poor compatibility with long inputs. However, the latency effects of waiting for the aggregation of batches is not discussed or evaluated.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper comes up with the technique of using LLM to help LLMs’ inference to be more efficient. It predicts the queries’ response length, and group the those with similar response length into the same micro-batch, so that the inference efficiency can be effectively improved.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In the experiments, the proposed method gains significant improvement in terms of inference speed.
It is reasonable that the token redundancy leads to inefficient inference in batch.
It is effective and easy to implement for existing LLMs.


Weaknesses:
The metric of horizontal axis in Fig. 2 (a) is missing.

Limitations:
None.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors propose to improve the throughput of the LLM inference systems by correctly predicting the length of the response. 

Method summary: 
1. Predict the length of the response (Binning length for prediction modules to learn better)
2. Use the prediction to batch the queries with similar prediction to improve throughput (use variable batch size to leverage GPUs while managing memory requirements) 
3. Failure collection module to cut-off mispredicted batch evaluation. To ensure that this module is not triggered too often, it is advisable to over-estimate length of the prediction


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper tackles an important problem, provides a simple recipe for the solution and works reasonably well.
2. The evaluation is to the point. Answers all the natural questions that might arise.

Weaknesses:
The writing can be better. Some details in the questions section.

Limitations:
Yes

Rating:
6

Confidence:
4

";1
2D7ou48q0E;"REVIEW 
Summary:
To solve the challenges with limited high-quality labels and non-IID client data in federated learning, the authors present a pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients possess unlabeled data. Meanwhile, they propose the FedSTO, which consists of selective training, orthogonal enhancement, and personalized EMA-driven semi-efficient teacher. Finally, FedSTO achieves 0.082 and 0.035 higher mAP@0.5 when compared to partially supervised and SSFL baselines respectively. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. Good motivation. On the one hand, selective training can address the primary challenge of establishing a robust backbone for object detectors in FL. Specifically, it fosters more consistent representations by sharing the same non-backbone part. On the other hand, orthogonal enhancement reduces the bias towards specific weather conditions and the heterogeneity of object categories, leading to improved performance.
2. Many experiments have also verified the effectiveness of the proposed FedSTO.

Weaknesses:
1. The authors do not detailed introduce Personalized Pseudo Labeling for Unlabeled Clients, which is not easy to understand. 
2. I don't understand what Fig.3 wants to express, why there will be Fully supervised in Unlabeled overcast, rainy, and snowy.
3. In Fig.1, I don't know how the orthogonal enhancement works on the neck and head. Besides, I can not understand how to generate pseudo labels and assign them to the client models.

Limitations:
Yes, the authors have described limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel framework called Semi-Supervised Federated Object Detection (SSFOD) to tackle the problem of object detection in a federated learning setting. In this framework, the server possesses labeled data, while the clients hold unlabeled data from different distributions. The proposed approach consists of two stages: selective training and orthogonal enhancement. In the selective training stage, the focus is on updating only the backbone parameters on the clients to establish a robust backbone for the object detector. This selective approach helps improve the model's generalization capabilities across different distributions. The orthogonal enhancement stage follows, where all parameters are fine-tuned with orthogonal regularization. This regularization promotes representation divergence and robustness, further enhancing the model's performance. The paper also introduces a personalized pseudo label assigner based on a local exponential moving average (EMA) model. This assigner generates high-quality pseudo labels for object detection tasks, facilitating the training process in the semi-supervised setting. To evaluate the proposed SSFOD framework, this paper conducts experiments on three datasets: BDD100K, Cityscapes, and SODA10M. The results demonstrate that the proposed method achieves state-of-the-art performance when compared to existing approaches in both semi-supervised and federated learning domains.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1, This paper is well-motivated. In practical applications, not all data on clients are labeled, and how to leverage unlabeled data is important for FL.  
2, The writing is clear and easy to follow.  
3, The paper performs extensive experiments on three diverse datasets, encompassing varying scales, complexities, and domains. Moreover, the proposed method is compared against multiple baselines as well as state-of-the-art techniques. The experimental results consistently demonstrate improvements across different metrics, object categories, weather conditions, and data distributions. This comprehensive evaluation reinforces the effectiveness and robustness of the proposed method, highlighting its superiority over existing approaches in various scenarios.  


Weaknesses:
1, This paper lacks discussion on the communication efficiency and scalability aspects of the proposed method, which are important considerations for its practical implementation. Specifically, it does not address the communication overhead associated with uploading only the backbone parameters or utilizing local exponential moving average (EMA) models. Furthermore, the paper does not investigate the performance of the method as the number of clients or the size of unlabeled data increases.  
2, It could be better to compare or relate the proposed method with existing works on semi-supervised or self-supervised FL [1,2,3,4]. What are the advantages of the proposed method over these existing methods? 
[1] Zhuang, Weiming, Yonggang Wen, and Shuai Zhang. ""Divergence-aware federated self-supervised learning."" arXiv preprint arXiv:2204.04385 (2022).
[2] Zhang, Fengda, et al. ""Federated unsupervised representation learning."" arXiv preprint arXiv:2010.08982 (2020).
[3] Wu, Yawen, et al. ""Federated contrastive learning for volumetric medical image segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
[4] Dong, Nanqing, and Irina Voiculescu. ""Federated contrastive learning for decentralized unlabeled medical images."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.


Limitations:
Please refer to the Weakness.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work focuses on a practical application of federated learning, federated semi-supervised learning for object detection. It assumes that the server has labeled data and the clients only have unlabeled data. The proposed method is two-fold: selective training and orthogonal enhancement.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper integrates multiple existing techniques for federated semi-supervised learning. It is technically sound.
- The paper is generally well-written, clear, and easy to follow.
- Experiments on three object detection datasets demonstrate the effectiveness of the proposed method.

Weaknesses:
- The novelty is somewhat limited. The novelty lies in the integration of existing methods and making them work on the target use case, while the key algorithms are more or less adopted from existing works.
- The evaluation scale can be extended to a larger number of clients. The majority of the experiments are run with 3 clients and only one experiment is run with 20 clients. The autonomous driving use case is more like cross-device FL. It’s important to evaluate on more clients with client sampling.
- Some existing works with related techniques are not discussed: [1] and [2] fixed the head and only trains the backbone like the selective training. [3] and [4] also uses EMA in local training.
- Since the first stage is to learn representation, a comparison could be done with federated self-supervised learning methods [3][4] for learning visual representations.
- The organization of section 3 is not very straightforward. For example, 'Personalized Pseudo Labeling for Unlabeled Clients' is more suitable in Section 4 instead of problem statement.

[1] Fedbabu: Towards enhanced representation for federated image classification, ICLR’22. 

[2] Spherefed: Hyperspherical federated learning. ECCV’22

[3] Collaborative unsupervised visual representation learning from decentralized data. ICCV’21

[4] Divergence-aware federated self-supervised learning. ICLR’22.

Limitations:
Limitations are not discussed in the paper. It could contains some of the aspects such as the scale of the experiments.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper explores Semi-Supervised Federated Object Detection (SSFOD), a pioneering framework for distributed data sources with limited high-quality labels and non-IID client data, particularly in applications like autonomous driving. The authors present a two-stage strategy, FedSTO, encompassing Selective Training followed by Orthogonally Enhanced full-parameter training, to address data shift while representing the first implementation of SSFOD for clients with 0% labeled non-IID data. The proposed approach includes selective refinement of the detector backbone to avert overfitting, orthogonality regularization to enhance representation divergence, and local EMA-driven pseudo label assignment to produce high-quality pseudo labels. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides helpful figures, explores the valuable direction of semi-supervised Federated Object Detection (SSFOD), and presents an effective two-stage strategy called FedSTO for addressing data shift in a distributed data source. The proposed approach achieves state-of-the-art results in multiple datasets. Additionally, the paper provides a clear problem statement that is easy to understand.

Weaknesses:
However, there are a few areas for improvement before the paper can be considered for publication. First, the references list is incomplete as some essential references are missing, such as ""Federated learning with label distribution skew via logits calibration."" 

Limitations:
The authors did not discuss limitation and ethical issues in the main body.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a Semi-Supervised Federated Object Detection (SSFOD) framework featuring a two-stage training strategy, FedSTO, designed to address the challenges of heterogeneous unlabeled data in federated learning. The proposed framework employs selective training and orthogonality regularization with personalized EMA-driven pseudo-labeling to facilitate robust and diverse feature learning, enhancing object detection performance across multiple weather conditions and data distributions. The empirical results provide evidence of the merits of FedSTO over existing federated and semi-supervised learning methodologies. Notably, despite non-IID clients having no labels, FedSTO achieves performance comparable to fully supervised centralized models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
•	The paper introduces a solution for SSFL for object detection with a one-stage detector in non-IID weather conditions. 
•	The paper outlines a new approach to SSFOD, combining existing methods and adapting them to the federated learning setting.
•	The paper is well-structured, has clear headings and subheadings, and is easy to follow.

Weaknesses:
1. The inclusion of a federated setting with fully labeled data on at least one dataset would have provided a valuable comparison to the proposed approach. Also, while the comparison with the partially supervised baseline helps establish the lower bound of server pretraining, it is not a fair comparison due to the significant difference in training data volume.
2. Although the models are evaluated on their respective datasets, it would have been beneficial to assess their performance on a global test set as well. Context-specific evaluations are necessary, but examining the models' generalization can help avoid the issue of overly specialized clients, which is a concern in personalization.
3. An interesting analysis would have been to investigate how the performance gap of the proposed method changes with increasing amounts of data. The reported performance of the Fully Supervised Centralized Yolo-v5 Large in the paper is notably lower compared to other papers on full-scale datasets (e.g., YoloV5s achieves 77.2 in https://arxiv.org/pdf/2108.11250v7.pdf). This raises the question of whether the low performance and the closure in performance with the centralized setting is due to the limited training data. It would be valuable to compare the proposed approach against these scenarios with more data available or if possible employing a full server pretraining.
4. Regarding Cityscape, since the dataset does not provide precise weather information for each annotation, the data is distributed uniformly at random, and it's not clear how it is non-IID. Also, the non-IID aspect mentioned in the paper is limited to addressing the skew in feature distribution induced by weather variations. The authors could have utilized foggy-Cityscape and KITTI datasets o obtain a more realistic non-IID setting. 
5. The proposed SSFOD problem setting is similar to unsupervised domain adaptative object detectors and test-time domain adaptation. Even, the ingredients of the proposed solution such as EMA are also known in the semi-supervised OD and domain adaptation literature.  To me, the only difference is that the model parameters are updated on local clients instead of central weight update.   Please clarify the differences and advantages of the proposed solution compared to similar domain-adapted object detection methods. 
6. Many key components such as FPT with orthogonal regularization are adapted from existing literature such as [16].  Although I agree that the proposed problem setting is unique and novel, it is also important to clarify the novel technical contributions of the proposed solution. 
7. What is the rationale behind using Yolov5?  It will be beneficial to show that the proposed solution is generalizable on other families of object detectors such as Faster-R CNN and recent transformer-based object detectors such as DETR/Deformable DETR. 
8. mAP@0.5 is not an ideal evaluation metric to qualify localization capabilities. Please report mAP@0.75 and COCO-style mAP for a better understanding of the precise localization capabilities of the model. 
9. EMA, pseudo-label assignment on unlabelled data needs to be better explained. What are the training loss functions used for backbone weight update during the selective training step? 
10. It is mentioned that the proposed solution uses augmentations such as Mosaic, left-right flip, large-scale jittering. But, it is not clear how are the corresponding ground truth box positions determined in the case of unlabeled images.
11.  As the authors mention that SSFOD introduces additional computational overhead, it is desired to quantify the computational resources employed. 
12. The paper contains some errors, such as a missing dot in Line 139 and the use of ""IID"" in Table 5.

Limitations:
The paper does not have an explicit section for ""limitations and negative social impact"", but that is not a major concern for the research topic studied in the paper.

Rating:
6

Confidence:
4

";1
AALLvnv95q;"REVIEW 
Summary:
The authors consider normalizing flows with linear transformations. Computing the determinant of their Jacobian is expensive, limiting their applicability when training via maximum likelihood. Therefore, the authors introduced an energy-based method to train normalizing flows with linear transformation, which circumvents the need of computing the determinant of the Jacobian. It only has to be computed once for inference, which leads to a significant speed-up in training.

Compared to the maximum likelihood-based training, the authors achieve similar results while getting a significant speed-up.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The idea of combining energy-based training with normalizing flows by phrasing the flow as an energy-based model is novel and interesting. It circumvents the need of having to compute the determinant of the Jacobian of some layers, which can save a lot of compute. It still needs to be computed for inference; however, since it is constant in the cases the authors considered, it has to be computed only once and can be stored for further use.

Weaknesses:
My biggest concern is the relevance of this work. Normalizing flows typically use transformations that are designed in such a way that the Jacobian determinant is easy to compute. For that reason, linear transformations are not very popular, also because they are not very expressive. The claim of the authors, that they are used in Glow and subsequent flow models for images, is true, but the linear transformation was only applied across channels, and since the number of channels is much smaller than the total number of input dimensions, i.e. the number of all pixels across all channels, they are not very costly in terms of evaluating their Jacobian's determinant.

The authors apply their method to MNIST and CIFAR10, roughly matching the performance of their baseline with their method. However, both perform significantly worse than Glow and related methods. Hence, it is not clear whether their method gives them any advantage over these methods, either in terms of performance or speed.

There are also procedures to estimate the Jacobian's determinant to cut down cost, such as the Skilling-Hutchinson estimator used by residual flows. The authors should have compared their method with using such an estimator. Unfortunately, their method cannot be used for residual flows, as the determinant of the Jacobian cannot depend on the input to be treated as a normalization constant in an energy-based method.

Limitations:
Limitations were mentioned in the weaknesses section of the review.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Normalizing flows are generative models trained by maximizing the log-likelihood of the dataset given the model, which is expressed as the sum of the base probability of a transformed variable plus a series of log-Jacobian terms that keep track of the volume distortion factors. Evaluating the log-Jacobian is usually the main bottleneck of normalizing flow architectures, so much so that the most popular architectures are designed so as to keep the log-Jacobian computations tractable. 

This paper introduces the use of techniques from the energy-based models literature in order to more efficiently train normalizing flow architectures. In a normalizing flow architecture, it is common to have both elementwise non-linear layers, which give data dependent but tractable log-Jacobian terms, and linear layers whose log-Jacobian is data-independent but expensive to compute. The main idea introduced in this paper is to split the log-likelihood of a normalizing flow into the product of two terms: 1) an ""energy function"" that contains all the data-dependent terms and 2) a normalization constant that collects all the data-independent log-Jacobians of the linear layers.

This mirrors the split in energy-based models, where the energy function is tractable but the normalization function is intractable and it is not directly evaluated. Consequently, the authors apply a series of training objectives developed in order to train energy-based models to this new setting, obtaining an efficient way of training normalizing flows with expensive linear layers. 



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The main idea is clever and original. It follows from the realization that the intractable terms of the log-Jacobian are independent from the data. This offers an interesting and well-motivated reason for borrowing techniques from the energy-based literature.

The experiment section is adequate, with experiments both on toy data and on real high-dimensional datasets. While the results are not spectacular, they certainly support the claim that the energy-based training can be used to train normalizing flows with a substantial speed-up and without a major loss of performance.

The paper is well-written and the relevant literature is properly referenced. 


Weaknesses:
Major points:
In general, I am not convinced that this approach solves a major problem in the current literature. In fact, most successful flow architectures are designed to have fast log-Jacobian evaluations. Moreover, both continuous flows and the more recent flow-matching models can be used to train architectures with intractable Jacobian terms. 

The experiments only compare with the normalizing flow baseline and therefore they do not provide evidence that this approach has a competitive advantage over continuous flows and diffusion-like models such as flow-matching models. 

Minor points:
I find the presentation somehow difficult to read, with several important equations and concepts scattered all over the paper. In particular, I think it would be useful to the reader to explicitly discuss the score-matching loss associated with Eq.9, for example, by writing the concrete form of Eq.5 as applied to Eq.9 in the main text. 


Limitations:
The limitations of the approach have not be extensively discussed. I do not see any direct negative social impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors use a connection between flows and EBMs to devise a new training method for flows via score-matching.
Their training procedure improves efficiency via avoiding computation of the Jacobian determinants that are independant to values of the samples (i.e. only contribute to the normalizing constant of the PDF).
Additionally they identify two methods for improving training of the flows via score-matching, namely (1) match-after-preprossing whereby the score-matching occurs to the pre-processed variables avoiding numerical instability from the pre-processing layers, and (2) using an exponential moving average for the weight updates.
In their experiments, they show their approach significantly improves training efficiency.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The paper identifies that computing the unnormalized PDF of a flow is significantly cheaper than the normalized PDF for linear flow layers, O(D^2) instead of O(D^3), noting that computing the Jacobian determinant is not necessary for the unnormalized PDF. 
This contribution is novel and valuable. 
- The experiments of the paper are strong, and demonstrate the utility of their approach.

Weaknesses:
For sample generation by inverting the flow, the inverse matrix for the linear flow layers will cost $D^3$ - this is not mentioned in the paper. 
This inversion may be performed once, and then re-used for sample generation but for large $D$ this could be prohibitive. 

Limitations:
There is no discussion around inverting the flow when $D$ is large (see Weaknesses/Questions). 

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper proposes a new flow based approach to generative modeling called EBFlow. EBFlow uses training objectives from EBMs to reduce training cost for flow based methods unlike previous approaches that mostly achieved this via different architectures or biased estimation methods. The main insight for EBFlow is that the change of variables formula for a series of transformations for a NF is broken in to two parts: one containing only linear flows term and the other containing terms from non-linear transformations. The linear flows term is then interpreted as the normalizing constant and this interpretation aids in faster training strategies for flow based methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The motivation for reducing training costs for NFs is discussed well and is a very relevant and difficult problem. Several studies have been conducted to address this and the authors here propose a nice and novel methodology to address this.

The exploration and discussion of the method as well as the motivation is thorough. 

The empirical analysis considered is detailed.

Weaknesses:
I am not sure how to interpret the empirical results. It is obvious from the plots in Fig 2 that the proposed method is much faster than ML based objective. However, the performance in all the tasks for the ML based objective is usually better or very close to EBFlow. I was curious why this is? 

Also, for the results reported in Tables 1 and 2, were the methods trained for the same wall clock time?

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper has proposed an energy-based normalizing flow model, where the computaiton of Jacobin determinants for linear transformations can be skiped with score-matching objectives. This could enable deeper layers of linear transformation and make the flow model more efficient.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
This paper has proposed an interesting idea about the relationship between flow model and energy-based model. The flow model could be interpreted as an energy-based model with a tractable normalizaiton term and the proposed score-matching objective could significantly improve the efficiency.

Weaknesses:
1. The experiments lack comparison with other energy-based flow model like [1][2][3] and recent flow models like [4][5][6].
2. Is there any theoretic analysis for the complexity between different objective function?
3. The author misses citations for the very first deep-learning-based energy-based models like [7][8].

Writing weakness:
1. The background in section 2.2 could break up into two sections, i.e., energy-based model and score-matching model.
2. It is hard to see the difference between results in the first two rows in Figure 1.
3. In line 51, even the tranformation function in flow model is invertible, the author should specify that $g()$ is the inverse transformation function.
4. In line 51, using $g_i(\cdot;\theta)$ is misleading. Does different transformation functions share the same parameters?
5. I think the author intends to model $g(\theta)$. Therefore, it is better to avoid using $E(x;\theta)$ to avoid confusion as it is usually used for representing a parametric energy model.
6. For Table 2, the author could change the unit to batch/second for easier comparison.

Reference:
[1] Xie, Jianwen, et al. ""A Tale of Two Latent Flows: Learning Latent Space Normalizing Flow with Short-run Langevin Flow for Approximate Inference."" arXiv preprint arXiv:2301.09300 (2023).

[2] Xie, Jianwen, et al. ""A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model."" arXiv preprint arXiv:2205.06924 (2022).

[3] Nijkamp, Erik, et al. ""Learning energy-based model with flow-based backbone by neural transport mcmc."" arXiv preprint arXiv:2006.06897 2 (2020).

[4] Chen, Ricky TQ, et al. ""Residual flows for invertible generative modeling."" Advances in Neural Information Processing Systems 32 (2019).

[5] Maaløe, Lars, et al. ""Biva: A very deep hierarchy of latent variables for generative modeling."" Advances in neural information processing systems 32 (2019).

[6] Vahdat, Arash, Karsten Kreis, and Jan Kautz. ""Score-based generative modeling in latent space."" Advances in Neural Information Processing Systems 34 (2021): 11287-11302.

[7] Xie, Jianwen, et al. ""A theory of generative convnet."" International Conference on Machine Learning. PMLR, 2016.

[8] Xie, Jianwen, et al. ""Cooperative learning of energy-based model and latent variable model via mcmc teaching."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.

Limitations:
This paper has propose an interesting view of taking flow model as an eneryg-based model with score matching objectives. However, the expriments lack comparison of many baseline models as listed in 'weakness' part and other eval metrics in generation. Additional, the author should rephrase some sentences and formulars to avoid misleading.

Rating:
5

Confidence:
4

";1
46x3zvYCyQ;"REVIEW 
Summary:

The authors tackle the problem of nondifferentiable nonconvex locally constrained federated learning (Eq. $FL_{nn}$), and the bilevel variant (Eq. $FL_{bl}$). The minimax settings (Eq. $FL_{mm}$) is a special case of the latter. 

The authors provide error, iteration and communication complexity for these settings where both non-convexity and non-differentiability is assumed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The reasoning for obtaining the algorithm is clearly explained in Sec. 1. 

The experiment section is relevant and provides a clear comparison with other methods. 

Weaknesses:
Contrary to the reasoning for obtaining the algorithm, the results of Thm. 1 and 2 are not discussed. Doing so would help understand their significance. Typically, do lower bounds exist in some specific settings? What are the interpretation of the different terms? What are the dominant ones? 

Of lower importance, the reading is a bit complicated: the content is dense, with lots of technicalities and equation in the main text.

Limitations:
Discussing the significance of the results of Thm. 1 and 2 would be helpful.

The comparison with other FL scheme in Table 2 is done with different metric of accuracy. Relating those metrics would provide insight on the theoretical benefits of Alg. 1, and would complements the finding of the experiment section. Notably, highlighting the role of $\eta$ in Table 2 and in Thm 1 and 2 would be welcomed too, as it is an important hyperparameter (complementary to the small remark `l. 263-265`). 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper considers the federated optimization problem with the non-smooth non-convex target function. The authors use a zero-order oracle, which allows to approximate the gradient. This approximation is related not only to the original target function, but also to its smoothed version (an additional theoretical object obtained using the random smoothing technique). For a new smooth target function, there are methods in the literature for solving it, and this is what the authors use. Mini-max problems and bilevel optimization are also considered. Experiments are given. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
For me, the article is nicely written and easy to follow.

Weaknesses:
I ask the authors to pay attention to this work:

Gasnikov, A., Novitskii, A., Novitskii, V., Abdukhakimov, F., Kamzolov, D., Beznosikov, A., ... & Gu, B. (2022). The power of first-order smooth optimization for black-box non-smooth problems. arXiv preprint arXiv:2201.12289.

It gives a more or less unified scheme of how by taking any practical first-order stochastic method for smooth problems to obtain a gradient-free method for non-smooth problems. It seems that using this scheme one can obtain the results of this paper as well. Moreover, this scheme is not new; it has been used for 10 years in most papers on gradient-free optimization.  In particular, the facts that are proven in the article under review are also used in the article I cited and there are links from where they are taken. 

Please also note the refs within the article I cited. For example, there are references to works with gradient-free methods of solving saddle point problems. 

Finally, the contribution of this paper looks rather technical and at the moment I am not ready to accept the work. But perhaps the authors will change my mind in the process of rebuttal. 

Limitations:
The paper is theoretical, therefore there is no need to discuss the social negative impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
**Summary:** The paper develops zero-th order methods for solving non-smooth and non-convex federated problems. In addition, the paper also develops zero-th order methods for solving bilevel and minimax optimization problems in a federated setting. The authors develop a randomized smoothing-based approach and present convergence guarantees for the proposed algorithms. Experiments on training ReLU neural networks and hyperparameter optimization tasks are presented to evaluate the performance of the proposed algorithms. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
**Strengths:** The paper considers an important problem class of solving non-convex and non-smooth problems with zero-th order oracle. The paper is very well written with ideas explained clearly.  

Weaknesses:
**Weaknesses:** Here, I list some points the authors should consider addressing:

1.	There is some existing literature on federated learning where randomized smoothing-based zero-th order methods have already been developed. The authors have not mentioned/or compared their approach against such methods Please see [R1], [R2] (strongly convex) below. 
 

     [R1] Fang et al., Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning   

     [R2] Li et al., Communication-efficient decentralized zeroth-order method on heterogeneous data


2.	The authors should make dependence on the problem dimension explicit in the tables and the communication complexities listed in all the theorems. The current presentation gives an indication to the reader that zero-th order methods can achieve the same guarantees as first-order methods which is certainly not true.   

3.	The authors should consider including some discussion on the bounded set dissimilarity since the assumption is relatively new in the context of FL. 

4.	Is the notion of Clarke stationarity point related to Goldstein stationary point (or are the two notions the same)? Also, the discussion on Clarke's generalized gradient of the original problem compared to the gradient of the smoothed objective appears before the definition of Clarke's stationarity point. Please consider moving the definition before the discussion. 

5.	In Line 172 and Prop. 1 please characterize sufficiently small $\eta$. 

6.	Theorem 2 ignores the additional communication complexity because of solving the lower-level problem to $\epsilon_r$ accuracy. The authors should report the complete communication complexity incurred by the algorithm for solving the bilevel optimization problem not only the upper-level complexity.  

7.	Finally, the experiments considered to evaluate Algorithm 1 are weak. Specifically, the authors have considered a very low-dimensional problem. It would be advisable to include training on Cifar-10/MINST datasets with practically sized neural networks to evaluate the actual performance of the proposed scheme. Moreover, for hyperparameter learning there are many baseline bilevel algorithms including BSA, TTSA, ALSET, SUSTAIN, stocBiO, etc. whose performance should be compared with the proposed scheme. 


Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
Existing federated optimization algorithms usually rely on the assumption of differentiability and smoothness, which may fail to hold in practical settings. To this end, this paper employs randomized smoothing approach and zeroth-order optimization techniques for the development of FedRZO algorithm to address this kind of problem. Theoretical analyses on convergence, iteration complexity and communication complexity have also been provided. This paper further extend the idea behind the newly developed algorithm to bilevel and minimax federated optimization problems with sound theoretical guarantees.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. This paper has studied three different federated setting that existing works seldomly consider with both theoretical guarantees and empirical justifications.
2. The combination of randomized smoothing technique with zeroth-order optimization appeals to me, which may inspires the future work on non-smooth zeroth-order optimization.
3. Empirical results are interesting with adequate interpretation.

Weaknesses:
1. This paper may give more intuitive explanation or interpretation for its equations to ease reader's understanding. E.g., the authors may need to provide certain motivations for the study of the three different cases in the introduction section instead of directly putting out these cases without any explanation. Besides, the authors may also need to provide intuitive explanation for eq. 3 to help justify the reasonability of this assumption as well as its connection with real-world examples.
2. This paper may need to compare with more federated optimization algorithms to verify the efficacy of their algorithms, e.g., SCAFFOLD, FedZO [R1], from both theoretical and empirical perspectives.

[R1] Fang, Wenzhi, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N. Jones, and Yong Zhou. 2022. “Communication-Efficient Stochastic Zeroth-Order Optimization for Federated Learning.”.

Limitations:
Limitations haven't been mentioned.

Rating:
5

Confidence:
3

";1
suzMI2P1rT;"REVIEW 
Summary:
This paper aims to develop a simple and scalable IL method, which is applicable to a wide range of IL settings (e.g., offline/online, LfD/LfO, etc.). To achieve this,  the imitation policy are decoupled into a contextual policy and a latent variable. Experiments on a wide range of tasks illustrate effectiveness of the proposed methods.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well written and easy to follow. I enjoy reading it.
2. Authors conduct extensive experiments. It is a solid work.
3. I notice that authors present a detailed implementation details on baselines (like GAIL, GAIfO, et.) in the appendix, which will serve as a good reference for imitation learning researchers.
4. The proposed framework is simple and compatible with many different IL settings.

Weaknesses:
1. Results of online HalfCheetah seems to be lost.
2. Adding comparisons with recently popular (decision-)transformer-style methods in their compatible settings (e.g., [1] for offline IL, [2] for offline multi-task IL, [3] for one-shot IL and LfO) will make this work more promising.
3. The proposed method, CEIL, is not a plug-in method for every IL setting. We still need to make considerate modifications and designations when meeting new tasks. And a lot of works upon the generative adversarial imitation learning (GAIL) framework has been proposed to deal with different IL settings and they work well (e.g., [4] for cross-domain IL, [5] for one-shot IL). A further discussion on the superiority of this work over existing GAIL-style works will be appreciated. 

[1] Carroll, Micah, et al. Unimask: Unified inference in sequential decision problems. arXiv preprint arXiv:2211.10869 (2022).

[2] Furuta, Hiroki, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsight information matching. arXiv preprint arXiv:2111.10364. 2021.

[3] Xu, Mengdi, et al. Hyper-decision transformer for efficient online policy adaptation. arXiv preprint arXiv:2304.08487 (2023).

[4] Franzmeyer, Tim, Philip Torr, and João F. Henriques. Learn what matters: cross-domain imitation learning with task-relevant embeddings. Advances in Neural Information Processing Systems 35 (2022): 26283-26294.

[5] Yu, Lantao, et al. Meta-inverse reinforcement learning with probabilistic context variables. Advances in neural information processing systems 32 (2019).

Limitations:
This works does not seem to have any negative societal impact. Authors discussed its limitations and planned to leave them for future work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The objective of this work is to create an imitation learning (IL) method that functions effectively across a variety of common IL settings, which include online, offline, Learning from Demonstrations (LFD), Learning from Observation (LFO), and cross-domain. To achieve this, a dual-level expert matching goal is proposed. This involves not only trajectory matching with expert demonstration but also matching the latent variable, z, with the expert in latent space.

Contrasting with traditional trajectory matching methods, matching expert trajectories in latent space appears to more accurately replicate expert behavior. Experimental results reinforce the efficacy of this methodology, demonstrating good performance across four different MuJoCo tasks examined in this study.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The proposed method can adjust to five different Interactive Learning (IL) settings with mere minor adjustments in the algorithm, lending the model practical applications in real-world scenarios. 

- The document provides an extensive review of existing methodologies across these five IL settings, thereby allowing for a broad understanding of the field. 

- Additionally, by comparing the suggested methods against multiple baselines, good results have been demonstrated. 
The presentation of the paper communicates ideas clearly.


Weaknesses:
- The claim that all Online, Offline, LFD, LFO, and cro-domain issues are addressed appears to be overly ambitious. Given that only two MuJoCo tasks were evaluated in an online setting and four in an offline setting, the experimental evidence provided is inadequate to substantiate this claim.


- Furthermore, the conducted ablation study does not provide sufficient insights into the effectiveness of the J_MI term, which is designed to align with the expert demonstration in the latent space. To magnify our understanding of J_MI's contribution, an additional ablation baseline CEIL without the J_MI term could be quite useful. If, subsequently, we find that the removal of J_MI leads to a performance reduction of 10%, 30%, or even more, we can then quantify the effectiveness of matching expert demonstrations in the latent space with the specific technique used in this work. Providing such a comparison can significantly enhance the reader's understanding of the extent of performance improvements achieved through the application of J_MI.

Limitations:
See weakness and questions.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposed ContExtual Imitation Learning (CEIL), a general method that can be applied to multiple settings, including learning from observations (LfO), offline IL, cross-domain IL, and one-shot IL. CEIL incorporates the hindsight information-matching principle within a bi-level expert matching objective, which decouples the learning policy into a contextual policy and an optimal embedding. Empirical analysis demonstrates that CEIL is more sample efficient in online IL and performs well in offline IL settings. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. CEIL is closely related to hindsight information-matching methods. CEIL introduces an additional context variable $z$ to learn a contextual policy $\pi_\theta(a|s, z)$ and an optimal contextual variable $z^*$. The idea is to use the learned $z^*$-conditioned policy $\pi_\theta(a|s, z^*)$ to recover the expert data. CEIL is learned through a bi-level expert matching objective: explicitly learn a hindsight embedding function in the inner-level optimization, and perform expert matching via inferring an optimal embedding in the outer-level optimization. Such a decoupling procedure enables CEIL to generalize to diverse IL settings. 
2. Unlike the prior hindsight information-matching methods, CEIL does not require explicit handling components such as explicit rewards in online RL and handcrafted target return in offline RL.
3. CEIL is a scalable method that can be applied to LfD, LfO, offline IL, cross-domain IL, and one-shot IL. CEIL was evaluated in diverse settings in the experiments. The results on four MuJoCo environments show that CEIL achieves better sample efficiency than other baselines in the online IL tasks. Extensive ablations demonstrated that CEIL is not sensitive to the number of demonstrations and window size of trajectory. 


Weaknesses:
1. It seems that the pre-defined return $f_R(\tau)$ in Equation (2) is closely related to the hindsight embedding function $f_\phi(\tau)$ in Equation (3). However, their relationship has not been discussed. 
2. In the cross-domain online LfO experiments (Hopper, Figure 2(d)), CEIL performs worse than AIRL (state only). There is no explanation for this result. 


Limitations:
1. CEIL lacks explicit exploration bounds; thus, the offline results generally outperform the online results, especially in the LfO setting.
2. The trajectory self-consistency cannot be applied to cross-embodiment agents once the two embodiments/domains have different state spaces or action spaces. 


---------------------
After rebuttal
---------------------
Thanks to the authors for their rebuttal. I'd like to increase my score to 6.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a method that aims to address Imitation Learning (IL) tasks by simultaneously updating the embedding function of a contextual variable, an optimal contextual variable, and a policy conditioned on that variable. The proposed method learns the conditional policy by minimizing the trajectory self-consistency loss based on the concept of hindsight information matching. The optimal contextual variable is updated by minimizing the discrepancy between the learned conditional policy and the expert policy. The embedding function is optimized based on both the self-consistency loss and the discrepancy loss. The experimental results demonstrated improved performance in the following tasks: (1) learning from observations (LfO), (2) online/offline IL, (3) cross-domain IL, and (4) one-shot generalization IL.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. The proposed method is novel, and enables solving a variety of IL tasks with minimal adjustments.
2. The experiments were conducted on 8 different IL settings and outperformed previous baselines in most environments.
3. The empirical analysis in Section 5.2 offers interesting insights into the practical application of the proposed method.

Weaknesses:
1. A number of hyperparameters are introduced in this work, such as the embedding dimension, the trajectory window size, the architecture of the encoder/decoder networks. However, the authors do not provide any guidance or recommendations regarding the selection of these hyperparameters.
2. There appears to be a disparity between the theoretical objective (Eq.(4), Eq.(5)) and the practical objective (Eq.(8), and Line 6 in Algorithm 1) when it comes to optimizing the hindsight embedding function $f_{\phi}$. Theoretically, the update of $f_{\phi}$ should solely be based on the trajectory self-consistency loss, as mentioned in Eq.(4). In practice, however, $f_{\phi}$ is also updated according to Eq.(8).
3. The notation for the regularization losses is ambiguous. Specifically, there are two regularization losses used in this work, one for regularizing the embedding function $f_{\phi}$ (Eq.(9)), and another for regularizing the optimal contextual variable $\mathbf{z}^*$ (Eq.(7)). Unfortunately, both of these losses are represented by the symbol $\mathcal{R}$, and their definitions are scattered throughout the manuscript, resulting in unnecessary confusion.
4. The confidence intervals are not reported in Table 2-4.
5. The expert trajectories generated by SAC used in the experiments have not been provided, which could potentially pose challenges for replicating the results in future studies.

Limitations:
Minor suggestions:
- Section 3.1, Line 93: The transition dynamics function should be $\mathcal{T}$, not $\mathcal{P}$.
- Section 3.1, Line 98: The transition dynamics function should be $\mathcal{T}$, not $T$.
- Section 3.2, Line 124: Missing citation for hindsight experience replay (HER) [[1]].
- Algorithm 1, Line 4: The two $\mathcal{D}$ here can be combined into a single $\mathcal{D}$ for simplicity.

[1]: https://arxiv.org/abs/1707.01495

Rating:
6

Confidence:
4

REVIEW 
Summary:
One recent idea in reinforcement learning is to learn a sequential model that can predict state-action transitions and rewards, and then obtain a strong policy by inferring actions conditioned on high reward. This has the major benefit that even low-reward trajectories provide useful information for learning.

The challenge in adapting this idea to imitation learning is that in imitation learning, we do not have access to rewards, but rather a dataset of expert demonstrations, plus (potentially) datasets of other suboptimal behavior (e.g. rollouts from a policy in the online setting, static dataset from suboptimal policies in the offline setting).

The proposed solution is to simply infer / learn a contextual variable $z$ that performs a similar role as reward: that is, it determines the “type” of policy / actions that make up the trajectory. To avoid $z$ from encoding too much information, we make it very limited and regularize it. We can then train a $z$-conditional policy that can produce actions that mimic both the suboptimal behavior as well as the expert demonstrations (using a self-consistency loss). We also learn an optimal setting $z^*$ that selects the high-performing policy, by ensuring that the $z^*$-conditional policy produces trajectories with high similarity to the expert trajectories. There is a significant amount of math to flesh this out, which I won’t get into.

The authors test their method on four MuJoCo environments, using expert demonstrations from a SAC policy, in both online / offline settings, in both full demos / observation-only settings, and with / without a distributional shift (modifying the torso length) in the test environment, and show that CEIL tends to match or improve upon the results from a variety of baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The area of imitation learning is important and relevant, and the approach suggested covers a wide variety of use cases.
2. The idea proposed is conceptually simple: train a network that models a variety of policies, and then select an appropriate high-performing policy through the use of context variable that controls the policy. Similar ideas have seen significant success in reinforcement learning.
3. The empirical evaluations are quite extensive with an impressive number of baselines, and show strong performance of the author’s method in a wide variety of settings.
4. I particularly appreciated Table 1, which provides an excellent overview of related work.
5. While I found the paper hard to read in an absolute sense, I think that is mostly because the ideas are quite technical: relative to other imitation learning papers focused on expert matching, I found this paper easier to read and understand.

Overall, I recommend accepting the paper. However, I should note that I am not very familiar with the imitation learning literature, and so cannot provide an evaluation of the following aspects:
1. How novel / original these ideas are
2. Whether the baselines chosen are appropriate (e.g. perhaps the paper has not compared to current SOTA)
3. Whether the performance of baselines is in line with expected numbers (e.g. perhaps the authors did not tune hyperparameters of baselines well)

Weaknesses:
1. The empirical evaluations are entirely based on MuJoCo. It is unclear whether the strong performance will generalize to very different settings (e.g. Atari).
2. Though the paper aims to provide a general and broadly applicable imitation learning algorithm, it doesn’t tackle the most typical imitation learning setting: where we have access only to a dataset of expert demonstrations. (In particular, even in the offline setting, CEIL assumes access to a dataset of suboptimal behavior.) In principle we could run the algorithm in such a setting, though my guess is that it will not perform as well as baseline methods like behavior cloning, since the major benefit of CEIL is in its ability to leverage suboptimal data. (However, the expert-demos-only setting tends to be very vulnerable to spurious correlations / overfitting, and so is not as significant as the other settings.)

Limitations:
The paper should mention that it does not tackle the setting in which we only have expert demonstrations (even in the offline setting, the paper assumes access to a static dataset of suboptimal behavior, specifically D4RL in its experiments).

Rating:
7

Confidence:
3

";1
0kz5RmHxmE;"REVIEW 
Summary:
The paper proposes a method for Graph Contrastive Learning (GCL) by contrasting the spatial and spectral views ($Sp^2GCL$). The spatial view is obtained using a message passing GNN. For the spectral view the authors propose an equivariant model called EigenMLP. EigenMLP precomputes the k smallest eigenvalues and corresponding eigenvectors for a graph. The eigenvectors are made sign invariant using positive and negative eigenvectors as in SignNet. Permutation/Basis equivariance is obtained by learning MLP weights from fourier features of the eigenvalues. The method uses the same node/graph representations as positive views and from different graph/nodes as negative views. The InfoNCE contrastive function is used as the objective. After the self supervised training, a linear classifier is used on the downstream task of node/graph classification/prediction.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1) Paper proposes an important direction of combining spatial and spectral views
2) The method obtains competitive results compared with baselines


Weaknesses:
Regarding the views for a node/graph, the spectral views are obtained from the eigendecomposition that are made equivariant to sign and basis and the spatial views are obtained from an MPNN. This would give a fixed view for every node/graph and the concern is for a node/graph how to obtain multiple positive views? In the absence of multiple views the learning may be limited to fixed representations and may not scale well for larger models.

**Minor Typos:**
1. The eigenvalues and eigenvectors **encoder** the global shapes [13] and node absolute positions $\rightarrow$ The eigenvalues and eigenvectors **encode** the global shapes [13] and node absolute positions
2. It can **encoder** both the information of eigenvalues and eigenvectors $\rightarrow$ It can **encode** both the information of eigenvalues and eigenvectors
3. In practice, the sign-invariant neural networks may slow down model **converge** $\rightarrow$ In practice, the sign-invariant neural networks may slow down model **convergence**
4. Therefore, EigenMLP can learn more stable representations **again** structural perturbations $\rightarrow$ Therefore, EigenMLP can learn more stable representations **against** structural perturbations


Limitations:
Some of the limitations have been addressed in the paper. Please refer to the Weakness and Questions section to address further limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors present a novel approach called Sp2GCL that combines spatial and spectral views of graphs using EigenMLP, an informative and stable spectral encoder. The proposed method shows promising results in learning effective graph representations and outperforms other spectral-based methods in terms of both performance and efficiency.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This work proposes contrasting two views in the spectral domain and spatial domain and introduces a novel encoder called EnigenMLP to encode spectral domain information, which has not been done by former work.

Weaknesses:
- The contribution of the article is considered limited as the traditional Graph Neural Network (like GCN), which can be understood as spectral filtering in the spectral domain, is similar to the EnigenMLP proposed in this work. 

- Simply contrasting these two representations might not lead to significant improvements, as indicated in the results where there is no clear enhancement and suspicion of cherry-picking.

- The article suggests that the proposed method may not offer significant improvement over Graph Convolutional Network methods, and the results indicate a lack of noticeable enhancement. This raises concerns about the effectiveness of the proposed approach.
The article mentions that the contribution of the work is relatively limited, indicating that the proposed method may not introduce significant advancements beyond existing approaches.


Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
In this paper, the authors propose eigenMLP,an informative, stable, and scalable spectral encoder, which is invariant to the rotation and reflection transformations on eigenvectors and robust against perturbations. Based on eigenMLP,  spatial-spectral contrastive framework is proposed to capture the consistency between the spatial information and spectral information. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The eigenMLP is very motivated. The motivation and the theoretical analysis are convincing.

2. Based on eigenMLP, the spectral augmentation is bounded by pertubation delta L.



Weaknesses:
There are some major issues:
1. I think the eigenMLP is the main contribution but I don't know why the authors try to highlight that the spatial-spectral contrastive framework is a contribution. In my opinion, it is not the first work using spectral-based GNN and spatial-based GNN as two different views for GCL.

2. Because of the sparsity, eigenvalues usually obtain by randomized SVD. The computational cost is nk^2 rather than n^2k usually. I guess you do not use SVD designed for dense matrices. If you do you can check randomized SVD that may help you.

3. The novelty of eigenMLP: the eigenMLP looks like SignNet + position embedding. 

Limitations:
N/A

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a graph contrastive learning model with spatial and spectral augmentations, with a novel spectral encoder EigenMLP that could address the stability issue from eigen-decomposition. To exploit the strength of spatial and spectral domains, SP2GCL deploys two augmentation views for the contrastive framework; in the spectral view, it introduces tricks to alleviate the ambiguity of signs and basis, in order to stabilize the training process. The performances of node-level and graph-level experiments, as well as the transfer learning tasks, show the advantages of the proposed framework.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Contrastive learning for graph data is a prosperous and promising field, especially for the label-sparse settings and for the exploration of properties of non-Euclidean data.
2. The spectral part in the proposed framework attempts to address the stability and overhead issues, which are the main obstacle in the spectral methods for graphs.
3. Bridging among the spatial and spectral views is an interesting attempt, which could also make full use of their complementary properties.

Weaknesses:
1. As the authors emphasize the stability of their method, more theoretical and empirical analysis are expected to validate it.
2. I feel confused about the introduction of Fourier features in Line 169; the reasons and influences of them could be more detailed.
3. The performances are not always competitive in the experiment section; it may help to explain based on the properties of data sets.

Limitations:
N/A

Rating:
5

Confidence:
4

";1
UNOeQGHNaN;"REVIEW 
Summary:
This paper empirically shows previous supervised adversarial training methods have two shortcomings: (1) The features of the natural examples and those from other classes are not distinguishable and (2) the features of the natural and adversarial examples are not aligned. To mitigate these two issues, the authors propose a regularization to push away the features from other classes and freeze the natural examples and a reverse attention mechanism that increases the weight of the target class. The empirical results seem to validate the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and can be easily followed. 

1. The proposed method is well-motivated. The empirical study on the feature spaces is interesting and inspirable. 

2. The authors conducted the experiments on comprehensive datasets to support their claim.


Weaknesses:
1. Regarding the experiments, the authors only provide the results on ResNet18 and PreActResNet18, which is limited. I suggest the authors provide the results on the WideResNet and make a comparison with the current state-of-the-art performance listed in the RobustBench (https://robustbench.github.io/) to validate the effectiveness of the proposed method.

2. The authors do not provide the error bar to validate the significance of the results.

3. It is weird the accuracies under PGD, FGSM, C&W are higher than the natural accuracy achieved by ANCRA shown in Table 1. It would be better to provide some explanations for these abnormal results. It seems the defence has the issue of the obfuscation gradients. I suggest that the authors report the results under the adaptive attacks that use the auxiliary probability $p$ and even different $p$ during the testing phase and AutoAttacks. 


Limitations:
Though the authors claim they have limitations, I did not find the discussion of the limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper focuses on robust feature learning by combining two approaches: Adversarial Contrastive Learning and Robust Feature Selection. Specifically, it defines two characteristics for features: exclusion and alignment. The authors aim to enforce exclusion through Asymmetric Negative Contrast (ANC), which ideally should separate different classes. On the other hand, they aim to achieve alignment through Reverse Attention (RA), which should enhance the model's robustness.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The proposed method sounds interesting, especially the Reverse Attention approach. However, I have some concerns and feedback that will be provided in the later section of the review.

Weaknesses:
Writing Style:

Overall, the quality of the writing style could be significantly improved. Reading the paper is not smooth, and understanding it requires reading back and forth several times. For instance:

1. Generally, the flow of the paper is not engaging, which requires the reader to go back and forth in order to understand it. And, there are some sentences that are vague and difficult to understand.
3. Caption: Overall, the caption writing is not good; it contains long sentences that are difficult to follow. Labels for each plot have not been provided. Specifically, in Figure 1, regarding the distance between natural examples and OEs, it would be preferable to show distances between each pair instead of comparing class 0 with others, as class 0 may share common features with some other classes. It would be more informative to have the following plots: 0-1, 0-2, ..., 0-9.
4. Typos: ""perturbation -> perturbations"" in line 28 | ""which does harm to robust classification -> which harms robust classification"" in line 35 | ""... negative pair (PP) -> (NP)"" in line 64 | ""import feature -> important feature ..."" in line 118 | ""a example -> an example"" in line 147 | ... 

Content:
1. There are certain terms that are vague. For instance, what does ""well-trained DNN"" mean in line 20?
2. The definition of an Adversarial Example is not entirely correct. The perturbation itself may be visible, but when it is added to a natural image, both the adversarial example and the original image may not be easily distinguishable by human eyes. Furthermore, it is important to note that it is not the adversarial example itself to which perturbations are added. Rather, the perturbations are added to natural examples so that they cannot be correctly classified.
3. Description about Figure 1 in line 45-47: It is not a Gaussian distribution; it is a bell-shaped plot but not a Gaussian distribution. The main condition for a Gaussian distribution is that the area under it should be 1. And indeed, those numbers show a significant distance between classes. A cosine similarity below 0.5 should be large enough to indicate a difference between the representations of one class and another. Again, indeed, a cosine similarity between 0.9 and 0.99 indicates that representations of NEs and AEs are very close to each other! And comparing AEs plots with the OEs plots, we see that they have a very close representation to their original examples that OEs. Furthermore, if the representation of NEs and OEs is not significantly distant, models should confuse them, leading to lower clean accuracy. However, that is not the case.
4. The definition of OEs is not clear from the beginning of the paper. The reader should read the whole paper to figure out what they are, especially since different strategies are considered to select or generate them. 
6. Similarly, the term 'partial parameters' is vague, and the reader doesn't understand what it refers to until the later sections of the paper, which confuses the reader.


Approach and Methodology:
1. IT seems the main weakness is that ANC does not make significant contributions to the clean accuracy and robustness of the model, as indicated by the results in Table 3 of the ablation studies. Specifically, a model with only RA performs just as well as ANCRA. On the other hand, considering this, a large portion of the paper is dedicated to introducing and explaining ANC. However, the most important component, which is RA, is not studied well enough and lacks supporting experiments and theoretical insights.
2. The training process of a model with ANC is not clear enough. It would have been better to depict it through a diagram. Furthermore, the last paragraph in the introduction section is abstract and confusing, making it difficult for readers to understand the exact contributions.


Experiments:
1. The proposed method in the main table (Table 1) is evaluated without considering ""p"" in attack which creates a false sense of security. However, in Table 2, attacks with ""p"" demonstrate a significant decrease in the model's performance.
2. Even though ""Error Bars"" is marked in the checklist, I don't see standard deviation values in the reported tables.
3. Similarly, although ""Reproducibility"" is marked in the checklist, the code necessary to reproduce the results is not available.
4. I conducted an experiment with TRADES, and the accuracy I obtained differs from what is presented in Table 1. The clean accuracy is 80.92%, and the PGD accuracy is 50.10%.
4. The results of the Tiny-ImageNet experiments are inconsistent with those of other datasets, and the performance of the model does not seem promising. Additionally, they have not been compared with baselines, which is essential.

 

Limitations:
1. It appears that the approach does not work properly with large datasets like ImageNet.

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the authors address a notions of exclusion and alignment in representaion learning for robust adversarial training (AT). They propose a generic framework for AT that includes asymmetric negative contrast and reverse attention in order to obtain robust representation. In addition, they propose to weight feature by parameters of the linear classifier as the reverse attention, to obtain class-aware feature and pull close the feature of the same class. The authors show empirical evaluations on three benchmark datasets, showing the improved robustness under AT and as well as giving increased performance in comparison to state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality. The proposed  AT framework that concentrates on robust representation with the guidance of the two characteristics like exclusion and alignment appears to be novel. Generating and using negative samples by targeted attack to assist learning also appears novel. 

Quality. The quality is good. The paper is nicely motivated. The idea seems reasonable regarding the use two characteristics for robust representation: (1) exclusion: by pushing away the feature representation of natural examples from the feature representations for examples coming from other classes; and (2) alignment: by pulling close to each other the feature representation of the natural examples and the feature representation for the corresponding adversarial examples. The approach can be used in a plug-and-play manner for a number of defence methods, while the empirical validation shows advantages under the setting of white-box and adaptive attacks. While on table Table 2, we see consistent improvement for the used attacks with p, while we do not have results for comparison when we do not use p. 

Clarity. The paper organisation, the presentation and the writhing is good. I find the paper easy to follow on most parts. It might be beneficial some parts of the paper to be explained more easily so that the main idea behind the paper to be more obvious. Fig 2. might be improved to more concretely pin point the cases of class confusion issue. In Sections 3.2.1, the section within the lines 162-169, a bit difficult to rad on the first pass regarding the cases covered by the class confusion issue. I would suggest an additional small figure to illustrate intuitively and more obviously the cases. In Sections 3.2.1, the section within the lines 170-172 is not clear (not sure when the some sentences end and begin). Section 3.2.2 could benefit further polishing. Maybe a table or small figure describing what is negative samples or pairs (i.e., OEs), natural negative examples/pairs, adversarial negative examples/pairs, positive examples/pairs, adversarial negative examples/pairs, etc. could be helpful. The text in the lines 234-257 could also benefit sharpening, since some explanations are not that smoot and straightforward at least to my understanding. The results on Table 4 are interesting, but maybe the table could be reorganised and more clearly shown so that we can have a better grasp of what is the actual advantage. 

Significance. Adversarial training is important topic for privacy and security applications. This paper proposes an approach towards more reliable defence of adversarial attacks. The approach also seems to provide improvements on top of other approaches like the TRADES approach. Generating and using negative samples by targeted attack to assist learning seems valuable. Adversarial training with reverse attention on the feature level seems interesting.

Weaknesses:
Weaknesses
- In the empirical evaluation, some tables could be made more obvious (please see the comment on clarity for Table 4).
- The black box attacks not considered and comparison for black box attacks not done.

 

Limitations:
Limitations

- By generating negative samples by targeted attack, we use prior on about the possible attack, so possibly, the defence of the approach is biased towards the generated negative samples by the used targeted attack (or similar attacks).
- In the case of black box attacks when we do not have a prior about the attack, I'm not sure how much the proposed approach could help.  

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents two characteristics of robust features, exclusion and alignment, and proposes a novel adversarial training method with asymmetric negative contrast and reverse attention. For exclusion, it introduces asymmetric negative contrast loss and generates adversarial negative examples by targeted attacks to push out examples of other classes in the feature space, and for alignment, it introduces reverse attention that weights features based on the parameters of the linear classifier to obtain class-aware features. Experimental results on three datasets show that the proposed method significantly improves the robustness of the models.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1. This paper introduces two characteristics that robust features should have: exclusion and alignment, and proposes a new AT framework that enables models to learn robust features effectively. It can be used in a plug-and-play manner and works well with existing AT methods.
2. Specifically, asymmetric negative contrast loss for exclusion of robust features, a technique to create hard negative samples through targeted attacks, and reverse attention using class information for alignment are proposed, which induce the characteristics of robust features. To my knowledge, these techniques are novel in AT.
3. The proposed method records the SoTA performance in experiments on CIFAR-10, -100, and Tiny-ImageNet. The performance improvement shown in Table 1 is impressive.
4. Overall, the paper is well written to help the readers understand the presented concepts. In detail, figures 1 and 3 show the statistical differences between NEs and OEs and NEs and AEs, making it easier for the reader to understand the difference in the distribution of features and the effectiveness of the proposed AT.  Figure 2 also helps to illustrate the problem of class confusion.


Weaknesses:
1. The practical implementation details for Reverse Attention in Section 3.3 are unclear. Looking at line 228 of the paper, it says that the proposed method uses p and p' together to train the model, but it is ambiguous in what way the proposed method uses them together (e.g., does it use p+p' or alternate between p and p'). Releasing the source code in the future will answer a lot of questions, but it would be nice if it could also be clarified in the paper.
2. The experimental results for ""adaptive attack"" in Section 4.3 raise the question of whether the experimental results in Section 4.2 are an unfair comparison. Therefore, it is necessary to clarify what the role of p is and whether this is the role of the key in the gray-box setting.
3. Table 1 in the supplementary material, the results on Tiny-ImageNet, does not have baselines, making it difficult to see performance gains.
4. Making hard negative examples via targeted attacks requires additional computation, but according to Table 4, the resulting performance gain is small. This paper also needs a clear and specific description of the negative sample generation (such as whether PGD-N was used).  In the supplementary material, it is described as if they used the AEs that are predicted as the classes of NEs in the current batch as OEs, which is difficult to apply in cases with a large number of classes like CIFAR-100 unless the batch size is large enough.


Limitations:
In this paper, possible limitations are not discussed separately.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work aims to improve the adversarial training (AT) techniques from the perspective of learning robust representation representations. Specifically, the authors highlight two characteristics of having robust features. Exclusion: the similarity of features of samples of one class should be very less from the features of samples of other classes, so that model can differentiate between features of different classes for better classification.  The second attribute is Alignment: the gap between features of adversaries and clean samples of same class must be very small, which would increase model's robustness against perturbed samples.

To effectively satisfy these conditions, this work proposes two techniques, (1) to enforce exclusion, a asymmetric negative contrast loss is proposed which minimizes the clean sample similarity with negative samples of other classes, crafted by the adversarial attack. (2) to satisfy alignment: reverse attention strategy is proposed which align together the features of training examples which belong to the same class.

The proposed method is compatible with existing adversarial training techniques. Extensive experiments are conducted where the proposed method improves the natural accuracy as well as the robust accuracy when combined with existing AT algorithms.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Strengths:
1) The idea of improving adversarial robustness of model by explicitly learning robust representations seems interesting. Although the traditional AT and contrastive learning based AT implicitly learns robust features, this method attempts to achieve the same in more explicit manner.

2) The proposed techniques of utilizing asymmetric negative contrast loss and reverse attention to achieve exclusion and alignment during adversarial training are intuitive and are properly justified in the manuscript. 

3) The method provides impressive results as compared to previous state-of-the-art approaches. 



Weaknesses:
Weaknesses:

1) There are concerns regarding the proposed reverse attention strategy. During the testing, the true labels are not known and reverse attention uses the predicted label h(x) to calculate z'. In case the model provides wrong predicted class label, the corresponding z' will be also then multiplied with wrong classifier vector. This will further lead to degraded performance. The authors have not tried to address this scenario. 
2) From the works of [1] and [34], how is the proposed reverse attention different? Unfortunately the authors have not provided any comparisons or contrast.
3) In the ablation studies provided in Table 3, combining ANC and RT marginally improves results as compared to individual ANC and RT results. It looks like there is some sort of competition between the both proposed techniques.
4) Similarly, the use of negative samples via proposed targeted attack in Table 4 shows marginal improvements overall.
5) The paper is very difficult to understand, especially for the readers who are new to the technique of adversarial training. The presentation can be significantly improved. 

Minor weaknesses:
typo at line 281 scenaios -> scenarios 

Limitations:
The authors have not discussed any limitations of their work. 

Rating:
5

Confidence:
3

";0
ECBK3TVmZl;"REVIEW 
Summary:
The paper provides a new algorithm for performative prediction under general inequality constraints. The proposed algorithm has sublinear regret with respect to the performative optimum if the distribution map follows the location family model. The theory is supported by simulations.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The motivation behind introducing constraints in performative prediction is strong and justified. The explicit regret guarantees are compelling. I thought Example 1 was well-motivated, clear, and a great example to keep in mind throughout. Section 3 is well-written and gives a clear step-by-step outline of the main method. The connection to existing work is thorough.

Weaknesses:
The novelty has limitations. The main method combines existing ideas in the literature quite directly. The method for constructing a zeroth-order estimate of the gradient follows the same recipe as the two-stage method of Miller et al., and in a way this is the heart of the proposed method. This I see as the most relevant weakness. Below I'm including additional points.

It is not clear what the meaning of the time horizon T is in your analyses. Normally this should be the number of collected samples. But in your paper this is not the case, and I think that for this reason your T has a somewhat arbitrary meaning. I would rewrite the result statements to make T the total number of collected samples (I don't think this should change your rates). This is important especially because when you tune n you set it to grow with T as sqrt(T).

Assumption 1 is just there to ensure convexity of the objective, right? If so, maybe it's cleaner to state that assumption directly, and give the conditions in Assumption 1 as one way to satisfy the convexity condition. Otherwise, the way things are stated right now, your Example 1 wouldn't be handled by the assumptions?

In Lemma 3 (and other results that rely on this lemma) I would probably write something like ""there exists a delta(\eta, L_g) such that..."" Right now the interval for delta is too cumbersome.

You mention that derivative-free methods such as Flaxman et al. could be used for your problem. I think it would be a good idea to spell out exactly what they give you, because although you would lose dimension-dependent factors the results would be applicable much more broadly (all convex problems, not just location families). Location families are interesting but they are too specific.



Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper attempts to study the problem of learning where the problem instances arise from a distribution. What is special is that this distribution is dependent on the decision-maker. The specific problem is when the distribution is given by a linear shift over a fixed distribution where the degree of shift is given by the decision parameters itself, and the decision parameters itself need to satisfy an inequality constraint. The authors propose a PD algorithm that  (under certain assumptions on the loss function) queries $T+2\cdot\sqrt{T}$ samples and violates $\sqrt{T}$ constraints to yield a $O(\sqrt{T})$ regret across $T$ rounds.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper makes a good attempt at trying to look at the optimization problem with the inclusion of inequality constraints and it able to consider two simultaneous performance measures, namely, regret and (extent of) constraint violations.

Weaknesses:
The paper promises a study in the direction of general optimization under (decision dependent) uncertainty but only tackles the problem in the limited case where the distribution is linear in the decision space. There is no discussion (or any insight) on how these techniques may generalize for more complicated distributions. The algorithm design itself does not appear to be very novel and uses fairly standard statistical estimation techniques.

Limitations:
Authors have addressed limitation adequately. 

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper studies the problem of performative prediction under inequality constraints and gives an no-regret learning algorithm that obtains $O(\sqrt{T})$ regret and uses $\sqrt{T} + 2T$ samples.

In the problem of performative prediction, the data distribution $Z\sim D(\theta)$ depends on the choice $\theta$ of decision maker, and one formulates it as an optimization problem $\min_{\theta}E_{Z\sim D(\theta)}\ell(\theta, Z)$ and one only has sample access to the distribution $D(\theta)$. The major difference from previous work is that the paper considers a constrained optimization problem, where $\theta$ needs to satisfies certain constraints.

The paper proposes to use primal-dual gradient descent-ascent to solve the problem. Especially, the paper considers location families (where $Z \sim Z_0 + A\theta$ for some unknown matrix $A$) and provides convergence guarantee when the loss function is well-behaved (e.g. strongly convex and smooth).

Numerical experiments are also conducted to verify the practicality of the proposed method.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The performative prediction task is a well-studied problem and the constrained one (studied by this paper) is well-motivated. The primal-dual method proposed in this paper is practical and the author also provides regret guarantee under assumptions. The writing is clear and easy to read.

Weaknesses:
The technical novelty is limited, the primal dual method (and its robustness to gradient error) is fairly common in the literature.

Limitations:
.

Rating:
5

Confidence:
2

REVIEW 
Summary:
 This paper studies performative prediction under inequality constraints. In particular, the paper develops a robust primal-dual framework that requires only approximate gradients up to a certain accuracy but achieves the same order of performance as the stationary stochastic primal-dual algorithm even without performativity. Based on this framework, the authors propose an adaptive primal-dual algorithm for location families. The paper also presents the regret analysis and the perform numerical simulations to validate their findings. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper studies and analyze the optimality of the performative prediction problem under inequality constraints, which is an important yet missing piece in the literature. As far as I know, this is the first paper attempts to provide a solution to this problem. The paper is well-written, easy to follow, and the proposed robust primal-dual method is intuitive. The theoretical analysis is throughout and it also provides empirical study to justify the findings.

Weaknesses:
The major weakness of the paper is the assumptions are very strong, and the setting is somewhat restricted. For example, the proposed algorithm only works for a particular family of distribution, namely the location family, and it requires an accurate estimation of all the parameters involved in the computation, which might not be realistic. However, it is also almost unavoidable for the performative prediction setting.

Limitations:
NA

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of performative prediction under inequality constraints. The authors propose an adaptive, robust primal-dual algorithm that achieves sublinear regret and constraint violations, where the regret is with respect to the performative optima (instead of just a performative stable point). The algorithm is robust in the sense that it only requires approximate gradients up to a certain accuracy level and doesn't require the estimated gradients to be unbiased.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
* The main paper is very clearly written. The remarks around lemmas and theorems are helpful in interpreting the formal results.

* The proposed primal-dual algorithm is very nice in the sense that (i) the gradient estimators don't need to be unbiased; and (ii) the authors prove a regret bound in terms of the accumulated gradient approximation error. This can be a valuable contribution to the research community.

* The theoretical results are supplemented by good experimental results on multi-task linear regression and multi-asset portfolio.

Weaknesses:
* The model assumes the learner can query the distribution for samples (lines 204 - 213). In particular, the learner can query the performative risk at $\theta_t$ and $\theta_t + u_t$. This is a strong assumption and should be stated more explicitly. What if the learner cannot query the distribution and only observes a single sample (or, say, $k$ samples) in each round depending on $\theta_t$? This is more similar to the setting of Jagdeesan et al.

* I acknowledge that Assumption 1 is standard in the literature on performative prediction, but I find the strong-convexity assumption quite strong.

* There is no discussion on lower bounds for the sample complexity.

* The bound seems to have a factor of $d$ (the dimension). It is unclear to me whether this can be improved and how this compares to existing results in the performative prediction literature and results on performative regret in the unconstrained setting. (See one my questions below.)

Limitations:
Limitations: Assumptions are clearly stated in Section 4. (The assumption that one can query the distribution could be stated more explicitly.)
Negative societal impact: N/A.

Rating:
7

Confidence:
3

";1
b4Tr8NWTDt;"REVIEW 
Summary:
The paper addresses the problem of multi-agent RL by using learned world models over multiple potential opponent policies. The technique uses a Dyna-style algorithm to train the core policy with a combination of experiences generated through a world model and experiences playing against opponents. One evaluation demonstrates the world models benefits from training on data from multiple distinct policies. A second evaluation compares ways to use experiences generated from a world model to train a policy, showing pretraining on purely generated experiences is effective to warm-start a policy. An ablation study compares the proposed Dyna-PSRO model to vanilla PSRO on three MARL games, showing improvements over the PSRO model.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
## originality

Modest. The paper extends existing lines of work on MARL and world models, specifically studying the question of the policy diversity for training the world model.

## quality

Modest. The core results (figure 5) show clear improvements over PSRO. This is limited to a small number of games and the games themselves are relatively simple game domains.

The paper does a good job of breaking down specific claims to isolated experiments.

## clarity

Low. It was difficult to interpret many of the figures (questions and suggestions below). Generally the results of each experiment were hard to understand and would benefit from a single clear statement of the core outcomes in each section.

## significance

Low. The core audience of this work is researchers in MARL and particularly those considering world models as a solution.

Weaknesses:
The experimental results are promising, but would benefit from expansion. There are a few experiments that would help:
1. More games from MeltingPot. I hate asking simply for ""more"", but in this case it would help to show how well the agents perform on a wide variety of tasks. The results would help clarify where DynaPSRO benefits and may reveal limitations or areas for improvement. The wider set of results would give others confidence in the generality of the improvements gained by planning against diverse other agents.
1. More complex games. Consider more complex environments from PettingZoo (https://pettingzoo.farama.org/) or SMAC (https://github.com/oxwhirl/smac) that would highlight the potential of these algorithms in more compelx scenarios. This would help address the point that world models can become unstable and the value of strategic diversity in scenarios that support a much wider array of behaviors.
1. Scaling experiments. For example, when do prediction improvements level off when adding more policies? The experiments only examine adding 2 policies, which is a sparse sample of the space of strategies for most games.

The evaluations would benefit from other baselines to compare. What other algorithms could be used aside PSRO?

The full evaluation (last experiment) would benefit from a set of ablation studies. This could easily replace some of the planning experiments as the ablations would examine similar capabilities. I ask for ablations as these will be more convincing that the parts of Dyna being used add benefit over PSRO.


Limitations:
Yes. The work is focused on integrating world models into game playing agents and recognizes the preliminary nature of this work along with the potential risks introduced by using a simulation (world model) for decision making in real world applications.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to PSRO algorithms, where a world model of the environment is learned concurrently to the iterative PSRO strategy expansion. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors are right that the problem of having to re-learn policies from scratch is a large problem in the PSRO literature. Therefore, the idea to co-learn a world model alongside the expansion of the empirical game, in so taking advantage of the diversity of experience created by agents with slightly different best-response targets is an interesting approach to this problem. To the best of my knowledge this is also a novel solution to this problem.  
- I really like the presentation of the paper, and in general I think the authors do a very good job in terms of analysing the different moving parts of the framework in a reasonable manner.


Weaknesses:
I have a few concerns with the paper, however none of these necessarily game-changing in my evaluation of the work.

- I think the greatest misgiving I have with this work is that the related work seems to miss quite a large collection of PSRO papers that probably deserve mentioning. PSRO-style algorithms is a fairly small research area and I am surprised that the authors fail to make mention to many variants. In particular, as there is a section on strategic diversity itself in the paper, it seems odd that the authors have failed to comment on the line of works on diversity-based PSRO frameworks. For example, [1], [2], [3], [4], [5] are all diversity PSRO approaches. It also fails to place itself in the literature involving PSRO algorithms that attempt to speed up convergence times such as [6], [7].

- Furthermore, I was additionally surprised at the lack of comparison to NeuPL [8] which is another population-based framework attempting to similarly deal with the best ways to transfer information between agents in the population. 

- I do not necessarily believe that the authors need to benchmark against all of the approaches that I have listed. I do however believe the paper still needs work in terms of placing itself within the current literature on PSRO and other population-based frameworks.

- Based on the above, my score is set at a borderline accept. However, I am willing to revise this upwards upon seeing a better framing of this work in the current literature.

REFERENCES  
[1] Policy Space Diversity for Non-Transitive Games - Yao et al. 2023  
[2] Open-ended learning in symmetric zero-sum games - Balduzzi et al. 2019  
[3] Modelling behavioural diversity for learning in open-ended games - Perez-Nieves et al. 2021  
[4] Towards unifying behavioural and response diversity for open-ended learning in zero-sum games - Liu et al. 2021  
[5] A unified diversity measure for multi agent reinforcement learning - Liu et al. 2022  
[6] Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games - McAleer et al. 2020  
[7] Neural auto-curricula in two-player zero-sum games - Feng et al. 2021  
[8] NeuPL: Neural Population Learning - Liu et al. 2022  



Limitations:
I think the authors actively engage with the limitations of the work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors consider learning world models for deep reinforcement learning in combination with the construction of empirical games through PSRO. They first show that world models benefit from training on a diverse set of strategy profiles as can be generated through PSRO meta-game solvers. They then empirically show that PSRO best responses can enjoy sample efficiency benefits by training with simulated world model experience. Finally, they present Dyna-PSRO, in which PSRO best responses make use of a world model trained on all available experiences collected thus far in a run of the PSRO algorithm. Dyna-PSRO provides lower-regret solutions with higher sample efficiency than PSRO without a world model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well written and presented, and the experiments are well designed. 
- World models are seeing increased use in the RL community, and PSRO is one of the more practical and general methods currently available for finding approximate game solutions. This paper provides insights on how to properly combine the two and make improvements to PSRO's sample efficiency, which is one of its largest issues.
- The proposed Dyna-PSRO method is sound.
- While many implementation details are not present in the main paper, the appendix describes these details thoroughly.

Weaknesses:
It would have been nice to see how current high-performing world model methods such as Dreamer, which employs latent state spaces [1,2] might perform with the same approach. It's not immediately clear if experiments like in section 3.3.2 would have had the same outcome.

[1] Hafner, Danijar, et al. ""Dream to Control: Learning Behaviors by Latent Imagination."" International Conference on Learning Representations. 2019.

[2] Hafner, Danijar, et al. ""Mastering diverse domains through world models."" arXiv preprint arXiv:2301.04104 (2023).

Limitations:
All limitations have been adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper describes combining two things: training a world model of a game, and doing Policy Space Response Oracles (PSRO) on the game.

Doing PSRO involves getting a lot of episodes from the game (episodes are used to train the RL best-responses, and also to estimate the payoffs of the empirical game). The novel algorithm in this work (Dyna-PSRO) can be thought of as a modification of PSRO where those episodes are **also** used to train a world model (which is essentially a learned simulator of the game engine). Then, the world model is used to improve the training process of the best-response policies. 

Through experimental results, the authors show that this improved training process (based on Dyna) can cause the best-response learners to learn a stronger policy than the normal method when using the same amount of interactions with the real game environment. It does this by training the policy using trajectories from the world model (in addition to the usual trajectories from the real game environment), and by equipping the agents with one-step lookahead planning during training.

This paper showcases experiments on the Dyna-PSRO algorithm in three games, and Dyna-PSRO outperforms PSRO in all three, as measured by an approximation of NashConv.

The paper also shows experiments to measure the quality of the learned world model, to test the hypothesis that Dyna-PSRO results in a good world model.

I think the paper has some flaws* in its current form, but the core work of the paper is good, the charts are beautiful, and the results are strong.

---

*Edit: many of the abovementioned flaws were addressed during the rebuttal/discussion period.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Overall, the paper is well-executed.
    - It is well-written and polished.
    - There has clearly been a lot of time and effort put into the engineering and writing of this paper. There are 4 sets of thorough experiments in the main paper, and more in the appendix.
    - The figures are extremely readable.
- The results concerning the performance of the best-response policies are impressive.
- The effort will surely be helpful to future researchers: the research directions of (1) improving the efficiency of PSRO response calculations and (2) training better world models should continue to flourish, and the work presented in this paper contributes to both.
    - The research direction seems natural, especially in the direction of using world models to improve the performance of PSRO.


Weaknesses:
I think the paper could be better in explaining or hypothesizing the ""why"" for a lot of things, even if just qualitatively.

I think the paper states some conclusions too strongly.

I think some things are not explained well enough and are confusing to the reader (at least, to me):

- SumRegret metric:
    - It's really not clear from the main paper how SumRegret works (even though it is explained in the appendix). This could be clarified by defining the terms ""method"" and ""combined game"".
    - Also, I would feel a lot better if I saw results measured by an alternative metric, where the deviation set is the set of **all** policies. The $max_{\pi_i \in \bar{\Pi}_i}$ could then be approximated by just training one more response policy (as if doing one last epoch of PSRO). This seems like it would be a more accurate approximation of the Nash Conv. Is there any reason to use the metric in the paper instead of this?
- Empirical Game Solution not described
    - Since the settings here are general-sum, it's probably important to specify what solution concept is used for the meta-strategies in the main paper (even though it is included in the appendix).
- Experiments in Section 3.1 Strategic Diversity
    - Looking purely at Figure 2, the conclusion ""Overall, these results support the claim that strategic diversity enhances the training of world models"" does not ring true to me. For example, there are three world models which perform better on the metric (accuracy) used in Figure 2 than the most diverse one, for Observations.
    - Even if I look in the appendix at E.1, there doesn't seem to be significant evidence to support the conclusion: multiple world models have similar recall scores than the most diverse one, and the one trained without the random policy seems to have better scores.
    - I would be interested in seeing the cross-entropy loss instead of (or in addition to) the accuracy.
- The discussion of the Decision-Time Planning results (3.2.2) seems incomplete:
    - ""**The main outcome of these experiments is the observation that multi-faceted planning is unlikely to harm a response calculation,** and has a potentially large benefit when applied effectively. These results support the claim that world models offer the potential to improve response calculation through decision-time planning."" (emphasis mine)
    - However, Figure 4 does show that decision-time planning causes the response to be *worse*: The solid blue line (top) has no decision time planning, and the dashed gray line (second from the top) has decision-time planning, and performs worse.
    - It would be nice if there was some discussion about this, perhaps an intuitive/qualitative reason why this is.
- Dyna-PSRO results need more details (Figure 5):
    - For each experiment, how many policies (iterations of PSRO) were there?
    - Does each policy train for a fixed number of steps, or until some measure of convergence is reached?
- Was ""policy"" vs. ""strategy"" ever defined like this before? In my opinion, we shouldn't define these terms like this, because they are usually considered synonymous. The terms I'm familiar with are ""policy"" or ""strategy"" for the former, and ""meta-policy"" or ""meta-strategy"" or ""meta-strategy distribution"" for the latter. Just my opinion!
- I was very confused by the definition of World Model while reading the paper.
    - Even after reading it through entirely, I was under the impression that each player had their own world model, and that it implicitly modeled the actions of the opponent.
    - If one misses the bold notation of the definition of agent world model from line 137 to 141, it's easy to think that this is the case, especially since the phrasing is that ""the agent learns and uses a ... world model"" (instead of, say, ""the agents learn and use a ... world model).
    - On one hand, the formal definition given for an ""agent world model"" is technically accurate, and I am just dumb. On the other hand, I suspect many of us are dumb, and will be similarly confused upon reading the paper. (Also, I'm not **that** dumb: it's really hard to tell that the O and A are bolded!) (Also, even if some of us are not dumb, we are likely lazy and will gloss over the explanation that boldface means joint.) This is all to say that I would suggest explicitly stating that the world model takes as input an observation and action from **each** player, and returns an observation and reward to each player. And that the world model does NOT model the actions of any player.
- The bolding is nice, but it would be less confusing to **also** say ""strategy profile"" or ""joint strategy"" anytime this is meant instead of just ""strategy"" and something bold, as it's very easy to miss or forget what something bold means (plus, it seems incorrect to call a strategy profile a strategy). Also, maybe emphasize that sentence that explains what boldface means, so that readers don't miss it?
    - For example in line 774 and 775 of the appendix:
        - ""This is typically not tractable, but instead draws are taken from a dataset generated from play of a behavioral strategy **σ**. And the performance of the world model is measured under a target strategy **σ∗**""
        - should be ""strategy profile"" and ""target strategy profile""
    - and same in Line 159 and 160 of the main paper, and throughout section 3.1


Limitations:
Limitations addressed

Rating:
6

Confidence:
4

";0
thbXgJ8gNK;"REVIEW 
Summary:
This paper comprehensively evaluates three efficient training algorithms for transformer language models: layer stacking, layer dropping, and selective backpropagation. Such an evaluation is crucial due to the extensive computational resources needed for training transformer-based models. 

The authors highlight the importance of specifying a training budget in performance comparisons. They introduce a measurement called ""reference system time"" (RST) to facilitate comparisons across different hardware configurations. The study examines these algorithms under various training budgets, model architectures, and datasets. The main finding is that these efficient training methods do not always significantly improve over the standard training baseline. Layer stacking tends to outperform the baseline and other methods in most instances. However, layer dropping sometimes falls short, while selective backpropagation often diminishes performance. 

The paper also explores how these algorithms perform in a downstream task setting, showing that pre-training performance does not necessarily correlate with the ability to generalize in these tasks. It concludes by stressing the importance of caution when using these efficient training methods due to their potential overheads.  Further implications of this study can guide the choice of efficient training methods in transformer-based language models by considering the trade-off between improved performance and computational resources.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Originality: The paper's focus on evaluating efficient training algorithms for transformer language models under a defined training budget is a novel perspective. The introduction of ""reference system time"" to compare training times across different hardware configurations shows ingenuity.

2. Quality: The paper exhibits high quality in its methodology, experimental setup, and analysis. The experiments are thorough and well-conducted, involving different parameters like training budgets, model architectures, and datasets. The findings are meticulously analyzed, providing an honest assessment of the capabilities and limitations of the methods.

3. Clarity: The paper is well-written and logically organized. The authors clearly explain the motivations behind their work, the reason for choosing specific efficient training algorithms, and their experimental methodology. They discuss their findings in an understandable and straightforward manner.

4. Significance: The paper notably contributes to training language models. Its focus on efficient training methods and the introduction of RST offer practical solutions to the computational challenges researchers face. This research opens the door for more studies on efficient training of such models. The paper's findings and suggestions are invaluable for researchers and will aid in making informed decisions when implementing efficient training strategies.

Weaknesses:
1. Limited Scope: The evaluation in the paper is limited to three specific efficient training algorithms (layer stacking, layer dropping, and selective backpropagation).

2. Limited Novelty: This paper mainly focuses on revisiting methods in a specific context. There are not many novel ideas or groundbreaking discoveries introduced in the paper. Studies expected to be published in top-tier conferences like NeurIPS should ideally contain significant novel contributions.

3. Lack of Rigor in Experimental Design: While the authors propose using Reference System Time (RST), they do not sufficiently justify its superiority over other potential measures of computational effort. 

4. Overemphasis on Computational Efficiency: The authors focused on computational efficiency, sidelining the quality or performance of the models. This imbalance in focus can give a skewed perspective.



Limitations:
The authors have clearly demonstrated some limitations in their paper, specifically highlighting that their study focuses on a select few efficient training algorithms and acknowledging that further exploration of additional algorithms is required. Furthermore, the authors admit that their review is primarily centered on language model pre-training and that the findings may not be necessarily applicable to fine-tuning or other data-intensive modalities. 

However, the authors could have expanded on potential negative societal impacts. For instance, if these inefficient training algorithms are utilized for large-scale projects, it could lead to unnecessary usage of computing resources, subsequently affecting energy consumption and carbon emissions. Additionally, the misrepresentation of algorithm efficiency could potentially mislead the academic and industrial communities in their pursuit of efficient models. 

To improve the paper, it would be beneficial for the authors to provide more context on possible wider implications, particularly focusing on the ecological impact and potential misdirection of research efforts. They might also consider suggesting some directions for future work in improving the efficiency of training algorithms, both in terms of reducing computational costs and minimizing environmental impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
Many algorithms have been proposed to make the training of ever larger models more efficient.
The authors present a critical empirical study of three selected training algorithms (layer stacking, layer dropping, and selective backpropagation) with fixed training budgets and find that these algorithms often do not make BERT and T5 pre-training significantly more efficient.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper re-visits evaluation standards, and offers a careful, rigorous, and clean re-evaluation of three efficient training algorithms. 
It is an excellent example for the importance of also publishing ""negative"" results (no gains in metrics, no new algorithm).
Specifically, this finding can save development time in the implementation and maintenance of these algorithms and reduce the complexity of the pre-training algorithms.


Weaknesses:
I agree on the limitations pointed out by the authors in the section ""Limitations and Future Work"", including evaluation of small subset of efficient training algorithms only, language model pre-training only.
Overall, the paper might be felt to be too simple and straight forward, the more as it addresses a well-known issue in a noisy field. 


Limitations:
Yes, see the section ""Limitations and Future Work"".

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper reevaluates several training algorithms aimed at enhancing the efficiency of Transformer-based models, such as layer stacking, layer dropping, and selective backpropagation. The authors effectively manage the training resources by employing a metric called reference system time. However, the key result of the study indicates that these three efficient training algorithms yield only modest improvements compared to standard training methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors conduct a thorough evaluation of efficient training algorithms, providing valuable insights into their effectiveness and highlighting the marginal gains such methods achieve. This revisit contributes to a deeper understanding of existing methods and offers a practical perspective on their applicability.
- The authors point out the shortcomings of using wall-clock time for reference, and instead propose reference system time (RST) for a better estimate, which is valuable rule to highlight.

Weaknesses:
- While revisiting these methods undoubtedly holds value for the research community, it is important to note that the obtained results may be somewhat trivial and lack significant insights. The paper might be better suited for more specialized venues. 
- While the primary focus of the study lies in evaluating the original approaches, providing additional discussions on methods and evaluations that build upon these evaluated techniques would offer a more comprehensive understanding of how these methods have been adopted and evolved since their initial proposal.

Limitations:
In Section 6, the authors acknowledge the limitations of their work and highlight that it is not feasible to evaluate all types of training efficient algorithms due to the potentially exorbitant costs involved.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies three efficient pertaining techniques for Transformer models (layer stacking, layer dropping, and selective backpropagation). At variance with previous works on the subject, the authors adequately control for the learning rate schedule by evaluating performance at fixed compute budgets (as defined through a scaled wall-clock time). In doing so, they find that most of these methods provide unclear gain over a vanilla baseline -- and this finding holds across T5/BERT-like models. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
* **S1.** At variance with previous works, the authors adequately control for the effect of the learning rate schedule. This is a common mistake in countless papers, which should be more discussed as it has previously impacted prominent work around scaling laws (see Hoffmann et al., 2022 as a response to Kaplan et al., 2020). 

* **S2.** This paper provides negative results on the timely topic of efficient pretraining of Transformers, allowing the community to take a step back and retrospectively analyse the validity of previous findings. Under that light, negative results can be as valuable as positive ones.

* **S3.** (minor) The color scheme (which is consistent across text & plots) is a good idea to visually help readers. 

Weaknesses:
* **W1. Limited significance due to limited adoption of the practices evaluated.** The impact and significance of this work is limited, as it evaluates three methods which have not been widely adopted by the community. For a negative result paper to be truly valuable, it is better if it addresses a common practice rather than a result that has not gotten traction anyway. 


* **W2. It is unclear how the proposed Reference System Time approach improves upon other methods.** The authors bring-up RST as one of the significant contribution of the paper, and base their analysis on its usage. However, it's unclear how it differs in practice from simply using wall-clock time -- especially since the authors don't actually perform cross-hardware comparisons, which would be one of the benefit of formalising RST. 
   * **W2.1** The authors claim ""Unfortunately, WCT can fluctuate even on the same hardware, for instance, due to the usage of non-deterministic operations, hidden background processes, or inconsequential configurations, such as the clock rate"". It is not clear how RST improves upon this -- the first recording used to scale RST may be noisy as well, and so could the recordings being scaled. 
   * **W2.2** It is unclear whether Figure 1 is based on real measurements or is simply there for illustrative purpose. 


* **W3. The results are disparate, incomplete, and lack clarity.** The results are middling, lacking a clear finding (overall it seems layer dropping is worth it across all budgets for BERT, layer stacking may bring gains for longer training budgets on BERT but only on shorter budgets for T5, and that selective back propagation is never worth it). For a paper with negative results, clarity is key to improve upon past work.
    * **W3.1.** Selective back propagation is compared in a completely different setup, on a single budget, and downstream performance is never measured for SBP+T5 -- only validation loss. Furthermore, while SBP+BERT are trained on 3 different datasets (a rather interesting study given the nature of SBP), only validation losses are provided for comparisons (no downstream task performance) and no conclusion is proposed on the influence of data source on SBP. The lack of a unified framework for comparisons between layer stacking/dropping and SBP make the paper feel more like a pot pourri of ideas rather than a principled & systematic comparison.
    * **W3.2.** Reporting training loss instead of validation loss in Figure 2 is questionable: as the authors later discuss in their ablations in Figure 5.b), layer dropping acts similarly to drop out -- so it makes sense it is behind on training loss in Figure 2. Validation loss here would be far more valuable. 
    * **W3.3.** The figures are not clear: the x-axis in Figure 3 is not specified, the legends lack an analysis/mention of the key results for each figure, it's impossible to see differences in Figure 4, etc. Specifically to Figure 4/Table 1, the authors mention the concept of Pareto front multiple times in the main text; maybe plotting it would be far clearer than the figures/table proposed for downstream evaluation. 


* **W4. Insufficient clarity and lack of added educational value.** Negative results papers can prove themselves especially valuable if they help correct a bad practice in the community; here, a general lack of clarity and of adequate framing make this paper fall short of being an educational read.
   * **W4.1.** The introduction of RST is confusing and frankly not necessary, as discussed in W2.
   * **W4.2.** The issue around learning rate schedules could benefit from additional discussion & illustration. In particular, it would be interesting to mention that this is the crux of the difference between the scaling laws of Kaplan et al., 2020 and Hoffman et al., 2022. Not accounting for the influence of the LR schedule caused the Kaplan work to inappropriately recommend scaling model size primarily, instead of scaling model and data jointly. 
   * **W4.3.** The paper contains numerous imprecisions and overall lacks clarity (see W3.3. as well). The authors sometime abuse the citations by citing far too many work at once instead of the most relevant ones (l14, l32, l78, etc.). The default citation style of NeurIPS doesn't help here, making it difficult to identify the works at a glance... l144 it's unclear for instance what the sentence of curriculum and deduplication brings to the work -- since this never further discussed. Note also that Figure 6 is never referenced in the main text, and that ""Besides the dizziness due to conflicting ideas"" on l30 is not adequate language for a paper. 

Limitations:
The authors have included a limitations section. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents an analysis of 3 algorithms for training transformer models with a focus on efficiency. The authors present a way to measure wall clock time irrespective of the underlying hardware and make a principled comparison of the 3 algorithms by predefining a compute budget and adapting each algorithm and training recipe for the available time. The results of the paper show that none of the methods is universally an improvement over standard training.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Meta-analyses are very common in other fields but are sorely lacking in ML/Deep learning so the goal of this work is very welcome and in my humble opinion useful to the community.
- Setting a compute budget and trying to optimize each algorithm accordingly is not as common as it should be in similar works.

Weaknesses:
- The main weakness of the work is possibly the choice of algorithms to be analyzed. There are only 3 while there could be a plethora of others as mentioned in the paper's related work. Moreover, the 3 methods evaluated are not particularly widespread (possibly because they don't work as well as the paper shows).
- Another possible weakness is the fact that there is separate hyper parameter tuning per method irrespective of the compute budget. This means that the results could be widely different were one to apply the above methods to a new problem where the optimal or near-optimal hyper parameters are not known a priori.
- RST although fair, it is not a transferable metric which means that a different paper will not be able to compare with the RST results of this paper and will have to re-run the experiments to compute the relative speed of the methods on their hardware. This severely diminishes its usefulness.

Limitations:
The authors properly present and analyze the limitations of their work.

Rating:
5

Confidence:
4

";1
LnZuxp3Tx7;"REVIEW 
Summary:
This paper considers the various settings of overfitting, including benign overfitting where an interpolating estimator is optimal in spite of perfectly fitting noisy labels, tempered overfitting where an interpolating estimator is inconsistent but error is a function of the label noise, and catastrophic overfitting where an interpolating estimator has no ability to generalize (random guessing in classification or infinite risk in regression). The authors follow-up prior works in benign overfitting by showing that a two layer neural network can benignly overfit so long as the input dimension of the data grows faster than the number of training samples, irrespective of the width of the network itself (assuming convergence to a max-margin solution, and additionally show another benign overfitting result that only requires convergence to a KKT point of a max-margin problem). 

They also provide a theoretical justification for tempered overfitting in two-layer ReLU networks, a phenomena that was empirically studied in prior work. The authors precisely characterize the excess risk of an interpolating network trained to fit uniform data with labels samples as +1 and flipped to -1 with probability p. The authors show that for one-dimensional data, the excess risk of such a network interpolating this problem is tightly bounded by functions of the noise level (e.g. theta(poly(p)) and theta(p) in a more restrictive setting).

The authors then observe that catastrophic overfitting can occur when forcing networks, of width at least 2, without a bias to interpolate the training data (drawn as specified before) when there is at least one sample with -1 label. They also show that you cannot obtain benign overfitting from convergence to a KKT point of the max-margin problem unless the network has a bias term.

Finally, the authors provide an extended version of an experiment from prior work on tempered overfitting in which fully-connected ReLU networks are trained to interpolate data on the unit sphere with +1/-1 labels sampled with probability p, as defined before. They observe that there are regions between benign and tempered that are a function of the ratio of input data dimension to number of training samples, namely: as the input data dimension grows higher than the number of training samples, the excess risk of the model approaches 0. They additionally show experimental results on 3-layer networks to empirically assert that, while their theory holds for 2 layer networks practically the same behavior is seen for 3. Finally, they experimentally show in their setting that the width of the network is not as important as the input dimension when evaluating consistency of the estimator.

EDIT: Thank you for the rebuttal and engaging response, this is indeed a very thoughtful work. I opt to keep my positive score.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper provides a clear theoretical analysis for tempered overfitting for two-layer ReLU networks that has not appeared in prior literature. There is some earlier theoretical work on tempered overfitting in kernel regression and for minimum description length settings, but as far as I know nothing for a standard finite-width neural network. The prior cited work (Mallinar et al. 2022) shows experimental evidence with neural networks that suggests tempered overfitting, and following their formalisms the authors of this work go a step further to give proofs of excess risk of one of the experiments in that paper (more on this in later section). 

I really appreciate the authors providing proof outlines and intuitions in the main text, as well as citing references where they are using techniques and following proof styles introduced in prior works. The style of explanation is accessible and gives me, at a high level, the right amount of information to contextualize the theorem statement while deferring details to the appendix should I want to review them.

There are many intriguing results from this paper. For one, I am initially surprised by the one-dimensional tempered results and Theorem 4.1 on benign overfitting feature bounds that don’t depend on the width of the network. I’ve written more about this in the questions section, but I would have expected a more subtle interplay between width of the network and input dimension of the data. Notably, though, the width of the network has to be sufficiently large to interpolate the data (e.g. overparameterized) and since the authors study 2-layer networks this might imply a large width, though their experimental results show roughly similar behavior for width 3000 and width 50 networks) across varying input dimensionality.

I find the role of bias section illuminating as well. To the best of my knowledge, I haven’t seen a generic negative result like this, that any bias-less network will have a risk lower bounded by a function of the network width or the support of the negative training samples.

Weaknesses:
The authors do acknowledge it and give a tighter bound subsequently under stronger assumptions, but Theorem 3.1 has an upper and lower bound gap that is growing with p. It’s not really a weakness of the paper, but it would be nice if the authors wrote a bit about when this bound can be very tight (as a function of p, m?).

Theorem 4.3 seems to indicate a dependence on p being smaller than some constant divided by the width of the network squared, which seems to indicate for very wide networks that this result holds for p shrinking -> 0. While interesting, I’m not sure if this is the most general result and the authors don’t discuss in too much detail the implications of this, and what settings of network width lead to various results apart from n=O(1). For instance, it could be useful to assert if p is meaningful for n >> m or n ~ m, etc. There are common settings of network width with relation to number of samples or input dimensionality and at first glance I don’t know whether this theorem will give me a useful result in many of those settings. 

Additionally, this theorem trades off one assumption (convergence to a max-margin solution vs. KKT point of max-margin problem) for another (output weights are fixed as +- 1 and only the input weights are trained). They mention this theorem stems from attempting to drop the strong assumptions of the prior theorem (Theorem 4.1), but to me it seems perhaps to still include a new, strong assumption. Granted, I’m unfamiliar with the works cited in the footnote that study this setting in particular. Maybe the authors can provide a little exposition about why this setting is of particular interest and what motivates it in the grander study of neural networks?

Missing concluding thoughts, would like to see some final discussion on everything and where the authors see next steps going.

Other notes / fixes:

- Page 5, 3rd paragraph: double “we we” in the line “we we get that the total length of all…”
- Figures could use grid-lines as well as separate line-styles to differentiate the settings for accommodation to colorblind people and for printing in black and white

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work examines asymptotic overfitting behavior on a simple class of toy classification problem for two-hidden-layer ReLU nets, finding that increasing input dimension tends to push the model from tempered towards benign overfitting.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality: this is, to my knowledge, the first analysis of tempered overfitting for nets outside the kernel regime. The broader problem's admittedly decently well-studied, and the conclusions here are more or less in line with intuitions from other works, but the authors are doing something new here.

Quality: while I haven't checked the proofs, the theoretical results seem conceptually solid. While they rely on certain assumptions (e.g., assuming you've reached a KKT point or local minimum), the authors are upfront about these. I have some Qs about the experiments below.

Clarity: the paper's very well organized and written. Most things were clear on a single linear readthrough.

Significance: to my mind, the nice takeaways here are (a) the identification of a very simple toy problem for the study of these regimes and (b) theoretical results for classification that complement those of Mallinar et al. (which only study (kernel) *regression*). It's nice that the authors recover the $\Theta(p)$ scaling in their Theorem 3.2. I hadn't seen the cited results about margin maximization used in a setting like this; it's nice that they let one avoid consideration of the network dynamics entirely.

Weaknesses:
One omission here seems to be comparison to the experiment of Figure 9 of Mallinar et al, which shows that Laplace kernels basically exhibit the core phenomenon found by the paper: i.e., larger input dim pushing fitting from tempered to benign (with no need to scale dim w.r.t. dataset size).

I suppose the biggest weakness here is that this paper's results seem like exactly what you'd intuitively expect from (the much simpler model of) kernel regression, and so it's sort of unclear what new thing we've gained from this long set of calculations. (For the purposes of review, I wouldn't count this *against* the work -- and I do appreciate that establishing facts for neural nets is hard, and improving techniques there is useful.) It's also sort of unclear to me that the proof techniques used here will scale to other problems -- they seem clever but ad-hoc, and the tempered results rely crucially on the input space being 1D -- and so it's not obvious that, e.g., the specialized + expected results here will enable extension to other more general + powerful conclusions later.

The empirics seem generally good, but a bit odd in certain ways. For example, the input dimensions are very large! The aforementioned Figure 9 of Mallinar et al. suggests that much smaller input dims should be needed to see the correct behavior here. (Could be due to the difference between classifcation + regression; see note A below). A bigger oddity's that the difference in training set size between the two plots of Figure 1 is only 4x, which is quite small. It seems like the authors are trying to say that maybe ""when [input dim] >> [dataset size], fitting is benign, and vice versa,"" but if that's the message, there are much more compelling ways of showing that, including having much greater variation in both parameters or making a 2D heatmap at some fixed and intermediate $p$.

Another concern with the empirics is that converging can presumably take a very long time when you're relying on the exponential tail of the loss fn to push you. Seems worth confirming that these experiments won't change after a much longer training time.

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies the phenomenon of overparameterized neural networks (NNs) generalizing well even when trained on noisy data. Previous research focused on ""benign overfitting,"" where interpolating predictors achieve near-optimal performance. However, recent empirical observations suggest that NN behavior is better described as ""tempered overfitting,"" with non-optimal yet non-trivial performance that degrades with increasing noise levels. In this study, the authors provide theoretical justification and empirical validation, showing that the type of overfitting transitions from tempered to benign as the dimensionality increases in a simple classification setting with 2-layer ReLU NNs. Their findings highlight the intricate connections between input dimension, sample size, architecture, training algorithm, and resulting overfitting.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
This paper establishes the connection between data dimensionality and the occurrence of benign or tempered overfitting. It presents three theorems that address benign, tempered, and catastrophic overfitting, respectively, corresponding to one-dimensional data, high-dimensional data, and intermediate-dimensional data.

The study focuses on a two-layer ReLU neural network that includes a bias term, which is a more comprehensive approach compared to recent works that examined the two-layer ReLU neural network without a bias term. Additionally, a specific data distribution is considered in this paper. Furthermore, the paper explores the relationship between the bias term and the occurrence of benign/tempered overfitting.

Weaknesses:
The findings presented in this paper are built upon the outcomes (specifically, the convergence to the KKT point of the max-margin problem) obtained in previous studies by Lyu and Li (2020) and Ji and Telgarsky (2020). To be more precise, the results in this paper depend on either achieving convergence to a KKT point or making stronger assumptions, such as converging to a local optimum. Nonetheless, these assumptions are reasonable considering that numerous other papers also derive their results based on the convergence to the KKT point, such as the work by Frei et al. (2023).

It is important to note that this paper exclusively focuses on a simplistic data model, where the samples originate from a unit sphere and the labels are fixed constants, specifically +1. The discussion does not extend to other more prevalent data distributions.

Limitations:
Same as weaknesses.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper explores the spectrum of benign to catastrophic overfitting in a specific instance of a 2-layered neural network with ReLU activation when the training data has noisy labels with proportion $p$. The authors when the input dimension is , the test error is lower bounded proportionally to $poly(p)$ under different assumptions. When the input dimension is high, i.e. $d=poly(n)$, a NN overfitting (i.e. achieving perfect training accuracy) is benign and good generalization performance is exhibited. The authors also consider the intermediate  regime for the input dimension and show a gradual behavior for the error rate as a function of the noise factor.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is very well written with a great exposition and tradeoff between results and intuition.
2. Results presented seem comprehensive and I like the fast the authors do not neglect the intermediate regime and provide empirical evidence where theoretical analysis is not attainable.
3. The authors make their theoretical results accessible and explain the importance of the bias for the analysis.

Weaknesses:
1. The paper is missing a discussion/conclusion section to tie the results to real scenarios. Consider shortening one of the sections/proof sketches in favor or the above.
2. The assumptions of the theoretical results are not very realistic, specifically in theorem 4.3, the input dimension is big with respect to both m and n, this does not follow common practice of deep learning where the number of parameters exceeds the number of examples but usually the input dimension is not very big.
3. The considered architecture is rather limited, it would be interesting to explore empirically with much greater depth and not only 2 and 3.

Limitations:
The assumptions made in the theoretical setup are not very realistic, that being said, the reviewer appreciates the difficulty in obtaining such theoretical results. I would recommend that the authors discuss the assumptions and limitations in a dedicated section.

Rating:
8

Confidence:
3

";1
rUFckPrzXR;"REVIEW 
Summary:
This paper proposes a novel method for training NeuralODEs, based on synchronization and homotopy optimization. They show that the addition of the synchronization module can smooth the loss landscape, on which homotopy optimization can be applied to enhance training. The new training method achieves competitive results in convergence speed and interpolation and extrapolation accuracy when compared with other baseline methods, especially for long training data. In addition,they demonstrate the robustness of the method experimentally.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The method of homotopy optimization has been introduced into the training of various neural networks, but this paper combines the idea of homotopy optimization with synchronization, and proposes a method that is very suitable for training NeuralODE on dynamical system-related tasks.
2. The new training method can effectively improve the efficiency of network training and has the potential to alleviate a series of problems caused by irregular loss landscape.
3. Compared with other methods, this method has no restrictions on the model structure or parameters at all, so it is more general.
4. The experiments are valid.

Weaknesses:
1. The explanations in this paper are all experimental and lack rigorous theoretical support. For example, whether it is possible to analyze if the hessian trace of loss can be strictly bounded by the traditional method after adding coupling items for homotopy optimization.
2. The authors only test the proposed method in low-dimensional toy models.
3. It is mentioned in the paper that there are many stabilized models that improve training by limiting the expressivity of the model, but these methods are not compared experimentally with this new method to verify the advantages of this new method.


Limitations:
The authors have adequately addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
Training neural ODE models on long sequences of data from a dynamical system is difficult. The authors argue empirically that this is due to a poorly conditioned loss landscape leading to difficulties in optimization. To rectify this difficulty the authors use tools from the literature on synchronization (which gives conditions for when two dynamical systems will converge based on an error correction coupling term. They provide a homotopy between the synchronized dynamical system and the original one and argue that the optimization dynamics will be nice on the sychronized system and progressively less nice along the homotopy to the original system. However by gradually training the parameters and moving along the homotopy the method “ratchets up the difficulty” in a way that makes the ultimate optimization problem easier. 

They demonstrate the success of the system on a collection of commonly used dynamical systems taken from the neural ODE literature when compared to vanilla SGD and a multiple-shooting algorithm that handles the longer time horizons by breaking them up. The authors show that their method archives better accuracy, robustness, and convergence than the baselines on these problems.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The work combines the idea of synchonization with the standard Neural ODE training to improve the performance on long-horizon data. Since “long-horizon” data is not that long when training Neural ODEs, this is a welcome development. The idea of synchronization is intuitive and theoretically justified as well as empirically demonstrated to be useful through the loss landscape Hessian. The empirical results are not broad but they are deep; the authors study many variations of the problem on a small number of dynamical systems (3) against a small number of baselines (2). However, the method performs well under noise, sparsity, and long data on these and with better test performance and more quickly. 


Weaknesses:
The theoretical explanation leaves a little bit lacking. I would love to know more about the properties of the dynamical system as it is more or less synchronized. If there were traces of such in an image I think it would be intuitive. It took me a while to work out that as \lambda -> \infty any setting of the parameters is optimal and I think it might be good to explicitly say / visualize that. Any more detailed theoretical statements about this method would also be welcomed. 


Limitations:
This seems pretty general and seems to widely apply to the breadth of neural ODE architectures and dynamical systems.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors present a new method for training Neural Ordinary Differential Equations (NeuralODEs), a modeling approach that merges neural networks with the paradigm of differential equations from physical sciences. While NeuralODEs offer significant potential for extracting dynamic laws from time series data, they often suffer from long training times and subpar results. In this paper, the authors propose a training method built on synchronization and homotopy optimization. Their results demonstrate that the new method achieves competitive or even better training loss, often requiring fewer epochs compared to traditional model-agnostic techniques, and also exhibits better extrapolation capabilities.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper addresses a notable challenge in the field of NeuralODEs: the long training times and subpar results. Their proposed solution is both innovative and logical, introducing a novel concept in the NeuralODE literature.
2. The authors demonstrate the advantages of their method convincingly, showing that it improves the training loss and requires fewer epochs for training.

Weaknesses:
1. The method's extension to high-dimensional systems or partially observed states is yet to be explored, limiting its current applicability.
2. The paper could have benefited from more real-world application examples, demonstrating the utility of the method beyond the context of benchmark experiments.

Limitations:
The paper mainly uses synthetic datasets for testing the performance of training algorithms. The findings might not generalize to real-world datasets, which can exhibit unique complexities and noise structures.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes a homotopy-based training method for NeuralODE models, in particular for the case with cases with long sequence of training data.  Comprehensive experimental results demonstrated the effectiveness of the proposed method. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The proposed method is based on homotopy method, which is a classic global optimization method, with sound theoretical foundation.
2. Comprehensive experimental results. Good results when compared with the baseline methods
3. The paper explains key concepts and results well. 

Weaknesses:
1.  The argument on the loss function landscape lacks theoretical insights. The connection is established by empirical results. 
2.  Although the overall idea of using homotopy is good, some details are not properly addressed. See the questions and limitation part for more details. 
3. A few key references related to homotopy methods are not cited. The references by JH He really put homotopy methods in the mainstream of scientific computing: https://scholar.google.com/citations?user=tzM7c2cAAAAJ&hl=en


Limitations:
No negative societal impact perceived.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a new training method for Neural Ordinary Differential Equations (NeuralODEs) that aims to improve their performance in extracting dynamical laws from time series data. The proposed method is based on synchronization and homotopy optimization, which does not require changes to the model architecture. The authors demonstrate that their method achieves competitive or better training loss while often requiring less than half the number of training epochs compared to other techniques. Furthermore, models trained with their method display better extrapolation capabilities.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
[+] The paper is well-organized and clearly written, which makes it easy to follow and understand the proposed method. The introduction to the relevant background is sufficient, and the presentation of the experimental part is well-structured.

[+] The proposed method is well motivated, and some visualizations play a significant role in understanding the problems with traditional methods and the performance of the newly proposed method.

[+] The newly proposed method is very easy to integrate with models. In fact, the new method does not change the model but only changes the training process. It is simple to use and has excellent results.

[-] Considering that there are five main hyperparameters, if there are targeted sensitivity experiments in the experiments to demonstrate how the selection of hyperparameters affects the experimental results to some extent, the experimental results might be more convincing.

[-] The few systems involved in the experimental part are low-dimensional ODE systems. Can the proposed algorithm be applied to higher-dimensional problems? What new challenges might be encountered?

[?] What is the relationship between the K in the algorithm framework and the hyperparameters: Coupling strength (k)?

[?] It would be better if there is an analysis of computational complexity or a comparison of computation time with the baseline algorithm.


Weaknesses:
see above

Limitations:
see above

Rating:
6

Confidence:
3

";1
FXU4aR2uif;"REVIEW 
Summary:
The article is innovative in addressing the problem of data insufficiency by introducing the mutli-task learning method into the meta-learning paradigm. In this process, the author identify the issue of task-relatedness between heterogeneous tasks in a single episode and propose a solution called Heterogeneous Neural Processes (HNPs) for the episodic multi-task set-up.

The strengths of the article include:
1) The author clearly explains the differences and connections between the proposed method and existing works, particularly the contribution of handling heterogeneous tasks and distribution shifts.
2) The article provides impressive theoretical proofs and derivation processes of the algorithm in the appendix.
3) The code of the method is provided in the appendix, ensuring reproducibility. 
4) The ablation study is comprehensive in exploring the impact of different factors on the method's performance, which is also impressive.

The weaknesses of the article:
The number of datasets used in the experiments is limited. While, the article provides more experiments in the appendix to supplement this issue. It does not significantly affect the credibility of the article's results. It would be helpful to provide more experimental results for the classification and regression tasks if possible.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
As already written in summary, this paper is impressive considering its innovation:

1) The author clearly explains the differences and connections between the proposed method and existing works, particularly the contribution of handling heterogeneous tasks and distribution shifts.
2) The article provides impressive theoretical proofs and derivation processes of the algorithm in the appendix.
3) The code of the method is provided in the appendix, ensuring reproducibility. 
4) The ablation study is comprehensive in exploring the impact of different factors on the method's performance, which is also impressive.

Weaknesses:
The number of datasets used in the experiments is limited. While, the article provides more experiments in the appendix to supplement this issue. It does not significantly affect the credibility of the article's results. It would be helpful to provide more experimental results for the classification and regression tasks if possible.

Limitations:
The authors adequately addressed the limitations in the Conclusion part. 

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposes HNP: Heterogeneous Neural Processes, an approach that combines multitask learning with metalearning to address the problem of insufficiency of data in the multitask learning setting. The paper introduces episodic multitask learning as a way to exploit task relatedness from meta-training to meta-test episodes. HNP combines a global latent representation and a set of local parameters to model each task-specific function, with each local latent-parameter conditioned on the global representation. The inference module for prior and posterior variational distributions is implemented through a transformer-structure where meta-knowledge is instantiated as learnable tokens. Experimental results are provided for both regression and classification tasks, in comparison to other NP-based, multitask learning and metalearning methods. 

---Rebuttal---
I read the authors rebuttal and considered it, along with other reviews. I left my score unchanged since I think this already reflected my overall evaluation of the paper.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Regarding originality, the paper makes what seems to be a relevant contribution to the field in the sense of proposing a method that combines the benefits of metalearning with multitask learning for contexts where tasks are related, relying on a neural process-based approach. 

- Regarding significance, the paper shows important improvements mainly in classification tasks, compared to baselines in NP-based models, multitask-based models and metalearning-based approaches. 

- The quality and clarity of the paper are remarkable. The paper is very well-written and easy to follow, even for people not specifically into NP-based approaches.

Weaknesses:
- The results on regression tasks (Figure 4) seem to show that approaches such as ANPs and even NPs only can be quite competitive with respect to the proposed HNPs for at least a few of the tasks. Results in classification tasks, however, seem to denote much higher advantages of HNPs, especially in 1-shot learning, and considering the much larger number of tasks (or classes, in this case). This leads me to guess that the number of tasks has an influence on the amount, and quality, of sharing knowledge for HNPs. Does this imply that the method relies on the number of tasks to be large? What is the influence of the number of tasks, and the degree of relatedness of these tasks, in the way in which the local parameters and the global latent representation interact with each other, if any?

Limitations:
Limitations have been addresses. There are no potentially negative societal impact of this work.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper tackles the problem of learning tasks with insufficient data within the context of multi-task meta-learning. Specifically, the method proposed, namely Heterogenous Neural Processes (HNPs), is designed to leverage information amongst tasks in a single episode to improve performance while retaining learning from previous episodes during episodic training. The model leverages the function distributions task-wise prior to modeling each task in the episode while using a global prior and learned weights to meta-learn from previous episodes. A transformer architecture is adopted for inference in HNPs. Experiments on regression and classification tasks demonstrate the empirical improvements of HNPs over NP baselines and other methods.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- HNPs are interesting and the architectural choices in the model are well-motivated and supported with ablation studies.
- Experiments clearly establish the efficacy of HNPs in regression and classification tasks.
- Paper is well-written and uses proper notation which helps with understanding the submission better.

Weaknesses:
- Multi-task meta-learning is an interesting problem paradigm that may not possess the same applicability as meta-learning and multi-task learning do in applied settings individually. What are some examples of real-life environments where multiple tasks are needed to process during a single episode beyond the benchmarks experimented on?
- Classification performance is compared to older baselines, especially in the context of meta-learning methods. More recent baselines should be included for completeness of comparison.

Limitations:
There is a clear discussion of the potential limitations of HNPs, although there is no explicit discussion of potential negative impacts; which I strongly encourage the authors to include in the submission.

Rating:
7

Confidence:
2

REVIEW 
Summary:
Learning multiple tasks with limited data poses a significant challenge. Addressing this issue, the paper proposes a novel approach that leverages heterogeneous information across tasks by combining the benefits of multi-task learning and meta-learning. This is achieved through the introduction of hierarchical neural processes (HNPs) within a hierarchical Bayes framework. To approximate the intractable integral in the posterior, variational inference is employed, facilitated by a transformer inference module. The performance of the proposed HNPs is thoroughly evaluated on both regression and classification tasks. The results demonstrate improved performance compared to typical baselines, highlighting the efficacy of the approach. The integration of multi-task learning and meta-learning through the HNPs framework showcases its potential for addressing the challenges associated with learning from limited data across multiple tasks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The concept of leveraging meta-learning and multi-task learning to enhance neural processes is both intriguing and well-grounded. The development of hierarchical neural processes (HNPs) is supported by established theoretical frameworks such as existing neural processes, hierarchical bases, and the natural alignment with variational inference.
* The incorporation of local and global latent representations to capture and share task-specific information is a logical choice, aligning well with the Bayesian interpretation of meta-learning. To facilitate approximate inference, a transformer-based inference module is employed, allowing for the fusion of meta-knowledge and information from various tasks.
* The experimental results showcase the adaptability of HNPs in both regression and classification tasks. Additionally, the conducted ablation studies shed light on the specific components of the models that influence the overall model performance, providing valuable insights into the model's behavior.

Weaknesses:
* While the utilization of neural processes in the proposed framework is intriguing, it is worth noting that the concept of combining multi-task learning and meta-learning is not novel. To provide a comprehensive understanding, it would be valuable for the authors to explicitly highlight the differentiating factors of their proposed work compared to existing approaches, as referenced in [1]-[3]. Furthermore, including these existing methods in the experimental comparisons would enhance the thoroughness and relevance of the evaluation. This would enable a more comprehensive analysis and facilitate a clearer understanding of the unique contributions and advantages of the proposed framework.
* The proposed framework imposes a constraint that the different tasks must share the same target space. This requirement significantly restricts the applicability of the method and narrows its scope. From the reviewers point of view, the framework aligns more closely with the domains of domain adaptation or multimodal learning rather than traditional multi-task learning. Acknowledging and discussing this limitation would provide a clearer understanding of the proposed method's potential use cases and its relationship with related research areas.
* References:
  * [1] “Multi-Task Meta Learning: learn how to adapt to unseen tasks”
  * [2] “Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation”
  * [3] “Efficient and Effective Multi-task Grouping via Meta Learning on Task Combinations”

Limitations:
The authors have briefly discussed the limitation in Section 5. As mentioned in the Weakness session, the requirement of shared target space can be one potential limitation of the proposed method. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes an NP-based approach to episodic multi-task learning called HNPs that uses transformer-based hierarchically arranged inference modules to model class-specific distributions. As a result, the predictions are obtained by a dot product of the input features and the sampled class-specific latent variables, thus making the latent variables prediction-aware.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper focuses on the setting of episodic multi-task learning and proposes Heterogenous Neural Processes (HNPs) -- a new member to the NP family for classification and regression tasks.
- The authors provide a well-motivated and well-explained method that emphasizes the potential of HNPs. A good highlight of their method remains a two-layer cross-task and intra-task arrangement of latent variables where the latter variables are prediction-aware. 
- The authors adapt transformer encoder layers as inference modules for HNPs which is a good alternative to traditional MLP-based NP layers that lack ample inductive biases, particularly for classification tasks.
- The experiments and ablations look quite diverse ranging from episodic multi-task regression to classification settings.

Weaknesses:
- While the NP-based methods in Table 1 sound fairly recent, the methods used from multi-task learning and meta-learning branches remain fundamental. Can the authors look into comparing their method with more up-to-date methods from these two branches?
- The main table of results compares HNPs with a range of other NPs with and without transformer-based inference modules. Besides Figure 5, it would also be valuable for the authors to provide a comparison of the computational and/or runtime cost at inference for these models.
- It would be valuable to clarify further the working of the inference modules under a more generic classification setting. E.g., does the episodic multi-task learning setup guarantee that each training batch consists of samples from all possible classes? In particular, if a minibatch has samples from a few classes missing, how does the HNP handle this given that the prediction-aware transformer encoder layer (possibly) expects samples from classes to be arranged as a sequence?
- Can the authors comment on the saturating gap between the performance of HNPs and TNP-D in settings with a larger number of classes? It would be interesting to see an analysis of the performance comparison between the two on settings with more number of classes.
- While the authors show that the probabilistic version of the HNP outperforms its deterministic counterpart (Table 3), it would be valuable to mention how well the probabilistic HNP makes use of the prior. In particular, how the two KL values in eq. (4) vary with training, do they approach to nearly zero in the latter training stages? And if so, is the model carefully considering the prior?



Limitations:
Overall, this paper presents a valuable contribution to the field of episodic multi-task learning. However, the current empirical results may limit its impact and applicability in real-world scenarios. Therefore, the authors may want to elaborate further on their empirical evaluations,  include more recent methods for comparison, discuss their saturating performance with more number of classes, or alternatively, provide a clear explanation for this shortcoming.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces a meta-learning paradigm to perform few-shot multi-task learning. It introduces a local latent representation $\omega$, which effectively combines heterogeneous information from various tasks. Additionally, the paper suggests using transformer-structured inference modules to infer both hierarchical latents for task-relatedness and learnable tokens as meta-knowledge. The experimental results demonstrate that the proposed method achieves good performance in the few-shot multi-domain image classification task.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- the problem formulation and the proposed method is novel to the best of my knowledge.
- the proposed meta knowledge and task-specific latent representation are well motivated.
- the method is proved to be effective in the few shot multi-domain image classification task.

Weaknesses:
- Minor improvement in 5-shot setting: while the proposed method shows promising improvement on 1-shot setting across the two datasets. The performance gap diminishes quickly when scale to 5-shot setting (less than 2% improvement v.s. TNP-D). It is questionable whether the method can maintain the superiority when number of unseen samples is further increased.
- Comparison Fairness: In the classification experiment, the paper evaluates the effectiveness of meta-learning methods (e.g., MAML) by treating each task separately. A more fair baseline implementation would merge all the tasks into a single task and provide the same amount of meta-training/meta-testing data. For instance, in a 4-task 20-way 1-shot experiment setup, baseline meta-learning methods should have a 20-way 4-shot setup.
- Evaluation Metrics: The claim regarding the regression task and the smoother predictive curves with reliable uncertainty estimation from HNPs lacks quantitative support and comes across as vague. Furthermore, the experimental setup from this single example raises doubts about whether it was cherry-picked.
- Clarification: The caption of Figure 1 stating, ""different colors denote different categories"" is not clear. Instead, different shades should be associated with class categories, while different colors (yellow, blue, red, etc.) should correspond to various training examples.

Limitations:
One limitation that can be highlighted is that the proposed method requires the target space to be strictly the same across all tasks during meta-training. This requirement could potentially limit the applicability of the method in realistic scenarios where taxonomies may differ across tasks. 

Rating:
6

Confidence:
4

";1
7cnMLZvTy9;"REVIEW 
Summary:
This paper considers the problem of certifying individual fairness (IF), which is of great importance to reliable machine learning algorithms. To this end, the authors propose a novel convex relation of IF constraints that greatly reduces the computational cost. In addition, the authors propose to certify distributional individual fairness, ensuring that the neural network has guaranteed individually fair predictions for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper is technically sound.
2. The extensive experiments validate the effectiveness of the proposed methods.

Weaknesses:
The paper studies individual fairness and distributional fairness. To my opinion, the two topics seem to be independent. However, it is possible that I misunderstand this paper. It would be better if the authors can present more relations between these topics.

Limitations:
none

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies formal guarantees for notions of individual fairness (IF) for predictors given by neural network models. After relaxing common definitions for IF metrics by means of $\ell_\infty$ balls (or orthotopes), they adapt methodology based on adversarial robustness to provide upper and lower bounds to the IF achieved by models on an empirical sample - and those within a $\gamma-$Wasserstein ball about it.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper studies an important problem of individual fairness
- The first half of the paper, Section 3 and 4, which cover Background, the DIF definition, and problem explanation are very clear and easy to understand.

Weaknesses:
- The key observation and novelty in the approach is not clearly noted (See below)
- Several of the nice advantages of their method (e.g efficiency) are not explained (see below).

Limitations:
The authors mention that they do not foresee negative societal impacts. Maximizing upper and lower bounds is great but in doing so we don’t really know what is happening to the true fairness violation. It may be that the true fairness violation is in fact increasing which is propagating unfairness. While I understand that solving for this value is not feasible and thus appreciate the results presented, I would also like the paper to acknowledge that there are potential negative effects.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of individual fairness in supervised learning. The focus is on studying how to certify distributional individual fairness (IF) (individual fairness over a set of distributions close to the observed empirical data distribution) in neural networks. Prior work has focused largely on certifying global IF, which is more expensive and thus can only be applied to smaller neural networks than the proposed certification/debiasing technique. The contributions of the paper are in showing how to certify distributional IF in neural networks and then using these bounds in the training process as regularizers to debias NNs. 

The main methodology for certifying IF is presented in Section 5. The first step is to certify local IF by over-approximating the similarity ball to find a conservative estimate of the IF violation. They can then use this bound to certify distributional IF around the empirical data distribution and apply finite sample guarantees to give an estimate of the true distributional IF. 

The authors then show how to use the bounds on distributional fairness as regularizers in the training procedure as a way to debias neural networks. They then provide experimental evaluation on a few benchmark datasets that demonstrates that their proposed training method indeed improves distributional individual fairness, at relatively modest degradations in accuracy. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The main advantage is a relatively lightweight way to certify and train NNs for IF, in a way that requires little additional computation, compared to previous methods which are not able to scale to large NNs. 

The experimental evaluation seems to confirm that DIF training as proposed by the regularization method does in fact improve significantly improve IF at modest degradation in classification accuracy. 

Weaknesses:
Section 5 is a little dense and it would be helpful for the reader if there was a little more discussion of the optimization procedure, particularly in Section 5.3. Theorem statements here might also be helpful for the reader to understand what the final guarantees are. 

Limitations:
-

Rating:
7

Confidence:
3

";1
8aunGrXdkl;"REVIEW 
Summary:
This submission claims to relax the Lipschitz assumption on the gradient of the objective function in nonconvex optimisation, and obtains convergence bounds similar to textbook convergence results.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The presentation of the material is clear and the narrative flows well.

Weaknesses:
The result presented in this manuscript is not of importance. When we take the gradient information about the initialisation point into consideration, we can correspondingly restrict the feasibility set to the neighbourhood around the initialisation point, and then the $(r,l)$-smoothness defined in this paper can be replaced by the classical Lipschitz condition.

Limitations:
As there is no novel discovery in this submission, the limitation discussion is not applicable.

Rating:
3

Confidence:
4

REVIEW 
Summary:
Relaxed smoothness conditions have been introduced to study the gradient clipping algorithm, and show that clipping in particular allows fixed step-size convergence without smoothness under this relaxed assumption. This paper further generalizes the relaxed smoothness notion used for clipping by bounding the Hessian by any non-decreasing function of the gradient norm. 

Then, fixed step-size convergence is shown under the relaxed smoothness assumption in a variety of settings. The key technical part is to show that assuming a large enough initial bound on the gradients and relaxed smoothness, the gradients remain bounded throughout the trajectory regardless of the setting (convex, strongly convex, non-convex, and even stochastic with some necessary caveats). Then, boundedness of the gradients along the trajectory implies the regular smoothness condition, and thus standard analyses can be used to show convergence of gradient descent. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Shows convergence of GD, NAG and SGD for relaxed smoothness conditions, which were not known without gradient clipping. This allows to prove convergence of GD for general classes of functions. 
- Show boundedness of the gradients along trajectories under relaxed smoothness conditions. This is a nice-to-have technical result, especially for NAG. 

Weaknesses:
- Fixed step-size rates seem quite pessimistic in this case, since step-sizes all along the trajectory depend on the initial bound on smoothness, which might be very large.

Limitations:
The authors have addressed the limitations of their work adequately. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper generalizes the recently introduced (L0,L1)-smoothness which itself extends the L-smoothness which is key in analyzing rates of convergence of optimization algorithms. The authors introduce the concept of $\ell$-smoothness where $\ell$ is a function of the gradient of the function to minimize (e.g.: a polynomial).
The key idea is to show that under the $\ell$-smoothness assumption, the gradient remains bounded along the iterates of a given algorithm.
The authors focus on smooth optimization and tackle both convex and non-convex settings, in which they (roughly) recover the bounds already known for L-smooth functions and show whether they hold or not with generalized smoothness depending on the function $\ell$.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is well written, and the presentation is clear. The mathematical statements are rigorous and I did not spot mistakes. The paper builds on existing works on (L0-L1)-smoothness but significantly extends them and covers many important settings (convex/non-convex, etc.). Additionally, I found the reasoning interesting and the proof techniques used depart from the classical ones (especially in the non-convex setting). 
Overall, I think that this paper brings a significant contribution to the important question of the convergence of optimization algorithms. My general feeling is very positive.

Weaknesses:
On the theoretical side I think that there are some minor bugs (see questions below), but nothing really problematic.
However, while the paper has extensive theoretical results, it does not provide any numerical experiments, which is important for machine learning and optimization papers. It would have been, for example, very informative to see how the theory allows making GD and NAG converging beyond Lipschitz assumption by using the author's step-sizes conditions.
The related work section seems too short to me given that gradient-based optimization is a very active topic of research. For example the authors call many results as ""well known"" where instead credit could have been given.

Limitations:
Some limitations are discussed, in particular those related to NAG in the non-convex setting. The limitations about existence of a minimizer might be further discussed (see questions). 
Societal impact is not really applicable here.


Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper introduces a new assumption generalizing classical smoothness, and named $\ell$-smoothness, motivates it by giving providing examples of non-smooth functions belonging to this class, and studies classical algorithms under this assumption.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper is clear and fairly compared to related work.
- The new class is well defined and motivated, and authors show that we can obtain results under it.

Weaknesses:
$\underline{\text{General remarks}}$:

- First, I would like to make a somewhat subjective statement about the class: in my humble opinion, it seems a bit flawed as it does not respect fundamental homogeneity properties. Let me explain:

Let $f\in\mathcal{F}_{\ell}$ the class of $\ell$-smooth functions. For $\alpha, \lambda > 0$, define $g$ as $g(x)=\frac{\alpha}{\lambda^2}f(\lambda x)$ (assume wlog that their minimum is in 0, otherwise translate $f$, then create $g$).
We verify $\nabla g(x)=\frac{\alpha}{\lambda}\nabla f(\lambda x)$ and $\nabla^2 g(x)=\alpha\nabla^2 f(\lambda x)$,
hence $\|\nabla^2 g(x)\| = \|\alpha\nabla^2 f(\lambda x)\| \leq \alpha \ell( \|\nabla f(\lambda x)\|) = \alpha \ell( \frac{\lambda}{\alpha}\|\nabla g(x)\|)$.

Finally, $g\in\mathcal{F}_{\alpha \ell(\frac{\lambda}{\alpha} .)}$.

Let us assume that after some analysis of an algorithm like GD (this reasoning also applies to momentum), we find out that the step-size that achieves the best worst-case guarantee on the class $\mathcal{F}_{\ell}$ is $\gamma_{\ell}$.

Then, if I need to optimize $f$, I will use $x_{t+1} = x_t - \gamma_{\ell} \nabla f (x_t)$.

Now, instead I minimize $g$, I will use $x_{t+1} = x_t - \gamma_{\alpha \ell(\frac{\lambda}{\alpha} .)} \nabla g (x_t) = x_t - \gamma_{\alpha \ell(\frac{\lambda}{\alpha} .)} \frac{\alpha}{\lambda}\nabla f(\lambda x_t)$.

Note that $f$ and $g$ have the same minimum and if we introduce the iterates $y_t = \lambda x_t$, we have

$x_{t+1} = x_t - \gamma_{\ell} \nabla f (x_t)$ when minimizing $f$,

and

$y_{t+1} = y_t - \gamma_{\alpha \ell(\frac{\lambda}{\alpha} .)} \alpha\nabla f(y_t)$ when minimizing $g$.

In short, we need to have $\gamma_{\ell} = \alpha \gamma_{\alpha \ell(\frac{\lambda}{\alpha} .)}$, or again $ \gamma_{\alpha \ell(\frac{\lambda}{\alpha} .)} = \frac{\gamma_{\ell}}{\alpha}$.
Indeed, they both optimize their dynamics.

This shows that $\lambda$ has no impact on the optimal way to tune an algorithm. And we can stretch the function $\ell$ as much as we want and notice that probably only $\ell(0)$ matters, i.e. the smoothness constant in the optimum.
In particular, applied to the case where $\ell(x) = L_0 + L_1x$, it is clear that the optimal parameter cannot depends on $L_1$.

This observation is explained by the lack of homogeneity in the formula: when scaling a function, only $L_0$ is scaled, not $L_1.$
In $L$-smooth class, the worst-case function generally do not belong to the $L-\varepsilon$-smooth class. Hence there is a hierarchy of the classes, but here, it seems that by scaling a function, the worst-case dynamics does not depend on some part of the class definition, leading to useless specification.

This point can be further discussed. This is just some thoughts I had based on the class definition which I never used and I am aware from the related works section that some people are working with it. Thus I would be happy if authors could discuss this point.

However, based on this remark, my first guess was that the analysis would basically lead as the same analysis as when $\ell$ is constant, or $L_1=0$, which leads to my next point.

- Second, we indeed recover classical analyses everywhere in this paper. There indeed is an additional argument, that is the sequence of iterate stays in a compact and by regularity of $f$, we can bound the gradients, hence the hessians and conclude with all the classical analyses.
Indeed:
    - Th4.2 and 4.3 use classical Lyap analyses.
    - Lemma 4.1: Using cocoercivity, authors prove $\|\nabla f(x_t)\|$ is decreasing.  Using the same proof, one could have that $\|x_t-x_\star\|$ is decreasing as well. And using descent lemma, we have that $f(x_t)$ is also decreasing.
Finally, we can prove the descent lemma without assumption of $\ell$-smoothness by just assuming the continuity of the hessian, and taking G as upper bound of $\lbrace \|\nabla^2 f(x)\| | x\in\mathcal{X} \text{ and } f(x)\leq f(x_0) \rbrace$ which is assumed to be a compact since authors assume that the function tends to infinity on the border of $\mathcal{X}$. We have the revisited descent lemma: $f(x_{t+1}) \leq f(x_t) - \eta\|\nabla f(x_t)\|^2 + \frac{1}{2}\eta^2 G \|\nabla f(x_t)\|^2$, and by taking $\eta$ sufficiently small, we insure in 1 calculus that
    - 1) $f(x_t)$ is decreasing and that all the hessians keep being smaller than $G$.
    - 2) the squared gradients are sommable, hence the classical complexity $O(1/\varepsilon^2)$ that is as in thm5.1, obtained in a much simpler way.  Of course, here we only assumed the hessian to be bounded, so $\ell$-smoothness + bounded gradients do the job.

In conclusion, in my opinion, most of the results are almost straightforward from what is known in the literature.

- Third, in my opinion, the stochastic assumption A4 is too strong. I am aware authors claim that some works in the literature assume even stronger assumption, but A4 is way too strong: no multiplicative noise, only additive. This paper claims generalizing smoothness, but linear regressions with MSE losses are not even covered by the section 5.2.  Plus, bounded variance are too easy to handle in general and leads to an analysis close the deterministic case.  Instead, authors should consider expected smoothness (or its equivalent in $\ell$-smoothness). Actually, under the assumption that there is a finite number of functions under consideration, I would guess we could generalize the same arguments as in the deterministic case to ensure all the hessians to be bounded on the optimization iterates and basically use classical SGD proof in the smooth case.

$~$

$\underline{\text{Minor}}$:

- Clarity:

    - Assumption 1: First recall definition of « closed function »
    - Also define « sub-quadratic »
    - l.468: It took me a while to find where this proof was. Please do as for other propositions: state it right before the proof. In a general way, please state all theorems right before their proofs if reported in appendix. Use the « restatable » latex package too avoid renumbering.
    - Proof of lemma B1: From convexity and local smoothness, authors apply the $\underline{\text{descent lemma}}$ on the $\underline{\text{Bregman divergence}}$ of the objective function to obtain local $\underline{\text{cocoercivity}}$. The proof is the same as for global smoothness. Yet, for completness and care of the neighborhood needed to obtain the cocoercivity, I understand that the proof is provided. However, It needs a reference to this classical result in the global smooth case and mention of the 3 underlined terms when used.
        - l.550: Cocoercivity
        - l.553: Bregman divergences
        - l.562: Descent Lemma
    - l.560: please introduce y: « let y [as in the lemma statement] »
    - l.248: « Theorem 5.1 gives the classical $O(1/T)$ rate, or $O(1/\varepsilon^2)$ gradient complexity » -> $O(1/\sqrt{T})$. Authors do not clearly state if they are talking here about gradient norm or its square, but they need to be consistent when talking about rate and complexity.

- Typos:
    - l.2: ""Lipshitzness"" -> Lipschitz continuity.
    - l.76: ""strong convex"" -> strongly convex
    - l.79: $\nabla$ is missing
    - l.144: « $x = x_2 = x_t$ » ? I guess ""$x$"" needs to be removed
    - l.169: « accelearted » -> accelerated
    - l.570: 1/L -> 2/L


- Misc:
    - Table 1: missing hline + be precise on the meaning of « - ».
    - Table 1: for GD non convex: « Inf or $\Omega$ … » could be summed up as «  $\Omega$ … ».
    - l.121-122: Put 2) under 1). It should not exceed the current number of lines.

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper generalizes the classic Lipschtiz smooth gradient condition, as well as a recent improvement. The proposed condition is essentially saying the Hessian norm is bounded by a non-decreasing function of the norm of gradient. Using such conditions, the authors proved convergence rates of gradient descent under various settings (convex, strongly convex, non-convex, stochastic). Classic optimal rates are recovered.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is well presented. The technical contribution looks original and significant, as it is a generalization of the classic smoothness. Intuitions are shared for proving the convergence.

Weaknesses:
Overall, I do not have major concerns. 

1. The proof of Theorem 5.1, in particular the split of two time steps in (4) seems delicate and interesting, but also a bit out of blue to me. Is this splitting novel, or did part of it show up in the literature? What motivates such a splitting? 
2. Do you have examples of objective functions that show up in machine learning or other applications, such that they satisfy the generalized smoothness, but not the conventional smoothness? This would add much significance to the paper.
3. It was not clear to me when we assume the function to be C^2 and when not. In the introduction, It appears at first sight that only twice differentiable functions are considered. Then I saw definition 2, so it is not true. Then I got confused again at Proposition 3.3: it seems that for l-smooth to hold you need f to be C^2. Did you assume it somewhere?
4. Line 27: “provide a lower bound”. Do you mean on the number of iterations or on the error, or both?
5. Table 1: Why is it called gradient complexity? Is it the number of times the gradient oracle needs to be queried? I thought a more common name is iteration complexity, but maybe I am wrong. 
6. Lines 46-48: “if gradients along the trajectory are bounded by a constant G, then the Hessian norms are bounded by the constant ℓ(G).” Why is this “if-then” true? Say we look at a univariate objective function f: R -> R. Its gradient g is also univariate. Are you saying that if g is bounded by a constant G, then |nabla g| is also bounded? This can not be true, since nabla g can be arbitrarily large.
7. Line 77: “condition number” of what? 
8. Proposition 3.6: “If” -> “Suppose”



Limitations:
Yes

Rating:
8

Confidence:
4

";1
LIsJHQHi4z;"REVIEW 
Summary:
The paper aims at improving the KDE estimator for the estimation of density ratios or KL divergences. To do so, they introduce a weighting function $\alpha$ whose role is to reduce the bias of the estimates. A variational problem is obtained using standard derivations, allowing to learn $\alpha$ in some model class (neural nets or RKHSs) using optimization principles.

The benefits of the approach are illustrated on synthetic and real benchmarks.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The problem is of interest to the machine learning community
- The approach is novel
- The experiments section shows improvements over existing techniques

Weaknesses:
- The paper is not well written and lacks mathematical rigor (see questions)

Limitations:
Limitations have been addressed.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents an optimized weight function for kernel density estimations, which reduces bias and improves estimates of prediction posteriors and information-theoretic measures. The weight function is derived using tools from multidimensional calculus of variations and utilizes information from the first derivatives to mitigate bias caused by the second derivatives.

Experiments demonstrate the reliable bias reduction achieved by using a simple choice of a Gaussian density model for the derivatives. Interestingly, this bias reduction is independent of the accuracy of the model, addressing a common dilemma in conventional methods. Typically, using a coarse and inaccurate model results in low bias but high variance. In this case, the use of a coarse model does not affect the flexibility of the estimation procedure, while effectively addressing high-dimensional bias.

However, there are some limitations to consider, such as the computational overhead of score learning using parametric or neural network methods, and the lack of impact on the asymptotic convergence rate, which depends on the convergence rate of the kernel density estimation. Nevertheless, the use of a non-flexible parametric model consistently improves the kernel density estimation.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Content: The paper presents a novel approach to kernel density estimation using an optimized weight function. This method reduces bias and variance, addressing the traditional trade-off dilemma between bias and variance. The proposed method is independent of the model's accuracy and effectively handles dimension-dependent bias. Importantly, it does not compromise the flexibility of the estimation procedure. Overall, the paper stands out for its innovative approach, solid theoretical foundation, effective bias reduction, flexibility, and superior performance.

Formalism: The paper is well-written (up to minor details, see below), full of details, perhaps slightly too formalistic. It makes a non-trivial contribution seem easy to follow.

Experiments: Several experiments in various complex ways are being presented. 


Weaknesses:
I have no major problems with the paper. While regularly using KDEs and all tools employed in the paper, I am not sure that I know all of the recent literature. 

There are some minor things, that can easily be fixed for the camera ready version:
 - l. 23-27: Please mention here the cause of underestimation and overestimation in convex and concave regions (Why does it happen? Perhaps refer to Figure 1?). 
 - Figure 1: In this figure, only the difference at a single point x_0 is depicted. Would it be possible to include the complete representation of $\hat{p_1}$ and $\hat{p_2}$ (dotted lines)? Here or elsewhere?
 - l. 30-35 or 36: Perhaps mention in the introduction that $\alpha$ is used as the product in the kernel density estimator.
 - l. 72-73: Explain $f$ in Equation 3. The symbol was not mentioned before.
 - Fig. 2: Write out the abbreviations (VWKDE) when they are used for the first time. Perhaps even state the name much earlier, like in the introduction or in the abstract?
 - Inclusion of code would have been nice. (However, the method does not seem to be difficult to implemented and does not strongly depend on hyperparameter choices. So this is not a major drawback.)

It is typical that the supplementary material does not reduplicate the paper. ;-)


Limitations:
No limitations are being stated. I do not see any limitation myself.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper studies the weighted kernel density estimation (wKDE) and proposes a method to learn the weights.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The authors illustrate biases in the standard KDEs and propose a mitigation by introducing weights in the KDE.
2. An estimator for the weight function is provided based on minimizing a quadratic loss.
3. Improved results are shown for the proposed method on the polluted MNIST data compared to several other approaches.

Weaknesses:
1. It seems that the bias occurs only when the data x is not sampled from the true distribution p(x). In this setting, the proposed method could reduce the bias. For x sampled from p(x), the standard KDE is a kernel mean embedding which is an unbiased estimator of the density.
2. The weighted KDE is essentially equivalent to the standard KDE with a kernel specified by the integral (Eq. 2). Thus, the weight learning may not outperform the standard KDE with an appropriate kernel.
3. The learning objective (Eq. 15) is based on a quadratic function, there could be better suited losses for densities.

Limitations:
N/A

Rating:
6

Confidence:
2

REVIEW 
Summary:
The authors propose an optimal weighting scheme to reduce bias in kernel density ratio estimation that alleviates the problem of under (or over) estimation due to the geometry of the distribution. They show that employing this weighting scheme leads to better estimates of prediction posteriors. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The method is clearly motivated, and is theoretically sound (as far as I could tell).
- I am not an expert in the area and found the paper clear and easily accessible. 
- I found the problem well motivated and the idea of adjusting the weight function variationally after estimating the first and second order properties of the underlying distributions to account for the under (or over) estimation interesting and well explored.
- The empirical analysis is detailed and shows the proposed approach leads to better estimation.

Weaknesses:
The proposed method is computationally more expensive. I am curious how the performance compares between different methods given the same amount of compute resources to make the comparison fair. Also, in the experimental results that are reported, what quantity has been kept constant across methods for the comparison? The weighting function does not improve convergence rates for KDE.


Limitations:
Limitations have been discussed.

Rating:
6

Confidence:
2

";1
5r3e27I9Gy;"REVIEW 
Summary:
This paper studies how to combine parameter-efficiently tuned models in the parameter space. The authors define two kinds of Arithmetic Operation for parameter ensemble: addition and negation. They evaluate their methods for distribution generalization, multi-tasking, detoxifying, and domain transfer. Extensive experiments are conducted to support the claims.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ The paper is clear and easy to follow.

+ The paper studies an interesting topic, i.e., parameter ensemble for LLMs.

Weaknesses:
- The idea is not novel. Parameter ensemble (weight averaging) has been proposed for a while, and both addition and negation methods are included in previous works [1,2,3,4]. Especially, there is an important missing reference [1] that defines linear interpolation in the parameter space, which I believe is a more general method that encompasses both ""addition"" and ""negation"". 

- Some explorations have already been conducted before, e.g., distribution generalization [1, 3], multi-tasking and detoxifying [1], and domain transfer [4]. I think the authors should explicitly discuss the difference and novel findings compared with existing literature.

- Although the authors study the effects of many aspects, the analysis is partial and doesn't reach saturation so hard to make strong deduction out of it. I think the authors should delve deeper into the superficial findings to better understand why parameter averaging brings these benefits.

Please correct me if I misunderstood anything.

[1] Rofin, Mark, Nikita Balagansky, and Daniil Gavrilov. ""Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models."" arXiv preprint arXiv:2211.12092 (2022).

[2] Wortsman, Mitchell, et al. ""Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time."" International Conference on Machine Learning. PMLR, 2022.

[3] Qin, Yujia, et al. ""Exploring Mode Connectivity for Pre-trained Language Models."" arXiv preprint arXiv:2210.14102 (2022).

[4] Ilharco, Gabriel, et al. ""Editing Models with Task Arithmetic."" arXiv preprint arXiv:2212.04089 (2022).

Limitations:
NA

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors propose composing parameter-efficient modules (primarily LoRA and IA3 modules) directly in weight space, and show that simple linear combinations are able to achieve good performance in distributional generalization, multitasking, unlearning/de-toxifying, and domain transfer. The authors also show their approach works for both T5 and LLaMA models. Analysis of the approach suggests that their lora compositions are not as sensitive to different initializations as adapters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Straight-forward method, and well-tested across a number of diverse settings (different modules, different tasks, different underlying models). 
The paper is clear and well-structured, and the initialization experiments in the analysis section are interesting. I think the exploring merging of parameter-efficient modules is important and interesting work.


Weaknesses:
No glaring weaknesses. I think how lambda is chosen for tasks needs to be explained, unless I missed it - comparing Table 2 and Figure 2, it seems like lambda for Table 2 is > .5, so explaining how you chose the number is important. I see for the unlearning experiment you did tune lambda somewhat (Appendix A). If you have to tune lambda in all cases, this makes this approach more difficult to apply in practice due to the extra tuning required.

The domain transfer experiment in 4.5 is unclear to me - do you also train a module on the classification datasets to get theta^{amazon_cls/yelp_cls}? Looking at Figure 3, it seems lambda = 1.0 does best, which implies just using a trained classifier module directly with no composition, right? 

Results are mostly over classification tasks - it would be interesting to see e.g. the multitasking-style experiments applied to tasks more challenging than GLUE, although I think this isn’t necessary for this work. However, the GPT-4 helpfulness scores seem promising.

Using GPT-4 for ratings is still a relatively new practice, and I think it is not yet well enough established that you can rely on it without having at least some testing of how well it correlates with human ratings - if you are not careful about things like positional bias this can result in results that may not correlate well with human judgement. At the very least, it would be useful to (a) check agreement with GPT-4 over a small subset of annotations, (b) compute the variation of GPT-4 scores over a set of different prompts to ensure no glaring biases.

Overall, I lean accept for this work, and think these weaknesses are mostly around clarifications, rather than weaknesses in the method. If the authors answer my questions well and make these clarifications, I am happy to accept.

Edit: I have raised my score after carefully reading the author's response and the other reviews. My issues have mostly been cleared up by rebuttal. Please see my response to the author rebuttal for more details.

Limitations:
The authors discuss their limitations to a reasonable extent.


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes an efficient way to adapt pre-trained language models using parameter-efficient fine-tuning (PEFT). Instead of fully fine-tuning these models, the authors develop lightweight modules for each dataset, resulting in compact modules with varied skills. These modules are combined using linear arithmetic operations in the weight space, providing flexible module composition without needing extra training. This composition technique is applied to achieve distribution generalization, multi-tasking, detoxification, and domain transfer. The authors further extend this method to detoxify Alpaca-LoRA, a large language model. Empirical results suggest that this approach can create more effective modules that perform better than existing ones.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- the paper is scientifically sound and easy to read
- the idea of adapting pre-trained language models is interesting

Weaknesses:
- Without deviations and statistical tests, it is challenging to ascertain whether the model surpasses LoRA, given the close performance results.
- Utilizing GPT-4 for model evaluation could potentially introduce bias (e.g., see: https://arxiv.org/abs/2305.17493), as GPT-4 is inherently biased.
- The work appears to be an iteration of the paper from Pfeiffer et al., 2023, with restricted novelty.

Limitations:
Yes, the authors have sufficiently acknowledged the potential limitations of their work. They have considered the inherent biases or safety concerns that may be present in the Parameter-Efficient Modules (PEMs) they used. They've also touched upon the black-box nature of neural networks that might inadvertently introduce toxicity in certain scenarios.

They identified two main limitations: the restriction to identical PEM architectures and similar module initialization in most experiments, and the necessity of tuning the weight hyperparameter λ. They have also provided a direction for future work to address these limitations, which includes exploring different PEM architectures, varied module initialization, and automated computation of the weight hyperparameter.

Regarding potential negative societal impacts, the authors do not explicitly discuss this. However, they do allude to safety and bias issues inherent in the PEMs they utilize, which indirectly covers potential societal concerns. It would be beneficial for the authors to further discuss potential negative societal impacts explicitly in future work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to compose parameter-efficient finetuning modules without requiring additional training. The modules can be added to combine capabilities, or negated to remove some abilities from the model. The paper shows how different combinations of modules may be used in multiple scenarios such as out-of-distribution generalization, multi-task learning, detoxification and domain transfer. The authors also show that their approach may detoxify an instruction-tuned language model.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
While there are existing approaches to combine adaptors or parameter-efficient finetuning modules, the introduction of the negation operator allows for more complex operations.

The approach is evaluated in multiple scenarios (distribution generalization, multi-tasking, unlearning, domain transfer) and for two different types of parameter-efficient finetuning modules. It is generally reasonably effective.

The paper is fairly easy to follow. The experiments are mostly ordered in increasing order of complexity.

Being able to combine parameter-efficient finetuning modules without additional training allows for a cheap and flexible mechanism to adapt large language models.

Weaknesses:
The main weakness of the paper (in my opinion) is the lack of comparison to existing approaches (e.g. Pfeiffer et al. 2021, Wang et al. 2022 (already cited)), especially for tasks where only the addition operator is needed. While the proposed approach is simpler than those requiring additional training, it is unclear whether performance is worse, comparable or superior.

For multi-task experiments, there is a noticeable drop in performance for RTE. While this may not be surprising, this is still somewhat concerning, especially given the limited comparison to other approaches.

[Minor] Given the weight $\lambda$, the addition operator is actually a weighted average operator.

Limitations:
Yes, the authors have addressed limitations of their work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors proposed to perform an arithmetic combination of PEFT Modules. Suggested combinations were evaluated on distribution generalization, multitasking, detoxifying, and domain transfer tasks. Authors showed that combining PEFT Modules produces new modules with desired attributes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is interesting for practitioners
- Experiments are mainly well designed
- The paper is well-written and easy to follow

Weaknesses:
- For Table 1, it would also be highly beneficial to include fine-tuning results on the full dataset to understand how merging modules compares to it.
- It would be interesting to see any ablations on the design choices of the PEM Negation operator. Claims that one could not naively negotiate weights of LoRA (L120) seem unsupported by any evidence. Analysis of results with different types of negotiation would make the paper better. Furthermore, FFT negotiation from Section 4.4 implied such naive negotiation of all weights.
- While the authors explored a wide range of different tasks to understand the performance of the proposed approach, I found that the paper needed an in-depth analysis of the results. E.g., when speaking of detoxifying (Section 4.4), the only available results are the final toxicity score and PPL of obtained model. Though, there are more automatic metrics for text generation, such as a number of distinct n-grams, which add more dimensions to understanding performance. Furthermore, while discussing detoxifying, there are many fascinating things to do with negotiation. E.g., for FFT, it is explored to perform negotiation with a weight larger than $1$ (https://arxiv.org/abs/2211.12092).

Limitations:
–

Rating:
7

Confidence:
5

";1
TegmlsD8oQ;"REVIEW 
Summary:
The paper addresses the importance of a versatile model that is not limited to single modality and task and proposes a multi-modal pre-training scheme called 4M. 4M is a single encoder-decoder architecture trained on a large set of image and sequence-like modalities. The modalities including text, images, geometric and semantic are brought into a joint representation as tokens through modality-specific tokenizers. The training procedure relies on multi-modal masked training, with only a small set of tokens used as inputs and targets. Extensive experimentations showed that 4M can solve many common vision tasks out of the box, can be fine-tuned to unseen tasks as well as perform multi-modal controllable generation.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
This publication has several strengths including:

1) The writing is very clear and easy to understand.

2) The proposed approach is scalable across three key aspects - data (more training samples increase performance), architecture (improve performance with model size as well as remain stable) and training objective (handle growing number of modalities without incurring excessive computational costs). 

3) Good experimental methodology with carefully designed ablations that justifies architectural design decisions especially impacts of  input modalities and target tasks, multi-modal masking strategy as model and data scaling.

4) Very exhaustive in-depth experimentation showcasing the key capabilities - zero-shot generalization to diverse set of vision tasks, fine-tune to unseen tasks, multi-modal controllable generation.

Weaknesses:
1) The paper seemingly lacks any quantitative evaluation of its generation capabilities or comparison with existing state-of-the-art methods. I'd appreciate it if the authors can elaborate on this.

2) Another concern is that the paper lacks any discussion on the robustness of the proposed approach to the quality of the datasets, since low-quality data is usually readily available compared to high-quality data. This will be critical for data scaling as well as will align with model scaling to 4M-XL and beyond.

3) Minor comment: I am curious if the authors have performed any out-of-distribution analysis.

Limitations:
Yes, the authors discuss the limitations in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a multi-modal masked modeling pre-training scheme (4M) that unifies a several modalities – including text, images, geometric, and semantic modalities, neural network feature maps. The tokenization and masked modeling enable the efficient pretraining of 4M. The pretrained model can 1) achieve reasonable finetuned performance on vision tasks, 2) achieve conditional generation under different modalities.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
This work is a good practice on multi-modal masked modeling pre-training and achieve reasonable performance on both finetuned downstream tasks and generative tasks.

Weaknesses:
1.	This work is a combinational work of existing methods and lacks technical novelty.
-	The multi-modal masked modeling has been utilized by various existing works. E.g. MultiMAE has proven the feasibility of multi-modal masked modeling. Vision-NLP multi-modal is also explored by exsting works,e.g. MAGVLT.
-	The tokenization is claimed to be a key part for the efficient pretraining of 4M, while the tokenization has been widely used existing works, and the tokenizers used in this work are mostly borrow from other works.
-	The major contribution seems to be the the multi-modal pretrained data based on the CC12M dataset. But this is no novelty on the pseudo-labeling of the data.
2.	The performance is not impressive given the large-scale data used in this work.
-	The paper claims 4M can perform a diverse set of vision tasks out of the box, but no experimental results are showed in the manuscript.
-	In Tab.1, the finetuned performance is relatively weak compared other pretrained methods. In addition, many stronger pretrained methods are not included in this paper.
-	The conditional generation results are interesting. While there is no quantitative performance comparison with other methods such as contorlnet. Also, one drawback of 4M based conditional generation is that the model accepts fixed modal after pertraining, while controlnet can be easily extended to new modal.
-	The paper claims the pretraining efficiency is an advantage, yet no experimental result is given to prove this point.

Limitations:
The limitation of this work is mainly on the lacking novelty and the experimental results cannot well support the claims.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper presents a foundation model for a variety of vision tasks. The authors show it can perform many key vision tasks out of the box and can also be fine-tuned to achieve highly competitive performance on unseen downstream tasks and input modalities. To handle the variety of modalities, the inputs/outputs are encoded into sequences of discrete tokens, and the model is trained on all the tasks simultaneously via a multi-modal masked modeling objective.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper presents strong results and a scalable method to perform a variety of vision tasks. The ablation study covers almost all the aspects of the model. The results indeed prove the superiority of multimodal training over the baselines, without any need for augmentations. The paper is well-written and paves the way for a variety of research questions about the interactions between different modalities. 

Weaknesses:
While to tokenization method allows the model to train on a variety of tasks with a single architecture and cross-entropy loss, it also introduces quantization of the space of inputs/outputs. While this quantization does not harm the results for text, it might decrease the quality of the results for other domains (the segmentation boundaries might not be fully aligned with the objects for example). This idea induces an upper bound on the performance of such an algorithm and should be discussed. One way to evaluate this upper bound is by encoding and decoding back the ground-truth results of different domains (e.g. - quantizing the ground-truth segmentation masks and decoding them back), to verify the reconstruction quality and the downstream task-specific performance.



Limitations:
The limitations of the paper are discussed and addressed in the last section.

Rating:
9

Confidence:
4

REVIEW 
Summary:
The paper presents a unified transformer model by using an effective multi-modal pre-training scheme. The authors propose to perform masked modeling across different modalities. This is made possible by unifying the representation space of the considered modalities by mapping them into discrete tokens and then performing multi-modal masked modeling on a small subset of tokens. Experimental results demonstrate several promising results.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- This paper is technically valid and interesting. By conditioning on arbitrary modalities, the model can have great potential for a variety of multimodal intelligence capabilities.
- The authors present comprehensive experiments and ablations, providing insightful discussions. The paper can be a good reference for future researchers.
- The paper is well-written and easy to follow.

Weaknesses:
- The multi-modal masking strategy is highly similar to prior works, like MultiVAE. The mask-modeling part of this paper is somewhat less interesting and less innovative. The innovation is more in the developed system framework. 
- I don't find other significant concerns in the proposed method.

Limitations:
The authors discussed some of the limitations, but this is more like descriptions of future work. No method is developed in address the limitations.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a multimodal pre-training framework named 4M, which employs the masked data modeling style to train a transformer encoder-decoder archtecture that is capable of performing different downstream tasks. Experiments show that 4M delivers competitive transfer ability on these tasks compared with MAE / DEiT III / BEiT v2.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The baseline settings in the experiments are fair and sound, especially the self-baselines to control other variables.
2. The ablation studies provide insightful discussions on the design choices of the pre-training strategy.

Weaknesses:
1. The multi-modal and multi-task training of 4M needs datasets with all required modalities and labels. However, this kind of well-annotated dataset is hard to obtain and not scalable. This research employs pseudo labeling to extend existing image-text datasets such as CC12M. Therefore, the performance of the off-the-shelf labelers are important. The authors should provide more detailed and careful discussions and ablations on that.  
2. The downstream tasks are limited, especially considering the target of this paper, i.e. ``massively pre-training''. Quantative results on more diverse tasks / datasets should be examined, especially on the transfer ability to novel tasks.

Limitations:
The model size and the data size could be further scaled up. The authors have discussed some limitations in their paper.

Rating:
6

Confidence:
5

";1
ubp5s2tgXq;"REVIEW 
Summary:
This paper aims to uncover the semantic meaning of embedding vectors
within a given space. The basic idea is to determine
a generalized Markov boundary by computing the cosine similarity of the
orthogonality projected vectors within a subspace. The top K
candidates are then selected. Furthermore, the authors provide a theoretical
analysis of the concept of Independence preserving embedding in section
five.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- the connection between graphical model theory and explanation of embeddings seems novel (although similar ideas are present in other fields, such as the study of knowledge graph embeddings)
- conceptually, everything is well defined and formally presented
- The research question is clear and meaningful.
- The structure of this paper is well-organized. In particular, in section
two, the authors explain the necessary background information clearly

Weaknesses:
- experiments and results analysis is rather sparse, the paper has more focus on the theory and definitions
- experimental setup can be criticized (see comments below)
- related work with respect to knowledge graph embeddings could be more thorough. Several studies have focused on using projection or rotation techniques for KG embedding to predict the missing relationship between two entities [THW, SLH, SW]. Can these KG meth-
ods be adapted to uncover meaningful word embeddings?

[THW] Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, and Bowen Zhou. Orthogonal
relation transforms with graph context modeling for knowledge graph embedding. arXiv
preprint arXiv:1911.04910, 2019.
[SLH] Tengwei Song, Jie Luo, and Lei Huang. Rot-pro: Modeling transitivity by projection
in knowledge graph embedding. Advances in Neural Information Processing Systems,
34:24695–24706, 2021.
[SW] Baoxu Shi and Tim Weninger. Proje: Embedding projection for knowledge graph comple-
tion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.

Limitations:
No

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper investigates the relationship between the semantics and linear algebraic structure of token embeddings. It proposes utilizing partial orthogonality to define the ""Markov boundary"" of token embeddings. Given that token embeddings have limited dimensions and the Markov boundary can consist of numerous embeddings, the authors suggest relaxing the definition of partial orthogonality. They subsequently introduce an approximate algorithm to identify this boundary, which iteratively locates embeddings with high cosine similarity to the target vector after projecting onto orthogonal complement subspaces. To justify the effectiveness of vector space, the authors present the concept of independence preserving embeddings, which serves as the foundation for studying linear algebraic independence in embedding vectors.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The paper formally discussed the relationship between meanings of tokens and their algebraic independence.  It generalizes the idea of Markov boundary and relaxes its definition so it can be practically applied to word embeddings.

2. To validate the use of linear algebraic independence relationships between embeddings for studying their semantics, the author introduces the concept of independence preserving embedding. This concept demonstrates that embeddings maintain the independence structure of distribution, making the paper comprehensive and self-contained.

3. The authors conduct experiments using CLIP embeddings and demonstrate that their algorithm effectively identifies intriguing patterns between word embeddings, indicating that these embeddings possess semantic meanings.


Weaknesses:
1. Although the authors aim to study the independence relationship between word embeddings, they do not provide an evaluation metric to substantiate the effectiveness of the proposed method. The experimental results are presented as case studies with a limited number of words as examples, which may not be compelling for readers. A more robust experimental section would be beneficial.

2. The experiments conducted in the paper focus solely on the CLIP embedding model. While it is understandable that CLIP, being trained with visual information, may encode semantics that are more meaningful to humans, it would be interesting to explore whether the proposed method can be applied to other models that rely exclusively on text-based training.

Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
The central question (quoting the paper) is ""How to make sense of an embedding vector in relation to other embedding vectors?""  For that
purpose, the authors propose to generalize the idea of the Markov boundary to embeddings, with a relaxed adaptation of this notion to
cope with word embeddings peculiarity. The paper then introduces an algorithm to find (approximately) what is called the generalized Markov boundary for a given embedding. Empirical evaluations are carried out on CLIP.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The real scientific goal should be first clarified before one can assess the strength of this proposition.


Weaknesses:
Maybe I completely misunderstand this paper, but I cannot tell what is its scientific goal. For me everything is confused in the paper: the notion of markov boundary for vectors in the context of contextualized embeddings (like CLIP), why so much formal definitions for at the end a rough relaxation.




Limitations:
No

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper presents some theory and a method for reasoning about information gain in embedding space via a relaxation of conditional independence, as well as some theory on independence preserving embeddings.

As information gain is inherently linked to independence, the paper focuses on defining a generalization of the Markov boundary that is meaningful in embedding space. The Markov boundary is a set of embeddings that ""separate"" the target embedding from all other test embeddings not in the boundary. Concretely, this means the cosine similarity between the projection of the target and test embeddings onto the orthogonal complement of the generalized Markov boundary should be 0. The proposed generalization relaxes elementwise orthogonality to distributional orthogonality, where the cosine similarities between the projections are allowed to cancel out, rather than all be 0. This criterion is motivated by practical concerns, where embeddings are low-dimensional representations where orthogonal residuals are rare.

Finding a generalized Markov boundary for a single target embedding is then accomplished by sampling a number of random sets of embeddings, finding the top $K$ embeddings that contain information about the target given the random embedding sets, then constructing the boundary from within those top $K$ embeddings.

Separately, the paper addresses another question of how to embed a set of independence assumptions in a lower dimensional space. A theorem is presented that shows that one can preserve independence assumptions as residual orthogonality to some degree, depending on the dimension.

Experiments are presented, using CLIP embeddings, that show that generalized Markov boundaries can indeed be found, that projecting onto the orthogonal complement of the span random embeddings is meaningful, and that the discovered generalized Markov boundaries are more aligned with the span of embeddings of related words than unrelated.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The first research question of reasoning about conditional independence and information gain in embedding space is appealing. Recent works in resolving ambiguity through dialogue, such as for image retrieval through 20 questions, reason over the space of individual images. However, if there are many images, this is not scalable. Intuitively, many of those images are likely to be very similar, motivating reasoning in the much lower dimensional CLIP embedding space.
2. The proposed definition of generalized Markov boundary and method for finding boundaries are reasonable.
3. The second research question about independence preserving embeddings is also worth studying for the same reason as above: potential applications would be very interesting.

Weaknesses:
1. The method and experiments for the generalized Markov boundary only involve a single target embedding.
1. The experimental evaluation only evaluates token embeddings, whereas the text encoder in CLIP can encode sequences. An experiment involving reasoning over sequence embeddings would make the paper much stronger, especially if the experiment involved a realistic task.
1. No experiments are performed for independence preserving embeddings.

Limitations:
I did not find a discussion of limitations.

Rating:
6

Confidence:
2

";1
NBMIsOS6B7;"REVIEW 
Summary:
The authors discuss a model of no-regret learning that simulates the use of *alternating* regret updates in a two-player game. In this model, the authors show that the classical lower bound of $\Theta(\sqrt{T})$ regret does not hold, and in fact, it is possible to achieve $\tilde O(T^{1/3})$ regret.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I enjoyed reading this paper. It addresses a gap that has been observed empirically many times (the fact that alternating regret updates improves practical performance), and does so in a theoretically very nice way. The paper is also well written and easy to understand. I vote to accept.

Weaknesses:
I see no major issues, and I have only a few relatively minor questions; see the next section.

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper consider an online linear optimization (OLO) problem motivated by alternating game play in two-player games. Specifically, after the learner chooses $x^t$ and the adversary selects loss vector $c^t$, the learner suffers loss $(c^t + c^{t-1})x^t$ in the alternating OLO instead of $c^t x^t$ in the standard OLO model. The adversary in the alternating OLO model is weaker than that in the standard OLO model and the main results show that $o(\sqrt{T})$ regret is possible for the following two cases: (1) $\mathcal{O}( (\log n)^{4/3} T^{1/3})$ regret for the $n$-dimensional simplex; (2) $O(\log T)$ regret for a $\ell_2$ ball constraints (and when the agent has only 2 actions).  

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The proposed alternating OLO model is interesting and has implication for game-play in two-player games. 
2. Although privous works show $o(\sqrt{T})$ regret in some restricted cases such as unconstrained domain or when two players use the same algorithm, the main results in this paper hold for the n-dimensional simplex and against an adversary, which is stronger than privous works. The proofs of the main results are non-trivial. 
3. The paper is fairly well-written. The authors provided helpful intuition and high-level ideas in the main body, which makes the paper easy to follow. 

Weaknesses:
1. The $o(\sqrt{T})$ results for the alternating OLO model only hold for n-dimensional simplex and the ball constraints, but not general convex sets. 
2. As the authors pointed out, the current analysis is limited to the linear loss setting, and does not directly extend to general convex losses. 
3. There is no lower bound results. 

However, I would like to remark that the current results are still very interesting and are important first steps towars solving these open questions. 

Some minor comments:

1. Line 108: ""...OGD with admits...""
2. ALG2-Line 3: check ""2\gamma (c^{t-1})...""
3. Line 195: $x^t$ should be $y^t$?
4. Check line 529-530
5. Check the last inequality on page 22
6. Missing squares in the inequality on line 609
7. The references of inequalities on line 637-639, 643 are broken.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper considers online linear optimization where losses are given by $x_t^{\top} (c_t + c_{t-1})$, where $c_t$ is the adversary’s choice at each round. The authors show that the standard $\Omega(\sqrt{T})$ lower bound no long holds, and give a $O(T^{1/3})$ regret algorithm over the simplex as well as a $O(\rho \log T)$ regret algorithm over a radius-$\rho$ ball.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper introduces a new setting for online learning with constrained adversaries, and gives two algorithms with new, non-trivial regret bounds. 
The paper is somewhat easy to follow, albeit with some space allocation and presentation issues, and results appear largely correct (although the second algorithm is more difficult to evaluate due to reliance on prior work).

Weaknesses:
On the whole, this strikes me as a paper whose core ideas have the potential to be quite compelling if sufficiently extended and contextualized, but which leaves many relevant questions underdiscussed and is not yet ready for publication. 

To start, the connection between the costs $x_t^{\top} (c_t + c_{t-1})$ and “alternating gameplay“ could be better motivated; in such turn-based games, why should we think of players as experiencing costs in each round regardless of whose turn it is? Why we should consider this as our cost model for such a game rather than those enabled by e.g. extensive-form games or Stackelberg games? Both of the latter are well-studied from a regret minimization perspective and should be discussed in related work. The formulation given seems more natural as perhaps an example of a more general framework (e.g. when the adversary’s movements are bounded each round) rather than a primary focus in its own right without further contextualization within the broader game theory literature. For example, Remark 1.2 regarding convergence to NE/CCE should probably be omitted unless there is further discussion as to why these solution concepts are appropriate targets for the “alternating” setup (vs. something like Stackelberg equilibria).

Theorem 2.2 is well-known and does not need to be proved in the body; you could maybe state it as a proposition and discuss more informally, but in general, it is not particularly surprising that a specific construction for one setting fails in a more restricted setting, particularly when there are many other known settings where faster rates are obtainable against constrained adversaries (e.g. strongly convex losses).

The alternating setup considered by [WTP22] is a feature of the algorithmic setup and not directly comparable here. It might be more natural to discuss relations to the long line of work on fast convergence rates (e.g. https://arxiv.org/abs/1507.00407). Perhaps there is a deeper connection between fast rates and bounds on the per-round movement of the adversary, which appears in these works as well.

Further, the results are only given for special cases (simplex, ball), and no lower bounds are shown. It seems initially a bit strange that the rates for the simplex and the ball differ so dramatically. Why are the algorithm designs for each so different? The intuition behind the algorithms should be highlighted further, possibly in exchange for moving some proof details to the appendix. Is O(T^1/3) optimal for the simplex? Also, it is difficult to sanity-check the $O(\log T)$ result over the ball for correctness; this result indeed seems surprising for such flexible adversaries, and the technical results should be explained more clearly in the body in order to highlight the primary difficulties and innovations.

Throughout, there are also a number of typos and broken sentences (ex. the statement of “Q2”).


Limitations:
The connection to concept of “alternation” in games isn’t particularly convincing, and seems like it should be generalizable to broader settings (e.g. movement-bounded adversaries). 

Why only the simplex/ball and not convex sets? 

Throughout, there are a number of key omissions of important related work, as well as too much emphasis on restating well-known prior results.


Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a variant of the usual online learning model with linear losses, whereby the environment is constrained to produce losses of a specific form. Such a form appears naturally when studying alternating no-regret dynamics in games. The authors establish that this restriction weakens the environment, in that the decision maker is now able (at least in certain domains) to attain unconditionally a regret bound of T^1/3, breaking the typical T^1/2 lower bound of regular regret minimization.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I believe this is a technically solid paper. While I have some reservations about the relevance of the model per se (that is, assuming adversarial losses of the specific form studied in the paper, which is justified by alternation), I think that taking the model as a given the result are interesting. The paper is also well organized and it is easy to follow the flow with a good understanding of the preliminaries on FTRL and online optimization.

Weaknesses:
My only reservation about the paper is about the specific assumption on the losses in general adversarial settings. While the motivation of alternation is clear in two-player games using self-play, I would be curious if the authors could make shed light on what situations their model can model.

The reason why I think the two-player alternation connection is not super satisfying is that in two-player zero-sum games, optimistic (non-alternating) learning dynamics attain constant regret per player. Similarly, in multiplayer general sum games optimistic dynamics guarantee polylogarithmic regret per player when used in self-play. Given that optimism does not come at a computational cost, it is not obvious why, in practice, one would like to focus on alternation but not optimism.

That being said, I have a positive opinion of the technical ideas used in the paper.

Minor comments:
The paper would benefit from a pass of proofreading, as it contains a few typos. A few examples:
- L294: I assume there is a ""beyond"" missing in the sentence?
- Display after L170: the dots should probably be removed for consistency
- L189: the symbol \mapsto should arguably be a \to in this case, at least according to what I consider standard notation (see also https://math.stackexchange.com/questions/473247/difference-of-mapsto-and-right-arrow).
- Algorithm 3 italicizes keywords such as ""select"", while Algorithms 1 and 2 bold them.
- L10: I believe it should be ""regardless _of_ the strategies"", but please double check with a native speaker 

Limitations:
No negative societal impact concerns 

Rating:
7

Confidence:
4

";1
o7HckkxOZH;"REVIEW 
Summary:
Learning with rejection is an important machine learning problem. Most of the existing papers focus on the classification setting, i.e., classification with rejection and selective classification, and seldom works are targeting at the regression setting. This paper aims to investigate regression with cost-based rejection. Although some papers have studied the selective regression problem, I consider that the problem of regression with cost-based rejection is new. 

To solve this new problem, this paper gives a formulation of the expected risk and derives the Bayes optimal solution. To train an ideal model, this paper also proposes a surrogate loss function that regards rejection as binary classification and provides conditions for the consistency. Experiments are conducted to demonstrate the effectiveness of the proposed.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The problem of regression with cost-based rejection is interesting and new.

- It is quite important to give the formulation of expected risk for cost-based rejection and the Bayes optimal solution is meaningful and significant, which might serve as a pioneer for follow-up works to check whether the derived model and rejector are consistent when the mean squared error is used as the evaluation metric.

- A reasonable approach to training a good regression model with rejection is proposed and theoretical analyses are provided.

- Experimental results are significant, which supports the importance of considering cost-based rejection in the regression setting.


Weaknesses:
In Theorem 4, I notice that the authors did not introduce the concept ""classification calibrated binary classification loss"". For this concept, I also think that some references are required, e.g., [1] and [2].

[1] P. Bartlett et al. Convexity, classification, and risk bounds. JASA 2006.
[2] A. Tewari and P. Bartlett. On the consistency of multi-class classification methods. JMLR 2007.

- I also notice that the first two cited references are repeated. I would suggest that the authors should further check the details of the references.

- I also find some typos in this paper, e.g., missing a right parenthesis in Eq. (2).

- It will be interesting to give a general Bayes optimal solution for arbitrary regression losses, instead of limiting to the mean squared error.


Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper addresses problem of regression with the reject option. The authors propose the cost-based formulation of an optimal reject-option regression rule and they derive (Bayes) optimal strategy for the case the distribution is known. The authors further propose a surrogate loss to learn the reject-option regression rule from examples. They prove the the consistency and regret bounds for the proposed learning approach.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is sound and it is very clearly written.

The proposed method based on surrogate loss is simple and potentially effective. 

The authors derive theoretical guarantees for the proposed estimator, namely, they show the consistency and the regret bound.

Weaknesses:
The first contribution, i.e. formulation of the cost-based reject option regression and the optimal solution, is a known result. E.g. it is given in [41], see equation (1) of the paper. Besides [41], deriving the optimal strategy for a generic reject option predictor (of which the regression with L2-loss is a special case) is straightforward and appears in pattern recognition textbooks, e.g. Schlesinger et al. Ten Lectures on Statistical and Structural Pattern Recognition. Springer 2002.

The proposed method, based on minimizing the surrogate loss, is not compared against any baseline solution nor any existing methods for learning reject option regression. As a result, when there is no reference, it is difficult to judge about efficiency of the proposed method. The minimal solution would be to use synthetic data with known ground-truth. On real data one could use any regression model which outputs estimate p(y|x), like e.g. Bayesian methods, and plugin Bayes rule.  The author may argue that most existing methods formulate the optimal reject-option regression using the concept of selective risk and coverage [41][20][38]. However, all methods (including the proposed approach) can be compared in terms of the selective risk and the coverage which are reported by the authors anyway in the experiments (section 5) although the authors use different terminology. Namely, the selective risk is denoted as the ""accepted loss"" AL and the coverage equals 1 - rejection rate (RR). Note the cost-based formulation and the selective risk vs. coverage formulation (known as the bounded-improvement or bounded-abstention rejection models) are equivalent in the sense that both lead to the same Bayes-optimal solution, i.e. setting the rejection cost (as in the paper under review) has the same effect as setting threshold on the coverage (or the selective risk), see e.g. Franc et al. Optimal strategies for reject option classifiers. JMLR 2023. 

Minor problems:

- Regarding the experiments in sec 5, errors observed on AgeDB dataset are excessively large. The mean error ~100, reported for the standard regression model (sup), makes no sense for age prediction regardless whether the authors report MAE or L2-loss which is not clear from the description.

- The observations derived from the experiments (section 5.5) are questionable or trivial: ""(1) Our proposed method significantly outperforms the supervised reression method"" It is not clear in what sense the propsed method is better as it solves a different problem than the non-reject model. ""(2) In most cases, the average loss of our method in the accepted test instances (AL) is always smaller than the average loss of the supervised regression model""; note that this holds true for any rejection rule regardless how good the rejector $r(x)$ is. Similarly the obsevation (3) is obvious and it hold for any rejection rule.

- Line 273: ""...RcR loss (RcRLoss) decreases"" -> increases

Limitations:
yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on the problem of regression with rejection, specifically the approach of specifing a cost function and learn the pair of regressor and rejector at the same time. The paper prensents a concrete path to solving the problem. It first properly defines the problem and shows the Bayes optimal solution to it. Since the Bayes optimal solution requires knowing the expectation and the variance of the underlying distribution, it then proposes a learnable risk defined using surrogate loss function. Then, the paper theoretically investigate and show the usefullness of the proposed approach, from the perspective of classification calibration and error bound. Finally it empirically evaluates the proposed method on several typical datsets using various metrics.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality:
This paper tackles the regression with rejection problem which is of significant importance in the field. The paper solves the problem from a novel perspective and can be seen as a novel combination of several well-known techniques. This paper addresses clearly how it is related and different from related publications.

Quality:
The paper is technically sound and self-contained. Its claims are properly supported by theoretical demonstrations.

Clarity:
The paper is clearly written and easy to follow. The structual is well organized.

Significance:
The proposed method has significance to some extent, as it considers a new approach to an important problem. 

Weaknesses:
- Empricail comparison is no sufficiently conducted.
  - There is no comparison with existing methods. 
  - There is no investigation on varying cost.
  - There is no investigation on slow-start. This would show the robustness of the proposed method, since slow-start introduces a hyper-parameter to the method.
  - There is no investigation on varying training data size. Some theoretical results shows how performance would change on different $n$. It would be pursuative to show it empirically to some extent.


Limitations:
Techinal limitations on loss function are addressed by authors in appendix.


Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper explores the framework of regression with rejection, in which the model can opt to refrain from making predictions on certain instances at specific costs, with the intention of avoiding critical mispredictions. The paper determines the Bayes optimal solution and introduces a theoretically grounded surrogate loss within the framework.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is pioneering in studying the regression with rejection setting.
2. The presentation of the paper is clear and concise.

Weaknesses:
1. While the regression with rejection setting represents a fresh concept in the literature, the technical approach seems to closely mirror the standard classification with rejection setting. This resemblance potentially limits the novelty of the paper. Could the authors elaborate on the specific technical challenges encountered within this setting?

2. A definition of regressor-consistency that parallels rejector-calibration (Definition 3) is missing. 

3. The use of a supervised regression method may not serve as an appropriate and fair baseline for rejection experiments. It would be more convincing to conduct experiments comparing some straightforward rejection methods against the proposed rejection methods.

4. Additional commentary on the experimental results is required. For instance, the setup considers a range of binary classification loss functions; which among these yields the best results based on the experiments conducted?



Limitations:
N/A.

Rating:
5

Confidence:
4

";1
DNdN26m2Jk;"REVIEW 
Summary:
DiffCSP is proposed in this paper for the task of crystal structure prediction task, which generates the stable crystal structures of given compositions. The diffusion process of DIffCSP considers both lattice matrix and fractional coordinates of atoms in the unit cell. Experimental results demonstrate the effectiveness and efficiency of this method. 

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The proposed method shows good performances compared to previous methods in the task of Crystal Structure Prediction.
- The problem setting is new to ML, and interesting.
- Writing is clear with informative figures, formulas, and proofs.
- Experiments are comprehensive with additional adjustments to change the current framework to the generative model setting and compare with previous generative models. Ablation studies are provided.

Weaknesses:
- Limited novelty: the proposed method is straightforward by further considering lattice matrix diffusion; the novelty is somehow limited. The way they diffuse lattice matrix is similar to previous works for molecule conformer generation. And the way they diffuse fractional atom coordinates is similar to CDVAE.
- Other distribution invariant constraints are not considered: (1) The distribution of crystal structures may need to be SO(3) invariant instead of O(3) invariant, given the fact that chiral crystals (similar cases in molecules) are different from each other. (2) Additionally, given the diffusion process proposed, it may not have the ability to deal with the distribution periodic invariance in the following case: given a lattice matrix L = [l_1, l_2, l_3] \in R^{3*3}, the crystal structure remain the same when you change it to L = [l_1, l_2 + l_3, l_3 - l_2], and many other cases when you choose a different set of periodic patterns to describe the same infinite crystal structure. 
- Experiments about CSP: adapting CDVAE to conditional generative model and comparing with CSP model is reasonable but not that fair. But they adjust their model to the generative setting and compare with CDVAE, so it may not be a big issue.

Limitations:
They listed the limitations in the main paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
DiffCSP is a novel diffusion model for predicting crystal structures conditioned on the crystal's atom composition; it can also be extended to an initio crystal generation (sampling both atom composition and positions). Crystal modeling is challenging due to its periodic symmetry. DiffCSP proposes to solve periodicity constraints by diffusing on fractional coordinates and jointly generating the lattice and atom coordinates. Fractional coordinate diffusion is achieved with torsional diffusion that constraints each intermediate state to stay within a bounding box, i.e. a fixed unit cell. When compared with existing methods, DiffCSP is state-of-the-art across DFT and ML-based methods in predicting the crystal structures on three datasets -- Perov-5, Carbon-24, and MP-20 -- while requiring less computation. It is also state-of-the-art on ab initio generation for the same datasets.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- Using fractional coordinates to handle periodicity is supported by several prior studies on crystal structures. It is encouraging to see DiffCSP explore this representation in diffusion models. The authors do a in-depth comparison with CDVAE [1] which alternatively explored a multi-graph construction for handling periodicity.

- DiffCSP studies joint diffusion over all three components of crystal structures: atom types, lattice matrix, and coordinates. The comparisons and benchmarks will be useful to the broader community.

- The denoising model, a modified EGNN, to achieve the periodic E(3) symmetry has architectural choices that are well motivated. Each component is ablated to demonstrate their individual importances.

- DiffCSP achieves state-of-the-art in crystal structure prediction and ab initio crystal generation.


[1] https://arxiv.org/abs/2110.06197
[2] https://arxiv.org/pdf/2102.09844.pdf

Weaknesses:
- The authors claim DiffCSP is the first generative model for crystal structure prediction (CSP). I am curious to know why a generative viewpoint of CSP is beneficial. Discussion of why diffusion is important for this task would help, i.e. a discussion about uncertainty in the problem and how it manifests.

- The benefit of diffusing over the lattice could use more explanation. The ablation seems to show there is marginal benefit of diffusing the lattice. It is also not clear to me if gaussian diffusion is the right choice for the lattice vectors which can also be parameterized with 6 unique, rotation invariant parameters using the Niggli algorithm [1].

- DiffCSP is E(3) and periodicity equivaraint however CDVAE is SE(3) and periodicity equivariant. Perhaps I missed it but I do not see discussion on the subtley between E(3) and SE(3). It would be important to note this difference and include it in the discussion.

- A major consideration when performing joint diffusion is the noise schedules used for each diffusion process. A discussion of whether this was important practically would be helpful in the main text.

 

[1] https://pubmed.ncbi.nlm.nih.gov/14691322/

Limitations:
- A potential limitation is SE(3) vs. E(3) equivariance. The authors do not discuss this. I have mentioned this already in the questions.

- The authors do not discuss choice of diffusion hyperarameters which I believe would have lots of practical use for researchers who would build on top of this work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper addresses the crystal structure prediction problem, one of the important problems in this field.
The authors propose a diffusion model generating both lattices and fractional coordinates given chemical composition.
The proposed denoising model satisfies O(3) equivariance and periodic translation invariance.
The experimental results show that the proposed method outperforms the conventional methods in generating existing crystals.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper is well-organized and well-written.
* The representation of crystals is reasonable.
* The empirical result is strong, and the baseline methods are appropriate for the CSP task.
* The results compared with USPEX, a strong GA-based method,  attract the CSP community's interest.
* Ablation studies support the design choice of the proposed method.

Weaknesses:
The study does not report any new crystal structures. It will increase the impact of this paper if the proposed method can find a new crystal that achieves lower energies via DFT than that of the known crystals of a given multi-component system.

A minor comment

""$\psi_L$ in Eq. (10)"" in the main text of section 4.3 seems to be a typo in Eq. (9).

Limitations:
Yes, the limitation is discussed in Section 6.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies crystal structure prediction, an important topic in the field of AI for scientific discovery. Diffusion models are designed considering necessary symmetries for crystal study, with diffusing lattice vectors and fractional coordinates for generating lattices and atoms, respectively. Experiments show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper presents novel and shining ideas, e.g., considers fractional coordinates rather than Cartesian coordinates. This eliminates the need of constructing multi-edge graphs and naturally guarantees periodic invariance. The theory for processing fractional coordinates in the diffusion model looks rigid and solid.

2. The joint learning fashion for lattices and coordinates should be the right way to go in the field.

3. Experiments are solid and performance is impressive.

Weaknesses:
1. One major concern is-- the technical contribution is not clear or strong enough. Diffusion models for 3D scientific data ate not new. The authors may need to clearly state the significant and unique design of the proposed diffusion model to distinguish it from other diffusion models.

2. Another concern is-- necessary symmetries are not completely considered. For example, similar to CDVAE, translation invariance (not periodic translation invariance) may not be considered.

3. A key foundation is-- the consideration of fractional coordinates can achieve periodic invariance, which is roughly correct. However, if the three lattice vectors are not defined in a deterministic way, this may not be true.

4. There are some issues in the organization. For example, much content in 4.1 overlaps with ref [36], and this content should be put in background/preliminary and credits should be given properly.

Overall, this is a neat work. I would raise my scores if my concerns are fully addressed.

Limitations:
N/A

Rating:
5

Confidence:
5

";1
mbaN0Y0QTw;"REVIEW 
Summary:
This paper considers to adaptively determine inference time steps of spiking neural networks to improve the tradeoff between accuracy and time. Two methods are proposed. The first one uses confidence score thresholding. The second one introduces an additional policy network to predict the number of timesteps by reinforcement learning. Experiments show the effectiveness of the proposed methods for direct SNN training and ANN-SNN conversion methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper considers early-exit of spiking neural networks, which can effectively improve the efficiency of SNNs. The idea of dynamically determining inference time steps based on the difficulty of inputs is interesting.

2. Extensive experiments on static and neuromorphic datasets as well as qualitative assessment are conducted.


Weaknesses:
1. Dynamic inference time steps of SNNs, especially based on confidence scores, were also explored by (possibly concurrent) recent works [1,2], which can be discussed. The idea of confidence score is very simple and straight-forward.

2. For SEENN-II, an additional policy network is required for inference, and it is unclear how this can be deployed. From the descriptions (and codes), the policy network is an ANN rather than SNN, so it is not compatible with the main SNN for hardware deployment. This poses challenges and also arouses questions of why considering such hybrid architectures. Additionally, SEENN-II seems not to be flexible, i.e. after training it cannot make tradeoff between accuracy and time/energy.

3. From the descriptions, it is not clear enough whether the energy consumption estimation of SEENN-II consider policy networks. This should be included and more detailedly discussed. For example, on ImageNet, is the policy network an ANN ResNet-34 (as shown in code)? Then it may consume more than the SNN part. The energy result on ImageNet is missing.

[1] Wu, D., Jin, G., Yu, H., Yi, X., & Huang, X. (2023). Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff. arXiv preprint arXiv:2301.09522.

[2] Li, C., Jones, E., & Furber, S. (2023). Unleashing the Potential of Spiking Neural Networks by Dynamic Confidence. arXiv preprint arXiv:2303.10276.


Limitations:
The authors do not discuss limitations and societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes an accuracy-efficiency tradeoff method by adjusting the number of timesteps in SNNs, which is new and interesting. The authors accomplish this idea with two methods that uses a confidence score thresholding and reinforcement learning. These methods can be applied to directly training SNN and the ANN-SNN both, and results show these methods can save energy greatly but with negligible accuracy decreasing. I like the work that has potential and would provide a new direction for the following SNN work. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. well-written, easy to read.
2.The idea to adjust the number of timesteps to balance accuracy-efficiency is new and interesting.
3. The method is simple and easy to follow.
4. Experimental results are really good. The method can keep similar performance while reduce cost.

Weaknesses:
To show the effectiveness of the method. A similar timesteps for other methods should be provided, for example, 2 or 3 timesteps for TET on ImageNet.

Limitations:
1.I find no potential negative societal impact.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper introduces a new inference scheme for spiking neural networks (SNNs), the early exit on the time dimension. To make sure that relatively easy images can be predicted with less number of timesteps, this work build two frameworks to identify the appropriate timestep to minimize the latency while maintaining decent performance. The first method is an on-the-fly approach and the second one is more complicated that requires finetuning.  The authors test their SEENN on various recognition benchmarks and obtain quite good accuracy with even lower number of timesteps compared to existing papers. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Compared to other SNNs papers that focus on either conversion and training, what is work studied is a universal approach (i.e. time) that can be applied to any types SNNs.
2. The methodology is technically sound and covers different user resources (whether finetune or just plug-and-play)
3. Experiments results is thorough and solid and verify the effectiveness of this approach.

Weaknesses:
1. There is no empirical evidence showing that the confidence score will increase along with the number of timesteps. The authors are suggested to add a figure to show the confidence score evolution.

The above limitations has been explained in the rebuttal.

Limitations:
see above.


Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel manner to improve the efficiency of the spiking neural network. Specifically, the SEENN is proposed to determine the appropriate number of timesteps, which therefore reduce the inference time-cost. The proposed method is evaluated on CIFAR-10 and Imagenet, achieving good performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The main contribution of this paper is to treat the number of timesteps as a variable in SNN model. Accordingly, several variations of early-exit manner is deigned for better accuracy-efficiency tradeoff.
+ The paper writing and organization is good.

Weaknesses:
- The proposed method for determining the best timesteps is not novel. The first manner is to simply set the right number with the confidence score, while the second one introduces a policy network for better prediction. Although above two manners could work, these would inevitably introduce extra computation cost or human-based prior, which is not expected. In addition, the proposed SNN model still rely heavily on the ANN backbones, e.g., Resnet or VGGnet.
- As for the hardware efficiency, the adopted nvidia V 100 maybe not proper, which is not designed for SNN application.
- The ablation study is not clear, it seems the SEEN adopts different backbones with SNN.

Limitations:
None.

Rating:
6

Confidence:
3

";1
PJhjkSFlbG;"REVIEW 
Summary:
The paper proposes a novel way of learning how to adapt policies in contextual RL. The work proposes to use ""adapter"" modules whose parameters are determined by a hypernetwork that solely focuses on the context variable. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is generally easy to follow and well written. Illustrations are well placed and utilized to communicate the content of the paper and to emphasize the concepts that are being discussed.
The work also addresses an important issue for RL, namely generalization, by combining existing ideas in clever ways to tackle this issue. I particularly commend the authors for the discussion of the limitations and the experiments that are conducted to explore the limitations. Overall, the presented method ""adapter"" seems well suited to be used for training more general RL agents. As such I believe the work is of high significance for the RL community and should inspire future works.

Overall I think this paper is a clear fit for the NeurIPS (RL) community

Weaknesses:
The presentation could be improved a bit. For example:
* The title does not seem to fully fit with the work, or rather, it is overly general and gives the impression that a novel cRL formulation will be presented.
* Similarly, the abstract only spends ~1/3 of the space discussing the ""Decision Adapter"" method
* The wording in line 45 makes it seem like context could not be observed by a sensor. However, the robot mass example would be a sensable context.
* The restriction on only changing dynamics and not changing rewards seems not really needed.
* In line 115, Procgen is cited for examples where context unaware policies can work well, but I'd argue that the context is embedded in the visual state representation
* Theorem 4.5 can only be understood when reading the Appendix. This is the case due to $\mathcal{C}\_{far}$ and $\mathcal{C}_{close}$ never getting defined in Section 4 but only in the appendix.
* Similarly, its easy to overlook why the inequalities in Definition 4.3 make sense, since it depends on the reward definition. 
* The theoretical results from Section 4 seem a bit arbitrary since they heavily depend on the construction of the example. It might make sense to move this part to the appendix and move some more of the empirical evaluation to the main paper.
* The AER metric is introduced but ""AER"" does never show up again in the paper. Is it maybe labeled as ""Mean Eval. Reward"" in the plots?
* It is not clear why AER can not be defined over a context set that is distinct from the training set
* The citation to Benjamins is by now a TMLR publication



There is one more niche paper using cRL for learning dynamic configuration of AI planning algorithms that should be of interest to you:
* Biedenkapp et al. (2022) https://rlplab.com/papers/biedenkapp-et-al-icaps2022wsprl.pdf (follow up to Speck et al. 2021 https://ojs.aaai.org/index.php/ICAPS/article/view/16008)

This work discusses how to treat state and context information separate and also discusses different architectural choices that one can make in this regard. I don't think this would form a reasonable baseline to compare against, however it feels like an appropriate reference. In particular, it feels like a good reference since the work considers 300+ noisy context features and how to learn with them, compared to the relatively few context features considered in the present work.


While I listed many bullet points for potential improvement, I am very happy with the state of the paper and I think it should be accepted. I am even willing to increase my score if code will be released.

Limitations:
Limitations are discussed thoroughly and experiments are provided that support the discussion on limitations.

Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper addresses the problem of learning policies that can generalize to multiple contexts. The problem is formulated as a contextual MDP (CMDP). The paper starts by analyzing analytically the performance of policies that are not aware of the context in two different scenarios of a particular instance of CMDP. After that, a network architecture (decision adapter), which incorporates contextual information through a hypernetwork, is introduced. The architecture is empirically evaluated in three environments and its performance is compared against 4 baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper tackles an important problem in RL. The idea of using a hypernetwork to incorporate contextual information is sound and the empirical results are promising. The paper is very clear and easy to follow.

Weaknesses:
I believe that the paper's novelty is somewhat limited. While I found the solution of using a hypernetwork quite neat, I was slightly disappointed to discover that a similar architecture called cGate had already been proposed to address the same problem. The cGate architecture is mentioned for the first time in Section 5 and is not introduced as a related work.

The authors argue that the adapter architecture is a more powerful generalization of cGate as it can also capture non-linear relationships between states and contexts. This is confirmed by the empirical results obtained in the ODE domain. However, the results of cGate on the other two environments (CartPole and Mujoco Ant) are not reported. Moreover, the ODE domain seems to be specially designed to make cGate fail, as the dynamics are given by an nth-degree polynomial (equation Line 229).

Finally, I found the example provided in Section 4 to be very helpful for understanding. However, in my view, framing the analysis o as a theorem is not appropriate in this case, since the applicability of the results seems limited to this specific instance of the problem. Typically, a theorem should offer broader insights into the overarching problem at hand.

Limitations:
The limitations of the approach are discussed extensively in Section 8 and Appendix G.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies how to better incorporate context (such as MDP-specific parameters) in context-conditioned robot policies. Traditionally, this is done simply by concatenating the context vector to the state vector and processing both inputs simultaneously with the policy network. This work proposes a context-conditioned hypernetwork to output the weights for an adapter module, which adds a residual value to the policy’s intermediate representation. Along with other proof-of-concept tasks, the paper evaluates on cartpole (varying pole length) and mujoco ant (varying mass) and finds that the proposed method is more robust to distractor dimensions but does not necessarily perform better than naive concatenation methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors study a problem that is important in multitask learning/generalization such as with respect to language/goal-conditioned policies.

The paper is well-written and proves some interesting theoretical results. Empirically, experiments test generalization in a meaningful way. For instance, the evaluated mass ranges are not within the training ranges in the ant environment, which is a challenging problem in robotics.

Weaknesses:
(1) Section 4 talks about performance guarantees for a context-unware policy. Why is this important to the rest of the paper, which is about how to best incorporate context into a policy for dynamics generalization? Furthermore, I don’t see the link between Lines 154-163 (talking about how context-dependent rewards are equivalent to context-dependent dynamics in a context of matrix rotation example) to the rest of the section or the rest of the paper.

(2) The abstract promises to “establish conditions for when it is necessary to incorporate context,” but when reading the text, I did not see any section of the paper that brought this out explicitly.

(3) The work would really benefit from architectural ablations, especially when the abstract said the paper would “examine how to best leverage this external context information to improve generalization.” Hypernetworks are not a common architectural component in context-conditioning, so were alternatives not involving hypernetworks considered? For instance, it would be good to compare with the following architectural results:
(i) The proposed approach, with no hypernetwork, where the adapter module (a simple MLP) only takes the context as input.
(ii) Something like cGate, but with an elementwise product with the context embeddings at every layer, instead of just at one layer.

(4) Figures 4 and 5, which contain the most difficult domains examined by the paper (ant and cartpole), show that in the 0 distractor dimension case, naive concatenation performs roughly the same as the proposed method (adapter). This means that in this case, naive concatenation is preferable due to its simplicity and training speed relative to the proposed adapter. It is only in the differential equations domain, a non-control task, where adapter clearly outperforms naive concatenation.

Limitations:
Authors were upfront of the limitations of the adapter to noisy contexts and its tendency to overfit when trained on narrow context ranges and had a very detailed limitations section.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents an architecture that uses a hypernetwork to learn network parameters for a context-dependent policy adapter. The authors discuss an illustrative example that emphasizes in what cases a universal (context-dependent) policy is necessary, and in which cases a non-context-conditional (""unaware"") policy is sufficient. In the experiments, they compare their adapter approach with a context-unaware policy, a policy that concatenates context information with state and processes them equally, and other approaches using a linear policy adapter (FLAP and cGate).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea to hyper-learn a nonlinear context-dependent adapter as the final few layers of a context-general policy is original and makes sense. The paper is well written, and experiments are quite thorough. The relatively detailed ablation experiments, some of which are found in the appendix, are quite interesting and a strength of the paper, for example the study of nuisance context parameters in section 7.2. I also think that the distinction between extrapolating and interpolating generalization is important and rarely seen (presumably because this distinction is usually hard to make in more complex environments), and interesting enough to maybe move it to the main part. General policies are an important and active research field, and the paper makes a significant contribution.

Weaknesses:
I appreciate the illustrative example for unaware vs aware policies in section 4, but I feel like this is quite intuitive for many readers, and could potentially be shortened a bit in favor of more experimental results in section 5. Also, I was a little confused by the section title being ""Theoretical Foundations"".

I think that the related work section should probably discuss recent advances in foundation-model-style generalist agents (e.g. https://arxiv.org/abs/2203.11931, https://arxiv.org/abs/2305.10912, https://arxiv.org/abs/2301.09816, https://arxiv.org/pdf/2306.11706.pdf) as well. The authors briefly touch on ""learning some unsupervised representation of the problem and inferring a latent context from a sequence of environment observations"", which is similar in spirit to this line of work, but then only cite one reference. Considering that this field has been quite active lately, it probably would make sense to discuss this briefly.

Generally, I think the paper would benefit from experiments in a larger and more diverse number of experiments. However, I think this is at least partly compensated by the fact that the authors report relatively detailed results for the two (or three) environments they consider.

Minor comments:
- The shaded areas in Fig 3 and elsewhere are very hard to read

Limitations:
The authors discuss limitations in quite some detail in section 8, and underpin this with additional experiments in the appendix. This is a strength of the paper.

Rating:
7

Confidence:
3

";1
v6YzxwJlQn;"REVIEW 
Summary:
The paper proposes a method combining Fourier Neural Operator (FNO) with the Deep Equilibrium Model (DEQ) for more efficient operator learning under noisy conditions. Overall, the paper is well-written with clear methods and theory, and some effective experimental results. It's a decent piece of work, and I recommend a weak acceptance. However, I believe there are still areas in the paper that could be improved.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The paper is well-written, with a clear presentation of the method and theoretical underpinnings.

2. The experiments show some positive results, demonstrating the potential utility of the proposed approach.

 

Weaknesses:
1. The paper lacks a thorough literature review. It hardly mentions operator learning methods based on attention, which have demonstrated significant advantages in many areas, such as Navier-Stokes equations and problems on irregular geometric regions. Therefore, I believe the authors should add some references [1,2,3] to cover these works.

2. Each experiment in the paper compares the effects of adding noise to either the input or output. However, the improvements observed in noisy settings do not seem to be as significant as those without noise. This makes me wonder why the authors considered these experiments necessary.

3. Additionally, as a new model structure, DEQ should be able to be combined with many other approaches, such as DeepONet or other neural operators. However, the paper seems not to mention this possibility and only tries to combine it with FNO. It would be interesting to see an exploration of the potential of DEQ in combination with other models.

References
1. Transformer for Partial Differential Equations' Operator Learning (https://arxiv.org/abs/2205.13671)

2. GNOT: A General Neural Operator Transformer for Operator Learning (https://arxiv.org/abs/2302.14376) 

3. HT-Net: Hierarchical Transformer based Operator Learning Model for Multiscale PDEs (https://arxiv.org/abs/2210.10890)


Limitations:
None.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper proposes two weight-tied neural network architectures for solving steady-state partial differential equations (PDEs) using the universal approximation capabilities of neural networks. The first architecture is a weight-tied version of Fourier Neural Operators (FNO), while the second architecture is a Deep Equilibrium Model (DEQ) that uses black-box root finding algorithms to implicitly train the model. The paper shows that both architectures outperform existing methods on benchmark problems and can be used to learn efficient solvers for PDEs. The contributions of the paper include the introduction of the weight-tied FNO and FNO-DEQ architectures, as well as the demonstration of their effectiveness in solving steady-state PDEs.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The main strengths of the paper are:
1. The paper proposes a new architecture called FNO-DEQ that uses weight-tied neural network layers to solve steady-state partial differential equations (PDEs). The architecture is based on the observation that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator.
2. The paper shows that FNO-DEQ outperforms other non-weight-tied architectures with 4x the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes.
3. The paper demonstrates that FNO-DEQ and weight-tied architectures are more robust to both input and observation noise compared to non-weight-tied architectures, including FNO.
4. The paper leverages the universal approximation results of FNO to show that FNO-DEQ can universally approximate the solution operator for a wide variety of steady-state PDE families.

Weaknesses:
Some potential limitations of the paper are:
1. The proposed FNO-DEQ architecture may not be applicable to all types of PDEs, as it is specifically designed for steady-state PDEs. Further research is needed to explore the effectiveness of weight-tied architectures for other types of PDEs.
2. The paper focuses on the empirical performance of the proposed approach and does not provide a detailed theoretical analysis of why weight-tying is effective for steady-state PDEs. A more rigorous theoretical analysis could provide deeper insights into the underlying mechanisms of the proposed approach.
3. The paper does not compare the proposed approach to other state-of-the-art methods for solving steady-state PDEs, such as finite element methods or spectral methods. A more comprehensive comparison could provide a better understanding of the relative strengths and weaknesses of different approaches.
4. The proposed approach may require more computational resources than other methods, as it involves solving for the fixed point of an implicit operator layer. This could limit its scalability to larger or more complex PDEs.

Limitations:
Refer to the weaknesses section.

Rating:
5

Confidence:
1

REVIEW 
Summary:
This research examined the solution of steady-state Partial Differential Equations (PDEs) using Fourier Neural Operator (FNO) based architecture. The author introduced a fix-point iteration mechanism into the FNO framework, leading to the proposal of weight-tied FNO and FNO Deep Equilibrium (FNO-DEQ) models. Comparative analyses revealed that these newly proposed architectures outperformed the traditional FNO when solving standard Darcy Flow and Navier-Stokes Equations.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper is well written, providing clear and rigorous mathematical definitions of the problem, its underlying theory, and the proposed solution.

The innovation of incorporating the fixed-point iteration mechanism (via the contraction mapping theorem) into the Fourier Neural Operator (FNO) is novel and captivating. The results clearly demonstrate a significant improvement when applying this technique to steady-state Partial Differential Equations (PDEs).

Further enhancing the strength of the paper, the author proves a universal approximation theorem for the FNO Deep Equilibrium (FNO-DEQ) model, which assures the boundedness of the approximation. This theoretical validation lends additional credibility and rigor to the findings.


Weaknesses:
The paper doesn't provide any loss versus training epochs data, which would allow us to assess whether the training had indeed reached convergence (as well as to compare the speed of convergence). While it's assumed that convergence must have been achieved for the results shown in Tables 2 and 3, the absence of this specific data inhibits a more comprehensive understanding of the model's training process.

Limitations:
The author focused only on the standard Darcy flow and Navier Stokes problem, which the classical FNO already works well. An immediete question is that if the FNO-DEQ can be applied to more challenging PDEs (for example 3D Navier Stokes), to show its effectiveness, where classical FNO struggles. 

Rating:
9

Confidence:
5

REVIEW 
Summary:
The paper tackles the problem of solving steady-state PDEs with weight-tying FNOs. The authors argue that instead of stack multiple FNO layers with different parameters, repeatedly applying one FNO layer computation is a better choice. This hypothesis is motivated by the fact that steady-state PDEs are solved for fixed-point solutions, where evolving PDE further will not change the solution. Moreover, instead of directly stack the same FNO layer and hope the fix point can be reached, the authors propose to use more advanced fixed-point solving method (FNO-DEQ). By using the fix-point properties, DEQ can have smaller training memory usage but is slower. Experiments are conducted on Darcy flow and naiver-stokes equations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well structured,
- The proposed method leverages well the fact that the solutions are fixed points.
- Experimental results look good. 

Weaknesses:
- There are very few discussions on the convergence of the fixed point solution. For example, if we apply the FNO layers more times than during training, would the solution remain the same?
- There are two major differences between FNO-WT and FNO-DEQ. The first is in forward pass FNO-WT directly applies the network multiple times while FNO-DEQ use fixed point solver. The second is in the backward pass FNO-WT directly propagates through the computation graph while FNO-DEQ use implicit gradients. These two components seem independent to each other. For example, since FNO-WT in some sense also solve for the fixed point solution, can we use the implicit gradient to train FNO-WT? 

Limitations:
None

Rating:
7

Confidence:
3

";1
iWWLgcUTZU;"REVIEW 
Summary:
This paper proposes  PCF-GAN, which aims to improve the effectiveness of the discriminator in differentiating the time series distributions by utilizing the path characteristic function (PCF) as a principled representation of the time series distribution within the discriminator. The authors also give the theoretical foundation of the PCF distance, proving its properties that ensure stable and feasible training of PCF-GAN. Extensive experiments show the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The newly proposed metric in the discriminator for handling sequential data is interesting. Also, the author provides theoretical evidence for the analytical properties of the proposed loss metric.


Weaknesses:
- The presentation of this work is kind of confusing, making it difficult to fully comprehend the content. Specifically, in Figure 3, it is unclear what the generator loss represents if it is computed on the embedding vector. Additionally, it is unclear why the generator and regularization loss can enhance the discriminative power in Equation 10. Clarifying these help audience better understand. 
- The related work section consists of only one paragraph, which may overlook some relevant research in the field.
- The ablation study of every component is required to demonstrate the effectiveness individually. 
- Regarding the sequential data, I am wondering how does this method work well on more challenging domains like video data since videos are also sequential in nature. 




Limitations:
Please see Weaknesses

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper looks at generative models for times series data. It proposes a new GAN method, based on a novel discriminator. The path characteristic function (PFC) is used as a representation of the time series distribution. Using this, a distance between two distributions is defined (PCFD) as well as a way to approximate it (EPCFD). Afterwards, it is shown how EPCFD can be used as a discriminator in a GAN scenario (resulting in the PCF-GAN). To scale it to larger input dimensions, the authors follow previous literature and introduce a parameterised dimensionality reduction of the inputs and introduce different losses in an effort to ensure that it behaves as expected. Finally, they conduct a number of experiments, showing that PCF-GAN compares favourably to competitive time-series GANs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method and its different components was well motivated and presented.
The paper presents a compelling theoretical motivation for the method, giving evidence on why it might be preferable to alternatives s.a. WGAN.
The experimental results are well analysed and presented.


Weaknesses:
The reconstruction functionality is not motivated, nor explained. It can be unclear to the unfamiliar reader why one would want it, given that we can just generate images.

Related work - similar work is only lightly touched upon and the advantages/disadvantages are not outlined, making it harder to compare.

The loss function in Eq. 9 is not arrived at in a principled way and has two hyper-parameters which might be difficult to tune?

In the experiments, the dimensionality of the inputs, as well as the length of the sequences appears rather low. This suggests potential scalability issues?



Limitations:
The method has computational limitations which limits the number of dimensions which it can be applied to. As mentioned before, an embedding functions is trained to reduce the dimensionality of the inputs, however, if the effective dimensionality is high, the method would fail.


Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper proposes an approach to improve time-series modeling in Generative Adversarial Network framework by using path characteristic function as the embedding of the time series sample. It explains the feasibility of PCF distance and how to integrates it as a distance measure between two time series data in order to help the discriminator learn better generative features. The goal and benefit of the proposed method is to empirically improve the generation quality of time series data. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper explores using a novel loss function in Generative Adversarial Network framework. It computes the proposed empirical path characteristic function distance between generated and given sample to learn better time series generative features. The proposed method outperforms other generative baselines on the given datasets. The paper is well written ( and includes the codebase in appendix for reproducibility).


Weaknesses:
It would be great to have to following experiments in order to compare fairly with the existing baselines:

1. Comparison with existing baselines like COT-GAN on metrics like 
(a) the sum of the absolute difference of the correlation coefficients between channels avg over time 
(b) absolute difference between the correlation coefficients of real and generated samples.

2. Comparison on high dimensional dataset (like Sprites and human action sequences with FID, KID scores )


Limitations:
As mentioned in the weakness section, having more ablations and experiments for comparison would help the reader better understand the efficacy of the proposed approach.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes a path characteristic function GAN, named PCF-GAN, for learning to generate time-series data. More specifically, the authors mainly employ the rough path theory to build the PCF distance, such that the temporal cues can be encoded by unitary features, enabling the PCF to learn sequential data. The PCF distance is then proved to be complete and favorable for minimizing the difference between two stochastic processes. Even though, an encoder-decoder-based embedding is proposed to compare two random processes in the latent space of reduced dimensions, catering for the ease of training in practice. Experimental results have also verified the effectiveness of the proposed PCF-GAN.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The established theory on PCF is novel and complete, by far my understanding. The authors also propose a practical way of minimizing the PCF distance, by using an encoder-like embedding that is able to reconstruct signals. 

Weaknesses:
1. I am appreciated the theory behind the PCF. However, the usage of path theory needs to be clarified, when aiming at improving characteristic functions to represent stochastic process. The authors claim that CF-metric fails to capture temporal dependency of sequential data, which is true. However, why using path theory on characteristic functions can well address the temporal dependency, given the fact that there are other representations of the characteristic function of a stochastic integral?

2. The authors claim that the embedding operation does not have any gradient constraints. However, when adding calculating the EPCFD after the embedding layers, the continuous and differential properties may not hold, without the restriction on the embedding layers. I am not sure whether the reconstruction regularization can compensate strictly. The authors are encouraged to elaborate more on this.

3. Another weakness is regarding the experimental validations, which were performed based on low-dimensional time-series datasets. The PCF-GAN is shown to beat the COT-GAN on these datasets. However, COT-GAN is also able to generate short video clips, in which the scenarios are much complicated. Would the PCF-GAN scale well to the higher-dimensional generation tasks, such as short video generation? Also the comparing baselines are not the state-of-the-art. It is the fact that many recent methods have been reported to surpass COT-GAN and TimeGAN, for example, [1] and [2].

[1] Seyfi, Ali, Jean-Francois Rajotte, and Raymond Ng. ""Generating multivariate time series with COmmon Source CoordInated GAN (COSCI-GAN)."" Advances in Neural Information Processing Systems 35 (2022): 32777-32788.
[2] Jarrett, Daniel, Ioana Bica, and Mihaela van der Schaar. ""Time-series generation by contrastive imitation."" Advances in Neural Information Processing Systems 34 (2021): 28968-28982.



Limitations:
Please see my weakness.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a new metric for the distributions on the path space via PCF and provides theoretical proofs for analytic properties of the proposed loss metric which benefits GAN training. It introduces a novel PCF-GAN to generate & reconstruct time series simultaneously. It compares the proposed method with prior work and shows improved performance.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is relatively well-written and the proposed approach is described clearly. 

- The authors provide theoretical grounding for their work. They prove PCF's characteristicity, boundedness, differentiability with respect to generator parameters, and its weak continuity which ensures the stability and feasibility of training the PCF-GAN. 

- Experimental results demonstrate improved performance compared to several prior works. The authors provide comparisons with TimeGAN, CotGAN and RGAN on RV, Stock, Air and EEG datasets using discriminative and predictive metrics.  

Weaknesses:
- There are concerns regarding missing comparisons with related work. [A, B] also propose generative models for time series generation. [A] combines the adversarial training of GANs and the exact maximum likelihood training of CTFPs into a single framework. It designs an invertible generator, and adopt an autoencoder, on whose hidden space of the GAN performs the adversarial training. [B] uses Deep Euler representation and Wasserstein distances to propose three generative methods for times series. Two generative method EWGAN and EDGAN demonstrate an accuracy similar to state-of-the-art GAN generators and show better performance for capturing temporal dynamic metrics of the time series. The third method CEGEN is based on a loss metric computed on the conditional distributions of the time series. These papers are not mentioned and no comparisons are provided with them.   

- There are metrics such as FID in prior work, e.g. [B], which are not used in the experiments. 

[A] GT-GAN: General Purpose Time Series Synthesis with Generative Adversarial Networks; Jeon et al.; 
 
[B] Conditional Loss and Deep Euler Scheme for Time Series Generation; Remlinger et al.;

Limitations:
The authors have adequately discussed limitations of their work. 

Rating:
5

Confidence:
3

";1
35dOU92OJM;"REVIEW 
Summary:
The paper shows how to achieve a truly idempotent image compression method, where f(x) = f(f(x)). The paper shows that it's sufficient to have E(D(y)) == y. This only requires a surjective E.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Interesting derivation of the required conditions for idempotence. Also nice to see this problem studied.

Weaknesses:
- Old baselines: I think GDN is barely used in SOTA image codecs, where it's all ReLU or Leaky ReLU. It would have been interesting to see this method applied to eg. ELIC from 2022 vs. Balle's 2018 method.
- The linear algebra presentation was hard to follow (p4). Some more intuitive insights like in the introduction would have been useful.


Minor: Typo in section 3 header (""Invertibie"") 

Limitations:
Only applied to old architectures from 2018 which contain 4 conv layers and GDN.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper argues that invertibility is sufficient but not necessary for achieving idempotent codecs, and proposes a framework for achieving idempotent learned image compression (LIC) with right-inverse, which allows more flexible and expressive transforms. This paper details the expressive and right-reversible atomic transformations used in LIC, including convolution (using blocked rearrangement and null-space enhancement), normalization, and quantization. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1、The paper first theoretically proves that idempotence can be relaxed to be right reversible and details the Right-Invertibie Codec.
2、The proposed framework achieves state-of-the-art RD performance among idempotent codecs. Also, it can be easily relaxed into a near-idempotent codec, which also achieves state-of-the-art re-compression performance.
3、The paper is well organized with detailed description of various parts of the whole right-reversible atomic transformations.


Weaknesses:
1、Lack of analysis of the computational complexity of the proposed framework: The paper does not provide a detailed analysis of the computational complexity of the proposed framework. While the paper mentions that the proposed framework is efficient and parallel-friendly, it does not provide a detailed analysis of the computational cost of the various components of the framework.
2、The First-time compression RD performance of the proposed codec dramatically falls behind modern LIC. For most scenarios, the first compression is also important. It will be interesting for the author to provide an analysis on feasibility to further improve RD performance of the proposed framework. 
3、Limited discussion of the limitations of the proposed framework: While the paper briefly discusses the limitations of the proposed framework.  For example, the paper could discuss the potential impact of the assumption that the input image is preprocessed to have zero mean and unit variance on the performance of the framework.
4、Lack of comparison with non-idempotent codecs: The paper only compares the proposed framework with other idempotent and near-idempotent codecs.
5、Limited analysis of the impact of null-space enhancement: The paper only shows the impact of null-space enhancement on two lower-bpp points and two higher-bpp points, and does not provide a more comprehensive analysis of the impact of this technique on the performance of the framework.


Limitations:
1. The proposed framework considers compression on integer latent, which is different from the common LIC method. 
2. The assumption that the input image is preprocessed to have zero mean and unit variance may not hold for all images.


Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a learned image codec with a right-inverse transform. The task is to ensure that an image can be re-compressed multiple times without significant quality degradation, while the compression is still lossy. This work is one of the few early attempts along this line of research. Its applications are rather niche.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The paper is easy to follow and the task is novel.

Weaknesses:
(1) The main idea of null space decomposition has been proposed in J. Schwab, S. Antholzer, and M. Haltmeier,  Deep null space learning for inverse problems: convergence analysis and rates. Inverse Problems, 35(2):025008, 2019. 

(2) The necessity and benefits of the blocked rearrangement convolution with coupling enhancement over the ordinary convolution are not clear. It seems to me that this part is not essential to null space decomposition. The authors argue that blocked rearrangement convolution can work more efficiently. However, there is no evidence or ablation study to support the argument. 

(3) The right inverse requires E o D to be an identity matrix. However, Eqs. (8) and (9) suggests D o E is an identify matrix. 

(4) Can the re-compression be done on different computation platforms while maintaining the right inverse property? 

(5) There is no ablation study on the near-idempotent idea. In Section 3.5, the authors mention that the near-idempotent codec has better first-time compression performance than the idempotent codec. Is there any insight into this? Also, how about the recompression performance as compared to the idempotent design? 

(6) The proposed method involves SVD. I wonder if this complicates the training process. 

(7) The authors should cite ""https://github.com/mahaichuan/Versatile-Image-Compression"" for invertible compression backbone design.

Limitations:
Please clarify whether the training would be complicated by the use of SVD.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this work the authors address the problem of stability of codec re-compression (idempotence). In particular the paper points out the difficulty of achieving idempotence in learned image compression, and existing methods rely on invertible models such as normalizing flows which limits their performance. 
The observation is that the only constraint is to have right-invertible transforms. Blocked convolutions are proposed, such that the right inverse matrix corresponding to the encoder convolution is used on the decoder side. 

Besides this main point, the paper also presents additional improvements to: 1) limit the effects of the block pattern in the receptive field, 2) extend the commonly used GDN layers, and 3) address the issue with mean-shift trick quantization that doesn’t guarantee idempotence. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In general the paper is well written and clearly motivated. There are several key contributions:
+ Idea of using right inverse and its implementation with the blocked convolution and right matrix inverse.
+ Addressing all remaining details in a sound way: GDN layer, issue with the mean-shift trick in the mean-scale entropy model

It is also important to mention that has:
+ State of the art results for idempotent image compression
+ An ablation study that shows the importance of each contribution
+ Code provided along with the submission.


Weaknesses:
In general I think the paper is doing a great job in presenting the problem and the solution I would have minor points that authors should address in the rebuttal:

1. I think the transition from function notation to matrix notation is not smooth, and in general the transition from idempotence to right inverse could be better explained for readers less familiar with the topic of learned image compression.

2. The word “Rearrangement” together with the kernel size 5 example in line 94 brings a lot of confusion. It took me a while  to realize that the paper was proposing a new convolution and receptive fields are not overlapping which is needed to make everything work. 

3. Too much space is used for the Blocked convolution while the description is not detailed enough for the rest. For example what is f(Y) exactly (eq. 11 and 12). How is it learned? 

4. The extension to Near-Idempotence is a bit rushed and it’s not clear of considering the modification applied to other layers might be better?

5. In the experiments, It should always be mentioned “idempotent” or “near-idempotent” to avoid any doubt. 

6. For reference, best performing non idempotent model should appear in Figure 3.a

7. For sanity check, both the idempotent version and [Helminger2021] should appear in Figure 3.b


Other details :\
l. 74: section title “Right-Invertibie” -> “Right-Invertible’\
l. 94: missing word in “if exists”?


Limitations:
The authors discuss limitations in a sufficient manner.

Rating:
6

Confidence:
4

";1
nXPqMyWUnx;"REVIEW 
Summary:
This work found that unfair LFs in programmatic weak supervision could introduce bias to the resultant training labels, and proposed to address the bias via source bias mitigation and provided theoretical guarantee. Experimental results show that the effectiveness of the approaches in both synthetic and real datasets.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. This work studies an important yet overlooked problem in programmatic weak supervision: the biases induced by unfair LFs
2. The proposed method, compatible with traditional fair ML methods, could mitigate biases and improve the performance at the same time.
3. The theoretical results are convincing, which show that the LF bias could be arbitrary yet can be fixed by the proposed method under some conditions

Weaknesses:
I am not aware of any major weakness except that the proposed model, if I understand it correctly, is a new label model built on an existing one, which means users have to use the proposed label model if they want to mitigate the biases. It is unclear how biases should be mitigated if users prefer other choice of label model.

In terms of label models that incorporate feature vector, one related work is missing: ""Leveraging instance features for label aggregation in programmatic weak supervision""

Limitations:
see weakness above

Rating:
7

Confidence:
4

REVIEW 
Summary:
*The paper studies the problem of unfairness introduced by weak supervision methods through noisy data augmentation. It proposes a mitigation strategy using counterfactuals that would create a more balanced/unbiased dataset. 
* Paper shows that Labeling functions can be arbitrarily biased by preferring examples away from the center of the distribution and show theoretically there is no change in sample complexity required to achieve the gains in accuracy and fairness, under strict assumptions.
* Empirical results show that by further augmenting data that counterfactually transforms examples across groups, both accuracy and fairness metrics can be improved - with results on synthetic and popular benchmark fairness and weak supervision datasets.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Strong empirical results on synthetic and real datasets
* New problem formulation that extends fairness methods to the weak supervision case

Weaknesses:
* Theoretical results are under strong distributional assumptions, how they can be relaxed should be better articulated
* Standard data augmentation techniques such as Autolabel [1] are missing in evaluation set up.

https://ieeexplore.ieee.org/abstract/document/10136178

Limitations:
This is missing, and should be included

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel bias mitigation technique to address the fairness issues in weak supervision settings. The core idea is to use a  counterfactual fairness-based correction method. The authors theoretically show that the proposed method can improve both accuracy and fairness and this is also supported by both synthetic and real datasets evaluations. Overall, this paper calls attention to bias and fairness studies in the weak supervision context, which has never been addressed specifically and provides an effective and theoretically sound method. To the best of my knowledge, the method considered in this paper is novel and is a valuable contribution to the community. Based on the above factors, I recommend acceptance.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
This paper is well-motivated and well-written. The proposed method is intuitively simple yet very effective and theoretically sound. I appreciate the comprehensive evaluation both with synthetic data and with WRENCH.

Weaknesses:
This paper focuses on a novel area of weak supervision studies. Though the method proposed is simple, I do believe it has any major weak nesses.

Limitations:
Overall, this paper is well organized and I have not identify any apparent limitations for the current content.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper focuses on the bias issue in weak supervision. The paper shows the labeling functions of weak supervision may produce biased pseudo-labels, which is first empirically shown in the paper. Also, they theoretically show that even though a dataset has fair ground-truth labels, the weak supervision process using labeling functions may produce biased weak labels for the dataset. To mitigate this issue, the paper proposes an optimal transport-based algorithm that modifies the weak labels to be fairer. In experiments, the paper evaluates the proposed algorithm in tabular, NLP, and computer vision datasets.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
S1. The paper considers an important research problem, the fairness issue in the weak supervision pipeline. It seems this paper is the first work to handle fairness in weak supervision.

S2. The paper theoretically shows the labeling functions may produce unfair pseudo-labels even though the underlying label distribution is fair. 

S3. The proposed method can be used together with the existing fair in-processing algorithms, which is another good aspect of the paper.


Weaknesses:
W1. The explanations on the labeling function itself are limited.
- For example, can the number of labeling functions affect the fairness performance? The Snorkel original paper [1] mentioned that the number of labeling functions highly affects the labeling performances, but the current paper does not explain how such details may make differences in the fairness scenario. 
- Also, as shown in the vision dataset experiments, using fairer labeling functions can reduce the effectiveness of the proposed algorithm. Then, why is the proposed transport algorithm better than making labeling functions itself fairer? Currently, the paper seems to use very simple labeling functions (described in the appendix), and it would be helpful if the paper could provide any comparison between the proposed algorithm and another possible direction of making fairer labeling functions.

[1] Ratner et al., Snorkel: Rapid Training Data Creation with Weak Supervision, VLDB’18


W2. Experiments show some questionable results.
- As several scenarios show large F1 score drops, it is unclear whether it is okay to use the proposed algorithm. I understand the F1 score can be affected by the imbalanced label classes, but the F1 score degradation is still severe. For example, SBM (OT-S) + LIFT case in the Bank Marketing dataset and all SBM results in the Civil dataset show large F1 score drops. It would be helpful if the paper could provide at least some possible way to prevent such F1 score drops.
- In the experiments, no error range is provided, which makes the empirical results less convincing.


Limitations:
The paper did not discuss the limitations and potential negative social impact of the work. A possible limitation of this work can be other prominent fairness definitions that this work cannot handle.

Rating:
5

Confidence:
4

";1
QFcE9QGP5I;"REVIEW 
Summary:
This paper presents first-order iterative methods for solving unconstrained minimization problem. The connection between the proposed methods, quasi-Newton and Anderson accelaration methods are illustrated, which gives an insight of the motivation. Under certain assumptions, the methods exhibit explicit gobal non-asympotic convergence rates adaptively using backtracking strategy. The effiency of the propsed methods is compared to a fine-tuned BFGS algorithm with line search in the numerical experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem is well-motivated.

- Under certain assumptions, the proposed adaptive first-order methods achieve explicit global convergence rates that blend those of gradient descent and cubic regularized Newton's method.

- The inputs of the algorithms are carefully discussed. It should be clear for others to implement.

- The algorithm complexity is analyzed and compared in the numerical experiments.

Weaknesses:
- Too many assumptions and requirements in the theoretical part. Can those requirements be verified? Seems the requirements are post to let the proof go through.  

- The algorithm setting for the numerical experiments in section 6 is a bit confusing. For example, what online techniques are used for 'Iterate Only', 'Accelareted Forward Only', 'Forward Estimate Only' and 'Greedy'?

- The performance of the accelarated algorithm is suboptimal and unstable.

Limitations:
- The performance of the accelarated algorithm is suboptimal and unstable.
- Analysis is provided only for Algorithm 1 not Algorithm 2 while the later seems better. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a generic framework for developing new iterative schemes for smooth optimization in the deterministic setting. The derived new methods show some similarity to quasi-Newton methods and Anderson acceleration, while using a backtracking line search for estimating the Lipschitz constant and having step sizes adaptively determined by minimizing an upper bound of the objective function or the gradient norm. The explicit, global and non-asymptotic convergence rates are established for one type of the derived methods. Numerical results are presented for some specific implementations of the framework.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a novel framework inspired by the recent development of cubic regularized Newton's method. This framework leads to several optimization methods that can be viewed as new variants of quasi-Newton methods and Anderson acceleration. The theory seems to be solid. The explicit, non-asymptotic convergence rates are established under different assumptions of the optimization problems, justifying the introduced techniques. The content is informative and the main idea is easy to follow. Numerical results are presented to support some claims of the paper.

Weaknesses:
The weaknesses are listed as follows:

1. The theory requires the Lipschitz continuity of the Hessian in $\mathcal{R}^d$, which is a strong assumption in practice.

2. The theorems only consider the iteration number towards convergence, which may overestimate the efficiency of the algorithms. Since the number of function evaluations can be large in each iteration, it needs to be clarified whether the claimed convergence rates are still valid when taking these hidden calculations into account.

3. The biggest drawback of the proposed methods is their suitability for solving high-dimensional problems. To well approximate the Hessian which is critical for the convergence, the memory usage needs to be large, which can cause failure of the algorithms when the memory resource is limited. What's worse, the algorithms are very complicated. They use a backtracking line search to estimate the Lipschitz constant and solve a minimization problem to determine the stepsizes in each inner step of the line search. It is unknown how many times the minimization problem needs to be solved during each iteration. It is likely that the computational cost of each iteration is much higher than the classical quasi-Newton methods or Anderson acceleration. However, there is no discussion about this issue.

4. The algorithms were only tested for solving small-scale logistic regression problems. Some numerical results do not support the theory. For instance, the accelerated method does not exhibit any acceleration in practice. In many cases, the simple BFGS with the default setting still achieves the best performance, while the proposed methods are less efficient due to higher costs.

5. Since many additional calculations are hidden in the subroutines of the algorithms, it is more convincing to report comparisons of the considered algorithms with respect to running time. However, these results are missing.

There are also some other concerns about the paper:

1. In Section 2, the paper claims that quasi-Newton methods and Anderson acceleration share the common property that the iterates are combinations of previous iterates and the current gradient. This claim may be true for quadratic optimization but seems to be wrong in general cases. There is no justification for this property throughout the paper. The presented framework is more like a generalized heavy-ball or Nesterov-like method, or can be viewed as an adaptive version of the subspace Newton method. Its connection with quasi-Newton methods and Anderson acceleration is not clear. It is misleading if such a connection is not valid.

2. Although the presented framework does not use the traditional line-search or trust-region technique to guarantee global convergence, it uses a more costly backtracking line-search strategy to estimate the Lipschitz constant. Such complexity can impair the significance of this work.

3. Section 1.2 says that Anderson acceleration does not generalize well outside quadratic minimization. It is not true since Anderson acceleration is well-known for its usefulness in accelerating fixed-point iterations in scientific computing.

4. Some descriptions of the classical methods are not standard. The formulas of BFGS and Anderson acceleration in Section 2 are quite strange. The formula of BFGS below Line 97 seems to be wrong. The formula of Anderson acceleration below Line 104 is incorrect since $d_0$ and $g_0$ are undefined. The formula below Line 513 is not the Anderson acceleration.

5. Many notations are not defined clearly, e.g., $P_i$ in Equation (2), $r_i$ below Line 104, and $R^\dagger$ in Line 600. 

6. The paper discusses many possible methods derived from the framework, but the pros and cons of each method lack clear clarifications. For example, the orthogonalized greedy method seems to outperform BFGS in some cases, but it also doubles the memory usage. So the comparison in the experiments may be unfair.

7. The proofs need to be reorganized to make them easy to follow. It is better to give more examples of the introduced notations and assumptions.

Limitations:
Some limitations have been mentioned in the main paper, but there is no discussion about the total computation cost and memory cost.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors introduce a generic framework for developing novel quasi-Newton and Anderson/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including accelerated convergence on convex functions. They also provide empirical results in the numerical experiments.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The structure is clear and the theoretical analysis and proof are correct. The results presented in the numerical experiments section are consistent with the theoretical results.

Weaknesses:
There is the following weakness regarding this submission.

1. First, the authors ignores a lot of existing quasi-Newton type methods or accelerated versions that achieve the non-asymptotic global convergence rates of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$ such as:

“Practical inexact proximal quasi-Newton method with global complexity analysis”. Katya Scheinberg and Xiaocheng Tang. Mathematical Programming160(2016), pp.495–529.

“Proximal quasi Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates”. Hiva Ghanbari and Katya Scheinberg. Computational Optimization and Applications69(2018),pp.597–627

“Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization”. R. Jiang and A. Mokhtari. https://arxiv.org/abs/2306.02212

Recently there also exists a paper proposing the quasi-Newton type method that could achieve a global explicit superlinear convergence rate:

“Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence”. R. Jiang, Q. Jin, A. Mokhtari. Conference on Learning Theory (COLT), 2023.

However, this global superlinear convergence rate assumes strong convexity of the objective function. The authors should compare the results in this submission to all these related literature in detail to check if the proposed algorithm could achieve an improvement in the aspect of convergence rate or computational cost per iteration.

2. The authors didn't compare the results of the this paper to the global convergence rates of first-order gradient descent and accelerated gradient descent. The authors claimed that the proposed algorithm could match the results of accelerated gradient descent, but is the constant in the convergence rate of this proposed method better than the constant of accelerated gradient descent? What is the improvement of this algorithm compared with the accelerated gradient descent? Notice that the computational cost per iteration of this QN-type method is worse than the computational cost per iteration of accelerated gradient descent.

3. It's better for the authors to use the notations of $s_t = x_{t + 1} - x_t$ and $y_t = \nabla{f}(x_{t + 1}) - \nabla{f}(x_{t})$. These notations are more commonly-used in the quasi-Newton methods.

4. The notations used from equations (3) to (7) are quietly confusing. What's the exact definition of $y_i$ and $z_i$? Is it $y_i = x_{t - i + 1}$ and $z_i = x_{t - i}$. Also it seems that $D$, $G$ and $\epsilon$ depend on the iterations index $t$. Then it should be $D_t$, $G_t$ and $\epsilon_t$. These notations are messed and make the readers difficult to understand and follow the ideas of the authors.

5. What is the definition or function of the parameters $M_0$, $M_t$ and $M_{min}$ presented in the algorithms? The authors said that the $M_0$ is the initial guess of the smoothness parameter, So $M$ is an estimation of the parameter $L$ in assumption 1? What's the value of $M_0$ in all the numerical experiments presented in this paper? What theoretical conditions should this $M_0$ satisfy?

6. The authors presented some designed requirements in section 3.1. These expressions of requirements are a bit strange for the theoretical results. The authors should either make these requirements as the assumptions of the algorithms or make these requirements as some lemmas/theorems of the theoretical analysis. However, these requirements are too strong or restrictive as assumptions. On the other hand, the authors didn't give any strict mathematical proof to give any conditions to satisfy these requirements. The authors argue that these requirements are not restrictive in the text of section 3.1. But this is not enough for the theoretical analysis of the algorithm. We need strict and clear mathematical proof or empirical results from the numerical experiments. It is not clear how to make these requirements to be satisfied. There is no formal proof.

7. The authors claimed that $N$ could be small around line 149. However, as expressed in the theorem 6, it seems that $N$ should be comparable to the dimension $d$ to reach a good convergence rate. But when $N = \mathcal{O}(d)$, the computational cost of solving the sub-problems could be as expensive as $\mathcal{O}(d^3)$ as presented in line 148. This is as costly as the Newton's method.

8. From line 203 to line 210, the authors argue that some propertied needed to be satisfied to retrieve the convergence rate of Newton’s method with cubic regularization. However, it seems that when these properties are satisfied, the computational complexity is the same as the Newton's method. Then, what is the advantages or improvements of this method compared to Newton’s method with cubic regularization?

9. What are the definitions of $b_i$ and $\lambda_t^{(1, 2)}$ in the equation below line 213? The authors should explain these parameters for the accelerated algorithms.

10. The most significant weakness or drawback of this submission is the lack of formal algorithm for the update of matrices $Y, Z, D, G$. Although the authors presented the online and batch techniques in section 5, these descriptions of the update scheme is just an outline and too introductory. We need a lot of details and formal description of the algorithm for the implementations or updates of matrices $Y, Z, D, G$. Without explicit algorithm like Algorithm 1 in the paper, there are a lot of unclear operations regarding the update of the corresponding matrices. The authors should present a formal algorithm regarding the implementations of these updates in the appendix. There also exists a lot of issues for these algorithms. For example, in the deviations of Iterates, Forward Estimates and Greedy updates after the line 239, it seems that column numbers of matrix $Y$ and $Z$ are $N + 1$ instead of $N$. The dimensions are not consistent. Also to implement the orthogonal iterate in the equation under line 236, it needs the matrix $P_{t - 1}$ defined in equation (12). However, this definition in equation (12) needs the operation of matrix multiplication and matrix inversion, which could be very expensive in high dimension condition. Also, the authors argue that Orthogonal Forward Estimates can ensure that the condition number $\kappa_D = 1$ and the norm of the error vector is small. But this observation is not obvious and need detailed explanations. There is no formal or strict mathematical proof and theoretical analysis to ensure that these updates of matrix $D$ could satisfy the requirements presented in the section 3.1. At last for the batch technique, the authors apply the QR decomposition. However, the computational cost or complexity of QR decomposition could be very high in the high dimension condition and make the implementation very slow in practice.

Limitations:
Please check the weakness section for limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In the paper, the authors propose a new framework that connects Quasi-Newton methods with Anderson acceleration. By exploiting the Cubic Regularization technique, the authors achieve new competitive convergence rates comparable to first-order methods in the worst case and second-order methods in the best case. The paper describes various problem setups with their respective convergence rates: non-accelerated methods for non-convex, star-convex, and convex functions, as well as an accelerated method for convex functions. The authors also propose different variants of approximation. Experiments are presented for both convex and non-convex functions.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
I believe the results presented in the paper are both original and significant. The novel connection between Anderson averaging, Quasi-Newton methods, and subspace sketch-Newton-type methods is both new and prospective. The provided proofs seem mostly correct.
Furthermore, introducing online approximation techniques adds value to the research. The thorough comparison of these techniques in practical applications showcases their effectiveness and underscores their relevance.
In conclusion, this paper contributes valuable insights to the field, and its innovative approach holds great potential for further advancements in optimization methods.

Weaknesses:
I may guess that the paper was created in a hurry before the conference deadline, which might explain why it contains a lot of small mistakes and misprints. While the mathematical results are sound, The mathematical results are sufficient but the overall presentation and clarity need improvement. A thorough review and revision by the authors are necessary to address these issues. Next, I will present some of the mistakes and misprints that I find.

1) Page 6, line 191, Theorem 5. The brackets on the right side are unnecessary. On the right side, $f(x_t)-f^{\ast}$ should be changed to $f(x_0)-f^{\ast}$. The same is in the Appendix. 
2) Page 8, line 239. All $Z_t$ and $Y_t$ contain $N+1$ elements, but they should contain $N$. So, I think the indices should be corrected.
3) Page 36, line 785. $r_i$ was not defined anywhere before, moreover, $r$ is used in other places for different things. It probably should be $G_i$.
4) Page 36, line 787. The last formula in the line is incorrect; the left side is a vector, and the right side is a number.
5) Page 36, line 791. The inequality is incorrect because $(r_i - \nabla^2 f(x) d_i)^2$ is a vector and shouldn’t be squared in such a way.
6) Page 36, line 795. The third transition is incorrect because $\|w\|$ is missing for the $L$ term.
7) Page 37, line 800. I believe it is better to explicitly prove that the term $\alpha^T D^TG\alpha$ could be upper bounded by $\alpha^T (D^T G+G^T D)\alpha/2$; otherwise, it may be confusing. 
8) Page 5, line 174. The notation $\varepsilon_t$ as a vector is confusing because a page before $\varepsilon_i$ was a number, the coordinate of a vector $\varepsilon$. 
9) Page 4, line 127. $x_{+}$ is defined in Theorem 1 but never used inside. So, I think it can be removed. On the other hand, $x$ is used inside but not really specified. 

Next are the moments that caused confusion for the first read. 

10) Page 3, Motivation. The transitions between line 97 to formula (2) and then to formula (4) are very confusing. It may help to specify what are the $\alpha_i$ and $P_t$ for that case. For formula (4), the author may say that it is a special case of formula (2) when $H_0=0$. It may help to understand such a transition.
11) Page 4, line 142, formula 10. The norm of $\|D\|$ is not defined, which may cause some confusion because $D$ is a rectangular matrix and could have different norms. 
12) Page 7, Algorithm 5. The notation $(M_0)_1$ is very confusing, especially when $M_0$ and $M_1$ also exist. Moving index $t$ of the step to the upper level, like $M^t_0$, may solve both this moment and $\varepsilon_t$ moment, but it is up to the authors how to resolve these issues with the notation.
13) Page 15, line 498, formula (19). As a small comment, I would suggest using the keyword “subspaced” or “sketched” instead of “stochastic” to avoid confusion with stochastic optimization methods such as SGD and others. 


Limitations:
Yes


Rating:
6

Confidence:
4

";0
HtMXRGbUMt;"REVIEW 
Summary:
This paper aligns with a range of recent works which investigate the tendency of diffusion models to memorize  training data and emit this during sampling, when prompted to, and presents strategies to mitigate this for data-processing, training and inference. Other than previous works, which mainly focused their analyses around duplicate images in the training data and model overfitting (i.e. dataset size, portion of train examples seen during training, …) , this work separately analyzes the influence of both these points and further adds to this the influence of captions (and their duplication) in the training data and complexity of the images in the training dataset. The main findings for these are that highly specific captions for every image promotes duplication which is furthermore also more likely for images which are simple (as measure by pixel histogram-entropies and jpeg-compressability). 

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
* The paper separates the main factors of influence during training from each other and bases the findings made for each of these separate categories on rich sets of empirical experiments, making the paper conclusive and interesting
* The main take-aways are novel and of high importance for the community, especially since the authors center their experiments around fune-tuning a foundational DM, which is what many researchers do nowadays.
* The key take-aways for every analyzed influencing factor are summarized below the respective sections which makes the paper well structured and raises comprehensibility.

Weaknesses:
* It is totally reasonable that the authors cannot train a foundation model like SD from scratch and therefore focus on finetuning based on these models. However, the exact influence of the pretraining on all the results presented on conclusions taken remains unclear to me after the paper. It would therefore be interesting to also include results of models trained from scratch for all of the presented experiments, since it would provide insights about the generality of the statements made.
* Related to the above point, I think the title may promise a bit too much, since it would need experiments from scratch (for models such as SD) to choose that title. Maybe sth. Like “Why finetuned diffusion models memorize and …” would be better. What du you think?
* To me, it is counterintuitive that the model trained with no duplication has a higher dataset similarit score than the one with partial dup. in Fig. 5. If I didn’t miss sth, this is never discussed and is therefore confusing. It might also raise concerns about the significance of the results presented in this part which I think is crucial for the entire paper. Could the authors share their take on this?
* For a reader not knowing SSCD it may be hard to estimate, how reasonable the chosen newly introduced metrics related to SSCD are, since many of these are based on thresholds. Therefore, it would be great of the authors could add similarity scores to all visual examples showing found duplicates (especially in Fig. 1). 
* In line 91 it is stated that FID would measure image quality and diversity. I don’t agree for diversity since there are many models with low FID scores which actually have very low diversity (many GANs, see e.g. [1,2]). I’d therefore recommend to either additionally include Precision and Recall Metrics [2] in the evaluation or remove diversity here.
		
[1] Sauer, A., Chitta, K., Müller, J., & Geiger, A. Projected GANs Converge Faster.
[2] T Kynkäänniemi, T Karras, S Laine, J Lehtinen, T Aila ,  Improved precision and recall metric for assessing generative models

Limitations:
There is not dedicated limitations section, however, the limited scope of the experiments, which focus only on finetuning is mentioned in the paper. I’d recommend to add  a dedicated limitations section and once more state this. Also it would be interesting to see this analysis for other types of models such as AR models or GANS (although I know that no comparably powerful models to SD are publicly available as of now). The fact that this misses could also go into such a section

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper discusses the issue of data replication in text-to-image diffusion models, specifically focusing on the memorization problem. It mentions that these models, such as Stable Diffusion, are known for unintentionally replicating their training data, which has led to recent lawsuits. The commonly held belief is that duplicated images in the training set contribute to content replication during inference. However, the paper's analysis reveals that the text conditioning of the model also plays a significant role in data replication. The experiments show that unconditional models don't replicate data as frequently as text-conditional models. Building on these findings, the paper proposes several techniques to mitigate data replication during inference by randomizing and augmenting image captions in the training set.






Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-organized and well-written
- The experiment setup is clear and sound.
- The originality of resolving the memorization issue of the diffusion model is interesting.
- The experiment results are convincing.

Weaknesses:
- Even though this paper is focusing on the empirical results and evidence of memorization of the diffusion model, I am still expecting a high level of theoretical understanding. Specifically, does it holds for the general case regardless of the type of SDE, data distribution etc.?
I think these questions are also important.

Limitations:
The observation is based on the heuristic observation, however, it is fine since it is impossible to have a white-box explanation of a neural network. It would be better to have some guidance in theoretical reasoning.

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper tries to explain the memorization and copying of training data in diffusion models. The paper establishes a connection between caption duplications and memorization on top of data duplications analyzed in previous works. In their main experiments, the authors fine-tune various SD models on ImageNette/LAION subsets and show that controlling data duplication does not completely prevent memorization and that highly specific captions amplify copying. They also relate memorization to training parameters and data complexity. Finally, the paper introduces several mitigation strategies based on randomizing conditional information. 


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
 - Overall, I find the experiment design in the paper to be good and got a better understanding of the memorization phenomenon from the paper. I also enjoyed the writing style of the paper. 
 - I find the connection between image complexity and copying to be reasonable and believe this is the first time this has been analyzed this way. 
 - While previous papers were mostly about analyzing the impact of data duplication on memorization, it makes sense to also control the captions and the paper shows that there exists a connection between the two. 
 - The problem setting of the paper is very relevant. 


Weaknesses:
 - Pretty much all experiments are based on the dataset similarity score which is defined as the 95-percentile of image-level similarities. As this is very important for the validity of the experiments, I believe that it would make a lot of sense to motivate this design choice further. From previous works, the authors quote that an image similarity larger than 0.5 shows strong similarities. In the worst case, the distribution of image similarities could contain slightly less than 5% near duplicates and slightly more than 95% unrelated images which would result in an arbitrarily small dataset similarity but still a decent memorization quote. Maybe the authors could simply demonstrate that the distribution of image similarities is well-behaved and also quantify the number of outliers, eg at a dataset similarity of 0.2, how many of the remaining images above the 95%-quantile actually achieve an image similarity larger than 0.5. Ideally, it would also be great to shortly demonstrate that results look similar with larger percentiles and that the 95-percentile is not critical for the outcome of the experiments. 
- The range of similarity scores in most of the experiments is actually somewhat limited. I do not have a good feeling about the scale of this metric, but for example, the data duplication factor seems to only influence the score in the range 0.64 to 0.70. As this is the 95% quantile and 0.5 can be assumed to be a duplicate, this means that in all cases we have strong duplicates. I do believe the authors that there is a connection between duplicates/captions and copying, however, either the dataset score is not the right metric for this kind of experiment or duplicates/caption can actually only explain a relatively small part of the problem. Also even with the best-performing train time mitigation strategies, the similarity score is 0.42/0.47 so from my understanding this could mean that we still have about 5% of duplicates. 
- The mitigation strategies are somewhat simplistic. I also believe that it is crucial to give a more detailed analysis of the ""minimal effect"" that the mitigations have on model performance. The images on the left half of Figure 8 work well in the sense that they seem to produce the same object that was also on the original image. However, most of the images on the right side of Figure 8 are not convincing to me. For example, I do not see any interaction between the hippo mother and her child (prompt: ""Mothers influence on her young hippo""), and from a short Google search I would say that the person in the image is not Ann Graham Lotz (prompt: ""Living in the Light with Ann Graham Lotz"") and I could not find any connection between the last image with mitigation and ""hopped up gaming east"". I think mitigating copying is clearly important, but if this prevents the SD model from following the user's prompt it becomes useless. I think Figure 8 should show the results of the best train and test time mitigation strategies for the same images and I would also like to see the results of the best train time defenses on the images on the right side as apparently they seem to be harder. For some images in Figure 16, I also do not find any of the test time defenses to produce reasonable results. 
 

Limitations:
In my opinion, one of the largest limitations is that even the best-performing training guidelines/mitigation strategies might not always prevent copying. The paper gives us a dataset similarity result of 0.42/0.47 and claims that in terms of image similarity, 0.50 would be a duplicate. So it is not clear to me that this actually solves copying. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper is a deep dive into the phenomenon of memorization in Diffusion models. I find that the paper is very well position given the current state of generative ML and the paper does a great job at asking the right questions about various factors concerning the memorization of images. Some of the key takeaways in the paper are really strong, such as randomizing captions, or partial duplication of images, but some others are not very well substantiated. Finally, the authors also conclude with some guidelines and mitigation strategies for practitioners, making this a refreshing read.

This paper is hard to review in a strengths/weakness pattern because each section has its strengths and weaknesses. First, this paper tries to do a lot. I think that ends up being a weakness of the paper because the high-level message from the abstract and intro gets swayed away through the body making this a very confusing read. Especially in the sections from 3 to 5.3. It is only after that the main narrative of the work catches on. I understand that the authors want to show a lot of experiments and analyses they did, but many of them are not at all central to the key message of the paper.


Soundness:
2

Presentation:
2

Contribution:
4

Strengths:
1. This paper tries to break down various factors that are responsible for the memorization of training data by diffusion models.
2. The discussions in sections 6 and 7 are really valuable to the community.
3. The paper asks the right questions and also performs experiments to support their hypothesis (more about this in the weaknesses). But overall, the paper serves as a great starting point for understanding the state of memorization in generative models, giving a peak at some thought-provoking questions, and using some of these hypotheses to actually suggest mitigation strategies.
4. I am curious about the image complexity argument. This is a refreshing take, but also very different from image classification literature where typically hard images are memorized because the model can easily learn the simple ones to classify. It seems that the take for generative models is the opposite. but again there is a distinction between anecdotal arguments of features versus the argument of compressibility and entropy studies here. A good metric potentially would be the embedding similarity as assessed by a CLIP model.

Weaknesses:
My biggest qualm with this paper is about the metrics. It seems that this paper is based in a delusional world where dataset similarity and FID are the sole indicators of model memorization and model performance. I know that evaluating both generation and memorization are hard problems, but this paper takes these yardsticks way too far, to an extent that following up with some results becomes uncomfortable. And in many sections, I get the sense that hypotheses and conjectures are drawn just to satisfy these so-called oracle notions of memorization and quality.

1. On generation quality: Section 7 severely needs an analsis on the impact of these changes on the quality of generation. Similary, in Figure 4, FID scores are all over the place. are these even good models?
2. On memorization: In Section 5.1 the authors note that they did not observe any duplication in the clusters they studies. Again telling how the metrics of DS are not informative or reflective of memorization in the first place. I do not understand why checking similarity scores with the top 5% of generations. Further, in Figure 5: lower SS when the image is duplicated. and this happens even in the right figure. But does this actually related to ""memorization"" or just stay till the idea of dataset similarity. For instance, in the de-duplication done for SD2, not all duplicates were ""full duplicates"" if I am correct. So i think the explanation saying random captions helps reduce SS is an over statement.
3. On section 3 and Figure 1 proper attribution should be given to the original authors who devised the technique
4. I have not seen people use clip text features to assess how close texts are. I would suggest using some text model such as RoBERTa for these use cases. 
5. ""possibly because FID is lowest when the dataset is perfectly memorized"" I do not see how this logically flows. And in general I did not like that this paper makes too many conjectures to explain every possible phenomenon they see in results. It just feels like the authors are trying to fit a jigsaw no matter what.






Limitations:
-

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper systematically investigates the memorizing phenomenon of stable diffusion. As they empirically observed, most of the memorizing phenomenon originates from the duplication of images and captions in the training set. Owing to this, the authors propose to deduplicate the captions during the training stage to obviate the memorization of the diffusion model.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The experiments in this paper are quite complete. In my opinion, most of the components that may be related to the mesmerization phenomenon are considered. 

Weaknesses:
To measure the memorization magnitude, the authors use a similarity score and set the threshold as 0.5, i.e., the score is larger than 0.5 means there is a memorization for a certain generated image. My concern is whether this score and threshold are valid criteria. 

For the stable diffusion trained on a large-scale dataset without duplication i.e., stable diffusion v2.1, the memorization phenomenon seems almost mitigated. Thus, my question is whether increasing the unduplicated data can fix the memorization phenomenon. 
 

Limitations:
no

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors conduct a large-scale empirical study about the memorization problem in diffusion models. Their experimental results indicate that text conditioning plays an important role in this problem. Based on these observations, they propose to randomize the text prompts during training to mitigate memorization and copying.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The memorization problem is valuable for the AI safety community.

2. The authors design a large number of experiments to study this problem.

3. The authors observe that text conditioning plays an important role in this problem, which is a new discovery compared with the existing works.


Weaknesses:
1. Some experiment designs cannot verify the arguments. And some results are not consistent with the arguments of the authors. Please refer to Question1-3.

2. The proposed solutions are not novel. Please refer to Question4.

3. Some discussion and references about mode collapse are missing. Please refer to Question5.

4. Some figures in the paper are messy and confusing. Please refer to Question6.



Limitations:
The authors do not discuss the limitations.

Rating:
6

Confidence:
3

";1
MamHShmHiX;"REVIEW 
Summary:
The paper presents a new approximation scheme for computing posterior marginals in graphical models such as Bayesian networks and Markov networks. The proposed approach is based on a previously introduced scheme called IBIA (Incremental Build Infer Approximate) which was initially developed for approximating the partition function. The idea is to construct a sequence of bounded size clique trees (called a clique tree forest) that eventually spans all the input factors and subsequently compute the posterior marginals by carefully selecting the appropriate cliques from the forest. The experimental evaluation is carried out on standard benchmarks for graphical models and the proposed scheme is compared with several state-of-the-art approximation scheme based on message passing. The results demonstrate that the proposed approximation scheme is competitive and some times outperforms existing approaches.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is relatively well written and organised. It is also relatively self contained and most of the concepts are discussed in sufficient detail. The empirical evaluation is sound and the results are presented in a relatively clear manner. Some theoretical properties of the proposed method are also discussed.


Weaknesses:
Firstly, the contribution presented in this paper appears to be quite incremental over the previous work the IBIA framework. Namely it consists of the message passing scheme over the additional links between adjacent clique trees. The proposed heuristic for choosing links appears to be quite arbitrary. 

Secondly, I think the proposed method is highly sensitive to the order in which factors are added to the clique tree forest, as well as the initial collection of factors chosen for initialisation. Therefore, the paper should discuss this aspect in more detail.

Thirdly, the paper does not mention anything about the convergence of the scheme nor its time/space complexity.


Limitations:
The proposed scheme is clearly related to several existing scheme that aim to bound the size of the join-tree. Prominent examples are the mini-bucket scheme for variable elimination, the mini-clustering approach for cluster tree (join tree) elimination, as well as the iterative join-graph propagation scheme (all from Rina Dechter's group). Similarly, the Incremental Thin Junction Tree scheme by Frank Hutter is also looking at bounding the clique size of a join tree by splitting a larger clique into smaller cliques. Therefore, I think a deeper discussion on the connection between these previous schemes and the proposed IBIA based one is clearly called for (it is currently missing from the paper).

The empirical evaluation only considers a subset of the existing message passing schemes for approximating marginals e.g, LBP, IJGP, and SampleSearch. These methods are more than one decade old (if not more), do not really fall into the more recent category of variational inference methods. I think it would be interesting to compare with more recent methods based on weighted mini-buckets and cost-shifting such as those developed in Alex Ihler's group (including the importance sampling based ones). If you look at the recent UAI-2022 competition results, it looks like these kinds of methods performed best in the MAP (and to some extent in the PR category as well). 


Rating:
4

Confidence:
5

REVIEW 
Summary:
The paper proposes a new approximate inference algorithm for probabilistic graphical models (PGMs). The approach uses a framework called IBIA to perform marginal inference in discrete PGMs. The main idea is to perform Belief Propagation on a sequence of clique trees. In each step the clique tree is approximated such that the number of variables in any factor is less than a bound and BP is performed on this approximate tree. Factors are then added to this tree and the messages are updated using heuristics to get the next clique tree in the sequence.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The approach proposed in the paper seems to work well over a large set of UAI benchmarks in comparison with well-known inference methods. 
The idea of computing marginals based on a sequence of linked Clique Tree Forests seems novel.
The idea of running BP for a single iteration to calibrate and obtaining good experimental results seems interesting since it can reduce computational complexity.

Weaknesses:
The presentation could be improved since it seems a bit hard to understand all the steps. Perhaps a pseudo-code like algorithm might have helped. Also, since there have been a lot of approximate inference algorithms in PGMs, the main motivation for this approach was a bit unclear. For example, if I understand correctly the mini-bucket methods (Dechter 2003) also have a similar idea of approximating factors through mini-buckets. I think in the related work section the significance of the method can be perhaps better motivated in relation to these methods.

One aspect that was not very clear was when BP converges on the approximate clique tree, if we change the cliques (by adding factors) we cannot just update the beliefs without performing inference again I assumed. In that case, if we use heuristics to update the beliefs (as is mentioned) won’t the errors propagate through the sequence of clique trees? I think there are convergence guarantees on he marginals (e.g. as we add more clique trees in the sequence), that seems like a significant drawback since the scheme will not be any-time (as compared to several other techniques which have this property).

Limitations:
Limitations are mentioned.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a new approximate inference method for marginals of probabilistic graphical models (PGMs) based on the incremental build-infer-approximate (IBIA) framework. IBIA was introduced by Bathla & Vasudevan in an arXiv preprint in April 2023 to estimate the partition function (normalizing constant). It achieves this by converting the PGM into a sequence of clique tree forests (SCTFs) with a user-defined clique size bound. The sequence is constructed iteratively: the first step (Build) adds factors of the PGM to the current CTF until the maximum clique size is reached; the second step (Infer) calibrates all clique trees using standard belief propagation; and the final step (Approximate) reduces the clique size to another user-defined bound.

This paper builds upon the previous IBIA work in the following ways: it observes and proves several properties of the SCTF generated by the IBIA framework, which it uses to design an extension of IBIA to approximate marginals (in addition to the partition function). This extension works by introducing links between cliques in subsequent steps of the SCTF based on shared variables, called sequence of linked CTFs (SLCTF). It backpropagates beliefs from the last CTF to previous CTFs via the links and re-calibrates each CTF with one round of message passing. Finally the marginal distribution can be inferred from the CTF containing the desired variable. The paper also describes heuristics for the choice of links and notes that in the directed case (i.e. Bayesian networks), it is advantageous to add variables in topological order during the incremental build step.

The experimental evaluation compares with two existing methods, loopy belief propagation (LBP) and iterative join graph propagation (IJGP), on benchmarks from recent UAI competitions with time limits of 2 min, 20 min, and, for some benchmarks, 1h. They report the number of instances solved within the time limit and the average and maximum Hellinger distance over all non-evidence variables. The new method performs performs better than LBP and IJGP on most examples and is similar on the remaining ones, even though IBIA is implemented in Python whereas the baselines are written in C++.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed approximate inference method is interesting and improves the performance compared to existing methods (LBP and IJGP). The extension of the IBIA framework with links is new and useful. The explanation is adequate given the page constraints and the use of a running example is very helpful to understand the algorithm.

Weaknesses:
The contribution of this paper (adding links) seems to me to be relatively minor compared to the introduction of IBIA itself in previous work. Extending it in some way to relate successive CTFs (here done via links) seems quite obvious (although that might be the benefit of hindsight).

Regarding the experimental evaluation, I had a look at the recent UAI 2022 competition, since the paper mentions that the same benchmarks were used for recent UAI competitions, if I understand line 14 correctly. It looks like IBIA was a competitor there and while performing well, it did not come in first, as the experimental evaluation of this paper might suggest. For this reason, I'm not convinced that the baselines in the experimental section are representative of the field of PGM inference and whether comparisons with more methods are needed to get an accurate picture of IBIA's performance (see my question to the authors below).

As a minor point: while the presentation is generally fine, I found the notation confusing at times, see below.

Limitations:
The authors have adequately discussed the limitations of their work.

Rating:
7

Confidence:
3

";1
RiwPYAMLur;"REVIEW 
Summary:
The paper introduces an algorithm for active representation learning in multi-task settings. The algorithm generalizes to settings in which the “task space” is either discrete or continuous.

The overall setting involves an (optionally) nonlinear map between an input space X and an intermediate feature space via $\psi_X$ and $\phi_X$, that then undergoes a final multiplication with “task features” obtained by applying $\psi_W$ and $B_W$ to task parameters $w$. The goal is to learn $\hat{\phi}_X$ and $B_W$ given the possibility of actively selecting tasks $w$ within a “source” domain, and evaluating on tasks in a “target” domain.

The proposed approach can work for both “coarse exploration”, “fine target-agnostic exploration” and “fine target-aware exploration”.

The authors provide a theoretical analysis for the “benign setting” in which the source task space satisfies specific assumptions. Moreover, they provide experimental results beyond this setting, and on some real-world datasets.


Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
The paper is heavily theoretical in nature, and seeks to formally introduce the problem with precise assumptions, also offering theoretical guarantees given a “benign” version of the problem setting.

The experimental results show that the target-aware method indeed beats the target-agnostic method by a large margin, giving credence to the significance of the target-aware method.


Weaknesses:
Overall, the paper is very hard to parse, and seems to take much background as granted. In the preliminaries, a very large number of assumptions are made on the problem setting. It is unclear given my limited expertise in the subfield whether all these assumptions undermine the significance of the approach or not. It would be useful to more thoroughly compare the assumptions and design choices with the literature on the topic, to better contextualize what the paper brings to the table and what the contribution is.

The setting itself seems interesting, but not enough emphasis is put on why it is significant and worthwhile exploring. It seems that the approach effectively requires us to be able to at-will sample tasks within their “task parameter space”, which seems to be extremely hard to do for anything beyond simulation. The drone experiment seems to do away with this by defining the parameter space as a one-hot vector, but not enough emphasis is placed on the significance of such a choice.

Furthermore, the results on the drone dataset experiment seem to show that the proposed method converges faster, but ultimately reaches the same accuracy as the passive strategy.


Limitations:
The authors address in the appendix some limitations related to the theoretical analysis. It would however be helpful if they could list some limitations from the point of view of how related the setting is to real-world multi-task learning problems.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This work considers the multi-task learning problem and proposes an algorithm to select related source tasks to tackle a known target task in an active learning like setup. The analysis is primarily for the linear function classes and handles discrete and continuous task spaces. The proposed algorithm operates in 3 stages: (1) a coarse sampling of the tasks (2) More efficient target-agnostic sampling of diverse tasks (3) target-aware sampling from a sub-space of related tasks. The 2nd and 3rd stage in particular allow for tighter sample complexity bounds since it allows us to efficiently sample from the task space. As a result, the sample complexity bounds depend on $k*$ which is the relationship between the target space and the relevant source space. Finally, this algorithm is used in three scenarios and an active learning strategy is shown to outperform a passive one.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper presents a theoretically motivated algorithm to sample source tasks that are relevant to the target task. While the theme of active sampling is not entirely new, few of them provide algorithms with sample complexity bounds. While there are many gaps to fill (linear function classes, isotropic Gaussian inputs), this paper takes a step in the right direction.

There are two interesting ideas in this work. The first is to reduce the number of tasks that we consider by selecting a diverse set of tasks (subspace $V$) that spans the task space. This is the primary role of stage 2 or the task-agnostic exploration. The second idea (similar to Yifang et al.) is to perform a task-aware search of the task-space and only use source tasks that are relevant to the target task. This improves sample complexity and the benefit is stronger if a smaller subspace of source tasks is strongly related to the target task.

The paper presents many results on 3 different experimental scenarios that primarily compare passive multi-task and active target-aware learning with clear sample complexity benefits for the latter. 


Weaknesses:
**Experimental setup**: To the best of my understanding, all the experiments in the manuscript (including appendix) compare passive to target-aware active learning strategies. How does the proposed method compare to a target-aware strategy that doesn't use stage 2 and only uses stage 3. I understand the theoretical benefit of stage 2 is clear and it allows us to work with fewer source tasks but is this reflected experimentally? 

**Improving the discussion on related work:** It would be nice to see the authors discuss some related work on multi-task learning and task-relatedness. This include some older work on passive multi-task learning [1,2], learning theory [3,4,5], task-distances [6,7,8], task-grouping [10,11] and weighted-training [6,12]. There are a lot more works that I haven't listed out that I think are very relevant to this work.


1. Caruana, Rich. ""Learning many related tasks at the same time with backpropagation."" Advances in neural information processing systems 7 (1994).
2. Baxter, Jonathan. ""Learning internal representations."" Proceedings of the eighth annual conference on Computational learning theory. 1995.
3. Ben-David, Shai, et al. ""A theory of learning from different domains."" Machine learning 79 (2010): 151-175.
4. Hanneke, Steve, and Samory Kpotufe. ""On the value of target data in transfer learning."" Advances in Neural Information Processing Systems 32 (2019).
5. Crammer, Koby, Michael Kearns, and Jennifer Wortman. ""Learning from Multiple Sources."" Journal of Machine Learning Research 9.8 (2008).
6. Thrun, Sebastian, and Joseph O'Sullivan. ""Discovering structure in multiple learning tasks: The TC algorithm."" ICML. Vol. 96. 1996.
7. Zamir, Amir R., et al. ""Taskonomy: Disentangling task transfer learning."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
8. Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.
9. Standley, Trevor, et al. ""Which tasks should be learned together in multi-task learning?."" International Conference on Machine Learning. PMLR, 2020.
10. Ramesh, Rahul, and Pratik Chaudhari. ""Model Zoo: A Growing"" Brain"" That Learns Continually."" arXiv preprint arXiv:2106.03027 (2021).
11. Fifty, Chris, et al. ""Efficiently identifying task groupings for multi-task learning."" Advances in Neural Information Processing Systems 34 (2021): 27503-27516.
12. Chen, Shuxiao, Koby Crammer, Hangfeng He, Dan Roth, and Weijie J. Su. ""Weighted training for cross-task learning."" arXiv preprint arXiv:2105.14095 (2021).




Limitations:
The paper discusses some of the limitations in Appendix C in sufficient detail.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a framework for active representation learning that allows learners to optimally choose which source tasks to sample from. The framework covers both task-aware and task-agnostic settings and is compatible with deep representation learning practices. The paper provides several instantiations of the framework, including a bilinear model and a neural network model. The authors demonstrate the effectiveness of their approach on a variety of simulated robotic tasks, showing that their method outperforms existing methods in terms of sample efficiency and generalization to new tasks. Overall, the paper's contributions include a novel framework for active representation learning, several instantiations of the framework, and empirical results demonstrating the effectiveness of the approach.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes a novel framework for active representation learning that allows learners to optimally choose which source tasks to sample from. This framework covers both task-aware and task-agnostic settings and is compatible with deep representation learning practices. It considers a more general setting where tasks are parameterized in a vector space, allowing for more effective leveraging of similarities between tasks.

The paper provides a thorough theoretical analysis of the proposed framework, including sample complexity bounds and convergence guarantees. The paper is well-written and organized, with clear explanations of the proposed framework and its instantiations.

The proposed framework has broad applicability in robotics and other domains where there are multiple related tasks to be learned.

Weaknesses:
The manuscript would be considerably enhanced by the inclusion of more comprehensive empirical assessments, specifically pertaining to real-world robotic assignments, as opposed to merely simulated tasks. Such addition would serve to confirm the applicability and efficacy of the suggested method in a broader, more realistic context.

Further elucidation through an in-depth comparative study with established methodologies, notably those tailored towards active representation learning, would aid in comprehending the proposed method's advantages and shortcomings in relation to contemporary state-of-the-art techniques.

Moreover, a thorough discourse on the constraints inherent in the proposed method is warranted, especially considering the presumptions regarding the task space and task distribution. An exposition of this nature would lend itself to an improved understanding of the circumstances wherein the approach proves most efficient, as well as areas of potential inapplicability.

Limitations:
Please see the Weakness.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies active representation learning under the assumption that the source and target tasks are parameterized by a vector space (instead of a discrete set of tasks). The proposed algorithm works by first finding a subspace of the task space that spans the representation space and then iterates between refining the subspace and estimating for the target. The theoretical analysis shows that under certain assumptions, their active approach can achieve a better sample complexity than a passive one.

Soundness:
2

Presentation:
4

Contribution:
4

Strengths:
- The paper explores an understudied problem statement in active learning and theoretically demonstrates its importance in comparison to less expressive active learning problem statements, which could have a large impact on future work on the applicability of active learning. 
- The paper is organized and the theory sections are well described.

Weaknesses:
- The problem statement and motivation were not described precisely or concisely. E.g., it's not clear to me why robotics is the only target for this work. The methods section was robotics-agnostic, but the introduction uses robotics as the motivating example and the paper is titled as a robotics paper.
- The experiment section lacks a comparison to baselines. E.g., a natural baseline would be to compare against an active learning approach that does assume a discrete source and task space. This would help show that the problem statement introduced is important empirically as well as theoretically. For this reason, I am recommending weak reject.

- Nit: The methods section also appeared to be accomplishing a lot of things at once---demonstrating the necessity of the continuous task space, showing the improvement in sample complexity over the passive approach, and minimizing expected excess risk---and it's not clear to the readers how these contributions relate or should be prioritized in our minds.
- Nit: the text on the LHS of figure 1 is cut off.

Limitations:
N/A

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper introduces and studies a theoretic and algorithmic framework for actively and optimally selecting the source tasks in multi-task learning, accommodating task-aware and task-agnostic settings. 
The authors present cohesive theoretical guarantees, particularly demonstrated through the pure bilinear setting with the task or sample complexity being improved. They further empirically validate their meta-algorithm with task-parameter-wise non-linearity settings, particularly with emphasis on applications in robotics, significantly outperforming baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The problem of active representation learning is interesting and important.
The proposed framework is novel and effective, demonstrated with both theoretical and empirical evidence.
The theoretical and experimental results are cohesive. 

Weaknesses:
- While the method and algorithms seem sound, their explanation could be better with more interpretation and transitions.
- The experiment section (in the main manuscript) is not well-presented, e.g., lack of baselines/setting description as well as the interpretation and justifications of results.
- To my best assessment, the experiments are not systematically and thoroughly conducted.
**Minor:**
- The paper misses the conclusion, thorough literature review, and limitation parts

Limitations:
The paper does not explicitly describe the limitations of the approach.
Please provide the limitations and the trade-off of the method.

Rating:
5

Confidence:
1

REVIEW 
Summary:
Which source task to sample from is an important issue in multi-task learning. This paper proposes a general and versatile algorithm and framework for active representation learning. The proposed algorithm can be adapted to arbitrary target and source task space, and can cover task-aware and task-agnostic settings. The experiments demonstrate the efficacy of the proposed algorithm on various datasets.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- Interesting and important topic.

Weaknesses:
- Incomplete contents.

- Poor writing and structure.

Limitations:
No limitations are addressed.

Rating:
1

Confidence:
1

";1
BOP5McdqGy;"REVIEW 
Summary:
This paper investigates the problem of social biases in the code generation procedure. This is the first paper to study the social bias issue in the code-generating model and demonstrate the existence of social biases. The author also introduces some evaluation metrics to quantify the bias in code as well as a classifier for scoring. They also conduct many experiments and ablation studies to provide more insight into this direction. In addition, they also validate their result and method by human annotation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is the first paper to study social bias in code generation model. 

The result are also interesting that since the training datasets are often human-irrelevant but it still shows that the model will contains biases.  It opens up new avenues for research and discussion about the nature of bias and how it can be subtly embedded in models. 

Also the human labeled prompt and dataset will be a great value for future research

It conduct pretty solid experiments on various models and also conduct human annotation to further evaluate the results. 

Weaknesses:
The authors of this paper have taken a novel approach to uncovering biases in code generation models by constructing biased prompts. While this methodology is sound, the results are somewhat predictable. Given a biased prompt, it is likely that a code generation model will produce biased output. However, this is more a reflection of the input rather than an inherent bias within the model. For instance, a model generating 'find_good_people == 'Hispanic'' is logically equivalent to generating 'find_delicious_apple = red', but we wouldn't label the latter as biased. The model is not able to realize this unless it is trained to recognize certain attributes or words as sensitive.

The study primarily focuses on 'direct bias', where the model is intentionally prompted to generate biased code. In real-world applications, it's unlikely that developers would intentionally craft such biased prompts unless they specifically aim to generate biased code. My main concern lies with 'unintentional bias', where biases are generated even when the prompt has no apparent bias. This is a more pressing issue in real-world scenarios.

Furthermore, it's worth considering that code may appear unbiased at the point of generation, but could introduce bias when applied in certain contexts. This is a complex issue that warrants further investigation.

Another weakness of the paper lies in its oversimplification of certain aspects in its analysis and evaluation. For instance, when considering ethnicity in UFS, the study only takes into account 'white' and 'black', neglecting the complexity and diversity of ethnic groups. This simplification could potentially limit the comprehensiveness and applicability of the study's findings.

I do appreciate that this study represents an initial step towards addressing the question of bias in code generation models. However, future research should aim to delve deeper into the nuances of unintentional and context-dependent biases to provide a more comprehensive understanding of this issue.

Limitations:
In the related work part, might need to include more papers that study social bias in NLP and the evaluation methods and benchmarks.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents among the first study on social biases in code generation models. The authors present a method to construct bias probes. Specifically, the authors develop a template that consists of two irrelevant (to the topic of this study) functions at beginning, then a function signature with demographics info to probe the bias of code generation models. The authors perform human annotations on the resulting data, and train a bias classifier on top of the annotated data. The authors further define metrics for quantifying bias. Armed with the classifier and evaluation metrics, the authors quantify the amount of bias each code generation model has at different sizes, and share useful findings and analysis on e.g. demographics and hyperparameters.  

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- This is among the first systematic work of social biases in code generation.
- The authors propose a full-stack study from prompt construction, dataset, evaluation metrics, to the quantitative and qualitative analysis, setting a foundation of social bias research in code generation.
- The annotated dataset can be useful for future research.

Weaknesses:
- The binary classes in demographics (Table 1) are questionable. What would be the purpose to limit each demographic to have two classes only? 
- Related, the UnFairness Score is for binary class only, however, most demographics are not binary. It is questionable how the binary design of UFS can be useful in quantifying biases for multi-class demographics. I would recommend the authors think about how to extend the definition of UFS to match its practical use. 
- Despite being able to elicit biases, the dataset is very artificial and out-of-distribution (three functions with the last one being a bias prober). Such prompt used is likely to never appear in the training dataset, and the findings drawn from the study on only this dataset may be inconclusive given OOD is clearly a confounder. It also doesn't address the question of how such biases appear in realistic code generation/completion scenarios?


Limitations:
The authors didn't explicitly discuss the limitation. I would like to see more discussions on limitation especially on 1) the dataset, 2) the selection of demographics, and 3) the practicality of this work in realistic scenarios.

Rating:
5

Confidence:
4

REVIEW 
Summary:
With AI applications becoming increasingly common, ensuring AI fairness is important. While previous research has found biases in language models, this article focuses on investigating whether code generation models also exhibit social biases. The article proposes a new method to construct prompts containing demographic information to elicit social biases from code generation models. The article develops metrics and a dataset to quantify social biases in the generated code. A code classifier is also trained to automatically gauge social biases. Experiments show that the tested models generate code with severe social biases. 


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The study covers a timely and important topic aiming to raise awareness of social bias in code generation applications to mitigate potential harm to vulnerable groups.   
2. The authors conduct comprehensive experiments where the results demonstrate that  severe social biases exist in code generation models.  
3. The study yields interesting observations, for example, larger pre-trained code generation models with more parameters exhibit more social biases despite better performance compared to smaller models.


Weaknesses:
1. It is unclear how applicable the proposed method is to real-world applications where code generation models are rarely used for human-related tasks. 
2. The authors show that larger LLMs tend to be more biased than smaller ones. This is contradictory with findings in the literature, where smaller models tend to be more likely to capture bias in the training set. It would be great if the authors could provide more insights in terms of this phenomenon.


Limitations:
Yes, the authors adequately addressed the limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work explores the social bias problem in pre-trained code generation models. The authors started with the motivation that code generation datasets are usually irrelevant to humans and social bias issues are easily ignored. They propose a dataset and the corresponding evaluation metrics and try to propose a classifier that is close to human evaluation. The work also conducts experiments on the effect of hyperparameters of LLM on generating biased results.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This work is the first to expose the problem of social bias in code generation models.


Weaknesses:
1. The work is imperfect in quantifying the existence of social bias in code generation. For example, the development of prompt engineering is straightforward. Experiments with large model hyperparameters are also inadequate.
2. Codex is based on GPT-3, which is currently offline. Codex is most likely not effectively using RLHF to correct values.  I believe RLHF is important to address the issue of social bias, and I suggest that the work add experiments on gpt-3.5 and gpt-4.

Limitations:
It is suggested that the authors add a discussion of the limitations of this work.

Rating:
4

Confidence:
4

";1
phnGilhPH8;"REVIEW 
Summary:
This paper proposes a novel approach called Federated Feature Distillation (FedFed) to mitigate the data heterogeneity problem while preserving privacy. In particular, FedFed partitions data into performance-sensitive features and performance-robust features based on the information bottleneck method. Only performance-sensitive features are shared among clients as they contain minimal private information and significantly contribute to performance. Moreover, incorporating the differential privacy (DP) mechanism can provide an additional layer of privacy protection. In summary, this work is interesting, and the authors provide some theoretical analyses to support their claims.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
This study mitigates the issue of data heterogeneity in Federated Learning (FL) by utilizing a promising approach to information-sharing. Empirical evidence shows that the proposed method is effective in enhancing model performance.

Weaknesses:
The reviewer believes the evaluation of the privacy leakage is insufficient. The details are as follows:

1. In Section 4.4, the authors conducted a model inversion attack to infer private data using shared features. However, quantitative measurement is missing in this evaluation. Previous studies [1, 2] have utilized metrics such as peak signal-to-noise ratio (PSNR) and Frechet inception distance (FID) to assess the quality of reconstructed data. We recommend that the authors include some quantitative results to strengthen the empirical evidence.

2. Federated learning is known to leak private information when sharing model parameters [3,4]. The FedFed method, which shares both model parameters and features, creates an opportunity for attackers to exploit these two types of information for privacy attacks. However, the experiment showed that the attacks only targeted the privacy information contained in the features, which is not a comprehensive representation of potential privacy leaks.

3. Tables 1, 2, 3, and 4 demonstrate that the FedFed method achieves a higher Top-1 accuracy than FedAvg. However, it's worth considering whether this increased accuracy comes at the cost of higher privacy leakage.

[1] The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural
Networks. https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.html

[2] Knowledge-Enriched Distributional Model Inversion Attacks. https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.html

[3] Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning. https://ieeexplore.ieee.org/abstract/document/8835245?casa_token=1iwvBNyN5q4AAAAA:BdxQzounj3eoNv0HIcdMoW7nCaM6xWJFPwZQIosqhvpiXWNaJd-q0MeW_xmiZJkZGVmZXDRE

[4] Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning. https://ieeexplore.ieee.org/abstract/document/8737416?casa_token=zyEVcT_x3oQAAAAA:FfpfqLHOsQJ33Br2OqnYE6fRI3EIgdNPlCSUOC74Yu6qhFjcgJvqHwFLAsPaShPaigK3sjz1


Limitations:
The authors correctly pointed out that the performance of FedFed is limited by the storage capacity of clients.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a method based on feature distillation to tackle data heterogeneity. The main contribution , as I see it, is in identifying performance-robust and performance-sensitive features and sharing the latter among clients to mitigate the impact of heterogeneity.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The method is simple (this is very important) and can be easily used with existing FL algorithms.
- DP is incorporated to protect the leakage of privacy-sensitive information when sharing performance-sensitive features.

Weaknesses:
- The evaluation is quite limited to vision datasets, which makes me unsure if it is going to generalize across modalities.
- Number of clients (K) are also quite limited. In real-world setting 100-1000s of clients are involved in FL process. 


Limitations:
- Do not see discussion on whether performance-sensitive features indeed leak information.
- Formal definitions of performance-robust and performance-sensitive features are missing. Please also make a subsection to provide detailed descriptions of how these two types of features are identified.
- The experiments are only performed using ResNet-18.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes Federated Feature distillation, a method that addresses the tradeoff between privacy and model performance. It involves extracting performance-robust and performance-sensitive features from local data. The latter is shared among clients after applying differential privacy for privacy preservation. The paper also includes an empirical evaluation demonstrating the effectiveness of the proposed approach.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written and easily comprehensible, with clear motivation and contribution.
This paper theoretically shows that the proposed method achieves the same level of privacy with a relatively smaller noise compared to sharing the raw data.
The proposed method can be seamlessly combined with existing universal methods,


Weaknesses:
- Additional communication costs:
As sharing globally shared data to all clients, it requires additional communication costs. In F.7 the authors analyze additional communication costs is as same as sending a classifier in approximately 14 communication rounds. But this analysis doesn’t consider the partial participating of federated learning. If we assume the 10% participation rate, sharing the global dataset is equal to the communication costs of 140 communication rounds, which is not small. 

- More local iterations on training:
The presence of a globally shared dataset in FedFed results in each client having a significantly larger amount of local training data, K+1 times more than before. It also results in K+1 times local iterations, which can be computationally expensive for edge devices in federated learning, especially when dealing with massively distributed data in realistic settings. 
Furthermore, the considerably higher number of local updates in FedFed compared to the approach without FedFed makes it hard to attribute the observed gain in empirical results to a specific factor. It is well-known that increasing local updates can expedite the convergence speed in federated learning. Therefore, the observed gain in empirical results cannot be solely attributed to the FedFed approach, as the increased local iterations inherently provide an advantage. This factor should be taken into consideration when evaluating and comparing the performance of FedFed against other approaches.

- There are lines of work that try to share the client data while preserving the privacy inspired by mixup, such as [Shin et al., 2020] and [Yoon et al., 2021]. These methods are not compared as baselines in evaluation results.


[Shin et al., 2020] MyungJae Shin, Chihoon Hwang, Joongheon Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. Xor mixup: Privacy-preserving data augmentation for one-shot federated learning. In ICML, 2020.

[Yoon et al., 2021] Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang. Fedmix: Ap- proximation of mixup under mean augmented federated learning. In ICLR, 2021.


Limitations:
Limitation about storage overhead is stated while potential societal impact has not been addressed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a federated learning framework (FedFed) to tackle data heterogeneity by utilizing an information-sharing approach. The method partitions data into performance-sensitive features and performance-robust features, based on their contribution to model performance. The performance-sensitive features are shared globally to mitigate data heterogeneity, while the performance-robust features are kept locally. The method employs DP to protect performance-sensitive features before sending them to the server.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
+ Improving how to handle data heterogeneity in federated learning is a timely and important problem.
+ The idea of tackling data heterogeneity from the Information Bottleneck perspective is interesting.
+ The solution appears to significantly boost training performance of various FL algorithms and (in most cases) reduces the required number of communication rounds, while maintaining privacy.

Weaknesses:
- Overall the practicality of the method is unclear.
- The evaluation only considers a single task and uses synthetic non-IID data.
- The evaluation should include additional baselines.

Limitations:
* The paper has discussed the storage limitation but didn't clarify how much extra storage overhead was required and the size of the shared global datasets. 
* The paper didn't address how much per-round extra communication overhead the method required; although the approach seems to reduce the number of communication rounds significantly in most cases, there were some cases (table 4) that required more communication rounds compared to the other baselines. Moreover, communicating $x_p$ and $\theta$ adds extra communication overhead. Clearly, FedFed gains are not a free lunch, and these limitations must be considered, discussed, and be part of the evaluation.

Rating:
6

Confidence:
3

REVIEW 
Summary:
Authors propose method (Federated Feature distillation) to share partial features in the data to tackle data heterogeneity, while the privacy issue is not compromised too much. FedFed partitions data into performance-sensitive features and performance-robust features. The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept for local training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Authors claim to propose a new prospect on alleviating date heterogeneity in Federated Learning: sharing partial features. It is a new high-level idea to solve heterogeneity in FL.  
2. To achieve the idea, authors further propose a method (FedFed). More specifically, it proposed to partition data into performance-sensitive features and performance-robust features is reasonable. The performance-sensitive features are globally shared to mitigate data heterogeneity, while the performance-robust features are kept locally. This idea is resonable. 
3. It outperfroms baselines

 

Weaknesses:
1. Insufficient baselines. I wonder how the choices of baselines are made? FedAvg, FedProx, FedNova and SCAFFOLD are early FL models. The latest among them is proposed in 2020. Why not adopt more recent FL models? For instance, FedGen (ICML 2021)[1]. Besides, I believe there are more FL models in recent years.

[1] Zhu et al., Data-Free Knowledge Distillation for Heterogeneous Federated Learning. ICML 2021.



Limitations:
Yes

Rating:
5

Confidence:
2

";1
S5wmbQc1We;"REVIEW 
Summary:
This paper takes a closer look at the mechanistic explanation of neural networks learning to perform modular addition expanding on recent work that argued that such networks discover a simple ""clock"" algorithm. The authors demonstrate that changes in initialization and hyperparameters can lead to the discovery of qualitatively different algorithms - most notably what is referred to here as the ""pizza"" algorithm. This provides evidence that even the simple learning problem of modular addition leads to the discovery of diverse solutions in neural networks and mechanistic explanation requires a more complex analysis.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and succeeds in clearly communicating the findings on an active topic in the field of mechanistic interpretability. The analysis is carefully conducted, empirical results are mostly convincing and the conclusion is of importance for the broader field.

Weaknesses:
One of the main claims of the paper is that ""some networks very similar to the ones trained by [1] preferentially implement a qualitatively different approach"" but most of the evidence presented for such different solutions only apply to models that (transiently) remove the attention mechanism.

Limitations:
Limitations have been addressed appropriately.


Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper the authors study neural networks doing modular addition of integers, i.e. output $= mod_P( a + b)$ for fixed integer P and input integers a and b. Previous work on these networks has found that small transformers implement a simple _clock_ algorithm. This work verifies this, but shows that if you simplify the transformer’s attention and make your network more like a simple feedforward ReLU network, then you find the networks implement a completely different algorithm they call the _pizza_ algorithm. Their evidence for this includes (i) strange patterns in the correct logit outputs, (ii) gradients that do not fit the clock model but can be understood in their pizza model, (iii) patterns in the logits when the inputs are restricted to particular 2D planes, and (iv) the need for error correcting that leads to particular patterns in a pizza embedding. They then study an algorithmic phase change from pizza to clock as you vary how much attention is included.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
I thought the large-scale motivation for this work was justified, previous people have claimed that this specific task leads many networks to solve it in the same way. Turns out that isn’t true. That seems important.
I thought the main discovery of this work was very interesting.
I thought the evidence to back up the claim was convincing.
I thought the experiments performed were pretty thorough.
For the most part the claims were not overblown (for example I really appreciated the discussion of non-circular algorithms for solving this task)
Finally the appendix was a bit of a treasure trove.


Weaknesses:
I think the paper’s clarity could definitely improve. At times, mainly in the appendix where a lot of the explanation gets relegated, the paper felt rushed and the explanations were terse, expecting a lot from the reader. 

I found this especially true of the equations in the appendix, for example:

1.	In appendix A you introduce $s$ on the first line, which is the same as $E_{ab}$ I think. Why do you introduce a new symbol? Why do you not make it clear it is the same as $E_{ab}$ in the main paper? 
2.	You then do the same, again in Appendix A, when introducing the symbol $P_c$, which is I believe $Q_{abc}$ from the main paper? You also say thus, which in maths makes me expect to understand that you have derived a result, instead at the moment it reads like a definition of $P_c$ so the use of thus is confusing. 
3.	Line 392, in step 1 you talk about an accompanied pizza in a section that readers are expected to reach long before they read about accompanied pizzas where that adds no value as far as I can tell, but just makes the whole thing confusing.
4.	Appendix G was a minefield of strange notational choices in my mind. The layer index was initially denoted with i, but then changes to t, and i is reused as the token index.  I found this needlessly confusing.
5.	You define $x^j$ as the value of the residual stream after j layers, but then talk about $x_{i}^j$ for a couple of lines of the algorithm, before dropping the lower index. I think this is because there’s only one output logit so after the attention you can drop which token the input is coming from, but I still found it confusing at first (because you never say what I just wrote). It felt important to specify that the lower index of x is tokens for understanding the sum over k in the constant attention equation.
6.	For the next description you switch notation but still use x, but now without the top index rather than the bottom one, and use y and z. Could you highlight what is changing?

And beyond the equations I thought that occasionally the appendix was a tough read. For example in appendix H you talked about adding an equal sign. I eventually looked at the caption for figure 19 and understood what that meant, but I thought I’d missed some previously discussed equals sign (I now realise this is a hangover from the original clocks paper). Making sure the writing doesn’t have these kind of moments when the reader hasn’t been told about something and isn’t completely sure where to find out about it seems good. Perhaps you could edit the writing and captions when you try and re-read the paper with fresh eyes to see what is confusing - or get some fresh eyes on it.

A few of small things I am confused by: 

1.	I think the sine and cosine addition formulae you are using in step 2 of the algorithm in appendix A, during the development of alpha and beta, are missing factors of 2. [since you use them so often maybe it would be good to state the sine and cosine addition formulae somewhere]
2.	In section 3.4 it says the condition for there to be no antipodal pairs is for p to be prime, isn’t it for p to be odd? What am I missing?
3.	The plots in figure 6 (and all figures like it in the appendix) are titled wrong.
4.	In the formula for distance irrelevance i on the top row should i be a member of $\mathbb{Z}_P$ not $\mathbb{Z}_P^2$?

It seems there are a class of pizza like algorithms (e.g. the two in appendix A), and the evidence listed does not distinguish them. Is this true? Do you know which is happening? If not, perhaps figure 1 is misleading. Instead your claim is that step 3 is one example of a class of pizza type algorithms that are being implemented, and perhaps figure 1 could say that?

Limitations:
The authors did a good job discussing limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
*Background*: Previous work in mechanistic interpretability has identified a particular algorithm that NNs implement (the clock algorithm) to solve modular addition. However, under certain architecture changes, the authors notice that NNs implement a different algorithm.

The main goal of this paper is to present inconsistencies in the clock algorithm for neural networks without attention (ie: an inductive bias that allows the model to implement multiplication) and motivate a different algorithm (the pizza) that explains a different algorithm through which such networks learn modular attention. The observations are backed by experiments on linearly interpolating between a NN with attention and an NN without it. The experiments also demonstrate that a single algorithm doesn’t always win: different models can and do ensemble multiple copies of both algorithms in parallel.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is very well written and the proofs and arguments presented seem airtight.  Most of the questions I’ve had while reading the paper are either addressed in subsequent sections or in the appendix. The experiments are simple yet, I believe, comprehensive in evaluating the arguments presented. This is also an exciting emerging area of research and should lead to interesting discussions in the mechanistic interpretability community.

Weaknesses:
This work raises a lot of interesting questions, but I really can’t find any egregious logical inconsistencies with this work.  
meta-(non)concern: This work largely relies on a problem from a paper that hasn’t been peer-reviewed. However, I do not think this is a reason to reject this work.

Overall, I recommend _acceptance_.

Limitations:
The authors have addressed limitations in the manuscript. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors present a novel algorithm as a mechanistic explanation of neural networks for modular addition. It is noted that the model without attention fails to implement the ‘Clock’ algorithm. This assertion is substantiated with evidence related to gradient symmetricity and logit patterns. The authors then propose an alternative solution, named the ‘Pizza’ algorithm, supported by evidence concerning logit patterns via circle isolation and accompanying ‘pizza’. Ultimately, they demonstrate the presence of an algorithmic phase transition along the attention rate and model width, employing metrics that indicate gradient symmetricity and distance irrelevance.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well-structured and supports its arguments with solid experiments. The authors demonstrate that a neural network is capable of learning diverse algorithms for the same task. They introduce an impressive procedure for interpreting the neural network via embedding vectors. This methodology has the potential for extension to more complex models and tasks.

Weaknesses:
The authors employ the term logit $Q_{abc}$ as well as the term output logit, which refers to the un-normalized log probability. The choice of terminology, however, proves to be confusing. Given that $Q_{abc}$ is not used in the model and is a concept introduced by the authors themselves, it would be beneficial to rename $Q_{abc}$ to a more intuitive term like ""value"" or ""rank"".

Limitations:
As the authors have noted, their focus lies on a simple learning problem. Significant further work is required to adapt their techniques for use with the more complex models typically employed in real-world tasks.

Rating:
8

Confidence:
4

REVIEW 
Summary:
In this work, the authors focus on the problem of learning modular addition in NNs. Using clock and pizza algorithms, they show that model exhibits sharp algorithm transitions, which are affected by layer width and attention strengths, often resulting in the parallel occurrence of these phases.  A series of experiments are performed on the single-layer network to support the hypothesis proposed in this work. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Well-written paper.
2, an useful contribution in terms of interpretability of NNs.
3. Novel contribution in terms of analyzing the algorithmic transitions.

Weaknesses:
1. Ablation study is missing
2. Did authors perform a grid search to select best hyper-parameters? Then that should be mentioned with ranges in the appendix.




Limitations:
As highlighted above, the main point is an ablation study to support the hypothesis and computational overhead.

**********
Score increased after Author rebuttal Responses


Rating:
7

Confidence:
4

";1
XrqqPDAsRE;"REVIEW 
Summary:
The authors consider the problem of bounding the privacy loss for a composition of DP mechanisms. This problem is well-studied in the literature and the particular setting here is when the mechanisms are Gaussian or have Gaussian sub-routines but the privacy loss is measured through the original (approximate) DP definition. This is particularly difficult for Gaussians because of the sub-exponential tail and other work has created new definitions better suited to the Gaussian mechanism. While the privacy loss random variable may be challenging to work with under the original DP definition, it can still be efficiently sampled from, so an estimate of the expected privacy loss can be computed efficiently. The authors show that these estimates are quite accurate with high probability and leverage this guarantee to turn the estimate into a formal privacy bound (or failure to run with tiny probability). As a result, for certain settings the authors give improved privacy composition bounds over the previous work.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
1. Clever new technique to use composition privacy loss estimates instead by rejecting the estimate with low probability.

2. Significant technical work that combines a variety of techniques.

3. Their work extends nicely to the subsampled Gaussian mechanism for DP-SGD.

4. Empirical testing run in a variety of settings for both Gaussian mechanism and DP-SGD.

5. Improves upon previous work for DP composition in a reasonably important setting.

Weaknesses:
Remark 6 feels exaggerated. For the settings considered in this paper, the delta parameter is just a result of concentration bounds and sub-exponential tails of Gaussians, not the probability of a catastrophic data breach. Unless the authors know of examples with delta <= 10^{-10}, I've only seen practical applications with delta at minimum 10^{-7} and 10^{-6} is most common. Of course there are privacy advocates that will always push for more privacy, but they are also likely to further push for ""pure"" differential privacy in which composition is easy and Laplace mechanism must be used instead.

$\textit{The authors pointed to the discussion here I missed, so strike this comment}$
There are privacy definitions better suited to Gaussians that have become very common both in practice and in the literature that allow for easier analysis and make this more difficult accounting less necessary (though still interesting).

$\textit{The authors adequately addressed this in the rebuttal}$
The improvement in epsilon for composition of the Gaussian mechanism over the previous work is quite small (<1%) for their empirical study (figure 5).

Limitations:
The limitations were appropriately discussed

Rating:
7

Confidence:
3

REVIEW 
Summary:
Authors introduce a new privacy accounting method to characterize the privacy loss random variable. The work reduce the classical privacy accounting problem into mean estimation problem following the previous work and give a Monte Carlo solution.  The work provides detailed analysis of the proposed method and its utility and also give some common distribution to show the effectiveness and performance theoretically. The numerical studies also show the correctness and practicability of the method in the real privacy accounting tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 Pros: 
1. The analysis for the proposed method is detailed and the writing is friendly to follow with a detailed preliminary.
2. The proposed tool works better than the compared existing accounting tools like CLT and FFT-based methods.
3. The fast speed and the online implementation show some potential for the method used in privacy accounting applications.



Weaknesses:
Actually, I have reviewed the paper in ICML. I think the paper has fixed almost the problem in the former cycle. The only problem I still keep is about whether the method will suffer from the dimension curse when deriving the prv samples for a general distribution. 

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a privacy accounting method called estimate-verify-release (EVR), whose basic principle is to convert an estimate of a privacy parameter into a formal privacy guarantee. The mechanism works by verifying whether the estimated privacy guarantee holds, and then releasing the query output depending on the verification result. The paper develops a Monte-Carlo-based verifier for this paradigm. The overall accountant is broadly applicable and is shown to give a tighter privacy-utility tradeoff than existing baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The mechanism is applicable broadly, in particular to important DP algorithms such as DP-SGD (that is, the subsampled Gaussian mechanism). It exploits the fact that existing work provides good privacy loss estimates and converts these estimates into a formal paradigm for ensuring differential privacy. The method is shown to beat a strong baseline, namely the FFT-based accountant from [19].

Weaknesses:
The paper places a lot of emphasis on the fact that we can't naively use an estimated privacy parameter as the truth, because DP is a strict guarantee, and this makes perfect sense. But then in the analysis and implementation of the accountant there are some steps of the new accountant that are not made completely rigorous, such as the number of Monte Carlo samples. Or, in Theorem 13 there is a nu parameter that is not known. So my suggestion is to write a fully formal version of the accountant for DP-SGD in the main body to show that the paradigm can indeed be applied fully rigorously.

In addition, it would be good to see at least one more experiment showing the same comparison as Figure 5. In Figure 5 the gains of MC over FFT are not clear for a small number of compositions. So it would be good to see if the comparison of Figure 5 is robust and generalizes to different problem settings.

A few minor comments:
- In the Conclusion, you say ""allowing safe privacy parameter estimates *without* provable assurance""?
- The Figures don't seem to be in vector format and are blurry if zoomed in.
- Typo in Line 132, ""privacy loss random variable *is* ..."" (right now there's no verb in the main clause)
- In Line 180, say where rho takes values.

Limitations:
NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose EVR framework for privacy accounting. The core idea is to estimate the privacy budget, verify whether the budget is approximately met, and then decide whether to release the result or halt. The workhorse is a Monte-Carlo verifier (also used as an accountant through binary search). The empirical evaluation shows significant improvement over existing techniques, especially in the large epsilon/small delta regime.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The authors propose a novel framework for privacy accounting: EVR. By trading off a small probability that the program halts with privacy budget, the authors manage to get a tighter privacy profile curve at larger epsilon/smaller delta regime.
2. The authors verify the proposed framework empirically and showcase the advantage of EVR.

Weaknesses:
1. One downside of the proposed approach is that there is a probability that the mechanism halts with a privacy budget cost. To get a tighter epsilon, you need to take the risk that you get nothing. Although this seems to be reasonable trade-off, it can be a problem in practical use case. More discussion should be put on this.

Limitations:
Yes

Rating:
7

Confidence:
4

";1
E8vGACczsQ;"REVIEW 
Summary:
The paper shows the existence of a phenomenon that the authors refer to as out-of-contect meta learning in large language models. The authors design experiments that show that this phenomenon causes the internalization of text that is broadly useful, meaning that the LLM is more likely to treat this content as true. The paper shows two forms of internalization, namely weak and strong internalization, the later being a form of meta learning. Two reasons are suggested for this phenomenon, one based on the parameters of the model, and another one relying on the implicit gradient alignment bias of gradient-based optimization methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper shows an interesting phenomenon 
* The proposed explanations are sound and intersting
* The paper is well written and easy to follow

Weaknesses:
* There is no conclusive explanation of the reasons why internalization happens
* The phenomenon is hard to formalize and study, which limits the advantage of the insights in the paper

Limitations:
The authors describe limitations in the paper

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work introduces the phenomenon of out-of-context meta-learning in large language models (LLMs). It demonstrates, through carefully designed experiments, that LLMs have the ability to internalize the semantic content of the text that appears to be from a reliable source. 
Specifically, they focus on exploring the existence of weak internalization and strong internalization in the context of LLMs and other vision models.
Potential explanations for the emergence of internalization are explored, including the way models store knowledge in their parameters and the implicit gradient alignment bias of gradient-descent-based methods. Finally, the implications of these findings for future AI systems are discussed, including the potential risks associated with internalization.



Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. Innovative Methodology: The paper introduces the phenomenon of out-of-context meta-learning in large language models (LLMs) and presents a series of carefully designed synthetic experiments to establish its existence. 
The methodology employed in these experiments is unique and provides valuable insights into how LLMs internalize and apply semantic content in different contexts. 

2. Comprehensive Experimental Design: The paper describes a series of synthetic experiments that evaluate the phenomenon of out-of-context meta-learning in LLMs from multiple perspectives. The experiments consider different variables and define tags and questions.


3. Implications and Risks: The paper discusses the implications of the findings for future AI systems and highlights potential risks associated with internalization. This analysis adds an important dimension to the paper, emphasizing the importance of understanding and mitigating the challenges posed by out-of-context meta-learning in LLMs.

4. Reproducibility: The paper provides detailed information about the experimental setup, including hyperparameters and performance evaluation metrics. 


Weaknesses:
1. This work is hard to penetrate. For example, the definition of statements involving two different define tags is not well-defined. Do the statements indicate 'definitions'? Furthermore, the authors' intention behind the phrase 'in every example in which it appears' is unclear.  Additionally, the explanation of weak internalization and strong internalization is confusing. By stating that ""LLMs will be likely to respond to questions as if the true statements from the training set are in fact true,"" do the authors imply that LLMs tend to generate correct answers when variables are defined with consistent define tags?
 

2. Confusing annotations. 

    - In section 2.1, the named entity is represented by a randomly generated 5-character string, whereas Figure 1 shows a 3-character string as the named entity replacement.

    - It would be helpful to use the example presented in Figure 1 for illustration purposes, as it could alleviate comprehension difficulties.

    - The definition of $X_2$ is introduced after its usage, which makes it difficult to understand.

3. The interpretation of experimental results is lacking clarity.

    - The description of 'in the same (inconsistent) definition' in Line 120 is ambiguous.

    - While the authors suggest that usefulness for predicting other datapoints is not the sole reason, they do not elaborate on the meaning of 'usefulness' or identify other contributing factors.

    - What conclusions can be drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3)? What is the purpose of the authors' explanation in Line 123-129?

    - How should internalization be understood in the context of 'resemblance to useful data'? 

    - Is pretraining necessary? In section 3.1, the authors only provide the experiment setups but fail to give a conclusion.

4. The title does not accurately reflect the content, since this work only focuses on LLMs and also explores such a phenomenon in computer vision models. 



Limitations:
I do not foresee any potential for negative societal impact from this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper argues for the existence of ‘out-of-context meta-learning’ as a characteristic of LLMs. The authors support this claim with cleverly designed experiments on QA using a 2.3B parameter pretrained Pythia model. They argue that the presented experiments demonstrate out-of-context meta-learning. They perform additional LLM experiments and a pair of simple toy experiments a synthetic language task and modified MNIST task. The authors discuss hypotheses which may explain the mechanism of their proposed phenomenon, and discuss its implications for the research community at large.

*after rebuttal, updated score from 4 (borderline reject) to 6 (weak accept)*

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The paper is well structured, flows nicely, and is clearly written. The experiment setup is motivated to test a specific hypothesis, and is highly creative, and the experiments are thoroughly analyzed. Many additional experiments were run. The results have tight error bars and seem likely to be correct and reproducible. The analysis of the implications of the central claim of the paper touches broadly on the capabilities of LLMs and is highly relevant to the general research community, especially as pertains to safety. 

Weaknesses:
Issue with the central claim
---------
The central claim of the paper is that the model is sensitive to the appearance of authoritativeness / usefulness of specific examples, and incorporates that assessment into some decision as to how thoroughly to 'internalize' those example. 

*    “is, or appears to be, *broadly useful* (such as true statements, or text from *authoritative sources*)” line 7
*    [the model] “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize” data more or less based on these features” lines 16-17
*    “Thus *usefulness for predicting other datapoints* is not the only reason why a definition might be internalized."" 121-122
*    “So after finetuning on X1, the neural net ends up at a point in the parameter space where gradient updates on consistent-seeming definitions result in more internalization than updates on inconsistent-seeming definitions. We consider this out-of-context meta-learning; it is as if the neural network “expects” the definitions with [blue,dotted]Define to be more useful for reducing the training loss in the future, and thus internalizes them more.” lines 137-143
*    “Our work investigates whether LLM training biases models towards internalizing information that *appears broadly useful*, even when doing so does not improve training performance” line 342
*    “learning can lead LLMs to update their predictions more/less when they encounter an example *whose features indicate it is reliable/unreliable*“ line 377

This is an extraordinary claim, as it supposes capacities of the model that are not immediately obvious in model behavior or in potential underlying mechanism (sec 4). The authors seek to demonstrate this behavior with the QA experiments in section 2. The experiments presented are thorough and interesting, but it is not clear to me that the results they show need to be interpreted as grandly as the authors do. It seems plausible that a simpler explanation may sufficiently explain the observed data without relying on imbuing the model with surprising new capacities.

Potential alternative explanation:
In Section 2, in Figure 2, the authors present the main evidence for their claims. For this argument, Let us suppose the 5-char sequence for the “inconsistent” tag ‘redDEFINEbar’ is “*YUIOP*”, and that the 5-char sequence for the ‘consistent’ tag ‘blueDEFINEdotted’ is “*GHJKL*”. This helps to ground these strings in how the model sees them as opposed to how they may be interpreted.

Incorporating a *GHJKL* sequence, as in QA1, gives the model the opportunity to recover from the loss of information in the entity-string masking (shown in the gap between QA4 baseline and QA3), but only through the medium of updating the parameters of the model themselves (as opposed to via the activations as in in-context learning). QA2 obfuscates further from QA3 by incorporating a *YUIOP* sequence which connects each entity-string to a random incorrect entity. In essence, the *GHJKL* sequences tell the model that the entity-string and entity in the sequence are identical. However, it is not clear to me that the *YUIOP* sequences should be interpreted as “inconsistent seeming [definitions]” (line 138). There is nothing that forces the model to view *YUIOP* as an indicator of identity and then to figure out that its an unreliable identity indicator (which would involve the kind of self-reflection capacities supposed in the claim of the authors). Is it not more parsimonious to say the *YUIOP* sequences are consistent markers of non-identity - simply non-sequitur statements which are true but generally useless? If it is always true that the entity-strings and entities in *YUIOP* sequences are inconsistent with the QA examples holding those entity-strings (‘perfectly correlated’ line 87), a reasonable pattern that the model may learn is “'*YUIOP* X Y' means that X!=Y”. This bizarre anti-definition is almost useless as Y could be anything other than X, and potentially confusing, which can account for the drop from QA3 (brown) to QA2 (pink) following similar reasoning as in lines 120-121. So far this is basically the same, but from this perspective, the “surprise” result of Figure 2, that D5 outperforms D6, is no longer surprising. It is not necessary to rely upon the suggestion that the model “pick[s] up on features that indicate whether a given data point is likely to help reduce the loss on other data points, and “internalize[s]” data more or less based on these features” lines 16-17. Is it not simpler to suggest that the model has learned correctly that *GHJKL* indicates identity and *YUIOP* indicates non-identity? In this case, the fact that non-identity is ‘internalized’ to a lesser degree is no surprise at all: non-identity is only loosely incorporated (or incorporate-able!) because it is a non-sequitur. The model can fail to 'internalize' this non-sequitur information on account of it's general irrelevance, without relying on an surprising capacity to learn conditional on an example's ""*usefulness for predicting other datapoints*"" (line 122). A similar explanation can be given if *YUIOP* is not understood to be non-identity at all, but just random noise with no consistent interpretation. The fact that the model 'internalizes' the *GHJKL* information more than that of the *YUIOP* can rely solely upon the fact that an interpretation of *GHJKL* is readily apparent and there is no obvious interpretation of *YUIOP*. This line of reasoning begs the question as to the loss curves of the specific *GHJKL* and *YUIOP* examples in QA1/QA2 over the course of the training. You might expect to see higher loss for the *YUIOP* examples. Note that this argument extends to the non-QA experiments presented as well.

What is strange in this perspective is not that D5 outperforms D6 but that D6 outperforms QA7! But this surprising result does not carry the significant implications of the previously surprising result highlighted by the authors. Regardless of how you explain the superior performance of D6 over QA7, it bears explaining why the above reasoning (which explains away the 'surprise' of the gap between D5 and D6 and which does not stipulate any particularly surprising characteristics on the behalf of the model) is confused. It seems a plausible enough explanation that without a convincing rebuttal the central claim of the paper, which makes an extraordinary claim of model behavior, seems shaky. 

There is no reason to presuppose that a *YUIOP* example would ever be interpreted as a definition by the model, despite it being labeled as such in the analysis of the paper. Without this presupposition, the claim that *YUIOP* represents an ""inconsistent-seeming definition"" to the model is unfounded, as is the subsequent claim that that the model “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize[s]” data more or less based on these features”. The gap supposed to demonstrate 'strong internalization' can be explained as nothing more than the difference between the model comprehending a useful control sequence marking identity, *GHJKL*, and a sequence marking random noise *YUIOP*.

(Note: the above arguments may indeed be plausible but subtly misguided and ultimately wrong, but the authors must address them convincingly in order to strengthen the paper.)

Other
-----
Lines 79-82 discuss ‘information leakage’ where replaced entities may be inferrable based on information present in the QA pairs and background information present in the pretrained model, and states that steps have been taken to reduce this possibility. Presumably the performance of QA3 in Fig2 would vary significantly with this information leakage, where highly ‘leaked’ entities would still have good performance? (Ie. training on “Q: xyz was the first president of which country. A: the USA” should yield better performance on xyz related test Qs than a more obfuscated relation). Is this interpretation correct? If so it seems that the function of this information leakage and the specific means of alleviating it are actually very important to the interpretation of the results, and should perhaps be given more attention than being left to the appendices / alluded to in lines 124-127.

Internalization is not formally defined in any way, yet it is a central aspect of the paper. It is 'measured' only via aggregated loss on each dataset. More time should be spent investigating and developing the idea of internalization (how does the 'internalization' of a specific example relate the loss on that example?).

Nits
-----

*    The title of the paper comes from a contrast to ‘in-context’ learning, which is referenced many times in the paper, but the meaning of the term is not made explicit until the Related work (line 314). It would improve clarity to define what is meant by in-context learning and to describe how the proposed ‘out-of-context’ learning differs when introducing the concept of out-of-context learning (line 42).

*    Figure 1 bottom right has a typo: “Q: What did qwe born? A:” is presumably a mish-mash of two different questions? And not actually in the test set? It would not be surprising to get a bad answer to this question as stated!
*    It would be helpful to label the data presented in Figure 1 with the dataset names (QA3, QA4..) used in Section 2.3.
*    Figure 1 depicts two stages of finetuning on two separate datasets X1 and X2, and their subsequent eval/analysis, but this is a little obfuscated by the presentation. Consider making it more organizationally clear. Perhaps draw a bubble around the box on the left and the box on the top right to show they are the same stage, and add a Train label to the dataset in the left box to be consistent with the others. Some explicatory text can be moved to the caption to make the figure itself easier to cartoonify.
*    Line 47 his -> this
*    Fig2: consider showing “the entities consistent with the QA pairs; the latter get accuracy 0 everywhere” (Line 153) in the plot as well
*    Line 291 vise-> vice
*    Consider adding the number of (entity, entity-string) pairs present in the datasets of Fig2. It might be helpful to add a table with each of the datasets presented in Fig2, showing their characteristics and size / number of entity - string pairs. This would help clarity / readability. 


Limitations:
The mentioned limitations are well selected (formalization of internalization, absence of obvious mechanism). The rebuttal of alternative explanations for the presented results is absent from the paper, a significant additional limitation. The potential social impacts are discussed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The authors assert that ""out of context metalearning makes LLMs better at internalizing useful information for understanding."" They intuitively frame understanding as ""treating content as true in question answering."" They analyze its application to tasks such as mapping novel phrases or words to attributes and then performing question answers.

They introduce ""define tags"" which perform the mapping of novel info rather than using the word ""define"" and natural language, which I like a lot as an approach. This allows them to isolate the effect of the metalearning as a mechanism for improving performance rather than being clouded by the existing notions of the meaning of ""define"" that may be acquired by the LLM during pretraining.

Though I have some gripes that verge on the political for the limitations section I think this is an interesting and well-motivated work that deserves acceptance.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Detailed analysis and clear statement of technique

Weaknesses:
Yudkowsky citation. I think that stuff is fundamentally unserious and hurts my willingness to recommend strong accept or award as an actual NLP expert.

Limitations:
In my opinion, perpetuating AI safety hype in academic papers is inappropriate, and citing Yudkowsky in particular is a negative signal. Opinions of others will vary and ultimately I don't think this is a reason to reject. Just wanted to register my discontent.

Otherwise discussion of limitations is strong.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors introduce the phenomenon of internalization in LLMs, specifically ""weak internalization"" and ""strong internalization"" (out-of-context meta-leanring). Weak internalization refers to LLMs' improved performance on questions with consistent definitions rather than with inconsistent ones. Strong internalization involves LLMs' ability to provide better answers for variables with a defined tag representing a consistent definition, demonstrating out-of-context meta-learning. The paper includes ablations to support their findings and discusses the limitation of the work.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper presents an interesting case of ""internalization"" in LLMs. 
- Authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Weaknesses:
Major Concerns.
- Not enough models are analyzed in ablations. The paper presents only the evaluation of Pythia and T5 family of models and claims that the internalization phenomenon is quite general. I would suggest performing experiments with more recent models such as LLaMa, T5Flan, etc.
- The number of datasets presented for evaluation is also quite small. Although, I understand that creating datasets for this specific format could be expensive.
- it is unclear if the size of a model affects the internalization phenomenon. Ablations of a few models varying their size would help to solve this doubt.
- Not clear how the phenomenon of internalization can be taken by the community in order to improve or avoid pitfalls regarding the development of LLMs

Minor comments:
- The paper mentions several times to look at the appendix but doesn't indicate to which section the reader should pay attention. I would suggest indicating the specific section in order to improve the readability of the manuscript.
- The notations of the datasets are quite difficult to follow, I think authors could provide a general overview (in a table or any other format) rather than explaining each component in line with the text. This would improve the readability of the paper.

Limitations:
Yes, the authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Rating:
6

Confidence:
3

";0
p40XRfBX96;"REVIEW 
Summary:
The paper proposes a new method to generate diversified, principle-guided synthetic data from LLM itself to ease requirement of large amount of annotated instruction-following data for supervised fine-tuning task for LLMs. The generation process follows four steps. First step is to generate adversarial topic-guided instructions, second step is generate responses using in-context learning with pre-defined principles, third step is supervised fine-tuning with generated instruction-following data. Fourth step is to leverage context distillation for verbose cloning. `LLaMA-65b` model is fine-tuned with the data generated from the aforementioned approach. Evaluations are conducted on TruthfulQA and Bigbench-HHH Eval dataset. The fine-tuned model shows strong performance. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper combines the approaches of self-instruct paper and constitution AI paper and proposed the topic-guided, principle-following self-instruct way of data generation which could ease requirement of large amount of annotated data and improve diversity of instruction-following data. Although nothing is new, the paper follows a clear and logical path to generate synthetic data in the era of LLMs. Given a strong pretrained model, we have reason to believe this will work pretty well which is to some extent verified in the evaluation results. 

Weaknesses:
- The pretrained LLMs are not instruction fine-tuned. It could be challenging to generate clean topic-guided instructions and principle-following response. The paper didn't talk if there is any filtering step following these generation steps. 
- How much data is generated, how much data are there in the eval set. These numbers are not shown in the paper clearly. 
- Evaluation is not enough. We might need more evaluations on instruction-following dataset
- Verbose clone is not working well, and no reason or discussion is given as why this is the case.

Limitations:
None is given

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes Self-Align, a method for aligning a language model from scratch (without previous RLHF training) with fewer annotations. Self-Align works by using the LM to generate a set of example instructions/tasks, generating from the LM conditioned on the instruction + a human-written set of principles, and then distilling this back into the model by finetuning without the principles and demonstrations in context. The paper shows that this method improves performance/accuracy and (synthetic human) ratings on variety of benchmarks.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- simple and clearly presented method
- strong performance / thorough comparison to both open- and closed-source baselines


Weaknesses:
- generally seems to underperform Vicuna, even though the Dromedary model is 5x larger. I still see that the Self-Align methodology could be a contribution (with better understanding of how much the different parts of the methodology matter), but there may not be a reason to build on the Dromedary model when Vicuna is available / more accessible in size.
     - If this is the case then perhaps the paper could be stronger if it shows that the Self-Align methodology still provides improvements *on top of* Vicuna's alignment (i.e. applying Self-Align with Vicuna as the base model).
- lacking in ablations: I don't have a great sense of how much the different design choices are contributing to the final performance of the model, e.g.: sensitivity to self-instruct instructions, particularly the 20 ""topic-specific prompts"" (were these hand-crafted to match the downstream tasks?), importance of / sensitivity to ICL examples during self-alignment (both the specific 5 examples and the number of examples). It would be great to get a sense of e.g. variance across prompts to understand how much of the method works because of the specific prompts the authors crafted, as opposed to the methodology in general.


Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors use the self-instruct approach combined with principle-driven prompting to self-instruct a pre-trained LLM. The instruction/response generation generally refers to what self-instruct and Alpaca did. The principle-driven prompting can be treated as a SFT version of Constitutional AI. Self-instructed LLaMA-65B can achieve slightly worse performance than ChatGPT, while outperforms the base LLaMA and Alpaca.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The principle-driven prompting is a very insightful idea to generate a large amount of fine-tuning data with minimal human annotation. The cost of labeling data for LLM alignment is a pain point in today's LLM development. The proposed method can be a worth-attempting approach for researchers with limited labeling budget.
2. The verbose version training and the discussion of verbose tax is very insightful. How to balance model's performance on specific tasks and its HHH is always an important question in the field. The authors show that simply SFT with context distillation has some limitations but can still reach better performance than non principle-driven distilled models.
3. The prompt design is very detailed and careful. Although the idea is clearly inspired by self-instruct and Constitutional AI, the details of the prompt design should be still considered as a novel contribution and helpful to the community.

Weaknesses:
1. The paper doesn't cover preference data generation and thus not applicable to RL tuning. A lot of OpenAI's work and talks (InstructGPT etc.) have stated the importance and necessity of RLHF. The model may finally face a performance ceiling if only using the current SFT-style self-instruct, which might be where the performance gap comes from. This is not only a con of this paper but also for those LLaMA family papers including Alpaca, Vicuna, etc.
2. The self-instruct style of data generation may lead to a narrow data distribution, e.g., the base LLM is not likely to generate a complex math problem and its answer --- and therefore the tuned model would fail to solve difficult math problems (e.g. possibly poor performance on MATH benchmark). This problem may be solved by adjusting the seed prompts and principles. But in general, how this approach generalize to different tasks is not well-discussed.

Limitations:
In the paper, actually there is no fair comparison on fine-tuning on human labeled data v.s. fine-tuning on self-instruct generated data. Dromedary v.s. ChatGPT/InstructGPT, they have different base model. Dromedary v.s. Alpaca/Vicuna, latter's data are not human labeled. But this is a very important question for the developers in LLM alignment teams as they have to decide whether to spend money on human labeling and how much. The discussion on this topic is kind of beyond this paper, but is an important question remains in future work.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The authors study the problem of language model alignment and propose to leveraged hand-crafted prompts, principles, and examples to provide guidance, instead of relying on manually annotated human preference data. The authors make comparisons with various ai systems and the results demonstrate the effectiveness of the proposed approach. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The studied problem is important, the proposed solution is novel, and the empirical performance is good. Also, systematic analyses are provided to better understand the effectiveness of the proposed method.  

Weaknesses:
The experiments mainly focus on demonstrating the effectiveness of the proposed algorithm. It would be better to have more analyses on the algorithm design. For example, whether the effectiveness of the algorithm has a heavy dependency on the number/quality of prompts. 

Limitations:
As mentioned before, it would be better to have analyses of the performance with various number of prompts/examples. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
Paper presents a SFT approach for instruction fine-tuning with minimal supervision.

(1) First uses self-instruct approach to augment instructions;\
(2) Using human-written rules & in-context demonstrations for thought process of response, final responses are generated by foundation models, and then distilled to model;\
(3) Further conducts context distillation to make verbose output;

Results show pretty powerful performance compared to other open-source and API models.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed approach is easily comprehensible and showcases its effectiveness in producing instruction question-answer pairs to train instruction-following models with limited human supervision.
This approach holds significant importance for the open-source community as it demonstrates a strong commitment towards democratizing large language models.

Weaknesses:
Paper shows cost-efficient approach to create powerful instruction-tuned models but I have following minor concerns.

1. According to the evaluation results depicted in Figure 5, it becomes evident that incorporating few-shot examples is essential in achieving high-quality answers. This observation leads to the suspicion that the model must need in-context examples to generate high-quality answers since you distill outputs from in-context learning (Appendix Figure 5 also shows that zero-shot performance is worse compared to other “instruction-tuned multi-turn'' models). This observation suggests that there is still a requirement for answers generated by human writers (e.g., LIMA and LLAMA2) or responses derived from instruction-tuned models (e.g., Alpaca and Vicuna), rather than depending on outputs generated by in-context learning, despite the associated costs?

2. In-context learning pipeline described in this paper incorporates a combination of instructional guidance (rules) and few-shot examples (thought process of response). I believe instruction-tuned models may have better capability for this kind of in-context learning compared to relying on foundation models (Wei et al., 2023). 

[1] LIMA: Less is more for alignment., Zhou et al., 2023\
[2] Larger language models do in-context learning differently., Wei et al., 2023


Limitations:
None

Rating:
6

Confidence:
4

";1
LAGxc2ybuH;"REVIEW 
Summary:
The work proposes a new kind of Shapley value (SV) based explanation method. The new methods GP-SHAP and BayesGP-SHAP are like TreeSHAP, LinearSHAP or RKHS-SHAP model-specific SV methods for Gaussian Process (GP) models. The authors further propose another approach of utilizing GP models to amortize (i.e learn) SVs for a model and data point in a predictive task (like FastSHAP). The authors name this the Shapley prior. The two main contributions (model-specific) SV method and the Shapley prior is illustrated in experiments with well-established benchmark datasets.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper has many strengths. The work is theoretically well-formulated and motivated. It combines uncertainty research with XAI research to enrich the quality of explanations. This is very important research. The contribution is well formulated and well motivated (formally and verbally). The experimental analysis of the model-specific GP-SHAP and BayesGP-SHAP is convincing. While the work is very theoretical in nature, it is still very pleasant to read, which is hard to achieve, since bridging the gap between two streams of research is not easy. 
The work adheres perfectly to the style guidelines, includes a well-rounded appendix with code and a small readme file for easier reproducibility. The proofs are to the most part understandable.

Weaknesses:
The main weaknesses I identified are elaborated below:

1. The predictive (FastSHAP-like) approach to learning SVs with GP models and the Shapley prior feels a bit tagged on and underdeveloped. In lines 256-258 the argument is made that GP models are trained only with x (instance) and y (SV for instance) pairs and the model is not required to train. This is posed as a positive side of this Shapley prior approach. This, of course, is correct that you only need the SV to train, however, doesn't this make the GP models less effective in comparison to, for instance, FastSHAP which is trained on this Shapley loss function (i.e. MSE over model output of coalition with feature i than without feature i). The main benefit I see is that you now have a GP model rather than a big black-box NN like FastSHAP proposes. The GP models, therein, are more explainable which is a desideratum in my opinion for an explanation system (this ""explaining a model with another model that needs to be explained in itself"" can be spun ad absurdum). However, the paper is missing such an experimental comparison of the Shapley prior with FastSHAP or an in-depth discussion of the benefits in my opinion. I want to reiterate that this is not a big problem, however, it is left hanging.

2. The work has some accessibility issues from a ML perspective. Some concepts are hard to follow even with a good background in SV and statistics. I recon that ML readers might have a hard time understanding the contribution well because they already have problems with the problem definitions (beginning of 2). If space allows I'd suggest to certainly enrich the definition section (lines 81-92) with some explanations.

3. The connection to uncertainty research could be a bit stronger. I.e. it would be nice to include a more extensive discussion of what parts of uncertainty are captured in the confidence intervals (i.e. aleatoric or epistemic)?

Minor things:
- Line 11 ... ""extensive illustrations"" sounds a bit counterintuitive consider changing this
- Write out the main contribution again in the introduction in a seperate paragraph that is easily digestible for ML engineers
- Line 134 ""rvs"" is introduced but (I think) not used. Even if it is used consider removing this abbreviation
- Lines 235-240: I would like to see a more extensive discussion here.

Limitations:
The work includes a limitation section discussing (a) limitations and (b) future work. However, the section is quite short and focuses solely on future work and avenues to explore (e.g. extension to Bayesian NN). A more detailed discussion of the limitations of this particular result would be beneficial.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors consider Shapley values computed from games $\nu$ modelled with a Gaussian process. Since the game is stochastic, the Shapley values are now stochastic, and the stochasticity reflects epistemic uncertainty in the explanations. This is unlike previous methods like BayesSHAP, where the uncertainty is estimation error. The approach of the authors, called GP-SHAP, can be used to explain predictions of Gaussian processes. It can also be used to construct a prior over Shapley values, which in turn can be used to perform regression on Shapley values. The authors demonstrate the utility of their approach is three experiments.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
I would like to thank the authors for their submission. I think this is interesting work.

## Strengths

* The paper is well written and mostly clear. The presentation is reasonably good.

* Pushing a GP through the value allocation computation to explain stochastic predictions seems like a very sensible thing to do.

* Doing the above to define a prior over value allocations and then using this prior for regression of Shapley values is, to the best of my knowledge, novel. I have not seen this before. I should, however, say that I'm not at all familiar with this part of the literature.

* The stochasticity in explanations by GP-SHAP represents epistemic uncertainty in the explanation, whereas the stochasticity in explanations by BayesSHAP is due to estimation error. I think this is an interesting finding.

* I have either skimmed through or looked in more detail at the proofs of the mathematical statements. Apart from some clarity issues (see below), I think everything generally checks out.

* The experiments nicely illustrate the benefits of GP-SHAP.


## General Remarks

* In Proposition 11, I was initially very confused by that the expectation of $v_x$ appears. After reading the proof, it became clear that this is an approximation that simplifies the computation, which should actually integrate over $f$. I think this can be better explained in the statement of Propostion 11.

* In Proposition 12, if I'm not mistaken, the transpose in the expression for $\kappa$ refers to taking inner products in the RKHS of $k$. This is _definitely_ not clear at all and should be clarified!

* In Figure 1, how does BayesSHAP produce its explanations? It is applied to the predictive mean of the GP prediction?

Weaknesses:
## Weaknesses

* I think that providing a proof for Theorem 4 is unnecessarily complicating the exposition, as the Shapley's original proof for the deterministic case can be applied to immediately prove Theorem 4: Let $(\Omega, \mathcal{F}, \mathbb{P})$ be the underlying probability space. For any fixed sample $\omega \in \Omega$, $\nu$ is d-game (using the authors' language). Therefore, according to Shapley's original proof, for that $\omega \in \Omega$, it can be written in the form of (1). Since (1) holds for all $\omega \in \Omega$, it obviously holds as an equality in terms of random variables.

* Generally, I think that the exposition takes a long time to arrive at the key idea of the paper: model $\nu$ with a GP, and push it through the value allocation computation in (1). I think that the exposition would be improved by having Section 3 starting out with immediately explaning this key idea. A paper is not supposed to be a novel: please signpost important ideas and conclusions as much as possible.

* In lines 326-339, you highlight the difference between the mean of absolute SSVs and the absolute values of mean SSVs, and state that this gives a different ordering for which feature is most influential. However, you do not analyse whether this different ordering is better or worse, which means that it is not clear whether, in this case, GP-SHAP produced a better ordering or not.

* In lines 340-346, you produce a local explanation graphical model. However, you do not analyse whether this graphical model is reasonable, which means that the reader is not sure whether GP-SHAP did something sensible or not.

* Building a GP prior over Shapley values to do regression is an interesting idea. However, the presentation would be more convincing if you were to point out some existing applications that would actually want to do regression of Shapley values.


Limitations:
See above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This works introduces the stochastic Shapley values to the explainable AI community. It formulates the stochastic Shapley values for GP models with a closed-form expression for the covariance matrix. Then, it proposes BayesGP-SHAP, which incorporates both sources (of uncertainty from the GP posterior and of the Shapley value estimation). Lastly, it constructs an inference model by proposing a GP prior to the Shapley value to solve the problem of predictive explanation.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
I enjoy reading the paper as it contains several major strengths: 
+ The motivation of predictive uncertainty in trustworthy ML models is convincing, which paves the way to the proposed GP-SHAP and BayesGP-SHAP.
+ The literature review is sufficient to highlight the novelty of the approach.
+ It introduces concepts of stochastic cooperative games and stochastic Shapley values to the explainable AI community.
+ GP explanations not only satisfy favorable axioms to standard Shapley values but are also able to measure explanation uncertainties and determine statistical dependencies between explanations. 
+ The proposed BayesGP-SHAP integrates both sources of uncertainty from the GP posterior and the estimation of the Shapley value.
+ It proposes the Shapley prior to dealing with the problem of predictive explanation, which is a nice Bayesian treatment to the predictive explanation problem.
+ In the experiments, the paper introduces several exploratory analysis methods for practitioners to understand stochastic explanations.


Weaknesses:
The paper may elaborate on other use cases of predictive explanations. Anyways, I find the Bayesian optimization application in the conclusion interesting.

There are very minor issues in the references as some words are not capitalized, e.g., Shapley.


Limitations:
The authors have adequately stated the limitations in the appendix.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper introduces a novel approach for explaining Gaussian processes (GPs) by utilizing the analytical covariance structure present in GPs based on extending the concept of Shapley values to stochastic cooperative games, resulting in explanations that are random variables. The GP explanations generated using this approach satisfy favorable axioms similar to standard Shapley values and possess a tractable covariance function across features and data observations. The proposed approach, GP-SHAP, and its variant BayesGP-SHAP, are shown to be effective in providing explanations for Gaussian process models.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper presented a novel and principled approach for explaining GP model using stochastic Shapley values. The proposed approach is sound and thoroughly evaluated. It also fills the void that there is no model-specific explanation method for GP. 

Weaknesses:
The paper should also discuss the limit of using Shapley value in explanation in general.  

Limitations:
Shapley-value based explanations have some limitations -- two references are provided as follows. 
* Shapley Residuals: Quantifying the limits of the Shapley value for explanations (https://proceedings.neurips.cc/paper/2021/file/dfc6aa246e88ab3e32caeaaecf433550-Paper.pdf)
* Problems with Shapley-value-based explanations as feature importance measures (https://arxiv.org/pdf/2002.11097.pdf)

These limits are inherited in this proposed method. 

Rating:
7

Confidence:
3

";1
Ge8Mhggq0z;"REVIEW 
Summary:
1. proposing a pretraining framework for conformation space including equilibrium and off-equilibrium.
2. this method also shows highly accurate in MD or relax, including molecules and polymers.
3. providing the community with a diverse set of DFT dataset of polymers.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well-written and well-organized, effectively presenting the significance of force-centric pretraining for molecular systems. The proposed framework demonstrates comprehensive and impressive results in molecular dynamics simulations of small molecules and polymers, as well as in quantum chemistry property predictions.
2. The authors also introduce an innovative concept in their work: the de-noising equilibrium balance and off-equilibrium co-training method.

Weaknesses:
1. As previously mentioned, I am particularly intrigued by the efficiency of the de-noising equilibrium tasks. It would be better to conduct an detail ablation study of de-noise equillibrium and force regularization.


Limitations:
In the discussion section, it is mentioned that both the backbone and time efficiency play crucial roles in the performance of the proposed AI model. 
Besides I recommend incorporating partial charge prediction during the pretraining or downstream stages of your framework, if feasible. This addition could potentially enhance the overall effectiveness and applicability of your model.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper incorporates off-equilibrium molecular conformations into consideration, in which the energy is not minimized. This paper defines pre-training objectives for force field learning on sampled off-equilibrium molecular conformations. Experimental results validate the effectiveness of pre-training on additional off-equilibrium data. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1) The presentation of the paper is quite clear and easy to follow. The writing is well-organized;

(2) The proposed idea is quite interesting and necessary. How to utilize off-equilibrium data is a useful and important research question for pre-training 3D molecular property prediction. This reviewer agrees with the significance of the proposed idea. 

Weaknesses:
(1) The novelty of the proposed method is not very high. Considering there are already published papers researching denoised pretraining strategies on 3d molecules, which lowers the novelty of the proposed method a bit. Then the major contribution of the proposed method is mainly about introducing additional data. This reviewer suggests that this paper might be more suitable for a domain journal; 

(2) The contribution of this paper might not be enough. Currently, the major contribution includes introducing additional data, and additional pre-training objectives. There are some important works like SCN (Spherical channel networks for modeling atomic interactions) utilizing separate force predictions. So separately predicting forces cannot be regarded as an important contribution;

(3) The experimental results do not demonstrate the effectiveness of the proposed method. It seems that the proposed method seldom outperforms previous baseline methods, which makes this reviewer feel a little bit confused. 

Limitations:
The major limitation of the proposed method is that the force prediction might only play a role as a regularizer since learning force field is also a very challenging task since force is a vector feature instead of a scalar feature like energy. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper propose a set of additional objectives for training machine learning interatomic potentials, in particular for equilibrum conformations as well as a denoising term. They show that this improves stability, when pretrained on a large dataset. They also introduce a new dataset of polymers.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The improvement from ET-ORE to ET-OREO is interesting and surprising. It doesn't really make sense that equilibrum forces should help The polymer data set is great!

Weaknesses:
Probably the biggest weakness is that the authors chose a weak baseline model with TorchMDNet. The entire paper hinges on work from Fu et al. that showed that in their experiments all methods except the NequIP potential were unstable in MD simulation. With many additional tricks and an enormous amount of data they now match NequIP. But will this entire improvement just go away when you apply the ideas to a stronger baseline model like NequIP? I would strongly suggest the authors to repeat the experiments with a better baseline and see if there are still improvements. Otherwise why wouldn't anyone just use NequIP to begin with? The speedup is small and as explained below mostly an artefact of hyperparameters.

Finally, a central weakness it that ET-OREO is pretrained on a massive dataset while the baseline methods that are stable can do this from a much much much smaller dataset. 

Another big weakness is that this unexplained improvement from ET-ORE to ET-OREO simultaneously adds the equilibrum and the denoising objective. Why no ablation study to understand which of them is the important one?

The inference speed comparison to NequIP is not thorough. NequIP has shallow pareto curves on accuracy vs efficiency which is well known, i.e. a L=2 models gets almost exactly the same accuracy as L=3, but is much faster (see original NequIP paper). The paper compares to a NequIP model that is solely optimized for accuracy, not for efficienc. This would be helpful to compare to. In addition, NequIP modern versions of NequIP now exists that are faster, e.g. Allegro or MACE. 

The presentation in addition has several obvious, outlined item-by-item below: 

- ""they are locally well-defined for atoms and generalizable across various molecules"" --> this statement is clearly not true in this generality. In what way is a force generalizable across molecules? Needs clarification what is meant here? 

- following immediately: ""they reflect quantum mechanical interactions among atoms and expose the underlying quantum structures of molecules;"" --> extremely generic, if even true, atoms should be electrons here... 

- ""By harnessing these factors, we posit that learning from atomic forces can
57 guide the model to learn a comprehensive and unified landscape of molecular conformations across
58 different molecules, datasets, and equilibrium states"" --> this has been explored for decades in every single paper training interatomic potentials, leveraging atomic forces is in now way a novel idea, this needs a strong rephrasing. 

- ""We introduce a novel force-centric molecular conformation pretraining paradigm that trains
87 a unified conformation representation encompassing both equilibrium and off-equilibrium
88 molecules"" this is a vast overstatement, see above

- 'The above-mentioned papers followed the same learning paradigm, where different models are
116 trained for different molecules. Consequently, these methods cannot be easily generalized to unseen
117 molecules"" --> there have been many efforts at building general-purpose force-fields, see e.g. the ANI series. 

- "". Despite their impressive accuracy in static prediction, their ability
119 of running simulations were not reported until [26] showed that the increased static accuracy does not
120 translate to better simulation performance"" again, horrendously false statement, many of the MLIP papers you cite ran various simulations, across diverse and difficult phenoma from ionic diffusion to heterogeneous catalysis to small molecules. 

- ""In molecular dynamics (MD) simulations, the molecular
149 conformations are moved according to forces and the thermostat of the simulation."" what about NVE simulations that don't have a thermostat? 

- ""For equilibrium molecular conformations x ∼ E , we assume their forces to be zero and impose a
zero-force regularization:"" --> what is the reasoning behind this, if you *have* the forces anyway, they will be zero at equilibrium, so you're just again training on forces, again what this community has been doing since the 60s. 

- the denoising objective seems like it has obvious failure cases: perturbing a relaxed structure and argueing the force must go into the direction of the GS completely ignores the many-bodyness of the PES, discussing these would be helpful

- a lot of the work seems to be based on improving one or two specific previously introduce pretraining objectives for equilibrium data. In the opening of the manuscript, these are cited several times, but never explained. This makes the manuscript difficult to read.

Limitations:
N/A

Rating:
5

Confidence:
5

";1
nafgeYknRT;"REVIEW 
Summary:
The paper presents a diffusion-based approach to compressing a policy archive discovered by a Quality-Diversity RL algorithm. The diffusion model operates in the latent space of a VAE and achieves high levels of compression together with a reasonable level of reconstruction. The algorithm is evaluated on a collection of 4 Brax environments.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- Clearly written paper and description of the approach
- Promising results showing high levels of compression on some Brax environments, together with good levels of reconstruction and coverage
- Strong visualizations of behavior during training and different synthesized behaviors

Weaknesses:
- The approach studied in the paper is limited to compression of the original archive. Equally, the sequential behavior composition experiments only reproduce what was possible with the original archive. An interesting next step would be understanding if the diffusion model can generalize to novel measure vectors or language instructions.
- The paper assumes very small 2-layer, 128-width MLPs trained by the QD algorithm, it is unclear if the algorithm can scale to larger and more representative networks
- High loss in diversity particularly on the ant environment, in Table 1.
- Line 8 in the Abstract, should clarify exactly what environments the authors see the compression ratio/coverage on

Minor:
- Line 42: typo in ‘uspample’
- Scale is hard to see in Figure 3, axes should also be described
- The idea of compressing policies into a single diffusion model is related to [1] which compresses offline RL datasets into a single diffusion model, [1] also achieves around 13x compression.
- It would be helpful to also indicate the level of compression in Table 1.

[1] Synthetic Experience Replay. Cong Lu, Philip J. Ball, Yee Whye Teh, Jack Parker-Holder.


Limitations:
The limitations of the method are discussed well in the paper.


Rating:
5

Confidence:
3

REVIEW 
Summary:
Quality-Diversity Reinforcement Learning generates a set of policies (here called the archive) that are learned to produce varied behaviors in the environment.  These archives can be large, and this paper aims to compress a previously learned archive into a single model by leveraging a conditional diffusion model.  Each of the policies in the archive is first represented in a latent space by using a variational auto-encoder that reconstructs the weights and biases of the policy's neural network.  Once the encoder and decoder of the policies have been trained, a diffusion model is then fit to the encoded latents, and conditioning information is used to help guide sampling.  The paper explores conditioning based on the measure of a policy (where the measure is a set of functions used to split the policies into different regions of behavior space), or text descriptions of the policies.  The paper shows that the learned diffusion model can generate policies that return similar rewards as the original archive in aggregate, and also have high overlap with the conditioning measure when the sampled policy is executed again in the environment.  

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper proposes an interesting application of powerful generative models to fit a Quality-Diversity archive of policies.  The ability to reconstruct the full archive from a single generative model increases the practicality of QD approaches to skill discovery in reinforcement learning.  The paper does a good job highlighting this contribution, and it is indeed an intriguing direction.  

Weaknesses:
- Evaluation of the model is thorough, in that ablations and several domains are explored, however it is difficult to assess the quality of the approach given that no alternative approaches are attempted.  In line 210, the paper argues that other approaches to archive distillation are not comparable because the underlying archive is different.  I disagree: since the main contribution of this paper is a distillation method, it should be able to compared to other distillation methods when the archive is held fixed.  
- I find the metrics used to evaluate the model difficult to interpret, perhaps related to the lack of baseline.  The Mean-Error in Measure (MEM) metric described by the paper is a reasonable one, but the paper does not describe what measures are used in the various tasks, so interpreting the scale of MEM is difficult.  QD score is similarly difficult to interpret.  Is a decrease of $0.6 \times 10^7$ QD score a meaningful one?
- The experiment on sequential behavior composition is similarly difficult to interpret.  Is 80% success of 4 consecutive behaviors good?  Perhaps including the success rate of the original archive would be a good start.  
- Graphs in Figure 3 are not immediately interpretable without prior exposure to that form of QD visualization.  

Limitations:
The authors have addressed the method's limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper tried to solve the high space complexity in Quality Diversity and proposes a method that uses diffusion model to distill the archive into a single generative model based on policy parameters.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
* This paper leverages the generation power of diffusion model and condenses one model instead of thousands of policies.
* This paper is well structured.

Weaknesses:
* Quality Diversity is not well-known. It's better to include a background section instead of including basic knowledge in the related work. 
* It's confusing no baselines can be directly compared. The metrics can not only be rewards but also space efficiency.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work presents a novel framework using latent diffusion models to distill the archive of polices into a single generative model over policy parameters. The latent diffusion model with VAE backbone compresses the high dimensional neural network (NN) parameters into a compact, making it possible to reconstruct policies parameterized by NN. Further, the conditioning mechanism of diffusion models is used to flexibly generate policies with different behaviors.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The author(s) use latent diffusion model to compress the high dimensional neural network (NN) parameters into a compact, making it possible to directly generate policies parameterized by NN.
2.	The proposed framework recovers 98% of the original rewards and 89% of the original coverage while achieving a compression ratio of 13x.


Weaknesses:
1.	The performance and accuracy of the proposed method show significant discrepancies when applied under text conditions. Experiments show that the success rate of the method is influenced by the selection of text labels. This suggests that the model possesses limited understanding and generation capabilities in terms of language descriptions.
2.	The proposed method shows poor performance on tasks with high-dimensional measure vector, such as Ant, suggesting its limited modeling capability in high-dimensional measure spaces. 


Limitations:
1.	The author(s) only demonstrated the model's ability to reconstruct the original dataset and did not conduct further experimental demonstrations regarding its generalization capability. However, the generalization ability should be an important consideration when evaluating generative models.
2.	At present, it appears that text conditioning does not show a sufficiently favorable influence within the model.


Rating:
7

Confidence:
4

";1
dJZ3MvDw86;"REVIEW 
Summary:
This paper focuses on counterfactual data augmentation for training robust machine learning models on text data. The concerned applications such as healthcare are safety-critical, highlighting the importance of this work. To generate data, the authors propose to employ a Large Language Model (LLM) to rewrite the documents. The authors also propose utilizing auxiliary data in the generation process to encourage robustness. In the experiments, the proposed method is proved to be both robust and effective.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The research topic, training robust classifiers on safety-critical applications, is clearly an important task. Therefore, the positive experiment results as reported are expected to bring practical value to the real world.
2. The model performance reported in the experiment section is good. Normally an out-of-distribution generalizable model sacrifices performance in the in-distribution setting. It seems that the proposed method does not suffer from this issue.
3. The paper is overall well-written and easy to follow. The experiment settings in the main paper and Appendix are detailed.

Weaknesses:
1. The novelty of the proposed method may be limited. It seems that the main difference between the existing and the proposed method is introducing auxiliary data, which may not be considered as a significant technical advance.
2. The competitors in the experiments are rather basic. In particular, it may be worth considering including IRM (Arjovsky et al., 2019) and GroupDRO (Sagawa et al., 2019) (or Just-Train-Twice (Liu et al., 2021) if the authors are interested in re-weighting) for comparison.
3. As this research targets at safety-critical applications, there are some details regarding model bias and robustness may require more clarification. The questions are in ""question to authors"".

Limitations:
 The authors adequately address the limitation.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper addresses the issue of text classifiers relying on spurious correlations, leading to poor generalization vis-a-vis out of domain, particularly in critical areas like healthcare. The authors propose using counterfactual data augmentation, guided by knowledge of causal data structure, to create text classifiers more robust to distribution shifts. Using an LLM to implement this approach, they demonstrate effectiveness in tasks such as predicting clinical diagnoses from medical narratives and observe improved out-of-distribution accuracy compared to baseline algorithms. They suggest that their method could be beneficial in dealing with various distribution-shift problems in machine learning applications. They demonstrate the utility of using language models to create counterfactuals that could improve model robustness. The paper also goes a step further in formalizing counterfactual data augmentation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors experiment with both real-world clinical diagnoses scenarios and semi-synthetic data, demonstrating the broad applicability and practicality of their proposed methodology. Working with healthcare data isn't always easy either, so credit for that choice as well.
- By using the capabilities of large language models for counterfactual generation, this work expands the potential applications of counterfactually augmented data while reducing costs associated with manually constructing counterfactuals.
- The paper offers a formalization of counterfactually augmented data, which paired with prior work allows easier understanding and replication of the process for future researchers.

Weaknesses:
- The method pre-supposes full knowledge of the causal structure of the data and overlooks complexity involved in real-world datasets where such information might not be readily available or accurately defined. Furthermore, assuming ""no unmeasured confounding"" isn't always realistic in real-world data sets especially those from healthcare, possibly limiting the model's robustness within certain contexts. Granted it is a common assumption in causal inference, but causal inference methods are also usually restricted to simple worlds that can be represented in a few variables. 
- The authors acknowledge that generating versions of clinical narratives as if they had been written by different caregivers is difficult to achieve in practice. However, there's not enough discussed on substantive solutions or workarounds they employed (or considered) to meet these challenges.
- The paper needs greater discussion of prior work that has attempted to address these issues, including attempts at formalizing counterfactually augmented data and counterfactual invariance (see [1] and [2]).

[1] Divyansh Kaushik, Amrith Setlur, Eduard Hovy, and Zachary C. Lipton. ""Explaining the efficacy of counterfactually augmented data."" ICLR 2021.

[2] Victor Veitch, Alexander D'Amour, Steve Yadlowsky, and Jacob Eisenstein. ""Counterfactual invariance to spurious correlations: Why and how to pass stress tests."" NeurIPS 2021.

Limitations:
The limitations are adequately discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper develops and analyzes the counterfactual data augmentation strategies for the setting where the data-generating process is anti-causal. First, they first show the Bayes optimal predictor in the setting they considered and then how can we obtain that predictor using counterfactual data augmentation. They give two different strategies to generate the counterfactual data (1) by prompting LLM and, (2) using the difference in difference method. Then they analyze the sample complexity of counterfactual data augmentation and show that it is better than that of a baseline reweighting method. Finally, they empirically show the effectiveness of their method on synthetic and real-world datasets. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**Clarity**: The paper will be well-written and easy to follow.

**Sample Complexity Analysis**: The authors show that the sample complexity of counterfactual data augmentation is better than the reweighting baseline introduced in Makar et. al.


Weaknesses:
1. Contribution 2 - Counterfactual Data Augmentation (CDA) as a method to deconfound target and spurious attribute: It is not clear how this observation/finding is different from the previous literature on CDA eg. Kaushik et. al. (Explaining The Efficacy of Counterfactually Augmented Data) and Joshi et. al. (An Investigation of the (In)effectiveness of Counterfactually Augmented Data). 
2. Lemma 1 is similar to the claim shown in Makar et. al. (Causally motivated shortcut removal using auxiliary labels)
3. Assumption 1 (constant effect): How justified is the assumption that the spurious attribute $c$ change the previous state in an additive manner?
4. The augmentation strategy in this paper is limited to the anti-causal setting which further limits the applicability. I understand this is standard practice in the OOD generalization literature to develop a method under specific data-generating process (causal and anti-causal) but then this paper doesn’t introduce something new than the previous works.

Overall this paper seems to re-instantiate the point mentioned in previous work that argues for the effectiveness of counterfactual data augmentation. Though this paper introduces two new ways to perform data augmentation in the context of medical datasets, the overall novelty seems low.


Limitations:
NA

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose to use knowledge from the causal structure of the data to counterfactually simulate interventions on spurious features and to learn more robust classifiers. They focus on text classification tasks and argue that this approach is appropriate in prediction problems for which the label is spuriously correlated with an attribute.

Since the authors argue that their approach emulates interventions, one assumes that they referring to the second layer of Pearl's Causal Hierarchy. Here, we assume the data and the Causal Graph (CG). However, the authors left the specification of the causal graph to be addressed as future work. 

Additionally, following the Pearl's Causal Hierarchy, when leading with counterfactual questions, one must assume the data and the Structural Causal Model (SCM). Given that the authors do not provide neither the CG nor the SCM, I have concerns on how their approach generates counterfactually augmented data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper presents an interesting approach to counterfactual-based augmentations focusing on textual data.

Weaknesses:
There are some important questions to better understand the proposed approach and the validity of results.

Limitations:
I believe that the authors only mention limitations of their work in the last sentence of the Discussion section. I believe that would be interesting to improve this discussion. For instance, it is very hard to have the correct Causal Graph for most important applications, hence, how this could be an issue, and, in the absence of the causal graph, what is the best approach?

Another issue is how can we guarantee that the counterfactual instances generated during augmentation are realistic?

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this work, the authors develop causally-driven data augmentation methods to improve model robustness.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
In general, I find the paper to be very, very well written and clear. The authors do a really great job of explicitly stating their assumptions, and acknowledging when certain assumptions are strong. I think this paper makes a very interesting first step in extending OOD generalization to the recent advances in LLMs!

Weaknesses:
I would have liked a section that describes some of the limits of using LLMs, and whether certain LLMs would be more appropriate that others. It feels like incorporating LLMs is a big part of this work, so I would have liked more context here.

I wasn't completely convinced that the extra information M would be enough to assist the model in achieving more accurate estimates, but the authors do acknowledge that this is a strong assumption.

Limitations:
The authors do acknowledge limitations of their work.

Rating:
8

Confidence:
3

";1
UlHueVjAKr;"REVIEW 
Summary:
The authors explain a method to improve the performance of a speech language model by reusing the weights of a language model trained on text. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Using a well-trained text-based language model as an initial model seems like a good idea.
If the authors' claims can be generalized, it can be used as a good starting point for speech language models, which are relatively difficult to train.

Weaknesses:
The authors empirically discovered that they achieved better performance in the same training step, but there seems to be a lack of theoretical analysis. The process of analyzing, estimating, and confirming evidence for why such improvement occurs is missing. This simple empirical discovery without the analysis and validation of why it happens would have limited contribution to the conference-level community.

Limitations:
The limitations have been well described.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes TWIST, Textually Warm-Initialized Speech Transformer-based LMs, a technique to initialize SpeechLMs with pretrained textual LMs. Different textual LMs, tokenizers, models and dataset sizes are evaluated using TWIST. The authors find that a warm start with a textual LM helps compared to a random initialization when evaluated on a number of metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- TWIST shows how a warm start with any pretrained textual LM (OPT, Bloom, Pythia) benefits a speech-based LM.
- This is one of the first works to use large-scale audio data (i.e., approximately 150K hours of speech) to train a speech-based LM.
- This work presents a new evaluation benchmark for speech-based LMs based on StoryCloze.

Weaknesses:
Warm start with pretrained textual LMs was found to be effective for speechLMs. A more detailed analysis of the learned representations with and without TWIST would have been useful for the reader. I elaborate on this further in my questions below.

Limitations:
Limitations have been addressed in the submission.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a simple initialization method for speech LMs named, TWIST (Textually Warm-Initialized Speech Transformer Language Models). Instead of cold-initializing a speech LM, twist initializes it with LLM weights (minus the token embeddings, which are replaced with speech vocabulary). The authors show than on OPT models of size 125M to 1.3B (+ BLOOM, Pythia of 350M size), the proposed method of initialization helps in convergence and final perplexity evaluation, as well as shows benefits over cold initialization over human evaluation (MMOS) of generated completions. The authors also create two spoken versions of the StoryCloze dataset.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed method is quite simple (just a straight-forward initialization trick) and the empirical results show small benefits across speech LMs of different sizes.

Weaknesses:
1. The experiments could be more comprehensive, e.g., only one size of BLOOM/Pythia is considered. Similarly, the authors do not report the  PPL over the transcription of the generated speech. Even if the results are high-variance, it is an important aspect of evaluation as far as the quality and diversity of speech continuations is considered.
2. The proposed method is extremely simple, it just initializes the speech LM with LLM weights. There is prior empirical justification to do this, however, the weight spaces as quite incongruent, as acknowledged by the authors, and this problem hasn't been addressed at all. As such, it is hard to consider the contribution very solid in terms of its technical merit. The authors show that text based initialization outperforms Image based initialization, but that again is not a very novel result.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the effect of textual LM on SpeechLMs. They propose TWIST, which initializes the SpeechLM model with a pre-trained textual LM, and then finetune with the speech datasets.
This paper provides a complementary exploration of generative spoken language modeling, including front-end processing of speech tokenizer, the core component of SpeechLM, and back-end speech synthesis of Vocoder.
TWIST uses a warm-start for SpeechLM and outperforms the cold-start model in all experiments. In addition, this paper also provides empirical results across various pretrained models, and presents the large-scale 7B-sized Speech language model.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. This paper provides insight into the design of SpeechLMs based on rapidly developing large- scale textual pretrained models.
2. This paper presents many empirical conclusions and findings, such as the HuBERT setting, results on different scales of training data.
3. This paper introduces new benchmarks for evaluating SpeechLMs.


Weaknesses:
1. Some details are missing. For example, warm-start allows better performance compared to cold-start, but training costs for both methods are not reported. These results help researchers estimate the training costs of large-scale SpeechLMs.
2.  Automatic evaluation results are missed. This paper reports the human evaluation of MMOS for speech generation, but no automatic evaluation results are included, like WER for ASR results. Therefore, it is different to compare results across related work.
3. Although TWIST shows powerful capability of speech understanding, SpeechLMs still lack deep semantic understanding compared to textual models, as stated in Limitation. Therefore, the proposed method is somewhat of a compromise.
 
Despite these issues, I still believe that this work is highly valuable and can provide direction for the following researchers.
 


Limitations:
Yes

Rating:
7

Confidence:
4

";1
gSyjaunurQ;"REVIEW 
Summary:
- This study proposes a new framework to combine neural coding concepts of information transmission and probability density modeling.
- This framework is based on an even code principle where the output response density strives to be even, given some arbitrary input density.
- The authors show that this coding principle produces sensible bases for low-dimensional inputs, and orientation-tuned filters for natural image patches.
- While conceptually straightforward, it is unclear to me whether this study provides unique insight into sensory coding in neural populations.

UPDATE: Sep 1, 2023. I have read the rebuttal, and maintain my score (see details below).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The study is clearly presented.
- The concepts of max/min entropy on the output and input densities are conceptually simple to follow.
- The numerical experiments are reasonable.

Weaknesses:
- This paper begins with what seems like a false dichotomy of information transmission vs. sensory probability density modeling. Indeed, from a pragmatic point of view, how can one guarantee optimized information transmission without having a good density model of the signal to be transmitted? There exists literature in this area (see questions section), and the motivation/framing of this present study is concerning.
- There is a bit of a conceptual leap from 1 or 2 pixels to full image patches, with additional complexity and machinery introduced. The described rationale seems reasonable enough, but it is unclear whether the two-pixel orthogonal case can provide adequate intuition for the multi-dim case. Would a 2D non-orthogonal example be illuminating at all?
- Unclear to me whether these results, which rely on binary coding provides theoretical insight for real neural coding. Spikes are inherently binary, yes, but typically spike counts/rate are what is considered the informative variable in neural coding.

Limitations:
There was no discussion of limitations. Unclear to me what the drawbacks are of this approach compared to existing literature.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper presents a method for the representation of elementary natural images, based on the observation that classical studies in computational neuroscience focus mainly on methods to improve code efficiency, but that this could be complemented by a study of probability density modeling between neighboring pixels to improve image representation. This work consists in studying a coding principle based on a probabilistic representation and its formalization in a form of variational optimization. The paper presents the elementary method for a single pixel, then extends it to two pixels, and applies it to  small images extracted from natural images. This method is enhanced by a heuristic that allows  to formulate a cost function and thus derive an optimization algorithm. The results allow  to numerically validate this principle by deducing output statistics, as well as the emergence of local contour detectors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
A major strength of this paper is that it derives the image representation algorithm from fundamental principles of machine learning, particularly probabilistic representations. In this way, it rigorously defines the problem of establishing dependencies between the luminance values of neighboring pixels.


Weaknesses:
The first limitation of this paper is that it applies to very elementary signals, i.e. a pixel, a pair of pixels, or small images of dots. As the initial aim of the paper is to understand the computational functioning of the biological networks that underlie the efficiency of vision, this approach is extremely caricatural, and dismisses many fundamental aspects, such as the largely parallel processing of large images, the use of large neural networks, or the ability to process multimodal images, in color or in motion, or more generally hierarchical processing that can be forward, but also modulated by feedback signals. Finally, the results that have been obtained, for example for the detection of local elementary contours, are difficult to interpret quantitatively and seem very preliminary.

Limitations:
Finally, these questions about the paper reveal the main limitations of this work.

In particular, the introduction to the paper presents at length principles that seem very general, such as Shannon entropy, and the rest of the paper does not sufficiently highlight the novelties that are brought forward. This brings to light a main limitation of the paper, which is the fact that the propositions that are put forward are very ambitious, but the results are applied to very limited situations.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper explores the relationship between the information theory approach and the probabilistic generative model approach in the context of understanding neural coding. The author suggests that maximizing the information-carrying capacity of output channels and modeling the input probability distribution can be pursued as independent dual objectives. To investigate this hypothesis, the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. The resulting codes obtained for the images exhibit similarities to edge detectors and orientation-selective neurons in V1, akin to many efficient coding models developed over the past two decades.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The presentation is reasonably clear.  It is rather interesting that the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. 

Weaknesses:
While the idea that both information transmission and probabilistic modeling of the images should be taken into consideration simultaneously might be new,  and is sufficient to learn edge detectors and orientation-selective neurons, the author has not established it is a necessary condition. In fact, literature in the last thirty years (from Law and Cooper's to Olshausen and Field and many others)  that such codes can be learned based on either one of the criteria. 

It is surprising that the V1 neural codes were assumed to be sparse binary codes. What is the evidence?  The distribution of output values as shown in Figure 2a has not been observed biologically.  This brings the Even Code hypothesis into serious question. 


Limitations:
Societal impact not discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors studies simple of neural encoding. The question is whether two
distinct goals, accurate transmission of information and learning the
distribution of environmental stimuli can be achieved simultaneously. The
authors argue that yes, it can, using the key assumption of a uniformly
partitioned input space. The coding principle of the authors is finally applied
to image patches, where it yields edge-like features.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The author studies an important question, namely simple coding schemes that
  reproduce filters that resemble those of deep convolutional networks or parts
  of the visual system.
- The author develops intuitions in simple toy models before moving to
  applications on real images.
- The filters shown in Fig. 3 bear a striking resemblance to the filters of a
  trained VGG model (although I have some questions on the methodology, see
  below)



Weaknesses:
I found the article confusing to read in a few places. For example, early on,
the author states that ""maximising the rate of transmission"" is equivalent to
maximising the entropy of the output distribution $H_Q$. I would think that what
you transmission of information requires maximising the mutual information $I(X;
Q)$ between the distribution over inputs and outputs. (around eqs 1 + 2; note
that the notation is rather confusing here, using lower-case $p$ for the
distribution over input stimuli $x$, and capital $Q$ for the distribution over
output states $y$). Why are you maximising simply the entropy of the
distribution over outputs?

Similarly, in the section on the even code principle, I'm confused by the
question of how the IPU models the input distribution. The way I read Sec. 2,
the IPU is considered a function of the stimuli $y=f(x)$ -- in that sense, it
doesn't model the input distribution, we cannot sample from it. It can give a
more or less faithful representation of $x$, as measured for example by mutual
information if the mapping is probabilistic, 

As you then move on to learn two pixel distributions, I'm confused about your
use of MLPs. MLPs are powerful neural networks, but you seem to use them to
""learn"" to partition the input space into equal partitions - is this not
possible by just writing down a simpler model?

Given my trouble understanding the first few sections, I cannot competently
comment on the experimental results - while the filters obtained by the authors
do bear a striking resemblance to the filters of a VGG network, I don't really
understand how the author obtained them. Some additional clarifications would
therefore be more than welcome.


Limitations:
See above

Rating:
3

Confidence:
3

";0
qPyvuFT0U9;"REVIEW 
Summary:
The paper investigate the performance of HSIC to test the independence of two random vectors. The focus is on high but not ultra high dimensional scenarios where the theory is lacking.  More specifically, the paper presents convergence rates for HSIC as the dimensions grow at different rates, and demonstrates how HSIC’s capacity to measure nonlinear dependence evolves as the dimensions increase.  
The paper also shows that the rescaled HSIC converges in distribution to a standard normal distribution under the null hypothesis and provides the conditions needed to have nontrivial power in high dimensions. The theory is validated by simulations and real-world data involving stock prices from the energy sector and raw material sector in the US stock market.





Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This is a solid paper with strong theory and convincing numerical support. A key advantage of Theorem 1, which provides the asymptotic distribution (with rates) of the HISC test statistic under the null hypothesis, is that it alleviates the use of permutation test to decide critical values, a requirement commonly observed in other tests of independence.

The phase transition of the convergence rates in Theorem 3 is illuminating. 


Weaknesses:
None.

Limitations:
Some constraints are imposed in the theory but they are fairly mild and reasonable. 



Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper provides insights into the properties of HSIC in high dimensions, more specifically the rate at which sample size must grow in order to detect non-linear correlations if data is high dimensional. The results are categorized based on scenarios where either one or both variables have a ""growing"" dimension, and they express various types of nonlinearity using conditional expected values of higher orders.

A great summary of the results can be found in lines 61 to 64 of the paper.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper effectively presents the main results and is written with clarity. In my opinion, the results are  significant within the sub-area of independence testing. It has been recognized  for some time that the power of HISC diminishes in high dimensions, but as far as I know, no one has provided conditions on the sample size, dimension and degree of non-linearity for the test to detect signal. Formulation of the results in the language of conditional expected values of higher moments yields insightful and concise characterization of the limitations of HSIC in high dimensions.




Weaknesses:
I'm happy to see empirical study (section 5.3) in a theoretical paper. Having said that, I would be surprised if a linear test would not reject the null  (pool the returns per sector and run a correlation based test).  

Edit: I read the response, and this still doesn't fully make sense to me. I would have chosen a dataset with a non-linear dependence only  and shown HSIC failing. However, ultimately, this does not detract from the quality of the findings and I stand by 7 (8 makes sense too). 

Limitations:
yes

Rating:
8

Confidence:
3

REVIEW 
Summary:
A paper providing tighter analysis and tests for HSIC statistics for independence in some regimes of interest

NB I have only a nodding acquaintance with this statistic, but have used it and regard it as of high importance. I have done my best to learn the background in the time available.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The model provides an interesting and non-trivial bounds for HSIC to test for dependence of high-dimensional covariates, which is an important and useful setting. If I understand correctly they constrain the complexity of polynomial mean dependence that can be detected given covariate sizes and dimensions.

Weaknesses:
The conditions are very stringent, and so we are left tow wonder if the results apply to real problems.

the isotropic kernel choice is a very strong restriction, and one that I would never use in practice - heuristically we expect low discrimination power for kernel-based methods under istoropy for ""most"" kernel methods; it would not be surprsiing if this was true for HSIC in particular. I am not clear how essential the isotropy is but the results start to look like a trivial if that is all they can handle: we might think of these as a ""gaussian thin shell""-type result in that case.

Independence is particularly important when it is conditional at which point it gives us Bayesian networks; Do any of these results survive for conditional independence? The authors mention such applications (l34-l37) but I believe thereafter discard them.

I am taking the validity of the proofs here largely on faith. Nothing ""looks"" odd, but I have not stringently checked.

Limitations:
The authors are clear about necessary conditions for their theorems to hold, but sufficient conditions are not obvious to me. See above for questions about generality.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors provide the statistical properties of HSIC, which is the measure of independency between two random variables. When the random variables have high-dimensionality and have nontrivial dependency, the authors provide the condition for the number of samples in order to successfully detect the dependency. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
For the two cases when only one variable is high-dimensional and when both variables are high-dimensional, the authors provide the number of data conditions that the nontrivial dependency can be reliably detected. In particular, when there is no lower-order dependency, the authors show that HSIC experience a difficulty to measure the dependence appropriately. Because the analysis is asymptotic, it is not guaranteed that the tendency should appear with finite dimensions. However in the experiments shown in this manuscript, the tendency is clearly shown in both synthetic and real data.

Weaknesses:
Choice of kernels should be related to the argument that HSIC only measures linear dependences (l336). What is the effect of the change of \gamma on the equations in Theorem 3 and the experiments?

Limitations:
The comparison with other methods such as distance correlation could provide more information about the difficulty in each case provided in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This article deals with the problems of measuring nonlinear dependence between random vectors living in Euclidean spaces and testing for their independence. The authors provide statistical insights into the performance of one of the two major criteria, the Hilbert-Schmidt independence criterion (HSIC), when the dimensions of the random vectors grow at different rates. Their theoretical contribution is completed with an empirical study involving both artificial and real-world data sets.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The major strong point of the contribution seems to be the real data application, which could be of interest even to the non specialist.

Weaknesses:
The paper will not appear as self-contained to the non specialist (like me).

The naive reader will find it contrary to intuition that the computation of a criterion measuring a basic statistical connection between random vectors should involve the choice of two kernel functions. This is all the more strange as the two vectors take their values in Euclidean spaces, but the Euclidean dot product is not an option to be favoured. Could this be motivated in a simple way?

The originality of the contribution is difficult to assess, all the more since the other major criterion, the distance correlation (DC) criterion, has already been the subject of a similar study.

Limitations:
This criterion does not apply here.

Rating:
6

Confidence:
2

";1
C9wlNF1Ooj;"REVIEW 
Summary:
The paper is concerned with mechanism design to maximize two objectives (welfare and revenue) when side information is available. The paper introduces a mechanism that is both incentive compatible and individually rational with lower bound guarantees for welfare and revenue even if the side information is incorrect.  

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
-The topic is interesting and impactful. I think the contribution is important given that side information is well motivated and welfare and revenue are perhaps the two most prominent objectives.

-The fact there is a lower bound on welfare even if side information is incorrect is significant. 

-The model captures many scenarios as indicated in page 4.  

Weaknesses:
-what is the run-time of the algorithm in general? Can it be exponential? I see that theorem 3.6 gives a characterization for polytopes.

-It would be interesting to see some empirical tests of the theory even for simulated examples.


Minor Point:
-line 280: typo for E[revenue] \ge b VCG not OPT 
 

Limitations:
Yes

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper presents a novel mechanism design based on the WVCG and proved that the mechanism could achieves both welfare and revenue guarantees parameterized by errors in the side information. Compared with previous work, it shows that the proposed the mechanism could degrade gracefully as the prediction errors increase. And finally, it also gives the application of the theory in the situation where the agent's type belongs to some low-dimensional subspace. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper presents a novel approach to mechanism design that takes into account both welfare and revenue. This bicriteria approach is innovative and could potentially lead to more effective mechanisms in various contexts.

2. The authors present a method for determining the quality of the side information used by the mechanism, which they refer to as ""predictions"". This focus on the quality of predictions is a significant strength, as it could improve the performance of the mechanism and lead to more accurate outcomes.

3. The authors extend their techniques to handle more expressive forms of side information that allow for varying degrees of uncertainty. This allows for finer-grained beliefs and can express quantiles of certainty, precise distributional beliefs, and arbitrary mixtures of these. This extension to more expressive side information further enhances the versatility and practicality of their proposed mechanism.








Weaknesses:
1. The paper primarily focuses on theoretical aspects and lacks experimental validation.

2. The allocation space in some previous seminal work is in the form of the probabilities assignments instead of allocating some items to some agents surely, such as the ‘p’ in Myerson's paper, so that the allocation space could be quite complex. In that case the results of the paper may be hard to use.

Limitations:
see weakness and questions.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors studied the problem of mechanism design that leverages side information on agent types to obtain both welfare maximization and revenue maximization. The paper generalized the previous results on the generalized VCG mechanism to obtain guarantees on both welfare and revenue depending on the quality of side information. Finally, the authors provided results in a setting where the principal knows the common constant-dimensional subspace of agent types.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- The studied problem of mechanism design with side information is interesting. 
- The paper is well-written. The example applications in Section 2 are useful to help the reader understand the problem.
- The theoretical results on welfare and revenue guarantees are strong contributions. The guarantees' dependence on the quality of the prediction is clear and is a useful observation that can be used for future research. 
 

Weaknesses:
- There is a lack of empirical experiments that can help support the theoretical claims. 
- There is a lack of discussion around the computation complexity of constructing the weakest-competitor hull. 

Limitations:
The authors have addressed the limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of mechanism design with side information. The authors develop a meta mechanism based on the classic VCG mechanism. The authors show that by incorporating a proper randomization scheme, the meta mechanism can achieve strong welfare and revenue guarantees parameterized by the errors in the side information. The authors further apply the meta mechanism to a setting where each agent's type is determined by a constant number of parameters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is generally well-written, clear, and easy to follow. The problem studied in this paper is interesting and relevant to the algorithmic game theory community.

2. It is nice to have a mechanism with performance guarantee decaying gracefully as the quality of the side information drops.

Weaknesses:
1. The idea of using randomization seems to be a bit straightforward. The paper would be stronger if the authors can provide matching lower bounds and/or provide improved bounds (maybe via assuming more structural properties of the side information).

2. The notion of ""weakest competitor"" is a bit confusing.

Limitations:
The authors adequately addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The article proposes a new method for designing mechanisms that can achieve high welfare and revenue by using side information about the agents' types. Side information can be any kind of data or prediction that is available to the mechanism designer, such as historical data, expert advice, or machine learning models. The authors do not assume any prior distribution on the agents' types or the side information. They introduce a meta-mechanism that combines the VCG mechanism with a novel notion of a weakest competitor, which is an agent that has the least impact on welfare. They show that their meta-mechanism can achieve strong bicriteria (i.e. social welfare and revenue) guarantees that depend on the quality of the side information, and that it can handle settings where the agents' types are determined by a constant number of parameters that lie on known subspaces. They provide examples and simulations to illustrate their results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The article addresses an important and timely problem of multidimensional mechanism design with side information, and provides a novel and versatile framework for achieving bicriteria objectives. It is well-written and organized, and the main results are clearly stated and proven. The authors also provide intuitive explanations and examples to motivate their approach and illustrate their findings.

It makes several original contributions, such as introducing the concept of a weakest competitor, designing a meta-mechanism that integrates side information with VCG, and obtaining the first welfare and revenue guarantees for subspace-type settings.


Weaknesses:
As the authors mentioned in the last section, the computational complexity of the proposed mechanism has not been sufficiently discussed. 
The article could also benefit from more empirical evaluation of their mechanisms, such as testing them on real-world data sets or benchmark problems, and analyzing their robustness and scalability.


Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
This work focuses on studying social welfare and revenue maximization in multi-dimensional auctions under predictions. Each agent is associated with a private type $\theta_i$ drawn from a distribution $\Theta_i$​, and the mechanism designer utilizes predictions $\tilde{\theta}_i \in \Theta_i$​ as additional side information for the agent types. These predictions offer insights, such as constraining the sum of values for multi-item auctions to be within a constant limit. The authors' objective is to design incentive-compatible mechanisms that ensure both consistency, the worst-case multiplicative error when predictions are accurate, and robustness, the worst-case multiplicative error regardless of quality of the side informations.

The primary focus of the authors centers around maximizing social welfare and obtaining maximum revenue through efficient mechanisms. On the surface, the problem may appear straightforward, as a simple convex combination of trusting or discarding predictions achieves satisfactory consistency and robustness bounds. Specifically, a probability $\beta$ is used to decide whether to discard or fully trust the predictions, ensuring 1,$\beta$ consistency and robustness for social welfare and $(1−\beta),\beta$ consistency and robustness for revenue. However, the paper highlights that the mechanism's revenue significantly decreases when predictions are even slightly inaccurate. Therefore, the main objective of the paper is to generalize the robustness/consistency approach and analyze the rate of revenue degradation as predictions become invalid.

The principal contribution of the paper lies in its novel mechanism that goes beyond the binary decision of discarding or trusting predictions. Instead, it introduces a third option of randomly expanding the predictions. The mechanism starts from the predicted type space and considers all types $\theta \in \Theta_i$ within the $l_{\infty}$ ball around $\theta$ with a radius r, where r is randomly chosen from a discretization of the ambient type space's diameter. The paper's main result is achieved through an appropriate convex combination of these three options. Additionally, the authors explore a special case where agent types lie on the line or subspaces and derive corresponding guarantees.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-written and effectively presents its contributions.
The introduction of the side information generalize the types of predictions in the standard consistency/robustness framework.

Weaknesses:
While the expansion of predictions is interesting, the results might not be particularly surprising.
Note that the work focusing on achieveing optimal revenue conditioning on maximizing welfare. This imposes significant restrictions on the revenue benchmark used for deriving theoretical guarantees. 

Limitations:
Not applicable

Rating:
5

Confidence:
3

";1
lT9n36RH1w;"REVIEW 
Summary:
This paper studies adaptive online convex optimization, with focus on adapting to unbounded domain and arbitrary time-varying comparator sequence. Different from previous studies which mostly consider the path-length as the prior which appears in the dynamic regret bound, this paper aims to enlarge the range of priors. For a given dictionary of orthogonal feature vectors, it is shown that an $\tilde{O}(\sqrt{E\cdot \text{Sparsity}_H})$ regret bound can be achieved, which not only recovers previous results on path-length, but also allows more flexibility. This paper also strictly improves the result of JC22 by replacing comparator-dependent terms in the regret bound by strictly smaller ones.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
This paper makes solid contributions to adaptive online convex optimization. It extends the scope of comparator adaptivity from the traditional path-length, to other priors on the comparator via a novel sparse coding framework. The achieved bounds involve more refined dependence on the comparator, and are never worse than previous results. In particular, the special case of the wavelet dictionary improves the SotA result in JC22.

Weaknesses:
The main weakness is the unclear presentation of technical results, which are hard to fully understand and interpret. Though the claimed results seem promising, I have to vote for rejection for now because I can't verify the correctness without a better understanding of the results. Here are some confusions I met.

For the size 1 dictionary case, isn't the assumption too strong which seems to trivialize Lemma 2.1? In line 211, the comparator is assumed to lie in the span of a single vector $h_{1:T}$, which is just a scaling of the vector $h_{1:T}$. Since the dictionary is fixed and revealed to the player, the range of potential comparators seems very restricted (the degree of freedom is just one through $\hat{u}$).

The same issue carries over to the main result Theorem 1, which assumes the signal $z^{(n)}$ lies in the span of a single vector. Even if the reconstruction error creates certain flexibility, to achieve vanishing regret one still needs to guarantee the comparator is very close to $\sum_{n=1}^N z^{(n)}$, which seems under-expressive: for a given dictionary, the overall degree of freedom is just $n$, while for arbitrary time-varying comparators the degree of freedom is $dT$.

Two examples are presented right after the main theorem. The case of static regret is well understood. However, I feel the case of orthogonal dictionary requires more technical details, at least to readers unfamiliar with signal processing. For example, what exactly does the ""orthogonal"" means here? (which vector is orthogonal to which?) It would be great if you can provide an intuitive example here to show the power of your method over previous results.

I'm also concerned with how to choose the dictionary. It seems a good choice of dictionary requires prior knowledge. It's not clear to me if one can adapt to a class of dictionaries.

Limitations:
The presentation of technical results is not very clear.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The problem that this paper tries to tackle is the unconstrained online convex optimization with dynamic regret. Previous works usually assume that the comparator sequence is arbitrary and maybe time-varying with some fixed form of comparator measurement in the final dynamic regret bound. For this paper, it proposes a new way of the comparator measurement by first using a pre-defined subspace (by the users) to characterize the comparator sequence and then upper bound the dynamic regret in terms of the pre-defined subspace complexity as well as the reconstruction error between the subspace and the comparator sequence. It shows that for the almost static environment, the proposed algorithm + wavelet constructed subspace could have better regret than the existing works' in (maybe) the constant part. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) It provides the researchers a new perspective of measuring the comparator sequence when upper bounding the dynamic regret, which is interesting and useful in real applications.
2) The specific wavelet based subspace version could result in a regret better than the existing works' with (maybe) smaller constant part.

Weaknesses:
1) The proposed algorithm framework only moves the dynamic regret from previous works in the constant value part. And in order to compare the actual improvement in terms of the constant part, the author/s need to provide the complete regret result comparison instead of an order O() based one.
2) The paper result depends heavily on the existing work [MK20], and the subspace based comparator sequence measurement is the only part that makes it interesting, although previous works like [HW15, ZLZ18] has already shown such an idea as pointed out by the author/s.
3) The author/s explained the motivation of tackling unconstrained dynamic setting rather than the constrained one. But for the finite range argument in the paper, more often than not, the finite range usually comes from the requirement of the model output and not just some heuristic estimation. I think it's better to also provide some examples to demonstrate the motivation of this paper.

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors examine the dynamic regret of Online Convex Optimization (OCO) within the context of unbounded comparator sequences. To address this issue, they introduce a novel framework of sparse dictionary coding for online optimization. Following this, the authors provide theoretical proof for the regret bounds applicable to different types of dictionary matrices - the general dictionary matrix, orthogonal dictionary matrix, and the Haar wavelet dictionary matrix. Of notable mention is the result pertaining to the Haar wavelet dictionary matrix, where the authors establish a regret bound that surpasses the current state-of-the-art.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Originality & Significance: The introduction of the sparse coding framework for Online Convex Optimization (OCO), complemented with the corresponding proof, is highly innovative. Furthermore, considering the dynamic regret of OCO in the unbounded domain is very crucial. The result that the authors obtained is related to the comparator average, first-order variability, and second-order variability, rather than the path length. This indicates that the authors have achieved smaller dynamic regret under more adaptive conditions, thereby enriching optimization theory within the community.  
- Quality & Clarity: The work is clear, well written, and technically sound.

Weaknesses:
- Comparing the proposed method with the meta-expert optimistic online gradient descent method as described in [1] could be beneficial. Particularly in Examples 1 and 2, it appears that the meta-expert optimistic online gradient descent method can also achieve a regret of $\mathcal O(\sqrt T)$, considering the path-length is actually $\mathcal O(\sqrt T)$. This comparison might offer a broader perspective.

- I'm concerned about the computational complexity of $\mathcal O(d\log T)$ for high-dimensional problems, especially when compared to the $\mathcal O(\log T)$ complexity of both the meta-expert OGD and the meta-expert optimistic OGD methods. If possible, an in-depth discussion on this issue would be helpful for readers.

- Additionally, the main text seems to miss Algorithms 3-5. This omission may cause a minor confusion for readers during their first read. Rectifying this could help in enhancing the flow and clarity of the paper.

Ref: [1] P Zhao et.al., Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the universal dynamic regret minimization problem with the unconstraint decision domain. The authors proposed the sparsing coding framework, which converts the dynamic regret minimization problem in the time domain into a static regret minimization problem in the transfer domain. The comparator-adaptive static regret bound in the transfer domain implies a dynamic regret bound in the time domain. Specifically, by choosing the dictionary as the Haar wavelet base, this paper achieves improved dynamic regret bound with better dependence on the range of the comparator sequence. Several concrete examples are provided to illustrate the superiority of the proposed bound.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ This paper provides a novel framework to obtain universal dynamic regret bound via sparsing coding. The conversion from comparator-adaptive static regret bound to dynamic regret bound in the time domain is interesting to me.
+ This paper achieves improved universal dynamic regret bound with the new framework, which is strictly better than the existing results. Several examples are provided to illustrate the advantages of the proposed bound.
+ This paper is well-written and provides a sufficient discussion of the related literature.

Weaknesses:
I do not find a major weakness in the paper, but there are still some minor comments:

- about the general formulation (Eq. (5)): it would be nice to mention that the dynamic regret bound (Eq.(5)) is not universal dynamic regret bound in general as it only holds for the comparator $u_{1:T}\in\mbox{span}(h_{1:T})$. The universal dynamic regret bound can only be achieved with an appropriate choice of the dictionary.

- about the examples: this paper has listed several examples with the specific choice of the comparator sequence to show the superiority of the proposed bound. However, since the main focus of this paper is the universal dynamic regret, it is unclear in which situation the listed comparator sequence is an appropriate benchmark that can minimize the right-hand side of Eq (2). I suggest the authors provide more concrete examples to illustrate the advantage of the proposed bound with certain specific loss functions.


Limitations:
One of the main limitations of the paper is that the tightness of the proposed bound is still unclear. The authors discuss this issue at the end of the paper.

Rating:
7

Confidence:
3

";1
B3UDx1rNOy;"REVIEW 
Summary:
Temporal graphs are more accurate for modeling real-world scenarios compared to static graphs, but the current approach of extending neighbor aggregation from static graphs to temporal graphs is computationally expensive when considering all historical neighbors. To address this, the authors propose a novel framework that uses recurrent neural networks with node-wise hidden states to integrate information from all historical neighbors for each node, ensuring complete neighbor information without subsampling biases.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The model proposed by the authors looks reasonable, at least  their experiments suggest so.

2. The authors do a good job in describing the model. They propose two measures of expressiveness for temporal graph models, specifically in the context of temporal link prediction and temporal graph isomorphism tests. They establish a theoretical connection between these measures and demonstrate the superior expressiveness of their framework, while also presenting new findings on the expressiveness of other baseline methods.

3. The ablation studies display the positive effect of each mechanism.

Weaknesses:
Some concerns in this paper should be discussed precisely. 

1. In introduction, the heterogeneity in temporal revision is not explained.

2. Figure 1 lacks a specific description. The other baselines and the framework in this paper can be specified in the legend.

3. This paper lacks a description of Table 1. The authors can add a description of Table 1.

Limitations:
The authors adequately addressed the limitations. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies the problem of temporal graph learning. The authors propose the recurrent temporal revision (RTR) layer, which involves learning a hidden state for each node and utilizing recursive neural networks to integrate information from all neighbors. Additionally, the authors introduce heterogeneity in RTR and theoretically demonstrate that this heterogeneous revision enhances expressiveness beyond Temporal-1WL.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. The majority of the paper is easy to follow.

S2. Experimental results show that RTR outperforms state-of-the-art methods.


Weaknesses:
W1. My main concern is with the motivation of paper. Why do we need to make each node embrace complete neighbor information? In real-world recommendation system data, the browsing/purchasing tendencies of users are often more closely related to their short-term interests. The inclusion of excessive and outdated information may introduce more noise to the model instead.

W2. The proposed RTR can be understood as weight learning and important neighbor selection based on complete historical information. Could changing the sampling strategy from selecting k nearest neighbors to selecting k most important historical neighbors for methods like TGN also result in similar performance improvements?

W3. The paper lacks an analysis of time complexity as well as a comparison of the model's parameter count.

Minor comments

M1. Page 1, Line 16: social network, recommender system -> social networks, recommender systems

M2. Page 4, Line 160: an learnable function -> a learnable function

M3. Page 8, Figure 4 caption: TGN and GRU-GCN fails to -> TGN and GRU-GCN fail to


Limitations:
I have some concerns about the scalability of the proposed method.

Rating:
5

Confidence:
5

REVIEW 
Summary:
In this paper, the authors propose a novel aggregation layer based on a recurrent neural network dubbed recurrent temporal revision (RTR) for temporal graph networks to solve the dynamic temporal graph embedding problem. Specifically, a new aggregation function is proposed, which encodes neighbor state information and event information. This paper also proposes two expressive measures and uses them to demonstrate theoretically the difference from previous work. Empirical results on several well-known datasets show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1．	The explanations of the components of the approach are clear and detailed.
2．	This work uses the proposed two measurements, and theoretically, fully justifies the difference from the previous methods. It also includes algorithm time complexity and running time analysis
3．	Ablation settings are very detailed.


Weaknesses:
1.	It is unclear which part of the ablation experiment corresponds to which of the two proposed measurements. It would be good to clarify that.
2.	It’s better to have an Algorithm line by line to clearly correspond to specific formulae and modules for Figure 1b.


Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel framework called Recurrent Temporal Revision (RTR) to address the challenge of neighbor subsampling in temporal graphs, an issue commonly encountered in real-world applications like social networking, recommender systems, traffic forecasting, and crime analysis. RTR serves as a standard building block for any temporal graph network, utilizing a recurrent neural network to integrate all neighbor information through a hidden node state. This hidden state mitigates the problem caused by neighbor subsampling and aims to capture complete neighbor information.

The paper also introduces a new concept of ""temporal revision"" to update the hidden state by capturing the state changes of neighbors. To further boost the theoretical expressiveness of this technique, the authors propose incorporating heterogeneity into temporal revision by recursively identifying and marking certain nodes as specialties in the revision calculation process.

The authors suggest that this new approach offers superior expressiveness in terms of temporal link prediction and temporal graph isomorphism test, a claim that is backed by theoretical proofs and experimental results. Additionally, they provide a new Ecommerce dataset for evaluating temporal graph models in real-world settings. Their experimental results indicate the proposed framework's significant improvement over state-of-the-art methods, particularly as the number of aggregation layers increases.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The authors introduce innovative ways of evaluating the expressiveness of graph algorithms by defining two novel metrics: the temporal graph isomorphism test and the temporal graph link prediction. They apply these new measures to several state-of-the-art models, successfully establishing an equivalence between Temporal-1WL and these baseline models which implies the shared drawbacks of these models.
- The paper presents a unique temporal learning model grounded on two inventive ideas: 1) Utilizing hidden states to retain aggregated information which is subsequently updated through a temporal revision layer, and 2) Embedding heterogeneity by giving special treatment to the central node during the recursive revisioning process. The authors compellingly argue that their proposed method outperforms state-of-the-art models in terms of expressiveness, as defined within the context of this paper.
- The proposed method demonstrates promising performance in real-world temporal graph tasks, particularly in the temporal link prediction task. Additionally, it excels in the graph isomorphism test examples, reinforcing its potential for practical applications in various fields.

Weaknesses:
- The rationale behind the new definition of expressiveness isn't effectively conveyed in the paper. While it's true that there isn't a universally accepted definition for the expressiveness of a temporal graph model at present, the paper falls short in clarifying why its proposed isomorphism test improves upon existing ones. The lack of a clear explanation leaves a gap in understanding the superiority of the proposed metrics.

Limitations:
N/A

Rating:
7

Confidence:
3

";1
gO60SSGOMy;"REVIEW 
Summary:
This paper proposes a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack (ACA). The author argues that current unrestricted attacks have limitations in terms of maintaining human visual imperceptibility, generating natural adversarial examples, and achieving high attack performance. This paper is well-written and easy to follow.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1) The paper introduces a novel attack framework that addresses the limitations of current unrestricted attacks. The use of a low-dimensional manifold and optimization along its adversarial direction allows for the generation of diverse and natural adversarial examples.
2) The paper provides a clear motivation and problem definition, outlining the challenges and goals for unrestricted adversarial attacks.
3) The paper includes extensive experimentation and visualization to validate the effectiveness of ACA. The results show significant improvements over state-of-the-art attacks in terms of adversarial transferability.

Weaknesses:
The paper does not include sufficient evaluation and ablation studies for the proposed method. Since ACA used skip gradient, I think I should compare Skip connections matter [1] in the experiment.

The paper could benefit from a more thorough review of relevant literature. While the authors mention existing unrestricted attacks, there is limited discussion on related work and the novelty  of ACA compared to previous approaches.



[1] Skip connections matter: On the transferability of adversarial examples generated with resnets, ICLR 2019

Limitations:
/Na

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper introduces a novel attack framework called Content-based Unrestricted Adversarial Attack, which aims to generate diverse and natural adversarial examples with high transferability. The authors argue that existing methods, such as lp norm-based attacks, have limitations in terms of perceptual similarity, naturalness, and robustness. To address these issues, they propose mapping images onto a low-dimensional manifold represented by a generative model trained on natural images. This manifold ensures both photorealism and content diversity. By optimizing the adversarial objective on this latent space, they generate unrestricted adversarial examples. The proposed method, called Adversarial Content Attack (ACA), utilizes Image Latent Mapping (ILM) and Adversarial Latent Optimization (ALO) techniques to optimize the latent in a diffusion model. The effectiveness of ACA is validated through experiments and visualization, demonstrating significant improvements of 13.3~50.4% in terms of adversarial transferability compared to state-of-the-art attacks. 

Overall, the main contributions of the paper are the introduction of the Content-based Unrestricted Adversarial Attack framework, the development of the Adversarial Content Attack method, and the experiments demonstrating improvements in generating diverse and transferable adversarial examples.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors effectively communicated the motivation, problem statement, and methodology of the proposed framework. By addressing the limitations (imperceptibility/photorealism/effectiveness) of existing methods, they proposed a novel attack framework that leverages a low-dimensional manifold represented by a generative model. By combining image mapping onto a latent space, optimizing adversarial objectives, and utilizing a diffusion model, the authors introduce a novel approach to generating diverse and natural adversarial examples. This paper might be the first to explore unrestricted adversarial examples through such a framework.

This paper offers a thorough explanation of the proposed attack framework, detailing the underlying techniques of ILM and ALO. The authors further support their claims through experimentation and visualization, providing evidence of the effectiveness of their approach and demonstrating improvements in adversarial transferability compared to state-of-the-art attacks. The improvements in adversarial transferability also shed light on the potential impact of this method in uncovering vulnerabilities in security-sensitive applications and advancing our understanding of robustness in DNNs.

Overall, this paper's strengths encompass originality in proposing a novel attack framework, quality in terms of methodology and experimental evaluation, clarity in explaining the concepts and techniques, and significance in addressing limitations and raising awareness of unrestricted but realistic adversarial examples.


Weaknesses:
While the paper has several strengths, there are some weaknesses that could be addressed to further improve the work:

**Comparison with State-of-the-Art Attacks:** While the paper mentions that the proposed method achieves significant improvements in terms of adversarial transferability compared to state-of-the-art attacks, a more comprehensive comparison would strengthen the evaluation. It would be valuable to include a thorough analysis and comparison with a wider range of existing unrestricted attack methods, such as [1]. In particular, Laidlaw et al. proposed an efficient way to generate imperceptible adversarial examples. The reviewer also suggests evaluating the proposed attack on other adversarially trained models, for example, the defense method that could be generalized to unforeseen perturbations [1], or having used synthetic data during adversarial training [2].

**Defense Method:** This paper does not discuss how to defend the proposed attacks but primarily on the efficacy of the proposed method. It would be great if the authors provide potential solutions or mitigation strategies for the threats.

**Generalization to Different Datasets:** The authors only evaluate their method on a subset of the ImageNet validation set and do not say how the results generalize to other datasets. It would be beneficial to investigate the generalization of the proposed approach to different datasets.

**Unclear Claim:** The authors mentioned the Dunning-Kruger effect to emphasize that current defense methods against lp norm adversarial examples overestimate their abilities. However, this work does not provide further details and arguments to support this. Although similar arguments have also been proposed in [3], in which the authors argue that lp-based robustness evaluation might be biased, the reviewer thinks that using the Dunning-Kruger effect here is not rigorous. The reviewer suggests that the authors rethink this argument, and even consider removing it if it is not an important contribution of the paper.

**Typos:** Although this paper is well-written and easy to follow, the reviewer found some typos and grammar errors. For example, in line 161, *follw*. It would be great if the authors have proofread the paper before submitting it.

    [1] Laidlaw et al. Perceptual adversarial robustness: Defense against unseen threat models. (ICLR 2021)
    [2] Croce et al. Robustbench: a standardized adversarial robustness benchmark. (NeurIPS 2021)
    [3] Hsiung et al. CARBEN: Composite Adversarial Robustness Benchmark. (IJCAI 2022)

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, authors propose an unrestricted untargeted attack based on optimising the latent space of stable diffusion model. The generated adversarial samples are empirically shown to be more transferable than the existing semantic attacks. Moreover, authors validate the effectiveness of adversarial samples when attacking different representative adversarial trained models and show consistent performance book.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The method is a replacement of popular GANs in the semantic attacks with the powerful stable diffusion models, pushing the state-of-the-art by many steps emperically.
- Experiments are thorough in attacking both normal and adversarial trained models
- Compared against many baseline unrestricted attacks and proposed attacks outperforms all the methods.

Weaknesses:
-  The paper  lacks technical novelty as the latent space optimisation for generating adversarial attacks as been popular from many years [17].

- Since this is a unrestricted attack, the generated image quality is difficult to assess with metrics. Nonetheless, authors have computed 5 metrics to understand the image quality

- Authors did not discuss about the code release to reproduce the experiments. For this particular paper, the implementation of this proposed method is not simple as the section 3.1 and 3.2 mostly discusses about the difficulties in optimising the latent space and how they overcome it through skip gradient, momentum and boundary function.

Limitations:
- The inference time of the attack is much higher taking 2.5 minutes for image due to many many levels of optimisation such as for null text embedding, perturbation in latent space.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes the Adversarial Content Attack based on the diffusion model. The proposed attack method first maps the image onto a low-dimensional manifold of natural images and then moves images along the adversarial gradient on the manifold to generate photorealistic adversarial examples. The authors conduct extensive experimentation and demonstrate the efficacy of the Adversarial Content Attack in both normally trained models and defense methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper presents a host of visualization and experiments to support the intuitions and conclusions.
2. The investigated topic is important and useful.

Weaknesses:
1. This work seems like an application of the diffusion model in model debugging research, finding some hard samples of deep models. However, it lacks mentions or comparisons with related work, including [1],[2], etc.

2. In Table 3, I can hardly understand why the generated adversarial image can achieve better photorealism than the real image.

3. In Table 2, Inc-v3$_{ens4}$ is used as the target model, which is a defense method. But confusingly, the attack success rate (**62.2**) surpasses the case when using normal Inc-v3 (**58.8**) as the target.

I will be pleased to raise my score if these questions can be properly answered.

[1] Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, Hui Xue. ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.

[2] Maximilian Augustin, Valentyn Boreiko, Francesco Croce, Matthias Hein. Diffusion Visual Counterfactual Explanations. Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021.

Limitations:
The paper has discussed the limitations and negative social impacts of the proposed attack method.

Rating:
6

Confidence:
4

";1
RJpAz15D0S;"REVIEW 
Summary:
The authours proposed a metric called `idealized runtime` to evaluate the inference efficiency among different LLMs as if they were run on the same standard hardware/software system. The main contributions can be summarized as following:

* the `idealized runtime` for a specific query with prompt length `p` and output length `o` on a target LLM can be collected by running the LLM on standard hardware/software (e.g., A100 and Megatron containers) **if target LLM's architecture is known/opensourced**
* the `idealized runtime` of transformer models can be modeled as a function linear to `o` and piecewise linear to `p` **if context window size is much smaller than embedding size of the model**. So with a few queries with different `p` and `o`, the coefficients of this function can be extracted by fitting the runtime of these queries to the function.
* with the fitted function, `idealized runtime` of queries can be estimated without running them exhaustively, so that the capacity of model (e.g., accuracy on a dataset/benchmark) can be compared against the pre-calculated/estimated `idealized runtime` of all the queries in the dataset to tradeoff/review scaling effect of LLMs.
* for closed models with api access, the authors proposed an alternative `denoised runtime` to approximate their idealized runtime.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
* The authors pointed out the scaling law of LLMs should not just focus on accuracy vs FLOPs/Model size but accuracy vs inference latency/cost/power as well, which is a valuable point in LLM production.
* The authors proposed a simple way to estimate the idealized runtime of queries on standard hardware/software systems given prompt length and (projected) output length.

Weaknesses:
* The inference latency of a well known (open sourced) LLM architecture can be easily estimated by total ops of the model (e.g., those calculated in line 105/115) and divided by hardware effective (typically around 50% for LLMs on A100, https://arxiv.org/pdf/2104.04473.pdf or BLOOM paper) FLOP/s. The authors only mentioned about the drawback of this proxy briefly in line 300-308, without evidences based on target LLMs. Thus the significance of approximating these latencies via fitting instead of analytical calculation is quesitonable.

* The work makes too many assumptions that devalue its practice,
  1. context length << embedding size. This is true when the common context lenght is only 2K, but new development on LLMs have pushed them to 32K(GPT4)/64K (MPT)/100K(Claude), so this assumption and therefore the linear function approximation is no longer valid. With long context/prompt length, the analytical equation is probably more accurate.
  2. the `idealized runtime` is collected on standard Megatron container assuming model architectures are known. This limits the effectiveness of the work to open sourced models with efficient (distributed) implementations. 
  3. the work assumes the pretrained/finetuned model is the final service/inference model. However in practice, the service model can be quantized/sparsified/distilled, so apple-to-apple inference latency comparision on pretrained models doesn't direclty translate to service model and the gap is typically huge, e.g., as shown by FIG3, the variance in idealized-runtime vs denoised-runtime is significant.

* The presentation of the work is not clear and sometimes repetitive/redudant.
  1. using FIG3 again as example, what's the diff of the two subplots in (a)/(b), it is not clarified and seems just different scales in axis. And as mentioned above, this FIG suggests for a couple models, the denoised-runtime is smaller than idealized-runtime, which is not a good evidence that idealized-runtime is a good approximation of roofline runtime.
  2. A lot of the equations are just rewrite/modifications to a simple linear function, and probably can be omitted or summarized in a table.

* Some of the conclusions are drawn without evidence, e.g., in line 257, the authors claimed ""This suggests that scale alone does not predict model capabilities."" Scaling law should be studied for the same model by varying model/dataset size. Comparing different model families with drastically different in pretraining dataset size/quality can not lead to any conclusion over model capacity over model size or inference latency.

* Overall the work is oversimplified by the assumptions above to a linear regression problem, which can be analytically calculated (and is commonly adopted, e.g., like Palm2 tech report) and becomes trivial to the audience of NeurIPS.

Limitations:
The limitations are mainly the assumptions the authors made, as mentioned in weakness. The authors either didn't discuss them or without referring to SOTA LLM developments.

Rating:
2

Confidence:
4

REVIEW 
Summary:
The authors proposed idealized runtime, a metric for measuring inference efficiency in LLMs, which measures the performance of models as if they were executed on a given hardware and software platform. Idealized runtime efficiently can be extended to estimate the idealized energy and dollar cost. The metric also takes into account the amount of hardware required to perform inference on a given model. Using these metrics, the authors compare ten state-of-the-art LLMs to demonstrate inference efficiency-capability tradeoffs. Overall, the authors aim to provide a more accurate and fair comparison of LLMs across different providers and to shed light on the tradeoffs between inference efficiency and capability.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The paper presentation is clear and the metric proposed is intuitive to understand.

2. The empirical study is thorough and examines the LLM inference efficiency from various perspectives.

3. The empirical findings demonstrate that the proposed metric can largely capture the efficiency-capability tradeoff in LLM inference.

Weaknesses:
1. The models on different tasks and metrics exhibit vastly different Pareto Frontiers (Figure 9). The cause of this variance hasn't been fully explored and explained in the work. Can these datasets/tasks properly evaluate the capabilities of the LLMs or is the variance due to something else?

Limitations:
The authors may further discuss the potential negative societal impact of their work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper aims to better understand the tradeoff between the inference efficiency and capability of LLM. It comes up with a metric called “idealized runtime” to fairly evaluate the inference efficiency of several popular LLMs, which is designed to factor out the irrelevant noises such as hardware, code stack, network latency and so on. With this metric, the authors analyised the efficiency and capability of LLMs and got some inpsiring insights.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
First of all, I found the motivation of this paper very realistic and helpful to the community. We have been seeing lots of efforts made by the industry and academia to reduce the cost of the inference of LLMs, but how to compare those models from different parties fairly is yet to be figured out. This paper would be an inspiring exploration to this end. Besides, the analysis regarding the LLM inference is thorough and clear. 

Weaknesses:
- Some technical details are kind of missing, which I will specify in the Question section.
- The concept of “idealized runtime” could be a bit unpractical because it requires one to implement and deploy the LLM on a very specific hardware and software setting, which may involve quite amount of proper engineering efforts.

Limitations:
The potential social impact is not discussed in the main paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
""This paper introduces a range of metrics designed to compare the tradeoffs between inference efficiency and capability of autoaggressive Language Model (LLM) based on Transformers. Alongside the raw run-time metric, which can be influenced by infrastructure optimization and performance variance, the paper proposes additional metrics, namely idealized runtime, denoised runtime, idealized dollar cost, and idealized energy cost. These metrics enable a comprehensive comparison of various LLMs, even when they are served through different black-box APIs and hardware/software implementations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Originality: The paper introduces a novel analytical approach to estimate the inference efficiency of autoregressive Transformer models, overcoming challenges posed by models accessible only through black-box APIs and running on diverse hardware and software. This distinct proposal stands apart from previous analytical models designed for non LLMs. Moreover, it addresses the limitations associated with other proxies, such as model size and FLOPs.

* Quality and clarity: The proposal is presented with a rigorous analytical description and validated through empirical verification. The paper's structure is well-organized, and the content is conveyed in a clear and concise manner.

* Significance: The metrics, encompassing both the newly proposed and conventional ones, are effectively employed to compare the tradeoff between capability and efficiency. This comparison leads to several noteworthy insights, elaborated upon in Section 5.4.

Weaknesses:
There are a few minor formatting issues that require attention. For instance, in line 153, the notation of $o^2$ is confusing. The ""2"" is intended to refer to the footer, but it appears as if it denotes an exponent of 2.

Limitations:
As mentioned in Section 3.1.1, the analytical model does not account for models with very large context windows (> 10,000); however, it is worth noting that these models are now becoming available.

Rating:
7

Confidence:
3

REVIEW 
Summary:
LLMs are dominating the NLP space at the moment, but efficient inference run-time and cost estimations have not been explicitly defined and tested. Raw run-times are not particularly useful across multiple services providing LLMs, as there is typically high variance overhead via the presence of black box prompt APIs. This paper proposes a few ways of estimating inference costs that attempt to provide an upper bound to the costs incurred by model computation. These metrics are demonstrated to be generally accurate and are cheap enough to be easily implemented, and the authors attempt to use them to generate new insights concerning various LLMs and generally large models across multiple services.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Very relevant, as LLMs are dominating conversations in the NLP space. Paper as a whole does a good job of making a case for the necessity of their work. Methodologies to estimate a separation of overhead costs from core model computation are inherently useful. Most, if not all, assumptions are fairly setup and seem correct.
2. The paper is very well-written. The problem setup is executed well and the initially proposed solution is very easy to follow. Explanations are mostly succinct, and most sections read as cogent. 
3. The results look to be generally correct and explanations related to them are succinct. The results are made more relevant by the scale of the experiments, rendering the paper somewhat unique. The applied denoising factor estimation technique seems to roughly work in providing an upper bound for expected model computation time. 


Weaknesses:
1. Some areas of the paper are a bit overexplained for this venue. Readers and reviewers of NeurIPS should, for example, be very familiar with autoregressive frameworks and how they generally work, or the forward pass design of generative AI in general. 
2. Several early conclusions are mostly obvious. A lot of time is spent on specifying a procedure to estimate the number of floating point operations (and subsequently runtime based on throughput), but that isn't really all that interesting or new. For example, LLMs having roughly linear costs with respect to the size of their input (their quadratic attention costs are dwarfed) is probably well understood by your audience. Empirical proof related to this likely shouldn't be core to the paper.
3. Unclear if particularly new insights were generated from the results. There were largely no clear winners in cost-capability tradeoff analyses, there is no comparison to other evaluation techniques in the core body of the paper, and insights into when models fall or do not fall on the Pareto frontier of a given task do not seem initially useful. 

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a new metric that attempts to more fairly evaluate machine learning model inference runtime performance cost, which the authors refer to as idealized runtime. The authors argue that the black-box nature of many large language models (LLMs) does not necessarily provide accurate estimates of inference cost using raw runtime results alone, due to several external factors (e.g., hardware used, compute resource contention, etc.). Thus, a goal of idealized runtime is to create a fair(er) inference evaluation metric for LLMs.

The authors analyze 10 state-of-the-art LLMs using their idealized runtime metric and discuss the efficiency tradeoffs between them.


Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
- The paper is well-written
- The research topic of creating a somewhat universal metric to measure spatial and temporal overhead of LLMs on inference is relevant and timely
- The authors’ intuitions on focusing on compute operations (e.g., floating point instructions) and performing an inference runtime calculation that is not overtly computationally intense, is a good foundations for such research



Weaknesses:
While I agree with the authors that this topic is certainly worth deeply investigating, I have too many open concerns to recommend it for acceptance (at this time). Below I list two areas that I believe would need to be addressed before can be reasonably considered for publication.

- floating point operation overhead (a critical paper weakness): in Section 3.1.1 the authors discuss some of the concrete details used for their idealized runtime equation. Of note is the focus on deriving expressions “for the number of floating-point operations required for each of the two steps, and then use these to derive expressions for runtime.” While the number of floating point operations required to perform a single forward pass of inference is quite likely to an important datum for estimating inference runtime overhead, it is (in my opinion) just the first of many (perhaps dozens?) of factors that may be required to make an accurate estimation of inference overhead. There are several reasons for this, I list a couple below (the below list is not exhaustive). 

(1) Not all floating point operations are spatially or temporally equivalent (e.g., a floating point ceiling or floor operation is notably less expensive than even a mathematical divide operation due to the execution “trap” that must exist for the divide, but not for the floor/ceil). Moreover, some floating point operations are orders of magnitude different in overheads (e.g., an FP divide is likely 10x-100x faster than a gather-scatter vector FP operation). To attempt to quantify all FP ops in a single computational class is logically unsound.

(2) The spatial and computational complexity of FP operations between different hardware compute classes varies significantly (often times more than 10x-100x). For example, GPUs can handle up to their channel size limit of FP data and continue to emit a fixed computation time for equivalent class FP operations. This is due to GPUs inherent SIMD properties. CPUs, on the other hand, emit approximately linearly increasing FP operation overhead as FP operation count increases. On the other hand, if the GPU channel size is exceeded even by a single datum, the GPU execution time may increase by 2x or more because it will generally require a second issuance of GPU block execution. Yet, CPUs only suffer roughly additive execution overhead when additional datum are added for FP operations. This means there is generally an inversely proportional runtime cost associated with FP operations and data between CPUs and GPUs. The paper seems to only focus on GPUs, which, in commercial practice, tends not to be representative (or at a minimum highly specialized) of which compute is used for inference.

- References: of the 50 or so references included by the authors, it appears that only around 10 of them are from peer-reviewed publications. The other 40 or so are from arxiv and websites. Moreover, the arxiv references are not necessarily recent (some date back over 5 years ago). The lack of peer-reviewed citation raises questions about the validity of substantiation as well as the authors’ understanding of the domain. A core problem with citing non-peer-reviewed published work is the lack of a fixed written artifact. Arxiv papers can have many versions as can website/webpages (which version are the authors referring to and how would a reviewer know this a priori?). I strongly encourage the authors to perform a more rigorous literature review and to substantiate their claims principally through peer-reviewed scientific research. It is a tall order to expect a reviewer to personally review / validate every non-peer-reviewed research paper to help verify its soundness and thus its appropriateness when used to substantiate the authors claims. Yet, not doing so would seem to imply that reviewers would need to implicitly “trust” the quality of such non-peer-reviewed artifacts. This seems antithetical to the evidence-based scientific research that is published at NeurIPS.


Limitations:
The authors seem to only focus on inference runtime overhead associated with GPUs (and a single class of GPU, as a reference point, Nvidia A100) and no other form of hardware compute. However, many other types of hardware are used to perform inference. Some of them are: CPUs, FPGAs, TPUs, CGRAs, etc. Moreover, each of these hardware classes have subclasses within that result in different temporal overhead and power footprint costs. None of these factors seem to be considered in the authors runtime equation.

Rating:
2

Confidence:
4

";1
zrLxHYvIFL;"REVIEW 
Summary:
This paper addresses the open-world semi-supervised learning (OSSL) problem and proposes taxonomic context priors discovering and aligning (TIDA), which considers the taxonomic hierarchy of classes.

Basically, the proposed method is based on [19], which assigns pseudo-labels to unlabeled samples by means of the Sinkhorn-Knopp algorithm. The proposed method is characterized by Taxonomic Context Discovery (TCD), which uses the sum of the losses at different hierarchical levels and Taxonomic Context-based Prediction Alignment (TCA), which adjusts the similarity between the feature and the class prototype by incorporating the weights based on the similarities between the prototypes at different hierarchical levels.

Experimental results on seven datasets show that the proposed method achieves better accuracy than existing methods.


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
* This paper proposes an OSSL method focusing on class hierarchy, which has not been considered in the past as far as I know. (However, as discussed below, similar ideas have already been considered in related problems.)

* The theoretical justification of the proposed method is discussed based on the principle of the EM algorithm (lower bound maximization of log-likelihood).

* Exhaustive experiments using seven datasets show that the proposed method is consistently more accurate than existing methods.


Weaknesses:
a. The idea of discovering unknown classes by focusing on the class hierarchy has been explored in the past [46], so the high-level novelty would be somewhat limited.

b. In this paper, the number of unknown classes is known, which is not realistic. The supplementary material shows the experimental results for the case where the number of unknown classes is unknown (Tables V and VI). However, only the results for the case with the best clustering accuracy are presented. I do not think this is realistic, since clustering accuracy is not always evaluable in practice.

c. The base networks used in the experiments are limited to ResNet-18/-50. Testing with more modern networks such as ViT and vision-language models would be nice to emphasize the effectiveness of the proposed method.

d. Table 3 shows that accuracy improves when both TCP and TCA are used, but always degrades when TCA is not used. The reason for this is not clear.

e. The results in Figure 4 suggest that the appropriate number of super-classes and sub-classes may differ depending on the dataset. Justification is needed.

f. Some typos: 

L10: pIrors -> prIors

L199: Sinkhorn-Koppn -> Sinkhorn-Knopp


Limitations:
Limitations are discussed in the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
Previous research has primarily focused on using pre-defined single-granularity labels as priors for recognizing novel classes. However, classes naturally adhere to a taxonomy, enabling classification at multiple levels of granularity and offering richer supervision through underlying relationships. To address this, this paper proposes TIDA (Taxonomic context pIrors Discovering and Aligning), a unified framework that leverages sample relationships across various levels of granularity. TIDA discovers multi-granularity semantic concepts as taxonomic context priors (e.g., sub-class, target-class, and super-class) to enhance representation learning and improve pseudo label quality. Extensive experiments on seven datasets demonstrate TIDA's significant performance improvement, achieving a new state-of-the-art.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The motivation is clear and convincing. It is reasonable that taxonomic context priors are helpful to extract discriminative features.
2. The writing is clear. I like the figures in this paper.
3. Experiments effectively verify the effectiveness of this work. Comprehensive experimental results are provided in the supplementary material.

Weaknesses:
1. The necessity of constructing taxonomic priors remains unclear. Various methods, such as using WordNet [1], can extract taxonomic priors among categories. I think that there is insufficient evidence to support the essentiality of constructing taxonomic priors in typical scenarios.

[1] I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively. ICLR 2020.

2. Some works, especially [2], on the taxonomic structure lack proper citation and comparison.

[2] Open-world Semi-supervised Novel Class Discovery. IJCAI 2023.

3. The Introduction section lacks high-level statements regarding the construction and correctness of taxonomic priors.  need to find these information in the Methodology section.

Limitations:
In conclusion, this paper exhibits a clear motivation and comprehensive experimental verifications. However, the necessity of constructing taxonomic priors remains a main concern. It would be beneficial if the authors could provide specific scenarios where the construction of taxonomic priors is indeed essential. In addition, the comparison with [2] is essential.

This paper fully considers potential negative societal impact of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper tackles open-world semi-supervised learning and proposes to use multi-granularity labels as taxonomic context priors to leverage hierarchical supervision to enhance representation learning and improve the quality of pseudo labels. A taxonomic context discovery module is used to construct hierarchical prototypes; a taxonomic context-based prediction alignment module is applied to enforce consistency across multi-granularity predictions. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation and idea of this paper are clear and well-explained. Multi-granularity supervision is introduced to improve the representation of base and novel classes. The alignment among each granularity is enforced.

Weaknesses:
1. Additional baselines are needed for comparison: 

[1] Pu N, Zhong Z, Sebe N. Dynamic Conceptional Contrastive Learning for Generalized Category Discovery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 7579-7588.

[2] Zhang S, Khan S, Shen Z, et al. Promptcal: Contrastive affinity learning via auxiliary prompts for generalized novel category discovery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 3479-3488.

[3] Vaze S, Han K, Vedaldi A, et al. Generalized category discovery[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 7492-7501.

[4] Wen X, Zhao B, Qi X. A Simple Parametric Classification Baseline for Generalized Category Discovery[J]. arXiv preprint arXiv:2211.11727, 2022.

[5] Chiaroni F, Dolz J, Masud Z I, et al. Mutual information-based generalized category discovery[J]. arXiv preprint arXiv:2212.00334, 2022.

2. The actual number of classes must be known. Otherwise, it is difficult to determine how many classes should be defined for sub-class, target-class, and super-class, respectively. 

3. The real composition of the sub-class and super-class may not go exactly as designed. It is difficult to guarantee each sub-class only contains samples of one target class, and each target-class only belongs to one super-class.

Limitations:
The number of novel classes needs to be known. The class-distribution needs to be balanced. These assumptions make TIDA not robust for practical usage. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, a new Taxonomic context pIrors Discovering and Aligning (TIDA) which exploits the relationship of samples under various granularity is proposed. TIDA comprises two key components: i) A taxonomic context discovery module that constructs a set of hierarchical prototypes in the latent space to discover the underlying taxonomic context priors; ii) A taxonomic context-based prediction alignment module that enforces consistency across hierarchical predictions to build the reliable relationship between classes among various granularity and provide additions supervision. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The authors identify the importance of multi-granularity priors in the context of OSSL and introduce the taxonomic context priors for solving the OSSL problem.
2. A uniformed OSSL framework, which can discover taxonomic context priors without any extra supervision is proposed. With the proposed cross-hierarchical prediction alignment, the framework can effectively enhance the performance of the model.
3. This paper is easy to follow.

Weaknesses:
The hyper-parameter $\alpha$ and $\beta$ depends on the specific datasets and for different datasets, experiments need to be conducted to determine the value. 

Limitations:
The authors addressed the limitations. 

Rating:
5

Confidence:
3

";1
FYqqvQdXhZ;"REVIEW 
Summary:
This paper proposes a novel method for obtaining ""intrinsic images"" for an RGB image, where in this context intrinsic images mean mainly depth, normal, segmentation, albedo, and shading maps. To this end, the authors investigate how a pre-trained StyleGAN generator model can be used to output the respective intrinsic image instead of an RGB image by only adding a constant in style weight space. More specifically, the proposed method optimizes an offset vector in the StyleGAN ""w+"" style space for a given editing constraint. The offset vectors are optimised via gradient descent where a comparably small dataset of generated images together with respective intrinsic images, predicted by SOTA models, is used as guidance information. The authors compare the proposed method to SOTA methods and find that it performs comparably. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The proposed approach is an interesting and valuable method for investigating what information is already contained in generative models such as StyleGAN. The finding that adding an offset vector in style weight space to output one of the other data modalities instead of RGB is surprising and very interesting. It highlights how much ""semantic and 3D scene understanding"" can be found in 2D generative models.
- The authors investigate the performance of the same proposed method with multiple different data modalities highlighting the generality of the approach.
- I appreciate that the authors also show an example decomposition that clearly does not work with the given method (L. 178). As only a style weight space vector is added, non-pixel-based transformations are expected to not work well / fail.
- The authors compare against relevant baselines quantitatively and qualitatively and show helpful visual comparisons.

Weaknesses:
- Performance across different datasets / Dependence on small labeled dataset: The proposed method requires a sub-set of ""labeled"" generated image and intrinsic image pairs, where the latter is provided by a SOTA method. Have the authors investigate how well the method performs if this ""fine-tuning"" is done on a different dataset, or on only a sub-set of the covered ""modes""? More general, investigating different factors of this training stage with the resulting end performance, e.g. in form of a graph showing number of training images against performances, would be very interesting.
- Labeled dataset: In similar spirit, how many labeled images (provided by the SOTA method) are used? (L. 124 - 126 is vague on this, and it would be beneficial to state clear numbers.)
- Evaluation: The authors argue that no ground truth is available, hence SOTA predictions are used as GT. But couldn’t the test set of these SOTA methods be used as GT? For the proposed method, the input image would be needed to be inverted in StyleGAN space, but this has been extensively done before in the literature. Then, the proposed method could directly be compared to the SOTA prediction model itself.
- Depth Comparison: It is unclear to me why the proposed method performs better in the L1 depth metric (Table 1) while it looks qualitatively worse than the other methods. Could the authors expand on this? I am not sure if this is caused by some scale/shift misalignments; as these are monocular predictors, the scale and shift is arbitrary, so this needs to be taken into account during evaluation.
- Caption Fig 6: “Note that our quantitative comparison in Table 2 to a SOTA segmenter [17] likely understates how well StyleGAN can segment;” -> Can the authors expand on this? This is not clear to me. The authors give the two-lamp example, however, here all methods were able to segment the two lamps.  	

Limitations:
- The authors show and discuss and example data modality / transformation that is cannot be tackled with the proposed approach due to the pixel-aligned features (L. 178).
- The authors investigate the performance of the proposed system for different lighting conditions of the images (L. 184).
- In the discussion (Sec. 7), the authors discuss unclear points that would require additional investigation. This includes to what extent these findings translate to other generative models and if more intrinsic image modalities next to the studies ones are contained in StyleGAN's weight space.
- The authors do not discuss potential negative societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposed the method of extracting intrinsic images (Normal, Depth, Albedo, Shading, and Segment) from a pre-trained StyleGAN model without any pre-training or finetuning. It is empirically shown to be better than the SOTA for getting images intrinsic and robust to relighting effects, unlike SOTA.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Showing that the StyleGAN W-space also learned the internal representation of intrinsic scene properties without being explicitly trained for the same.

Proposed a simple, effective, and generalizable method for extracting intrinsic information without explicit training StyleGAN. Optimization based method is used to search the desired latent code for intrinsic image generation.

Extracted intrinsic images as robust to lighting changes as compared to SOTA for extracting intrinsic images.



Weaknesses:
This work can be considered as StyleGAN W-space exploration for intrinsic image generation.

As mentioned in the paper, pre-trained StyleGAN is used, but all the quantitative and qualitative results are only shown for the LSUN Bedroom dataset. Does it applicable to other datasets, also such as the LSUN Church dataset, Animal Faces, FFHQ face, etc.?

As per the method proposed in the paper, pre-trained intrinsic SOTA models are used for the optimization, so exploring W-spaces for these datasets directly depends on how well SOTA generalizes for other datasets.

For real images, if we can use the SOTA models for intrinsic extraction directly, why is there a need for GAN inversion, as discussed in the paper? One of the purposes of GAN inversion is to give control over image editing of real images, and as per the problem statement in the paper, there is no such task of editing images based on intrinsic parameters.

The proposed work uses the SOTA model for optimization, then it is possible for the SOTA model to make incorrect predictions for some images due to domain shifts in train-test data, and it can carry forward to latent space optimization as well as in quantitative and qualitative evaluation.

Minor mistakes:
On line 117, it should be w+’ not w’.


Limitations:
The proposed method does not work on real images. The authors pointed out that this is due to GAN inversion, which does not preserve the parametrization of the W-space. With improvement in the GAN inversion method, the proposed approach can work on real images also.



Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper uses StyleGAN latent space properties to extract properties like normals, depth, segmentation, albedo and shading. The method does not train a new model to achieve this. The method relies on the properties of w+ space to predict these properties. In practice, the method uses the off-the-shelf predictors to use as pseudo labels and the signal is back-propagated to the w+ code. The authors evaluate their method on the LSUN bedroom dataset. The generated images and their corresponding predictions show that the StyleGAN is able to generate consistent results on bedrooms which suggests that the style codes can be modified to generate results in other domains not seen by the network.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1)  The paper explores a method to generate multiple properties of an image just using the w+ space of the StyleGAN. The paper shows that these properties like normals, depth, segmentation, albedo and shading can be produced by the latent space of the StyleGAN which is primally used for style transfer tasks. The results in the paper shows that the method produces consistent results on bedrooms.

2) The method works good on StyleGAN generated bedroom images. The paper shows multiple figures demonstrating consistency of the method and the qualitative evaluation.

3) The paper evaluates the method using the metrics of the corresponding tasks. The results show that the scores are not random and the method produces meaningful predictions of the properties.

Weaknesses:
1) Firstly I feel that the abstract of the paper can be improved. To me it felt more like an introduction than an abstract. I would ask the authors to revisit the abstract and write it in the conventional way. What problem are you solving ? Why is it important/ what is the motivation? What is your solution? What results you show in the paper. In the current form it is not informative to me.

2) While the paper has proposed to solve some discriminative tasks using StyleGAN, there are other papers like DatasetGAN[1], Lables4Free [2] etc for segmentation and others related like [3] that already show that the features of the StyleGAN can be modified to achieve the goal or a dataset derived from StyleGAN can be used for the training. It is not clear if the method is generalizable especially when the previous methods have shown to exhibit these properties. How general is the method compared to these methods for example in segmentation domain? I would suggest adding a comparison.

3) I am not sure about the evaluations done in the paper. While pseudo labels for evaluation is a way to evaluate the results, another (more significant) way would be to train a supervised network (see Labels4free) based on the predictions of the StyleGAN. Then it can be tested on the real images rather than the generated ones. What about the inversion of a real photograph? Is the method generalizable to the real domain? The current evaluations do not show that. 

4) The authors only test on LSUN bedrooms dataset. It is difficult to access the contribution of the paper if its evaluated on only one dataset especially when other pre-trained StyleGANs are available. Why did the authors only evaluation on this particular dataset?

[1] Abdal, Rameen, Peihao Zhu, Niloy J. Mitra, and Peter Wonka. ""Labels4free: Unsupervised segmentation using stylegan."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13970-13979. 2021.

[2] Zhang, Yuxuan, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. ""Datasetgan: Efficient labeled data factory with minimal human effort."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10145-10155. 2021.

[3] IMAGE GANS MEET DIFFERENTIABLE RENDERING FOR INVERSE GRAPHICS AND INTERPRETABLE 3D NEURAL RENDERING, ICLR 2021

Limitations:
The authors have not discussed the limitations of the work clearly. When does the method fail? Does the method work without the truncation of the latent space where the samples can be diverse but not high quality? How do the off-the-shelf predictors behave in this scenario? What about other datasets for example ImageNet, (StyleGAN-XL)?

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper finds that for a GAN generator that is solely trained on images, intrinsic information such as depth, albedo, normal, etc. are implicitly learned during GAN’s training, even though such information is not provided as supervision. More surprisingly, such intrinsic information can be easily retrieved by calculating a direction offset of the latent codes via optimization an L1 loss on monocular predictor’s predictions on a set of generated images. This operation is not per-scene, means the offset is generalizable to the entire latent space.

Soundness:
2

Presentation:
4

Contribution:
4

Strengths:
- Originality: To my knowledge, the work is an original finding. It is the first work to explicitly show that intrinsic information is implicitly learned during an image-trained StyleGAN. It is also novel in finding that the intrinsic can be retrieved by a very straightforward optimization that results in a constant additive direction in the latent space, without any finetuning of the generator itself.

- Quality: The work is technically somewhat sound. The finding presented is appropriate and steps logically. Claims are supported via qualitative results, and quantitative experiments compared against SOTA monocular predictors. The authors have discussed the limitations of their method honestly and in much detail---I really like the “Discussion” part, which contains several questions which I raised too while reading the paper, and Figure.7, which shows a clear example of the limitation of the intrinsic nature of the presented claim. However, see my 1st comment in weaknesses for a factor that makes me hesitate to accept this paper.

- Clarity: The paper is overall very clearly written. The claims and approaches are clearly presented and are also easily understandable, with many of my main concerns while skimming through the paper addressed in significant places.

- Significance: I think this paper presented a fascinating finding. Researchers have been suspecting that the intrinsic information should be learned implicitly in the generator, but it is definitely surprising to see that the operation is as simple as a constant additive offset to the latent space that scales across scenes, without any further training. I believe the paper is a major result for the community.


Weaknesses:
- I think the main issue with this paper comes from its narrow evaluation criteria. All experiments conducted in the paper are only based on bedroom images which may have a bias to support the claim of this paper. As the authors have pointed out, perhaps StyleGAN ‘knows’ intrinsic images because they are an efficient representation of what needs to be known to synthesize an image. It could be difficult to see the finding scale to other datasets, especially in the wild image datasets where there is no clear principal component.

- In addition, the paper seems to be a solely ‘scientific finding’ paper, I have difficulty thinking about what aspect of this finding will be useful in terms of the application side since there is clearly still a performance gap with (even non-SOTA) predictors, and it needs SOTA monocular predictor to guidance the finding of the direction offset at the first place.


Limitations:
The authors have discussed the limitations of their method honestly and in very much detail.

Rating:
6

Confidence:
4

";1
GuErIOGLie;"REVIEW 
Summary:
This paper proposes a unified Seg2Seg framework for simultaneous sequence generation tasks. The framework introduces a latent segment as the pivot between the source and target, exploring all potential mappings via the proposed expectation training. With this approach, the authors achieve high-quality generation with low latency across multiple simultaneous generation tasks, including streaming ASR, SimulMT and SimulST.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper introduces latent segments as the pivots to explicitly model the mapping between source and target in simultaneous generation tasks.
- The proposed Seg2Seg framework exhibits better performance across various simultaneous generation tasks compared to existing methods.



Weaknesses:
- The current version of the proposed framework lacks some details, making it unclear how the prediction of target tokens is learned.
- As the proposed framework leverages wav2vec 2.0 to convert audio into a sequence of discrete hidden states, streaming ASR and SimulST tasks actually could be viewed as special SimulMT tasks. It is necessary to conduct experiments, such as GSiMT and HMT, to verify the performance of previous SOTA SimulMT methods on these tasks.
- There are some confusing experimental results. Figure 4 shows that the performance of the offline model is better than that reported in the HMT paper, despite using the same training dataset and model structure. It is unclear whether the improvement of the proposed method contributes to the better offline model or the Seg2Seg framework compared to HMT.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents an approach (Seg2Seg) for simultaneous sequence generation, which attempts to balance high generation quality with low latency. Rather than processing the entire source sequence at once (incurring high latency), the model splits it into segments. Latent segments are used to aggregate source tokens and generate the corresponding target sub-sequence. While latent segments are discrete for inference, Seg2Seg uses expectation training to learn optimal segment boundaries, combining the cross-entropy loss with a latency loss. Experiments are conducted on streaming automatic speech recognition, simultaneous machine translation and simultaneous speech translation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The model introduced in this paper learns when to stop aggregating information based on latent segments instead of using pre-defined heuristics.

The approach is applicable to multiple simultaneous generation tasks (written and spoken modalities).

Over the 3 considered tasks, the proposed approach generally has favorable BLEU/lagging trade-offs compared to many baselines.

Weaknesses:
The notion of adaptive wait policies has been explored in previous work (cited by the authors). This work incrementally improves on known concepts.

Some figures are quite small and might be difficult to read if the paper is printed out.

Limitations:
Yes, limitations are addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a method for simultaneous sequence to sequence modeling.
The proposed approach introduced a latent segment, which represents the number of consecutive reads followed by consecutive writes.
The method is applied to three simultaneous sequence to sequence modeling task, streaming ASR, simultaneous MT and simultaneous ST.
Results demonstrate better latency-quality tradeoffs compared to a number of baselines. Finally, the framework is applied in a multitask setting (ASR + MT + ST) where it is shown that combining more tasks presents better latency-quality tradeoffs.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
* the method nicely generalizes previous ideas on simultaneous modeling and is broadly applicable to both ASR and translation
* the main results are strong compared to the state of the art.
* the ablation analyses convincingly show that the improvements come from the proposed approach

Weaknesses:
See more detailed comments/questions below on how the paper can be improved.

Limitations:
N/A

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper addresses the task of simultaneous sequence generation in real-time scenarios, where the target sequence is generated while receiving the source sequence. The authors propose a unified segment-to-segment framework called Seg2Seg to tackle this challenge. Seg2Seg aims to learn the optimal moments for generating by alternating between waiting for a source segment and generating a target segment, using a latent segment as the bridge between the source and target. The proposed framework employs expectation training to explore all potential source-target mappings and achieve adaptive and unified learning. The authors applied their proposed method to three real-time tasks: Streaming Automatic Speech Recognition (ASR), Simultaneous Machine Translation (SimulMT), and Simultaneous Speech Translation (SimulST). The experimental results demonstrate that their approach achieves a favorable balance between performance and latency in all three tasks.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors aim to introduce a unified framework for training and inference in real-time tasks, and they have successfully achieved this unified approach.

Weaknesses:
1. The proposed approach lacks novelty as it bears similarities to the MMA framework.
2. The efficiency of the proposed method is relatively low as it requires backpropagation through multiple training objectives, resulting in increased training time.


Limitations:
N/A

Rating:
6

Confidence:
4

";1
B4tkwuzeiY;"REVIEW 
Summary:
This paper studies generating strings from highly structured languages for large language models (LLMs). The authors propose grammar prompting to enhance LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus–Naur Form. Serving as intermediate reasoning steps, these grammars (metalanguage) enhance the model's performance in generating highly structured languages in domains such as semantic parsing and molecule generation. The idea is simple and effective, and the experimental results well support the claim.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
+ The paper is well-written and easy to follow.

+ A simple and effective method was presented, leading to improved downstream performance compared with simple prompting.

Weaknesses:
- The idea is not that novel, which uses metalanguage as the bridge that leads to the ultimate structured language. The method is an intuitive extension of the standard prompting (or chain-of-thought prompting) method.

- Improvements over simple prompting methods are validated, but whether the method can beat more sophisticated algorithms (e.g., the recent tree-of-thought reasoning [3]) is unknown. However, as the authors claimed, the focus of this paper is not to achieve SOTA in downstream tasks.

- The proposed constrained generation method is applied at the sub-sentence level, instead of the token level. Besides, the idea of constrained decoding has been explored in previous works [e.g., 1,2], which in turn challenges the novelty of this paper.


[1] Hokamp, Chris, and Qun Liu. ""Lexically constrained decoding for sequence generation using grid beam search."" arXiv preprint arXiv:1704.07138 (2017).
[2] Huang, Wenlong, et al. ""Grounded decoding: Guiding text generation with grounded models for robot control."" arXiv preprint arXiv:2303.00855 (2023).
[3] Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" arXiv preprint arXiv:2305.10601 (2023).

Limitations:
NA

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors investigate the effectiveness of grammar prompting as a simple approach to help LLMs utilize external knowledge and domain-specific constraints. This approach is motivated by the goal of enabling LLMs to generate DSL outputs that differ significantly from those encountered during pretraining. The authors achieve this by using a BNF grammar during in-context learning. In their framework, the LLM first predicts a BNF grammar based on a test input and then generates output constrained by the rules of the grammar. The experiments conducted demonstrate that grammar prompting enables LLMs to perform competitively on a diverse range of DSL generation tasks, such as semantic parsing, PDDL planning, and molecule generation.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- Novel method for prompting and constraining LLM generation using BNF grammar.
- Strong experimental results.
- The method is described clearly and is sound.
- The method is relatively simple yet effective. The framework is refreshing and is expected to have a significant impact on the semantic parsing community.


Weaknesses:
Minor: The contributions and novelty should be stated at the end of the introduction section.


Limitations:
No negative societal impacts


Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper presents an approach for improving few-shot prompting of LLMs for tasks that produce structured outputs, where the structure can be described by a grammar in BNF form. The approach is similar to chain-of-thought prompting: for a given input x, the LLM is prompted to generate the minimal subset of grammar rules that can produce the corresponding output G[y], then conditions on x and G[y] to produce y. Decoding can be done in the standard way (e.g. greedy decoding), but the paper also presents an improved version that uses a modified Earley parsing algorithm to ensure that the outputs y conform to the predicted grammar G[y]. All variants of the approach are effective, providing substantial benefits over reasonable baselines (e.g. standard few-shot prompting and constrained decoding) across a range of tasks, including realistic semantic parsing benchmarks, SMILES molecule generation, and plan initialization for PDDL planning.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The approach is elegant and seems simple to implement, treating the LLM as a black box and needing only knowledge of the output grammar. While the constrained generation procedure is a bit more complex (and requires multiple LLM calls), it's also well-motivated and elegant, and isn't required for strong performance. I could definitely see the overall approach being practically useful in tasks that require DSLs that were low-resource in pre-training, such as tool use.

The experiments are thorough, with strong results across a range of tasks and datasets. The compositional generalization results on GeoQuery were especially interesting to me, as they indicate that the grammar examples in the prompt don't need to fully cover the grammar rules used in the output.

The paper is very clearly written, especially given the number of experiments presented and space constraints (but see below for a few questions). 

Weaknesses:
One minor weakness is that it would benefit the paper to also show results on other LLMs to give evidence that the approach is broadly applicable, especially since of the three models evaluated, GPT-3.5 derives from Codex (I believe, if the paper is using code-davinci-002) and GPT-4's training data is unknown. StarCoder-15B could be one such model, although it may not be few-shot promptable. 

A few details could be made clearer, if space is available, see below.

Limitations:
The paper's limitation section did a good job, I think.

One additional potential limitation, though, is that, like in chain-of-thought prompting, the approach effectively introduces a pipeline: the model needs to first predict the grammar rules for an example, then condition on these rules to predict the output. This could lead to error propagation if the first step is wrong. However, the results are strong enough to indicate this probably isn't happening, and if it did it could be addressed with self-consistency / consensus decoding or another MBR-like approach.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This work explores grammar prompting as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus–Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. 

The authors apply grammar prompting to various domain specific languages for semantic parsing (SMCalFlow, Overnight, GeoQuery), AI planning (PDDL), and molecule generation (SMILES), and find that it can meaningfully improve upon standard prompting baselines in the few-shot setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

This paper is interesting and well-written. 

This work is an efficient prompting generation algorithm for LLM. 

Experimental results show that the proposed method seems effective in generating sequences with grammars. 

Weaknesses:



In experiment part, how do you ensure the results are enough to verify the proposal? For example, the superiority in molecule generation is not well established compared with the other graph-based generation methods.  



Limitations:
See above.

Rating:
5

Confidence:
3

";1
vKpVJxplmB;"REVIEW 
Summary:
This paper builds on recent work connecting the hippocampus to the transformer architecture by introducing a new hippocampus-inspired activation function for the transformer's feedforward modules. In a toy navigation task, several empirical investigations show that the choice of this activation function has a large effect on the model's ability to store and recall information from longer-term memory, and that models using brain-like activation functions contain neurons with place cell-like firing patterns.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* This paper continues an interesting research direction -- exploring the connections between the hippocampus and the transformer architecture. This is exciting work that is relevant to both machine learning and neuroscience.
* The empirical analysis is presented well with many attractive and easy-to-interpret figures, seems quite thorough, and largely supports the paper's conclusions.

Weaknesses:
* The paper presents a convincing argument that the nonlinearity in the feedforward module of a transformer is important for forming and recalling certain types of memories, and that the brain-inspired nonlinearity is among the best. However, it provides essentially no understanding (even intuitively) of why this should be the case.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper uses a transformer on neuroscience relevant task of spatial navigation, and shows when trained on both novel and familiar tasks simultaneously, place cell representations appear is different parts of the transformer depending on the current task – place cells in the feedforward net (post self attention) for familiar tasks, and in the self-attention layer for both novel and familiar tasks. The ability for the transformer to perform on familiar tasks is related to the activation function in the feedforward net.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Interesting question of different representations for different task distributions. Gets at hippocampal consolidation which is under-explored with models. Convincing simulation results. 

Weaknesses:
1)	The NMDAR part of the paper is the least convincing, being that standard activation functions like ReLU seem to work just as well. Don’t think you can have sentences like ‘We find that NMDAR-like nonlinearity is essential for shifting short-term working memory long-term reference memory in transformers’. 
2)	The NMDAR impairment is essentially just making the activation function more linear, which of course will mean that it can’t effectively store long term memories. 
3)	To effectively make the NMDAR receptor point as a different activation function class, you’d need to test it on more standard ML tasks, rather that bespoke neuroscience tasks.
4)	You have shown a nice potential neuro-ai link via NMDAR, but it’s currently just a potential relationship. It would need a lot more testing to make it concrete, e.g. showing that alpha=10 (the best performing model) is the regime that the brain operates in etc.


Limitations:
As described in the weaknesses, the main limitation is about the interpretation of NMDAR and the relevance of the activation function for ML. Otherwise the paper is good.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Many recent papers have shown a relationship between Transformers and biological structures, particularly the hippocampal formation. This work demonstrates that the types of non-linearities provided by NMDAR dynamics (which have known biological importance) can be beneficial in a Transformer architecture for a toy what-where memory/navigation task.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
This paper deepens the connection between Transformers and biology, which helps both the neuroscience and machine learning communities in understanding these systems. It may also help improve Transformer models, which are already very popular and performant but may be further improved by inspiration from neuroscience.

Weaknesses:
The values of $N$ used in Figure 3 suggest that in this case there is a ceiling effect, i.e., once N is sufficiently large, the benefit of the NMDA-like non-linearity becomes negligible. Further, while error bars are included, I could not find the statistical testing to demonstrate differences between the groups. I think it is important to do such tests to measure the size and significance of any effects.

Although the what-where task is clearly neuroscientifically-inspired and has been used in past similar studies, it is not clear whether anything contained here will generalise to other, more ecologically-valid tasks from neuroscience or more naturalistic real-world data from machine learning. The authors should test on more complex tasks/data.

Limitations:
Limitations need more discussion.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper investigates the resemblance between the NMDA receptor (NMDAR) in the hippocampus of the human brain and the activation functions used in the transformer architecture (e.g., ReLU, GELU). Then, this paper presents a new activation function that exhibits similarities to NMDAR. It demonstrates that by adjusting the hyperparameter associated with this activation function, the memory capabilities of transformers can be fine-tuned. A 2D grid navigation experiment with transformers is investigated to examine the working memory and the reference memory.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper explored the famous transformer model from a neuroscience perspective, by drawing connections with the hippocampus in the human brain.

2. This paper first investigates the similarity between the NMDAR in the hippocampus and the activation function such as ReLU and GELU in transformer models.

3. The paper in general is well-written and easy to follow.

Weaknesses:

1. Although the proposed idea is interesting and neuro-inspired, the technical contribution seems limited (for ML venues). Based on my understanding, Sec 2.2 is a theoretical review of existing work, and the derivation of the NMDAR activation function in Sec 2.3 (w/. A.3 in appendix) is in general straightforward given previous work.

2. The empirical results on the 2D navigation task seem promising, but it may be worthwhile to explore more general tasks that transformer models are typically applied to, e.g., language tasks, to better validate the efficacy of the proposed activation function.

3. In Figure 3 (a), the result shown in the upper right subfigure (test) demonstrates that the proposed method could not predict the unvisited nodes in the novel map. Are there more detailed explanations for the phenomenon?

Limitations:
I did not find potential negative societal impacts in this work. See the “Weaknesses” section for my concerns.

Rating:
5

Confidence:
2

";1
hoyL1Ypjoo;"REVIEW 
Summary:
This paper proposes a new black-box optimization framework, called WireMask-BBO, for macro placement, which is an important problem in the electronic design automation (EDA) community. By using different black-box optimization algorithms, The experiments show it can achieve improvements (shorter half-perimeter wirelength (HPWL)) over previous methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The general framework WireMask-BBO proposed by this paper somewhat provides new angle for macro placement problem. The paper is well-written and it is easy to follow.

Weaknesses:
The scalability of the proposed method is doubtful because Bayesian optimization and evolutionary algorithm may not be suitable for large-scale problems.
The experiments are insufficient. The experiments don’t compare with the state-of-the-art macro placement method. The experiments don’t compare the runtime of the proposed method with other methods.

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a framework using BBO for macro placement in VLSI designs. Any placement solution for a set of macros can be optimized using the wire masks (presented in a prior work using RL) where the optimization goal is to minimize the HPWL of the output. In addition to random inputs, the framework can also be used for further enhancement of any existing solutions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The work tackles a critical problem in VLSI designs. 

The idea of casting the placement problem to a BBO is interesting and novel.

This framework can be used to further improve any existing solutions for the macros. This can be used as another step in PnR with reasonable runtime.

Overall, the paper explains the problem, the existing solutions, and the proposed work clearly.

Weaknesses:
The idea of casting the placement problem to a BBO is interesting and novel. However, as the authors admit in Section 1 that this work does not develop any new BBO algorithm. To the reviewer, the experimental results are not extensive and convincing; this is described in the limitation section. 

The paper also states in the conclusion that it can only place macros but not standard cells without explanation, which limits the application of the proposed work to the actual VLSI problem. On the other hand, this framework can be used to further improve any existing solutions for the macros. This can be used as another step in PnR with reasonable runtime.

Limitations:
The computational time for the proposed work seems to be high. Therefore, in the experimental section, the paper does not include the complex benchmark that has thousands of macros to be placed. This contradicts with the claim the paper that this approach is scalable.

The evaluation of the proposed method only uses 5 random seeds. It would be more convincing if the paper includes more experimental data.

The proposed framework cannot place the standard cells in the design. Therefore, the comparison between this work and the existing methods seems to be unfair.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a new placement method that is based on the black-box framework. The framework leverages the wire mask-guided information and can achieve significant placement results compared with the state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The novel method is based on black-box optimization, which has not been implemented on the placement task before. Although many RL methods have been proposed, they are not very efficient enough. Black-box optimization might be a viable direction. 

2. The wire mask as the guide for generating phenotype representation is also very novel. It can quickly render a suitable solution based on the initial representation, improving efficiency. 

3. The experiments are very comprehensive and solid, showing that the performance of the proposed method can consistently outperform existing methods.

4. The paper writing is well-written and easy to understand.

5. The code is open-source, improving reproducibility.


Weaknesses:
1. The full placement cannot surpass the DREAMPlace, which means the proposed method can only work well in macro placement.

2. The reasons for the improvement in the congestion metric are not clear. The method does not consider the congestion metric in its search process.

3. The recent work [1] based on Maskplace should also be discussed in the related work part.

[1] Lai Y, Liu J, Tang Z, et al. Chipformer: Transferable chip placement via offline decision transformer.


Limitations:
Yes.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel black-box optimization (BBO) framework, namely WireMask-BBO, for macro placement in chip design. Specifically, it devises a post-processing technique that legalizes any searched placement solution while optimizing the half-perimeter wirelength (HPWL). The post-processing technique allows us to perform BBO algorithms to search for solutions with better HPWL. Experiments demonstrate that WireMask-BBO outperforms previous state-of-the-art (SOTA) methods, achieving better HPWL performance in less time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	This paper explores BBO methods for macro placement, which may provide a new insight for the research community.
2.	The proposed post-processing technique is simple yet effective. It also has a good versatility because it can be combined with other placement methods and BBO methods.

Weaknesses:
1.	The propose of the post-processing is purely heuristic. The motivation is unclear and intuitive explanation for the advances is insufficient.
2.	The proposed method only targets on optimizing HPWL, without explicitly considering other important metrics like routing wirelength or congestion. It does not consider cells and routing either. Moreover, the framework can be hardly transferred to tasks with those metrics under consideration, which limits its real application in EDA.
3.	Because introducing BBO-based methods is one of the core contributions of this paper, the authors may want to illustrate the implementation of BBO algorithms for macro placement more detailly.
4.	In Algorithm 1, the macros are ordered decreasingly according to areas, while in Figure 3, the smaller macro-A is considered first.
5.	A recent related work [1] is missing.
[1] Lai Y, Liu J, Tang Z, et al. Chipformer: Transferable chip placement via offline decision transformer. ICML 2023.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
CzkOzKWpMa;"REVIEW 
Summary:
The paper studies the cross-learning problem in contextual bandits, and proposes a $\sqrt{KT}$ regret algorithm.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes an algorithm achieving $\sqrt{KT}$ regret using an epoch-based schedule. The algorithm achieves optimal regret without requiring knowledge of the context distribution.

Weaknesses:
In applications (section 1.1 and section 4), it might be better to point out the explicit connection between cross-learning and bidding in 1st price auction / sleeping bandits. E.g. what is the context here in 1st price auction? 

The technical part was very hard to follow. I have doubts about the soundness of the technical approach, see questions below. 

The writing contains noticeable grammatical errors, e.g. line 214, line 237...

Limitations:
The authors did not address limitations.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper studies contextual bandits in the ``cross-learning' setting. They propose an algorithm that attains nearly tight regret of $\tilde{\mathcal{O}}(\sqrt{TK})$. They achieves this by carefully separating the horizon into blocks of fixed size and coordinate the estimation of unknown distribution and the action played by the algorithm to remove the correlations. They apply their methods in first-price auctions and sleeping bandits and obtain nearly tight bounds.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is in general well-written. The considered setting is hard yet the theoretical bounds are nearly tight. The algorithm is carefully designed to decouple the distribution of the estimated unknown context and actions chosen and is nicely presented. 

Weaknesses:
The cross-learning setting is not so meaningful to me. At least, I think the authors are not doing a good job in motivating this setting. The only example provided is bidding in an auction, where different counterfactual values of the items. However, the context here is only one-dimensional, and the loss monotonously depends on the values, as a result, the cross-learning setting is more likely a byproduct of the specific one-dimensional structure rather than something common in practical applications.

Limitations:
I don't see any limitations or potential negative societal impact of this work.



Rating:
5

Confidence:
3

REVIEW 
Summary:
This manuscript studies the following missing piece in [Balserio et al. 2019]: in contextual bandits with cross learning (i.e. with a complete feedback graph across contexts), suppose that the contexts are stochastic with an unknown distribution, is there an algorithm that achieves a \tilde{\Theta}(\sqrt{KT}) regret? Note that when the context distribution is known, an EXP3-type algorithm in [Balserio et al. 2019] achieves the above bound. The key difficulty in this manuscript is to estimate the sampling probability when carrying out the EXP3 algorithm. 

There are two main ingredients in this manuscript: 

1. Bypassing the high-probability UCB. Specifically, when \hat{f} is an estimate of f with |\hat{f} - f| \le a with high probability, the naive argument tells that 1 / (\hat{f} + a) \le 1/f with high probability. By leveraging the curvature of 1/x, the authors improved this result by showing that if in addition |E[\hat{f}] - f|\le b, then E[1/(\hat{f} + b)] \le 1/f provided that a \le \sqrt{f*b}. The final inequality is further ensured by the Bernstein concentration. This observation shows that one can use a smaller confidence bound (b instead of a), and essentially implies that the estimation of sampling probability is a much simpler problem and requires only sqrt(T) samples. This is the key to the success of the proposed algorithm. Significance: high. 

2. Dealing with the dependence issue. To this end, the authors applied several tricks. First, decouple the rounds for loss estimation and sampling probability estimation. Second, use the target sampling distribution given two epochs before. Third, use the sampling probability estimates one epoch before. This involves additional technical analysis for the continuity of EXP3-type probability updates. Significance: medium. 

This result is applied to two problems. The first is the bidding in first-price auctions in [Balserio et al. 2019] with binary feedback, where this manuscript complements the result of [Balserio et al. 2019] by establishing a T^{2/3} regret when the private value distribution is unknown. Significance: medium. 

The second application is sleeping bandits with stochastic availabilities, where a sqrt{KT} regret is shown. This result greatly improves the existing ones in the literature in the sense that both the regret and the computation time are polynomial in (K, T). Significance: high. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
As summarized above, I really like two components of this manuscript: 

1. An improved bound of |E[1/(\hat{f}+b)] - 1/f| taking into account both the L_infty and L_1 norm of |\hat{f} - f|. 

2. Formulating the sleeping bandits as a cross-learning problem and its application. 

Weaknesses:
I do not see a major weakness for this manuscript. At the beginning I felt that this manuscript is a bit narrow in scope (i.e. finish a small missing piece in [Balserio et al. 2019]), but the application to sleeping bandits is very interesting. 

However, I still have to say that the writing quality is poor and the presentation of high-level ideas is obscure. I strongly urge the authors to improve the writing, including: 

1. The explanation of the first technique on Page 4. Could you formulate it in a similar way to my point 1 in the summary? Also this result could be stated as a standalone lemma. In the paragraph, C_N is undefined too, and I don't know what N is. 

2. There are lots of notations in the algorithm description, including p, q, s, f. The most confusing quantity to me is s, for which I spent quite a bit of time understanding its role. For an algorithm with several components, my general suggestion is to explain them piece by piece. 

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the sequential learning problem with the presence if context, where the context is drawn iid from some unknown distribution and the losses are chosen by the adversary.  The feedback, that learner observes, is the loss for any context for the chosen action. This paper improves the previous result in this setting, by showing the $\tilde{O}(\sqrt{KT})$ regret upper bound for the algorithm with running time that scales with the number of context at each time step.
Authors demonstrate a range of real-world applications where such feedback model is useful.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Authors improve the previously known result for the unknown distribution setting, by proposing the new technique on the estimation of the loss for this problem that allows remove the dependence between the data from which the losses and the context distribution are estimated.

Weaknesses:
Some steps of the proof are omitted, so I could not verify the correctness of the proof. See Questions. 

For proposed applications, issues of the computation efficiency were not discussed.

Limitations:
I hope that my question can be answered, so I could verify the correctness of the analysis.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes an efficient contextual bandit algorithm in the cross-learning setting, introduced in Balseiro et al., in the case of contexts sampled from an unknown distribution and where losses are chosen adversarially. In contextual bandits, the learner only utilizes the reward associated with an action for the current context. In contrast, in the cross-learning setting, the learner also exploits the reward associated with different contexts, which results in regret bounds that are independent of the total number of contexts. This approach can be employed in scenarios where that extra information is actually available such as bidding in nontruthful auctions, multi-armed bandits with exogenous costs, or repeated Bayesian games with private types.

The proposed algorithm results in an optimal regret bound of $\tilde{O}(\sqrt{KT})$, with $K$ the number of arms and $T$ the number of rounds, thereby closing the existing regret bound gap in the literature. Building upon this optimal regret bound, the authors derived nearly tight regret bounds for two applications: bidding in first-price auctions, and sleeping bandits with arbitrary stochastic arm availabilities.

The proposed approach bypasses the existing challenges in extending cross-learning contextual bandit to the unknown distribution setting by: 1) constructing an estimator with an in-built upper bound on the expected sum 2) introducing a novel technique for scheduling the learning algorithm that removes the correlation between the empirical estimate of the unknown distribution and the actions chosen by the algorithm, which is an obstacle for employing concentration inequalities.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written, and the setup is clear. The limitations for extending existing algorithms are clearly explained, and the approach employed to overcome them is novel. To the best of my knowledge, the authors' result is new and shows a clear improvement over the state-of-the-art for cross-learning contextual bandits with unknown distributions. Thus, I recommend acceptance.

Weaknesses:
Although the paper is technically solid, the contribution seems incremental, and the actual impact of extending the work of Balseiro et al. to the unknown distribution setting is not entirely clear.

The computational efficiency aspect of the algorithm is not compared with what seems to be the most obvious baseline, i.e., the algorithm of Balseiro et al. that tackles the unknown distribution case.

Lastly, I feel that the paper could benefit from numerical experiments that support the derived theoretical results.

L. 117: “the we begin”

L. 161: “we’d” → “we would”

L.152/159: $\widehat{f_{tk}}(p) = \frac{1}{t} \sum_{s=1}^{t} p_{tk}(c_{t})$ → $\widehat{f_{tk}}(p) = \frac{1}{t} \sum_{s=1}^{t} p_{sk}(c_{s})$ ?

Limitations:
The limitations of the proposed approach are not entirely clear.

Rating:
6

Confidence:
2

";1
lM1UnEssuX;"REVIEW 
Summary:
The paper studies the problem of hypothesis selection under a memory constraint. Here one is given $n$ distributions $H_1, \dots, H_n$ with access to an oracle that can output 1) $H_i(H_j > H_k)$ for any $i,j,k$ and 2) $1(H_i(x) > H_j(x))$ for any $i,j$ and any point $x$ in the underlying space. Given a stream of observations $x_1,x_2...$ from some unknown $P$ the task is to select an estimator $\hat{H} \in (H_i)_{i=1}^n$ that is as close as possible to $P$ in TV distance up to a multiplicative and additive constant $\alpha,\epsilon$ respectively. Moreover, one wishes to do so using at most $b$ bits of memory at any point during the algorithm. 

The author's main result Theorem 1 shows that $s$ samples from $P$ are sufficient to output a suitable estimator with constant error probability, provided $bs \gtrsim \tilde{\mathcal \Omega}(n\log(n)/\epsilon^2)$ and note that $bs \gtrsim n$ is necessary by previous work. They achieve this result by introducing multiple technical ideas, the main one being the 'random ladder tournament'. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is mostly clearly written 
- The paper appears technically sound
- The paper introduces simple but novel technical ideas that not only near-optimally tackle their proposed problem, but also recover other known results. 

Weaknesses:
- My main concern is that due to the technical nature of the paper the short format of neurips is simply not enough for a meaningful presentation of the results. The motivating problem is under a memory constraint, yet just as one would learn how one can trade off memory for samples in the random ladder tournament (Lemma 3.1) the paper ends. More generally, from the main text, I don't feel like I have learnt how the algorithm actually works. In sections 1.2, 2 and 3 we are given glimpses of the key technical ideas necessary but it doesn't feel cohesive. 


Typos:
Page 4 ""Results in a better...""

Limitations:
The authors address limitations. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper studies hypothesis selection in a streaming setting, with samples arriving online. The main result gives a near-optimal memory-sample trade-off for hypothesis selection.  Along the way, the paper invents several new techniques to avoid expensive memory usage of prior work.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
This is one of the most classic questions in nonparametric statistics. Prior work [MS08, DK14] have tackled the run-time problem. I believe that its memory efficiency is also an important aspect. This paper offers exactly such a study.

At a technical level, the paper nearly resolves the question, up to some minor logarithmic factors. The high-level approach is simple and may be practical. The techniques, as far as I know, are novel and could potentially be used by later works. I have not checked the proof details in the appendix, but based on the main body, the main arguments are sound.

The paper is generally well-written. 


Weaknesses:
I do not find any major weakness of this paper. 

One thing I should point out is that the imperfect comparison issue (discussed in section 2.2.1) has been studied in the literature of noisy sorting. In fact, I believe the main result from the random ladder tournament, *at least* in the offline setting (section 3.1), can be derived simply in a blackbox way from Theorem 3.8 of Sorting and Selection with Imprecise Comparisons by Miklós Ajtai, Vitaly Feldman, Avinatan Hassidim and Jelani Nelson (TALG 2015) (https://dl.acm.org/doi/10.1145/2701427). In particular, their $\delta$ is your $3\Gamma$, the error gap between good and bad hypotheses in the comparison procedure (Algorithm 1). Their number of items there is your number of hypotheses, both denoted by $n$. So up to another factor of $k=3$, applying their randomized max-finding algorithm would yield a hypothesis that’s $9\Gamma$ close to the optimal.  (Another related work is https://arxiv.org/abs/1606.02786 where an improved algorithm is given and an application to hypothesis selection is explicitly derived. I have not checked the details, though.)

I would ask the author(s) to confirm this and see if an alternative argument using the paper above can be made and whether it’s interesting. 

I should note that this, if true, does not trivialize the results in this paper. The algorithm of Ajtai et al cannot be implemented in the memory-constrained setting, below a memory size of $n^{1/3}$. Also in the offline setting, the resulting constant is not optimal for the hypothesis selection problem. 


 

Minor comments
---
Line 62–67: list these two query access assumptions as \begin{itemize}

Line 65: “ the probabilities of the Scheffé sets”  — I think it’s more clear to refer to this as “the probability masses of the Scheffe sets”.

Line 117: “we sample a uniformly random new hypothesis from H” — Please clarify: is this sampling with replacement, if I view the meta-distribution as uniform over the n input hypotheses? That is, a hypothesis can be selected multiple times against the same single hypothesis in memory. 

Line 202: Regarding [MS08], mention that the simple linear-time selection from Appendix A doesn’t do such expensive preprocessing.

Line 265: Can you clarify in which scenario or parameter regime would you use Scheffé counts? (I didn’t read the appendix.) Note that the memory usage of this technique is quadratic in $n$. Hence, are you tracking these counts on a small subset of hypotheses?

Limitations:
The author(s) have discussed some future directions, though I think a conclusion section summarizing these would be great.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors study the problem of hypothesis testing for pdfs in a streaming model. The problem is to find a hypothesis H* (corresponding to a pdf) from a family {H1,...,Hn} that is closest to an unknown pdf P. The input is a stream of points drawn i.i.d. from P. At any time step the algorithm can ask for a new point or query a scheffe set of two distributions Hi and Hj in {H1,..,Hn}. 

The main theorem in the paper shows that the algorithm can ""learn"" P properly by using O(n\log n/b\cdot\frac{\log(1/\eps)}{\eps^2}) queries, where b is the memory used. This is close to optimal - within log n factor. 

Note: 
- proper learning a pdf means: |P-H*|_TV\le\alpha\cdot OPT({H1,...,Hn}, P)+\eps.
- scheffe set of two distributions H1, H2 is {x : H1(x)<H2(x)} 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Clean presentation and summary at the beginning of the paper.
- Almost tight bounds (upto log factors)

Weaknesses:
- Perhaps the model can be motivated better? Why the restriction to scheffe sets?
- Worth comparing the algorithm to vast statistical literature on Neyman-Pearson tests using log-likelihood computation.
- Need to compare and cite some of the vast literature on orienteering tournaments in directed graphs - I suspect some of the underlying results may already be known.

Limitations:
NA. The paper is theoretical in nature and is fairly upfront about it.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of agnostic distribution learning where i.i.d. samples are generated from an unknown distribution $X$. The goal is to find the best distribution from a given set of finite distributions $\\{H_1,\cdots,H_n\\}$ that is closest to $X$ under total variation distance. The authors specifically study the sample complexity, under which the memory that is needed to store the information of the samples is bounded. The main contribution is a trade-off between the number of bits of the memory and samples needed, which is tight up to a $\log n$ factor in the minimax sense.

The main proof technique is based on the so called ""random ladder tournament"". This differs from conventional approach that first estimating the probability of Scheffé sets of all pairs in $\\{H_1,\cdots,H_n\\}$ and then selecting a distribution closest to the estimates. In contrast, the authors propose an ingenious streaming algorithm that tracks the ""best"" distribution in an online fashion, which is suited for achieving their desired trade-off between memory usage and sample complexity.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper's primary strength lies in its novel ""random ladder tournament"" algorithm. This algorithm effectively achieves a memory-efficient sample complexity for agnostic distribution learning. The approach incorporates some interesting technical ideas that may be of independent  interest. As far as I'm aware, this particular scenario has not being studied in the literature.

Weaknesses:
My main concern regarding this paper is its overall significance to the machine learning community. The paper has a strong TCS flavor, and the introduced model appears somewhat contrived. Specifically, the derived trade-offs, though mathematically intriguing, do not offer any surprising insights. Thus, its practical implications and utility within a broader machine learning context may be limited.

Additionally, the paper's presentation could certainly benefit from some improvements. The authors dedicate a significant portion of the introduction to outlining their techniques, which, unfortunately, are quite challenging to grasp without delving into the substantive technical details found mainly in the appendix.

I would recommend eliminating most of this preliminary discussion and focusing primarily on the ""random ladder tournament"" argument, as outlined in Section 3. Providing a more detailed and accessible explanation of this central argument could make the paper more digestible for readers. Following this, a more straightforward discussion of the techniques used to improve the logarithmic terms would be better placed and easier to understand. This reordering should contribute to a more coherent and engaging narrative throughout the paper.

Limitations:
No issue with negative societal impact.

Rating:
6

Confidence:
3

";1
bTidcHIK2t;"REVIEW 
Summary:
Overfitting in deep Q-learning agents is a recent topic of interest in the RL community, and several methods have been proposed to mitigate this problem, including data augmentation (DrQ), random ensembles (RedQ, DroQ), and resets (Nikishin et al.). This paper builds upon prior work on periodically resetting weights and addresses one of its core limitations: by resetting weights, an agent will perform poorly immediately after resetting, although it eventually recovers and often exceeds its performance before resetting. This paper proposes to learn an *ensemble* of agents and periodically reset only one of the agents at a time (in sequence). Experiments are conducted on tasks from DMControl, Minigrid, and Atari100k, and indicate that resetting with ensembles is effective at mitigating deterioration of performance immediately after a reset.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper is well written, considers an interesting problem, and experiments appear sound. The description of the proposed method is easy to follow, and especially Figure 4 is useful for understanding how the ensemble resetting works in practice.

Weaknesses:
- It would be useful to include more discussion on related works that seek to understand and mitigate overfitting in deep Q-learning besides resetting. There is a lot of literature in this area and I imagine that the authors are familiar with the literature, so I will refrain from mentioning any specific references (besides the methods mentioned in my summary) to remain impartial.

- I would like to see more ablations to get a deeper understanding of the trade-offs in ensemble resetting vs. prior works. How does sample-efficiency and performance drop change when the number of ensemble agents $N$ varies? How often should one reset agents? Is the rate of resets and number of agents dependent on the replay ratio? What is the computational cost of using additional agents vs. increasing the replay ratio? Would the proposed method benefit from using the ensemble in other ways as well, e.g. by using the RedQ trick for computing TD-targets? Addressing some or all of these questions would likely increase the impact of the work.

Limitations:
There is sufficient discussion of limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper combines the resetting method proposed by (Nikishin et al., 2022) as a remedy to the primacy bias affecting deep RL algorithms with the use of ensembles of agents. The proposed RDE method, apart from generally improving performance, has the goal of minimizing the regret associated to a learning agent that uses periodic resets, mitigating the severeness of the performance drops it experiences right after a reset. To do this, without sacrificing too much on exploration and online data collection, the probability of executing an action in the environment is evaluated according to the oldest value function. Empirical results show benefits in using this approach, at both low and high replay ratio, in standard robotic locomotion and navigation tasks, as well as a safety domain.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**Originality**: despite the combination of resets and ensembles is not particularly original, the focus on developing a technique for leveraging their combination to avoid performance collapse while resetting is, to the best of my knowledge, new.

**Quality**: the quality of the work is generally good. The experiments cover a reasonable number of domains and the ablations mostly answer natural questions.

**Clarity**: the clarity of the writing is good. The paper would benefit from some small tuning to the presentation here and there, but the overall flow makes clear what the contribution is.

**Significance**: harnessing the performance benefits originating from the mitigation of the primacy bias while at the same time not incurring a cost in terms of regret is a worthy research direction which could be interesting to many practitioners and researchers.

Weaknesses:
**Major Concerns**
- If I understand it correctly from Algorithm 1, a different policy could be potentially selected at each step in the environment. This could conceptually create problems in terms of inconsistent behavior, since each policy will have to deal with the actions previously sampled from a potentially very different policy, and in terms of lack of ""deep exploration"", since this would harm temporally-consistent behaviors. A reader would benefit from a discussion or analysis of this aspect, to understand whether this is happening at all, or, if not, why that might be not happening in this kind of tasks of setting.
- The idea of combining periodic reset and ensembles of agents in continuous control has been explored in ""Unleashing The Potential of Data Sharing in Ensemble Deep Reinforcement Learning"" (Lin et al, 2022). I find the idea of the paper of using this combination to avoid performance drops to still be valuable, but discussing the relationship with that paper could better contextualize the contribution.

**Minor Concerns**
- Y and X labels are missing from all Figures in the paper, making it hard to parse the plots at first sight. For most plots, it is either quite easy to infer the quantities of interest, or they are explicitly mentioned in the caption, but it can still be very misleading for many readers.
- The paper would benefit from the extension of some of the ablations to more tasks. In particular, I find the performance of the ensemble-based approach without any resetting on top to be an important baseline to contextualize the results in Figure 3, and I think results concerning it would be a good addition to the Figure.

Limitations:
The authors do not discuss any computational consideration resulting from their use of ensembles of agents. I encourage the authors to add such a discussion to the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The work proposes an extension to the resetting strategy proposed by Nikishin et al. The extension intends to mitigate the catastrophic performance collapse often observed for the simple resetting strategy while keeping the properties that help avoid the primacy bias.
To this end, the work makes use of ensembling techniques, such that the policy/value network of an RL agent is not completely reset, but can fall back to different checkpoints.
The method is evaluated on a variety of environments and a further extension is presented that enables application in safety critical systems.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The presented method cleverly combines ensembling with resetting to improve RL agents.
* It seems general enough that it should be usable as a plug and play method for a broad variety of RL agents.
* The work mostly easy to follow
* It is demonstrated that the method is flexible and can incorporate auxiliary information, such as safety-critical information to provide a safer method than the vanilla resetting one

Weaknesses:
The presentation could be improved:
* A lot of the discussed ""preliminaries"" seem irrelevant for the content of the paper. For example, the paragraph on ""Off-policy RL."" seems not necessary and the content in ""Primacy Bias and Resetting Deep RL Agent."" is largely a repetition of the introductory text.
* Algorithm 1 is never discussed in the text and feels wholly redundant with Fig. 1. This half page might be used to show more experimental results.
* Algorithm 1 is not consistent with the text. From the Algorithm it looks like every ensemble member is reset every $N\times T_{reset}$ time-steps and not $T_{reset}/N$ as stated in line 149. 
* Lines 178 - 180 are concerned with expressing that the ""oldest Q-function"" is used to normalize in the selection mechanism. This can be expressed in much clearer terms ""oldest Q-function"" as is shown in line 183.
* In line 195 it is claimed that RDE effectively prevents performance collapses, however I disagree with this wording. It can mitigate it to some extent, but the experiments clearly show that there is still performance collapses happening.
* The description of Figure 2 is wholly confusing. The long sentence explaining what the y-axis is showing is expressed in a very convoluted way.
* In figure 2c it should not be possible that the performance of the baseline ""Base"" is below 1 when using an RR of 1.
* Design decisions are often not well enough explained.
* Figure 1 does not explain what RR is
* The paragraph heading in line 204 should be ""Baselines & RL Agents"" not ""Baselines & DNN Architectures""
* Line 246 claims that there is no significant drop on the humanoid-run example. The reward for the RDE method drops from 150 - 100. This represents a significant drop in my opinion since 1/3 of the performance is lost.
* The choice for designing the selection mechanism is very unclear. The discussion about using the oldest Q-function in the selection mechanism seems more to point to only using the oldest Q-function. Indeed, this seems to be supported in the experiments and should have been at least an ablation. Additionally the initial paragraph of Section 3.2 seems to also point to only using the oldest Q-function
* It is claimed that a $\beta$ set to 50 ""nearly eliminated"" performance collapses (line 270). However, the performance collapse in the Figure is again the 1/3 loss in performance.
* Section 5 feels like an added afterthought. I don't see why it warrants an own section. It repeats some of the discussion of safe RL from the preliminaries section. Instead the ""safe"" selection mechanism should have been discussed in Section 3 and then only the results should be a subsection of Section 4.
* Where does the value for the reset frequency $4\times 10^5$ in line 221 come from?

The experiments seem to have an unfair comparison and are likely not reproducible:
* All details about how hyperparameters were determined seems to be missing
* The value of $N$ is never stated. Only from the plots can it be assumed to be 2
* The ablation does not take into account all confounding factors and should be redone
* It is claimed that using a reset frequency for the individual members being equal to the single network case is fair. I fail to see how this is fair. This seems to just give benefit to the RDE method since it can benefit $N$ times as much from resetting.
* Without stating how hyperparameters were set of the methods, it seems like an arbitrary comparison of the methods.
* N is not really ablated and it's not clear how the ensemble increases computational overhead.

The description of an MDP is wrong. An MDP is an abstract representation of an environment. The MDP does not consist of an agent.
The bounds for the discount factor in line 74 should be $\gamma \in [0,1]$ not $\gamma \in [0,1)$. It is totally valid to have undiscounted cases.

Overall, the work would need significant rewriting and an overhaul for the experiments for me to accept it. I am very doubtful that this can be done in the time-frame of a rebuttal.

Limitations:
Limitations of the method have not been discussed. An obvious limitation is that ensembles likely will increase the computational overhead. For example, in line 131 it is said that the simple resetting strategy often requires high replay ration and therefor more resources. This should likely be worse for the presented method and the conducted experiments are not convincing enough to show that the novel method would require fewer resources.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel reset-based method that leverages deep ensemble learning to address the primacy bias issue in deep reinforcement learning. The authors construct N-ensemble agents and reset each ensemble agent sequentially to prevent performance collapses and improve sample efficiency. The proposed method is evaluated through experiments on various environments, including safe RL scenarios, and the results demonstrate its effectiveness and potential for real-world applications.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper addresses an important issue in deep reinforcement learning, namely primacy bias, and proposes a novel method to mitigate its effects. This is valuable as primacy bias can lead to overfitting and performance deterioration, which affects the applicability and efficiency of deep RL algorithms.
2. The use of deep ensemble learning in the proposed method is innovative and practical. Deep ensemble learning has shown effectiveness in domains such as image classification and RL, and leveraging the diversity gain of ensemble agents can enhance performance and prevent performance collapses.
3. The paper provides a comprehensive analysis of the proposed method, including its underlying operations and how it effectively prevents performance collapses. This analysis adds clarity and depth to the understanding of the proposed method.

Weaknesses:
1. The paper stated that RDE was conducted for tasks with safety requirements, but it only did one experiment on safe RL benchmark. Thus the paper lacks effective validations for RDE on violations. Simply stating that RDE does not cause performance collapse in general continuous control tasks cannot explain its role in safe RL.

Limitations:
The paper stated that RDE was conducted for tasks with safety requirements, but it only did one experiment on safe RL benchmark. The paper has no negative social impacts.

Rating:
4

Confidence:
4

";1
llP6lmMiXE;"REVIEW 
Summary:
This work proposes to use the triple correlation to achive the group invariance. Unlike max pooling, the triple correlation preserves the entire information from the original signal, which may improve the quality of solutions for downstream tasks. Experiments show effectiveness of the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The proposed method looks very natural. The proposed triple correlation layer achives the group invariance and at the same time preserves the entire information from the original signal. As far as I undertand from this paper, it seems to be the only group-invariant operation that enjoys this nice property in the deep learning literature.
2. The results from experiments look quite encouraging.

Weaknesses:
1. The novelty of the proposed method seem to be quite limited. It uses a well-known operation and apply it to NN.
2. The computational complexity of the proposed layer look quite high. Although the authors are able to save half of operations by using symmetry, the order of computation is still O(|G|^2).


Limitations:
yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel method to enhance robustness and group-invariance in group-equivariant convolutional neural networks (G-CNNs), named the G-triple correlation (G-TC) layer. This approach uses the concept of triple correlation on groups, a polynomial invariant map that is also complete, unlike many other invariant maps like the max. This ""completeness"" only removes variations caused by group actions but retains the signal's structure, contributing to the G-TC layer's strong robustness, particularly against invariance-based adversarial attacks. The authors also state that the G-TC layer results in improved classification accuracy over the standard Max G-Pooling in G-CNN architectures. An efficient implementation method for any discretized group is provided, and benefits are demonstrated on various groups and datasets.

The paper provides the context, highlighting the central role of the pooling operation in CNNs. The pooling operation has remained unchanged over the years, and while it serves its purpose of local-to-global coarse-graining of structure, it fails to construct robust representations that are invariant only to irrelevant visual changes. The paper then goes on to explain the role of group-equivariant convolutional networks (G-CNNs) and how they exploit group theory to achieve precise generalized group-equivariance. However, the pooling operation in G-CNNs is still susceptible to the lossiness of the max operation. To tackle this, the authors propose uncoupling the invariance and coarse-graining operations and introducing a new method for robust G-invariance through the group-invariant triple correlation. The authors demonstrate the superior performance of this approach through experiments showing improved classification accuracy.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper tackles the loss of information of the pooling operation in equivariant architectures and how to circumvent it. I really appreciated the fresh take on the importance of pooling and how even a very standard architectural block can still be improved. As a result, the paper brought to my attention the triple correlation operator which to my knowledge has yet to be extensively used in ML applications. Thus, this paper has a novel contribution that has the potential to benefit the community. Furthermore, I found the overall presentation and motivation of this paper exceptionally clear and thus it was an easy-to-read and parse paper. Finally, the empirical results do suggest that the introduced G-TC layer does have some benefits---although this analysis is a bit underdeveloped. 

Weaknesses:
There are a few notable weaknesses of this paper. The first being that the proposed approach can only work on discrete groups due to the pairwise nature of the G-TC layer. Many of the current breeds of equivariant networks have found application domains with continuous groups such as $SO(3)$, $SE(3)$, etc ... but the current setup does not seem easy to extend here. Can the authors maybe comment on this or if this can be even a direction for future work?

Furthermore, the scaling of this is $O(|G|^2)$---I understand the reduction by a factor of 2---but this is still quadratic scaling. This means that many potential groups of potential interest are eliminated. For example, one finite group that is not considered in this paper but could possibly be used is the symmetric group of $N$ elements---i.e. $S_N$. But I believe the computational complexity here makes it not possible to scale this up to permutation networks that also have invariance. Can the authors comment on this?

The background section on signals should really be in the established language of fiber bundles. This has already been done multiple times in equivariant ML papers see E(2)-CNN, ESCNN, ""A General Theory of Equivariant CNNs on Homogenous Spaces"" Cohen et. al 2018. 


The experiments in this paper are very limited to toy datasets in MNIST. I believe there is an avenue for multiple more important datasets that are larger scale. For example, E(2)-CNN has Cifar-10 but even this dataset is missing here. I encourage the authors to try larger-scale image datasets to show the benefits of their proposed approach.


**Minor**:
- Typo: Eqn 4. LHS your $g$ is written as $\tilde{g}$ but the paragraph below assumes regular $g$

Limitations:
N/A

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a $G$-triple-correlation ($G$-TC) pooling layer in order to achieve group-invariance in group-equivariant convlutional neural networks (G-CNNs). Compared to the widely used $G$-pooling layers, the proposed $G$-TC layer is supposed to be ""complete"" in the sense that it removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. In other words, the $G$-TC layer is injective over different group orbits. The authors claim that this property endows the $G$-TC layer with strong robustness, leading to resistance to invariance-based adversarial attacks.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is relatively well written and well organized. It is easy to read.

Weaknesses:
1. The proposed method is only applicable to ""regular"" G-CNNs, where feature maps are signals over a group. It is hard to see how this can be extended to steerable G-CNNs where feature maps can be arbitrary fields.
2. One particular disadvantage of ""regular"" G-CNN is its computational and memory burden -- one has to physically store a function over the group $G$, which, after discretization, could be very large. The feature dimension after the proposed $G$-TC layer increases from $|G|$ to $|G|^2$. Even if there are potential ways to reduce the computational cost of the $G$-TC layer as speculated by the authors, this memory cost can not be circumvented.
3. The authors have claimed that the ""completeness"" of the $G$-TC layer leads to robustness to invariance-based adversarial attacks. However, there is neither theoretical nor empirical evidence to verify this claim.
4. Also, I am not completely convinced why ""completeness"" is so important. $G$-pooling is typically adopted close to the end of the model, by which time ""coarse-graining"" of the feature is usually intentional.
5. Also, can completeness be achieved by simple registration of the feature maps (e.g., according to the max-magnitude function value over the group)? In this case, the signal can stay at the length of $|G|$.
6. The experimental set up of this paper is non-canonical. There are too many adjustment to the baseline (e.g., same filter size as the input, only one $G$-Conv block, etc.) To make the results more convincing, the authors should simply replace the last $G$-max pooling layer of a E(2)-CNN with the proposed $G$-TC layer. I understand the authors are not trying to achieve SOTA, but the results displayed in Table 1 are simply too unconvincing.

Limitations:
Yes.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a concept called G-triple-correlation (G-TC) layers, which are used in combination with G-equivariant convolutional layers to achieve G-invariant mapping functions. In contrast to conventional methods such as max or average G-poolings, which result in information loss, the proposed G-TC layers are completely identifiable: they selectively remove only the variations caused by group actions while preserving all the essential content information in images. The invariance and completeness of G-TC layers are proven under certain mild assumptions. The proposed G-TC layers are empirically validated for some representative examples, e.g., SO(2)-MNIST, O(2)-MNIST, SO(3)-ModelNet, and O(3)-ModelNet.



Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The paper has a clear motivation, i.e., replacing the excessive invariance (G-pooling) with more informative and precise approach (G-TC).

* The proposed method is technically sound. The assumptions used in this paper seems to be mild. The useful theoretical properties (G-invariance, completness, uniqueness) and computational complexity of the proposed method are nicely presented.

* The paper is generally well-written and easy-to-follow. Figure 1 is a nice overview for the motivation of this paper.

* The experimental results are convincing, especially for the cases of 3-dimensional groups.

Weaknesses:
- As the authors mentioned, the main bottleneck of the proposed G-TC method is its computational cost.  Although the paper includes a theoretical analysis on the time complexity of G-TC, it would be beneficial if the study also presented experimental comparisons of the training and inference computational costs.

Limitations:
Computational cost might be a bottleneck for the proposed G-TC method. This limitation is adequately addresed in the paper.

Rating:
6

Confidence:
3

";1
sXMQPKbLXf;"REVIEW 
Summary:
The paper proposes DiffPack, a torsional diffusion model that accurately predicts the conformation of protein side-chains given their backbones. DiffPack learns the joint distribution of side-chain torsional angles by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, the paper proposes autoregressively generating the four torsional angles and training diffusion models for each torsional angle. The method achieves remakrable improvements in angle accuracy on benchmark datasets and enhances side-chain predictions in the AlphaFold2 model. 



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
-The paper is well-written and easy to follow, with a clear description of the proposed DiffPack model. While the idea of using torsional diffusion is not entirely novel, the paper builds on the work of Jing et al. at NeurIPS 2022 on small molecule torsion diffusion and applies it to the problem of protein side-chain packing. 

-The paper's technical contributions lie in the development of a torsional diffusion model that considers the restrictions imposed by covalent bond lengths and angles, and the autoregressive generation of torsional angles. 

-The paper's results demonstrate the potential of DiffPack in advancing protein structure prediction and design. 



Weaknesses:
-Is there a metric available for assessing the overall conformational plausibility of the generated structures? This question is relevant because in the context of small molecule conformation generation, deep learning methods have been shown to produce conformations with lower overall RMSD, but there are still significant challenges with conformational plausibility (e.g. generated benzene rings are not necessarily planar). t would be beneficial to have a metric that takes into account both the geometric accuracy and conformational plausibility of the generated structures to ensure that they are reliable for downstream applications.

-It would be helpful to include a comparison of model run times between DiffPack and other methods such as Attenpack and force field-based methods. This would enable us to better assess the potential gaps in downstream applications when making proteomic predictions with these methods. 

Limitations:
The authors didn't discuss the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes DiffPack a torsional diffusion model to learn side chain placements. In particular, the authors presents a few modification to vanilla torsional diffusion models that improve the empirical results obtaining strong empirical performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors propose a number of modifications to the vanilla application of torsional diffusion to side chain predictions and these have enables them to obtain state-of-the-art performance on this important task.

Weaknesses:
Although the empirical results are very good, not many parts of the method are significantly novel. Moreover, among the few modifications that the authors made the presentation of them with respect to the rest of the field and the justification should be improved:

1. Annealed temperature sampling: the low temperature sampling procedure derived from the assumption of the Gaussian distribution seems to be very similar to the one presented in Ingraham et a. (2022) [20]. The authors should include the reference of this technique in the relevant section and clarify the difference between the methods. Moreover, I believe the statement “ideally the sampling process converges to the global optimum when T→0” is wrong or at least misleading (does not specify what the ideal situation is).

2. Multi-round sampling: this is related to the mixing Langevin steps that multiple papers have proposed before (e.g. Ingraham et a. [20]). However, unlike in those approaches, the authors do not limit themselves to a Langevin step that preserves the expected distribution but instead keep the “ODE-term” making therefore very unclear what the process is “theoretically” doing in this process. The authors should provide further discussion of these challenges.

3. Autoregressive diffusion: some of the comparisons (excluding the inference performance) used to motivate autoregressive diffusion versus vanilla are misleading: 
 - Figure 7: presents the number of steric clashes during the diffusion process not of the generated structures. This is misleading because these steric clashes may not matter or even help the model to detect them to avoid them. As the autoregressive method (that removes following atoms) artificially removes these steric clashes and this may prevent it to reason about them when generating the distributions. The number of steric clashes should be compared based in the resulting generated structures. 
 - Figure 5 compares the loss values of different methods however these are not comparable. E.g. one of the reason for the loss of X1 is lower than X4 might just be fact that the entropy of the distributions of X1 (which is correlated to the lower bound of the score matching loss) is lower than that of X4. Similarly this makes the comparisons of the losses of the different methods not comparable (e.g. the entropy of the conditional distribution obtained by the autoregressive is lower than that of the unconditional obtained by the joint loss).

Limitations:
In some of the tables the (e.g. Table 3) the bolding of the best number is wrong and biased to the proposed method.

I believe that the discussion on the number of parameters is misleading as this is not a very useful measure of computational feasibility. Instead the authors should compare the methods by runtime (and potentially memory cost).

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on the task of sidechain packing in proteins, where one wishes to predict the positions of the side-chain atoms given the positions of the backbone atoms. To this end, the main contribution of the paper is DiffPack, an extension of recently proposed torsional diffusion models to the side-chain packing task. This is achieved by combining three aspects:

i) autoregressive sampling of the four side-chain torsion angles, with a separate diffusion model trained for each, conditioned on the previous torsion angles

ii) multi-round and annealed temperature sampling to improve quality of generated samples

iii) Modifications to the transition kernel formulation on the torus for residues where the torsion angles have a periodicity of $\pi$. 

Experimental performance compared to various baselines for side-chain sampling showcases the improved performance offered by DiffPack.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is very well written, clear and easy to understand. The technical choices made throughout the paper are sound and well motivated, and the application area considered is well suited for the torsional diffusion framework. 

2. The authors compare their method to a variety of baselines for side-chain packing and showcase improved experimental performance of their method.

Weaknesses:
1. The technical contributions are largely incremental - the formulations regarding torsional diffusion have already been well explored in previous recent papers. 

2. Some questions / clarifications regarding the experimental evaluation:

    * It is unclear to me the benefits offered by using auto-regressive models for each torsion angle as opposed to joint diffusion. From Table 1, 2 and 4, one can see that, for about 4$\textdegree$ variation in MAE of $\chi_1$, the RMSD drops by 0.1. From Table 4, the $\chi_1$ MAE between the autoregressive model and the joint diffusion model is about 2$\textdegree$, giving a much smaller corresponding drop in RMSD. Using auto-regressive models definitely seems to accelerate training / convergence as noted in Fig 5. However, the errors in discretization during sampling, idealization of bond lengths and angles when reconstructing coordinates could end up reducing the improved training performance of the autoregressive models, and eventually offer similar values in RMSD. 

    * Could the authors add a table regarding the run times associated with the evaluations of the different deep learning baselines? How many samples are generated for each protein before selection with the confidence model?

3. An anonymous link for the code submission is not provided, making it harder to verify reproducibility.

Limitations:
The authors have addressed limitations associated with their work

Rating:
5

Confidence:
3

REVIEW 
Summary:
The protein side-chain packing problem consists of predicting the positions of atoms in amino-acid side chains given the backbone structure and residue identities. The paper proposes to do this using a diffusion model that accounts for physical constraints and models side chain structures as joint distributions over torsional angles. Furthermore, they find that generating the torsional angles autoregressively improves the generation quality. DiffPack outperforms existing methods with fewer model parameters and can enhance side chain predictions from AlphaFold2. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper provides a unique solution to the important and well-studied protein side-chain packing problem. The writing is generally clear, and the work is well-contextualized in the literature. The autoregressive diffusion framework is intuitively effective, and the formulation seems correct. This is backed up by strong empirical results when comparing to previous work and in the ablations. Providing confidence scores, reducing the number of parameters compared to previous models, and being able to refine AlphaFold2 predictions will make the method very useful for downstream practitioners. 

Weaknesses:
In general, this is a strong submission with few major weaknesses. 

### Major

- Clarity: it's a bit unclear to me how the model moves between atomic coordinates in GearNet and predicting the scores and confidences on each angle. A few more lines of text or a subfigure could be very helpful here. Likewise, it would be nice to have some more details about model size, training hyperparameters, and training hardware. 
- Soundness: The paper talks about confidence scores and shows that they improve generations. It would be better to also have a table or figure showing how well the confidence scores are calibrated. 
- Soundness: are the baselines also given the chance to generate multiple conformations and then to have a confidence model pick the best one? If not, the comparisons are not quite one-to-one. 
- Significance: While I appreciate the case studies shown in 5.5, the paper would be stronger with more context here. What downstream biological or engineering applications, if any, can DiffPack do that are not accessible with existing methods?

### Minor

- There are a few minor points on clarity: The standard in the field seems to be DDPM (denoising diffusion probabilistic models) instead of DPM, as used in line 252. In Figure 4, the legend says blue and yellow, but I see blue and red in the figure. In Figure 5, the colors for the four autoregressive curves are very difficult to tell apart -- it might help to also vary the linestyle. 

Limitations:
- The authors should address where DiffDock still does not solve the side chain packing problem, either in general or in specific cases. 

Rating:
7

Confidence:
4

";1
c9fXCzR5fK;"REVIEW 
Summary:
This paper proposes a novel dataset distillation method called SeqMatch which focuses on extracting high-level features from later training trajectories. The authors highlight a limitation in state-of-the-art data distillation methods, which tend to condense low-level information from easy data while overlooking the high-level information contained in hard data. In response to this issue, the paper introduces a novel optimization technique that generates multiple small sets of synthetic data. Each of these sets distills distinct knowledge from various stages of the training trajectories. By addressing the inherent problem observed in previous dataset distillation methods, the authors conduct experiments to showcase the efficacy of SeqMatch.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1.	The idea of condensing different sub-datasets for different stages is novel and interesting.
2.	The proposed method does not require extra computation cost.
3.	The writing is good and easy to follow.


Weaknesses:
-	The paper lacks evaluation numbers for certain settings, such as SeqMatch-IDC, on datasets like CFAIR100 50IPC, Tiny-ImageNet, and ImageNet subset. (SeqMatch-IDC seems to be the most favorable setting) This omission makes it difficult to determine the performance of SeqMatch in comparison to other methods in these specific scenarios.

-	SeqMatch underperforms the baseline method FTD on Tiny-ImageNet and ImageNet subset, suggesting that SeqMatch may not scale well to larger datasets. This raises the question of why SeqMatch was chosen over FTD in these cases. 

-	The paper lacks an ablation study, which would provide valuable insights into the impact of the number of subsets, K, on the performance of SeqMatch. Including such an evaluation would enhance our understanding of how SeqMatch operates and how different parameter settings influence its performance. 

-	The caption of Table 1 is somewhat misleading. Although IDC is not categorized as a factorization-based method, it does employ data parameterization (factorization can be treated as a special data parameterization). Therefore, it would be more appropriate to compare IDC with RTP and HaBa rather than other methods distilling information into a single image.

-	Minor: There are citation inconsistencies in the appendix. For example, FTD is referred to as [12] in the appendix but as [11] in the main text. Additionally, the appendix lacks a reference section, making it difficult to trace the sources of the cited works accurately.

- The reproductivity checklist is chosen as Yes, but no code is provided.


Limitations:
Yes, the authors have discussed the limitations.



Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes a change to existing dataset distillation methods by sequentially optimizing different subsets at a time. At each iteration, the existing subset is frozen and a new subset of data is *added* to it and optimized. This method allows different subsets of the synthetic data to capture different levels of features required by a network to learn during training time. This method boosts the state of the art for subsets with >1 IPC (since this method does not make sense with 1 IPC).

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I like this paper a lot. The authors addressed an obvious problem with existing dataset algorithms (all samples capture the same level of features) in an elegant way that is adaptable to all existing and future backbone methods. The authors both empirically and theoretically show that jointly optimizing the entire set couples the gradients in a way that prevents the synthetic set from learning the necessary variety of features.

The visuals are all very nice, clearly illustrating the authors' points.

Weaknesses:
Algorithm 1 is a bit confusing to read. According to line 4, it seems that a single network initialization is used to optimize the entire subset (but this can't be true since it would catastrophically overfit). It is also unclear what the $n$ parameter is. As the algorithm is currently presented, I cannot see how MTT can be slotted into it.

Maybe it would be clearer if an additional inner loop was included along with a generic distillation loss and doing away with the sum over $m$?

Limitations:
yes

Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper propose a new method called sequential subset matching (SeqMatch) for dataset distillation. The proposed method is designed to continuously generate synthetic images at different training (distillation) iteration. This strategy is inspired by the general mechanism of optimization, which captures characteristics (low-level feature) of easy instances in an early stage, but takes characteristics (i.e., high-level feature) from increasingly difficult instances. SeqMatch was applied to various dataset distillation methods and showed marginal but better performance than the method in which SeqMatch was applied or other baseline methods in various four datasets.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
1. The analysis about the general mechanism of optimization is so insightful that it deserves a lot of attention in other studies.

2. The plots of several figures provide good support for the arguments in this paper. e.g., Figures 1 and 2 well illustrate the effect of the general mechanism of optimization on dataset distillation and the coupling issue, respectively.

Weaknesses:
First of all, I don't understand the motivation behind designing some SeqMatch.

1. How was the claim for the optimization mechanism that captures low-level features in the early stages and high-level features in the later stages verified? Figure 1 seems to have been used to verify this claim, but can hard instances and easy instances represent high-level and low-level features, respectively?

2. I don't understand how the analysis of the general mechanism of optimization was used to design the SeqMatch method. In particular, what is the motivation for applying the SeqMatch method when training f_theta in the evaluation phase?


In addition, in Table 1, most of the performance improvements acquired via SeqMatch are very marginal (~0.2). Performance improvement is not an absolute determining factor in judging the superiority of the proposed method, but it can be used to verify that the method works as claimed in this paper.

Limitations:
This paper adequately addressed the limitation and promised to solve it in future work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper investigates an issue with dataset distillation, where synthesized datasets tend to overly condense low-level features but fail to efficiently incorporate high-level ones. The authors argue that this is due to existing methods treating the synthetic dataset as a unified entity and equally optimizing each instance, leading to a coupling issue as the size of the synthetic dataset increases. To address this problem, they propose a new dataset distillation strategy called Sequential Subset Matching (SeqMatch). SeqMatch divides the synthetic dataset into multiple subsets and optimizes them in sequence, mimicking the learning process from low-level to high-level features. This approach allows each subset of the synthetic dataset to progressively capture more complex, high-level features, reducing the coupling issue and enhancing overall performance.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
This paper is well-written, well-motivated, and well-organized. The authors provide comprehensive experiments on various datasets such as CIFAR-10, CIFAR-100, TinyImageNet, and subsets of the ImageNet. They provide insightful analysis of the experimental results, discussing the impact of different factors. The authors provide a detailed algorithm description that translates their theoretical insights into practical application.

Weaknesses:
1. It's not clear how generalizable these results are to other tasks and datasets. Their experiments are based on specific DNN architectures and datasets, and the proposed method's effectiveness might vary under different conditions.
2. Since the method divides the synthetic dataset into subsets and optimizes them sequentially, there might be a risk of overfitting, especially when the number of subsets is high or when subsets are small.
3. The method proposed requires a series of sequential optimization processes, which could potentially increase the computational cost and time required for training.

Limitations:
In section 5.4, the authors discussed two limitations of their work. Their openness in acknowledging these limitations adds to the credibility of their research and provides useful guidance for follow-up studies in this field.

Rating:
5

Confidence:
4

";1
DvRTU1whxF;"REVIEW 
Summary:
The authors make two claimed contributions:
1. They propose an architecture for an encoding model. This architecture consists of three key components.
* A frozen DINO trained ViT-B backbone + learnable convolutional layers on top of ViT feature maps
* A differentiable spatial sampling layer (implemented via pytorch grid_sample, similar to [1]), where the 2D coordinates are predicted from 3D voxel coordinates
* A differentiable softmax based layer selector
2. They propose an ""All-for-One"" training recipe. Which incorporates the following:
* ""dark knowledge distillation"" (typically referred to as knowledge distillation or network distillation in most other machine learning works), where they use ROI specific networks to train larger networks
* They propose a new parcellation across brains which they call ""veROIs"", which is extracted via k-means clustering of voxel weights.

To validate this method, the authors visualize the learned spatial sampling grids and how the preferred layer varies across voxels. The authors further perform an ablation study of the encoder, and perform image retrieval using their network.

The authors provide an illustration of how the sampling grid evolves during training in the supplementary.

[1] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is novel in the combination of techniques, which uses online-learned sampling grids with soft backbone layer assignment to predict the brain activations, this design is reasonably biologically motivated.

The authors proposal of clustering based ROI assignment is also novel in the context of decoding (image retrieval).

The paper provides definitions and dimensions for most variables where appropriate, and the figures are illustrative.

My view is that the proposed decoder architecture is moderately novel in the context of encoding models. There is insufficient information (lack of detail) to judge the all-for-one training. 

Weaknesses:
The paper is interesting, however my key concern lies in the lack of details for the all-for-one training scheme, and the lack of evaluation in comparing against simple linear-regression + backbones (ResNet, CLIP ViT [1])
* On the clarity of the paper
  * The high level clarity of the paper regarding the all-for-one training is poor and could use significant improvement. 
  * It is not clear how exactly the veROIs are used. Is the encoding model ultimately at the voxel-level? If so, are the veROIs used for the network distillation and image retrieval steps? Do you share the backbone and just use independent linear weights for the voxels?
  * It is not clear which subjects you use for Stage 1/2/3. Do you train with one subject's ROI? Or do you train on all subjects' single ROI (using stage 1 as an example). If so, do you take veROI-1 for example, then train a model on all subjects from all modalities for veROI-1?
  * It is not clear how you get the all-ROI model to derive the veROIs. Is this model trained on all subjects and all modalities? Do you train just the last linear weights? Or you use the DINO features? Do you train the conv weights?
  * It is not clear how you train across subjects and datasets where the number of voxels are not the same. This is a central claim in your paper.

* On motivation
   * Currently the justification for repeated distillation is weak. Your experiments show that distillation does help from a performance standpoint, however your motivation differs from the typical use of distillation (which is to accelerate inference using a student model). Could you better motivate this? 

* On prior work
  * For the topic of brain encoding models that utilize differentiable spatial sampling, I recommend citing [2,3] which are similarly brain motivated, as well as [4] which is one of the more significant papers that uses differentiable sampling.

* On the soundness of the baseline
  * For the ""FrozenRM"" encoder, you mention that every voxel is mapped to the center. Since you use DINO based on ViT, there is an extra classifier token (usually the first token). Typically when using ViT based architectures for spatially invariant tasks, this is the embedding you use. I recommend modifying the FrozenRM baseline, or adding an additional baseline when you use this token
  * The same criticism applies to the GlobalPool token, I recommend adding/replacing a baseline where the global pooled representation is replaced with the classifier token

* On the lack of baselines
  * Currently the authors perform ablation studies, but do not perform qualitative or quantitive comparisons against other works [1]. The paper would be strengthened by adding comparisons to linear-regression based single subject voxel-wise encoding models based on different architectures (global pooled ResNet features , VGG, Gabor features, GIST features) trained on ImageNet, or different objectives (CLIP/OpenCLIP/EVA-CLIP, DINO, any of the SSL/Masking work) adapted to a single subject. I don't expect their multi-subject network to necessarily perform better (nor would it be a negative if they perform worse), but some evaluations are still necessary. 

* Minor
  * Figure 4 was quite confusing to me. In the retinagrid case, the colors indicate spatial extent. However in the retinamap, the colors indicate layer selection. I would ask that you provide additional clarity here.
  * The notation in Table 3 is quite confusing, you do not specify what veROIsX is. It is implied that it corresponds to what stage of network distillation you use. Please clarify this. 
  * Line 199 ""regulirazation"", minor misspelling. 

On balance, the paper is interesting from an architectural standpoint. I would be happy to take another look at the paper if the authors can clarify their training scheme and add additional baselines. 

[1]  Conwell, Colin, et al. ""What can 5.17 billion regression fits tell us about artificial models of the human visual system?."" SVRHM 2021 Workshop@ NeurIPS. 2021.

[2] Mahner, Florian, et al. ""Learning Cortical Magnification with Brain-Optimized Convolutional Neural Networks."" Conference on Cognitive Computational Neuroscience. 2022.

[3] Jun, Na Young, Greg Field, and John Pearson. ""Efficient coding, channel capacity, and the emergence of retinal mosaics."" Advances in neural information processing systems 35 (2022): 32311-32324.

[4] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Limitations:
The authors adequately address the limitations of their model.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper develops an all-for-one training model to address the challenge of one big-model problem by converting it into multiple small models, in which the small models aggregate the knowledge while preserving the distinction between the different functional regions. With the proposed method, biological knowledge of the brain, particularly retinotopy, is used to introduce inductive bias into a 3D brain-to-image mapping that ensures a) each neuron knows which regions and semantic levels to gather information, and b) no neurons are left out. Overall, it is an interesting paper, however, there are several concerns about the machine learning novelty, validating the empirical studies, and the clear presentation of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Please refer to the question section


Weaknesses:
Please refer to the question section


Limitations:
Please refer to the question section


Rating:
4

Confidence:
4

REVIEW 
Summary:
The manuscript addresses the challenge of generating brain encoding models (specifically for visual stimuli) - which seeks to predict brain responses at the voxel level to visual stimuli. A challenge facing brain encoding is heterogeneity in data modality, individual variability, and functional differences across brain region. Existing models often address the problem of heterogeneity by fitting separate models for different brain regions. However, the authors seek to to fit a single model that encompasses the entire visual brain by leveraging a dark knowledge distillation method in which each ROI distills the dark knowledge present in the other ROIs.  The authors evaluate their method on a variety of functional imaging datasets spanning fMRI, MEG, and EEG.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Originality: The authors developed a novel training pipeline utilizing dark knowledge distillation in order to allow ROIs to collaborate during training. The authors also develop a new method for incorporating retinotopy, a biologically realistic feature, into their encoding model.

Quality: The model seems to work across different data modalities (e.g. EEG, MEG, and fMRI), making it broadly applicable.

Clarity: The manuscript provides several ablation studies that investigate the limitations of the approach and which features of the architecture are important for improvement.

Significance: The study introduces a new approach for creating visual brain encoding models from functional imaging data. Their approach could be employed by other groups studying non-visual stimuli as well.

Weaknesses:
Unless I am mistaken, the authors do not evaluate their model against the state of the art in  computational speed, making it hard to evaluate whether their model represents a significant improvement, as the improvement in correlation appears to be modest. Furthermore, the message of the paper is a little unclear - what is the main breakthrough the authors are trying to present: the all-for-one training, retinotopy, or ability to work with multiple data modalities. Evaluation against state of the art should occur for each of these topics. Finally, the model is trained on data across fMRI, MEG, and EEG modalities, but the held out data consists only of fMRI data, making evaluation of model generalizability difficult.

Limitations:
Limitations are addressed except for points raised earlier about comparison to state of the art.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The authors tackle the brain encoding task, which predicts brain voxel-level responses to image stimuli. The authors aim to train a comprehensive brain encoding model using the vast amount of public data from diverse imaging modalities and numerous participants. The proposed method, the All-for-One training recipe, divides the one-big-model to multiple small models and aggregates the knowledge together in inference time. An intriguing technique the authors propose is to use retinotopy to introduce inductive bias to learn the mapping.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The figure quality (especially aesthetics) is not often seen in NeurIPS submissions. Take a look at Figure 2, 4, 5 and 6. Those figures are on par with Nature family submissions. 
2. Decent ablation studies.

Weaknesses:
1. The design choice is not the most straightforward to process. I have some trouble understanding the logic behind the three-staged design of the proposed All-for-One recipe — specifically, why is it necessary to have these 3 stages?
2. RetinaGrid and RetinaMap seems a bit far-fetched. By far, my impression is that the authors are simply trying to draw an analogy from image formation process in the retina and provide a fancy visualization. The authors are welcome to defend with explanations.
3. Lack of other alternative baselines.

Limitations:
Nothing that I am aware of.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on the task of brain encoding model, which aims to predict brain voxel-wise responses to stimulus images. First, the paper proposes the All-for-One (AFO) training recipe, which enhances interactions among multiple ROI models to handle the large diversity within the data. Second, the paper introduces the RetinaMapper to learn a 3D brain-to-image mapping and the LayerSelector to selectively merge features from multiple layers. Finally, the paper trains the model on a very large scale of data and demonstrates the effectiveness of the model through extensive qualitative and quantitative analysis.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The overall motivation is clear, and the techniques used are straightforward and reasonable. For example, enforcing cross-model interaction/distillation is a reasonable way to enhance each model, and selectively merging information from multiple layers with different receptive field sizes is sensible.

2. The experiments are extensive, and adequate qualitative and quantitative analysis is provided. For example, Table 2 demonstrates the effectiveness of TopyNeck, and both Figure 4 and Figure 5 show the effectiveness of the LayerSelector.

3. The work pre-trains the model in large-scale data, which is impressive.

Weaknesses:
1. need to provide more details and insights into specific techniques
2. missing more explanations/references
3. some writing issues

Limitations:
The authors have addressed the limitations

Rating:
6

Confidence:
2

";0
6EDHfVHicP;"REVIEW 
Summary:
The paper presents DDF-HO, a method for handheld object reconstruction based on Directed Distance Fields. Given a single RGB image containing a hand grasping an arbitrary object, DDF-HO reconstructs the object without requiring a template or depth priors. Previous methods addressing this problem have relied on the Signed Distance Field (SDF) for the same purpose. The paper includes experiments comparing DDF-HO with such methods on multiple datasets demonstrating the advantages of the proposed method in terms of accuracy.


**Final Rating**

After reading the numerous reviews and the responses of the authors, I believe that this is a paper that can/should be accepted. As I wrote in my previous post, thorough rewriting is needed in some parts to improve clarity.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. DDF-HO obtains 3D reconstructions of much higher quality than recent methods [25, 32, 62]. The paper provides sufficient evidence that this is due to the use of DDF instead of SDF as the representation, the use of a ray-based feature aggregation scheme, and a 3D intersection-aware hand pose embedding. These contributions lead to significant increases in reconstruction accuracy.

S2. The method is described clearly and with sufficient detail to enable reasonable reproduction. The code is also included, but I did not try to work with it, or identify the key pieces. Handling of symmetry is the exception to this comment.

S3. Three large-scale, widely used datasets, one synthetic and two real, are used to generate the experimental results. The protocol follows that of IHOI [62], which is the most closely related prior work. Two additional recent baseline methods have been chosen for the experiments.

DDF-HO outperforms the baselines on all three datasets. It also shows good zero-shot generalization properties, suggesting that it does not overfit. The experimental results section includes some analysis of potential reasons for the differences in performance across algorithms. The ablation studies are also informative, especially the comparisons to IHOI.

Weaknesses:
W1. My primary concern about the paper is the number and complexity of steps. DDF-HO requires sampling rays, computing and aggregating 2D and 3D features, measuring geodesic distances etc. The authors acknowledge that their method has higher complexity than the baselines in the limitations section, but I would like to see more analysis and data on the tradeoffs between speed and accuracy. How long do training and inference take for IHOI and DDF-HO on similar hardware and data? Is interactive deployment feasible?

W2. Handling of symmetric objects is not presented clearly overall. How is the reflective plane of symmetric objects discovered?


Minor Comments (not affecting my recommendation)

I find the use of “undirected” as a property of SDF confusing. The sign and the distance value of SDF direct us to the nearest surface. I do not have a good suggestion of an alternative.

Small language errors can be found throughout the paper. Examples include missing articles and minor inconsistencies.

98: “Arbitrarily” is more appropriate than “randomly.”

Figure 4 and Tables 1 and 2 can be placed closer to the text that refers to them.

Refereed, rather than arXiv, versions of papers should be cited whenever possible.

The first paragraph of Section 3 of the supplement is very important for understanding the algorithm, in my opinion. I suggest finding some space for it in the main paper. One of the weaknesses I had noted was lack of clarity on the ray sampling process.

Limitations:
More information on complexity and run times would have been useful.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work proposes a novel pipeline that uses DDF as the shape representation for hand-held object reconstruction from a single image. DDFs provide benefits over SDFs, eg. they are directed, provide intersection with object information, and can capture symmetry. Extensive experiments on ObMan, HO3D, and MOW datasets show the effectiveness of the proposed approach over existing methods.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This work proposes to use DDF, which is more expressive than SDF, for hand-held object reconstruction. While SDFs are undirected and cannot capture symmetry, DDFs provide a directed distance to the surface along with intersection with object information (visibility) and can capture symmetry.
- The proposed approach uses ray-based feature aggregation and intersection-aware hand features to better capture hand-object interaction compared to existing SDF-based methods.
- Extensive experiments on ObMan (Table 3), HO3D (Table 1) and MOW (Table 2) datasets show the effectiveness of the proposed approach over existing methods.
- Ablation studies on different components (Table 4) and robustness to noise in hand pose (Table 5) are helpful in understanding the capabilities of the proposed approach. Also, error bars over 5 training seeds are provided in the supplementary.

Weaknesses:
- The HO3D splits used are different than IHOI[62]. Is there any reason for this?
- The IHOI[62] scores on MOW (Table 2) are different than those reported in IHOI paper, even though the same splits are used (L221-222). Why is this the case?
- It'd be useful to have ablations on the different ray sampling strategies used during training, as stated in Sec. 3 in the supplementary. Specifically, how well does uniform sampling perform by itself? This could be a limitation when scaling DDF-HO to cases where 3D ground truth object models are not available (eg. in-the-wild settings).
- How does the training & inference time for DDF-HO compare to IHOI? Since several rays need to be sampled, it seems that DDF-HO would be slower than IHOI.
- It'd be helpful to include more details about ray sampling during testing in the main paper, eg. how many rays, how are the origin & directions sampled.

Limitations:
Limitations are discussed.

---
I appreciate the additional ablations and clarifications provided by the authors. After reading the rebuttal, other reviews, and discussion, I think that the authors have addressed the main concerns pointed out in the reviews. So, I am retaining my rating of `Weak Accept`.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a directed distance field-based method for hand-held object reconstruction from a single RGB image. The paper aggregates 2D ray-based features to capture ray-object intersection and 3D geometric features of ray-hand intersection. In particular, it extracts local to global cues via the above features and introduces a symmetry loss term to handle symmetric objects. Experiments on three datasets and ablations on the introduced modules show the effectiveness of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed idea of extracting local 2D image features along the ray and local 3D features from the ray-hand relationship to provide geometric cues is novel in hand-held object reconstruction.

2. Supporting experiments show that DDF is a more suitable representation than SDF for handheld object reconstruction.

Weaknesses:
1. Unclear descriptions. It's better to provide a clearer description of how to sample the bijection sets on the reflective plane for symmetry loss, preferably with an illustration.

2. Insufficient qualitative results. It's suggested to provide qualitative results of the ablation studies, especially the ablation on symmetry loss term. Besides, it should be declared that whether adding such a symmetry loss term leads to a wrong distance field for asymmetric objects.

3. Insufficient ablations on input hand poses. It would be better to add Gaussian noises at a larger scale (e.g., \sigma=1.0,1.5) to the input hand poses.

4. Insufficient ablations on K_l and K_{3D}. Why K_l=8 is set to sample points along the projected 2D ray. And why K_{3D}=8 (nearly half of hand joints) is set to select the neighboring hand joints, since considering fewer hand joints seems to be more efficient for capturing local ray-hand features.

Limitations:
The paper has well clarified the limitations of the proposed methods, which are inherited from the shortcomings of DDF.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors introduce a novel approach called DDF-HO, which uses Directed Distance Field (DDF) for 3D hand-held object reconstruction. Unlike SDF, DDF includes origins and directions of the views in the 3D space. They show that the ray-based feature aggregation scheme and 3D intersection-aware hand pose embeddings are more suitable for 3D hand-held object reconstruction. The DDF-HO method outperforms prior work.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Proposed a new data structure DDF for hand-held object reconstruction.
2. Ray-Based Feature Aggregation technique has better representations of local geometric features.
3. The interaction modeling reflects the interaction between the hand and object rather than just hand as in the prior work.
4. Significant improvements over prior work.
5. Comprehensive evaluation and ablation studies.

Weaknesses:
1. Lack of efficiency comparison between SDF-based methods and proposed DDF. Such as the amount of resources required for reconstructing the same input.
2. No appearance on the reconstruction results

Limitations:
1, As the authors mentioned, it is hard to train DDF on higher dimensional inputs.
2. As the authors mentioned, the reconstructions do not reflect the translucency, material, and appearance of the objects
3. The reconstruction quality is not good, especially on real-world images

Rating:
8

Confidence:
2

REVIEW 
Summary:
The paper proposes an algorithm for reconstructing hand-held object from a single RGB image. Instead of using the traditional Signed Distance Fields (SDF), this paper proposes to leverage Directed Distance Field (DDF) as the shape representation. Experiment shows that the proposed algorithm outperforms SOTA.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed pipeline utilizes DDF as the shape representation to reconstruct hand-held objects, which has a stronger modelling capability for this specific task, e.g., reconstruction for hand-held objects from a single RGB image. It introduce a 2D ray-base feature aggregation and 3D intersection-aware hand pose embedding.
The experiments are conducted on both real and synthetic dataset,

Weaknesses:
The paper did not mention the running speed, model complexity, I wonder if it's comparable with SDF based models.
The paper mentioned that the DDF is harder to train, and requires more complex data, algorithm, and network structure, I wonder if the paper can give more quantitative measurement? e.g. 1 or 2 magnitude harder/longer/more parameters?

Another alternative to SDF will be the Occupancy, the paper did not mention Occupancy at all. Won't Occupancy be a strong baseline model? or replacing SDF with Occupancy will make the algorithm (proposed and SOTA) perform better?

Limitations:
did not talk about potential negative societal impact

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper presents a system for joint hand and hand-held object 3D reconstruction. The authors propose a pipeline to (1) predict the hand (MANO hand model) and camera poses with an off-the-shelf pose estimator; (2) extract image features with an off-the-shelf ResNet; (3) sample ""3D ray representations"", project them to the 2D image plane, and extract the corresponding image features and aggregating them with cross-attention along the ray directions; (4) for the same 3D points, extract 3D features from the global hand embedding from the geodesic nearest joint; (5) predict a directed distance function (DDF) with an MLP, using the above features as input. Experiments show that the proposed system predicts more accurate shapes.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The proposed method is evaluated on synthetic and real hand-object interaction datasets, showing improvements over baselines on nearly all metrics. Ablative analyses are also provided to better understand the behaviors of the proposed DDF-HO.

Weaknesses:
- The presentation is poor. Specifically, it is unclear what the use of ray sampling is, and how the random ray directions transform to the directions pointing to the target hand (R-A to R-B in Fig 2). Also, it is unclear how one would know a sampled ray would (not) point to the direction closest to the hand shape, without the DDF even being optimized. In addition, it seems that the major contribution of the paper is a system proposal as a whole, rather than the choice of representation (DDF vs. SDF). I think the message of the paper is somewhat misleading in this sense.
- Why is the final output in the form of DDF, if the end goal is to extract the surface of the hand shape? Predicting an SDF or occupancy field would serve the same purpose as well. Also, it is not clear how DDF is being taken advantage of in the proposed system. The evaluation metric is based on surface point cloud representations as well.
- It is not clear to me how the proposed method is better in the real-world datasets. Visually, they seem to be very far from good predictions of the object shapes.
- It would be good to analyze failure cases to better understand the limitations of the proposed system.

Limitations:
Yes

Rating:
3

Confidence:
3

";1
sJDkwMVqb9;"REVIEW 
Summary:
This paper addresses the bias between internal links and cross-links by augmenting cross-links and combining two models consisting of the original and debiased models. Specifically, the authors show that the number of cross-links is fewer than internal links in three real-world datasets. Thus, with Jaccard coefficient score, they augment cross-links and train a debiased model on the augmented graphs. To resolve the trade-off between utility and fairness, they fuse the representation of the original model (which is trained on the original graph), and the debiased model. In experiments, they show that the proposed method resolves the bias and even improves the overall performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- In contrast with the existing methods, the authors mainly target the bias based on graph topology.
- The suggested method mitigates the bias without compromising performance on several datasets under various architectures.

Weaknesses:
- It seems insufficient to demonstrate that internal-links are more common than cross-links based solely on three datasets. For heterophilous graphs, cross-links would be more prevalent when each node label is regarded as a community. In this case, it is more reasonable to augment internal-links, but the suggested method is not able to do it.
- The rationale for supervision augmentation, which is the core component to resolve the bias, seems weak. Why is the Jaccard coefficient score better than other options? Due to its formulation, it would be difficult to augment the connections between a node with a high degree and a node with a low degree. However, using an edge predictor does not have this limitation and is a more simple approach. (Specifically, train an edge predictor and predict edges with this predictor. Then, choose edges based on the confidence of the predictor.)
- The performance of models without fusion is inferior to base models in Table 3. Since supervision augmentation is the core component to mitigate the bias, this component is more important than other components, but it is weak. Based on this observation, it is probable that using UGE as a debiased model would show better performance than the current approach. (Slight tuning is needed to combine UGE.) Then, the contribution of this paper would be marginal.
- Comparing baselines only under GraphSAGE seems insufficient. Could you compare the proposed method with UGE under LightGCN and GAT, too?
- The datasets used in this paper are quite different from the baselines such as FairAdj and UGE. UGE uses  Pokec-z, Pokec-n, and MovieLens-1M, while FairAdj utilizes Oklahoma97, UNC28, Facebook#1684, Cora, Citeseer, and Pubmed. Are there any reasons to evaluate the methods on different datasets? According to FairAdj, Cora, Citeseer, and Pubmed have more internal-links than cross-links. Thus, the performance superiority on these graphs can further support the effectiveness of the proposed method.
- The connections between sentences in the abstract seem unnatural.

Minor issues

- In line 24, “research concerns” seems unnatural. I recommend “research interests”.
- In line 394, “Epilogue” seems non-academic expression. I recommend “Conclusion”.

Limitations:
The authors provide several limitations in the main paper. However, as I mentioned in Weakness, it would be better additionally to address other issues such as the applicability to heterophilous graphs.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work finds that current GNN methods have severe data bias because GNNs like to connect new links inside the local neighbors and ignore the distant ones. To address this problem, the authors investigate the bias across different communities and propose a general framework. In this framework, the authors devise three key components, including supervision augmentation, twins-GNN, and embedding fusion module. To display the effectiveness of either component, the authors perform the ablation study, and with all of them, the Debias model has a promotion on three datasets compared with six GNN backbones.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
+ The idea of rethinking the data bias, especially the fact that existing GNN models tend to connect local neighbors, is novel and has basic value.
+ The writing is clear and easy to follow. 
+ The framework has a strong generality that can be applied to most GNNs (six examples used in this paper). 
+ The framework is an end-to-end framework and is easy to accomplish.


Weaknesses:
- The clusters should be pre-computed by some community detection algorithms (Louvain algorithm in this main content and METIS algorithm in the appendix); however, the impact of the quality of the community detection is unknown, and the results vary greatly under different community detection algorithms.
- The promotion of link prediction on DBLP and LastFM seems not to be apparent, especially the bias is still very high. (This phenomenon is referred to by the authors in the limitation part.)


Limitations:
Yes. The authors acknowledge that the data bias might not be the mere reason, and the supervision augmentation lacks theoretical analysis.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors aim to explore the issue of bias in the link prediction task for GNNs. Specifically, they develop methods to mitigate the bias resulting from graph topology - on internal links versus cross-community links. Their work relies on debiasing node embeddings and a fusion component that retains aspects of both the original and the debiased node embeddings. The main goal is to ensure that the implicit creation of information silos does not degrade link prediction performance. The overall architecture borrow from retrieval model literature and is designed to be partially agnostic to model choice.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The problem is significant enough in that enough research has been devoted to it in earlier literature, and that topology-induced bias is an interesting direction to consider.

2. The authors experiment on a number of baselines to show the relative superiority of their method. They also include a number of ablation studies to show the effectiveness of their architecture.


3. The architecture is model agnostic and allows for plugging in more powerful GNN models, for example.


4. The loss function of the link prediction objective does not have to be modified (supported by a regularizer) and so the loss surface is not directly affected. Instead, supervised augmentation provides a kind of regularizing effect.


Weaknesses:
1. The main weakness of the paper is the small variety of datasets that the experiments have been run on. Ideally there should be multiple kinds of graphs, varying by size (nodes, edges), or even types of communities, their strength or internal cohesiveness and the degree to which they overlap. In comparison, the number of baselines is acceptable. The authors could add more graphs, for e.g. from the SNAP repository and experiment on more graph parameters like the above. Further, synthetic datasets generated by a particular model could help serve as a baseline and also possibly study the evolution of such cross-community bias in social networks.

3. While community detection is done mainly via the Louvain algorithm, one could consider clustering based on node features and other methods such as stochastic block model as another baseline.


Limitations:
The authors address the fact that bias is not entirely eliminated by their procedure and that theoretical support does not yet exist for their contribution. The work has a positive societal impact as the key goal is to reduce bias.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a twin-structure framework for mitigating bias in link prediction methods based on Graph Neural Networks. Current link prediction approaches often prioritize performance without considering biases on sensitive attributes of nodes, leading to social risks and information cocoons. The proposed framework divides the graph into communities, distinguishing internal-links from cross-links, and employs supervision augmentation to increase signals for cross-links, generating debiased node embeddings. An embedding fusion module preserves the performance of internal-links while alleviating bias between them and cross-links. Experimental results on real-world datasets demonstrate the framework's effectiveness in reducing bias and improving overall link prediction performance compared to state-of-the-art baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The framework author propose can achieve good improvement on a lot of GNNs.
* The logic of this paper is easy to understand.
* The paper conduct experiments on large datasets.

Weaknesses:
* The paper doesn't mention subgraph-based GNN for link prediction.
* The algorithm is not so clear about how to inference.

Limitations:
yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper addresses the issue of bias in GNN link prediction and proposes a twin-structure framework to mitigate the bias and improve performance. The framework includes an embedding fusion module and a debias module, which work together to reduce the bias between cross-links and internal-links without hurting overall performance. Experiments on three datasets with six different GNNs show that the proposed framework can both alleviate the bias and boost the overall GNN performance.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper addresses an important issue of bias in GNN-based link prediction and proposes a novel framework to mitigate the bias.
- The experiment results show that the proposed framework almost always provides both debias and performance gain, even on different GNNs.
- The twin-structure and embedding fusion is simple and clear.
- Limitations are discussed and code is provided for reproducibility


Weaknesses:
- Paper presentation can be improved. For example, Figure 5 is too small, numbers are hard to read and colors are hard to distinguish. Also, if space permits, I feel like moving Algorithm 1 in Appendix to the main body would be better.

Limitations:
Two limitations are mentioned. 1. The bias is not completely eliminated. 2. Lacking theoretical understanding. I think pointing out these limitations is a plus and both limitations can trigger meaningful future work.


Rating:
6

Confidence:
3

";1
q4HlFS7B7Y;"REVIEW 
Summary:
This paper introduces VerT, a method to distill *verifiable* models from black-box models. Concretely, These verifiable models are distilled by fitting a model $f_v$ that reproduces the predictions of the black-box model $f_b$ on a training set. Unlike the black-box model, the model $f_v$ is made verifiable by making its predictions self-consistent if a mask isolating the signal is applied to the input data. This mask is fitted by adding an extra objective to the model distillation objective, which guarantees that the mask is sparse and that the predictions of $f_v$ is similar for both the masked and the unmasked input. The methods is evaluated on 3 image datasets, including 2 where ground-truth feature importance is known. This analysis demonstrates that VerT outperforms standard gradient-based feature importance methods. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
**Good writing.** The paper was really easy to read and follow. The notations are on-point and intuitive. 

**Solid empirical validation.** The experiments provided in Section 5 are convincing and thoroughly verify important claims: the features identified as salient by VerT have the strongest impact when masked (which is not surprising given the optimization objective underlying VerT), these features have a reasonable overlap with the ground-truth salient features in a setting where the later are known and VerT shows improved robustness. The authors also show that VerT improves the robustness to adversarial attacks on the explanations  with respect to gradient-based feature importance methods (which is unsurprising as the attacks are designed to fool gradient-based methods specifically). Insightful illustrative examples to understand the gains of VerT are provided in Figure 2.

Weaknesses:
**Restrictive assumption on replacement distribution.** In Lines 125-137, the authors discuss the importance of the choice of the replacement distribution $\mathcal{Q}$ in order to avoid creating OOD examples by masking. It should be mentioned that some important masking stategies, such as [Gaussian blurs](https://arxiv.org/abs/1704.03296), are omitted from this discussion as these are conditioned on the input image $\mathbf{x}$. I am wondering if it is even possible to define masking strategies with replacement distributions *independent* of the input $\mathbf{x}$ that do not create OOD examples. Intuitivelly, it is legitimate to expect the replacement input $\mathbf{q}$ to have (at least) some information about $\mathbf{x}$ to avoid replacing the features of $\mathbf{x}$ by OOD values. I would recommend the author to discuss this point thoroughly and (possibly) acknowledge this as a limitation of the work.

**Unrealistic signal-distractor decomposition.** The signal-distractor decomposition defined in Definition 3 is key for Theorem 1. It appears to me that this decomposition is unrealistic for a simple reason: in the underlying DGP, the mask $\mathbf{m}$ and the signal $\mathbf{s}$ are independent. To make this point more clear, I would like to consider the example discussed in the paper. If we consider a cow detection task, the signal $\mathbf{s}$  would typically correspond to the cow-part of the image. If that is the case, the mask $\mathbf{m}$ should depend on the position of the cow on the image (e.g. performing a translation on the signal $\mathbf{s}$ should result in a similar translation on the mask $\mathbf{m}$ in principle). Again, I would recommend the authors to comment on the realism of their assumptions.  



### Minor Weaknesses 

- Algorithm1: what is $M(\mathbf{x})$? Is this the same as $\mathbf{m}(\mathbf{x})$? Also, should there be a minus sign in front of the gradients if the objectives are minimized? 
- In the appendices, Theorem 2 corresponds to Theorem 1 the main paper. 

Limitations:
Some limitations of the work are discussed in Section 6 of the main paper. I do not believe that negative societal impacts are a real concern for this work. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The present work introduces a theoretical framework of verifiability of feature attributions based on the sparest (binary) feature attribution mask that only barely changes the models' output. The authors theoretically (and empirically) show that for signal-distractor decomposable datasets off-the-shelf black-box models cannot be verified according to their definition of verifiability. The main reason being that off-the-shelf models cannot handle the OOD samples created by the masking intervention (""feature replacement""). To overcome this, they propose a finetuning scheme, in which they make off-the-shelf black-box models robust to such feature interventions. Specifically, they alternatingly optimize for the sparsest mask (that only barely changes the models' output) and apply a distillation loss. While the former makes the model robust to the feature interventions, the latter ensures the similarity to the original model. Experimentally, they show that VerT improves interpretability over gradient-based methods. VerT robustly identifies the signals (most salient features) of the inputs, while retaining the performance of the original model.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The theoretical verifiability framework is simple yet sound.
- The theoretical analysis (Sec 3.2) is interesting and supported both theoretically as well as empirically.
- The finetuning scheme is simple yet effective. It does not change the models' output while simultaneously enhancing its interpretability by making it robust to the input feature removal.
- The paper is clearly written (except for some small issues; see questions & suggestions below) and easy to follow.
- Code is provided.

Weaknesses:
- The major weakness of the present work is that it does not compare to any removal-based feature attribution methods (e.g., [SHAP](https://proceedings.neurips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html), [FastShap](https://openreview.net/forum?id=Zq2G_VTV53T), etc.), models also trained on feature attributions (e.g., [right for the right reasons](https://www.ijcai.org/proceedings/2017/371)), nor inherently interpretable models (e.g., [JAM](https://arxiv.org/abs/2103.01890) or [B-Cos](https://openaccess.thecvf.com/content/CVPR2022/html/Bohle_B-Cos_Networks_Alignment_Is_All_We_Need_for_Interpretability_CVPR_2022_paper.html)); only comparisons to gradient-based methods are provided.
- The experiments always include manually-defined spurious correlations (to contain clear signals and distractors). While this provides empirical evidence for their theoretical framework, it would be meaningful to compare the method on “untouched”, more challenging datasets besides Celeb, e.g., ImageNet.
- The finetuning scheme may change the model’s behavior (but not its output due to Eq. 2). While Tab. 2 shows that the prediction stays similar, it may change what type of signals in the inputs the classifier considers for its prediction (and its inner workings). Consequently, the finetuned model may not be faithful to which signal it uses (or how it processes it) for its prediction.

Limitations:
The limitations are adequately addressed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a method called Verifiability Tuning (VerT), which transforms black-box models into models that naturally yield faithful and verifiable feature attributions. Authors further conduct experiments on semi-synthetic and real-world datasets to verify the effectiveness of the proposed VerT method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation of this paper is clear.

2. This paper focuses on the faithfulness of post hoc explanation methods, which is a very important topic in XAI.

Weaknesses:
1. The equation in Definition 1 is problematic. For example, let three positive input variables $a=b=c>0$ have a MAX operation, $output = \\max \\{a,b,c,0\\}$. Then, we mask any two input variables will result in different results. Specifically, we can mask $a$ and $b$, and keep $c$ unchanged. We can also mask $c$ and $b$, and keep $b$ unchanged. These two masking methods will result in different explanation results, but the actual importance of $a$, $b$, $c$ to the inference is the same. Hence, the equation in Definition 1 is problematic.

2. I disagree with your claim on the optimal Q. Specifically, the optimal Q can be found. Theoretically, the optimal Q should be the same as the distributions of the input image, i.e., setting q=x (in this case $\epsilon=0$ in Definition 1). Although this setting q=x conflicts with the motivation of masking the input, it is indeed the optimal solution to q in mathematics.

3. The ground truth of attribution methods constructed in experiments (in Figure 2) is too simple.

(1) If a classifier is powerful enough to just use five pixels for inference, instead of using the entire foreground of “4,” then is this classifier better or worse? The proposed evaluation does not consider this as a good explanation if annotating the entire foreground as the ground truth.

(2) Defining “4” as the foreground also seems problematic. The edge feature of “4”contains both pixels in foregrounds and pixels in background. Hence, information encoded in background can be used for inference theoretically. In this way, pixels outside of the “4” should also be considered as the ground truth. However, there exists another problem that how many pixels outside of“4”should be included as the ground truth.

(3) The correlation between dark hair and glass in CelebA dataset is just assumed. Specifically, this correlation is not a necessary condition for hair color classification, because the DNN can either exclusively use glasses for classification, or exclusively use hair, or use both hair and glasses for classification. The correlation is assumed and used for evaluation, but the correlation is not a certificated truth of a DNN.

This is a typical case when the input information for inference is redundant. When a small part of foreground objects are already enough for classification, it is difficult to annotate the ground-truth attention of a DNN.

4. Pixel perturbation tests are circular arguments, because the proposed method is learned by minimizing the loss. The loss is designed to mask unimportant pixels for inference. 

5. Only the accuracy is not enough to evaluate the faithfulness. Please compare the advantages of the proposed method with advantages of the Shapley value. We cannot assume that the direct change of the output caused by the masking of an input variable is the exact importance of the input variable. Please apply more sophisticated evaluation metrics for attributions proposed in recent years.

6. Authors just compare the proposed method with gradient-based explanation method, which is a quite weak competing method. Authors are suggested to compare the proposed method with more sophisticated baselines, such as Shapley values, DeepLIFT, IG, etc.

7. Why “QFA applied to a model from a Q-verifiable model class” can be considered as “a verifiable feature attribution?” Authors do not provide proofs to support this claim. Moreover, what is the definition of “a verifiable feature attribution?” Please clarify how a feature attribution can be considered as “verifiable.”

Limitations:
No, the authors do not discuss limitations of the proposed method.

Rating:
3

Confidence:
5

REVIEW 
Summary:
The paper proposes a way to verifiably get feature attributions of the ground truth signal when the input can be decomposed into independent signals and distractor features, assuming there is a counterfactual generator Q that can provide sparse attributions. The process consists of first deriving (\epsilon, Q) feature attributions for each point to get optimal masks, and second use distillation to train a new model that matches the outputs of the QFA with the original prediction while staying close to the original model. The first and second steps are alternated to minimize the overall loss. A rounding scheme is used to stabilize training with hard masks and superpixels are used. The approach is tested on MNIST, Chest X-ray, and CelebA. 

Update: After the discussion, I have updated my score accordingly. Since the authors believe scenarios that satisfy the signal-distractor decomposition are likely to exist, it would be great to incorporate such a concrete example somewhere in the text.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper proposed an approach that, in principle, could potentially extract the ground truth signal (if all assumptions are met). 

The approach, even though it trains a new model, replicates the original fairly well. 

The experiments are couple with 2 ablations on adversarial robustness and sensitivity to hyperparameters.

Weaknesses:
The signal decomposition assumes that the signal and distractors are generated independently, and that therefore the correct feature attribution is the sparsest one. This is not going to always be the case in real settings. 

The method requires a strong counterfactual generator Q to ensure correct recovery of sparse attributions. However, in practice the authors just use a dirac delta of the dataset mean which is not really a counterfactual generator. 

The evaluation is specific to vision, and the two non-MNIST datasets have spurious correlations injected into the dataset. It'd be nice to have some kind of breadth here beyond vision, or to have some results on more natural data that did not need to be artificially correlated. 

The claim that one can change any black box model into a verifiably interpretable model might be a bit strong---it seems to also depend on a number of assumptions (ground truth decomposability) and having a powerful generator in order for the ""verified"" part to hold true.

Limitations:
The limitations do in fact mention that a decomposition must exist.

Rating:
5

Confidence:
4

";1
9iafshF7s3;"REVIEW 
Summary:
Current VLMs suffer from a noticeable semantic gap between visual and textual modalities since many visual concepts present in images are easily missed in their paired captions. This work proposes Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, CoCu establishes a concept archive that maintains potential visually-matched concepts using vision-driven expansion and text-to-vision-guided ranking. This approach enables the identification of relevant concepts through cluster-guided sampling, which are then fed into the pre-training process. As a result, CoCu bridges the gap between visual and textual semantics. Extensive experiments conducted on eight segmentation benchmarks demonstrate that CoCu achieves exceptional zero-shot transfer performance and significantly enhances the language-supervised segmentation baseline by a substantial margin. These results underscore the importance of closing the semantic gap in pre-training data. The code for CoCu will be made available to the research community.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Overall, the paper is well-written, and the study of this work has a good motivation.
2. The mechanism of CoCu is intuitive and easy to follow.
3. CoCu demonstrates significant empirical results: it outperforms GroupViT (re-implemented baseline) by 4.6% mIoU in average on eight popular semantic segmentation benchmarks.
4. CoCu also yield good qualitative results, with more accurate and smooth masks than its baseline.

Weaknesses:
1. Despite the fact that CoCu indeed enriches visual concepts during VL pre-training, the cost seems to be heavy. During training, a CLIP model is employed to perform the key components of CoCu such as vision-driven expansion and text-to-vision guided ranking, which introduces a lot of additional computation. Thus, the comparison to your baselines such as GroupViT might not be strictly fair. A detailed comparison of training time or FLOPS should be presented to support the effectiveness of CoCu.

2. My bigger concern lies in how CoCu relies on the pre-trained CLIP, i.e., if using a weaker CLIP model for CoCu, how will the performance change? If CoCu is not sensitive to the CLIP's performance, you can directly use GroupViT to perform vision-driven expansion and text-to-image-guided ranking, so that no additional parameters will be introduced. As shown in Table 4, GroupViT obtains acceptable zero-shot classification results. However, if CoCu highly relies on a strong CLIP, your contributions might also be challenged.

Limitations:
As CoCu basically follows the architecture of GroupViT, it has the same limitation that for each image, the number of semantic concepts segmented by CoCu/GroupViT has a maximum of the number of group tokens fed to the vision encoder. Thus, given a high-resolution image with many semantic regions, these methods might fail to generate accurate segmentation masks.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper first points out the semantic gap problem of the existing language-supervised semantic segmentation method. It shows that not all visual elements are included in the corresponding language annotations. Then the paper proposes Concept Curation (CoCu), which includes Vision-driven Expansion, Text-to-Vision-Guided Ranking, and Cluster-guided Sampling strategies to solve the semantic gap problem. The experiments show the effectiveness of the CoCu.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The semantic gap problem sounds very reasonable and significant for language-supervised semantic segmentation.
2. The proposed method CoCu could alleviate the semantic gap problem to some extent.
3. The experiments and ablation study are comprehensive.

Weaknesses:
1. The authors should summarize the Sec. 2.3 at the beginning or the end of this section. For example, in general, Sec. 2.3 provides a method to find better class candidates for the loss in Sec. 2.1. Otherwise it confuses the readers about the ultimate goal of Sec. 2.3.
2. The overall pipeline of CoCu is a kind of dataset pre-processing method, and it is complicated. Is CoCu done online or offline? If online, how much time does it cost to find the final class candidates?

Limitations:
None

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed a novel data curation/argumentation process, named Concept Curation (CoCu), for language-supervised semantic segmentation. In the setting of language-supervised semantic segmentation (e.g. GroupViT), the network is trained with image-text contrastive loss on large-scale image-text pairs. Authors identified several issues in the vanilla data distribution of the original contrastive learning, e.g. semantic gap, and semantic bias. The proposed CoCu mitigates these issues and improves over prior work GroupViT under a controlled setting, showing a faster convergence rate and high accuracy. 

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
1. In the quantitative evaluation, authors re-implemented GroupViT and compare CoCu and GroupViT under the controlled experiment setting, i.e. 1024 batch size. The authors also report the accuracies on additional evaluation datasets, IN50, IN300, Cityscapes, etc. CoCu outperforms GroupViT on all the datasets by a margin. Although CoCu doesn't achieve state-of-the-art results on some datasets, it doesn't degrade the effectiveness of the method. 
2. The proposed method is well-motivated. The web-crawled image-text pairs are indeed quite noisy. And GroupViT is also known to be bad at segmenting background classes like grass and sky. Moreover, from the visualization in Figure 3(b), the CoCu shows that could focus on background grasses instead of the foreground fox. 
3. In the Table 2 ablation study, when only trained on CC3M, CoCu improves over GroupViT by a large margin on Pascal VOC. It is a very interesting result and justifies that CoCu speeds up the convergence. 

Weaknesses:
1. Insufficient qualitative comparison. Since one major claim of CoCu is that, compared with vanilla contrastive loss used in GroupViT, the proposed dataset curation mitigates semantic cap and semantic bias issues. So besides quantitative evaluation metric mIoU, more visualizations compared with GroupViT are expected. I would suggest authors add more visualizations in the supplementary materials if there is no space left in the main submission. 
2. In the ablation table 3, the authors studied the effects of different components of CoCu. But the average mIoU may not be an insightful metric. Since different datasets have very different category vocabulary, authors may try to include a detailed ablation study table in the supplementary material and elaborate more on the results. 

Limitations:
Yes. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper targets learning unsupervised semantic segmentation from image-text pairs. The authors specifically address the issue of training data quality in previous method Group-ViT and propose an approach to enhance the captions with additional visual concepts through an automated pipeline. The paper demonstrates the effectiveness of this data filtering pipeline by evaluating it on official segmentation benchmarks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The semantic gap problem the authors studied is intresting and meaning full.
- The writing is good and the paper is easy to understand.

Weaknesses:
The proposed data filtering pipeline relies on the CLIP model for collecting visually similar samples. However, the CLIP model has been trained on a much larger scale, with hundreds of millions of image-text pairs, whereas the training data used in this paper is relatively smaller. Does it work by distilling the CLIP model? This raises concerns about **the effectiveness of the pipeline when scaling up to larger training data sizes**. It would be helpful to explore **alternative self-supervised methods such as DINO, MAE, or Group-ViT itself to replace the CLIP model** in the pipeline and evaluate its performance.

Limitations:
NaN

Rating:
6

Confidence:
4

";1
A86JTXllHa;"REVIEW 
Summary:
This paper proposes an easy-to-use dataset-level method for predicting OOD scores. The authors analyze two desiderata in representation learning: high inter-class dispersion and high intra-class compactness. Through some experiments on CIFAR and TinyImageNet, the authors reveal that the inter-class dispersion is strongly correlated with the OOD performance while intra-class compactness does not really correlate with the OOD accuracy.  

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The proposed method is well-motivated and explained. The authors first claim that MMD and Fr\'echet distance is not good surrogates for  OOD error prediction and suggest using dispersion score instead. Figure 3 indicates that the dispersion score is indeed better than conventionally used distance.

2. The proposed method is easy-of-use and training-free. It is also flexible to different OOD data in sample size and class distributions.

3. The method outperforms previous approaches in most benchmarks. 

Weaknesses:
1. The experiments are all conducted in elementary and simple experiments settings, i.e., the OOD dataset is set to be some augmentation and corruptions applied on the ID set. This is not really the real-world OOD benchmark setting as the corrupted OOD dataset still has some class-overlapping information with the ID set. Can it apply to the standard OOD benchmark in CIFAR and ImageNet? For example, the authors could use ImageNet-1k as ID and iNaturalist, Places, Textures, and SUN as OOD. With the current experimental setting with simple data augmentation, it is really hard to judge whether the method can be useful for real-world usage.

2. It would be much more interesting to have non-overlapping ID and OOD dataset in experiments. For example, the authors can train model on CIFAR100 and use CIFAR10C as OOD or vice verse (CIFAR10 as ID and CIFAR100 as OOD). It also meets more real-world setting as CIFAR10/CIFAR100 is a commonly used OOD benchmark.

3. Though the intra-class compactness alone is not useful. Would it be better if the intra-class compactness is combined with the dispersion? Would it be an interesting ablation study?


Limitations:
My main concern is still the simple experiment setting. In my opinion, it would make the proposed method useful only if the OOD benchmark is replaced with real-world ones. I suggest the authors verify the proposed approach in standard CIFAR and ImageNet-1k benchmarks as done in [1,2,3].  Of course, it is not necessary to plot the curve of accuracy versus distance as done in Figure 1 and Figure 3 (a single correlation value would be sufficient), but it would be important to show the method of predicting OOD error is useful in practice especially given that the method does not rely on the class distribution. 

[1] React: Out-of-distribution detection with rectified activations. NeurIPS21

[2] On the importance of gradients for detecting distributional shifts in the wild, NeurIPS21

[3] RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection. NeurIPS22

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors focus on the task of predicting model performance on unseen/shifted datasets, without support of annotations. To do so, they first show the connection between feature separability and test accuracy, with an intuitive example and theoretical explanation. Based on the analysis, they propose a novel metric, Dispersion score, which measures the inter-class divergence from feature representations. They also reveal that intra-class compactness does not reflect the generalization performance, while inter-class dispersion works as a good indicator. They conduct experiments on CIFAR-C and TinyImageNet-C datasets to show the efficiency and effectiveness of the proposed metric. Furthermore, they also show the advantages of their method in some extreme cases (limited data, partial label set, class imbalance).

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1.The motivation of this work is reasonable. Intuitively, the prediction performance should be tightly tied with the quality of the learned feature. High inter-class dispersion is one of the goals of self-supervised learning for learning a good representation. The authors also provide an interesting explanation from a theoretical perspective, which is one of the highlights of this work.

2.The analysis of those methods with distribution distance is thought-provoking. In my view, the shifted distance might not be necessarily connected to the test performance, as the shifted features might not be important for the final prediction. For example, an image classifier with sufficient generalization ability would not change its predictions when the background is changed a lot.

3.The proposed method is novel and interesting. To the best of my knowledge, this is the first work to exploit the feature properties of test instances for predicting OOD error. It does not require access to the training data and only uses a forward propagation, which is much faster than existing SOTA methods.

4.The empirical results are extensive and convincing. The authors not only show the simple method outperforms existing training-free methods, but also compare to ProjNorm on the computational efficiency (575 second v.s. 11 second). The most exciting part for me is the analysis on the flexibility in OOD data, which considers some real-world settings, like class imbalance and limited data. The authors also present that a high intra-class compactness is not necessary for good prediction performance, which may provide a new insight for representation learning. 

Weaknesses:
1.The presentation of some Figures can be improved. For example, in Figure 1b, the magnified box seems not match the original area. The four images of Figure 2 are a little small, which might be improved by reducing the blanks between images.

2.Why intra-class compactness does not work is not clear. Although the authors show that intra-class compactness is not an effective indicator empirically, it could be better if the authors can provide an intuitive explanation.


Limitations:
The authors discussed the limitations of the proposed score in the adversarial setting, where the feature quality is also broken by adversarial attacks. The authors also provide analysis to show the underlying reason, which is also an interesting contribution. 

Rating:
8

Confidence:
5

REVIEW 
Summary:
This research focuses on predicting test accuracy on shifted datasets without access to ground-truth labels. The authors begin by analyzing the potential issue of the existing methods which were based on the shift distance and point out that the previous distributional distances were not always correlated highly to the out-of-distribution (OOD) error. Then, they proceed with an intuitive example and theoretical explanation, showing the connection between feature separability and test accuracy. Based on this, they propose a novel metric that measures the inter-class dispersion, which is demonstrated as an effective factor for the OOD error estimation. They conducted experiments on CIFAR-C and TinyImageNet-C to validate the advantages of the proposed method. The authors further demonstrate the robustness of the proposed method against class imbalance and data shortage.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The task studied in this paper is practically important. In some real-world applications, it is necessary to assess the model performance on a given unlabelled dataset. Under those scenarios, OOD error estimation becomes inevitable and valuable.
2. The proposed method is supported by both empirical observations and theoretical analysis. The motivation is clear.
3. The proposed method is novel, effective, and efficient. Previous SOTA methods like ProjNorm need to update the model which is computationally expensive (It can be unachievable with some large models). They also show that intra-class compactness cannot work well, which motivates deeper exploitation of the properties within the features distribution rather than the coarse-grained property on the whole dataset adopted in AutoEval.
4. It is also interesting to see that this paper further investigated the performance under some scenarios with imperfect data, e.g., class imbalance, smaller sample size, and partial OOD error prediction. To the best of my knowledge, this is the first work providing such a complete analysis. Compared with the previous studies, the proposed approach achieved comparable performance even under these extreme cases.
5. This paper is well-written and easy-to-understand. The analysis and figures provided by the authors are clear and informative. I believe readers can easily get the core idea and implement it.

Weaknesses:
1. Some potential typos should be corrected in the next version. See the Questions part for more details.
2. The discussion about the pseudo labels used for cluster centroid determination can be further extended. See Questions for more details.

Limitations:
I did not see any severe limitations in this paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work aims to predict classifier accuracy on unlabeled test samples. To achieve this goal, this paper proposes a feature separability-based dataset-level score to check whether features have high inter-class scatter. This feature separability score is calculated by measuring how far the centroids of features that share the same pseudo-label predicted by the model deviate from the center of all features on average. The experiments show that such inter-class dispersion is strongly correlated with model accuracy.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ [***Good clarity***] This work is well-written and easy to follow. The method is well presented, the visualizations are helpful, and the experimental settings are clearly introduced. 

+ [***Measuring feature separability is well-motived***] Under distribution shifts, the features of the source and target can be scattered differently. Using such information to reflect model accuracy seems reasonable.

Weaknesses:
- [***More results to illustrate the relationship between distribution gap and accuracy***] In the accuracy prediction, there are two metrics are proposed to measure the distribution gap. Please show their results in Figure 1 to well illustrate the motivation regarding the potential limitation of the distribution gap for accuracy estimation. 

- [***The definition of dispersion score is not sound***] The class cluster is based on the classifier's prediction. What if the classifier gives bias predictions on test sets? For example, in adversarial examples, the classifier maintains class-class separability but totally misclassified data. Moreover, why using the gound-truth label to define class clusters does not give stronger correlation strength (Section D).

    Moreover, whether the proposed method can handle the cases where some classes do not appear or some unseen classes appear. For example, ProjNorm discusses the label shift where some classes are missing. Under such a scenario, ProjNorm is less effective than other methods. 

- [***The experimental setting is somewhat limited***] This work only provides the experiments on small-scale datasets (e.g., CIFAR-10/100 and TinyImageNet). Considering the literature, the results on iWILDS and ImageNet should be included. For example, DoC and ATC report the results on ImageNet datasets with several natural distributions, such as ImageNet-V2, ImageNet-R and ObjectNet. Without the results on such realistic datasets, it is hard to conclude the robustness and effectiveness of the proposed method.

Limitations:
This work reports results for some special cases, such as class imbalance and adversarial attacks. Another potential limitation is the open set problem, where some unseen classes arise. Also, some classes may be missing during testing. It would be better to mention and discuss both cases, as the proposed scatter score can be significantly affected.

***[Post-rebuttal]*** 

> I would recommend excluding the open-set results from the main paper due to the evaluation metric's limitation in assessing only the seen classes. Moreover, ATC reports the results on some real-world and large-scale datasets like iWILDS and ImageNet, including such datasets would make the submission solid. This addition would enhance the robustness of your research and highlight its practical applicability. Given that you've already showcased results on domain adaptation datasets such as PACS, Office-31, and Office-Home, I am inclined to think that the current evaluation is sufficient.

> [Additional suggestions which do not impact the rating of this paper] You might consider referring to two relevant works: ""Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples,"" which also features results on domain adaptation datasets, and ""Characterizing Out-of-Distribution Error via Optimal Transport."" This latter work, which assumes a consistent marginal label distribution between training and test sets, stands in contrast to your method and may help highlight the merits of your approach.

Rating:
5

Confidence:
5

";1
rbw9xCU6Ci;"REVIEW 
Summary:
This paper considers the federated learning setting of adaptive test-time personalization. Traditional test-time adaptation (TTA) can only handle specific target domain distributions, while federated learning requires flexible handling of multiple target domains. Existing TTA methods pre-define which modules to adapt, which limits the application of TTA in federated learning. Therefore, this paper proposes the Adaptive Test-time Personalization algorithm called ATP to automatically decide which modules to adapt and how much to adapt.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The experiment in Section 3.2 is crucial as it effectively illustrate the inherent challenges encountered by current TTA methods when applied to federated learning, thus offering valuable insights and guiding directions to potential enhancements.
2. The proposed method is simple and easy to implement, and it has achieved impressive performance in the current experiments.

Weaknesses:
1. The paper claims that it is the first to propose test-time personalized federated learning. However, this claim is questionable because previous works, such as [1], had already explored test-time personalized tasks in the context of federated learning. 
[1] Jiang, Liangze, and Tao Lin. ""Test-Time Robust Personalization for Federated Learning."" ICLR2023 (preprint arXiv:2205.10920 (2022))
2. I am concerned about the significance of the “supervised refinement” step. If labeled data is available in this step, using the labeled data itself already leaks the distribution about the test samples in TTA tasks (while the test distribution is not known in the TTA setting), which obviously reduces the difficulty of TTA. If labeled data is not available, the proposed method for adaptively learning alpha seems unworkable.
3. The datasets used in the experiments are small-scale. It would be more convincing if experiments were conducted on real-world images, such as DomainNet, with at least a resolution of 224x224.

Limitations:
There is no limitations or potential negative societal impact discussed in this paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies test-time personalization in a federated learning setting --- after training on participating clients, the goal is to locally adapt the  global model given unlabeled test data. The paper's main idea is by pointing out that label non-IID and domain non-IID require adaptation on different layers of DNNs, and propose a novel way to learn the adaptation learning rates of each layer automatically in a data-driven fashion.  A simple learning method that alternatively do SGD on the DNN parameters and the layer-wise learning rates shows effective improvements in test-time personalization.  

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The method proposes a novel aspect that to tailor different types of non-IIDness, the degrees of adaptation are different across layers. it is an interesting point. The solution is simple, sound, and effective. 

- Empirical results are overall satisfying. The experiments provide enough comparisons to centralized TTA algorithms; see some suggestions below. 


Weaknesses:
Although I am positive on this paper, I observe several important concerns. If the authors could address them, my score can be higher. 

[Major 1] Overclaims: considering TTA in PFL setting was first introduced in [15]. This is a natural extension of centralized TTA. It is unnecessary and imprecise for this paper to make ""test-time personalized federated learning (TTPFL)"" a new setting. The definition in L38 about the combination of distribution shifts of labels and styles itself is nothing to do with federated setting; centralized TTA can have both label and style shifts.

[Major 2] Misleading section 4.2. I cannot understand why the proposed refinement has a connection with meta-learning. ClientTrain in Alg 1 is simply a coordinate gradient style optimization. Alternatively, a discussion about hyperparameter optimization should be more relevant.
 
[Major 3] Theorem 5.1 is not informative. It's simply extending the classic FedAvg generalization bound [18] for the adaptive parameters. It will be more informative and fit to the context to discuss the generalization to the new clients of different distributions ''after test-time personalization'' in TTA sense. Otherwise, I would recommend avoid this laundry theorem. 

[Minor 1] The datasets are rather small scales and synthetic. it will be better to include natural federated datasets like FEMNIST or iNaturalist-GEO or large-scale common datasets in TTA (like ImageNet).

[Minor 2] Why FedTHE or FedTHE+ in [15] are not compared? [15] seems to be the closest work about TTA in FL. Is the discussion in L76 faithful? It seems to me FedTHE does not need labeled data.  


Limitations:
See weakness.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a new setting called test-time personalized federated learning (TTPFL) and proposes an Adaptive Test-time Personalization algorithm. The authors show effectiveness of proposed method over other test-time adaptation methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper proposes an Adaptive Test-time Personalization algorithm and shows its effectiveness over other test time adaptation methods.

Weaknesses:
1. The proposed setting is strange and not self-consistent. The authors claim that in this setting 'clients adapt a trained global model in an unsupervised manner without requiring any labeled data.' However, the proposed method involves labeled clients to learn the learning rates for different modules (Figure 3, left). At least, real-world scenarios as examples should be provided.
2. Missing representative federated learning baselines. This paper does not compare with any federated learning method. If the proposed method uses the labeled datasets for the training stage, then many federated learning methods can be seen as baselines that are trained on the labeled datasets. Baseslines include FedAvg, FedAvg with Fine-tuning, FedProx, FedProx with finetuning, Ditto, pFedMe.
3. Since the proposed method focuses on different operations on different modules of a model, model architecture is important to this paper, thus experiments on different model architectures are required.

Limitations:
Generally, I am really confused about the setting in this paper after reading. Please provide comprehensive explanations to correct me if I am wrong,  and I would consider re-rating.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel setting where personalized FL during the test procedure is considered and multiple distribution shifts are involved. A method termed ATP is proposed to solve the challenges posed in this setting. Adaptive learning rates are learned for the model. Both theoretical and empirical studies are carried out to demonstrate the effectiveness of ATP.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-organized and easy to follow. 

2. The main claims, e.g., the capability of dealing with multiple distribution shifts and the effectiveness of adaptive learning schemes, are well supported by empirical studies. 

3. Theoretical analyses on both the convergence and generalization ability of ATP are provided.

Weaknesses:
1. One of the key factors of TTPFL is confusing. Usually, we assume test data are unlabeled and the target of the classification task is to predict the label of the test data. However, in the summarization of TTPFL, it is emphasized that each testing client only has unlabeled data for personalization. It is better to further clarify this factor.  

2. The relation between FL and test-time shift is weak. It seems that the proposed adapting trainable parameters, adapting running statistics, and adapting rates can also benefit centralized test-time shift problems. For FL systems, the unique challenges brought by test-time shift issues and how they motivate these adapting solutions are not explicitly demonstrated.

3. Lack of detailed discussion on the difference between this work and the previous study[15]. [15] also considers feature shift, label shift, and a mixture of these shifts in their recent paper. There are various feature shifts in [15], which can also be considered as part of the experimental setting in this work.

Limitations:
This paper does not adequately address the limitations in terms of privacy issues, efficiency, etc.

Rating:
5

Confidence:
4

";1
WK8LQzzHwW;"REVIEW 
Summary:
The authors address the topic of rejection of samples in an unsupervised anomaly detection setup. Their approach focuses on determining a constant rejection threshold, which allows the detector to reject examples with high uncertainty. The new proposed method introduces this rejection threshold based on a confidence score given by another existing model (ExCeed). The authors provide theoretical analyses as well as empirical experiments and show that it is possible to set a constant rejection threshold with strong theoretical guarantees. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper provides a good structure but also requires some pre-knowledge in this area to be able to follow the presented thoughts e. g. research questions are later connected according to sections. 
- In-depth theoretical methodology as well as empirical evaluation.
- Detailed overview of single results in the supplement is given.

Weaknesses:
- It was not always straightforward to follow the paper, especially because a lot of variables are introduced but are defined much later in the paper (e.g. t_1(n, \gamma, T) (line 131) and  \gamma = \mathds{P}(Y = 1) (line 199)). Starting with a more explanatory part would let the reader build an intuition about which factors are important to calculate the rejection threshold. With all the factors in mind, it will get easier to follow the complex theoretical contribution. 
- in line 135 \epsilon is defined as 1 - 2e^(-T). In the formula between lines 123 and 124, the rejection threshold is defined as \mathcal{T} = 1 - \epsilon = 1 - 2e^(-T), which would result in \epsilon = 2e^(-T).

Minor comments: 
- ""Our approach is called **RejEx** (Rejecting via ExCeed)(line 110)
- In Theorem 3.8 it is refed to Theorem 3.5 for the definition of g. Theorem 3.5, however, refers to Theorem 3.4; It would be easier to directly refer to Theorem 3.4


Limitations:
Difficult write-up.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper presents a rejection scheme for the task of unsupervised anomaly detection.  Learning to reject enables a predictor to withhold from making a prediction; this paradigm is more common in unsupervised learning. Here, the authors extend the rejection idea to the unsupervised anomaly detection task.  The idea is to reject samples based on a stability metric; namely, of the prediction is unstable to small changes in the feature space, the prediction is rejected. This type of stability metric was recently proposed and is termed EXCEED. The authors present a theoretical analysis of the EXCEED metric and derive upper bounds for the test rejection rate and expected prediction cost. The new scheme is evaluated for several anomaly detectors on real datasets and outperforms other rejection schemes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written, and easy to follow.  Overall, the presentation is scientifically sound. The problem of unsupervised anomaly detection is extremely challenging and important; the paper presents a rejection scheme that could improve trust in commonly used detectors. The idea of using stability and specifically the EXCEED metric, makes sense. The theoretical analysis strengthens the work and offers bounds on the expected values of the presented scheme. The empirical evidence presented in the paper is promising and demonstrates the merits of the method. 

Weaknesses:
Background on the EXCEED method is missing; adding more information on this metric could help the reader. Some recently proposed NN anomaly detectors are missing from the evaluation, for example:
[1] Qiu, Chen, et al. ""Neural transformation learning for deep anomaly detection beyond images."" International Conference on Machine Learning. PMLR, 2021.
[2] Shenkar, T., & Wolf, L. (2021, October). Anomaly detection for tabular data with internal contrastive learning. In International Conference on Learning Representations.
[3] Lindenbaum, et al. (2021). Probabilistic robust autoencoders for outlier detection. arXiv preprint arXiv:2110.00494.
There are many other NN that could be included, I think some NN baselines should be considered. 
The description of the experiments conducted is too brief; it would be good if the authors could expand on the implementation and evaluation protocol. For example, what is $\lambda$ in all experiments? Or how is it tuned?

Sample size is limited in the evaluation to 20K.

Limitations:
Limitations are discussed in the last section, I would however add information on the cases in which rejection increases the cost, for example using statistic across datasets. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper suggests applying the stability metric computed by EXCEED for anomaly detection. The authors present theoretical findings regarding this metric, including the test rejection rate, as well as upper bounds for both the rejection rate and the expected prediction cost. Furthermore, comprehensive experiments are conducted to validate the effectiveness of the proposed method.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The presentation of the paper is clear, and the proposed method is simple but effective.
2. This paper offers a theoretical analysis of EXCEED, deriving the upper bounds for both the rejection rate and the expected prediction cost.
3. The effectiveness of the proposed method and the validity of the theoretical results are confirmed through comprehensive experiments.

Weaknesses:
1. The methods compared in Figure 1 appear to be significantly dated. It would be valuable if the paper could include additional results pertaining to recently proposed methods.

Limitations:
Please refer to [Weakness].

Rating:
5

Confidence:
3

REVIEW 
Summary:
- The authors proposed a selective predictor (learning to reject) for fully unsupervised setting in anomaly detection problems given an unsupervised anomaly detector.
- The proposed method is based on the theoretical supports and the threshold can be selected without any labeled data.
- The experimental results show that the proposed method can significantly reduce the cost of selective prediction.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is grounded on the theoretical supports that could be beneficial on generalization.
- The experimental sections are extensive and multiple ablation studies show its superiority across various settings.

Weaknesses:
- It seems like the proposed method is only applicable when the anomaly ratio is given. In some cases, anomaly ratio itself is not provided.
- It would be great if the authors can provide more extensive experiments when we have some labeled data in comparison to the baselines.

Limitations:
Limitations are clearly stated.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to perform learning to reject for anomaly detection in a completely unsupervised manner. The authors make three major contributions: (1) a novel theoretical analysis of a stability metric for anomaly detection, (2) a mechanism for designing an ambiguity rejection mechanism without any labeled data that offers strong guarantees, and (3) an evaluation of the proposed approach on an extensive set of unsupervised detectors and benchmark datasets. The authors show that their method outperforms several adapted baselines based on other unsupervised metrics and that their theoretical results hold in practice.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: The paper proposes a novel approach to perform ambiguity rejection for anomaly detection in a completely unsupervised manner. The authors provide a novel theoretical analysis of a stability metric for anomaly detection and show that it has several previously unknown properties that are of great importance in the context of learning to reject.

Quality: The paper provides a thorough theoretical analysis of the proposed approach and demonstrates its effectiveness through experiments on an extensive set of unsupervised detectors and benchmark datasets. The authors also provide strong guarantees for their proposed method.

Clarity: The paper is well-written and easy to follow. The authors provide clear explanations of the proposed approach and the theoretical analysis.

Significance: The proposed approach addresses the challenge of uncertainty in traditional anomaly detectors and provides a solution through Learning to Reject. The authors show that their method outperforms several adapted baselines based on other unsupervised metrics and that their theoretical results hold in practice. The proposed approach has significant implications for anomaly detection in various domains.

Weaknesses:
The author may provide more intuition on how EXCEED works to estimate the stability.
The paper could benefit from a more detailed discussion of the limitations of the proposed approach and potential directions for future research. 
While the paper provides a thorough theoretical analysis of the proposed approach, it could benefit from more detailed explanations of the experimental setup and results. Specifically, the paper could provide more information on the hyperparameters used in the experiments and how they were selected, as well as more detailed comparisons with other state-of-the-art methods.


Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
3

";1
pCucay08Co;"REVIEW 
Summary:
In this work, the authors characterize the problem of the Barren Plateau from different perspectives: (1) local unitary within a QNN on the cost function, particularly the randomness for the generic cost function; (2) quantum information theory; (3) the optimization methods during training. This work discusses those factors impacting the Barren Plateau landscape.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The work provides a theoretical understanding of the Barren Plateau problem and determines what factors actually impact the training of VQC, which is very interesting. 

(2) Solid mathematical formulation is given and the experiments can corroborate the theoretical analysis. 


Weaknesses:
(1) Some latest work on the Barren Plateau problem in the training of VQC should be included, such as Refs. [1], [2], and [3]. Ref. [1] aims at the QNN architecture for dealing with the Barren Plateau problem, Ref. [2] focuses on the initialization strategy, and Ref. [3] puts forth the pre-training method for mitigating the VQC training problem of Barren Plateau. 

[1] Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, ""Theoretical Error Performance Analysis for Variational Quantum Circuit Based Functional Regression,"" npj Quantum Information, Vol. 9, no. 4, 2023

[2] Zhang, Kaining, Hsieh, Min-Hsiu, Liu, Liu, and Tao, Dacheng. Gaussian Initializations Help Deep Variational Quantum Circuits Escape From the Barren Plateau. In Neural Information Processing Systems, 2022.

[3] Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, ""Pre-Training Tensor-Train Networks Facilitate Machine Learning with Variational Quantum Circuits,"" arXiv:2306.03741v1

Limitations:
It is expected to have experimental simulations on real data like the MNIST dataset to demonstrate the effectiveness of the proposed analysis approach. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper examines the critical issue of trainability in quantum neural networks (QNNs) by adopting a perspective centered around the locality. Through extensive analysis, the authors convincingly demonstrate that the adjustment of local quantum gates within a diverse range of QNNs results in an exponential decay of the loss function range as the number of qubits scales up. The authors bolster their claims with carefully conducted numerical simulations, providing compelling evidence that locality plays a fundamental role in shaping the behavior of QNNs. Building upon prior research on barren plateaus, the paper makes a technically sound contribution, albeit with an incremental advancement in the field.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The analysis of Theorems and Propositions, which shows the exponential decay of the loss function range by adjusting local quantum gates, is technically sound. Additionally, the ideas, concepts, and results are well presented. The authors effectively communicate their methodology, theoretical framework, and experimental simulations, making it easier for readers to comprehend and follow their arguments.  

 







Weaknesses:
The main weakness of this paper lies in its limited impact. While the authors conduct a clear and thorough analysis of how the concentration results of random circuits depend on the locality unitary, the technical tools employed bear a striking resemblance to prior literature concerning barren plateaus. The achieved results can be derived from existing works, with the only notable distinction being the introduction of a parameter, m, related to the locality in the derived bound. Additionally, the authors' claim that few rigorous scaling results exist for generic QNNs is contradicted by the abundance of relevant research, as evidenced by references [1], [2], [3], and [4], which address similar theoretical aspects the authors aim to explore. Previous studies have already established that deep ansatz can lead to the concentration of the cost function, rendering the observation regarding the exponential vanishing of the loss function range via the adjustment of local quantum gates less novel.


[1] Leone, Lorenzo, et al. ""On the practical usefulness of the Hardware Efficient Ansatz."" arXiv preprint arXiv:2211.01477 (2022).
[2] Thanasilp, Supanut, et al. ""Subtleties in the trainability of quantum machine learning models."" Quantum Machine Intelligence 5.1 (2023): 21.
[3] Garcia, Roy J., et al. ""Barren plateaus from learning scramblers with local cost functions."" Journal of High Energy Physics 2023.1 (2023): 1-79.
[4] Larocca, Martin, et al. ""Diagnosing barren plateaus with tools from quantum optimal control."" Quantum 6 (2022): 824.

Limitations:
No, the authors did not address the limitations of their work

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper investigates the trainability of random quantum circuits from the perspective of their locality and demonstrates the variation range of the cost function via adjusting any local quantum gate vanishes exponentially in the number of qubits. This theorem unifies the restrictions on gradient-based and gradient-free optimizations. The paper also verifies their theorem on three applications with numerical simulations and deepens the understanding of the role of locality in QNNs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and provides a rigorous analysis of QNN trainability and scalability from the perspective of their locality. 
2. The paper applies the proposed theorem to three representative QNN models, including the VQE, quantum autoencoder, and quantum state learning, and provides the numerical simulation results.

Weaknesses:
1.  Although Line 66-73 provides the advances of the proposed method, the comparison with previous works is not clear enough. It is important to review the previous methods and compare their specific differences. 
2.  The contribution of the paper seems weak, the novelty, comparisons with related works, and guidances for future QNN training or design need to be highlighted and enhanced.

Limitations:
The paper can be improved by considering the above weaknesses and questions.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proof a result on the range of possible values that the cost function of a variational quantum algorithm can take when one optimises over a given unitary that is before or after random gates that form unitary 2 designs. This quantity vanishes exponentially with the number of qubits. This generalises previous results on Barren plateaus, concerned with the vanishing of the gradient of the cost function.
The material is presented clearly and the paper also has numerical verification of the scaling in the case of VQE, quantum autoencoder and quantum state learning. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written, with figures explaining concepts. It presents both theory and numerical checks
- The problem studied is relevant in scaling up quantum neural networks 
- The main theorem allows the authors to recover and unify previous results on exponentially vanishing gradients and cost function differences

Weaknesses:
- The paper does not comment on recommendations to avoid the exponentially vanishing variation range
- The comparison to previous works is limited. The authors mention that their work opens a new venue for analysing trainability of QNNs but it is not clear to me what new insights are gained. It would be useful to comment on what is gained wrt previous literature. Also, on whether the methods used to prove their main theorem are similar to those used in the literature or not.
- The VQE experiments are taken with circuits of depth 10 x n. That depth was chosen so that the hardware aware ansatz approximates a 2-design. However no comment on the required depth to compute the ground state of the Hamiltonian is presented and it is not clear whether the choice of ansatz and depth is something that practitioner would actually do.

Minor

- Sentence Line 71 - 73 does not read very well, you could rephrase it
- Line 153 - 155: it would be helpful for the reader to have an explanation of the connection between parameter shift rule and $e^{-i\theta \Omega}$ with $\Omega^2=1$. Also why does this imply the existence of $W$ as claimed? 

Limitations:
- The method applies when the $V_1$ or $V_2$ are 2-designs. This limits applicability.
- Strategies to overcome the exponentially vanishing range are not discussed. 

Rating:
6

Confidence:
3

";0
j2EaW49Rk7;"REVIEW 
Summary:
In this paper, the authors propose a gradient-boosting algorithmic framework for rule ensemble learning, emphasizing the interpretability of produced rule set. Various gradient-boosting algorithms are reviewed in the rule-learning context, and the authors argue that a specific boosting algorithm, called fully corrective orthogonal gradient boosting (FCOGB), is particularly suited for rule boosting. The intuition is that existing additive rule-boosting procedures operate in a strictly greedy fashion - the weight of each added rule is fixed in later iterations. In contrast, FCOGB allows the weights of preceding rules to be adjusted in each later iteration, which may help to reduce the number of required rules (to reach a certain accuracy) and thus the cognitive complexity of the final rule set. Based on FCOGB, the authors derive the stepwise boosting objective function for single rule search, which is similar to existing gradient boosting objectives but with a different regularization term. The overall algorithm looks like the conjugate gradient method - in each iteration, the rule aligning best with the gradient in the orthogonal complement of the subspace spanned by previous rules is added. The authors demonstrate the effectiveness of FCOGB through experimental comparison with existing rule-boosting algorithms on classification, regression, and Poisson regression tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Applying FCOGB to rule learning, to the best of my knowledge, is a novel idea, and the authors provide a comprehensible justification for this choice. Figure 2 is helpful in understanding the difference between FCOGB and existing rule-boosting algorithms.
- How to search the optimal rule in each iteration is especially considered, which is a key step in rule boosting. The authors propose a strategy that exploits the nice structure in the boosting objective function to speed up the bound calculation in branch-and-bound search of optimal rules.
- The proposed algorithm is evaluated on a wide range of datasets and tasks. The authors provide a detailed analysis of the results. Figure 1 clearly shows that FCOGB can achieve a better accuracy-risk trade-off than existing rule-boosting algorithms.


Weaknesses:
- The presentation of the paper can be improved. For example:
  + In the ""Rule Boosting"" section, the ""Gradient boosting"" subsection mixes the description of general gradient boosting and the more specific rule boosting. This makes it hard to understand these objectives for readers who are not familiar with the rule-boosting literature. For example, obj_gb(q) = |g^T q|/||q|| is nonstandard in the general gradient boosting literature. It would be better to separate the general gradient boosting and rule boosting parts.
  + The ""Single rule optimization"" subsection assumes too much prior knowledge about the rule learning literature. I would suggest the authors merge this subsection with the ""4.3 Efficient Implementation"" subsection to make the paper more fluent and self-contained.
  + The authors should provide more details about the proposed algorithm, especially the BnB/beam search of a single rule.
- Lack of comparison with rule induction algorithms based on column generation, e.g., [30] and [b]. In the column generation approach, the weights of all added rules are also adjusted in each iteration when solving the restricted master problem, which is similar to FCOGB. I am interested in how FCOGB compares with this approach experimentally.
- The presentation of the prefix optimization problem is misleading. The authors claim that ""This function can be efficiently computed for many objective functions by pre-sorting the data in time O(n log n)"" in Section 3, but is this true for the objective function obj_{ogb}(q)?  The authors should clarify this point. I cannot immediately see how the optimal solution to (2) under this objective function is contained in the prefix of the data sorted by some (what?) criterion. If this is true, the authors should provide a proof or a reference to support this claim.
- There is a mistake in Lines 233-234.
- Missing references:
  + [a] Jonathan Eckstein, Noam Goldberg. An Improved Branch-and-Bound Method for Maximum Monomial Agreement. INFORMS Journal on Computing, 2012.
  + [b] Jonathan Eckstein, Ai Kagawa, Noam Goldberg. REPR: Rule-Enhanced Penalized Regression. INFORMS Journal on Optimization, 2019.
  + [c] Fan Yang, et al. Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach. NeurIPS 2021.

Limitations:
The limitations of this work are not explicitly discussed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach to gradient boosting of decision rules for interpretable machine learning models. By incorporating a
weight correction step and orthogonal projections, the method maximizes predictive gain per rule.
Their experimental evaluation on various classification, regression, and Poisson regression tasks confirms that the resulting rule learner
enhances the trade-off between comprehensibility and accuracy in the fitted ensemble. Moreover, it maintains a comparable computational
cost to previous branch-and-bound rule learners.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Originality: The paper introduces the first rule boosting algorithm that consistently optimizes the accuracy/complexity trade-off of produced rule sets. This represents a novel contribution to the field.
2. Quality: The research exhibits high quality as it adopts the fully corrective boosting approach, which entails re-optimizing all rule consequents in each boosting round. The study's rigorous algorithm development provides a strong foundation for the research, ensuring
the reliability and robustness of the findings.
3. Clarity: The paper explains the new objective function for selecting individual rule bodies, the corresponding efficient algorithm for cutpoint
search along with some other algorithm details. The clear explanations contribute to the overall clarity of the research.
4. Significance: The research demonstrates significant improvements over previous boosting variants in terms of the risk/complexity tradeoff.
The better risk reduction per rule and the affinity to select simpler rules contribute to the overall significance of the findings. Additionally,
the comparable computational cost to previous approaches adds to the practical relevance of the research.

Weaknesses:
In terms of the compared established methods, SIRUS [1] is the most recent work included in the analysis, published in 2021. However, it is
worth noting that some more recent publications, such as [2,3], are not included in the experiment section.
One limitation of the paper's presentation is the heavy reliance on text and equations, with less emphasis on the use of figures and intuitive
example case studies. This approach may hinder the reader's ability to grasp complex concepts and visualize the practical applications of
the proposed methods. Incorporating more visual aids, such as figures and illustrative examples, could enhance the clarity and accessibility
of the research.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Limitations:
A limitation of the study is that while it includes the most recent work, SIRUS [1], which was published in 2021, it does not incorporate some
more recent publications like [2,3] in the experiment section. This omission limits the comprehensiveness of the analysis and may overlook
potential advancements or alternative approaches introduced in these newer works.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents a new algorithm for learning rule ensembles and claims that these are interpretable, but does not present any support.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The proposed method is reasonable, but, in the context of other work in this area, not ground-shaking. The experimental evaluation is done well, but does not touch on interpretability.

Weaknesses:
There is no evidence that the learned rule sets are interpretable.
Rule complexity has not much to do with cognitive complexity.
The efficiency of algorithm is over-stated in the paper.

Limitations:
This is another paper that claims that rule ensembles are interpretable. No evidence is presented to that end, the claim is just derived from the fact that rules are, by themselves, interpretable. However, for example, random forests are well-known to be not interpretable, and they are also just a rule ensemble. In addition, the situation is even worse here, because in a random forest at least each individual rule is interpretable, and may be viewed as an explanation for all the examples it covers. In an additive boosting setting, this property also does not hold, because each rule corrects and refines predictions of previous rules, so rules can no longer be interpreted in isolation, but only in the context of all previous rules. Even a single example can not be easily explained by a gradient-boosted rule set, because one would have to understand the interaction of multiple rules.

The authors use the term ""cognitive complexity"" for something that is essentially the size of a rule set. Again, this is a complete misnomer, as the cognitive effort to parse a rule set does not only depend on the size of the theory. As explained above, there might be dependencies between rules, or rules may be considered in isolation (the latter having a much lower cognitive complexity). There are also factors such as the familiarity with the used concepts. For example, the cognitive effort required to read a page of text in your mother tongue is much lower than the cognitive effort required to read a page in a language that you are just learning, even though both, the content, as well as the syntactic length (essentially the author's measure of cognitive complexity) is the same.

It is a pity that they authors make such unfounded claims about intepretability, where they could simply present their work as an attempt to learn a simpler rule ensemble. As such, the work is reasonable, but also not great break-through. What they essentially propose (following previous work) is to re-optimize all weights once a new rule is added, and build an efficient algorithm around that idea. It gains a little in performance, as can be expected, but it is not great break-through.

The small advantage seems to be bought with an increase in computation time, which the authors interpret as ""in the same order of magnitude"" except for one case, where it is by a factor 26 slower. Actually, it seems to be the case that in most of the datasets, the algorithm is at least a factor of 2 smaller, sometimes worse. 

Minor comments:

Some of the numbers in Table 1 are obviously wrong (e.g., testing risks of 109.5 or 4.115 for XGB).





Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces Fully-Corrective Orthogonal Gradient Boosting (FCOGB), a novel algorithm aimed at facilitating interpretable rule learning. The study contends that existing rule learning algorithms often yield complex models that pose challenges for interpretation. FCOGB addresses this concern by generating simpler and more easily understandable models.
FCOGB is an extension of the widely employed gradient boosting algorithm, utilized for constructing predictive models. It employs a branch-and-bound search algorithm to identify the optimal set of rules that minimize prediction errors.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed method is supported by theoretical justifications and intuitive explanations using figures. Additionally, the paper proposes algorithms with computational complexity analysis to efficiently implement the method, demonstrating practical applicability.

Weaknesses:
Despite an increase in the required training time (takes several times longer computation), the generalization performance does not improve. If this weakness is addressed, I believe it would become a very strong paper.

(Minor comments)
- Despite Figure 2 being referenced on page 5, the figure is actually inserted on page 3.
- The scatters plot in Figure 3 are difficult to interpret due to overlapping points. Please set alpha (transparency).

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a framework of fully corrective orthogonal boosting. The main algorithmic difference here is the objective for each next weak model. It is the cosine between the gradient (which is orthogonal to previous weak learners by construction) and the part of the new model that is orthogonal to previous models. Authors motivate their work by the need of interpretable models, so they restrict themselves to the case of rules as weak learners. Also, they use a variant of b&b algorithm for optimal weak learner search instead of commonly used greedy construction in depth. 

Authors claim that this the paper proposes an algorithm for constructing shorter and more interpretable rules for Gradient Boosting of Decision Rules model. Experiments show that described method outperforms standard implementations of GB in case of using models of low complexity.

Although theoretical part is sound, practical questions are not thoroughly addressed or answered.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The main part of the paper is well-written, the terms, designations and ideas are clear. The proposed method is sound, reasonable and well described. The idea of orthogonal rule search in conjunction with fully-corrective goosting looks good. The theoretical part is described very well, the main formulations are correct, and the obtained contributions look important and are novel to the best of my knowledge. The proposed algorithm is justified and has the potential to compete with SOTA in the outlined formulation that refer to ""cognitive complexity"".

The main part of the paper is well-written. The terms, designations, and ideas are clear. 

The only point I did not buy is the Poisson loss defined in line 109, in my opinion, incorrectly (or unclear), because, formally, from that definition, its minimum is at $f(x_i)=0$ independently on $y_i$.

Weaknesses:
I have the following concerns about the research direction itself. Claimed advantage of ensembles of rules over ensembles of trees is their human interpretability. However, I cannot agree that the decisions a rule ensemble makes can be treated as interpretable. Particularly, I argue that in the domains where interpretation is important summation of even two terms is usually not interpretable for humans. Most critical decisions in such domains like medicine and justice, partly science and risk management are usually based on several binary factors, not a sum of dozens of rules. Where ensembles of rules are really used in practice?

Second, I am disappointed that the term ""cognitive complexity"" was left without any background. I would expect references to some papers using this metric or explicit statement that this way to estimate models' complexity is originally proposed in the current paper. Futhermore, I would expect some consideration of actual research in psychology domain that address the problem of cognitive complexity of calculations.

For example, we can see in ""Human knowledge models: Learning applied knowledge from the data."" Plos one, 2022, by E. Dudyrev et al., that a human decision is usually based on:
-	Boolean operators: OR, AND, NOT, and thresholded Boolean SUM (arithmetic sum of
Boolean variables, compared to an integer threshold)
-	At most four (Boolean) variables, where each variable is used at most once\

These ideas are rather far from the concept of sums of dozens of rules

See also:

Lemonidis C., “Mental Computation and Estimation: Implications for mathematics education research, teaching and learning”, 2015,

Marois R et al, ""Capacity limits of information processing in the brain,"" Trends in cognitive sciences, vol. 9, no. 6, pp. 296–305, 2005

Nys J. et al, ""Complex Mental Arithmetic: The Contribution of the Number Sense,"" Canadian journal of experimental psychology, vol. 64, no. 3, pp. 215–220, 2010.

At last, but not least, the experimental part spoils the impression of the work and requires improvements:

- First of all, I see no hyperparameter tuning step description (e.g. regularization terms for XGB, number of boosting rounds, length of decision rules) in the section on experiments. Are there any hyperparameters which may have a significant impact on the performance of FCOGB? Where they left ""defaulted"" or were they were tuned by a separate step of an algorithm?

- In the beginning of Section 5, it is mentioned that you use only 5 runs for each dataset with < 50 cognitive complexity (CC) limit. But then I see averaging over complexities between 1 and 50 in the description. What does it mean? I suppose that CC may alter in different runs but it is limited to 50, is it true? Or did authors perform exhaustive search of all possible CCs and averaged over them? If the first is true, I have a doubt that different model may have had different mean CC values, so that it is not quite fair comparison results. Is the second is true, then it is unclear why such an averaging can prove something

- It would be interesting to see the dynamics of quality with respect to increasing CC. In particular, some graphs that plots quality vs CC to see which algorithm uses the CC limit more effectively.

- It may be useful to provide comparison with other variants of fully-corrective boosting implementations since the quality gain may origin from described by this scheme only

- Time limitations should be discussed more in terms of time per CC point and pareto curves (time to achieve the desired quality)

- How should we interpret relatively low quality for regression problems?

- This paper addresses interpretability of trained decision rules, so it would be profitable to demonstrate a difference in the simplicity of interpretation for FCOGB rules and, e.g., XGB rules

Limitations:
I do not see any particular limitation of the proposed work

Rating:
7

Confidence:
4

";0
z06npyCwDq;"REVIEW 
Summary:
The paper analyses the two common types of normalization layers in Transformers: LayerNorm and RMSNorm (root-mean-square-norm). LayerNorm scales all vectors to be of the same norm, while changing the vectors' ""directions"". RMSNorm, in contrast, keeps the same direction, but just rescales the vectors to the same constant norm.
The authors ""unify"" both approaches, by showing how to force LayerNorm to act like RMSNorm. Then, since RMSNorm is faster to compute, the authors argue that this conversion (LayerNorm -> RMSNorm) can get the expressivity benefits of LayerNorm with the speed of RMSNorm.
Finally, the authors propose C-RMSNorm, which saves 1 dimension. For example, if the model hidden size is 1024, C-RMSNorm allows reducing it to 1023 while calculating the norm. 
Overall, the authors argue that these changes can reduce the training and inference time of LayerNorm transformers by 10% (although without discussing whether there's a reduction in performance/accuracy).

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
## Strengths
+ The authors provide a thorough explanation of LayerNorm vs. RMSNorm.
+ The authors show the relation between LayerNorm vs. RMSNorm, which I haven't seen in any other paper.
+ The proposed approaches can sometimes reduce inference and training time, which is an important problem that solving can save millions and save carbon emmission.

Weaknesses:
## Weaknesses
- I'm not sure about the soundness of some of the claims. The authors argue that LayerNorm is **equivalent** to RMSNorm and even write explicitly `PreLN Trasnformer = Pre-RMSNorm Transformer` (Eq 4). However, they only show one direction of this equality: the authors show how to take a Pre-LayerNorm transformer, and by imposing specific values to weights and enforcing constraints it can be equivalent to RMSNorm. That is, by constraining LayerNorm you can achieve RMSNorm. To my understanding, this means that Pre-LayerNorm is **at least** as expressive as Pre-RMSNorm. But what about the other direction? Maybe Pre-LayerNorm is more expressive if we do not impose these constraints? Can we take a Pre-RMSNorm and make it a Pre-LayerNorm?
- Similarly, I am not sure that the authors prove both directions when claiming in Section 3.2 that `Pre-RMSNorm = Pre-C-RMSNorm`.
- Novelty - The authors claim to ""propose Pre-RMSNorm"". Is this proposal novel? aren't models such as LLaMA already using Pre-RMSNorm?
- Another issue with soundness: the authors write multiple times that these conversions are ""free efficiency improvement"" and ""free lunch"". However, in Line 228 they write: ""We have to pay the extra cost for training for the parameter change"". So, what exactly do the authors mean by ""free lunch"" if there is an extra training cost, and the accuracy is not guaranteed to be the same?
- Many evaluation details are missing. 
    - On which datasets do the authors perform the experiments? 
    - How can the authors perform training and inference with **GPT-3**? As far as I know, GPT-3 is not open-sourced. Further, the authors mention they used ""**GPT3 XL and 2.7B**"". What exactly do they mean here? What is ""**GPT3 XL**""? and what is ""**GPT3 2.7B**"", Is there such a model? The sizes of GPT-3 that I know about are much bigger than 2.7B. Where exactly this model was taken from and what exactly do the authors mean?

Limitations:
NA

Rating:
7

Confidence:
3

REVIEW 
Summary:
This study explores the relationship between Pre-RMSNorm and Pre-LN Transformers, demonstrating that these two variants can be theoretically reparameterized into one another. Additionally, the authors introduce a novel Transformer variant called Pre-CRMSNorm, which reduces one hidden dimension while maintaining the same representation power as Pre-LN and Pre-RMSNorm. Experimental evaluations on ViT and GPT models reveal that the proposed approach significantly improves computation efficiency during both model training and inference stages.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The findings are interesting and reasonable. The proposed approach is novel and demonstrates a new direction to improve model efficiency via parameterization. The authors conduct experiments on both image data (i.e., ViT) and natural language (i.e., GPT). 

Weaknesses:
1. The study would benefit from additional evaluations of the proposed method beyond computation efficiency. While the comparisons conducted on computation efficiency are useful, it is crucial to include discussions on the effectiveness of the resulting model. The absence of such results makes it challenging to assess the proposed method and its claims, particularly regarding training speedup. For example, it remains unclear whether the proposed reparameterization yields comparable training performance and stability when compared to Pre-LN Transformers.

2. The efficiency gain diminishes as the model size scales larger, which affects the empirical contribution of the proposed method. Additionally, without sufficient discussions on model performance, it is unclear whether the proposed method is compatible with low-precision data for model inference and training, which plays an important role in making deep learning efficient. Incorporating experiments with fp16 and bf16, as well as exploring the use of quantized models, would significantly enhance the paper and provide valuable insights.

3. The main finding of the study fails to provide a very deep insight beyond the reparameterization connection. Also, I feel the equivalent claim is a bit ambiguous, and would recommend the author phrase the claim as reparameterization. This is crucial because different reparameterizations for the same network can lead to substantial variations in training stability, training performance, and even inference performance. Emphasizing the reparameterization aspect would address this ambiguity.

Limitations:
The authors mentioned the potential compatibility issue with low-precision data types but did not provide adequate discussions on this issue. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes modifications to the popular transformer architecture's normalization mechanism in order to improve efficiency without sacrificing performance. It starts out with the baseline architecture Pre-LN (LayerNorm), and derives two new architectures: Pre-RMSNorm and Pre-CRMSNorm, which are inspired by the use of RMSNorm instead of LayerNorm mechanism in certain transformers in the literature to improve efficiency without noticeably affecting performance.

The paper formally proves the arithmetic equivalence of all three architectures. It also evaluates the two proposed architectures over the baseline by conducting experiments on ViT and GPT3.  It reports modest improvements in training time (around 2%) and slightly bigger improvements in inference time (1-9%) for a variety of different hyperparameter settings.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper sheds light on normalization mechanisms in transformer architectures by relating two of the most popular such mechanisms: LayerNorm and RMSNorm.

By starting out with the empirical observation in the literature that RMSNorm is more efficient than LayerNorm, it derives two new architectures based on RMSNorm and proves them to be arithmetically equivalent to the LayerNorm architecture.

The empirical evaluation is quite thorough, using both ViT and GPT settings, and demonstrates modest improvements in both training and inference times across a wide range of hyperparameter settings.

Finally, the presentation is crystal clear: the paper is extremely well written even for an informed outsider to follow.


Weaknesses:
The single major weakness is that the savings in training and inference time is not too significant (around 1-3% in training time and about the same or slightly more in inference time, depending on the setting).

The authors acknowledge this by noting that normalization is not the elephant in the room, and that attention and MLP dominate the computation especially as the model size grows.  They even show an empirical upper bound on the speedup (which is the time taken by the normalization mechanism as a fraction of the overall time -- around 12-18%).

A minor nit: The last sentence of the abstract ""Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by up to 10%"" is a bit misleading.  I would appreciate it if you paraphrase this sentence, by 1) separating out the training and inference time numbers, and 2) providing both lower and upper bounds instead of just the 10% upper bound.

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes two novel modifications to the Pre-Layer Normalization (Pre-LN) Transformers, introducing the Pre-Root Mean Square Normalization (Pre-RMSNorm) and Pre-Compressed Root Mean Square Normalization (Pre-CRMSNorm) Transformers. The authors aim to improve computational efficiency by simplifying LayerNorm to RMSNorm and introducing a lossless compression technique for zero-mean vectors. They claim these changes maintain the arithmetic functionality of the original models, reducing training and inference time by up to 10%. The paper includes extensive mathematical proofs and technical diagrams to support their claims. Experiments using Vision Transformer (ViT) and GPT models demonstrate the improvements in speed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper presents novel ideas in Transformer optimization, leading to improved computational efficiency. The use of compression in the Pre-CRMSNorm Transformer is particularly inventive. The detailed mathematical proofs and the experimental results provide a strong basis for the claims made, enhancing the paper's quality and clarity. Also, the potential for the method's applicability in various domains enhances the paper's significance.

Weaknesses:
Although the paper presents comprehensive mathematical proofs, the practical effectiveness of the proposed model is not extensively demonstrated. More empirical results across different domains and data types would further substantiate the authors' claims. The authors acknowledge that the proposed modifications might slightly increase the training workload, which could be a limitation for some applications. Lastly, the handling of vectors in $\mathbb{R}^{d-1}$ by accelerators is assumed, which might not hold true in all scenarios.

Limitations:
The paper acknowledges that the input and output of certain blocks are assumed to be zero-mean, which might not be the case in all scenarios. There might also be potential issues in the decompression stage of the CRMSNorm method that are not discussed. Further empirical validation is necessary to fully understand the impact of these modifications on various use cases and data types. Also, the proposed models might not be as efficient when the handling of vectors in $\mathbb{R}^{d-1}$ by accelerators is not optimal.

Rating:
7

Confidence:
3

";1
etYk6TeO2q;"REVIEW 
Summary:
The authors suggest a method of causal discovery for multivariate time series under the regime when the data is being sampled at constant skips in the time dimension. Under mild assumptions, they prove that their method works asymptotically.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper's contribution is clear, the methods are interesting and novel, and the subject (causal inference in multivariate time series) is of interest to many. Other than some places I will mention, the explanations are clear.

Weaknesses:
I would like to see a little more discussion on previous work. I see citations about work that inspired the current one, for example [17, 19, 25] on using descendants of an unobserved variable to differentiate direct causation from hidden mediation, but I do not see citations for competing methods for causality under the regime of sampling on the time dimension. It would be good to be explicit about what the current state of the art is in that direction, and how the current work compares. You mention them in Section 5 (SVAR-FCI, NG-EM, Dynotears, PC-GCE) but I'd like to see a bigger discussion on the differences. Specifically, it would be great if we had a basic example in mind where ""naive"" approaches clearly fail, and make us understand how the proposed method avoids the issue intuitively.

Limitations:
None.

Rating:
7

Confidence:
2

REVIEW 
Summary:
# Summary
In this paper, the authors address the problem of inferring causal structures from subsampled time series data, where the frequency of measurement is much lower than that of causal influence. This presents challenges in identifying the causal structure, as hidden variables at unobserved time steps can induce bias. Existing methods that tackle this problem are limited to linear cases or fail to achieve identifiability.

The main contribution of this paper is a constraint-based algorithm that can identify the entire causal structure from subsampled time series without any parametric constraints. The authors propose a proxy-based causal discovery algorithm that leverages the temporal structure of time series data to remove the bias induced by hidden variables. The algorithm is nonparametric and can achieve full causal identification. Specifically, the author leverages the proxy variables to test the edge directions of the summary DAG from the uniquely identified MAG. 

The authors demonstrate the theoretical advantages of their method and provide experiments on both synthetic and real-world data, showcasing improved performance over existing methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
# Originality

The paper presents a novel approach to causal discovery in subsampled time series data by proposing a constraint-based algorithm that leverages proxy variables to handle the challenges posed by hidden variables at unobserved time steps. The originality of the method lies in its ability to identify the entire causal structure without any parametric constraints, setting it apart from existing methods that are limited to linear cases or fail to achieve full identifiability. The authors draw inspiration from the recent progress in proximal causal discovery and adapt it to the subsampled time series setting.

# Quality
The quality of the paper is high, as it presents a well-formulated methodology with solid theoretical foundations. The authors provide rigorous proofs for their proposed algorithm's identifiability properties, ensuring that the algorithm is grounded in strong theoretical underpinnings. Although I have some questions regarding its generality and required assumptions, I will elaborate about them later.

As for the experiments, the author conduct 1 synthetic and 1 real-world experiment, which demonstrates its effectiveness against the baselines.

# Clarity

The paper is well-written and clear in its presentation. The methodogy is clear, thanks to the intuitive explanation provided by the authors. They also provide necessary background information on causal discovery and related literature. 

# Significance

The problem that this paper aims to talks is significant in the field of tmeproal causal discovery. Although I don't think this paper fully addresses it, it made a reasonable progress towards the final destination. 

Weaknesses:
# Weakness
Although the paper provide a solid progress towards the final destination, here are some limitations I am a bit worried about: 

## Assumptions are too strong:  
One assumption is that only 1 step window is considered in this paper. However, is this true in practice? Is it possible that some factors has long-standing effect (higher-order Markovian) to the target variable? Can your method handle such scenario? 

For summary DAG, does it has to be a DAG? Since from general full-time DAG, the summary graph may not be a DAG and can contain cycles. Does your method handle such cases? For example, A(1) -> B(2) and B(1) -> A(2), and the summary graph is A<->B. 

What are the consequence if some variables are not self-caused? That means the proxy variables may not exists. In that case, full identifiablity cannot be establish, right? You may want to add something like to what extend of identifiability your method can establish without such assumption. 

## Comparison with baselines
The author includes some of the well-known baselines. Surprisingly, Dynotears performes reasonably ok considering that it is a linear model (in its original form). However, after Dynotears, several state-of-the-art baselines are proposed, like Rhino [1]. Can you compare your method with this nonlinear model?

[1]Gong, Wenbo, et al. ""Rhino: Deep Causal Temporal Relationship Learning With History-dependent Noise."" arXiv preprint arXiv:2210.14706 (2022). Code at https://github.com/microsoft/causica/tree/v0.0.0 

## Scalability

How expensive it is to run such algorithm, since you need to test every pair. For real-world experiment, you scale it to 90 variables, how long does it need to run it? If your method can be scaled to higher-order SVAR, how does it scales with the window size?

Limitations:
The author did discuss its limitations but they only discuss the limitations of conditional independence test. Howver, in the weakness section, I have raised several improvements the author can consider. I suggest including these discussions in the limitations as well. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposed a non-parametric constraint-based algorithm that can identify the entire causal structure from subsampled data, which leverages the proxy variable to adjust the bias induced by the hidden variable.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Concise and clear theoretical derivation. Introduce the proposed method by discussing the connections between different graphs step by step.


Weaknesses:
- The paper is rather incremental as the core of the method is based on [19] to use proxy variables to detect and eliminate the confounding effect brought by the subsampling.  
- Moreover, the method proposed by [19] seems to require more assumptions but is not disclosed in this paper. And it is unclear whether those specific assumptions can be satisfied in this work.
- In addition, this work assumes the self-exciting property of the time-series which is not necessarily held and it is interesting to see how to find the proxy variables and what would be the result in this case.
- It could be better to provide more than 5 vertex numbers in the experiment.
- What would be the result if the causal graph becomes denser?


Limitations:
Yes.

Rating:
5

Confidence:
5

REVIEW 
Summary:
In this paper, the author(s) propose a new technique to learn the summary graph of time-series data. As a motivation for their work, the author(s) discuss the interesting application of learning causal pathways in Alzheimer’s disease. The time series model studied in this work is quite general, and suitable for many applications. The author(s) provide a simple algorithmic approach to learn the summary graph. This algorithm is essentially based on verifying d-separation, by testing conditional independence. The author(s) conclude with an experimental comparison, showing that their method achieves superior performance than a baseline, using various synthetic datasets and a real-world medical application.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper studies an very important and difficult problem. The application of learning causal pathways in Alzheimer’s disease is very relevant. The paper is also well-written and easy to follow.

Weaknesses:
My main concern pertains the faithfulness assumption. Faithfulness it is useful for getting identifiability results, and it allows to recover the causal structure by testing conditional independence. However, in many scenarios there is no practical reason to assume faithfulness. The use of faithfulness significantly limits the novelty of their work. If I understand correctly, the implementation of their algorithm essentially uses conditional independence to recover the causal structure. Testing conditional independence is practically problematic. Hence, I also have doubts on the scalability of their method.

Limitations:
Yes, the author address limitations in Section 6.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of subsampled time series in causal discovery, in which the unobserved time steps may lead to the existence of latent confounders. To this end, this paper proposes a constraint-based algorithm by leveraging proxy variables to remove the bias induced by latent confounders. The experimental results verify the effectiveness of the proposed algorithm.


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper addresses the problem of subsampled time series in causal discovery, which is important but challenging.

2. The paper is well-structured and written.

3. The experimental results show that the proposed method outperforms several representative baselines.

Weaknesses:
1. In Theorem 2.8, extra assumptions are required for testing conditional independent relations in related literature, but they are not discussed in this paper. 

2. How to search the proxy variable of target hidden variables. If an invalid proxy variable is selected, what is the output of the proposed algorithm?


3. In simulation data, the dimension of the random graph is only five. Can you show the performance of the proposed method in a larger-scale network?

Limitations:
Refer to Weaknesses

Rating:
6

Confidence:
3

";1
muFvu66v7u;"REVIEW 
Summary:
- The paper investigates the use of Lipschitz constrained networks to replace clipping functions and limit gradient sensitivity in DP-SGD.
- Lipschitz constrained networks are utilized as an alternative to clipping in order to address the issues of clipping's impact on convergence and performance in DP-SGD.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The idea of removing clipping as an alternative to clipping itself is promising, as clipping is known to have detrimental effects on convergence and performance of DP-SGD, even without noise addition [1].
- The paper introduces the replacement of Vector-Jacobian product with Scalar-Scalar product to reduce computational complexity. The proposed methods outperform existing SGD approaches by a significant margin in terms of speed, which is crucial as memory usage and time inefficiency are major drawbacks of DP-SGD.

[1] Differntially Private Shaprness-Aware Training (ICML’23)

Weaknesses:
- Please refer to the questions.
- (Minor) There are several typos, such as the use of ""cotangeant vector"" which sounds little awkward, and inconsistencies in figure references (e.g., Fig 4 vs. Figure 5). Please carefully review the grammar and correct the typos.

Limitations:
The paper provides a detailed discussion of its limitations.


Rating:
5

Confidence:
4

REVIEW 
Summary:
Differentially Private (DP) Deep Neural Networks (DNNs) face challenges in estimating tight bounds on the sensitivity of the network’s layers. Instead, they rely on a per-sample gradient clipping process (as argued by the authors). This process not only biases the direction of the gradients but also proves costly in both memory consumption and computation. To provide sensitivity bounds and avoid the drawbacks of the clipping process, the authors provide a theoretical analysis of Lipschitz constrained networks, and uncovers a previously unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, the authors argue it will guarantee DP training of these networks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well-structured and clearly written.

The theoretical part is simple and easy to follow.

Weaknesses:
Estimating Lipschitzness with respect to parameters may not be necessary. If the network is Lipschitz continuous with respect to the input, its gradient will be bounded, and thus the weight update will also be bounded. So, the motivation may not be rational.

Experimental results do not support the arguments. The validation accuracy of the DP-SGD is lower than several referenced works.



Limitations:
See weaknesses and my questions

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of efficiently bounding the sensitivity of gradients in DP-SGD by using special architectures the layers of which can be proven to be Lipschitz with respect to the parameters, hence bounded gradient. They show how to recursively calculate the sensitivity of a sequence of layers, and incorporate the method in an algorithm to perform private SGD without clipping.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The writing is exceptionally lucid. The concept is original and potentially significant, although there are at present many limitations.

Weaknesses:
There are a lot of constraints on the architecture that severely limit the potential of the method for short-term impact. I'm torn, because introducing the concept at this stage is of value, but far more work must be done -- both theoretical, in establishing the requisite bounds for popular architectures -- and experimental, in demonstrating that the approach achieves good points on the privacy/utility/efficiency Pareto frontier -- before we can assess the significance of the work.

I'm not convinced that it isn't a major problem that the gradients can vanish during training. This is the reason for the success of adaptive (layer-wise) clipping strategies. In particular see ""EXPLORING THE LIMITS OF DIFFERENTIALLY PRIVATE DEEP LEARNING WITH GROUP-WISE CLIPPING"" which would seem to enjoy the efficiency of your approach without the drawbacks of vanishing gradients or restricted architecture class.

Limitations:
Limitations are honestly and adequately discussed. No potential negative societal impact.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper studies the question of how to do differentially private optimization without using per-sample gradient clipping, in order to simplify and speedup the iteration cost.
The paper proposes to restrict the class of functions to feed-forward neural networks for which it is feasible to compute bound on the gradient norm (Lipshitz constant), and proposes to compute adaptively the bound on the gradient norm (layer-wise) at every step of DP-SGD depending on the current iterate point. Paper provides the description of the algorithm, as well as evaluates its practical behavior.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- An efficient implementation of the algorithm is provided.
- Experiments show that per-iteration runtime of the proposed algorithm is indeed faster.
- Overall the paper is interesting and novel and provides a new direction for future research.

Weaknesses:
1. No clear comparison of the proposed algorithm to the baseline method (DP-SGD) is given in terms of the final accuracy. When restricting to the same architecture, it is unclear if the proposed algorithm can still reach the good accuracy compared to the classical DP-SGD with gradient clipping. Without clipping the gradients, the amount of the added DP noise to each gradient is larger than if you clip the gradients, which might hurt the final performance. 
2. From the experiments on CIFAR10 one might conclude that for the same privacy $\epsilon$ the final accuracy of the baselines is much better than of the proposed algorithm, which makes the proposed algorithm not applicable.
3. In the “local” strategy (line 201), how exactly did you calculate the amount of the noise to be added? I did not find a clear description of the “local” strategy, and how it is different from the “global” strategy.
4. Some parts of the paper are not very clearly written (see questions below).

Limitations:
yes

Rating:
6

Confidence:
3

";0
eLH2NFOO1B;"REVIEW 
Summary:
This paper extends on existing works on Flow Matching with minibatch OT solutions to the case of invariant cost functions. In particular, where the invariance is given by an SO (permutation + rotation) group. This is mainly a method of correcting the minibatch bias, since (non-equivariant) minibatch OT will still converge to the correct mapping when the minibatch size goes to infinity. Empirically, it is shown that equivariance OT matching results in lower transport costs, and hence shorter path lengths which may indicate that it is computationally faster to simulate after training than non-equivariant OT (though not shown).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
  - Well-written and easy to understand the high-level idea.

  - Straightforward extension of existing works to training equivariant flows.

Weaknesses:
  - Eq 13 and 14 are not easy to solve, and it is unclear how much compute (or wall-clock time) these subproblems require.

  - Lack of comparison to other baselines (equivariant diffusion models, standard flow matching).

  - The empirical differences between equivariant OT and OT seem marginal.

  - While the paper discusses transport cost, it is not shown that these translate to faster sampling algorithms.

Limitations:
Overall, I think the benefits of equivariant OT compared to OT (and regular flow matching) can be showcased more. Theoretically, I can believe what the authors are claiming, but empirically I don't quite see these emphasized as part of the empirical results yet. I suggested some ideas for additional plots in the Questions section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The manuscript builds on recent progress in simulation-free loss functions for continuous normalizing flows which allow to scale CNFs to significantly larger dimensions and can be thought of as a generalizion of continuous time diffusion models. Specifically, the manuscript extends the recently proposed conditional flow matching approach which proposed to use results of optimal transport to construct particular efficient probability paths.

A notable novelty of the paper is to consider the question of how to incorporate equivariance in the conditional flow matching loss. This is achieved by modifying the cost matrix c(x, x') of the Wasserstein distance to be the minimal cost *over the entire orbit* of x' (or equivalently x). 

The proposal is studied experimental in the context of Boltzmann generators which are normalizing flows trained to act as sampling density for importance weighting (or, alternatively, Markov Chain Monte Carlo) of unnormalized physical target densities of Quantum Chemistry. Normalizing flows are particularly suited for this task as they provide a tractable likelihood as well as fast sampling. Equivariance is of pivotal importance for Boltzmann generators since the studied physical systems often have a high degree of symmetry, e.g. SE(3) and permutation symmetry. 

The paper establishes in detailed numerical experiments that the flow matching approach is beneficial in the context of Boltzmann generators and compares favorably to likelihood-based training. This is shown for standard benchmarks such as Lennard Jones particles and Alanine dipeptide.  

Overall, I think however this is a valuable contribution in a very exciting and rapidly evolving field of research. I therefore tend to recommend acceptance. I would however encourage the authors to add an appendix summarising the conditional OT flow matching procedure of 2302.00482 on which their method builds. Unfortunately, the conditional flow matching paper is not the most readable and it would make the ms for self-contained and accessible to add a brief summary.



Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper establishes, to the best of my knowledge, for the first time that flow matching is benefical in the context of Botzmann generators. It also presents a detailed comparison to both likelihood-based as well as OT conditional flow matching training objectives on standard benchmark sets.
- The question of how equivariance is of high relevance as CNFs are widely deployed in physics applications for which symmetry is a fundamental ingredient for successful learning.
- The presentation is well structured and clear


Weaknesses:
- The proposed method is rather specific to the case of SE(3) and permutation invariance. It seems non-trival to me how one would extend the treatment to larger symmetry groups, such as in applications to Lattice Field Theories. 
- A crucial element of Boltzmann generators is that they are often trained using self-sampled energy training (Variational Inference). The flow matching condition does not allow for such type of training. In the original conditional flow matching paper, a reweighting procedure was proposed. I think the authors made a good choice in not discussing this in this contexts as it is to be expected that reweighting will fail for reasonably sized systems such as alanine dipeptide. Nevertheless, the fact that (an efficient version of) energy-based training is not available for flow matching is a major downside of flow-matching.
- I am a bit unclear on how much the proposed equivariant method actually helps. There is a substantial gain in the LJ55 setup while on Alanine dipeptide the previously proposed OT flow matching leads to higher ESS (at the cost of slightly longer trajectory length). 

Minor comments:

P.3 L.80-81: Continuous time diffusion models provide a tractable likelihood. In fact, they are continuous normalizing flows albeit trained with a different objective as can be seen by noticing that the reverse process is equivalent to a deterministic ODE (known as probability flow ODE).

P.3. L.100: distribution -> density

P.3 Eq 3: I find the notation a bit contrived. Why not simply use f_\theta(t, x)?

P.4 L .111: Jacobian trace -> trace of Jacobian or divergence

P.4 Eq 6: x_1 \sim \mu(x_1) -> x_1\sim \mu

P.4 L. 137 \sim q(x_0) -> \sim q and \sim \mu(x_1) -> \sim \mu

P.6 Table 1: Please mention in the caption how the errors are determined. Most readers will wonder about this while looking at the table so it would be good to provide this information already here.

P.6 Eq 15-18: Various commas are missing after the equations

P.6 L. 208: mean free Gaussian is not equal to \mathcal{N}(x_0| 0, 1), as it has different normalizer. 


Limitations:
Limitations are properly addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies equivariant flow matching, an extension to the flow matching paradigm for training continuous normalizing flows by regressing parametric vector fields to conditional vector fields. Building upon prior work, conditional vector fields can be derived from the distribution of the 2-Wasserstein optimal transport map $\pi(x0, x1)$  between the prior $q(x0)$ and the target $\mu(x1)$. The central contribution of this paper is to bake symmetries into the optimal transport cost $c(x_0, x_1)$ through a sequential search procedure for the symmetry group $S(N)$ and $SO(N)$. This is done by first applying the Hungarian algorithm followed by finding a rotation matrix that minimizes the cost, for each element in the cost matrix. Finally, the authors parameterize an equivariant vector field using the EGNN architecture of Satorras et. al 2021. Experiments are done on $n$-body particle dynamics and training a Boltzmann Generator for small proteins in Alanine dipeptide. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
**Originality**

The main strength of this paper is that all presented material follows naturally from the desire to bake in equivariance in continuous normalizing flows. As equivariance has already been studied extensively in the generative modeling literature this application to flow matching is a reasonable extension. The originality of this work is limited to solely baking symmetries in the cost matrix as equivariant vector fields using EGNN have already been employed in the literature e.g. $E(N)$-normalizing flow (Satorras et. al 2022). 

**Quality**

The quality of the presented work is a good first attempt at tackling the problem of equivariant flow matching, but unfortunately, it is below the standard that is expected on several fronts which are outlined in the weaknesses section. 

**Clarity**
In general, the work is fairly clear. The presented ideas are straightforward to grasp but a few technical details are omitted which could improve understanding and readability. For example, how do you align the rotations after performing the Hungarian algorithm? We are minimizing over $SO(D)$, this is a non-trivial manifold. Of course, the appendix and code have this information but given that this is a crucial part of the contribution more detail would improve readability.

**Signifance**
As equivariance in generative modeling has largely been studied in the literature the contribution of this work is limited. While equivariance has not been exclusively studied in flow matching, this extension is an early step and has limited novelty. This would be fine if there were large benefits empirically, theoretically, or computationally but this is not the case as far as I can tell and as a result, this paper has limited significance at present.


Weaknesses:
This paper has many potential weaknesses some of which are already alluded to in the previous section. 
Firstly, it has very little novelty as many of the concepts regarding equivariance and flows are known in the literature. In fact, using EGNN in normalizing flows for this very symmetry group has already been studied by Satorras et. al 2022. Moreover, one of the main benefits of flow matching is that one sidesteps the training complexity of regular CNFs as we do not need to backprop through an ODE solver. This brings huge computational benefits during training, albeit inference is still costly. In this paper, training is also expensive as solving for the optimal coupling in mini-batch OT is already expensive but even more so because the Hungarian algorithm is employed which scales $O(N^3)$. This is prohibitively expensive for any large-scale machine learning system. I encourage the authors to investigate other means of approximately solving for permutations. Some examples and directions for investigation can include learning the permutations (see Git Rebasin Ainsworth et. al 2022) using the straight-through estimator, or using Gumbel Sinkhorn (Mena 2018).

With regard to the proposed approach, the authors break down the problem by doing a sequential minimization by first finding the best permutation and then finding the best rotation. But this is not the original problem which may be indeed intractable. This is a ripe avenue to do a bit of theory to justify the proposed approach. For example, why is this sensible and not the first thing one can do? Can we do a little bit of error analysis to bound the error of the optimal cost matrix found in the sequential search to the actual one?

The experiments section is also rudimentary. While I appreciated the visualization of the shorter and straighter OT-Paths it is difficult to rationalize the gains here knowing that the overall algorithm is likely more expensive. A detailed time-complexity analysis of the overall method is needed here and proper comparison to regular flow matching and $E(N)$ NF is also a good idea. Moreover, the results seem a bit mixed as equivariant models do worse on Alanine dipeptide compared to non-equivariant models. The current explanation in the main text for this result is unsatisfactory. We expect equivariance to help here, why is it not?


Limitations:
Yes

Rating:
3

Confidence:
5

REVIEW 
Summary:
The authors propose equivariant flow matching, which provides a way of incorporating syymetries into flow matching objectives.
Specifically, they propose to replace the squared Euclidian distance cost used in the flow matching objective, with (approximately) the minimum squared Euclidian distance over all possible group actions.
They demonstrate in their experiments that this improves training of equivariant flows - obtaining similar or improved performance than vanilla OT flow matching with shorter paths.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 - They identify a clear problem of how symmetries (especially permutation symmetries) cause issues with OT flow matching, resulting a (possibly prohibitively) large batch size required.
 - Their solution is simple and fits the problem well.
 - Their experimental results are consistent with the theory (their CNF has shorter paths).

Weaknesses:
 - The performance results are mixed - the flow trained with vanilla OT is sometimes better.
Specifically for alanine dipeptide the flow trained with vanilla OT performs better - thus the Equivariant flow matching method does not seem relevant to one of the headline results ""for the first time we obtain a Boltzmann generator with significant sampling efficiency without relying on tailored internal coordinate featurization"". 
On first read of the abstract the wording makes it seem like this result was obtained due to the authors proposed  method rather than applying the existing flow matching technique to the problem.
 - A CNF trained with a classic score matching loss instead of flow OT would be a good obvious baseline but is not included. 

Limitations:
The Equivariant flow matching method results in longer training times for LJ55 and alanine dipeptide, but no discussion of why / analysis of this is provided (see Questions). 
One of the headline results from the abstract (re alanine dipeptide) is obtained using an existing method (without the new proposed method). 

Rating:
6

Confidence:
4

";1
EjiA3uWpnc;"REVIEW 
Summary:
This work presents a new model architecture that combines the coefficient learning scheme with coordinate-based residual operator layer for learning mapping between continuous function in 3D Euclidean space. The authors further show the proposed model can be interpreted as graph convolution on a graphon. Experiments on various datasets show the effectiveness of the proposed model.


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Generalizing neural operator learning to 3D Euclidean space with equivariance guarantee is an interesting and underexplored research question.
2. The authors provide theoretical explanation to the model by generalizing graph convolution to graphons.
3. The proposed method performs better than other neural operators in considered datasets.

Weaknesses:
1. The paper is generally hard to follow as many parts are not clearly stated or explained. For example:
    - Why the target feature function $\rho$ can be expressed as the equation in line 117? What is the rationale behind the equation?
    - Why the basis function is constructed as in equation 4? How it is connected with atomic orbitals? Is such a construction necessary for achieving equivariance and good empirical performance?
    - How is the spherical tensor in section 3.3 connected with the basis function in the construction of $S$?
    - What is the motivation of using residual operator layer? Why it can mitigate the expressivity issue?
2. Following above, the presentation could be significantly improved by giving some preliminary introductions of the key technical parts used in the method, e.g. coefficient learning, concepts and background in quantum chemistry, TFN.
3. Since the method appears to be a mixture of existing approaches from different areas, it is unclear what is the novelty of this work compared with prior art on methodology aspect.
4. The authors claim the proposed neural operator to be a general framework for learning mapping between functions but only test on electron density datasets. The chosen baselines are somewhat outdated, most mainstream equivariant GNNs e.g. [1] are not considered.

[1] E (n) equivariant graph neural networks

Limitations:
There is no negative societal impact as far as I can tell.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes an architecture for learning mappings between signals/functions, which is equivariant to rigid body transformations. The architecture is based on a spectral graph convolutional network based on a novel design of a graphon/graph shift operator (or sequence of graphons/graph shift operators). 

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The construction of the graphon/graph shift operator underlying the spectral graph convolutional network is motivated by geometric and physical considerations. The method seems to achieve good results in experiments.

Weaknesses:
The construction of spectral convolution for graphons is well known, and not a contribution of this paper. See for example works by Alejandro Ribeiro, Luana Ruiz, Ron Levie, Gitta Kutyniok, and Nicolas Keriven.

The exposition and mathematical formulations are often not clear and inaccurate. It is hence hard to follow the paper. For example, the description about the construction of the graphon is not clear. Is there a single graphon, or is there a different graphon for each pair of basis elements? Moreover, the discussion around bases lacks mathematical rigor.  
There seems to be a confusion between SE(3) and SO(3). Unless I am missing something, the theorems show equivariance with respect to SO(3), but the abstract and introduction talk about SE(3).
See more details in “questions.”

The exposition of the method and its motivation could be explained better. What the paper does is define a good graphon (or sequence of graphons) for problems (primarily) in quantum chemistry, and uses this graphon (or this sequence of graphons) in spectral graph convolutional networks.

It seems like this paper has potential, but it is not ready for publication at its current state.



Limitations:
Limitations are discussed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a novel framework for learning mappings between continuous functions in the 3D Euclidean space. First, the framework combines the coefficient learning scheme and the residual operator layer to maintain the sensitivity of the model to equivariance. In addition, the proposed approach utilizes both continuous and discrete graph structures of the input data to efficiently capture geometric information. Experimental results on a large-scale electron density of datasets show that the proposed model outperforms the current state-of-the-art architectures.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper proposes a method to ensure that the model preserves equivariance by introducing a residual operator layer in the coefficient learning framework. The paper discusses related work, outlines the approach, presents experimental results, and concludes with the potential implications of the proposed approach. The strengths of this paper are the focus on the 3D Euclidean space problem, the innovation of the equivariance techniques, and the emphasis on discrete structural information.

In terms of contributions, this paper introduces a new equivariant neural operator learning framework for the 3D Euclidean space problem. The framework effectively combines coefficient learning and residual arithmetic to leverage the strengths and mitigate the weaknesses of existing approaches. In addition, this paper provides a detailed theoretical explanation of the proposed neural operator learning scheme from a graph spectrum view, treating the proposed model as applying the transformation to a spectrum of continuous feature function. Experimental results conducted on a widely used large-scale electron density of datasets validate the superior performance of the proposed approach.


Weaknesses:
Some issues are as follows:

1. Although the proposed approach performs well in this paper, further baseline methods from recent years need to be added for comparison to more fully validate the effectiveness of the proposed approach.

2. It is worth noting that the proposed method might demand substantial computational resources, and its computational complexity is not explicitly discussed. As a result, its scalability to handle very large graphs might be a concern.

3. It is also necessary to pay attention to the writing standard of the paper format. For example, the title names of sections 2.1 and 3.3 are the same and need to be corrected.


Limitations:
While the paper does not explicitly state the limitations of the proposed method, it is important to consider certain factors that may affect its effectiveness. Specifically, the performance of the method may vary depending on the characteristics of the particular input graph. Further research is necessary to assess the generalizability and scalability of the proposed method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a method for learning SE(3) equivariant continuous functions in R^3 given an approximation of a continuous input and structural information in the form of a graph. The authors represent continuous inputs using ideas from coefficient learning (i.e., learning functions as the sum of learned coefficients multiplied by elements of a predefined basis), and incorporate structural information from discrete graphs by centering the basis functions at the node coordinates. To improve the quality of the learned function a residual term inspired by coordinate-based interpolation methods is used. The authors also provide a graph spectral interpretation of their method showing that the approach resembles graphon convolution. The proposed method outperforms baselines.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* Originality: The work appears to be original. Several components of the proposed method come from existing efforts, however, the method to integrate structural information and the graph spectral interpretation appear to be new.
* Quality: The work appears to be of very good quality.
* Clarity: The paper is well written and the ideas are communicated in an accessible way.
* Significance: This paper proposes a learning based approach for electron density estimation. Traditional approaches are computationally expensive, and therefore do not scale well. The proposed approach addresses this challenge by allowing for estimation in a feedforward deep learning framework. 

Weaknesses:
* Quality: Although the model is constructed to be SE(3) equivariant, this doesn’t seem to be reflected very well in the analysis. Can the authors explain why the error for rotated QM9 is so high relative to the unrotated version in table 1 and 2?

Limitations:
The authors have communicated limitations of their work

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces an rchitecture combining a coefficient learning scheme with a residual operator layer for learning mappings between continuous functions in 3D Euclidean space designed to achieve SE(3)-equivariance. The proposed method can be seen as convolution on graphons, which are dense graphs with infinitely many nodes. They claim that their approach, called InfGCN, captures geometric information while preserving equivariance by leveraging both the continuous graphon structure and the discrete graph structure of the input data.
The authors perform experiments on large-scale electron density datasets, and they claim that the proposed model demonstrates significant improvement over current state-of-the-art architectures. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and the topic is very relevant. The notation is clear despite the technical nature of the work.

Weaknesses:
A bit more clarity in the definition of equivariant is desired. Even though SO(3) irrep representation are mentioned, it should be clearer which group is referred to, e.g., SO(3), O(3), E3, etc?

Limitations:
The authors discuss the limitations of their work. 

Rating:
8

Confidence:
3

";1
VsbrdJpwpT;"REVIEW 
Summary:
The paper tackles user-oriented tasks, e.g. personalized recommendation, and proposes a self-supervised method named AdaptSSR to replace the contrastive learning pre-training target task. It adopts a ranking loss that selects samples of smallest similarity differences and assigns dynamic weight coefficients to ranking parts based on the estimated similarity between the augmented views. Experiments on 6 downstream tasks from 2 datasets and several empirical analyses are conducted to verify the effectiveness of AdaptSSR.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The pre-training method and objective are clearly explained, and the equations in the text concisely demonstrate the proposed loss function.

Weaknesses:
The Multiple Pairwise Ranking loss, which is the core of the method, is not an original contribution of this paper, but an adaptation from Yu et al [49]. However, there is almost no mentioning of this work except for the source of the loss function, casting doubt on the novelty of the paper.


Limitations:
The authors have not discussed the limitations and broader societal impacts in the paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Recent studies have explored pre-training user models with contrastive learning tasks to address data sparsity issues in user-oriented tasks. However, existing augmentation methods may introduce noisy or irrelevant interests, leading to negative transfer. To overcome this, a new approach called Augmentation-Adaptive Self-Supervised Ranking (AdaptSSR) is proposed, which replaces contrastive learning with a multiple pairwise ranking loss. An augmentation-adaptive fusion mechanism is also introduced to combine learned ranking orders based on the similarity between augmented views. Extensive experiments demonstrate the effectiveness of AdaptSSR across various tasks and datasets.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
1. The paper's motivation is reasonable, directly adopting contrastive learning may lead to consistency problems in recommendations.
2. This paper proposes a novel approach to combine implicit and explicit augmentations.

Weaknesses:
1. The main contribution of this paper is adding an order constraint in the loss function, 
   which is a rather incremental modification of existing contrastive learning framework. 
   The main idea of paper is a fusion of explicit augmentation and implicit augmentation by the loss function.
   Thus, the novelty is limited.

2. It's unclear whether the added constraint is necessary. Since ""$u$ and $u^+$ originate from exactly the same input behavior sequence"" as the authors commented in line 123, I think $sim(u, u^+)\ge sim(u, u^-)$ and $sim(u, u^+) \ge sim(u, \hat{u}$ should always hold. I don't understand why we need the $sim(u, u^+)$ term here. Without the $sim(u, u^+)$ term, the proposed method reduces to common contrastive learning.

3. Even if the constraint is meaningful, the authors' analysis cannot convince me why such constraint may help generalization. Why may $sim(u, \hat{u}) > sim(u, u^+)$ harm the downstream performance? Intuitively, suppose the objective of original contrastive learning is overly strong, we should loose the constraints. For example, $sim(u, \hat{u}) \ge sim(u, u^-) - \epsilon$. However, the authors make the constraints even stronger by adding another constraint term. This does not make sense to me.

---

Edit after rebuttal: The authors' response resolved my primary concern about technical correctness. I'd like to raise my score to a borderline reject regarding the novelty of this work.


Limitations:
Please address the issues highlighted in the Weaknesses section.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes Augmentation-Adaptive Self-Supervised Ranking (AdaptSSR), a new user model pre-training paradigm, which alleviates the requirement of semantic consistency between the augmented views while pre-training a discriminative user model. Conventional methods assume that different views of the same behaviour sequence constructed via data augmentation are semantically consistent, while in practice existing augmentation methods tend to lose certain interests of the user or introduce noisy interests that the user does not have. AdaptSSR addresses this issue by adopting a multiple pairwise ranking loss which trains the user model to capture the similarity orders between the explicitly augmented views, the implicitly augmented views, and views from other users. An explicit hard negative sampling strategy and an augmentation-adaptive fusion mechanism are also introduced to facilitate model training. Extensive experiments on both public and industrial datasets verify the effectiveness of AdaptSSR.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed approach is technically sound and the empirical results validates the effectiveness of the method.
- The paper is well-written with very clear figures.
- Code is available which makes it easy to reproduce the results.

Weaknesses:
An important hyperparameter sensitivity analysis is missing: how does the value of $\lambda$ affect the model performance? Compared with existing models, AdaptSSR introduces an extra SimCSE-inspired implicit augmentation approach. It remains unclear in the paper if the performance improvement is primarily due to the introduction of implicit data augmentation. 

Limitations:
The extra computational cost introduced by AdaptSSR is not analyzed in the paper, it would be useful if the authors can demonstrate the tradeoff between training time and performance for various models.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors tackle the problem of doing self-supervised learning for user modeling.  Inspired by the successes of contrastive learning approaches in the image setting, they adapt contrastive learning to the user modeling setting.  However, in user modeling the augmentations typically used are not very suitable for contrastive learning because they can change the semantics of the data, thus forcing similarity between augmented views can be problematic.  They instead produce three views: the anchor, a similar ""implicitly"" augmented view, and a less similar ""explicitly"" augmented view.  The implicitly augmented view is trained to be more similar to the anchor than the explicitly augmented view.  This escapes the problematic similarity training that plain contrastive learning would have in user modeling.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.  The paper was well-written and the diagrams were easy to understand.  It made the paper easy to read and review.  The contributions were clearly stated and explained in the paper.

2.  The method is novel and original as far as I know.  This method could be generalizable to other domains where augmentation choices are not straightforward and could alter the semantics of the data.

3.  The method is well-designed: the ranking loss does help mitigate the ""make semantically different augmented views the same"" problem, and furthermore helps balance the focus of the loss between the implicit vs explicit contrast and the explicit vs other user contrast.  

4.  The improvements in the empirical results are consistent and seem to be significant.  


Weaknesses:
1.  Going back to the example where the user behavior is represented by a sequence of images, is it possible to just do per-image augmentation (choices for these exist and are widely used) and then perform a typical InfoNCE style contrastive loss on the user embeddings?  For text one could do something similar using masking augmentations and such.  I did not see a comparison to this baseline and I wonder how well it would perform.  I think this is something that would be critical to compare against.

2.  While the paper is written well, I think the paper should define what user modeling is and what the downstream tasks are earlier in the paper (or in the abstract).  For a while it was not clear to me what problem the paper was trying to solve, as someone who has not worked on user modeling.

3.  Adding error bars into the results tables would help in understanding the significance of the results.

Limitations:
Seems sufficient.

Rating:
6

Confidence:
4

";1
MLIs5iRq4w;"REVIEW 
Summary:
The paper discusses the distillation of pre-trained diffusion models. The paper introduces a framework called Diff-Instruct, which allows for the transfer of knowledge from pre-trained DMs to other generative models in a data-free manner. Diff-Instruct utilizes a mathematical foundation based on minimizing a divergence called Integral Kullback-Leibler (IKL) divergence. This divergence is specifically designed for DMs and is more robust in comparing distributions with misaligned supports. The text also highlights connections to existing works such as DreamFusion and generative adversarial training. The effectiveness of Diff-Instruct is demonstrated through two scenarios: distilling pre-trained diffusion models and refining existing GAN models. Experimental results show that Diff-Instruct achieves state-of-the-art performance in single-step diffusion-based models and consistently improves pre-trained generators of GAN models across various settings.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
* The paper tackles the problem of distilling diffusion models and addresses the important topic of accelerating inference for DMs. 

* The proposed method utilizes a novel formulation based on the Integral Kullback-Leibler (IKL) divergence, which adds further interest to the approach. The authors make an effort trying to explain why this divergence is a reasonable choice for distillation.

* The experimental results showcased the effectiveness of this method, particularly in the context of 1-step distillation on datasets such as CIFAR and ImageNet64, where the results were highly promising. 

These findings demonstrate the potential of the proposed method for enhancing the efficiency and performance of diffusion models in small domains.

Weaknesses:
* While the distillation results for CIFAR and ImageNet using the proposed method are good, they may not have practical utility in larger domains. Distillation becomes particularly crucial for such larger domains, and it remains unclear whether the method will be effective with larger images. 

* Additionally, although the method doesn't require additional data, it does necessitate the use of two extra models, namely the student model and an auxiliary model for gradient propagation. This requirement can be prohibitive when dealing with larger domains and bigger models. 

* I have some doubts about the notation (see questions). I would try to be consistent with the notation in the literature (see https://arxiv.org/abs/2011.13456).

Limitations:
* Notation confusing or inconsistent.

* Not clear what is the goal of the paper (transfer vs distillation).


Rating:
5

Confidence:
2

REVIEW 
Summary:
This work distills diffusion models to a GAN-style generator. The author provides mathmatic connection to other works, as well as experimental comparisons between the distilled GAN-style generator with other (mostly) diffusion models. 
 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- This work introduce a way to distill diffusion model to GAN-style generator. 
- And shows the connection to other works.
- The experiments show the proposed method works well. 


Weaknesses:
- Lack baseline method. Table1 and Table2 shows the performance many diffusion methods, but they are either diffusion model trained on different datasets or distilled diffusion model that may not use the same teather model as the author used (i.e. EDM). In this case, the numbers in table1 &  2 are not compariable. There should be at least one baseline method that use the same teather model (EDM), the same generator architecture (one-step GAN-style generator), but different distillation method. Here is a simple baseline idea,  generating N images by the teather model (EDM), training a GAN on the generated images. 

- Lack time comparison. One big benefit of one-step generator (as well as distillation) is inference speed. Otherwise, we can use the teacher model directly.  



 

Limitations:
this work has no negative societal impact.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces a method to transfer the knowledge of a pre-trained diffusion model to an implicit generator model, such that the distribution of the generated samples from the generator matches that of the pre-trained DM. The training is data free, and it can be divided into two alternating phases: training a diffusion model on generated samples from the generator model, and updating the generator model to produce samples that produces similar score with the pre-trained DM. 

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
1. The paper tackles a very interesting idea. While there are many papers targeting at improve the sampling speed of diffusion models, and multiple works have designed methods to distill diffusion models, the idea of distilling the knowledge to another generator model is, to the best of my knowledge, novel.

2. The method itself is solid and interesting. With the definition of IKL, the training objective becomes an explicit divergence minimization.

3. Results are comprehensive and strong. It shows results on both distilling diffusion models to a single step and enhancing existing GAN models. It also shows impressive performance on single step generation. The fact that it can further improve SOTA GANs is impressive.



Weaknesses:
1. The presentation of the algorithm can be improved. Figure 1 isn't very informative, and a more detailed introduction should be included in the caption. Algorithm 1 isn't very clear, as it only shows the gradient formulation, without introducing how to compute it. For example, I am not very clear about how to estimate the integral in the gradient formulation. Did you just sample one random t as a very rough approximation? 

2. As you mentioned, when we assume the generator output has dirac delta distribution, the formulation is essentially the same as SDS in dreamfusion. But we know that GAN is indeed a generator model with dirac delta distribution, so what is the real benefit of assuming non-dirac delta distribution for the generator? Did you actually find any benefit of training with algorithm 1 with an auxiliary DM over just training with 3.3?

3. I do not quite understand the distillation part. When doing the distillation (instead of GAN enhancement), at early stage the generator just output non-sense images, and matching the score on it can be meaningless. Is there any explanation on how it can be applied to distilling DM to a non-pretrained generator?

Limitations:
Limitations have been discussed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a method for data free knowledge distillation from large scale diffusion models. It tries to distill the knowledge contained into a diffusion model, into another diffusion model or an explicit image generation model like a GAN. They propose a novel divergence measure obtained by integrating the KL divergence along the path of the diffusion model to match distilled model with the original instructor model. They show that the gradient update for the generator can simply be obtained as a function of the score functions as long as the generator is a differentiable function in its output. Using this the paper shows that they are able to distill knowledge from pre-trained diffusion models into generative models like GANs which enables faster inference. The paper provides empirical results to support their claims.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper discusses an important problem of distilling pre-trained diffusion models, especially into single-shot prediction models which enables faster inference.
2. The proposed method is simple to understand and implement in practice.
3. Since the generator is a single-step predictor, it provides extremely fast inference at test time compared to standard diffusion models.

Weaknesses:
1. The convergence of the proposed algorithm is not clear in the paper. The authors should elaborate as to why will the generator generate images which resemble the pre-trained diffusion model. In the absence of the generator, one can always sample random images and try to match a new diffusion model to a pre-trained model. Why will this approach not work? (One can use lesser number of reverse diffusion steps for this model)
2. This method requires training two models, which increases the space complexity of training

Limitations:
1. Authors need to provide an elaborate discussion comparing the proposed method with its baselines for instance consistency models, and justify why the method is better compared to the baseline in terms of the theoretical convergence or inference

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes a general framework for distilling a pretrained diffusion model into an arbitrary one-shot latent-variable model. The objective is a reverse KL divergence integrated over time, and an extra score model is learned for the current model distribution. The generator model parameter and the score model parameter are updated alternatively. Empirical results were shown on distilling a diffusion model to a model of same architecture, and finetuning a pretrained GAN model. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper proposed a general framework for distilling pretrained diffusion model on a model-architecture agnostic fashion. Meaning that the distilled model doesn't have to have the same model architecture as many other distillation approaches for diffusion model. This can be useful for many potential applications such distilling 2D diffusion models for 3D.
- Introducing a learnable score network for the generator model removes the need of learning an inference network as the usual variational inference requires. 
- Overall speaking I think this framework is nice and can potential have impact not only for 2D image distillation / refining, but also cross-modality knowledge transferring. 

Weaknesses:
- One piece missing and I'm eager to see is, if the implicit model is randomly initialized with an arbitrary architecture (e.g. a styleGAN generator), would this approach work? The experiments show that it works well if the implicit model is either initialized from a pretrained DM or a pretrained GAN model. It makes me doubt if this approach works only when the generator is already well-initialized. Adding this piece will make the paper much more stronger IMO.
-  The approach involves training two models jointly which inevitably introduces the issue of mismatching between two model classes and potentially results in instable training. I'd like to see more analysis towards this issue. 

Limitations:
Yes.

Rating:
8

Confidence:
5

";1
DEiNSfh1k7;"REVIEW 
Summary:
This paper introduces a new dataset consisting of images generated by prompting the Stable Diffusion models, aiming for studying the mid-level image similarities. Human perceptual judgments are collected for image triplets via both 2AFC and JND tests. A perceptual metric is built upon large pretrained vision models, which is empirically shown to be highly consistent with human judgments.



Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
+ It makes one of the first attempts to study the evaluation of AIGC images, the contribution of the new dataset is significant and the pipeline to collect human perceptual data is reasonable.
+ Experiments are well organized, which manifests the shortages of current perceptual metrics in evaluating AIGC images and verifies the effectiveness of the proposed metric.
+ The feature inversion part is interesting, aligning well with the Analysis-by-Synthesis model evaluation methodology. 

Weaknesses:
- The definition of distortion seems a bit confusing. In the literature, distortions are generally associated with degradation of image quality. In this work, distortions refer to mid-level changes, which do not necessarily lead to the loss of visual quality.
- It is unclear how the JND experiment is conducted. When the participants are expected to give an answer of yes? Two images without any noticeable pixel changes, or perceptually the same in terms of the mid-level change?
- More details of the feature inversion part are expected.

Limitations:
Not applicable.

Rating:
8

Confidence:
5

REVIEW 
Summary:
In this paper, the authors develop a perceptual metric that assesses  images similarity holistically. They first collect a new dataset of human similarity  judgments over image pairs that are alike in diverse ways. During the dataset construction, the authors utilized Stable Diffusion to create synthetic data, aiming to model mid-level similarity. Based on the dataset, the authors observe that popular perceptual metrics fall short of explaining the new data, so they introduce a new metric, DreamSim, tuned to better align with human perception. Finally, authors applied this metric in image retrieval and showed that it can generalize to real images. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Human visual similarity is an important and practical topic in the domain of computer vision. It has many potential applications such as image retrieval. One of the major contributions of this paper is the new dataset (NIGHTS), which contains human similarity judgments of 20k synthetic image triplets. I believe this dataset will be useful for fellow researchers in the related domain. Another strength of this paper is the proposed similarity metric that captures how humans naturally perceive image similarity. I also appreciate the authors detailed analyses and comparison of the metric and its performance. 

Weaknesses:
Though the paper is well written, and the topic is interesting and practical, the paper has several weaknesses. First of all, as the authors have mentioned, due to the potential biases and other limitations of Stable Diffusion, the authors deliberately avoided human faces during dataset construction. However, human faces are a common subject in general visual scenes and human perception of images containing human faces should be studied. Second, though the authors have applied the metric in image retrieval and used Figure 8 as a vivid proof to demonstrate that the proposed metric can be generalized to real images, I still have several concerns: first, is it possible to test the proposed metric on another benchmark dataset of real images, instead of the image retrieval task? Second, who are the users who rated the preference on the image retrieval results? More details of performance evaluation are needed. Third, not all metrics in Figure 4 are evaluated in the image retrieval task, why? Finally, while the authors avoided human faces in the proposed dataset, the models retrieved all images with human faces in the bottom right block in Fig 8. Does it show that the model focused on features other than human faces? 

Limitations:
A more comprehensive evaluation of the generalizability of the dataset and metric is needed. Furthermore, humans are sensitive to faces from birth. Visual similarity on images with human faces is important.  

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper expands the fundamental task of measuring perceptual image similarity to encompass factors
 beyond low-level and high-level similarity. To achieve this, it leverages the recent advances in generative visual AI and generate synthetic images with text prompts. These synthetic images are expected to contain multi-faceted notions of similarity, which are then handed over to humans for A/B similarity test. The proposed model then learns from these human similarity ratings and is shown to uncover the emergent hidden factors defining visual similarity. In extensive evaluations, it is shown to surpass a number of baselines, including DINO and CLIP.



Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
-- Research angle on visual similarity is timely, given its rather narrow scope of examinations in the past. Breakthrough on this angle can benefit vision community at large,.

-- Using synthetic images for visual similarity learning is a promising technical path forward given our increasing controllability on generating target visual contents. Instead of harnessing human ratings on the similarity between any two real images and letting models to learn as a black-box, using synthetic images has the advantage of leaving the discretion to human designers on what constitutes visual similarity -- for example in this paper, although only superficially, manipulates on mid-level visual variations.

-- Using LoRA to efficiently tune pre-trained visual ensembles towards a visual similarity judge is also reasonable.

-- Extensive analysis is extensive.

Weaknesses:
The paper is itself is quite intact by itself. So what I propose here is not necessarily a weakness, but rather with such a new tool on visual similarity, I keep wondering what can it, immediately, benefit vision tasks at my hand. The paper showcases a pilot study on the application of image retrieval, but in my understanding, the paper's contribution is best leveraged as a universal (differentiable) plug-n-play module to provide a complementary power to many computer vision tasks.

For example, it would be lovely to see how this paper opens new opportunities to image quality assessment community, where the intrinsic ambiguity of ground-truth human ratings is painstaking for generalisable modelling. Can DreamSim help alleviate this problem as a regularisation force by requiring similar visual contents to be of similar quality score? Same for the problem of sketch-based image retrieval community, where sketch-image ground-truth paired data is ill-defined. In the work of [A], they have shown how leveraging a visual similarity metric like LPIPS can greatly advance the understanding of the problem. Can DreamSim bring even further breakthrough?

[A] Photo Pre-Training, But for Sketch. CVPR 2023


Limitations:
N/A

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper leverages recent advances in synthetic image generation (namely text-to-image) to explore the space of image similarity assessment. They generate a novel dataset based on human forced choice input on pairs of synthetic images generated with respect to a reference image as well as interleaved visual memory tasks to assess when images are noticeably different on first assessment. The metric they learn over this novel dataset appears to better capture mid-level features such as pose and viewing angle.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is overall clearly written. The experiments are well-described and compelling, with multiple complementary facets supporting each other. The paper provides both a dataset and metric which may be of interest to the image similarity domain.

Weaknesses:
I apologize for a somewhat unclear criticism, but the JND task appears to be a visual working memory task, not a perception task. As we know from investigations of change blindness, there can be quite distinct differences between images that humans will only notice given sufficient time to explore the image, but if they happen to be attending the location that is different, they will immediately notice the difference. This seems to be getting at a different aspect of similarity than the perceptual choice task, which makes me want more of an explanation for why these two tasks are paired, or how they interrelate and why they are paired in this way. 

Limitations:
The limitations seem clearly articulated.

Rating:
7

Confidence:
3

";1
IEJzoOBM0z;"REVIEW 
Summary:
The authors study a class of causal effect that are called “g-identifiable” (gID) following Lee et al. (UAI 2019). There are two main results.
1. Expressing these causal effects as transformations of generalized multi outcome sequential back door adjustments (g-mSBDs).
2. Proposing doubly robust estimators for the adjustments, which then imply estimators for the causal effects if the transformations are smooth.

It appears that Jung et al (ICML 2023) previously provide these types of results for a subset of gID causal effects. The contribution is to extend the previous results for the full class of gID causal effects.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Originality: The first contribution is, in my view, the stronger one. Its connection to prior work is clearly stated. It appears that the workhorse is Lemmas 2, 3, and 4, which demonstrate the good properties of Definition 1. The second contribution follows quite closely from prior work.

2. Quality: The introduction was generally well written, though it would make more sense to choose Examples 1 and 2 to be examples that are not in the class of Jung et al (ICML 2023).

3. Clarity: Overall, the organization of the paper was very logical.

4. Significance: The contribution seems sound but possibly incremental.

Weaknesses:
I would be willing to improve the score if these weaknesses are addressed by the authors.

1. I find the title and framing too broad. There are many papers that could have this title, and it does not clearly convey what is in this paper but not in other papers.

2. Line 56 gives a list of doubly robust estimators. At the very least, the sequential regression paper by Bang and Robins (Biometrics 2005), the targeted learning in data science textbook (van der Laan and Rose 2018), and the debiased machine learning paper by Chernozhukov et al. (2018 Econometrics Journal) should be cited. 

3. More relevant are the sequential doubly robust estimators such as Luedtke et al. (arXiv:1705.02459, 2017) and Chernozhukov et al. (arXiv:2203.13887, 2022) and the references in those papers. Definition 3 and Proposition 1, are very closely related to the prior works listed above, so explicit comparisons would be helpful here.

4. The delineation between Lee et al. (2019)’s algorithm and this algorithm should be clearer. The sentences in lines 127 and 156 seem to clash in explaining the relationship to the prior work. 

5. The notation in Definition 1, Lemma 1, Lemma 2, and Lemma 3 is very heavy, and no interpretation is assigned to any of the variable symbols. It would help to revisit Examples 1 and 2 here.

6. In the appendix, the data generating processes are not described analytically; instead, python code is pasted. Please describe them. Please explicitly write what are the functions mu and pi.

Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a method for causal effect estimation through a combination of experimental and observational data. 

Soundness:
4

Presentation:
1

Contribution:
4

Strengths:
- The paper addresses a very important problem 
- The solution appears to be mathematically sound
- Experimental results appear to show improvement over other methods

Weaknesses:
- The paper is incredibly  hard to parse. Pages 4-8 are just a wall of mathematical notation with little to no explanation. 
- The paper would improve significantly if after each definition / lemma / proof a plain word intuition was given. 
- Running an example through an algorithm in the manner its done in page 6 is borderline incomprehensible. 
- Figure 2 is too small to read even when pdf is zoomed 

  Honestly this is a great paper that is severely limited by the way its presented. 

Limitations:
Little discussion is included about the limitations of the method

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a new framework for estimating the causal effect when multiple observational and experimental datasets are available. The authors proved that any g-identifiable causal effects can be written as a functional of ""g-mSBD"" operators, based on which they further developed a doubly robust estimator. Asymptotic properties of this estimator are then analyzed, and the performance of this estimator is confirmed in simulation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper tackles a hard problem with theoretical soundness and practical applicability.

Weaknesses:
I have two questions/comments. 

The first regards the assumption imposed in their Proposition 2 as well as Assumption 2 -- the authors assumed the nuance estimates are L2-consistent. While I understand that such a property would make the theoretical analysis more amenable, I am curious and would appreciate it if the authors can provide a concrete (and maybe idealized) example, under which the assumption can be verified.

My second question is about practical performance of the proposed estimator. While I am convinced about its utility in the simulation study, it would be great if the authors can apply their methodology in a real dataset and verify it still outperforms its competitors.


Limitations:
Please see the ""Weakness"" section.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper aims to fuse the recent literature on causal effect identification and estimation. It expresses any identification (in the general sense) of a causal effect in terms of generalized mSBD adjustments, and then develops a corresponding estimator. For the latter robustness properties and empirical studies are shown.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The contribution made by this paper is well-situated within the problem space and it's a natural generalization of other recent papers. All the results appear to be sound.

Weaknesses:
I believe the contribution to be valuable but the methods are not novel; they seem to be iterative applications of existing ones.
In many places it's too dense: for example, Examples 3, 4 are not illuminating, and readers may just skip them.

Limitations:
Yes, they have (there is ample discussion of e.g. the assumptions relied on).

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper builds on prior work in causal effect estimation, from observational as well as experimental data, by providing a more general framework with desirable properties. This more generalized framework is shown to be robust and consistent in its estimation and overcomes the limitations of some of the prior work in this area. The new, general framework, operating in the realm of generalized identification or g-identification, exhibits multiply robust properties, which is experimentally validated. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The new proposed estimator could be now the go-to framework for any g-identifiable causal functionals, potentially representing a significant theoretical advance in the field of learning causal effects.  
The paper is well-scoped and specific in what is being advanced and corroborates the desirable properties of the estimator via simulations.  
The exposition is quite clear and well-written. 

Weaknesses:
It is not clear what challenges remain for g-identification in practice with the proposed new estimator. Some guidance for practitioners and other researchers as to the context of problems and types of data they can consider using the new estimator would be highly useful.  

Limitations:
It is not clear what are the limitations of this work and the proposed new framework, and a lack of such an elucidation might hurt the uptake of what seems to be a promising new framework. There does not seem to be any discussion of any kind of limitations or potential broader impacts. 

Rating:
7

Confidence:
1

";1
uJmsYZiu3E;"REVIEW 
Summary:
This paper studies the allocation of m indivisible chores among n agents with non-additive preferences. The authors show that, for the case of approximate MMS, the best approximation factor is super constant, and specifically they give a lower bound of min{n, log m/ log log m } for submodular costs, and an upper bound of min{n, log m} for subadditive costs. The lower bound also implies a negative result for 1-out-of-d MMS allocations in this setting.
The authors proceed to study special cases of subadditive costs, and specifically costs encoded by combinatorial problems, and namely bin-packing and job scheduling. So, for example, in the case of bin-packing, the cost for a subset of items is the minimum number of bins to pack them. For both cases, the authors give an algorithm for finding a 1-out-of-(n/2) MMS allocation.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The paper studies an interesting, natural problem: chore allocation beyond additive costs.
- The results are relatively complete, and the authors settle, up-to-constants, all their questions.


Weaknesses:
- The main algorithmic results (for bin-packing and scheduling) seem like twists to the standard approximation algorithms (e.g., Next Fit or Best Fit for bin-packing). Of course, this is expected, but these connections/insights are not explicit in the text, so it’s harder to see what’s new about this work.

(Some typos:
Line 134: “which is somehow the most unfair algorithm” -> drop “somehow”.
Line 342: “or the job scheduling setting, we restrict us on the case”)

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the MMS fair allocation of combinatorial tasks (indivisible chores) problem where the cost function is submodular or subadditive. For submodular functions, they prove that no algorithm can ensure better than min{n, log m/log log m}-approximation. For more general subadditive cost functions, they prove that there always exists an allocation that is min{n , log m}-approximation MMS, which is (almost) asymptotically tight. What’s more, for ordinal relaxation, 1-out-of-d MMS, they prove that for any d≥2, there is an instance for which no allocation is 1-out-of-d MMS. Finally, the authors give two specific settings which are bin packing setting and job scheduling setting and prove that 1-out-of-[n/2] MMS allocations always exist for these two settings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper provides solid and clean theoretical results on MMS chores allocations. It also contains some interesting techniques. For example, the author gives a quite interesting example for Theorem 1, and I also find the proof of Theorem 2 mathematically natural and complete.

Weaknesses:
I do not find any obvious weaknesses. Perhaps the relevance of this paper to machine learning is not very strong. 

Limitations:
Not applicable.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper deals with problem of allocating indivisible chores to agents whose valuation functions for bundles of chores are subadditive or submodular, where an allocation that guarantees every agent their maximin share (MMS) may not exist. Here, an agent's maximin share is the agent's worst-case disutility from a partitioning of the items that minimizes the disutility of the worst (maximum disutility) partition. The paper provides new upper and lower bounds on the approximability of MMS allocations.

I have reviewed a previous version of this paper submitted to IJCAI 23 where I was positive about the technical contributions of the paper. This revision addresses the concerns with writing and a minor technical issue raised there. The revisions have certainly helped with the readability.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem setting of subadditive and submodular (dis)utility functions is novel. 

The new results for these classes of valuation functions provided is this paper are likely to be of interest to the comsoc / fair division research community.

The technical results are interesting, non-trivial and use interesting techniques that are new to me. I was able to verify the technical results.

Relevant related work is well cited and discussed.

Weaknesses:
The relevance to NeurIPS for what seems a very AI/computational social choice focused paper is not clear although this is attempted in the introduction.



Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies the classical fair allocation of indivisible items setting with two twists: (1) Items correspond to *tasks* instead of *goods*, i.e., any agent would prefer receiving no item at all. (2) Valuations are not additive but may be subadditive. The paper also considers some special cases of subadditive valuations, namely, submodular, ""bin packing"", and ""scheduling"" costs. As an objective, the paper focusses on the classic notion of the maximin share (MMS). Since MMS allocations do not always exist, the paper considers both a multiplicative as well as an ""ordinal"" relaxation of the MMS notion. 

The contribution of the paper is threefold (I am omitting the ordinal approximation results for simplicity): (1) For the subadditive case, the paper presents a lower bound of $\min\{n,log(m)/loglog(m)\}$ as well as a mechanism providing an approximation of $\min\{n,\log(m)\}$. (2) For bin packing, the paper presents a multiplicative $2$-approximation and a tight lower bound of $2$ for any mechanism. (3) For job scheduling, the paper presents a mechanism providing a multiplicative $2$-approximation, however, without a matching lower bound.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper makes significant progress within the classic setting of fair allocation of indivisible items. While this literature has been long focussed on the case of additive valuations/costs, in recent years there has been a growing body of literature studying more general valuations/costs, with the paper under review being one of these. Hence, I am optimistic that the paper will lead to follow-up work. 
- The paper develops new mechanisms that are tailored to the studied cost functions. These mechanisms and their analysis is certainly non-trivial and lead to a significant technical contribution. 


Weaknesses:
- Since the result for the general, subadditive case is rather negative (i.e., there is no constant approximation for MMS), the main contribution of the paper is for the special cases of bin packing and scheduling. Having said this, this can hardly be seen as a critique for the paper, but rather as a sign of the challenging endeavor to study beyond additive costs. 
- I think that the writing of the paper could be improved, as I had to reread several parts of the paper. I added a list of suggestions within the minor comments. 
- Unfortunately, the newly developed mechanism is not very elegant, and one can't help but wonder whether there exists a simpler mechanism to achieve the same approximation guarantee. Also, the mechanisms do not come with a polynomial-time implementation, hence, leaving the question open whether the same guarantees can be achieved efficiently. 

**Minor Comments**
- The paper uses the term ""tasks"", while a large fraction of the literature uses the term ""chores"". I would suggest to add a comment on that. 
- line 21: You mention ""functions"" without clarifying their role in the problem. (Of course this is clear for any person knowing fair allocation, but for others it may not.)
- line 60: ""As far as we know, all the above works also assume additive costs"" - Sounds a bit weird in this context, since checking these papers should be doable. 
- line 71: ""the asymptotically tight multiplicative"" - I think it is weird to use this phrase in a theorem environment, especially since you are ignoring log-factors. I would suggest to just mention the upper and lower bounds. 
- line 99: ""Note that no bounded approximation"" - At this point, it is not clear what should be approximated. 
- Proof of theorem 1: I was very confused of the usage of the term ""covering planes"" since, as far as I understand, these objects are actually (partial) grids, i.e. finite set of points. 
- line 214: I think it would be helpful for the reader to learn about the meaning of the abbreviation ""IDO"". 
- line 228-236: I did not find the intuition for the algorithm very helpful before reading the algorithm (and even after that). I would suggest to refine this, having in mind that the reader has not read the algorithm at this point. 
- line 316: I think that $j \in P_i$ is a bad choice for an index since here $j$ is a machine but before $j$ used to correspond to jobs/items. 
- Section 5: It would have been nice to hear some (very brief) summary of how Theorem 4 is achieved, i.e., how does the mechanism look like? 

Limitations:
The limitations of the paper are well addressed, in particular, within Section 6. Here, the paper transparently communicates all resulting open questions. 

Rating:
5

Confidence:
3

";1
CFQBcz7k8n;"REVIEW 
Summary:
The paper studies adversarially robust learning with uncertain perturbation sets, where the set of points to which an adversary can perturb any test input is random, in contrast to previously studied settings of fixed known or unknown perturbation sets and unknown perturbation sets. The perturbation set is assumed to be coming from some fixed perturbation class.

Given a perfect attack oracle which certifies robustness or gives a successful attack, learnability is shown for concept classes with finite VC dimension with finite bounds on sample complexity and query complexity to the oracle, with some assumptions on U which are satisfied by commonly studied perturbation classes e.g. finite union of totally ordered perturbation sets which includes $L_p$ balls for a finite collection of $p$ values. 

When the classifier is equipped with abstention, learnability is possible under weaker assumptions e.g. finite disagreement coefficient of H wrt the data distribution. The authors further propose new notion of finite disagreement cover that depends on the perturbation class U, and reduce robust learning under abstentions to successful learning of a finite disagreement cover in the sense of their proposed notion.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The work considers removing a common and unrealistic restriction in prior theoretical work that the adversary uses a fixed perturbation set.
- The authors give examples demonstrating power of abstention and the difficulty of agnostic learning under their definitions.
- Proofs or proof sketches of several major results are included and discussed.
- Comparison to prior works and techniques e.g. Montasser et al. 2021 helps clarify the novelty and technical contributions of the present work.

Weaknesses:
- Realizability: Realizability of perturbation set seems like a strong assumption on the adversary and should be discussed. No positive results are given in the main body in the agnostic case, which could be a limitation of the model under study.
- Some results are not adequately discussed, e.g. Theorem 5 and last paragraph on Pg 9.

Limitations:
I do not anticipate potential negative impacts as the work is primarily theoretical, but encourage the authors to summarize limitations and further questions in a conclusion section e.g. lack of positive results in the agnostic case, tightness of sample complexity bounds, etc.

Rating:
6

Confidence:
3

REVIEW 
Summary:
- This paper bridges the gap of PAC learning theory for adversarially robust learning between completely known and completely unknown perturbation across various settings.
- The authors introduce a notion of hypothesis class-induced partial ordering on the class of perturbation type which they use throughout the paper (setting 1).
- They show that in a realizable setting, hypothesis class with finite VC dimension and setting 1 are robustly learnable. They show that the same is not valid for agnostic setting without extra assumptions such as access to perfect attack oracle.
- They further show the existence of perturbation type and hypothesis class which cannot be robustly learned without abstention even with access to perfect attack oracle.
- They also investigate hypothesis classes with finite disagreement coefficients and present results for this setting.
- The define an $(\epsilon, \mathcal{H})$ cover for a class of perturbation types and use this definition to provide a generalization of their previous results.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written for the most part and provides a good explanation for the results. Furthermore, the claims seem mathematically sound and the authors have made good use of statistical learning theory to present their results. The results are interesting and close the gap between two known settings. They have also introduced some new notions such as the partial ordering of perturbation types with respect to hypothesis class and  $(\epsilon, \mathcal{H})$ cover for a class of perturbation types which are interesting concepts.

Weaknesses:
I liked the results and the general writing style of the paper. At times, the novelty seems to be a derivative of existing works with proofs being a clever extension of the proofs of existing settings. I believe this is bound to happen as the problem setting itself is an intermediary between two known settings. 

The following is not a criticism but a remark - It would have been better to include some more mathematical proofs in a theoretical paper like this. However, I understand that space limitations sometimes prohibit this.  

Limitations:
The authors have addressed the limitations adequately. In that, this paper doesn't propose any (efficient) optimization algorithms, it rather presents sample complexity guarantees with Oracle access to the solution of the optimization problems.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies robust classification when the perturbation region an adversary can access is unknown to the learner, but where it is guaranteed it comes from a certain class of perturbations, which is known to the learner. The authors show various results when the learner has access to a perfect attack oracle (PAO) (which either returns a robust loss of 0, or an admissible perturbation on which the hypothesis is not robust). 

For perturbation classes with total order, without access to the PAO, finite VC dimension is sufficient in the realizable setting, but not in the agnostic one. With access to the PAO, this becomes possible, by a reduction from agnostic to realizable robust learning. 

When the perturbation class is a finite union of totally ordered perturbations, robust learning is also possible with the PAO (but in general, not without it). Removing structure on the perturbation class renders robust learning impossible (even with PAO). 

The authors then consider robust learning with abstentions, where the learner cannot abstain on an unperturbed point, but can on a perturbed instance. They show that, in this set up,  robust learning is possible. They then relate robust learnability with the disagreement coefficient of a hypothesis class. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Important question, nice set up that lies between unknown perturbation sets and models where the perturbation function is known a priori 
- The results are interesting and draw on multiples different concepts (learning with abstentions, disagreement coefficients, previous robust learning results, etc.)
- The paper is clear and well-written
- Meets the technical standards of NeurIPS

Weaknesses:
- No conclusion section - please add one for the final version!
- MRERM: it seems to be quite a strong assumption to have access to this oracle? Especially since it returns the perturbation $u^*$! The existence of $u^*$ is fine, but that an algorithm can find it for any/most $\mathcal{H}$ seems quite strong. Could you expand on this, and give examples? 

Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors present theoretical results on classification in a version of the PAC model where an adversary can perturb input instances.  Previous work assumed either a fixed perturbation type, known to the learner, or an unknown perturbation type.  This paper explores a middle ground where the perturbation type is a member of a fixed class of perturbation types.  The learner knows the class, but does not know which perturbation type within that class is being used.  For example, the perturbation class might correspond to balls with different radii.

The paper presents results on learning for a class U of perturbation types and a hypothesis class H.  It defines a partial order on perturbation types in U, defined with respect to H.  It considers variants of the learning model where the learner can abstain from predicting the label of a given example (which is counted as a misclassification error unless the example has been perturbed).  It also considers a variant of the learning model where the learner can has query access to a ""perfect attack oracle,"" a type of oracle studied in previous work on learning with unknown perturbation sets.  

The results in the paper are all with regard to sample size.  (Computational complexity is not considered.). The first two main results are as follows.  (1) When the class U is totally ordered with respect to H, and H has finite VC dimension, then learning in the realizable case is possible with a polynomial-sized sample (treating VC-dimension as a constant). (2) For the unrealizable case, when U is totally ordered wrt H, a polynomial-sized sample also suffices if the learner can also make polynomially many calls to a perfect attack oracle.  

The next result considers a similar situation to that in (1) above, except in a more general setting where where U is a union of totally ordered subclasses.  In this setting, even with finite VC dimension and realizability, learning from (finitely many) examples is not possible, but can be achieved with a combination of a polynomial-sized sample and the ability to make polynomially many calls to a perfect attack oracle.  

The remaining results are concerned with learning with abstentions.  The main results are (1) Learnability in the realizable case, with abstentions, when U is totally ordered and H has finite VC dimension (2) Learnability in the realizable case, with abstentions, when U is the class of all perturbation, when the class has finite VC dimension and finite disagreement coefficient.  A result is also given on learning with abstentions in the realizable case if there is a successful finite-disagreement-cover learner for (H,U).

The paper also contains simple negative results demonstrating the need for abstentions, total order, etc.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
A comprehensive exploration of uncertain perturbation sets, bridging work on unknown perturbation sets on the one hand, and a single known perturbation set on the other hand.  The paper is well-written.

Discussion phase:  I raised my score from 5 to 6.

Weaknesses:
From the presentation in the main paper, it does not seem that the paper introduces new techniques or that the presented results are surprising.  It seems as if the techniques have already been used in previous work on perturbations and that the results are similar.  Overall, the work seems like a solid contribution, but lacking in real novelty.

Limitations:
I have no concerns in this area.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In adversarially robust PAC learning to test time attacks, usually we assume that the learner knows the perturbation function. 
Montasser et al. ['21] studied a setting were the learner doesn't know the perturbation function, but can interact with it through some oracles. In this paper, the setting is in between, assuming that there is a class of possible perturbations $U$ known to the learner, and the performance of the learner is measured on the worst-case perturbation function in this set. For example, the perturbation can lie in a ball centered in the original input, but the norm isn't known. 

The main contributions are as follows. 

- Considering an order on $U$ w.r.t. $H$, finite VC is sufficient for learning in the realizable case, but not in the agnostic case. 

- When the learner can interact with $U$ through a perfect attack oracle, and assuming some structure on $U$, finite VC is sufficient for learning. This is an improved result compared to the setting of an unknown perturbation function, where finite Littlestone dimension is sufficient, and $VC(H)\leq Lit(H)$ and the gap can be arbitrarily large.

- Introducing a setting where the learner can abstain, and showing that finite VC is sufficient for learning (assuming some structure on $U$).

- Assuming that $H$  has a finite disagreement coefficient (a parameter that is related to active learning), then $H$ can be learned with respect to every class of perturbations.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The in-between setting of robust learning suggested in this paper is natural and complements the picture of the other two settings (known $U$ or completely unknown $U$).
This paper studies thoroughly various scenarios, showing some limitations of this model, and when we can guarantee robust learning. 
The results look correct to me. 
I think that this paper could be of interest to the community of theoretical robust learning.

Weaknesses:
If I'm not missing anything, the technical contribution of the paper is moderate and mostly relies on standard ideas from PAC learning. 

Many references on theoretical robust learning are missing. For example, H-consistency bounds for surrogate loss minimizers (ICML 2022), Multi-class H-consistency bounds (NeurIPS 2022), Theoretically grounded loss functions and algorithms for adversarial robustness (AISTATS 2023), Cross-Entropy Loss Functions: Theoretical Analysis and Applications (ICML 2023), A Characterization of Semi-Supervised Adversarially Robust PAC Learnability (NeurIPS 2022), Adversarially Robust PAC Learnability of Real-Valued Functions (ICML 2023), Sample complexity of robust linear classification separated data (ICML 2021)...and many more!

Limitations:
I don't see any.

Rating:
6

Confidence:
5

";1
pQvAL40Cdj;"REVIEW 
Summary:
This paper proposes a framework for human-object interaction (HOI) detection that leverages vision-language foundation models and large language models to achieve universal and flexible recognition of complex interactions in images. The framework, named UniHOI, consists of three main components: a visual HOI detector that extracts three levels of features from images, a HO prompt-guided decoder that queries the foundation model for high-level relation representations associated with human-object pairs, and a knowledge retrieval module that uses a large language model to generate descriptive texts for interaction categories. The framework supports both supervised and zero-shot settings, and can handle any textual input for open-category interaction detection. The paper demonstrates the effectiveness and superiority of UniHOI over existing methods on two public benchmarks, HICO-DET and V-COCO, as well as in the wild scenarios.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The performance is quite impressive.
- Leveraging LLM and foundational models to augment CV tasks is the future, and this work gives a try to use them simultaneously.
- The authors promise to release the code to ensure reproducibility.

Weaknesses:
1. There is no ablative study of each component (perhaps only one component, i.e., HO prompt decoder) under the close-set setup.
2. This work uses BLIP2 with ViT-L while existing work like GEN-VLKT typically uses CLIP with ViT-B. It is evident that the former is much more powerful. Could you provide the performance on HICO-DET using HO prompt-based learning with CLIP ViT-B under the close-set setup? As the improvement may be brought by more advanced large visual-language pre-trained models.
3. How about the inference speed? As shown in Table 8, **three** times longer than GEN-VLKT. Note that existing work like GEN-VLKT does not involve the computation of large visual-language pre-trained models at the inference stage, since all of the feature of objects or verbs is pre-computed. However, in this work, the feature for prompting should be computed for each image individually. Considering the extremely large backbone (e.g, ViT-L), there would be a heavy burden at inference.
4. The core contribution of this work is actually the HO prompt-based decoder. However, there is nothing novel with it, i.e., directly using spatial location to get foundational model output features.
5. Knowledge retrieval is solely used in open-world setup, is it possible for it to augment the closed-world setup?

Overall, this is technically solid work, and LLM for knowledge retrieval is interesting. But the comparison is unfair (i.e., a much more powerful visual-language pre-trained model), the inference time is unacceptable, and the novelty of the prompt-based decoder is limited. I will be very happy to update my score if the authors can address my concerns above.

Limitations:
There is no discussion on limitations or failure cases.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In view of the limited scalability and the suboptimal zero-shot performance of current HOI detection methods, the authors propose a novel method for HOI detection based on VL foundation models.
With in-depth analysis and adaptation of HOI detectors, the foundation model is effectively adopted to reason about HOI relationships based on human/object tokens.
Furthermore, LLM is adopted as a knowledge base to diversify HOI descriptions, enabling open-vocabulary HOI detections.
With the VL foundation model and LLM, extraordinary HOI detection performance is achieved upon both conventional and zero-shot setting.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The proposed HO prompt-guided decoder is brilliant in addressing the feature alignment issue.

Adopting GPT as knowledge base is an interesting idea to incorporate the recent progress in LLM with HOI detection.

The performance is amazing with significant margins upon previous SOTAs, especially for zero-shot setting.

Extensive experiments are conducted, providing valuable insights on the effect of VL foundation models in HOI detection.

In the wild HOI detection illustration is quite impressive.


Weaknesses:
The comparison between GEN-VLKT and the proposed method is not totally fair to me. It might be better to replace BLIP-2 with CLIP for a fair comparison.

Fig. 2 is not very clear. It would help if annotating the encoders in the figure with corresponding notations. 

The baseline of ablation experiments is chosen as GEN-VLKT. However, a major difference between GEN-VLKT and UniHOI is the VL foundation model used. And there are also other differences, like VLKT is not used for UniHOI. It might be better to change the baseline to make the ablation more reasonable.

Limitations:
Limitations of the paper is not well-discussed in the paper. 
I would like to see more discussion on extending the use of LLM further than a static knowledge base.
Also please refer to the weakness part.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper investigates the problem of human-object interaction (HOI) detection. The authors introduced UniHOI, a method for universal HOI detection in an open-world setting. They also explored the universal interaction recognition with Vision-Language (VL) foundation models and large language models (LLMs), and proposed HO prompt-based learning for high-level relation extraction aimed at VL foundation models.  Experimental results show the effectiveness and significance of the proposed method.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. Overall, the manuscript is well-written and easy to follow. The figures are pretty and can convey the concepts clearly.
2. Pushing the problem of human-object interaction detection toward an open-world setting is of great importance. This is also a trend for most existing computer vision applications.
3. The extensive experimental results show the superiority of the proposed UniHOI method.

Weaknesses:
1. In Table 4, the results from the third row come from ""ConsNet [31]"" but not ""ATL [15]"", according to the paper of ""GEN-VLKT"".
2. The conclusion part lacks objective reflections on the deficiencies of this study and future prospects for improvements.

Limitations:
Please refer to the weakness section.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper addresses human-object interaction (HOI) detection task. The authors propose a new method named as UniHOI, achieved by prompting BLIP2 using human-object paired features, as well as linguistic semantics generated by a LLM. The proposed UniHOI demonstrates significant performance gain on HICO-DET and V-COCO in both fullly supervised and zero-shot scenarios.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
If the BLIP2 is a fundamental model pre-trained on a lower-level task than HOI detection (e.g., classification, object detection), I would say this paper is an excellent work  in terms of both model design and performance. Actually, I think this is the first work to transfer a fundamental model into the domain of HOI detection, which may open doors to further exploration on prompting learning  for HOI. However, I cannot accept the choice of using BLIP2 as the fundamental model for HOI detection. I will explain the reasons for this in the  weaknesses below.

Weaknesses:
1. Prompting engineering aims to transfer a fundamental model pretrained on **lower-level tasks** to **higher-level** tasks. At a minimum, **the task for pre-training needs to be decoupled from the downstream task**. Otherwise, transferring a model pertrained on higher-level tasks to low-level tasks is not prompting, but fine-tuning. BLIP2 is a powerful model pertrained for VQA, image captioning, and similar tasks. However,  as widely acknowledged,  HOI detection is a sub-problem for  these detailed scene understanding tasks.  Namely, BLIP2 itself is a powerful HOI detector (I have tried using BLIP2 directly for HOI detection, and the performance is impressive).  From this point, BLIP2 cannot be used as a fundamental model for HOI detection since it has a great capability for HOI detection by itself, and is capable of even higher-level tasks. Therefore, this paper is more like a work that fine-tunes BLIP2 on HICO-DET and V-COCO, at the cost of giving up the ability to use BLIP2 for other tasks, e.g., captioning.
2. While direct use of BLIP2 for HOI detection may fail to achieve as impressive performance as that of UniHOI on HICO-DET and V-COCO, **BLIP2 has already achieved the goal of doing HOI detection, i.e., detailed scene understanding**.  Therefore, is it a case of putting the cart before the horse to use BLIP2 for HOI detection only?
3. In a real open-world scenario, I think BLIP2 is more capable of HOI detection compared to UniHOI. After all, the  zero-shot HOI detection capability of UniHOI is mainly inherited from BLIP2.
4. The performance of UniHOI on HICO-DET and V-COCO is impressive.  I think this is the first work that achieves a mAP  being larger than 40% on HICO-DET. However, the comparison is not so fair. As aforementioned, BLIP2 itself is a powerful HOI detector, which has been pre-trained with a large amount of **interaction-specific** data.  Note that, **on a fair comparison,  a fundamental model should not be pre-trained using data with annotations involving downstream tasks**. Otherwise, the authors need to report the results without using these extra data. For instance, if we first collect all data involving HOI detection from the dataset used for BLIP2 pre-training. Next, we use these data  to pre-train a HOI detector listed in Table 1, (e.g., GEN-VLKT) to get GEN-VLKT-2. Finally, we fine-tune the GEN-VLKT-2 on the HICO-DET and V-COCO. I think it can also get an excellent performance. This is another reason why I think that transferring a model pre-trained on a higher-level task to a lower- level task (especially when the lower-level task is a sub-task of the higher-level task) is not a prompting, but a fine-tuning.


Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes a universal HOID pipeline, which utilizes decoded ho-pair feature as spatial prompts to prompt the VL foundation model with the aim to implement effective prompt-based learning on base VL model and extract HOI related features from it. It also proposes knowledge retrieval for HOID in open-category manner by large-scale pretrained language models. Experiments show the effectiveness of this approach in both generic and zero-shot HOID.  



Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. It explores universal interaction recognition by tansferring the rich knowledge inside Vision-Language foundation models and LLMs to HOI pipeline, which broaden the research scope of HOID.
2. The experimental results are promising. In both generic and zero-shot setting, this approach reaches a new state-of-the-art and surpasses previous methods by a substantial margin.


Weaknesses:
1.In line152, ‘P_h’ and ‘P_o’ are described as ‘excellent spatial position features’ and further utilized as HO spatial prompts. However, these features are generated by learnable position embedding and query, which is identical as many transformer-based HOID approaches before such as GEN-VLKT. Can you provide some evidence that these feature are indeed ‘excellent’, why could it provide accurate spatial information concerning ho pairs? Or are there some unique designs I overlook?   
2.In Line 177, HOPD is designed for the alignment issue between VL foundation model and HOI pipeline. And the output V_f is incorperated with V_i for final predicition of interaction. But the performance of pure V_f, i.e., only utilizing V_f for prediction is unexamined. This results may more directly show the effectiveness of alignment between VL models and HOI pipeline.
3.Some typos. Line 171, ‘the guidance of’ repeated twice. Line 177, ‘V_i’ is mismarked as ‘V_f’. 



Limitations:
None

Rating:
5

Confidence:
4

";1
F5FVsfCxt8;"REVIEW 
Summary:
In this paper, the authors propose a scheme for training decision trees on a combination of public and private data. By using local differential privacy the algorithm guarantees that adversaries learn little about the private data from the outcomes of the algorithm. Under assumptions and a specific splitting rule for the algorithm's public part, the authors prove properties about the convergence and generalization of the algorithm. The algorithm is empirically evaluated on 15 real and 1 synthetic dataset and compared against private histograms and deconvolution.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is written in clear English.
- Substantial theoretical analysis.
- Empirical evaluation on many (15) datasets shows that this method outperforms private histograms and deconvolution.
- The effect of different hyperparameter settings was clearly explored.

Weaknesses:
Major:
- Assumption 3.2 is key to deriving error bounds but I am unsure if this holds. Decision trees are discontinuous models that are often used to model discontinuous data.
- Sections 2 and 3 are very dense in mathematical notation and use complicated phrasing to say simple things. E.g. section 2.2 spends a full page that is hard to read on explaining that: an estimated regression tree predicts the mean of samples in a leaf and that the private tree predicts it based on private estimates of reaching the leaf (U) and the sample value (Y), privatized using randomized response and Laplace mechanism.
- Only relatively high values of $\epsilon$ were explored. While it is debatable what is a good value, to the best of my knowledge $\epsilon < 1$ is generally considered 'good privacy'.

Minor:
- Although I understand the 'max-edge partition with variance reduction' was introduced to prove theoretical properties it suffers from data that is non-uniformly distributed or contains useless features.
    - Useless features: the algorithm will never choose the same feature to split on twice unless depth > d (as a result of splitting on the largest distance). This means that if there are few informative features these will be split on only once.
    - Non-uniform data: the splits are determined on the midpoint between minimum and maximum values of a feature instead of looking at how data is distributed in the feature. This means data with e.g. long tails will cause a split to create one leaf with almost all data and one leaf with almost none.
- The algorithm and analyses are based on continuous features but the datasets used for evaluation also contain categorical features. E.g. 'sex' in Abalone.
- Limitations and broader impact have been moved to the appendix while these should be part of the main text.
- The visualizations only show mean estimates, no standard deviations / standard errors.
- It would be nice to compare to (global) differentially private decision trees to get an idea of the cost of decentralization in this task.

Limitations:
The limitations are moved to the appendix with the only limitation discussed being assumption 3.3.

Depending on the answer to question 1 I believe assumption 3.2 needs to be added to the limitations and a discussion on the choices of $\epsilon$ should be added. The limitations should be given in the main text.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper studies nonparametric regression under local differential privacy (LDP) constraints with the aid of public data. The paper proposes an algorithm, locally differentially private decision tree (LPDT), which uses public data to construct the splitting criteria for the decision tree and private data to compute the regressed values for each leaf node. The paper shows that under certain assumptions, with a small amount of public data, the algorithm can achieve a near-optimal convergence rate for decision tree regression under LDP constraints. The paper also demonstrates the effectiveness of the algorithm through experiments.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper studies an interesting problem, which has not been studied before to the best of my knowledge. The paper also proposes an algorithm that achieves an optimal convergence rate under reasonable assumptions. 

2. The paper shows that without public data, decision tree learning will have a nontrivial risk with any amount of private data. This suggests the importance of public data for decision tree regression under LDP constraints.

Weaknesses:
1. The importance of public data has been established for different learning tasks recently, especially under the central notion of differential privacy. Extending a similar observation to decision tree learning under local DP constraints is interesting, but less surprising.

2. The similarity assumption between P and Q, Assumption 3.3, looks weird. The assumption doesn't pose any requirement on how y is correlated with x. Imagine the case when P and Q have the same marginal on the feature space while the correlation between X and Y are completely different under P and Q. It would be hard to learn useful information about how x can be used to predict y from public data, and hence the structure of a good decision tree. I am not sure whether I have missed other important assumptions.

3. Another baseline to compare (theoretically, and empirically) would be to only use public data to train the decision tree. How would the proposed method compare against this baseline? E.g., in the experiment in Section 4.2.

Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work studies non-parametric estimation under local differential privacy with public data. The authors propose a locally private decision tree algorithm and show that it is min-max optimal under some regimes. The authors also test the algorithm on real and synthetic datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The idea to leverage public data in LDP estimation is practically relevant. 
2. The experiment results are extensive. It includes both synthetic and real-world data and discussion on the effect of various parameters. Empirically the algorithm outperforms existing algorithms in most experiment settings.
3. The authors provide a thorough theoretical analysis of the proposed algorithm in terms of sample complexity and computation cost.  The proposed algorithm has improved time complexity over prior works.

Weaknesses:
1. The proposed algorithm does not seem to improve too much in terms of sample complexity with public data. We normally hope that using public data should improve the performance somehow (e.g. in reference [8], with public data the prior bound on the mean can be removed), but it seems that the algorithm merely attains the optimal bound without public data only when $\varepsilon\lesssim 1$. Existing algorithms that do not use public data can also achieve the same sample complexity.
2. Given the first point, the fact that the proposed algorithm does not work without public data appears to be a significant disadvantage. While LPDT has advantages in time complexity and empirically outperforms existing methods, it comes at the expense of additional resource of public data. It is thus in some sense unfair to compare with existing methods that do not use public data.

#### Some minor problems

1. The lower bound only matches the upper bound in Theorem 3.4 when $\varepsilon\lesssim 1$ (since $e^{\varepsilon}-1\simeq \varepsilon$ only with small $\varepsilon$), but all experiment results are for large $\varepsilon\ge 2$.
2. The notation $p$ is used first for the density function and then again used for decision tree depth. Please consider changing one of them to avoid confusion.

#### Update
- Increased my score to 5 after the authors added new results for small $\varepsilon$ and introduced a new algorithm that matches the lower bound for all ranges of $\varepsilon$.
- Increased to 6 after the authors resolved my question for removing range parameters.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a locally differentially private (LDP) decision-tree (DT) regressor algorithm that takes advantage of public data for utility improvement.
The proposed algorithm, LPDT, uses Randomized Response (RR) and the Laplace mechanism to protect the tree partition step.
This paper also introduces a new splitting rule named ""max-edge partition rule"" by using the variance as a reduction criterium.
Theoretical results were presented for convergence rates, training/testing time and space complexity, with important gains in comparison with the state-of-the-art.
Experimental results were provided with synthetic and many real-world datasets to validate the proposed approach, with significative gains over the state-of-the-art.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The motivation behind using Locally Differentially Private (LDP) methods for decision trees is well-founded. The idea of using public data is new and has proven to bring advantages in many fields (e.g., vision, languages).
Theoretical and experimental validation were presented showing advancement over the state-of-the-art.

Weaknesses:
The experiments only considered medium privacy regimes \epsilon >= 2. The paper would greatly benefit from including high privacy regimes \epsilon <=1 and the utility gain of using public data.

Limitations:
Although using public data can lead to utility improvements, the authors did not discuss the limitation of having such data in real-world applications. Indeed, following my last question on ""data heterogeneity"", the learning process could be really damaged if public and private data have different distributions. I recommend the authors to further discuss this paper's limitations.

Rating:
6

Confidence:
3

";1
lENeWLXn4W;"REVIEW 
Summary:
This paper presents a novel hyperparameter tuning method in the presence of a privacy budget: linearly extrapolating from observations with very low privacy loss.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
The core technique presented here is certainly interesting and deserving of future study. The paper tackles an issue which is often unaddressed in the literature on training DP models: that of choosing hyperparameters subject to a privacy budget. This problem itself is also deserving of further study.

Weaknesses:
* A primarily empirical paper will live and die with the strength of its baselines (as well as its upper bounds in a case like this one where upper bounds on the efficacy of the technique can be computed). The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it.

* Some baseline issues: the citation problems with [51], [52] (detailed below). Lack of comparison to the 'naive baseline' of directly applying gaussian mechanism to results of grid search, say given known training statistics / optimal hparam values for nonprivate datasets (to avoid infinite regress, and here not so much of a problem since the experiments are all focused on public feature extractor settings). Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand).

* On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8\% at $\epsilon=1$ and 98.9 at $\epsilon=\infty$ (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1\% and 90.6\% at the $\epsilon=1, \infty$ for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\infty just by tuning hyperparameters (see figure 2). 

* The statements of timing on Imagenet seem wrong? The cited paper [51] seems to be pointing to a version from Nov 2022, clicking through to [52] seems to show a version uploaded in May 2022--so where are Jan 2023 and 'within the last month' coming from?

* Some more consideration required in decomposition of $r$--do we know that random decomposition 'is enough'? Presumably it's not, since we _can_ generate an $\eta$ for which the problem will presumably diverge?

Limitations:
Societal impact not immediately applicable.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a linear scaling rule for finding the optimal value of the learning rate and number of training steps for differentially private SGD (DP-SGD). The idea is simple, small amount of privacy budgets are allocated for two initial DP learning rate optimization procedures, and then the values are extrapolated to bigger epsilon-values using linear scaling (as a function of epsilon). The work is mostly experimental, and the experimental results e.g. with CIFAR-10 show that for epsilon between 0 and 1, the scaling rule seems to nicely fit the optimal values found by the grid search. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The idea seems very interesting and novel.
- The paper is mostly written well and is easily readable.


Weaknesses:
- The technique is restricted to optimising the learning rate and the length of the training. I wonder if similar extrapolation (perhaps more generally polynomial extrapolation) could be used to find optimal the optimal hyperparameter values for other hyperparameters.

- The technical part could be written more carefully. It remains unclear whether you use RDP or GDP. The hyperparameter tuning cost of the method by Papernot and Steinke is in terms of RDP, but you list theoretical results in terms of GDP. In the end of Alg. 1 you write that the total cost is ""$\varepsilon_f + \varepsilon_0 + \varepsilon_1$"". Is that approximate DP? In case you use the classical composition result where you just add up the privacy parameters, what happens to the $\delta$-parameters?

- Some conclusions are a vaguely formulated/confusing. On p. 7 you have the subtitle ""Linear Scaling is robust to distribution shifts"", but then you seem to show and also claim in the subsequent text that DP itself is robust to distribution shifts. Somehow the message is vague here.

- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge.


Limitations:
Some of the limitations are discussed in Section 5 but it could be expanded I think.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This study proposes a new algorithm for privately selecting hyperparameters subject to maximizing the model utility. The new algorithm draws inspiration from the linear scaling rule that suggests increasing learning rate as batch size increases. Given the number of hyperparameters in DP-SGD the proposed algorithm simply scales learning rate and number of iterations as the privacy budget increases. This introduces a new hyperparameter that is selected privately with a portion of the privacy budget while the rest is used to perform the normal hyperparameter search. The study provides brief theoretical intuition for why we can expect this linear scaling rule to more efficiently determine optimal hyperparamters compared to previous methods and extensive empirical evidence on 20 different benchmark datasets. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- One of the first papers to demonstrate improved privacy-utility tradeoffs that takes hyperparameter tuning into account. This is substantial as the field has mainly focused on evaluating the privacy-uility tradeoff without considering the privacy cost of hyperparameter tuning. As we move towards more practical implementations, this will be necessary. 
- Clever use of the linear scaling rule to perform hyperparameter search and the resulting algorithm is simple to use. 
- Extensive empirical evaluation and insightful analysis. For example, very few analyses have been done on the intersection of DP and distriutional shift. Yet, this linear scaling rule that is proposed holds in the presence of distribution shift. 


Weaknesses:
- “We are 165 the first to show that DP-SGD is capable of learning to handle distribution shifts without using any 166 techniques from the distributionally robust optimization (DRO) literature” -> There are a couple of other papers that draw this connection. [1,2]
- Lack of comparison to other private hyperparameter selection algorithms or hyperparameter free private learning algorithms [3, 4]
- Unclear why the initial hyperparameter search can be done with such a small privacy budget even though this is a key factor driving the performance of the algorithm.

[1] Kulynych, Bogdan, et al. ""What you see is what you get: Distributional generalization for algorithm design in deep learning."" arXiv preprint arXiv:2204.03230 (2022): 13.
[2] Hulkund, Neha, et al. ""Limits of Algorithmic Stability for Distributional Generalization."" (2022).
[3] Mohapatra, Shubhankar, et al. ""The role of adaptive optimizers for honest private hyperparameter selection."" Proceedings of the aaai conference on artificial intelligence. Vol. 36. No. 7. 2022
[4] Koskela, Antti, and Tejas Kulkarni. ""Practical differentially private hyperparameter tuning with subsampling."" arXiv preprint arXiv:2301.11989 (2023). 


Limitations:
The paper does address the technical limitations of the paper (specifically the assumption of access to public and private data). The main improvement for the limitations is to address the comparison to other tuning algorithms or optimization algorithms that don’t require as much tuning. 



Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method to conduct hyper parameter tuning for DP stochastic gradient descent. The method is based on a linear scaling rule, with two pilot runs using small PLBs and a third run chosen based on a linear extrapolation from the first two. The pilot runs are used to establish an estimate of the interpret and the slope that the total step size r would have with the PLB. The author uses this linear scaling rules to demonstrate that it works as well as grid search in optimizing for the accuracy in a suite of benchmark tasks, and attempts to apply this rule to perform empirical analysis on the potential of making existing model architectures DP and the issue of robustness against domain shifts.

My assessment, consisting of strengths, weaknesses, and questions, can be found in the sections below.



Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The best thing about this paper is that it develops a method based on an intuition that is potentially worthwhile. This intuition is captured in the small paragraph in Section 2, titled Linear Scaling is Intuitive. What the authors have proposed is essentially a dimensional reduction to the hyperparameter search, and the reason why that works, in the sense that what you end up finding may not be so far off from a greedier search, is due to the geometry where you force the updates to be more congruent with each other. The whole idea of a linear scaling would otherwise be rather unremarkable, but if the author can further develop this intuition, formalize it and expand on it, it would contribute some insight to the literature.

Weaknesses:
The most damning weakness of this paper is that it is written without due care. As a consequence, the main results and the accompanying algorithm are not correct as stated. I don’t suggest that the author is not capable of presenting the correct science -- to that question I do not know the answer. However, as things stand, the paper is not ready to be published.

The presentation in the introductory and main result sections wanders seemingly fluidly between epsilon-DP, (epsilon, delta)-DP and Gaussian DP:
1. Definition 1.1 is given in the language of (epsilon, delta)-DP;
2. The DP-SGD Definition is given without a quantification of its DP guarantee at all;
3.  Algorithm 1, which employs the DP-SGD given before, states that its output is epsilon-DP, where an alleged PLB accounting between epsilon and sigma is not supplied. (In reality, a delta would be needed, so the provided guarantee is incorrect to begin with.)
4. Then Proposition 2.1, which concerns Algorithm 1, gives a GDP guarantee in relation to sigma only, where sigma is not constructed as a function of epsilon (or the missing delta) in Algorithm 1;
5. Corollary 2.2 now qualifies Algorithm 1 as (epsilon, delta)-DP, with a one line proof given in the Appendix citing another work and has no substance on its own.

All of the above is confusing at best. For a standard reader, a student coming into the DP world for example, these are not pedagogically informative.

Back to Algorithm 1:
1. It contains four privacy loss budget expressions: epsilon, epsilon_0, epsilon_1, and epsilon_f. Based on the context, am I to infer that epsilon is the sum of the rest of the three? 
2. The quantity r on the 12th line (beginning with Decompose). Is this a generic r, as you use it on line 7, or is it in fact referring to r* on line 9?
3. When you speak of the “decomposition” or r, what is to be found exactly -- eta given r and T (my guess), T given r and eta (please explain), or both eta and T given r (please explain as well)? If my guess is correct, then do we know that the eta found here will automatically satisfy the condition given in Theorem 2.3? 

Line 143 begins with “We apply this theorem to logistic regression.” Then Line 151 continues, “While our theorem only holds for linear models…”. Nothing said between Line 143 and Line 151 constitutes a proof that Theorem 2.3 applies to linear models. This point should either be rectified with a formal analysis or deleted, so as to not be an exaggeration of contribution.

Section 3.1 is misleading and should be thoroughly rewritten to rid all expressions of “randomly”, “sample”, and “uniformly”. The author picked the experimental values. No sampling, particularly random sampling nor uniform random sampling of values took place. It is not clear to what is “r = 75” an approximation (Line 179).

In addition, based on my reading of Section 3.2 I believe it should not be presented as is.  My understanding of what Section 3.2 does is that it uses the linear scaling rule proposed in this work to construct ""accuracy hypotheticals” for the listed models and datasets as well as the domain shift situations, and compare those numbers with existing experimental results. If that is the case, this is a dangerous operation. The linear scaling rule, when used as a heuristic to make tuning faster, is fine as the worst that could happen is that one misses out on the most efficient model tuning. However, the way that the rule is employed in Section 3.2 it is taken as a scientific theory between epsilon and accuracy. The accuracy numbers you get from it is no different than a terribly extrapolated number based on a linear model fitted with two data points. If you really want to use the linear scaling rule to poke at the said questions, actual experiments should be conducted to confirm these extrapolations. Of course, I may have misunderstood what was actually done and in particular, whether actual experiments were performed — although if so, what would be the contribution from the linear scaling rule?


Limitations:
As stated before, I believe the paper is written hastily to the point that the central results presented are incorrect, significantly harming the quality of the contribution and its readability. I am also concerned with the scientific merit of Section 3.2. These points are elaborated in detail in my comment section on Weaknesses. 

Rating:
5

Confidence:
3

";0
TUGoUNkccV;"REVIEW 
Summary:
This paper seeks to present a biologically plausible learning approach for supervised learning in deep neural networks. Unlike backpropagation, the approach does not require symmetric weights in the forward and backward directions. The approach relies on an information theoretic approach which seeks to maximize mutual information between layers in the forward and backward direction. The approach is demonstrated on simple data sets (e.g. MNIST) as well as on 3-compartment models of pyramidal neurons.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The manuscript addresses an important problem--essentially how can supervised learning be implemented in biological neural networks. It proposes a solution to the well-known weight symmetry/transport problem. And it seeks to do so in a principled way using information theoretic notions

Weaknesses:
Unfortunately, the paper is poorly written, with heavy notations and equations which often obscure the approach rather than clarifying it.

There is no clear expression for the learning algorithm. It is hard to see that the learning algorithm is local both in space and time, which is a major requirement in a biologically plausible network.

For the experiments, the authors report the test accuracy. However other metrics would also be interesting, for instance, the degree of symmetry between the final weights in both directions. 

It seems that the approach still requires propagating error information over long distances (across many layers) which may also be problematic from a biological point of view.

Supervised learning is not particularly biologically plausible.  This point should be addressed, as a minimum using self-supervised learning in combination with the proposed approach.

The authors should mention more clearly that the weight transport problem is completely solved by random backpropagation or feedback alignment. Thus the advantages of their approach, if any, should be contrasted with feedback alignment.

Limitations:
See some of the remarks above. There is no discussion of the limitations of the proposed approach.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors present a novel strategy for learning in neural networks. In particular, the authors derive update rules for neurons/synapses which maximises the correlative information between layer activations. This strategy avoids the weight transfer problem, and naturally gives rise to a biologically emulating architecture of multi-compartment pyramidal neurons with lateral inhibition. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- the authors present what seems a mathematically sound and creative strategy for credit assignment. Without extensive knowledge in this area, the derivation of update rules seems original and of good quality
- The resulting likeness to a multi-compartment model with lateral inhibition is interesting
- the text is generally well written (though the presentation itself is dense, see below)

Weaknesses:
- in general I found the paper very dense - I personally think 27 equations is too many for a main text. I appreciate that the main contribution of this paper is analytical, but the think the authors would do well to sacrifice some of the less key equations (move to SM) to make space for additional intepretation/experiments
- As stated above, I would have liked to have seen more intepretation and experiments with respect to the model. For example, what predictions does the model make in terms of the balance of bottom-up/top-down signals? How does this change over learning? How does it compare to biology? Same for interneurons
- The actual performance of the model does not seem too impressive, at least compared to standard backprop (e.g. on the CIFAR-10 dataset). Moreover, given that a key property of the model is to avoid the weight symmetry issue, I would think it sensible to compare the model to backprop with random feedback weights (feedback alignment).
- I think the authors coud make it more explicit what are the differences between their model and the model in Golkar et al. 2022. In particular, explicitly highlighting the similar and new terms when presenting the mathematical formulation

Limitations:
I would recommend a limitations section (or at least more discussion). For example, I would be interested to know if the sensitivity of the model to hyperparameter choices is high, or whether there is a strict need for the feedback matrix at the last layer to be the identity.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors introduce a biologically plausible training paradigm for a deep neural network that sidesteps the weight transport problem while achieving competitive results. Their approach is normative, in that both the network's architecture as well as its learning rules can be derived from an information maximization approach. The asymmetry between forward and backward weights is achieved by leveraging two different formulations of the inter-layer correlative information.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
*Originality:*
The work provides a novel approach for deriving biologically plausible strategies for learning in deep neural networks.

*Quality:*
The paper contains a significant amount of work to support its findings. Importantly, both theory and computation are used in tandem.

*Clarity:*
The presentation is mostly clear, but some significant explanations are missing or too sparse. See below. I also want to praise the authors for including the code that they used with the submission (something that I believe should be true for all papers, but sadly is not).

*Significance:*
The work is significant for neuroscience because the learning algorithms used by the brain are not yet understood. Having a good grasp over the range of possible mechanisms that biology could have used to train natural neural networks is essential to allow experimentalists to probe what choice(s) is (are) actually used. The work is also of potential significance for machine learning, since the algorithms used by the brain might provide advantages over the gradient descent with backpropagation methods used to train artificial neural networks.


Weaknesses:
1. The correlative mutual information metric requires a bit more discussion. The regularization coefficients $\epsilon_k$ appear in eq. (2) but are not discussed at all until much later, and even in the derivation in Appendix A, the need for this regularization is not explained. On first guess, the need for $\epsilon_k \ne 0$ is due to having a low rank covariance matrix $\mathbf R_{\mathbf r^{(k+1)}}$. However, this seems inconsistent with the importance of these coefficients in the network dynamics and learning rules. This requires a more detailed discussion in the main text, and especially in the Appendix. (If space is an issue, I suggest removing most of lines 170-174, which are almost identical to 149-153; it can simply be stated that the sample covariance matrices from eq. (6) need to be used instead of their exact counterparts to get online training rules.)
2. Related to the regularization coefficients, I am a bit perplexed by eqns. (10), (11). The Taylor expansion in these equations is performed around the identity, but that makes the expansion parameter be $1 / \epsilon$. Since $\epsilon$ is small, $1 / \epsilon$ should be big, making it hard to justify ignoring subsequent terms in the Taylor expansion. This needs to be explained in detail.
3. The jump to the dynamics equations (15)–(17) is too abrupt. Either an explanation should be provided or a reference to a relevant Appendix section.

Limitations:
The authors have adequately discussed limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes Correlative Information Maximization as an underlying objective for biologically plausible learning. The objective produces a multi-compartmental neuron model, and can operate with feedback connection that are plastic, but not tied to the feedforward ones. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The (approximation to the) CorInfoMax objective produces a tractable model of a neuron with several compartments. This resonates with previous ideas of credit assignment with apical dendrites, and (I guess) generates experimentally testable predictions due to the specific interactions between compartments and weights. 

The approach to weight symmetry is interesting and might implicitly lead to weight symmetry (although see weaknesses). 

Overall, this is a novel idea, even though it is very related to previous works that use apical dendrites/predicting coding/etc. as a mechanism for credit assignment. 

Weaknesses:
The experiments in Tab. 1 have multiple problems. There's no comparison to backprop and no explanation of used architectures in the main text. Presumably the architectures were pretty small, given poor CIFAR10 performance.

Related, all experiments show feedback alignment-level performance (i.e. good on MNIST, OK on CIFAR10 for a small network that reaches about 50% accuracy). Thus, we can’t draw any conclusions about the effectiveness of this approach without considering at least larger networks and maybe harder datasets (as feedback alignment doesn't scale beyond those cases). The minimum aim would be to train a standard ResNet18 on CIFAR10 with backprop (should be around 90% accuracy), and compare it to all algorithms in Tab. 1. 

The authors also missed an important previous work -- Deep Learning without Weight Transport by Akrout et al. (2019). That paper proposes a simple mechanism for the weight transport problem that is a bit different from the one here, but it is still worth discussing in the context of backprop approximations/alternatives. 

Limitations:
The limitations and potential impacts have been addressed.

Rating:
6

Confidence:
4

";1
KbqQMoqfLQ;"REVIEW 
Summary:
The paper extends the work in Flash Attention to apply block wise attention to not just the self attention block, but also to the feedback forward part of the transformer block. While doing so, the output of the self attention block is not stored for backprop, and recalculated. This leads to very efficient gain in memory requirements for training on large sequences.

As already shown by FlashAttention work, performing these operations at block level can take advantage of limited SRAM by scheduling operations in the right order to reduce the amount of communication thereby not just leading to memory efficiency, but also improved throughput and latency.

Experiments on multiple datasets show that this improves upon the foundation laid by FlashAttention by being able to support longer sequences along with throughput improvements.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- Efficiency of Transformer Architectures is a pretty important field, and any improvement in this area has a wide impact on various applications of Transformers.
- Memory Efficient architectures are key to the future of both research and deployment.
- The paper does a reasonable number of experiments to prove that even though the operations are done blockwise, the throughput improves because of better SRAM utilization with reduced communication.
- Table 4: By increasing the sequence length, they are also able to get better validation loss.

Weaknesses:
- Line 23: ""such as"" is repeated.
- The contribution on top of Flash Attention is pretty limited, but still the impact is pretty significant.

Limitations:
Limitions are well noted in the paper.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposed to merge the computation of the FFN layers in Transformer into the block-wise computation of self-attention. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence, because FFN is entirely position-wise and hence block-parallelizable.

Experiments on large language pretraining and reinforcement learning demonstrate that BPT reduces the peak memory costs and allows training large-scale Transformer models on longer sequences.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed method is straight forward and well-motivated. The paper is writing well, easy to follow. Experiments show the benefits of BPT on long sequence training, especially the cost of memory.

Weaknesses:
The novelty of the proposed method is not that significant, and there are still some parts unclear (see questions).

Limitations:
NA.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents a novel variant of the Transformer model, the Blockwise Feedforward Transformer (BFT), which is designed to address the memory inefficiency issue in standard Transformer models. The authors propose a blockwise computation method that significantly reduces the memory footprint without compromising the model's performance.

The key innovation of the BFT is the introduction of a blockwise computation method. Instead of computing the feedforward network on the full sequence, the BFT computes it on intermediate blocks, resulting in substantial memory savings. The computation for a query block is given by: Outputi = FFN(Attention(Qi, K, V ) + Qi) + Attention(Qi, K, V ) + Qi.



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The Blockwise Feedforward Transformer (BFT) presents a novel approach to address the memory inefficiency issue in standard Transformer models. This is a significant contribution as memory efficiency is a critical factor for training long context language models
2. The authors conducted extensive experiments on both reinforcement learning and language modeling tasks, providing a comprehensive evaluation of the model's performance across different domains.


Weaknesses:
1. The paper lacks a comprehensive analysis of the model's performance, particularly in terms of perplexity or its impact on downstream tasks. This omission makes it challenging to assess the effectiveness of the BFT in maintaining performance when compared to other models.



Limitations:
The authors argue that ""The application of long context models to NLP training remains uncertain due to the scarcity of large datasets encompassing extensive context information."" However, resources such as the Books Corpus, Github, and Arxiv offer a wealth of long documents. It would be beneficial if the authors could experiment with training their proposed models using these resources and conduct a more thorough evaluation of the long context capabilities of Language Models.






Rating:
7

Confidence:
3

REVIEW 
Summary:
""Blockwise Parallel Transformer for Large Models"" presents a novel approach to handle the memory demands of Transformer models, mainly when dealing with long sequences or tasks involving multiple sequences or long-term dependencies. The authors propose a method called Blockwise Parallel Transformer (BPT) that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. 

The critical contributions of the paper are:

1. The authors propose a blockwise computation of self-attention and feedforward approach that enables 16 to 64 times longer and 2 to 4 times longer context lengths than vanilla Transformer and previous memory-efficient Transformers, respectively.
2. They demonstrate the effectiveness of their approach through empirical experiments. BPT can reduce the memory requirements of Transformers, enabling training at least 8 to 64 times longer sequence than vanilla attention-based GPT models and at least 2 to 4 times longer sequence than prior state-of-the-art FlashAttention and Memory Efficient Attention.
3. The authors also show that BPT significantly improves performance and achieves better results on challenging reinforcement learning benchmarks by conditioning on multiple trajectories.

The authors argue that their approach has the potential to enable the training and evaluation of more complex models that require longer input sequences, which could lead to further breakthroughs in AI research.

The authors also explain the memory bottleneck of Transformer models and how their proposed method addresses this issue. They explain how self-attention can be computed blockwise without materializing the softmax attention matrix, resulting in significant memory savings. They also show how this blockwise computation can be applied to the feedforward network, resulting in further memory savings. The paper includes a detailed algorithm of the BPT method and key parts of its implementation in Jax which is nice to see.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**Originality:** BPT enables processing longer input sequences while maintaining or improving performance. This is achieved by computing both the feedforward network and self-attention blockwise, significantly reducing memory requirements (Page 9). This approach is original as it combines the techniques of blockwise computation of self-attention and feedforward networks, which is a distinct approach from previous works.

**Quality:** The paper demonstrates the effectiveness of the BPT approach from the perspective of speed/throughput (but not quality/capability of the model) through empirical experiments. It shows that the BPT approach can train on 8 to 64 times longer sequences than vanilla attention-based GPT models and 2 to 4 times longer sequences than prior state-of-the-art models like FlashAttention and Memory Efficient Attention.

**Clarity:** The paper is well-structured and clear in its presentation. It provides a comprehensive background on Transformers and their challenges in handling long sequences. The paper also clearly explains the proposed BPT approach and its advantages, such as its ability to leverage hardware with significantly faster SRAM speed compared to HBM speed. The evaluation of the method is also clearly presented with comparisons to other attention mechanisms.

**Significance:** The paper's significance lies in its potential to enable the training and evaluation of more complex models that require longer input sequences.

Weaknesses:
The use of RL as a benchmark for testing the quality of BPT, is a weird choice to say the least, especially given how noisy we know RL is. Language modeling is the primary use case for transformers and by extension BPT. Therefore, evaluating BPT's performance (perplexity) on language modeling tasks would be more appropriate.

I initially misunderstood that this is not an approximation but an exact implementation of attn+ffn (similar to flash attention). Therefore it's not imperative that there's LLM benchmarks but regardless would be nice to see that this implementation gives exact same output as standard attention in all precisions int8-fp32.

Limitations:
**Dependence on Hardware:** The paper mentions that the BPT can leverage hardware with significantly faster SRAM speed compared to HBM speed (Page 5). This implies that the performance of BPT might be dependent on the specific hardware configuration, and may not perform as well on hardware with slower SRAM speed.

**Sequence/Tensor Level Parallelism:** How does it work with parallelism schemes that chunk on sequence or FFN.

Rating:
7

Confidence:
4

";1
zyZkaqNnpa;"REVIEW 
Summary:
This paper theoretecally and empirically showed that the inductive bias of default- ERM maximizing the margin causes shortcut learning in  a linear perception task.
It proposed uniform margins that leads to models that depend more on the stable than the shortcut feature and suggested loss functions encourage uniform-margin solutions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper analyzes shortcut learning theoretically in terms of margin maximization.
I have not seen an analysis from this perspective before.
It also proposes the concept of uniform margin from theory and suggests a method to prevent shortcut learning.

Weaknesses:
The theory itself has limited applicability due to the linear model, and we do not know how well it can actually be explained in general terms in actual deep learning models.

Limitations:
The theory itself has limited applicability due to the linear model, and we do not know how well it can actually be explained in general terms in actual deep learning models.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper provides an in-depth analysis of the phenomenon of ""shortened learning"" in machine learning models, especially in the context of perceptual tasks. The authors confirm that basic empirical risk minimization (ERM) methods tend to prefer models that depend on shortcut features, even when models can achieve zero loss using only stable features. They attribute this to ERM's inductive bias to maximize margins across all samples. To address this, the authors propose an alternative loss function biased inductively with a uniform margin called MARG-CTRL. The paper demonstrates that MARG-CTRL mitigates shortcut learning on multiple vision and language tasks without the use of annotations of the shortcut feature in training or even validation. They also show that MARG-CTRL performs on par or better than the more complex and costly two-step shortcut mitigation method.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
-This paper presents a thorough analysis of the shortcut learning problem. 
-The authors propose a novel solution, MARG-CTRL, which has been shown to effectively mitigate shortcut learning in several tasks.
-The paper is well-written and easy to understand.

Weaknesses:
-Although MARG-CTRL is shown to perform well across different tasks, it is not clear how MARG-CTRL perform in scenarios where the shortcut features and the stable features are highly correlated.
-The paper compares MARG-CTRL with two-stage shortcut-mitigating methods like JTT and CNC, it would be helpful to understand the specific scenarios where MARG-CTRL outperforms these methods and where it does not.

Limitations:
Please see the weaknesses part.

Rating:
5

Confidence:
3

REVIEW 
Summary:
Having had my concerns addressed by the authors I have updated my score.
-----------------------------------------------------------------------------------------



* The paper proposes an explanation for why neural networks tend to learn spurious features over stable fratures which lead to a lower loss and better performance.
* The main argument for this is due to the maxmargin losses induced through cross entropy, to counter this the authors propose several losses which induce a uniform margin.
* The authors propose a nice example and demonstrate the results on vision and language datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper addresses a very important topic of the simplicity bias, and goes a significant way in addressing it. 
* The comments and explanations are clear and concise, and follow a clear narrative.
* I find the arguments that inductive bias towards uniform margins for perception tasks to be convincing.
* The experiments are well conducted and clearly support their statements

Weaknesses:
* I find the scatter approach to testing many margin based losses a little un elegant, I would rather know which is the most effective in the paper and then see the results for other attempted losses in the Appendix. But this is just me preference.
* Is Corollary one explicitly followed up with an experiment? I would like to know if it actually exhibits this behaviour or is there just the potential to?
* The experimental datasets are rather limited, Waterbirds, CelebA, Wilds etc. Not a negative per se, but it would be nice to see authors trying to improve the testing framework for this probelm.

Limitations:
Not explicitly discussed

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper explores the phenomenon of models using easy-to-learn spurious features (aka, shortcuts) instead of reliable but harder-to-learn true features. They find in their theoretical that max-margin relies on the spurious feature while controlling for uniforming margin induces learning the true feature. With this insight, they try numerous loss functions that provide a bias towards uniform margins and find improved worst-group accuracy on a variety of tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Experiments show good results for their methods.

Weaknesses:
* The informal statement of Theorem 1 in the main text is too vague in my opinion. In particular, it's not stated what the regime of $n$ is and how these other quantities interact with it. 
* The graphs are tough to read (too small) and the legends overlap with the plots.

Limitations:
The authors adequately addressed the limitations.

Rating:
6

Confidence:
3

";1
1FVmMlifl7;"REVIEW 
Summary:
This work focuses on the teacher-student paradigm in theoretical machine learning and shows that, for a unique optimization scheme that involves directly optimizing on the eigenvalues/eigenvectors of the data, a stable subnetwork in the student can be identified that can mirror the complexity of the teacher network. The area of this work is outside of my domain, so I am unable to comment further on the contributions. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The approach seems theoretically-motivated and the result seems interesting. The area of this work is outside of my domain, so I am unable to comment further on the contributions. 

Weaknesses:
The experiments seem limited, but the area of this work is outside of my domain, so I am unable to discern what level of experimentation is normal for this kind of work. 

Limitations:
I did not see a section dedicated to limitations of the authors' work. 

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper tried to understand and extend a new parametrization, spectral parametrization, for fully connected networks. They empirically show that in the teacher student setup, even when the student network is highly over parametrized, the student network under that parametrization will converge to a somehow ""sparse"" network that can be compressed, using standard optimizers. And they show that standard parametrization for the student network cannot do that.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
No.

Weaknesses:
1. Lack novelty. Why this parametrization can lead to sparsity is almost well-understood in the literature. For instance, it's well known that for such model $h_{\theta}(x)=h(x;u\odot v)$, if we initialize $u,v$ to be small and use standard gradient based algorithms, we will have a gradual rank increase for $u,v$, which leads to sparsity more easily. For some reference we can look at Abbe's paper https://arxiv.org/abs/2306.07042 (This is the most recent reference) or Jason's paper https://arxiv.org/abs/2207.04036. Or we can simply compute GD dynamics for using diagonal linear network to learn a linear target with small initialization. In a word, this result is not surprising to me. I feel like I already know this/ expect that happens.

2.For the empirical result, the input dimension in the experiments is too low. It's only 10. We have high dimensional input in practice. Also, the experiments are insufficient. Like you can try different optimizers, try different hyperparameters (width, initialization scheme), etc.

3.The authors didn't discuss the related works sufficiently. This phenomenon is clearly related to training dynamics of fully connected networks and there are many theory papers discussing the same things (even similar implication/conclusion).

Update: since the authors update the experiments, I have changed my score.

Limitations:
I don't think they discuss their limitations adequately. Here are my suggestions for improvement.

Since this phenomenon on simple network structures is actually almost well understood, if you really want to show this kind of reparametrization works, you should do some larger scale experiments. If there are some larger scale experiments to show that that kind of parameterization really works in practice and helps practice problems, then I think it's a good paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a novel technique that allows identifying an invariant subnetwork in a student model that mirrors the characteristics of the teacher in terms of computing neurons, path distribution, and topological attributes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 - The manuscript is clearly structured, and the subject of research is relevant
 - The authors have developed a novel technique to identify invariant characteristics of a student model mirroring key characteristics of the teacher network.

Weaknesses:
 - The authors have used a single synthetic dataset to perform the experiments.
 - There is little reference to related work, and no baselines are considered when comparing the proposed approach

Limitations:
The authors should acknowledge the limitations of their research and provide some insights on how these limitations could be addressed in future work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper analyses the performance of a spectral parameterisation/regularisation scheme for neural networks. After first introducing the spectral approach, the authors describe student-teacher experiments where they attempt to distil a fixed teacher network’s behaviour into a student network. The authors show that the spectral parameterisation gives equivalently good predictive performance, but that the structure of the trained network is considerably different. In particular, they show that the spectral network shows significant sparsity when it is over-parameterised with respect to the teacher. Intriguingly, they show a way to measure the “size” of a dense computational core that seems to be invariant with respect to changing the size of the student network.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is on the whole well-written and easy to understand. The authors have done a very good job of taking a theoretical topic and making it accessible and understandable.

The idea they present is simple at its core, and the results are very interesting: both that their parameterisation/regularisation scheme leads to marked sparsity and the presence of a “core” with invariant size.

While I think the results are somewhat limited (see below) the results that are presented are clearly described and care has been taken with the experimentation, leading to a convincing presentation. EDIT: the authors have fully addressed my concern around limited evaluation.

I am unable to judge the significance or novelty of these results, as I am not an expert in the field of this paper. I would say that from the perspective of a general Neurips participant that I found the results interesting and exciting. So while I would defer to experts to place the work within the literature, I will comment that I think the results may be of interest to the general Neurips audience.


Weaknesses:
I think the only real weakness of the paper is that the experiments are quite limited in scope. The authors use a single source of “teacher” data, which is a fully-connected network with a certain shape. I think the paper would be considerably stronger if the authors were to repeat their analysis with further sources of data. In particular, I think it would be an interesting complement to use a non-synthetic source of data as a training objective for the network, if a suitable source could be found. While this would limit the ability of the authors in making the exact correspondence between the size of the teacher and the effective size of the student, I think seeing the same behaviours - enhanced sparsity as compared to the direct parameterisation, and a “phase transition” like behaviour showing a “computational core” - would considerably strengthen the results of the paper. I think without some strengthening of the results it is difficult to accept the paper, but I think further results would change this assessment. EDIT: the authors have fully addressed my concern around limited evaluation.

While on the whole the paper displays good clarity, there are some places where the use of language leads to confusion. I have called out areas below in the questions section where I think there was particular confusion, and I think if these are addressed the paper will be sufficiently clear.

I didn’t understand the “path analysis” section, or what the significance of this result was. Perhaps being more precise with the description of the analysis would have helped (see questions).


Limitations:
I think the only considerable limitation is what I mentioned in the questions section about the weakness of evaluating with only one source of data.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors consider a knowledge distillation setting involving a teacher and a student neural network.
The authors exploit a few interesting tricks -especially the use of a spectral parameterization of the network, and special regularizers- to show that it is possible to enforce learning a submodule within a student network (as long as it is larger than the original teacher), that implements the teacher behavior with a minimal number of neurons, and which can thus be used to estimate the effective size of the teacher.

The authors present thorough theoretical work, along with experimental verification; the results are convincing, but the topic is outside my area of expertise.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is interesting and well written.
- The results are convincing.


Weaknesses:
- The empirical experiments could be more extensive.
- The dataset used is extremely simple, and it is not clear whether the method would work in more typical scenarios.


Limitations:
- Evaluation is performed only on small networks and on toy data. It remains to be seen how well the approach would work on complex, modern datasets and network architectures.



Rating:
6

Confidence:
2

";1
OjlZqQzw51;"REVIEW 
Summary:
This paper proposes a novel dynamic programming algorithm for optimizing the percentile criterion in offline Reinforcement Learning (RL) without explicitly constructing any uncertainty sets. The authors introduce the Value-at-Risk (VaR) Bellman operator for optimizing the percentile criterion, and show that it is a valid contraction mapping that optimizes a tighter lower bound on the percentile criterion compared to Robust Markov Decision Processes (RMDPs) with Bayesian credible region (BCR) ambiguity sets. The paper also provides theoretical analysis and bounds on the performance loss of the proposed framework and compares the asymptotic sizes of BCR and VaR ambiguity sets. Empirical results demonstrate the efficacy of the VaR framework in several domains.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
As a non-expert in this field, I find the paper interesting and addressing an important problem in offline reinforcement learning by proposing a novel dynamic programming algorithm.
The authors provide a thorough analysis of the proposed VaR framework, including performance guarantees and comparisons with BCR ambiguity sets.
Empirical results show the effectiveness of the VaR framework in various domains, outperforming baselines in terms of robust performance.

Weaknesses:
The clarity of this paper could indeed be improved in several aspects. For instance, the authors use ""w"" to represent the index in line 89 and also use ""w_{s,a}"" in line 139 to represent the values, which might be confusing for readers.

Additionally, the authors attempt to answer a key question in this paper: ""Are Bayesian credible regions the optimal ambiguity sets for optimizing the percentile criterion?"" However, this problem is not clearly defined, and the criterion for optimality is not explicitly explained. 


Limitations:
The paper does not discuss the application of the VaR framework to continuous state-action spaces in RL domains, limiting the understanding of its applicability to a wider range of problems. The time complexity of a single iteration in the algorithm, as mentioned, is O(SAN), which may make it challenging to apply the method to MDPs with large or continuous state and action spaces.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel offline RL algorithm, which is a dynamic programming algorithm to optimize a tight lower-bound approximation on the percentile criterion based on Value-at-Risk. 
A solid theoretical analysis is made to demonstrate the proposed VaR value iteration algorithm implicitly constructs tight uncertainty sets that are smaller in size than any optimized Bayesian credible region (BCR).
Furthermore, this work provides both finite-sample and asymptotic bounds on the loss within the VaR framework.
Empirical results show that VaR outperforms BCR Robust MDPs on several tasks.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The proposed VaR algorithm is devised to optimize a lower bound on the percentile criterion without explicitly constructing ambiguity sets compared to BCR.
2. The paper provides comprehensive theoretical analysis on various aspects, including the Lower Bound Percentile Criterion, finite-sample and asymptotic bounds on the loss within the VaR framework, as well as the Equivalence of VaR Bellman optimality operator and a unique robust MDP with SA-rectangular ambiguity sets. These analyses offer reliable guarantees and contribute to the robustness of the proposed approach.

Weaknesses:
1. The VaR algorithm heavily relies on a precisely estimated transition model $\tilde{p} _{s,a}$. However, in offline RL, the availability of a limited dataset poses a constraint on the quality of this estimation. This limitation could potentially pose challenges for the VaR algorithm, particularly in complex tasks where the accurate modeling of the transition dynamics becomes more challenging.
2. The proposed VaR, as originally designed, focuses on MDPs with discrete state and action spaces. 
Extending the algorithm to complex environments with continuous state and action space can be really hard.

Limitations:
**Limitations** is discussed in *Section 6*.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper considers robust policy optimization problem, which optimizes the percentile criterion to get a policy. It constructs an uncertainty set that contains the true model with high probability, and optimizes for the worst model in the set. Previous works use Bayesian credible regions as uncertainty sets, but it was usually considered to be overly conservative. This paper proposes Value-at-Risk based dynamic programming approach that does not explicitly construct uncertainty sets, but learns with a modified backup operator. The paper shows that theoretically and empirically the proposed approach constructs much less conservative robust policy compared to Bayesian credible regions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- First paper to consider VaR framework and VaR Bellman operators, which are quite novel.
- Concrete theoretical results for proposed framework and algorithm
- Explicitly comparing the size of ambiguity sets theoretically seems to be a novel approach
- The algorithm itself is quite simple and practical

Weaknesses:
- mainly only compares with BCR theoretically and empirically, while there exists a number of other different approaches in solving Robust MDP, as far as I know.
- I does not think showing the returns alone in the experiments are enough to prove the efficiency of proposed methods, since any RL algorithm that is independent to given $\delta$ will give the best return. In other words, we need to be sure that the algorithms satisfies the percentile criterion. In that sense, the empirical experiments do not seem complete.
- While this paper only focuses on methods to solve for percentile criterion and robust MDPs, it would be interesting to also compare against other safe RL or offline RL methods to see whether the algorithm works practically compared to those methods.

Limitations:
The authors addressed the limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper studies a type of robust offline reinforcement learning methods, percentile criterion optimization. Percentile criterion optimization optimizes the policy under the worst \alpha-percent of the models in an uncertainty set of models, based on the data. In this paper, the authors proposed a type of value iteration algorithms, replacing the standard Bellman backup values with value at risk (VaR) with respect to the posterior distribution of underlying models. The work is built on top of the previous work on Bayesian ambiguity sets and has a less conservative value estimates compared with previous work BCR.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. This work proposed a VaR based dynamic programming method to optimize the percentile criterion, and avoid explicitly construct the uncertainty set of models. The proposed dynamic programming method is more tractable computationally compared with some methods constructing the uncertainty set explicitly.
2. The theoretical analysis in this paper shows that the proposed method can always yield a value estimates that dominate the value function produced by BCR methods, and be a lower confidence bound in the meantime.


Weaknesses:
1. The notations and proofs details about the results to section 4 is not very clear.
 - a) It is not clear what is the definition of $v^\pi$. Which dynamics/models the value function is with respect to. 
 - b) The top paragraph in Page 22, in the proof of Proposition 4.1, is very unclear. Please cite and state which theorem it uses, and what's the condition and results, or derive the complete proof for this particular results.
 - c) What does the notation of a set minus a vector (or multiplied by a scalar) means in Eq (10)?
 - b) How does Theorem 4.4 shows ""the asymptotic radius of the BCR ambiguity sets grows with the number of states"" and is ""larger"" (claimed in the last paragraph of introduction), without showing the asymptotic shape of the BCR ambiguity sets. Theorem 4.4 only shows there exists certain elements in $\mathcal{P}^{BCR}_{s,a}$ that can be from a distance (depending on $|S|$) with $\bar{p}$.

2. The proposed methods and analysis are limited to the finite state-action space. 

3. The empirical study only includes different variants of BCR as baselines.

4. This paper lacks some discussion about related work.
 - a) It lacks of discussion on more practical, robust MDP inspired methods such as [1].
 - b) The discussion of related work is limited to the robust MDP literature and especially Bayesian robust MDP. It lacks of discussion on methods based on frequentists' lower confidence bounds, which is similar to Proposition 3.2, for example pessimistic approaches with tabular settings or linear function approximation ([2] and many similar work).

[1] Model-Based Offline Reinforcement Learning with Pessimism-Modulated Dynamics Belief. NeurIPS 2022.

[2] Is Pessimism Provably Efficient for Offline RL. ICML 2021.

[3] Robust Dynamic Programming. Garud N. Iyengar

Limitations:
The author discussed the limitations of their work.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes using percentile criterion optimization for offline and robust reinforcement learning. Instead of using the Bellman optimality operator for value iteration, this paper suggests an algorithm using the Value-at-Risk (VaR) operator to account for pessimism. The VaR operator takes the given percentile of the return among ambiguity sets rather than the maximum. The authors show theoretically and empirically that such an operator leads to tighter value estimation with a given confidence level compared to prior works using Bayesian credible regions. Experiments on several tabular MDPs show that the VaR framework performs better than prior works using Bayesian credible regions.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper investigates an important problem of robust RL: Bayesian credible regions can be unnecessarily large, leading to overly conservative policies. The proposed VaR operator provably improves prior methods using the Bayesian credible regions.

2. The paper provides a statistical complexity analysis and a computation complexity bound for the proposed empirical VaR operator.

3. The paper is generally well-written and easy to read.

Weaknesses:
1. The proposed method degrades to frequentist pessimism with confidence sets (e.g., [1,2]) for Gaussian cases, which means the method is essentially the same as prior frequentist methods up to constant factors when the posterior is normally distributed, or the approximation in line 8 of Algorithm 3.1 is used (i.e., *VaRN* in the experiments). From the experiments, it seems that *VaRN* performs similarly to *VaR*. It is unclear how much improvement the proposed algorithm has over prior frequentist methods, especially when the posterior is not normally distributed.
   
2. The experiments are conducted on small tabular MDPs, and it is unclear how to scale up the proposed method to real-world settings. While the focus of the paper is on the theory side, real-world scale experiments would greatly strengthen the paper.


[1] Jin, Chi, et al. ""Provably efficient reinforcement learning with linear function approximation."" Conference on Learning Theory. PMLR, 2020.

[2] Jin, Ying, Zhuoran Yang, and Zhaoran Wang. ""Is pessimism provably efficient for offline rl?."" International Conference on Machine Learning. PMLR, 2021.

Limitations:
Limitations are well discussed in the paper.


Rating:
5

Confidence:
4

";1
G8nal7MpIQ;"REVIEW 
Summary:
The paper proposes an interesting and novel idea of using large pretrained mulitmodal models to compute image-text alignment 
score to use as a reward to train return-conditioned policies using a Decision Transformer (DT). They propose a novel 
imitation learning framework called Multimodal Reward Decision Transformer (MRDT) which trains a return-conditioned policy
using adaptive reward signals from image-text multimodal rewards.

The authors also propose a fine-tuning scheme that uses VIP and IDM objective to fine-tune using CLIP-Adapter using data from in-doman demonstrations to improve performance.

The paper also shows MRDT successfully improves generalization to unseen levels and unseen goals on 3 environments from OpenAI procgen benchmark. 

Authors also present an analysis on how MRDT helps mitigate goal misgeneralization and present a set of metrics to evaluate the quality of multimodal rewards.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Paper is well-written and easy to follow
2. The proposed approach is interesting and is shown to be effective on the OpenAI Procgen benchmark when generalizing to 
unseen levels and unseen goals. It outperforms prior work InstructRL on all 3 environments by a decent margin.
The results also show that finetuning using proposed fine-tuning scheme further improves the results.
3. The analysis on quality of multimodal rewards is quite informative. It gives a clear insight on how the multimodal rewards differ 
for different cases . The analysis shows finetuned multimodal rewards better capture distinction, distance (S) and distance(F). 
4. The analysis in figure 8 shows the rewards generated with instructions aligned with the goal state gets higher reward compared 
to a random instructions. This demonstrations a pretrained multimodal model like CLIP can give good reward signal to the policy.

Weaknesses:
1. The paper proposes an interesting idea but the experiment section is lacking breadth. I'd appreciate if authors consider evaluating their 
method on multiple benchmarks which have multi-modal input. Similar to InstructRL paper authors can consider a subset of tasks from RLBench 
for evaluation of the proposed method. The environments in ProcGen are quite simple and the generalization is only being tested for
 unseen instantiations of a single task (for 3 environments). Does MRDT work well if we want a single policy that can achive different goals? 
One example is Object Navigation problem (but in simpler grid world environments), where the agent has to navigate to single instance of one of the n target object categories i.e. Find a chair/Find a sofa.
 It'd be nice if authors can add more experiments from different benchmarks to the paper.

2. It seems like for all 3 environments in ProcGen benchmark a better reward can be cosine between image embeddings of current state vs the expected goal state. 
Have authors tried using this simple baseline? Does this lead to better or worse results than MRDT? The concern I have is that these environments are quite simple 
and might not require multi-modal rewards. 

3. Does using multimodal rewards from CLIP like model leads to goal misrepresentation problem? For example, let's consider a augmented version of Maze II environment. 
If in addition to the yellow gem and diagonal line there was a straight red/yellow line and the task was """"Navigate a maze to collect the diagonal line"""" or """"Navigate a maze to collect straight line"""",
 how would all the baselines perform? Are these multi-modal rewards capable of clearly distinguishing these similar looking goals? or this leads to agents learning a average policy where it
 sometimes confuses the straight line with diagonal line because of misrepresented reward signal?

4. The qualitative results in appendix B are quite interesting and I appreciate authors added these results in the supplementary. In Figure 9, for the coinrun task the 
Fine-tuned CLIP rewards are almost as high as the reward that agent achieves at the goal state. This seems concerning and hints towards multi-modal rewards not being able to distinguish 
goal state from a random subset of states. This could lead to policy learning arbitary behaviors or not learning the task at all if trained/finetuned using RL instead of imitation learning. 
Do authors have any insights on why the finetuned reward model show unexpected behavior? And how can this be mitigated so that we can successfully use these rewards for online training/finetuning

Limitations:
1. The experiments section seems lacking at the moment and could benefit from evaluation on different benchmarks
2. Multi-modal rewards post finetuning exhibit unexpected behavior which could prove to be a big problem when finetuning policies 
using these rewards. These rewards can sometimes misrepresent the value of the state agent is in.
3. It is not clear how well the approach would work in more complex tasks with partial observability and non-markovian states. 


Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose a formulation of the reward-conditioned decision transformer but do so using a learned reward function estimated by a pretrained CLIP style model which maps expert demonstrations and textual descriptions of these demonstrations into the same domain. They evaluate their method on the coinrun and Maze navigation domains. 


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The method is principled and the intuitive: The authors describe how to finetune existing language and vision models and use alignment between the observation and the text a reward for a decision transformer.
2. The method figures are clear and easy to understand (except Fig1 see weaknesses).


Weaknesses:
1. Example poorly motivated: How is goal misgeneralization any different from policies overfitting to training environments? Figure 1 needs improvement since the whole paper is motivated by it and currently it is impossible to tell what is happening in the 2 panes.
2. Comparison with baselines: The idea of using CLIP for reward generation is not entirely new and Zest https://arxiv.org/pdf/2204.11134.pdf is a very close related work published about a year ago. Comparing to this paper is a natural baseline.
3. Generalization across language commands : Does the method enable generalization using language commands? The only experiments are demonstrated are domain generalization (unclear how OOD the test tasks are in comparison to the train).
4. Unclear how the method prevents goal misgeneralization: While the introduction motivates the work from this problem, the experimental section does not lend any credence to the fact that this method would prevent such a problem. Experiment


Limitations:
The paper evaluation is a bit tenuous and I would encourage expanding the scope of the experiments. Currently, a lot of the experiments are geared towards evaluating the reward. Since a pretrained model, i.e., CLIP is being used to generate the rewards, it would be useful for readers to know how much of a domain alignment is needed for this work and to what extent the method breaks when trained on vastly OOD environments. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a framework called Multimodal Reward Decision Transformer (MRDT) that uses the visual-text alignment score from pre-trained vision-language models (after careful fine-tuning using the in-domain data) as the reward signals in visual-based reinforcement learning. Specifically, the authors propose to train a return-conditioned policy based on Decision Transformer (DT) with the improved reward signals learned by the tuned VLMs. The method is shown to generalize better than without the multimodal reward strategy in environments with unseen goals partly due to the knowledge captured in pre-trained models.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
+ The proposed method leverage pre-trained VLMs in a simple yet effective way for reward learning.
+ This is an important direction to study foundational models for decision-making. Compared to directly using pre-trained visual embedding models, this strategy of reward learning is more promising so far, IMO.

Weaknesses:
- Limited experiments. The authors only perform their studies on three tasks in ProcGen, far from most published work on this subject.
- Due to the limited experiments, it might require higher technical novelties. However, the novelty is also limited since the proposed method is built on top of several existing methods (VIP, IDM, CLIP-Adapter, etc.)

I am open to raising my score given more experimental evidence.

Limitations:
The major limitation is the lack of adequate evaluations. The approach is an otherwise promising one.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work tackles the goal misgeneralization problem in goal-conditioned RL agent setups. They propose to leverage pre-trained multimodal models, i.e., CLIP, to serve as a reward function that simultaneously estimate both text and image modalities for deciding an engineered reward for each time step. The authors train a transformer-based model to take a sequence of encoded state representation, estimated reward from CLIP, and historical actions, to predict the next reward and state representations for learning a return-conditioned policy. Additionally, to encourage smoother and more robust reward functions, they adopt value implicit pre-training and inverse dynamic model to incentivize better adapted rewards. The experimental results on OpenAI Procgen benchmarks demonstrate better generalization abilities over baselines without the proposed reward engineering scheme.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- Utilizing CLIP as an adaptive reward function is sound and easy to implement.
- The proposed idea is neat and should be transferable or up-scalable to more complex tasks and environments.
- The adoptions of both VIP and IDM are to the point for the proposed return-conditioned policy training.
- The reward evaluation protocols are well-motivated and justified.

Weaknesses:
- While the experimental setups are solid, the adopted environments (testbed tasks) are far from real-world imageries, and the adaptation may not utilize very well (and much) what CLIP has learned during the pre-training. More challenging environments are needed to justify whether the multimodal rewards that coming out from a pre-trained models are indeed useful. I.e., a valid baseline is to just train a small sizable multimodal transformer (with both VIP and IDM applied) and see if the pre-training alignment is indeed that important/useful.
- To really (and effectively) showcase the generalization ability, I suggest experiments that are conducted on generalizing to unseen (but perhaps visually similar) tasks as a testbed for the multimodal rewards. Using the testbeds this work adopt, for example, Maze I transferring to Maze II would be an interesting and insightful experimental setup.
- Literature reviews: using CLIP scores or CLIP-based perceptual loss [1] has been quite popular these days. The author should provide more in-depth discussion on related works along using pre-trained multimodal models as a strong supervision signal.

[1] Vinker, Yael, et al. ""Clipasso: Semantically-aware object sketching."" ACM Transactions on Graphics (TOG) 41.4 (2022): 1-11.

Limitations:
- The authors did not explicitly point out the limitations of this work. I suggest the authors discuss more on this matter in both their proposed ideas and the execution of the experiments.

Rating:
7

Confidence:
4

";1
Vfp8sDST4g;"REVIEW 
Summary:
This paper studies a graphical lasso problem where the precision matrix is restricted to be symmetric M-matrix and the associated GMRF graph has a special structure. Specifically, the authors consider the situation when the graph allows bridge-block decomposition so that vertices can be partitioned into k parts by cutting k-1 ""bridges"".  Under this situation, the authors show that the original problem can be decomposed into k subproblems, illustrated in theorems in section 3.2. Since the existing algorithm's complexity quickly increases as the dimensionality increases, decomposition into subproblems offers large computational benefits. Authors compare the performance of four different algorithms, using synthetic and real data, with and without leveraging the bridge-block decomposition and show that bridge-block decomposition provides huge computational benefits if the given graph has many edges that are “bridges”.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Authors provide a closed form of the elements of precision matrix corresponding to bridges, and make a connection with existing literature when underlying GMRF graph is an acyclic graph. This method is useful when one wants to solve MTP2 constrained graphical lasso problem with a penalty parameter that allows block-bridge decomposition. This work is resembles the existing literature on graphical lasso (without MTP2 constraint), such as Witten et al (2011); Mazumder and Hastie (2012), where graphical lasso problem can be decomposed to the smaller subproblems when graph has many connected components, and adds a contribution in the context of MTP2 constrained graphical lasso when graph has many bridges.


Witten, D. M., Friedman, J. H., & Simon, N. (2011). New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4), 892-900.

Mazumder, R., & Hastie, T. (2012). Exact covariance thresholding into connected components for large-scale graphical lasso. The Journal of Machine Learning Research, 13(1), 781-794.

Weaknesses:
Simulation settings and real data examples have some discrepancy from usual (graphical) lasso settings and its motivation. The main goal of graphical lasso is to discover the underlying conditional independency structure from the multivariate Gaussian data with various choice of penalty parameter (aka lasso path diagram). However authors first fix the graph in their settings such as preferential attachment graph and stochastic block model and choose penalty parameter according to the graph. It is understandable in the simulation study setting to show the computational benefits with block-bridge decomposition, but fixing the graph in the real data is not convincing, and also it is not clearly stated why MTP2 graphical lasso is appropriate to this crop image data problem (is it reasonable to assume that crop image data comes from high-dimensional multivariate Gaussian distribution? what is the interpretation of the resulting graph? why MTP2 constraint is necessary / plausible in this problem?)

In practice, graphical lasso is ran under various settings of penalty parameters and chosen appropriately such as cross validation. The existing graphical lasso decomposition (Witten et al (2011); Mazumder and Hastie (2012)) are useful since graphical lasso with high penalty parameter often leads to a graph with many connected components. In similar spirit, I suggest authors to illustrate how often the graph that allows block bridge decomposition appears by the choice of penalty parameter.

Limitations:
The main results are meaningful but simulation results and real data settings are less convincing. No potential negative societal impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors show that, in Gaussian MTP2 distributions, bridges in the graph structure have a closed form solution.  They use this observation to suggest practical solutions that can be applied whenever such models are being fit.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
A nice, more or less self-contained theoretical work that unifies and extends some existing lines of research into MTP2 Gaussian distributions.

Weaknesses:
Some room for improvement in terms of presentation and some minor typos.  It's harder for me to judge practical utility.  While it is true that MTP2 distributions have some practical utility (I've used them myself), it isn't clear that many real datasets meet that requirement (even approximately).

Limitations:
Some discussion of limitations, though it would have been nice to see a more clearly identified set of problems yet to be solved.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of estimating the precision matrix, which is the inverse of the correlation matrix, of a given Gaussian random vector $y$. The precision matrix $\Theta$ is assumed to satisfy a technical condition called MTP2 which states that $\Theta$ is symmetric and $\Theta_{i,j} \le 0$. This seems to be a well motivated assumption from various applications. The contribution of this paper is a technique for estimating $\Theta$ as follows: given a predicted sparsity pattern on $\Theta$ in the form of a graph $G$ , the natural optimization problem for estimating $\Theta$ can be solved by first solving the optimization problem on smaller 'blocks' and combining them across 'bridges'. They are defined as follows. Bridges are single edge cuts in  $G$ and the resulting connected vertices are called blocks.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper shows that given a block bridge decomposition, the optimization problem of estimating $\Theta$ can be efficiently solved by first solving the problem on the individual blocks and then combining the solution across bridges. The paper gives an explicit formula for doing so. In the case where $G$ is sparse, this can represent significant computational savings over estimating the entire precision matrix at once. Furthermore, since the work provides a structural theorem, any optimization algorithm can be used in conjunction with their observation.

Weaknesses:
I am not familiar with the literature but it seems like a big assumption to know the threshold graph explicitly. What happens if this graph is unknown? It seems to be more natural that the graph is unknown and one must estimate it.

An intermediate setting which also seems interesting is in the case where we know a noisy approximation to the block bridge structure. How do the proposed methods perform under such noisy information? Are the derived formulas robust?

What is the motivation for even assuming the block bridge structure? I can see social networks being one motivation but it would be more convincing if there were experiments on (real) social networks.


Limitations:
No ethical concerns.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of learning Gaussian Graphical Models (GGMs) satisfying a certain positive associativity condition among the variables, namely that the precision matrix has nonnegative off-diagonal elements. This condition is known as being ""multivariate totally positive of order two"", or MTP$_2$, and has applications in ML (where it corresponds to attractive Markov random fields), finance, and more.

MTP$_2$ GGMs come with the benefit that the traditional optimization procedure used to estimate the precision matrix of a GGM, namely the graphical Lasso, takes on a particularly simple and smooth form. Prior work had shown various polynomial-time convergence guarantees for the graphical Lasso under the MTP$_2$ assumption, but these are still not very suitable for practical applications (scaling with the dimension $p$ as $O(p^3)$ or $O(p^4)$). Other prior work had shown a closed-form solution for the graphical Lasso under the assumption that the ""thresholded sample covariance graph"" (an object defined in terms of the sample covariance and the regularization parameters used in the graphical Lasso), or thresholded graph for short, is acyclic.

The main contribution of this paper is essentially to generalize the latter closed-form result in terms of the ""bridge-block decomposition"" of the thresholded graph. The bridge-block decomposition of a graph is essentially a partition of the graph into components connected only by ""bridge"" edges. Formally, a bridge edge is an edge such that deleting it increases the number of connected components in a graph; it is effectively ""the only edge"" bridging two different components (see Fig 1). The paper's main theorem (Thm 3.3) essentially says the following: to solve the graphical Lasso for an MTP$_2$ GGM, compute the bridge-block decomposition of the thresholded graph, run the graphical Lasso for each component separately, and stitch them together using a simple closed-form formula. Moreover, this theorem also readily recovers as a special case the prior closed-form result for acyclic thresholded graphs. This is because in an acyclic graph, every edge is a bridge, and the bridge-block decomposition is particularly simple.

Thus the main result amounts to a divide-and-conquer recipe for learning MTP$_2$ GGMs, and the authors show various numerical experiments suggesting the practical superiority of this method over all prior methods (which operate on the entire graph). A key benefit is that the subproblems may be solved using any graphical Lasso implementation whatsoever, and potentially in parallel.





Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Disclaimer: I am not very familiar with the literature in this area, and my review should be taken as that of a relative outsider.

The paper's main result is both an interesting structural result about MTP$_2$ GGMs as well as a genuinely practical algorithmic advance in learning such models. From a conceptual point of view, the idea of leveraging the bridge-block decomposition seems novel. The overall result seems like a useful and nice contribution to the literature on this problem, and to the extent that one considers MTP$_2$ GGMs significant, one should consider this result significant as well.

The paper is largely clear and easy to follow (modulo some occasionally confusing bits; see the Questions section). It does a good job of setting up the main problem as well as the necessary context. I did not manage to verify the proofs in detail, but they seemed fairly clean, relying on an analysis of the KKT conditions of the graphical Lasso as well as some clever algebraic manipulation.

Weaknesses:
I think the main things to really evaluate about this paper are its novelty and significance. As an outsider to this area, I find this hard to accurately gauge, but I think the paper scores well on these fronts.

I do think the paper could benefit from a better conceptual overview of the main proof and the role of the bridge-block decomposition. The context and benefits are discussed adequately, but the key ideas in the proof do not come through very well, and the main proof seemed slightly magical to me. Why would one have expected the bridge-block decomposition to help? Was its role surprising?


Limitations:
The paper could use a couple additional lines in the final section about the technical limitations of this work and what the major next steps could be. I am not aware of any significant potential negative societal impact of this theoretical work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper focuses on the problem of learning large-scale Gaussian graphical models (GGMs) that are multivariate totally positive of order two (MTP2). The high-dimensional, sparse MTP2 GGMs are not easily manageable due to their size and complexity. The authors propose a novel approach, introducing the concept of a ""bridge"", to optimize the entire problem into several smaller, more manageable sub-problems and a set of closed-form solutions. The approach is based on the bridge-block decomposition of the thresholded sample covariance graph, which leads to reductions in computational complexity and improvements in existing algorithms. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed bridge-block decomposition framework on Gaussian graphical models seems novel. The problem is motivated nicely, and according to the authors, the framework could significantly reduce computational and memory cost. 

The proposed method seems to subsume various network structures, including the BA graph and the SBM, which are common models used in network analysis.

Experimental results are provided, and the computational results look promising. 

Weaknesses:
As the authors mentioned in the paper, the proposed method might not generalize to dense cases. Still I feel in many settings like BA and SBM, sparsity is a reasonable assumption.


Limitations:
See Above.

Rating:
6

Confidence:
3

";1
FkpMm9avyP;"REVIEW 
Summary:
This work focuses on deriving a uniform-in-time bound to the KL divergence between the NN-derived solution $\rho_t^f$ and the true solution $\bar{\rho}_t$, where $\rho_t^f$ is obtained via a push forward of samples of $\bar{\rho}_0$ with velocity parameterized by the neural network $f$. The neural network $f$ is trained to match the operator $\mathcal{A}[\rho]$ via the the Entropy-dissipation Informed Neural Network (EINN) loss given as an expectation over $\bar{\rho}_0$ of the integral from time $0$ to $T$ of the difference between $f$ and $\mathcal{A}[\rho^f]$. The method is then demonstrated to perform better than SOTA on McKean-Vlasov equations with singular interaction kernels: Coulomb interaction and Biot-Savart interaction.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Uniform-in-time bound to the KL divergence for finite duration $T$.
2. Method was shown to work on singular interaction kernels: Coulomb interaction,\ and Biot-Savart interaction.
3. Singularity for Biot-Savart kernel was removed in gradient computation during training. 

Weaknesses:
1. Addressed in the last paragraph of section 4, the similarity in methodology to Shen et al. [2022]. Though the implementation is similar, the theoretical contribution is different. Perhaps include a comparison of their result in Figure 1 with and without generalization to MVE. And perhaps discuss the generalization to MVE.
2. Also addressed in the second last paragraph of section 4, the similar in loss function to Random Deep Vortex Network (RDVN) of Zhang et al. [2022]. Though the losses are similar, the authors mention a lack of discrepancy control in Zhang et al. [2022]. Perhaps provide more discussion.


Limitations:
1. Addressed in Section 3, approximation error of neural network that if a neural network can represent the true $\mathcal{A}[\rho]$, EINN loss is zero. Perhaps reference some papers on neural networks designed to represent $f$ for MVE. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method for learning solutions to partial differential equations of the McKean-Vlasov type, and presents a theoretical analysis of the proposed method. In particular, for a certain class of equations, it is shown that the error bound does not increase exponentially in time. This is a significant result that takes advantage of the characteristics of the equations.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The uniform bound presented by the authors is a very strong theoretical result. Although I do not understand the details of the proofs, the proofs seem to be reliable because several references are provided. In my opinion, this paper is an excellent paper as a paper in the research area of numerical analysis.

Weaknesses:
The most important weakness is that it is not clear what practical applications the McKean-Vlasov type of partial differential equations have.  Unlike numerical analysis, which is a research area in mathematics, there are many people who are interested in applications in the machine learning community. So these results may not be of much interest to them. 

Another significant weakness is that the adjoint method and the Monte Carlo method are required in the proposed approach, which are computationally expensive. Considering the computational cost, the proposed method is not considered superior to classical numerical methods, such as the finite difference method.

In addition, the uniform bound seems to hold only for a class of equations, but this is not clearly written in Introduction. 

Limitations:
No potential negative societal impact is expected.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a novel method for solving the McKean-Vlasov Equation (MVE), a Partial Differential Equation (PDE) that models the dynamics of stochastic particle systems with mean-field interactions. The new approach, named Entropy-dissipation Informed Neural Network (EINN), leverages neural networks to estimate the velocity field underlying the MVE and minimize a proposed potential function. This method is particularly designed to tackle singular interaction kernels, where traditional methods struggle. The authors provide empirical comparisons with state-of-the-art (SOTA) neural network-based MVE solvers and demonstrate the superior performance of the EINN framework. Despite the advancements, a noticeable weakness in this work is the omission of a discussion on the neural operator framework for solving PDEs, which represents a significant progression in the field. This lack of consideration limits the comprehensiveness and depth of the study.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper provides a novel approach to solving MVEs. The idea of using entropy dissipation in the underlying system is inventive and beneficial to the community. 
2. The research is solid and thorough. The authors have established theoretical guarantees for their EINN framework and provided clear derivations and proofs. 
3. The empirical comparison of EINN with state-of-the-art methods and the positive results strengthen the claim of the proposed method's effectiveness. The authors ensured a fair comparison by using NNs with the same complexity for all methods tested.

Weaknesses:
1. The lack of discussion on the neural operator framework for solving PDEs. This approach has been a considerable advancement in the field, and the authors' failure to address or refer to it is an oversight. Further exploration or comparison of the neural operator approach could enhance the comprehensiveness and relevance of the paper.
2. While the authors did a commendable job providing rigorous mathematical proofs, the paper could be difficult for those without a strong background in the relevant mathematical fields. Providing more intuitive explanations or graphical illustrations might have made the methodology more accessible. 
3. The empirical testing seems to be limited in scope. More extensive experimentation, including larger-scale problems and more varied examples, would have strengthened the empirical evidence. 

Limitations:
The authors have specifically designed this approach for solving MVEs, which might limit its general applicability for solving various types of PDEs. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposed an entropy-dissipation informed neural network for solving certain PDE systems. Using energy dissipation, the paper designed a special structure that propagates information from previous dynamics. They provided a theoretical guarantee for this framework, including the control of the KL divergence between the target and the estimator and a discussion of the approximation error of the NN. They also did some numerical experiments to verify their claims.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using energy dissipation to design a framework is very interesting. EINN also takes the previous dynamics precisely and provides a uniform-in-time solution. 

2. Mathematically speaking, the proofs are solid and well-written.

3. McKean-Vlasov type PDEs is an important class of PDEs for people from both math and physics communities. The presence of the singular kernel does bring challenges, which are handled by EINN.

Weaknesses:
1. The authors specifically take advantage of the special structure of this type of PDEs. On one hand, it performs well for those PDEs; on the other hand, this limits the application of this method to other PDEs. 
2. Numerical experiments are done with only comparison with PINN and DRVN.

Limitations:
yes

Rating:
7

Confidence:
4

";1
rfTFJvTkr2;"REVIEW 
Summary:
This paper introduces the parallel spiking neuron (PSN) and several variants. The primary benefit of PSN over the existing spiking neurons is the parallel implementation on digital hardware, which brings dozens of times of acceleration on GPU. The accuracy also demonstrates the effectiveness of this paper. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of parallelized neuron implementation is interesting and can be efficiently processed on GPU.

2. The experiments show good performances. 

Weaknesses:
The authors pinpoint a disadvantage of current SNN, that is, low running speed on GPUs. However, it is known that GPU is not the most ideal device for deploying SNN, rather, it should be the neuromorphic hardware. PSN focuses on the optimization of SNN on GPU, however, GPU cannot utilize the binary spikes to lower the energy. So, even if PSN can accelerate the inference on GPU, compared to ANN on GPU, there is no advantage of efficiency still compared to ANNs. I was wondering whether the optimization on GPU is really useful cause people can always use ANNs on GPU.



Limitations:
Listed above. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents an approach to improve the efficiency and accuracy of Spiking Neural Networks by using a dependency method to generate hidden states, resulting in parallelizable neuronal dynamics and a significant increase in simulation speed. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The authors analyze the impact of removing the reset function from standard charge-fire-reset neuronal dynamics, proving that the parallelization of the spiking neuron can be achieved without it. They also present a general formula by rewriting the neuronal dynamics without a reset and introduce the PSN, a spiking neuron with entirely parallelizable neuronal dynamics. The paper also assesses the PS family's performance on sequential, static, and neuromorphic data classification tasks, showing that they attain higher accuracy than traditional spiking neurons.

Weaknesses:
Please see Questions.

Limitations:
It would be great if the authors could include what are the key limitations of this work and what are the trade-offs with the current vanilla spiking neurons.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes the Parallel Spiking Neuron (PSN), which generates hidden states that are independent of their predecessors, resulting in parallelizable neuronal dynamics and extremely high simulation speed. The weights of inputs in the PSN are fully connected, which maximizes the utilization of temporal information. The authors evaluate  the PSN family on simulation speed and temporal/static data classification, and  the results show the overwhelming advantage of the PSN family in efficiency and  accuracy. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The motivation considering the parallel spiking neurons for high simulation speed is interesting and important for the futural application of spiking neural networks.
2. The experiments on the large datasets, such as ImageNet, improves the techinical soundness of the spiking neural networks. 

Weaknesses:
1. Since the main goal of the proposed parallel spiking neuron model is to improve the simulation speed, the current experiments do not reflect that advantage over other methods, in which only focusing on the accuracy and the firing rates analysis.
2. The network architecture of the spiking networks in Table 2 is not clear.

Limitations:
1. Since the authors claim that the main goal of the proposed parallel spiking neuron model is to improve the simulation speed, the current experiments do not reflect that advantage over other methods, in which only focusing on the accuracy and the firing rates analysis.
2. The network architecture of the spiking networks in Table 2 is not clear.

The above limitations have been explained clearly in the following response.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper removes the reset mechanism from the dynamics of conventional LIF/IF neurons, and proposes to reformulate the neuronal dynamics using matrix multiplication instead of the iterative updating for the membrane potential. This matrix multiplication then can be simulated in parallel to accelerate the training of deep SNNs.
This paper makes strong assumption that hidden states are independent of their predecessors, and proposes the Parallel Spiking Neuron (PSN) and its variants, masked PSN, sliding PSN. 
The whole framework is built on unfolding the computing graph over the latency $T$ and then uses a fully connected layer $H = W X$ to replace the neuron dynamics.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Parallelization: Parallelizing in spiking neurons is an interesting topic in SNNs. By removing the reset mechanism, the neuronal dynamics can be reformulated in a non-iterative form. The proposed Parallel Spiking Neuron (PSN) framework allows for parallelized neuronal dynamics, enabling efficient computations across multiple processing units or threads.

2. Utilization of Temporal Information: The PSN utilizes fully connected weights for inputs, maximizing the utilization of temporal information and potentially enhancing the model's ability to capture temporal patterns.

3. High Simulation Speed: The PSN framework, with its independent hidden states and parallelizable dynamics, achieves extremely high simulation speed, which can be advantageous for real-time applications and large-scale simulations.


Weaknesses:
1. Lack of Reset Mechanism: The removal of the reset mechanism in the PSN may limit its ability to handle certain types of dynamics or tasks that rely on precise timing and reset behavior. The authors did not provide reasonable explanation of why neuronal resetting can be ignored, what should be done to compensate for reset removal. 

2. Large number of trainable parameters introduced by the new Weights: The use of matrix multiplication in the PSN introduces additional trainable parameters in the new weights, the performance improvement could benefit from using more trainable parameters, but not the new PSN model. With the same number of parameters for both PSN and LIF, will the performance still perform good using the new PSN compared to LIF?

3. Using future information: when calculating $H[t] = \sum_{i=1}^T W_{t,i} X[i]$, in this PSN the future information is used. For the sliding PSN and masked PSN, the authors tried to avoid using future information. So that in order avoiding using the future information, it's better to use masked PSN and sliding PSN, but in the experiments, it's not clear which PSN version is used. 

4. This paper makes strong assumption that hidden states are independent of their predecessors, the Parallel Spiking Neuron (PSN) is proposed based on this.  How to describe the neuron dynamics along the time-dimension if we remove this hidden state dependency?



Limitations:

1. The whole PSN framework is built on the condition $u(t) < V_{th}$, not clear how to get spikes if this is the precondition for PSN.

2. Using future information: when calculating $H[t] = \sum_{i=1}^T W_{t,i} X[i]$, in this PSN the future information is used. For the sliding PSN and masked PSN, the authors tried to avoid using future information. So that in order avoiding using the future information, it's better to use masked PSN and sliding PSN, but in the experiments, it's not clear which PSN version is used. 

3. This paper makes strong assumption that hidden states are independent of their predecessors, the Parallel Spiking Neuron (PSN) is proposed based on this.  How to describe the neuron dynamics along the time-dimension if we remove this hidden state dependency?


Rating:
7

Confidence:
4

";1
JIKM2vS8XU;"REVIEW 
Summary:
The paper proposes a new framework called DatasetDM that leverages the rich visual knowledge contained in diffusion models to generate synthetic data and corresponding annotations for various perception tasks, The core component of DatasetDM is the P-Decoder for generating perception annotations. The primary process is as follows: 1) During the training stage, a pre-defined text template is employed to extract feature maps and cross-attention maps from the pre-trained Stable Diffusion UNet, which are then concatenated to create a feature representation. The feature representation is then utilized to train P-Decoder, which employs a unified architecture that can adapt to various perception tasks; 2)During the generation stage, random sampling is conducted from a vast quantity of prompts provided by GPT-4 to enhance diversity. Subsequently, new images are generated by the VAE decoder of vanilla Stable Diffusion, while corresponding annotations are generated by the P-Decoder. The synthetic data and annotations will be utilized for various downstream tasks. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper proposes a novel approach to leveraging the visual representations contained in the pre-trained Stable Diffusion for solving multiple perception tasks. The methodology is well-organized and clearly illustrated. Notably, there is limited research focusing on the utilization of the rich latent space in diffusion models for downstream vision tasks. Thus, the methodology presented in the paper is informative and enlightening.

Weaknesses:
1) The methodology lacks some novelty, as it bears a resemblance to the paper VPD[a]. Specifically, the method employed in DatasetDM for extracting visual representations from Stable Diffusion is nearly identical to that of VPD and only with minor differences in implementation details, including using a similar technique to extract multi-scale feature maps and cross-attention maps and employing the same manipulation of concatenation. Additionally, DatasetDM adopts a resembling design to VPD, as VPD also employs task-specific decoders for perception tasks and also uses a unified architecture with minor task-specific differences for the architecture design of the decoder.
2) The experiments are insufficient. Firstly, to further illustrate the effectiveness of synthetic data, an ablation on the number of synthetic images can be conducted, especially investigating the performance of using a small number of synthetic images(e.g., 400 real images and 400 synthetic images versus 400 real images only). Secondly, more SOTA methods can be included as baseline to further evaluate the effectiveness of DatasetDM. Furthermore, evaluation on some less common datasets is necessary, which are more likely to face issues of data and annotation scarcity
[a] ""Unleashing Text-to-Image Diffusion Models for Visual Perception""(VPD)


Limitations:
The limitations of the paper are not explicitly addressed, and it would be beneficial to add a section about limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel dataset generation method producing diverse synthetic images along with perception annotations including semantic/instance segmentation, and depth/pose estimation. The technique leverages a pretrained text-to-image generation model, i.e., Stable Diffusion, to extract cross-attention maps and multi-scale feature maps as features input to multiple trainable decoders for different tasks. By training these add-on decoders on only a small number of labeled images (typically 100-800 images), the model can generate annotations along with the synthetic images. This method is evaluated on many tasks and achieves promising results. 

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
+ This paper addresses an interesting and impactful problem.
+ The proposed method can generate pseudo-labels for several tasks segmentation, depth estimation, etc. with a small number of labeled images.


Weaknesses:
+ Limited novelty: the presented method is fairly similar to [a], published more than 2 months prior to this submission. The most significant difference is that in [a], the authors work directly on real images while this method synthesizes new images to train a corresponding network to produce prediction. No mentions or discussions are provided.
+ The current setting is equivalent to the semi-supervised segmentation, the authors should compare with several semi-supervised methods in the corresponding tested perception tasks. 
+ The authors should include experiments to examine the contribution of cross–attention maps or multi-scale feature maps alone and in combination with ablation studies to analyze the contribution of each module to the overall performance. 

Minor: 
+ In L194, the authors use the notation Q for the learnable queues, however, in L207, the authors notated Q as query embedding. I would suggest using different Q_{x} for separate notations. 
+ The mask generation process should be (briefly) discussed in the main paper instead of the supplementary documents.

[a] Zhao, Wenliang, et al. ""Unleashing text-to-image diffusion models for visual perception."" arXiv preprint arXiv:2303.02153 (2023).

---
After reading the rebuttal, the highest score that I can give for the paper is 4.

Limitations:
No discussion on the method's limitations was provided. 

Rating:
4

Confidence:
5

REVIEW 
Summary:
With the success of text-to-image generation models, the authors propose a novel approach to text-to-data that can produce synthetic data by adding a perception decoder (P-Decoder) on top of Stable Diffusion, which only requires a few labeled images for training. The authors show various experimental results to demonstrate the effectiveness of the proposed approach.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow. 

2. The idea of text-to-data is interesting, and the proposed approach is easy to train with 1% of labeled data.

Weaknesses:
1. In Section 4, the DatasetDM results are compared to the baselines trained with less than 1000 real images, which is unfair regarding the number of training samples (e.g. 400 vs. 80k). To claim the effectiveness of the proposed model, more realistic baselines with the same number of training samples must be considered.

2. One of the contributions of the work is that the DatasetDM model can generate an unlimited amount of synthetic data. However, the authors do not conduct an analysis of how the number of generated samples affects the performance.

Limitations:
The authors haven't discussed the limitations of the work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a framework based on pre-trained Diffusion Models to synthesize images along with their perception annotation for various downstream perception tasks. With limited real labeled images as a baseline, the extra synthetic training pairs can boost the performance by a large margin.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The motivation to synthesize perception datasets is promising.
- The designed P-decoder is generic by taking advantage of the well-trained features from Diffusion Models.
- Various downstream tasks are validated.

Weaknesses:
My major concern comes from the experimental settings, especially the number of used real labeled images. I understand that, with limited real labeled images (e.g., 50 or 100 images), the performance boosted by extra synthetic pairs can look more appealing, because the baseline achieved by 50 labeled images is extremely weak.

However, considering a real-world scenario, we really hope to use the synthetic training pairs to boot **an already well-established baseline with all available real training pairs**, for example, using all the 10,000+ training images on Pascal VOC. Will the proposed DatasetDM further boost this strong and realistic baseline?

Another concern is that, what if the generated synthetic images do not belong to the same domain as original images? I am worrying the performance may be even degraded in some domains that are not so common as COCO and Pascal VOC when integrating the synthetic images, such as medical and remote sensing applications. Are there any mechanisms to avoid this?

---

**After discussion:**

I agree with most of the authors' claims about the differences between real and synthetic images. However, as for the second difference ""low-quality synthetic images result in high-quality pseudo labels"", I politely disagree with it. Even though real unlabeled images are more challenging to assign pseudo labels, they are much more informative to learn than synthetic images (also supported by results in Response 3 above, 100 real images: 65.3 vs. 100 synthetic images: 40.1). For the hard-to-predict regions, we can easily skip them during learning by setting a pixel-wise threshold for them, which is a common practice in semi-supervised learning. I believe the reported COCO results above can be much higher when applying a stronger semi-supervised framework and set an ideal threshold.

The contribution of this work seems to only lie in replacing the real unlabeled images with synthetic unlabeled images, which can be deemed as a side work of semi-supervised learning paradigms. Moreover, there is no strong evidence to prove that synthetic images can play a superior role to real unlabeled images. There is also a severe domain gap issue when using these generative models for some rare domains, such as medical images and remote sensing images.

Based on the above considerations, I would change my rating to 4: Borderline Reject.

Limitations:
Yes, they have been discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Large-scale datasets with quality annotations are often costly and time-consuming to collect and annotate. Since for most applications, large amounts of annotated data are required by deep neural networks, the authors propose a framework for text-guided synthetic data generation - called DatasetDM - which can produce an infinite number of images and their corresponding annotations, such as semantic segmentation masks or depth maps. Their method builds upon a pretrained diffusion model (specifically, Stable Diffusion) which is extended to include a simple decoder tasked with translating the latent space of the diffusion model’s time-conditional UNet to what the authors call “perception annotations”. The method is evaluated on several downstream tasks: semantic segmentation, instance segmentation, depth map prediction, human pose estimation, and zero-shot semantic segmentation. For each proposed downstream task, the authors use less than 1% of the training dataset to train the perception decoder on multi-scale text-to-image representations, and demonstrate competitive or improved performance over baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper attacks a problem relevant to the field: how to efficiently generate pairs of synthetic images and corresponding labels for a variety of core tasks within computer vision, with a framework that requires only minor modifications in architecture or representation depending on the task.

- The framework seems adaptable to several tasks of interest in computer vision: depth map prediction, semantic segmentation, instance segmentation, open-vocabulary version of the latter, and pose estimation.

- The paper is clearly written and easy to follow.

- Results across several downstream tasks (specifically semantic segmentation and instance segmentation) show impressive improvement over baselines.

- Interesting observation that training on synthetic and real data in the ablation studies section helps raise mIOU (Table 4) by a nontrivial amount.


Weaknesses:
The evaluation methodology and experimentation proposed in the paper is generally sound, but there are a few notes I'd like to make on the choice of baselines and dataset selection, which would strengthen the contribution claims of the paper.

Choice of baselines / experiments 

- The authors cite the recently published work HandsOff [1], which proposes a very similar dataset generation paradigm – although utilizing the latent space of a pretrained StyleGAN model instead of Stable Diffusion, and using GAN inversion instead of diffusion inversion to create a hypercolumn representation per pixel (that is then fed through a simple label generator to produce similar “perception labels”: semantic segmentation masks, depth maps, human pose keypoint prediction, etc.). 

- HandsOff uses fewer images for all downstream tasks (16 or 50 depending on the dataset and task), and also uses the same datasets for evaluation on downstream task performance as other cited works like DatasetGAN [2], BigDatasetGAN [3], and EditGAN [4]. In addition, [1], [2] and [4]  also share the same class consolidation in each dataset, making comparison easier. A comparison on similar datasets and similar class structure to these papers (eg. CelebA-Mask-HQ [6] or DeepFashion-MM [9]) with a similar number of training images (instead of the variable “1% of the training dataset” the authors use, which they say averages to around 100 training images) could be added in the performance comparison to establish more firmly a SOTA claim across the proposed tasks.

- I would like to see a comparison against DatasetDDPM [7] – a diffusion-model-based synthetic dataset generation method akin to the proposed approach and concurrent work – to give the reader a more comprehensive understanding of model performance. However, as per the supplementary material of [1], HandsOff actually establishes SOTA performance over all of [2], [4], and [7] with fewer labelled images, thus making a comparison against DatasetDDPM redundant. This could challenge the claim the authors make on L42-44: “Due to the limitations of the representation ability of previous GAN models, the quality of the synthesized data is often dissatisfactory, leading to an inferior performance on downstream tasks”. I see a comparison against HandsOff in the supplementary on Cityscapes semantic segmentation, where the proposed model does indeed outperform, but the same is missing for the pose estimation evaluation on DeepFashion-MM [9]. The best result presented is mIOU of 45.1 with 100 labelled images in 24 classes and with the Swin-B backbone, while HandsOff presents best mIOU of 68.4 with 50 labelled images in 8 classes and with the ResNet151 backone. 

- In summary, what would be beneficial to add to the baselines section is a fair comparison on pose estimation on DeepFashion-MM [9] against HandsOff, and a comparison on semantic segmentation on CelebA-Mask-HQ [6] (where HandsOff outperforms DatasetDDPM, another diffusion-based data generation framework).

Missing references 

- EditGAN [4] and DatasetDDPM [7] should be added to the list of citations.	

Minor 

- The authors refer to results on the Cityscapes dataset typically used for semantic segmentation evaluation (L242), and outline an example of a generated prompt inputted to Stable Diffusion to generate images akin to the urban driving scenes (L226+). I did not see a reference to the supplementary for a table of the results; I also think mIOU improvement is significant and could be added to Table 3 in the main paper.

- L60: possible typo: “... with minimal human-labeled…”

- L241 states: “To comprehensively evaluate the generative image of DatasetDM, we conduct seven groups of experiments for the supported six downstream tasks”, but L261 says “Tab. 1 provides a basic comparison of the selected four downstream tasks.” I also found it confusing from the introduction and abstract to form an expectation as to which downstream tasks the authors evaluated on, and whether they achieve SOTA performance on all or some. It would be helpful to enumerate specific tasks with SOTA results in the abstract and in the claims section of the introduction.

References

[1] A. Xu, M. I. Vasileva, A. Dave, and A. Seshadri. Handsoff: Labeled dataset generation with no additional human annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.

[2] Y. Zhang, H. Ling, J. Gao, K. Yin, J.-F. Lafleche, A. Barriuso, A. Torralba, and S. Fidler. Datasetgan: Efficient labeled data factory with minimal human effort. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

[3] ] D. Li, H. Ling, S. W. Kim, K. Kreis, S. Fidler, and A. Torralba. Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[4] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. EditGAN: HighPrecision Semantic Image Editing. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[6] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGAN: Towards Diverse and Interactive Facial Image Manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

[7] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-Efficient Semantic Segmentation with Diffusion Models. In International Conference on Learning Representations (ICLR), 2022. 

[8] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[9] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

Limitations:
Nothing of note with regard to potential negative societal impact of the proposed work.

Rating:
5

Confidence:
5

";1
oML3v2cFg2;"REVIEW 
Summary:
This paper focuses on learning from demonstration via offline inverse reinforcement learning (IRL). Offline IRL suffers a similar problem as offline reinforcement learning (RL) and imitation learning (IL), where the policy cannot generalize well on unseen states and actions---this problem is known as distribution shift. To address this problem, this paper proposes to first learn a dynamic model, and formulates a maximum likelihood (ML) objective to simultaneously recovers both the reward function and the policy. Notably, the policy is optimized using a maximum entropy objective along with pessimism based on the uncertainty of the learned dynamic model. This paper provides PAC-style bounds to quantify the amount of samples required to achieve $\varepsilon$-optimal solution to the MLE objective. The paper further provides an algorithm that obtains such a $\varepsilon$-optimal solution under specific assumptions. Finally, this paper provides empirical evaluation on the D4RL benchmarks.

## Contributions
- A new MLE objective for recovering a policy that is close to the expert policy.
- A theoretical analysis that describes the statistical guarantees of the objective
- An algorithm that obtains to near-optimal solution under linear parameterization of the reward function
- An empirical evaluation on standard benchmark, D4RL, that indicates statistically significant improvement over existing baselines in majority of the tasks.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- The paper is well written and easy to follow in general---I particularly appreciate the presentation on providing formal statements followed by the high-level intuitions.
- The paper proposes a novel formulation for offline inverse reinforcement learning.
- The paper provides numerous theoretical justifications and an algorithm that is inspired by said analyses.

Weaknesses:
- In practice, how do we ensure assumption 2 (ergodicity)? It seems like this assumption actually ""hides"" some part of the coverage requirement?
- I am curious as to how the MLE objective connects to the policy error---I completely understand that if $\pi_\theta = \pi^E$ then the policy error is zero. However, it does not seem to me that achieving $\varepsilon$-error on the MLE (i.e. $L(\theta^*) - L(\hat \theta) \leq \varepsilon$) does not directly tell us the policy error.
- I think the training description for BC is somewhat vague---on page 8, line 314: what exactly does it mean by ""train the algorithm until convergence""? Do we have some form of validation checking for BC? [1, 2, 3] have results regarding how BC would perform based on specific validation. Secondly, was there any hyperparameter search on BC, ValueDICE, and CLARE?
- Regarding the experiment on recovered rewards, what is the performance if we were to fix the reward to 0? Isn't it better if we were to consider the correlation between the true reward function and the obtained reward function?

## References
[1]: Hussenot, L., Andrychowicz, M., Vincent, D., Dadashi, R., Raichuk, A., Ramos, S., ... & Pietquin, O. (2021, July). Hyperparameter selection for imitation learning. In International Conference on Machine Learning (pp. 4511-4522). PMLR.  
[2]: Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., ... & Martín-Martín, R. (2021). What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298.  
[3]: Ablett, T., Chan, B., & Kelly, J. (2023). Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. IEEE Robotics and Automation Letters.

Limitations:
- The paper's proposed method requires uncertainty estimation which is still an active research problem, especially in the context of neural networks---as far as I understand the paper leverages existing work that lacks theoretical guarantee.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper addressed the issue of covariate shift in offline imitation learning. The authors extended the uncertainty-regularized model-based offline RL to the imitation learning setting. The key idea is to first learn transition dynamics from samples, and then solve an optimization problem that jointly seeks a policy such that it optimizes the learned transition dynamics accompanied by a reward model and maximizes the log-likelihood of actions in data. The authors provide theoretical guarantees for the maximization of action log-likelihood and empirical results for the learned policy.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The issue of covariate shift is indeed important for offline IRL.
2. The efficacy of the algorithm is partially supported by empirical results.
3. Analysis is provided for the model-based part of this algorithm.

Weaknesses:
1. The paper is somewhat hard to follow. See questions below.
2. The effect of overcoming the distributional shift is not emphasized in the experiment section. None of the experiments was carried out on small datasets where the coverage of state–action space is limited. In fact, the medium datasets in D4RL contain 1M transitions, and the medium-expert versions contain 2M transitions. The datasets for results in Figure 2 contain 5000 expert demonstrations, which might correspond to 5M transitions if each expert trajectory contains 1000 transitions.
3. The paper does not have an informative conclusion part.

Limitations:
No. there is no discussion on limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
Offline inverse reinforcement learning (IRL) is a method for finding an unknown reward function optimized by an agent from demonstrations using only a finite dataset. The most common framework is maximum entropy (MaxEnt) IRL, which attempts to find a reward function that induces a policy which achieves the same expected reward as the trajectories from the expert demonstrations while maximizing its entropy. Prior work has shown that this is equivalent to finding a policy which maximizes the likelihood of the demonstrations under the constraint that this policy comes from solving a MaxEnt RL problem. This formulation as a bi-level optimization problem reduces the computational burden that results from alternating between finding the policy and updating the reward. However, it requires access to the true dynamics of the environment, which is incompatible with the offline IRL setup. Instead, this work proposes to learn the dynamics model in an uncertainty-aware fashion and incorporate a measure of this uncertainty in the learned reward function. This results in a two-stage procedure: 1) fitting a dynamics model from transition samples in the dataset and 2) recovering the reward function using the maximum likelihood (ML) formulation of the IRL problem. To perform the second step, the authors propose a novel decomposition of the upper-level objective, which consists of a surrogate objective that is more computationally tractable to optimize. The authors provide statistical guarantees about the optimality of the recovered reward function in terms of dataset coverage, a concept common in offline RL. Importantly, their bounds depend on dataset coverage on expert-visited state-action pairs, not the full joint space.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Offline inverse RL is an important area for tackling challenging sequential decision making problems in potentially safety-critical applications. 
- The paper is well organized and clearly written. It does a good job explaining the novelty and results and provides enough information to support its claims.
- The paper presents an extensive experimental evaluation on several benchmarks, comparing to both model-based and model-free offline IRL algorithms and existing imitation learning approaches. Their algorithm outperforms these baselines in most cases.
- The authors provide a nice analysis of surrogate objective and its relation to the true upper-level objective. This provides a nice motivation for optimizing the surrogate instead, which is more computationally tractable.
- The authors present a nice optimality guarantee of the stationary point of their algorithm in terms of the surrogate objective in the case where the reward function is linear in a feature vector of states and actions. They also relate this stationary point to the true optimal solution of the original problem.
- The additional reward transfer experiments indicate that the learned reward function may transfer to dynamics models trained on different state-action distributions. This appears to hold even if the state-action coverage used to train the reward function is close to expert-visited states.

Weaknesses:
- The alternating optimization scheme discussed in this work appears to be identical to that presented in [21]. If true, that is fine, as the main contribution of the work lies in modeling the conservative MDP and providing novel bounds in the offline setting. However, it should be made explicit in the paper and mentioned in the contributions.
- Theorem 2 seems very similar to Theorem 5.4 in [21], except that the Q function approximation error is considered explicitly. If this is true, it should be discussed that this is the novelty in the text.
- Section 6 should discuss the differences in the three dataset types used, as the current text does not explain what they entail. This makes it difficult to understand the performance of the proposed algorithm in each setting without carefully looking at the Appendix. It should also talk about the purpose of using these different datasets. From the Appendix, it appears that they are only used to train the dynamics model. Thus, they are evaluating the effect of dataset coverage around the expert on performance. This should be explicitly discussed in the paper. I know space is limited, but these are important details that should be in the main text.
- A minor comment is that it would be nice for the main paper to end with a conclusion section rather than ending abruptly. And this conclusion should mention limitations of the current method.

Limitations:
There is no discussion of limitations in the paper. The paper would be made a lot stronger if this was discussed in a conclusion section.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper the authors present a two level maximum likelihood based framework for offline inverse reinforcement learning, where both a world model and a reward model are learnt from expert demonstrations. In this two level algorithm the outer level or loop involves estimating the reward function, while the inner loop estimates the optimal policy for the chosen reward function in a conservative MDP setting, where a penalty is added which is loosely proportional to the uncertainty in the learnt model. The authors provide theoretical guarantees for the performance of their algorithm under fairly standard technical assumptions. They also show the numerical performance of their algorithm on 3 MuJoCo environments, comparing them with other state of the art offline RL algorithms.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The paper is novel, clearly written and is easy to comprehend.
2. The authors have stated their results formally in the form of Lemmas and Theorems and have proved them in the supplementary material. This analysis proves the validity and utility of their proposed approach.
3. While model based offline inverse RL has been studied, I think the theoretical guarantees from this paper are novel and important.

Weaknesses:
1. The authors have demonstrated performance on only 3 environments, in which in one of the cases, their proposed algorithm is not the best.

Limitations:
The authors address the limitations of this work and also some suggestions to overcome some of these limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper ""Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning"" proposes an innovative approach of offline inverse reinforcement learning. After a deep theoretical analysis of the inter-dependence between errors arising from dynamics modeling from limited offline data and performance gaps of resulting policies, authors propose an efficient algorithm to practically exploit conclusions for reward/policy learning. A small experimental section validates the approach.    

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Very well written with every notation well defined and every choice well justified
- Very interesting problem and strong theoretical analysis
- A practical algorithm that looks easy to reproduce
- Good results 

Weaknesses:
- My main concern is that there is very few discussion about model uncertainty U in the paper, and particularly in section 3. I am surprised to not see it involved in the derivations and bounds, with no assumptions about it (except that it is bounded). No real meaning is given to it and it seems that it could be removed without changing anything in the theoretical conclusions. Is it true ? If yes, why introducing it in that part ? Also its impact is therefore not well understood from the theoretical analysis, which is a little be limitative to me (as it looks to have importance). 
- Still on U, I feel that experimental results on the choice of U would have been very useful.      

Limitations:
. 

Rating:
8

Confidence:
3

";1
1recIOnzOF;"REVIEW 
Summary:
The authors present Decorate3D, a technique for text-driven texturing of a 3D mesh given a NeRF representation of a given scene. To this end, the authors introduce a two-stage texturing scheme. First, the NeRF is decomposed into a 3D mesh and view-dependent texture map. Second, given the reconstructed diffuse UV texture map, the authors edit the mesh using a modified score-distillation objective that considers the structure, or depth, of the input. Finally, to mitigate some jittering artifacts of the resulting editing texture map, the authors propose a few-view resampling technique. The authors compare their texturing scheme to existing techniques and provide many visual examples to show the 3D consistency of the textured meshes. Additional ablation studies are provided to validate the core design choices of Decorate3D.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The visual results achieved by Decorate3D are impressive and appear to surpass existing texturing methods. Additional quantitative evaluations across numerous objects are used to further validate the effectiveness of the technique.
- I am not familiar with existing works that operate over a real 3D scene and NeRF model. For example, to the best of my understanding, most works assume that a 3D mesh is provided. Here, the authors operate in a real-world setting, which adds an additional challenge that is overcome quite nicely.
- Although the overall system is quite complex, the different components are presented quite nicely and can be understood after careful reading. The intuitions provided by the authors to motivate the different component help in understanding the design of Decorate3D.
- Finally, many ablation studies are provided to validate the different components of Decorate3D. Although I would have liked to see more visual results, I believe this can easily be added to the revision.

Weaknesses:
**General Points:**   
- The visual results of TEXTure raise some concern on whether the method was run correctly by the authors. From my experience with the official code base, the results should be of much higher quality. In the TEXTure paper itself, the authors show an Ironman texture of a mesh and the results look far better than those presented in the paper. Moreover, in DreamAvatar [Cao et al. 2023] the authors also compare to TEXTure and achieve much better results for TEXTure. These visual results also seem to contradict the quantitative results, which placed TEXTure quite closely to Decorate3D. I want to give the authors the benefit of the doubt here, but could the authors please clarify and verify how the results for TEXTure were obtained? 
- The method cannot edit the geometry, which is needed in real-world applications. Specifically, the authors do not explore the robustness of the quality of the resulting mesh. For example, does the method still work nicely if the mesh contains defects such as holes or a few faces? Moreover, the authors assume that there is some semantic relation between the prompts and the geometry in order to get reasonable results. 
    - This is discussed by the authors as a limitation. 

**Ablation Studies:** 
- After the decomposition stage, is the resulting diffuse texture map consistent across all views? From my understanding, already at this point, the texturing should be 3D-consistent. However, I could not find results obtained after the decomposition stage (e.g., reconstruction) to verify this. 
- It is difficult to assess the contribution of the structure-aware SDS from a single visual example. This also holds for the other ablations performed by the authors (e.g., the FVR training). Additional visual results, and ideally, more quantitative evaluations (e.g., as done in Table 1) would greatly assist in truly evaluating the contribution of each component. 
- I had a difficult time understanding the contribution of the few-view resampling training. If I understood correctly, Figure 8 is designed to show the improvements obtained using the FVR training. Could the authors provide some additional examples that illustrate this improvement? Could the super-resolution model be applied directly to the previous result? And if we do so, would this also help with the jittering effect? That is, I am wondering whether the improved results are from the FVR or from the super-resolution model. Based on the results provided, it appears that the FVR does provide some minor improvements, but additional examples and an ablation study on applying the super-resolution model directly to the previous step would be helpful to highlight this.

Limitations:
The authors discuss the limitations of their method.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes decorate3D, a method for re-texturing real-world 3D objects using text-conditioned image diffusion models. The proposed method can be split into a 3D reconstruction phase and a re-texturing phase. In the 3D reconstruction phase, a 3D mesh is reconstructed from set of multiview images via NeuS, and a view independent texture map is distilled via differentiable rendering. In the re-texturing phase, a depth-conditioned latent diffusion model is combined with SDS to optimize the texture map. The texture map is re-rendered by passing it through the encoder-decoder of stable diffusion to remove neural artifacts, but this step introduces jittering artifacts. Jittering artifacts are then removed via optimizing a MLP through few-view resample training to reconcile view-inconsistencies. Lastly, a super-resolution diffusion model is used to up-res the produced texture map.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
1. The method is technically impressive, utilizing many interesting tricks to overcome problems associated with latent diffusion models, and thereby obtaining visually impressive experimental results.
2. The presentation of the method is precise and easy to follow. Despite the many moving parts, never once did I feel a need to backtrack due to inconsistent notations or frivolous math equations.
3. The experimental procedure is detailed and well documented. One can be confident of the reproducibility of the results (as long as the authors release the real world data they've collected). The ablations are also fairely thorough, giving clear intuitions as to the effect of each component.

Weaknesses:
1. Novelty:
Most components utilized in this method are either well known to the literature, or straight-forward extensions of existing workflows, such as NeuS for mesh reconstruction, disentangling view-dependency via differentiable rendering of two MLPs, using depth condition for text-to-3D, and appling super-resolution diffusion models on UV textures. Though the problem of SDS with LDMs as observed in figure 3 has not been formally studied in a research paper, knowledge of this problem is folklore within the community and the proposed neural renderer solution is rather simplistic. As such, it is not clear to me whether this paper contains enough technical novelty to be impactful in the text-to-3D field.
2. Fairness of comparisons:
The experiments can be more convincing if other SDS based approaches (namely dreamfusion and latent paint) are also equipped with depth conditioned diffusion backbones instead of the vanilla backbone. I think these are sufficiently simple modification such that they can still be considered the same method, but adapted for the re-texturing task. By the same token, none of the included baselines was designed for the task of re-texturing, and the use of an initial texture provides a significant performance boost as illustrated in one of your ablations. It would be more fair if the view-independent MLP is provided as initialization for the baselines as was done for Decorate3D.

Limitations:
I think the computational cost of this method is a limitation worth mentioning - it is by far the most expensive method to run versus its baselines, whose runtime ranges from seconds to tens of minutes on a single GPU, whereas Decorate3D requires hours on full 8 GPUs.
Potential negative societal impacts such as identity theft, deep fakes, and manufacturing of disinformation should be mentioned.

---------------------------------------------Post rebuttal:
I think the changes to the manuscript promised by the authors will significantly improve the delivery and message of the paper by firmly substantiating their claims regarding the effectiveness of proposed techniques with more ablations. Therefore I'm changing my suggestion to acceptance.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a method to edit the textures for neural fields (NeRFs) using score distillation sampling and also export a mesh model with texture that can be used in traditional graphics pipelines (i.e. game engines, VFX). More specifically, the main contributions that I see from this work is the ""Few-view Resampling Training"" which can take an SDS-optimized RGB diffuse texture map (which is noisy due to the nature of SDS with LDMs), and refine it through LDM-driven re-rendering, which takes advantage of both ""LDM as a renderer"" and having a real 3D consistent 3D model that can be used. This is specifically a general technique that could be widely applicable in a variety of different tasks.  

In addition to this, they also create an entire pipeline to extract editable & high quality mesh representations from multi-view images (i.e. ones with good geometry, good UV parameterization, diffuse + specular separation, mostly based on existing tools) as well as another case study on SDS-driven texture generation.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
The main strength of this paper is in the ""few view resample training"", which takes as input a noisy 3D model, renders the 3D model, refines the rendered image using a ""neural renderer"" (which in this case is the VAE of an LDM), and back propagates the refinement back to the 3D model. This as far as I'm aware is an original idea that I have not seen at least in this specific context. The method also seems to be effective from the limited results I am able to see, and is something that can likely be incorporated into many different contexts. 

The paper also proposes an end-to-end pipeline for doing NeRF -> mesh -> editing, and evaluates several different tricks to make this pipeline effective which they also evaluate in some limited ablation studies. This is significant as it provides a case study for implementation tricks in making this pipeline work (which in my experience tends to be a big part of SDS based pipelines).

The clarity of the paper could be improved, but is not something that significantly detriments the paper. This will be discussed further in the weaknesses.

Weaknesses:
The biggest weakness of this paper is in its clarity. With some restructuring and refinement, however, I think that this paper could be very convincing.

First, the paper introduces the problem of 'mesh decoration'. The task really at hand is 'retexturing' or 'texture editing'. I'm not sure what the motivation for using the word 'decoration' is, but this is something that makes the paper unnecessarily confusing to grasp. 

Second, the paper puts a lot of weight on discussing the end-to-end pipeline from reconstruction to retexturing. In reality, the meat of the contributions for this paper lies in the retexturing method (and specifically the few view refinement), and the rest feels like a distraction that is not core to the contribution. Making the writing and contribution statements more specific to the retexturing part of the pipeline, and treating the end-to-end pipeline as almost an 'implementation detail' would make the paper much more convincing. (i.e. the decomposition stage isn't really core to the method, since the same pipeline could be applicable for an existing 3D mesh). 

Third, the paper does not sufficiently compare and contrast their method with concurrent works like TEXTure which can be considered prior art given its more than 2 months before the deadline. The paper does compare against them in the evaluations, which is great, but it could use more discussion on _why_ these prior arts produce bad artifacts in their generation, and what fundamental differences makes this paper more advantageous. 

Fourth, the results shown on the core contributions are rather light. It would be very illustrative to show (on multiple models) the rendered 3D models after the SDS optimization (with their artifacts & UV textures), after ""neural rendering"", and after refinement to really showcase the efficacy of the refinement method. 

Fifth, the ablations are good but they could be on different figures with more examples. Some of the text space that is currently used for the description of 'prior things' like SDS and the decomposition stage could probably be placed in the supplemental or taken out to make more space for results. More results on especially the effects of higher viewpoints for the refinement step could be very useful.

Lastly, it would be useful potentially to write the approximate time for completion for each stage in Figure 2 to make the costs more clear.

These are not things that affect my rating, but nitpicks:

22: ""Since the implicit representations of the NeRF model are tightly coupled"" this should be explained in more detail. I believe the authors are referring to the fact that it's difficult to disentangle geometry and texture from a typical NeRF model, but this phrase does not communicate this at all. 

41: ""The reason is that the optimized UV texture stands for neural features in effect, which produce rendered neural images that necessitate a neural interpreter"" I find the whole section starting with this sentence rather confusing and hard to interpret what it really means (without having read the rest of the paper at this point yet). Trying to describe this more precisely would help. For example, what does ""stands for ... in effect"" mean? What is a ""neural interpreter""? I can make inferences but those are then inferences. At this point I'm also confused as a reader why the UV texture needs to be 'neural' or 'latent' at all.

113: ""However, diversified 3D generation is often infeasible due to a lack of enough data pairs of text and 3D models"" I assume this sentence is in reference to an auto encoder sort of framework, but this is not explained anywhere. Also, I'm not really sure what ""diversified"" means. 

131: It's not made clear in this general section that the thing that is being passed to the encoder is a rendered image, not the texture map. Although this is clear from the figure, explicitly stating this here would be nice.

The writing could be improved stylistically in various places, like not starting sentences with ""And"".

Limitations:
The authors have adequately addressed the limitations. I think they could make a broader statement on the societal impacts, as content creation tools like this are something that could potentially impact labor markets (and displace artists) and is something that is based on diffusion models trained on large amounts of data (which often also means they are unattributed / has no provenance back to artists).

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces Decorate3D, which enables text-guided 3D model editing by extracting and editing a learned UV texture. Specifically, Given multi-view images, Decorate3D first generates 3D mesh and UV textures based on NeuS. Then, it optimizes neural textures by the guidance of 3D structure (depth) and stable diffusion model. Finally,  an RGB UV texture is optimized and upsampled to generate the final result. Experiments demonstrate the state-of-the-art performance of Decorate3D.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The idea of this paper is well-motivated and presented.

- A carefully designed pipeline (Nerf rendering, depth-aware texture optimization, few-shot texture re-optimization, and texture super-resolution) enables high-quality generation results. The 3D consistent decoration phase is novel and effective. 

- Best performance is achieved compared to SOTA. 

Weaknesses:
- The proposed method uses few-view resample training to obtain a UV texture that reduces the jittering effects. I am wondering if this step can be done in the decomposition phase or the text-driven neural texture optimization.

- The super-resolution is applied to the UV texture. However, the UV texture is not a natural image, if the model is trained with natural images, will there be some domain gap leading to inferior results? Moreover, as super-resolution is only a post-processing step that enhances the results and is not one of the main contributions, I recommend spending fewer texts on this point.

- I would like to see some quantitative ablation studies like Table 1.

Limitations:
Important limitations have been discussed in the paper.

Rating:
8

Confidence:
4

";1
8SUtvEZCF2;"REVIEW 
Summary:
The authors proposed a method for semantic segmentation applied to unmanned aerial vehicle (UAV) laser scanning (ULS), namely SOUL, to discriminate leaf from wood points. It is based on PointNet++ with an additional sampling scheme and an innovative training loss function to handle the high imbalance between the classes. The SOUL method relies on the coordinates of the points to increase its range of application to other forests and other sensors. It also includes 4 point-wise geometric features computed at 3 scales to characterize each point.
The geodesic voxelization decomposition (GVD) is also introduced as a preprocessing method to partition the ULS data while preserving the topology of the point cloud.
Experiments have been conducted on a dataset recorded in a French Guiana tropical forest. The proposed method has reached the best results on 3 over 6 evaluation metrics, including metrics adapted to unbalanced datasets.
The approach has also been qualitatively tested on open source datasets recorded in Australia and Germany with open source datasets showing a potential generalization on other forests and other LiDAR sensors.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1/ The application of semantic segmentation to LiDAR recordings of forests is a high priority for climate change and global warming understanding and mitigation. This original work could have a potentially high impact for reforestation/afforestation monitoring and carbon stock estimation.

2/ The proposed SOUL method is the first to be adapted to the density and unbalance of ULS forest recordings. The proposed GVD preprocessing method is also relevant for other LiDAR sensors and type of forests.

3/ The authors have successfully performed experiments on a datasets recorded in a tropical forest in French Guiana, showing best results compared to a few competing methods and according to metrics specifically adapted to unbalanced datasets. Additional qualitative experiments conducted on unannotated datasets have shown a potential generalization on other forests and other LiDAR sensors.

4/ The paper is well written and motivated with relevant arguments.

Weaknesses:
1/ There is a lack of related works and comparisons using other point cloud architectures which could have led to better performances [1, 2]. The selection of PointNet++ has been motivated by L127 ""the lower GPU requirements compared with transformer-based models developed in recent years"". The model should be selected as a trade off between the GPU consumption and the performances of different architecture. These experiments would have been appreciated to support the argument.

2/ An ablation study of the proposed geometric features would have been appreciated. What is the impact of each proposed feature?  What is the actual impact of these features against using the raw point cloud? Is a model such as PointNet++ capable of estimating these geometric characteristics internally?

3/ Even if the LiDAR point cloud is affected by atmospheric characteristics, it would have been interesting to see the performances of the proposed method using the reflectance as an additional feature per point.

4/ The lack of experiments, in particular with competing methods, ablation studies and standard deviations (see next section), makes the submission questionable as to the significance of the results.

[1] Y. Guo et al., Deep Learning for 3D Point Clouds: A Survey. In TPAMI 2020.

[2] B. Fei et al., Comprehensive Review of Deep Learning-Based 3D Point Cloud Completion Processing and Analysis. In TITS 2022.

Limitations:
The limits of the presented method has been addressed by the authors. The potential negative societal impacts have not been mentioned (e.g. military applications, UAV surveillance). The release of the dataset has not been mentioned which is a major limitation since the proposed method has been designed and validated on it and the only numerical results are based on its annotations.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a dataset and an algorithm for 3D semantic segmentation in forest scenes. From data collection to the algorithm design, this paper covers the whole pipeline that are oriented for forest segmentation. In terms of the algorithmic part, the solution itself is not fully satisfied with me, but I enjoyed the geometric feature computation (Sec.3.1). Moreover in data pre-partitioning part, I understand the intention of such design and it makes sense to me.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper covers the whole pipeline, from data collection to algorithm inference (semantic segmentation), for tree segmentation. While most of the 3D semantic segmentation methods, PointNet++, Point Transformer (ICCV 21), PointMixer (ECCV 22), PointNeXT (Neurips 2022) only deal with 3D semantic segmentation within the limited indoor scenes (S3DIS dataset or ScanNet dataset), this paper newly introduce the tree/forest segmentation using LiDAR point clouds.

Not just an algorithm part, this paper also covers data collection and data preprocessing such that this paper covers the whole pipeline for the forest segmentation task.

Weaknesses:
Honestly, I could not find the weakness of this paper. Nonetheless, I have a minor question. Can you conduct an ablation study for the rebalance loss? If possible, I want to see the quantitative/qualitative result based on this loss design.

Except that question, I am fully okay with this paper.



Limitations:
It's fine with me.

Rating:
7

Confidence:
3

REVIEW 
Summary:
They describe an approach for automatically segmenting a LIDAR scan of a forest into wood and leaf points.  They train a PointNet++ model and use resampling to address the extreme class imbalance in the data.  In comparison to previous methods they achieve a much higher balanced accuracy on their dataset.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The approach they propose is novel to the best of my knowledge.  The presentation is fairly detailed and clear.  

According to their experimental results (Table 1), they greatly outperform previous methods in terms of specificity and balanced accuracy.  

Weaknesses:
There is probably not enough of a contribution here in terms of machine learning methods for this paper to be appropriate for NeurIPS.  The paper is rather narrow in scope and specific to the application of wood-leaf segmentation.  Furthermore, they combine existing techniques such as extraction of PCA features, PointNet++, and resampling to handle imbalanced data.  As an application study, I would see this type of work as more appropriate for an applied machine learning conference / journal or a forestry / ecology journal.

Also, PointNet++ has been used before for wood-leaf segmentation -- this paper was not cited:

Xi, Zhouxin, et al. ""See the forest and the trees: Effective machine and deep learning algorithms for wood filtering and tree species classification from terrestrial laser scanning."" ISPRS Journal of Photogrammetry and Remote Sensing 168 (2020): 1-16.

Comments on presentation:
* L149 need spaces in the vector notation [0 0 1] (you can do $[0 ~ 0 ~ 1]$ for example)
* Eq. 7: if the voxels are adjacent, wouldn't the manhattan distance be 1 anyway?  How is ""adjacent"" defined?
* Figure 3: the numbers in the color bar are too small to read
* Eq. 9: the loss doesn't seem to be properly defined.  When y_k = 0, the loss term = 0?
* L254: ""big trunk"" -> ""large trunks""
* Figure 5, 6: what do the colors mean here?  The statement ""any other approaches developed for dense point cloud are ineffective"" is too broad.

Limitations:
Some limitations were discussed but there was not a section specifically labeled ""limitations.""  Ethical implications were not discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a neural network model based on the Pointnet ++ architecture which makes use of point geometry only (excluding any spectral information). To cope with local data sparsity, it proposes a sampling scheme that aims to preserve local important geometric information. It also proposes a loss function adapted to the severe class imbalance. Experiments show that the proposed model outperforms state-of-the-art methods on UAV point clouds.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper applies the mature pointnet++ on lidar tree classification with some variations of the methodology. The results seems good and working on the low resolution UAV LIDAR point clouds.

Weaknesses:
I am not quite sure of the novelty part as most of the techniques seem mature technique.

Limitations:
The innovation part seems lacking.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This submission proposes an end-to-end approach, Semantic segmentation On ULS (SOUL), for leaf-wood semantic segmentation that is based on PointNet++ [8]. By considering the imbalanced class label in the collected ULS dataset, a rebalanced loss is used. Moreover, a geodesic voxelization decomposition (GVD) method is introduced for data refinement through pre-partition. A ULS dataset with 282 tree-labels is collected for network training and testing. Experiments on the collected dataset demonstrate the effectiveness of the proposed method compared with the chosen baselines.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This submission proposes an end-to-end approach, Semantic segmentation On ULS (SOUL), for leaf-wood semantic segmentation that is based on PointNet++ [8]. By considering the imbalanced class label in the collected ULS dataset, a rebalanced loss is used. Moreover, a geodesic voxelization decomposition (GVD) method is introduced for data refinement through pre-partition. A ULS dataset with 282 tree-labels is collected for network training and testing. Experiments on the collected dataset demonstrate the effectiveness of the proposed method compared with the chosen baselines.

The strengths are:
1) A new dataset has been collected.
2) A new approach with comparable results.


Weaknesses:
The weaknesses of this paper are listed as follows:

1) The writing and the organization of the submission need to be improved. 

2) The benefits of using the data pre-partitioning (in Section 3.2) are not clear. It would be better to provide more details and an ablation study w/o the GVD method.

3) The details of the provided baselines are missing. It would be better to consider more baselines in Table 1, e.g., PointNet++ with the proposed sub-modules.

4) The novelty is not sufficient for NeurIPS standards.

5) There are lots of approaches for imbalanced data labels. It would be better to provide more experiments to demonstrate the effectiveness of the rebalanced loss used.


Limitations:
See Weaknesses.

Rating:
3

Confidence:
5

";1
8kyIChWsAG;"REVIEW 
Summary:
This paper elucidates the relationship between proper losses and calibration by providing the minimal necessary and sufficient condition for proper losses to induce calibrated models.
The condition, local optimality, delineates the concept that no post-processing functions can improve a proper loss anymore.
This is related to a specific measure of calibration called smooth calibration error, which is a kind of correlation between miscalibration and model predictions.
The smooth calibration error is used here because it does not suffer from a discontinuous nature, unlike the popular expected calibration error, and it naturally emerges from the Bregman divergence structure.
By leveraging the Bregman divergence structure of proper losses and convex analysis, a theoretical connection between the smooth calibration error and the post-processing gap (the quantitative version of the local optimality condition) is established, which indicates that minimizing the post-processing gap is necessary and sufficient to achieve sufficiently small calibration error.
As an implication of the theory, the authors point out a potential connection between the implicit bias regularization of DNNs and the post-processing gap so that sufficiently over-parametrized networks may achieve a small post-processing gap, leading to the current well-calibrated neural networks.
Overall, this paper pushes the understanding of proper losses and calibration toward the context of modern neural network regimes.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- A new connection between proper losses and calibration: Though the two concepts seek similar goals, they have been studied independently, and the relationship has not been understood well. The main theorem of this paper draws the connection by establishing the upper and lower bounds of the post-processing gap (related to proper losses) by the smooth calibration error (related to calibration). As far as I know, this is the first attempt to connect the two concepts. The concept of the post-processing gap is well motivated by deep neural networks (regarding the post-process as the layer addition).
- A transparent proof: The proof of the main result (Theorem E.8) gives us a nice picture of the relationship between the calibration error and proper losses. Specifically, the proof of the bounds mostly leverages the smoothness of a function $\\psi$. This proof is simple and gives us an insight that the structure of $\\psi$ essentially governs the connection.
- The clarity of the presentation: Though the concepts introduced in this paper are rather dense, the authors did a nice job of presenting them gradually from a conceptual level to a technical level, which helps readers who may not be familiar with those concepts understand them easily.

Weaknesses:
- Potential gaps between the theory and Guo et al. (2017): The authors argue that ""the previous generation of image classifiers were poorly calibrated [Guo et al., 2017]"" and ""state-of-the-art DNNs are often well-calibrated: because their test loss cannot be improved much by adding a few more layers."" However, I feel that the architectures used by Guo et al. (2017) are already very deep. For example, in their pilot study (Figure 1), they chose to use a 110-layer ResNet, which would be sufficiently over-parametrized. Since Guo et al. (2017) argued that DNNs are poorly calibrated even with that number of layers, I would like to see the authors' discussion on this line.
- Missing key references: Some results and claims in the paper are substantially related to previous works that are not mentioned in the paper. It would be great to give credit to them. For example, the dual mapping of the form $\\mathsf{dual}(v) := \\ell(0,v) - \\ell(1,v)$ (l. 299) is known in Eq. (47) of Buja et al. (2005); The composition of a proper loss $\\ell$ and the dual mapping $\\psi$ is known as composite losses, coined in Reid and Williamson (2010). The dual loss form in Eq. (6) and Definition 4.3 is known as the Fenchel-Young losses, and the expression was shown in Eq. (14) of Blondel et al. (2020) and Eq. (11) of Duchi et al. (2018); The convex conjugate structure $\\nabla\\psi(\\mathsf{dual}(v)) = v$ (l. 329) was pointed out in Figure 1 of Bao and Sugiyama (2021); Some parts of Lemma E.4 are closely related to Proposition 2 of Blondel et al. (2020).
- Restriction to binary classification: The attention of this whole paper is restricted to binary classification, as mentioned in the conclusion. This is far more restrictive in practical situations. But I don't think this limitation undermines the impact of this paper.


**References**

- Reid and Williamson (2010) ""Composite Binary Losses""
- Buja et al. (2005) ""Loss Functions for Binary Class Probability Estimation and Classification: Structure and Applications""
- Blondel et al. (2020) ""Learning with Fenchel-Young Losses""
- Duchi et al. (2018) ""Multiclass Classification, Information, Divergence, and Surrogate Risk""
- Bao and Sugiyama (2021) ""Fenchel-Young Losses with Skewed Entropies for Class-posterior Probability Estimation'

Limitations:
The authors discuss the limitations of this work at the end of the paper and point out that there is room to investigate the connection among calibration, DNN architectures, and optimization.

Most of the work is theoretical, and few negative societal concerns would apply.

Rating:
9

Confidence:
3

REVIEW 
Summary:
This work introduces a local optimality condition for models (with respect to proper losses) based on (additive) post-processing with a 1-Lipschitz function that is necessary and sufficient for calibration. The authors also connect their results to the idea of implicit regularization, showing that structural risk minimization with an appropriate class of complexity measures achieves good calibration.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
## Originality
The introduced notion of post-processing error and its connection to (smooth) calibration is novel and useful, as it proves plausible explanations for why current SOTA deep learning models are better calibrated when compared to previous generations. Additionally, the introduced local optimality condition based on post-processing is distinct from conditions seen in optimization, but remains non-trivial since the class of post-processing functions is restricted to be 1-Lipschitz.

## Quality
The paper is technically sound and polished; the definitions are well-motivated and simpler results are accompanied with proofs (or at least intuitive justification) in the main body of the paper.

## Clarity
The theory is easy to follow, and the authors have done a good job of scaling the complexity of the results as the paper progresses (introducing simple examples and high-level ideas first, and generalizing later). The authors also qualitatively connect their results to recent empirical phenomena in deep learning, which is helpful for grounding the theory.

## Significance
Calibration is an increasingly important problem, and the paper provides insights into how to think about the relationship between calibration and post-processing procedures.


Weaknesses:
- *Experiments:* While I understand this is a theory paper, I do think it would be nice to have some experimental analysis of whether the local optimality condition is satisfied for modern architectures (i.e. simply adding an extra layer as suggested in the paper and analyzing calibration performance).
- *Addressing Popular Post-Processing Methods:* If I understand the discussion around Definition 2.2 correctly, the post-processing operation defined does not include temperature-scaling-type techniques, since $f(x)$ includes the sigmoid operation and one would have to apply the re-scaling to the logits. If this is not the case, some clarification in this section linking the post-processing definition back to the standard post-processing approaches would be useful.

Limitations:
The authors appropriately address limitations; namely, they acknowledge that their results do not explain _why_ the optimization process for current models leads to calibrated predictors.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper considers calibration in binary classification when training was performed with proper losses. The authors showed that
the post-processing gap of a predictor, which is a maximum improvement of the loss given any 1-Lipschitz update (calibration) function,
could be both lower and upper bounded by a simple expression of smooth calibration error. Moreover, the authors proved that the constants used in upper and lower bounds are optimal.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper describes the connection between proper loss landscapes and calibration error improvement, which is an important and quite original problem under the current formulation.

The paper has good positioning among other papers in the field, with clear references to many studies and a detailed indication of what was their contribution.

The clarity of the paper is good.

Weaknesses:
The link with potential practical application is missing (or rather not convincing). Adding small-scale experiments to support theoretical results would be beneficial, e.g. to check whether the pre-requisites of the theoretical results are (approximately) fulfilled in practice.

The discussion and explanation of theoretical results could be improved. The current impression is that either theoretical results are not that impressive (and based mostly on properties of Bregman divergence) or that the potential impact of derived theory is oversold.


Limitations:
The limitations have been covered at the end of the paper. 

No need to discuss societal impact in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper provides a novel characterization of calibration as local optimality of the predictor w.r.t the global loss under post-processing of the prediction through a class of functions. The paper proves that any predictor satisfying such a condition is smoothly calibrated in the sense of [Kakade and Foster, 2008, Błasiok et al., 2023] and vice-versa. The paper further provides arguments for why such a condition should be satisfied by Deep Neural Networks (DNNs).

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The paper is well written, with a clear structure and ideas being developed in a coherent and logical sequence.
- The topic is of large importance in the present context of research in machine learning and has numerous implications for practical applications.
- The authors contribute a novel perspective on the 'implicit' calibration of machine learning models without the use of algorithms designed specifically for calibration, an aspect that is not extensively explored in current literature. 
- The explicit characterization of calibration in terms of local optimality under post-processing transformations, while implicit in earlier works, appears to be novel. This equivalence could be useful for further theoretical analysis of calibration, especially for deep neural networks.
- The proposed framework is general and does not rely on specific choices of model's architecture or data-distribution.
- The paper provides partial explanations for the observed calibration of deep neural networks trained on large training datasets.

Weaknesses:
- **Limited technical contributions**: Claim 2.1 itself directly follows from the definition of perfect calibration. The main theoretical results in the paper are generalized formulations of Claim 2.1 to smooth calibration, lipschitz-post processing functions, and general proper losses. While these generalizations themselves are novel, their proofs involve straightforward algebra and convex analysis. Therefore, the technical and mathematical contributions of the paper are limited. The results can be strengthened from examples of non-trivial results that can be derived using the local-optimality based characterization of calibration. For deep neural networks, the present results are only suggestive and would benefit from additional details and concrete results.
- **Missing references**:
     - On double-descent in uncertainty quantification in overparametrized models: Lucas Clarte, Bruno Loureiro, Florent Krzakala, Lenka Zdeborova, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, PMLR 206:7089-7125, 2023.
     - Theoretical characterization of uncertainty in high-dimensional linear classification, Lucas Clarté, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová, Machine Learning: Science and Technology, Volume 4, Number 2

    The above papers analyze calibration for empirical risk minimization and like the present paper, also highlight the role played by 
    regularization. 

- **Dataset size and overparameterization**: The paper does not address aspects related to generalization and the effect of the training dataset size. While modern training setups utlize large training dataset sizes and one-pass SGD, their behavior in the proportional regimes of high-dimensional inputs and parameters
is non-trivial and not equivalent to the minimization of population loss. For instance, the above papers establish a double-descent like phenomenon for calibration for varying levels of overparametrization.

- **Experiments**: In light of the limited technical contributions, the paper could benefit from experimental justification of the validity of the local optimality condition for deep neural networks in realistic training setups.

Limitations:
Yes, the limitations have been adequately addressed. The work is theoretical in nature and does not have direct societal impacts.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper seeks to explore and formalize the relationship between minimizing proper loss and calibration in machine learning models, particularly deep neural networks (DNNs). It presents a local optimality condition that is necessary and sufficient to ensure model calibration. The work discusses the implications of these findings on the calibration properties of modern DNNs and presents algorithms that can guarantee calibration. The paper also contrasts the differences in calibration between current and previous generation models through the lens of generalization.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
NA

Weaknesses:
NA

Limitations:
NA

Rating:
7

Confidence:
3

";1
77Nq1KjmLl;"REVIEW 
Summary:
In this paper, the authors proposed contrastively and predictively strategies for pretraining GNNs based on graph fragmentation. Using principle subgraph extraction, the authors pretrain two separate encoders for molecular and fragment graphs, capturing structural information at different resolutions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-organized and easy to follow.
2. The authors conduct comprehensive comparisons with baselines to show their advantages.
3. The fragment-level contrastive pretraining framework is novel, which captures both granular patterns and higher-order connectivity.
4. The t-SNE visualization in Figure 3 clearly shows the effectiveness of the authors' design.

Weaknesses:
1. The technical contribution is limited. For example, the principle subgraph extraction module is borrowed from [19].
2. The authors do not clearly state how to choose hyperparameter alpha.
3. In Table 3, the effects of the vocabulary size are only explored on three datasets.
4. The pertaining time is not reported in the paper.

Limitations:
The limitation is not clearly discussed. Please provide the discussion in the rebuttal period. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper presents a novel approach to pretrain Graph Neural Networks (GNNs) at the fragment level for property prediction on molecular graphs. By utilizing a compact vocabulary of prevalent fragments and introducing fragment-based contrastive and predictive pretraining tasks, the authors overcome the limitations of node-level and graph-level pretraining. Two different GNNs are pretrained to capture structural information at multiple resolutions, and the fragment information is utilized during finetuning. The proposed models show improved performances on both common molecular and long-range biological benchmarks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is easy to follow.
- The idea that using motif for pretraining is novel and reasonable.



Weaknesses:
- Empirical performance is not strong enough. Especially in Table 2, it's hard to distinguish the absolute gain over the baselines. The authors are encouraged to report the average score over all tasks in molecular property prediction.
- The authors are also encouraged to compare with stronger baselines. For example, the authors can also compare JOAO V2 in addition to JOAO.
- Missing ablations: the authors add many components and loss functions in the system. It would be interesting know how each contribute to the performance.

Limitations:
Yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
Based on the belief that learning with fragments can help capture structural information at multiple resolutions, this paper proposes a fragment-based strategy for pretraining and fine-tuning. 

First, the paper extracts fragments by an existing heuristic algorithm called Principle Subgraph Mining algorithm to obtain fragments from a large molecular dataset (i.e.,  ChEMBL database). Then, the paper uses two separate GNNs (one for molecules and one for fragments) to learn the node embeddings. In particular, the node embeddings obtained by molecular-based GNN are pooled into fragment embeddings. 
The model is trained with respect to three tasks: a contrastive task between fragment embeddings obtained by molecular-based GNN and  fragment-based GNN, a fragment existence prediction task, and a fragment graph structure prediction task. 

The experiments are done on 8 binary graph classification tasks from MoleculeNet and 2 graph prediction tasks on large peptide molecules from the Long-range Graph Benchmark.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ The proposed method is easy to follow and conduct. 
+ The results on Long-range Graph Benchmark are particularly good. 
+ Figure 1 clearly shows the method.
+ Codes are provided. Appendix further provides more details.

Weaknesses:
The idea of using molecular fragment can be interesting. The proposed method shows some effectiveness, although how it obtains can be less illuminating. 

Many design choices need to be further described. Please reply to my questions below.

In addition, some writing issues exist. Sentences cannot start with ""[reference]"".

Limitations:
No potential negative societal impact of their work as far as I know.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose a novel method for generating representations for molecule graphs where two GNNs are contrastively learned. Using this new represntations, the authors achieve good results compared to a variety of baseline methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper and method are presented clearly.

The results are strong and the method is interesting + well explained 

Weaknesses:
I found the presentation of Figure 3 a bit confusing

Limitations:
N/A

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper proposes a contrastively and predictively strategy for pretraining GNNs based on graph fragmentation. Specifically, it leverages a frequency-based method for extracting molecule fragments, and performs fragment-based contrastive and predictive tasks to jointly pretrain two GNNs on a molecule graph and a fragment graph. It also enforces the consistency between fragment embeddings and atom embeddings for multi-solution structural information.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	The paper is easy to follow.
2.	The paper investigates an interesting fragmentation strategy for pretraining tasks.
3.	The proposed method enforces the consistency between fragment embeddings and atom embeddings for multi-solution structural information, which is a promising trick. Experiments demonstrate the effectiveness of this method.

Weaknesses:
1.	The technical novelty is limited, because it is a combination of existing methods. While the performance improvement is not very remarkable.
2.	The authors may want to conduct ablation studies on the effect of molecule fragmentation strategy and the pretraining strategy, respectively.
3.	Table 3 shows that the performances worsen on some downstream benchmarks as the vocabulary size grows larger. The authors may want to investigate a smaller vocabulary size. When it reaches 1, the method is the same as without fragments.
4.	A related work [1]---which leverages a similar frequency-based motif extractor and uses contrastive learning for generative training---is missing.

[1] Zijie Geng Z, Shufang Xie, Yingce Xia, et al. De Novo Molecular Generation via Connection-aware Motif Mining. ICLR 2023.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
DP2lioYIYl;"REVIEW 
Summary:
This paper proposes a theoretical framework for understanding unsupervised translation and provides bounds for sample complexity. Specifically, the unsupervised machine translation setting they consider is that the translator $f_\theta$ is the one that maximizes the plausibility $\rho$ of the translation of examples $x_1, x_2, \cdots, x_m$ in the source language:

$$\theta = \mathrm{MLE}^{\rho}(x_1, x_2, \cdots, x_m) = \arg \max_{\theta} \sum_{i=1}^m \log \rho(f_\theta(x_i))$$

The framework they propose has the following key components:

- Prior $\mu$: the distribution of the source language.
- Prior $\rho$: the plausibility of the translation in the target language.
- Translator: $f_\theta$ which they assume belongs to a finite-sized family.
- Translation density $\tau$: The distribution induced by the translator, i.e. $f_\theta{x}$ where $x \sim \mu$.
- A ground truth translator: $f_*$ that maps any input $x$ to a plausible translation almost surely.
- Semantic loss: $L(\theta)$ that evaluates how the translations by $f_\theta$ is correct. They use exactly match (0-1 loss) as $L$ in the following analysis.

As instantiations of their framework, they propose two models of language:

## Random knowledge graph model

This model makes the following assumptions:
1. Text in this language represents an edge between a pair of nodes in a knowledge graph. 
2. The translator translates an edge by translating the two nodes respectively. 
3. The plausible edges in the source language largely agree with the plausible edges in the target language.
Based on these assumptions the authors construct the distributions $\rho, \tau \mu$, and then they provide the sample complexity bound for this model.


## Common nonsense model

This model assumes no linguistic structure on the source text. They only assume that there is a set of translations that are implausible. They provide a sample complexity bound for this model. They also make connections to the complexity of the translator (description length).


Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
1. In my opinion, this paper’s assumption about the existence of a set of plausible translations is very realistic and insightful for understanding the mechanism of unsupervised translation. As far as I know, attributing the success of unsupervised translation to this characteristic is novel and I think it is convincing (though may not be a complete explanation).
2. This paper also demonstrates how this assumption can be used to analyze unsupervised translation.
3. I also like the consideration about having a plausible prior in addition to the ground truth translation distribution. It handles the mismatch between the domains of the text in the source and target language. I think this kind of domain mismatch is very common in practice and thus considering this helps us understand unsupervised translation better.
4. The knowledge graph model is inspiring to me. I think it suggests that the success of unsupervised translation is because of the interaction between some high-level structure and some low-level structure of language. In this model, the high-level structure is the plausibility of the combination of nodes, while the low-level structure is the one-to-one mapping of the nodes (if I understood correctly).

In sum, I think I learned quite a few things from this paper. I think the community can benefit from these ideas.


Weaknesses:
## Clarity Issues

The first major issue to me is that this paper has a very unusual structure, i.e. having 5 pages of introduction. 
1. I would expect the introduction to talk more about the motivation and high-level intuition of the theory. However, the authors tried to elaborate many definitions in the introduction but at the same time mention a few things without clear definition. For example, I couldn’t understand “broader prior (line 69)” and “agreement (line 99)” when I was reading the introduction. 
2. Another problem of this long introduction is that there seems to be a lot of overlap between the introduction and the content in the following sections. Information in the later sections does not seem to be totally aligned with the description in the introduction section. For example, at line 101, the authors assume the compositional encoding of language, but this is not mentioned in Section 3.1.

The second  issue is that this paper never provides a high-level explanation for the proofs in the main text. It’s hard to assess the correctness of their results. I found the proofs in the appendix are not easy to follow either. 

The third issues is that they rarely provide explanations for the symbols or the formulas either. For example, in Definition 3.1, they define many sets (e.g. $P, S$ etc) without describing their intuitive meanings. The usage of the symbols is also not very intuitive. For example, it seems that the $T, S$ and $P$ in Section 3.1 have different meanings than the ones in Section 3.2.

## Technical Issues

I have two main concerns:

1. I doubt whether Theorem 3.4 is correct. Intuitively speaking, based on this model, there are many translators that never map any source text to an implausible target text. However, only one of them is correct. How can it be possible that the translator learned from MLE can be a good one without the access to other information? I also checked the proof in Appendix C.1.1. Line 714 seems to be saying it is suffice to prove that “large semantic error ($A$) implies many implausible translations ($B$)”, but the formula in Line 717 is more like saying “it’s impossible to have low error ($\neg A$) while having many implausible translations ($B$)”. If the goal is to prove $A \to B$, then I can’t see why we should prove $( \neg A \wedge B)$ is impossible.
2. I think there needs to be more justification for how the instantiations are related to real world scenarios. One apparent example is the distributions of $\rho$ and $\nu$ constructed based on Definition 3.1, which is very complicated and is not intuitive. I would like to know whether this is also an abstraction of real-world scenarios.

In sum, I think this work is not ready for publication in this form, despite the interesting insights it has.


Limitations:
In my opinion, the authors should list their assumptions more explicitly and clearly, e.g. including them in a list or indexing them with numbers. Other than that, I think the discussion section addresses the limitations well.

Rating:
3

Confidence:
3

REVIEW 
Summary:
### After rebuttal

Thanks the authors for the detailed rebuttal, especially with new experiments. Provided the condition that the authors will add the new experiments and adjust the presentation after reviewers' comments, I gladly change my scores and recommendation!

----
The paper presents a rather interesting theory about how unsupervised machine translation (UMT) without any parallel data. The paper taps into the analysis of world knowledge graph models, which may model relationships between objects/subjects of the world, and presented by words in the data. And suggests that it is the compositionality of languages somewhat allows UMT to work.

The paper also analyze the common nonsense model concept, where UMT generally tries to rotate the source distribution so that it can fit in a unique orientation in the target distribution, given enough nonsense data.

The paper presents some conclusion about the theorical error rate of UMT in relation with data and languages, though no experiments were done.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
* The concept and analysis of the UMT in the paper is very interesting and novel.
* The theoretical models of random knowledge graph and common nonsense models are interesting and should be investigated further.
* Some observation from such formulation is that error rate is inversely related with language complexity, which is indeed non-intuitive, but there is no experiment to confirm such observation. 

Weaknesses:

* The paper is very math heavy with limited visualization or intuitive explanation. This makes it rather difficult to comprehend and and grasp the content. More intuitive explanation with figures and illustration are needed to understand. I am also unsure about the correctness of the formulation and the math, and can only assume it to be correct in the review.
* There is no experiments to validate the theory at all, even with a toy experiments. This makes it hard to conclude the completeness of the paper.
* The organization is not very well-structured and rather strange. There are no experiments and related work sections.
* Despite heavily advertised in the initial part of the paper, there is limited correlation, conclusion, or elaboration about how we can model such human-animal communications, which is quite disappointing.
* There is no clear indication and suggestion of what to do with UMT after reading this theory. The paper does not make recommendations on how to design better UMT models, how to train them, which aspects of existing UMT models need to be fixed so that it can handle low-resource languages and more ambitiously human-animal translations. This is real challenges that UMT performs very poorly when domains of unlabeled data sources are very distant.

Given the high degree of novelty of implication of this work, I would give much higher score if the rebuttal addresses some of the concerns.

Limitations:
There is no discussion of limitations.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper introduces a theoretical framework for unsupervised machine translation (UMT) when no parallel data available and source and target language do not share similar linguistic structures. To do this, they first define and clarify the three main challenges: understanding the goal, no linguistic structure shared and domain gap. In response to these challenges, they propose a general framework and instantiate it with two models: knowledge graph model , which is highly structured with text representing an edge between a pair of nodes, and ""Common nonsense"" model which is completely unstructured. For both models, they establish the error bounds and demonstrate that the error rate is inversely related to the amount of samples, common ground, and the language complexity.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The paper presents a novel theoretical framework for UMT, which is needed but challenging in the field of UMT.
* The authors propose two complementary models of language, one that is highly structured and one that is completely unstructured, which provides a relatively comprehensive analysis of UMT.
* The authors provide theoretical bounds on the data requirements for UMT, which can inform how much data are collected for such tasks.
* The authors' ideas are interesting and insightful, such as the motivating example in the knowledge graph model where one can complete the mapping based on the similar structure of the graph, and enough ""nonsense"" make a nearly unique mapping in ""common nonsense"" model.

Weaknesses:
* The paper makes strong assumptions about the language, which may limit the applicability of the proposed models in real-world scenarios. 
* The paper does not provide any empirical validation of the proposed theoretical framework and models. The absence of experimental results makes it difficult to assess the practical effectiveness of the proposed methods.
* The paper does not provide a clear explanation of how the proposed models can be implemented in practice, which may limit their usability for practitioners in the field.

Limitations:
Same as Weaknesses. Or can we consider a real experiment, such as mutual translation between knowledge graphs constructed by English and Chinese respectively?

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper describes a theoretical analysis of error bounds for unsupervised machine translation, motivated by considering the unusual, and so far only speculative, case of translation between animal ""languages"" to human natural languages.

The paper consider the case where a number of examples of the source (animal) language utterances are available, as well as a prior on the target language (e.g., a conventional pretrained LLM), and attempts to investigate what error an invertible translation function trained to maximize a MLE objective may achieve compared to a hypothetical golden truth translator.

The paper considers two specific cases with different assumptions over the languages and the priors: in the first case the languages are assumed to be Erdős–Rényi random graphs parametrized by an edge agreement parameter. The authors prove a bound which does not vanish even in the limit of infinite source sentences but vanishes in the limit of perfect agreement. I do not find this model to be very realistic for natural languages.

The second case consider the two languages to be uniform distributions over a subset that excludes a ""nonsense"" set. The authors prove an bound similar to supervised learning. I am not sure why uniformity is considered a plausible hypothesis here, as actual natural languages are characterized by power law distributions.

Overall, I think there is not enough justification connecting the formal models with the motivation of translating animal communication.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Novel and interesting topic.

- Theoretical analysis.

Weaknesses:
- Assumptions not well justified

- Formal model not well connected with motivation

- Too much background and related work is missing. Some is provided in the supplementary material, but it should be in the main paper.

Limitations:
Yes.

Rating:
4

Confidence:
2

REVIEW 
Summary:
### Update

I'm satisfied with the author's rebuttal and decided to overall improve my score.


This paper proposes a theoretical framework for analyzing Unsupervised MT motivated by animal communication. The paper is only the first step in the theoretical analysis of UMT. The theoretical framework was analyzed for two models: Knowledge graph and Common nonsense models. The authors show theoretically that the error rate is inversely related to the number of samples, common ground, and language complexity.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The authors work on providing a theoretical framework for analyzing unsupervised MT. The task is motivated by a real-world challenge of understanding animal communication.

The paper is well-written and was not too hard to follow. The general framework was well-defined and motivated. 

The paper instantiated two models with common-nonsense and knowledge graphs. The paper derived theoretical bounds for error rates and provided comprehensive proof.

Weaknesses:
Purely descriptive: what is the key takeaway from the paper? How can we improve unsupervised MT based on these findings? Where can we use the theoretical framework?

No experimental results? I realize the paper focuses on the theory of unsupervised translation. However, none of the author's theoretical claims are validated using experimental findings which I find a bit hard to follow. 

The authors claim that the error rate is inversely proportional to language complexity. This is counterintuitive to what I would expect in Machine Translation. However, there is no in-depth discussion, reasoning, or intuition as to why this is the case. Additionally, the related work or motivation section is missing in the main paper, where I believe they should motivate UMT.

I missed the connection in the paper with learning animal communication. The paper initially has a strong motivation to learn animal communication, however, the discussion section is weak and I could not find what to conclude, or whether can we actually use the unsupervised MT theoretical framework.

Limitations:
The authors did not provide any limitations on their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The first contribution of this work is formalizing and analyzing a model of Unsupervised Machine Translation (UMT). The model applies even to low-resource source languages with massive domain gap and linguistic distance. Second, the paper exhibits two simple complementary models for which it is proved that:
(a) more complex languages require less common ground, and 
(b) data requirements are not significantly greater than those of supervised translation (which tend to be significantly less than generative language modeling). These findings may have implications for the quantity and type of communication data that is collected for deciphering animal communication and for UMT more generally.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This work is the first theoretical work formally proving error bounds for prior-assisted translation, which shows that the sample complexity should remain roughly the same between the supervised and unsupervised settings (specifically, the two stylized models of language -- Random sentence trees and Random knowledge graphs), barring computational constraints.

Weaknesses:
There isn't any empirical evaluation aside from using text-davinci-02 to evaluate sentence likelihoods

Limitations:
N/A

Rating:
9

Confidence:
4

";1
z2BHMLA8pM;"REVIEW 
Summary:
Techniques such as Deep Kernel Learning and extensions such as Deep GPs have been proposed in order to overcome the flexibility constraints associated with standard shallow GPs. However, although these techniques are better suited to model non-stationary data, they are still susceptible to issues such as pathologies when extended to more than a few layers, as well as overfitting. These techniques also lack the interpretability of the latent space and associated length-scales that make standard GPs so appealing. In order to mitigate these limitations, the authors propose a novel approach that generalises over the more widely-used compositional DGPs via a hierarchical model that incorporates locally linear deformations of stationary kernels. A synthetic example is crafted in order to showcase how TDGP improves over competing methods for problems exhibiting heavy non-stationarity. This is further complemented by an analysis on a real-world dataset, as well as 4 benchmark datasets from the UCI repository.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well-written and a pleasure to read. I commend the authors for structuring the paper in a manner that clearly shows connections to related models, while also showcasing the key contributions of this work.
- I particularly appreciated Sections 4.1 and 4.2, which capture the various dimensions along which TDGP can lead to improvements over other methods (predictive performance, uncertainty quantification, and interpretability).
- The problem statement tackled in this paper is well-motivated, and the limitations of other techniques are clearly pointed out both verbally and visually in the experiments section. One possible improvement to this could be highlighting application domains where TDGP is expected to be most impactful – i.e., which application domains tend to non-stationary data where well-calibrated uncertainty is especially useful. While fairly minor, this could help clarify the significance of this work earlier on the paper.  

Weaknesses:
- The authors comment in the *Limitations* section that the increase in the number of variational parameters could slow down optimisation – it could be worth supporting this statement by specifying the computational complexity, and maybe an analysis reporting wall clock time for one of the experiments in Section 4. 
- I would also be interested in whether the increased parametrisation of the model results in models that are less stable or difficult to consistently converge. This has implications on the practical utility of the model.
- The paper focuses heavily on using the SE kernel throughout – can this be extended to other kernels or is the value-add of this extension not considered to be worthwhile for the paper?

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper extends the CDGP to address two weaknesses in deep Gaussian processes. The solution is similar intuitively to a residual connection. Instead of letting each layer depend only on the last layer, they let each layer depend on the last layer and the input. This allows the model to induce a manifold and use a lengthscale field.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This seems like an important problem, and the solution seems principled.

Weaknesses:
There should be more discussion of what it means to not induce a manifold, why violating the triangle inequality leads to that, and why not inducing a manifold is a drawback. I think inducing a manifold implies that one can make the latent space plots: why are these valuable? Why do they require the triangle inequality?

There should be more discussion of how a lengthscale field leads to interpretability.

In both the latent space and the inverse lengthscale plots, there should be some discussion about why we want to see these plots: for instance, in figure 7, we have the two sets of plots: what can we glean from them about the data/problem/etc? Figure 8 has a little bit (the zone of high correlation extends into the mountains), but this is not obvious for a non-expert application reader. You could have a short tutorial perhaps of how to read these plots in the supplement.

Limitations:
They have a good discussion of limitations: mainly that it is computationally expensive and leads to requiring more parameters.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of non-stationary kernel design for Gaussian processes. Two main approaches for this problem are deformation kernels and length-scale mixture kernels. On one hand, deformation kernels, eg deep GPs, have found great success while trading off expressivity and the ability to learn latent manifolds in intermediate layers, for lack of interpretability and being prone to pathologies. On the other, length-scale mixture kernels retain interpretability but typically break the triangle inequality in the induced latent space, precluding a manifold structure. The authors propose thin and deep GPs, which aim to combine the benefits of both these approaches, while mitigating their drawbacks. The approach learns a position-dependent matrix, parameterized by a GP, which learns a pointwise linear transformation of the data. The output of such a layer can then be composed to learn a new matrix acting on the original data, enabling a deep learning structure. Numerical examples are provided which appear competitive with existing techniques.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is clearly presented and combines existing ideas in a novel way. The numerical examples seem well selected, with a mixture of synthetic/analytic examples that demonstrate the utility of the method, with standard benchmark datasets allowing for meaningful comparisons on real data. A case study is also provided that is geared towards the strengths of the technique.

Weaknesses:
The main weakness of the paper in my view is Theorem 3.1. It appears that the proof that the CDGP is a generalization of deep GPs hinges upon the introduction of a new variable that plays the role of the deep GP, and then turning off their method's parameters by concentrating their priors around zero. This seems highly misleading unless the method is introduced in a way that uses the augmented space as default, or some numerical evidence is given that it should be turned off if appropriate. In this case, presumably the deep GP should also be tested with and without an augmented space for fair comparison. It seems that the augmented space is not used in any of the experiments in any of the models presented, since the local linearity leaving the data unchanged around zero is listed as a limitation.

Limitations:
Besides those listed above, the computational expense is listed as a major limitation. This is a common drawback of related work, and it may be helpful to list the memory and computational complexity of this method with L layers, as is commonly done in the support literature.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper presents a new deep Gaussian process (DGP) model, the thin and deep GP (TDGP), which does not suffer from a diminishing signal as the number of layers increases. The crux of the TDGP model is the covariance function that, for each layer $\ell$, acts on a linear combination of a (non-linear) transformation of the outputs of the previous layer and the inputs $\mathbf{x}$, $\mathbf{h}^\ell(\mathbf{x}) = \mathbf{W}^\ell(\mathbf{h}^{\ell-1}(\mathbf{x})) \mathbf{x}$. This hierarchical construction is shown to cover the usual DGP models while not suffering from diminishing signals as the depth increases. To perform inference in this model, the authors adopt a mean-field VI scheme, which is demonstrated to be effective through a number of experiments on both synthetic and real-world data.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper presents an interesting and original approach to DGPs, an important field that should interest the NeurIPS community.
2. The paper is of high overall quality and very well-written.
3. The presented model has the potential to be of considerable significance, given that it generalises previous DGP models while not suffering from the same pathologies.

Weaknesses:
1. While I think the paper presents a great idea with lots of potential, my main concern is the lack of a deeper analysis of the model. Given that the proposed model is essentially just a particular transformation of the inputs at each layer and a rather straightforward mean-field VI scheme, I would expect some insights into the behaviour of the model, even if these were just empirical. The current experiments do present some interesting insights, but, say, what happens to the model's performance as we increase the depth of the model? What if we change the width of the layers? What if we use different covariance functions than the squared exponential? How does the induced covariance matrix look at different layers of the model compared to, say, DNSGP? Why does the model encourage low-dimensional latent spaces, and does this happen too for models deeper than just two layers? A Gaussian prior shouldn't encourage sparsity, so this aspect of the model is particularly puzzling.
2. As mentioned, a standard mean-field VI scheme seems very rough given that this is known to perform poorly for many models, including DGPs. To my knowledge, the current state-of-the-art is still the doubly stochastic VI scheme by Salimbeni & Deisenroth (2017), so it would make sense to try something similar (and unless I misunderstand something, it seems fairly straightforward to do this for the proposed model). 
3. One clarity issue (perhaps the only one) I find with the paper is that it is unclear to me which DGP model the authors refer to as ""CDGP"". Deep GPs have evolved dramatically from the original formulation of Damianou & Lawrence (2013), and the mentioned pathologies are not as pronounced anymore (to my knowledge). It is great that the authors compare against DNSGP, but comparing against the doubly stochastic DGP rather than the original formulation makes much more sense. As it is unclear if the authors actually compare to the current state-of-the-art, it is also unclear if the proposed model addresses actual problems.
4. Experimentally, the model seems to work well, but the authors use only two layers for all models. This is particularly strange as a key selling point of the proposed model is that it doesn't degenerate as the number of layers increases. But it is also an odd choice since deeper models should work better (and DNSGP demonstrably does for two of the UCI datasets). It is also strange that MRAE statistics are only reported for the GEBCO dataset; these should also be reported for the UCI experiments.
5. The model is claimed to be ""interpretable"", but this only seems to be the case for a two-layer model (i.e., one with a single latent space). For deeper models, the distance matrix is still a highly non-linear function of the inputs, which I don't think will be particularly interpretable.


References:

- Andreas Damianou & Neil Lawrence, ""Deep Gaussian processes"", AISTATS 2013.
- Hugh Salimbeni & Marc Peter Deisenroth. ""Doubly Stochastic Variational Inference for Deep Gaussian Processes"", NeurIPS 2017.

Limitations:
The authors have adequately addressed limitations in a dedicated paragraph.

Rating:
7

Confidence:
3

";1
VbYdaK8ek0;"REVIEW 
Summary:
This work presents an efficient adapter design for transfer learning of large retrained models. The key idea is to enable adapter to be shared across layers and imposing low-rank constraint, so that the overall trainable parameters can be reduced. Given the re-composed linear adapter, existing reparameterization methods can be further used to avoid extra computation cost. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Parameter efficient transfer learning is an important approach for making large pertained model useful in many applications.

- The paper is written to be easy to read.

Weaknesses:
First of all, the motivation of this work is weak -- Existing adapters are already lightweight, e.g., less than 0.5% of the pretrained model, and the scope for further reducing the size of adapter is marginal. That being said, this adapter efficiency problem is not significant. 

In introduction, it is hard to read out the novelty of this method since more related works such LoRA is not discussed and compared at all.

Limited novelty with the proposed method: Low-rank constraint is not new, as already used in LoRA [24] although not in the same way, and parameter sharing across layers is also not novel idea. This work combines the two in a single place, which could be considered as being not significant.

Limited performance gain: 
- In most cases, the proposed method cannot achieve a good margin over previous methods (e.g., SSF). 

- Another important metric, FLOPs, would be useful to report.






Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces a parameter-efficient transfer learning method named Adapter Re-Composing (ARC), which mainly focuses on investigating the reusability of adapted parameters. The authors propose to apply a shared adapter to all the layers (blocks) of the pre-trained model, and they use different Re-Scaling Coefficients (diagonal matrices) in different layers to ensure the diversity of parameters in different layers.
The motivation behind this design is the adapter module's low-rank property, shown in Fig.3 in the main text.
Moreover, the authors conduct extensive experiments to demonstrate the effectiveness of their designs, where they train fewer parameters and achieve competitive or even better performance compared to prior arts.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1: This paper is well-written and well-organized.

2: The ARC design in the paper is well-motivated.

3: Although the technic is simple and easy to implement, the performance is impressive.

4: The experiments are comprehensive.

Weaknesses:
1: The ARC leverages learnable re-scaling coefficients in different layers to maintain diversity. However, the authors didn't disscuss the numerical difference among the re-scaling coefficients across different layers. If they are also similar, a share-weight adapter can replace the ARC. 

2: In Tab. 1 and Tab. 2, the SSF* applies additional techniques (e.g., data augmentation) during training and gains non-trivial improvements. Why did the authors not use those techniques to improve the performance of ARC further? Will these techniques further benefit the ARC?

3: I wonder about the computational overhead of the ARC, e.g., the GPU memory usage during training, because the number of learnable parameters may not necessarily be positively correlated with the GPU memory usage. Could authors provide the additional GPU memory usage comparison between ARC and other related works?

4: How to use ARC in Hierarchical Vision Transformers such as Swin Transformer is unclear in Lines 277-279. Adding more details here may be helpful.

Overall, my major concern is weakness 1. Happy to raise my rating if my concers are well adressed.

Limitations:
The authors disscuss the limitations and boarder impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper explores ARC, which is a novel parameter-efficient fine-tuning method which uses a similar architecture as adapters but introduces inter- and intra- layer weight sharing. Some down- and up- projection weights are shared but every adapter position uses an independent set of per-channel scaling factor on the channel-reduced intermediate features. The proposed method obtains competitive results on various vision transformer adaptation benchmarks in terms of recognition performance and number of parameters. Moreover it can be re-parameterized into adjacent fully-connected layers so no overhead is incurred during inference.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
* The paper is generally well written. The figures are clear and the text is easy to follow.

* The experiments are comprehensive and cover a wide range of visual recognition datasets and the results are competitive.

Weaknesses:
* The effectiveness of the proposed method is not well justified. Section 3.3 does not make much sense to me: Having long-tailed singular values only implies that the weight difference can be approximated well by a low-rank matrix and thus justifies the bottleneck design. Shared projections further requires the adapters to have largely overlapped kernel and image spaces, which, instead of the singular values themselves, are determined by the direction of the singular vectors corresponding to the top singular values. It also doesn't theoretically justify the intra-block sharing design (i.e., using the transpose of down-projection as up-projection).

* In multiple places (e.g., caption of Table 1 and Line 229), the paper's claim of using simple augmentations on baselines for 'fair comparison' is questionable. Due to the vast differences in nature of different methods, it is expected that their optimal training configurations are different, and each method, including this paper's own, should have the right to choose its optimal training configuration as long as it does not violate some principal rules of machine learning (e.g., leak of test data): Conversely, it is also improper to request that the paper's method being run under SSF's data augmentation for 'fair comparison'.

Limitations:
There are no unmentioned limitations in the paper to my mind.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to further reduce the parameters of the adapter by introducing a weight-sharing scheme between different layers. To accommodate the variations across different layers, re-scaling coefficients are learned to re-compose the layer-adaptive adaptation matrices. Experiments are conducted on 24 downstream image classification tasks using various Vision Transformer variants.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ The observation that “learned adaptation matrices naturally exhibit low-rank characteristics” as shown in Fig. 3 is quite interesting, making the motivation for the weight-sharing design compelling. 
+ This paper is well-written and easy to follow. The figures are well-prepared and illustrate the core idea clearly. 
+ The experiments are extensive. 


Weaknesses:
- The comparisons to SSF shown in Tables 1 and 2 are re-implemented by the authors and data augmentations are removed. Their original performances of SSF are much higher than the proposed approach. It would be better if the authors reported the performance of the proposed approach with these advanced data augmentations.
- Fig. 3 shows the singular value distribution of the original MHA and FFN adapter. It would be better to show how the proposed approach alleviates such a problem by visualizing the value distribution after using the proposed approach.
- Though the observation of “learned adaptation matrices naturally exhibit low-rank characteristics” and the proposed weight-sharing scheme is very interesting, the motivation to further reduce the parameters of the adapter is not very compelling as the parameters of the adapter are already relatively small. 


Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel parameter-efficient fine-tuning method called Adapter Re-Composing (ARC). ARC effectively reuses parameters across different layers, resulting in remarkable improvements in performance across 24 image classification datasets while utilizing fewer learnable parameters.
The experimental evaluation conducted on various downstream datasets provides compelling evidence of ARC's superiority. It outperforms existing methods and establishes a new benchmark for parameter-efficient fine-tuning techniques.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed ARC method is simple yet effective, achieving superior performance on multiple downstream datasets while utilizing fewer trainable parameters.

2. The experiments conducted provide compelling evidence, as they encompass various datasets, attention-based architectures, and ablations, ensuring the robustness and reliability of the findings.


Weaknesses:
1. In Table 3, it is observed that the performance of ViT-Huge is lower than ViT-Large. It would be beneficial if the authors could provide an explanation for this disparity.

2. The utilization of symmetric matrices ($W_{up} = W_{down}^T$) in the bottleneck design helps reduce the number of learnable parameters. However, it would be interesting to explore whether further improvements in performance can be achieved by making the downsampling and upsampling matrices independent. It would be valuable if the authors could provide a comparison of performance and parameter statistics to address this potential enhancement.

3. The experiments conducted have demonstrated the effectiveness of the proposed method. However, it would greatly enhance the strength of this paper if the authors could supplement these empirical results with theoretical analysis.

Limitations:
Please see Weaknesses.

Rating:
6

Confidence:
4

";1
Lt3jqxsbVO;"REVIEW 
Summary:
This paper develops bounds on how close the approximated Koopman modes and eigenvalues are to the true eigenvalues and modes, for two important classes of methods used to compute the Koopman mode decomposition. The authors find that one class, Principal Component Regression, which included extended dynamic mode decomposition (EDMD), can suffer more from poorly chosen kernels and can have larger bias than Reduced Rank Regression (RRR). They additionally provide an empirical method for determining spurious eigenvalues, which can be used for model selection.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well written and easy to follow. 

2. This paper provides new techniques for computing bounds on the approximated Koopman spectral objects, and the discovery that PCR can have larger bias than RRR, are important ones for the field. 

3. This paper provides a new empirical method for identifying spurious eigenvalues and making model selection. Again, both of these are important topics for the field and for the application of numerical methods in applied settings. 

4. The numerical examples provided in Figs. 1-3 are helpful for understanding the theory developed, and provide support for the developed claims. 



Weaknesses:
1. Klus et al., 2016 and Korda and Mezic, 2018, as examples, proved the convergence of EDMD to the true Koopman operator, when $M \rightarrow \infty$, where $M$ is the number of data points. This work seems to be missing from the paper, as does discussion surrounding how the paper differs from that work. I assume the primary difference is that this paper's results are not in the asymptotic limit (although, they are in the sense that the bounds for RRR, for example, in the Gaussian case converge as $1/\sqrt{n}$, which approaches $0$ as $n\rightarrow \infty$). Additionally, the results for PCR obtained by this paper would suggest that in the asymptotic limit EDMD does not converge, since it has a bias. Discussion on how this is reconciled with the work of Klus et al., 2016 and Korda and Mezic, 2018, is necessary. 

2. Fig. 3 was confusing. Was the best estimator found on the test data set, and then the red line in Fig. 3 the result of applying it to the validation data? A secondary panel in that figure describing what was being done would be helpful. 

MINOR COMMENTS: 

1. It was unclear to me how $| \lambda_i - \mu_{j(i)} | \leq || (A_\pi - \lambda_i I)^{-1}||^{-1}$ leads to observing that $||(A_\pi  - S\hat{G})\hat{\psi}_i|| \leq \mathcal{E}(\hat{G})\eta(\hat{\psi})$ (lines 154-155). Adding a little more detail/comment on this would be helpful.

2. ""left hand side"" (line 156) should be ""right hand side"" no?

3. The connection between DMD and KMD should be discussed (lines 22-25) (Rowley et al., 2009). 

4. The original EDMD paper (Williams et al., 2015) should be cited when discussing EDMD for the first time (line 31). 

5. Very minor but both ""non-linear"" and ""nonlinear"" are written. 

Limitations:
The authors did a good job being clear that their work was limited to self-adjoint operators. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the approximation and learning of the Koopman operator. Koopman operator is helpful in modeling a broad class of Markovian dynamical systems. In this paper, two types of approximation strategies are studied, namely, ""Principal Component Regression (PCR)"" and ""Reduced Rank Regression (RRR)."" Both types are based on general reproducing kernel Hilbert spaces. With the assumptions that the population covariance operator bounds the population cross covariance operator, the RKHS feature map being L-infinity, and the eigenvalues of the population covariance operator decay as O(i^(-1/beta)) with 0<beta<=1, the operator norm error, the eigenvalue estimation error, and the eigenfunction approximation error are bounded.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper's originality is high, because of the new error estimation provided. This paper represents solid research results with high quality. The writing is clear and easy to follow. The paper's significance is guaranteed by the broad scope of applications of the Markovian dynamical systems, including Langevin dynamics.

Weaknesses:
We do not have significant concerns about this paper. No weakness is identified.

Limitations:
No limitation issues were found.

Rating:
9

Confidence:
3

REVIEW 
Summary:
The authors provide bounds for the spectral decomposition of the estimated Koopman operators. The bounds are given for self-adjoint time-reversible operators in terms of two new metrics. Compared to estimation guarantees given in terms of the Hilbert Space distance norm, the proposed bounds require less restrictive assumptions. The theoretical results are specialized for two existing estimation algorithms.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Estimating the eigenvalue of the Koopaman operator is a widespread problem. And the work seems to extend to general self-adjoint operators. The proposed new evaluation metrics are interesting, especially if they produce theoretical bounds that hold under less restrictive assumptions.

Weaknesses:
It is unclear how the bounds depend on the sample size and why it is interesting to specialize them for existing estimators. The experiments show the performance of two existing estimators but have no straightforward link with the theoretical part of the paper. It
would be more interesting to plot, for a given estimator, the wideness of the proposed bounds versus the (a posteriori) empirical estimation error.

After assuming that the process is time-homogeneous and stationary, the learning task looks similar to standard non-parametric regression. The authors should specify what are the challenging aspects of the dynamical setup. Otherwise, the contribution of the paper is unclear. The core part of this work seems to be applying classical spectral bounds to a finite-dimensional approximation of HS operators. If the novelty is to use new metrics, the paper should focus more on explaining why these new metrics are better than the HS norm.

The paper's conclusion is somehow expected. Direct learning of low-rank representations is better than projecting the data and solving an unconstrained problem. The latter option may have computational advantages. But the authors do not comment on it.


Limitations:
The authors say that restricting the analysis to self-adjoint operators is the main limitation of their work. This is mentioned in the very last lines of the paper. It would be better to elaborate on this limitation in the introduction, where the self-adjoint assumption is made. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper analyses two approximation techniques for self-adjoint compact Koopman operators. Both estimators are constructed based on regularized least-squares with a certain rank condition in mind. The Principal Component regression (PCR) performs unconstrained Tikhonov   estimation of data projected on a low-dimensional subspace. The Reduced Rank Regression (RRR) minimizes the Tikhonov objective function subject to a rank constraint on the optimizer. Both estimators admit closed form solutions. The paper study spectral properties of these estimators, specifically deviation of eigenvalues and eigenvectors. The novelty of this study resides on the error measures: operator norm error of the Koopman operator estimation (instead of Hilbert-Schmidt) and the ""metric distortion"" that compares the two Hilbert space, the RKHS that approximates the process, and the ambient L^2 Hilbert space where the Koopman operator is initially defined. 
The conclusion of this study is that both estimators have a similar variance, but the PCR may have a potentially larger bias, particularly for badly chosen kernels.
 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The authors employ existing state-of-the-art bounds in spectral theory of compact operators. The study sheds light on the phenomenon of ""spurious eigenvalues"". The asymptotic rates of convergence are shown to be optimal. 
Overall it seems a solid paper.

Weaknesses:
The rates of convergence and error bounds are tight, but only asymptotically. Since both PCR and RRR have similar asymptotic rates (for variance), the constants become important.  A more careful analysis of the constants would strengthen the paper. However it is understandable that such a study might be analytically too complex.

The authors mention that results are limited to self-adjoint Koopman operators. This is true, however, compact operators admit a SVD factorization with similar spectral properties (control of singular values) as self-adjoint operators. At a cursory reading, the results obtained in this study seem extendable to non-self-adjoint compact Koopman operators.  


Limitations:
Nothing to be reported here.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proves sharp upper bounds for eigenvalue estimation of a Koopman operator for a time-homogeneous Markovian dynamical system using either reduced rank regression or principal component regression. The bounds include operator norm error and metric distortion. These results are illustrated on simple models and some discussion of a higher dimensional molecular example is included. The error estimate also yields a design principle for kernels, based on spectral bias. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is clearly written and articulates the both the theoretical results and their consequences on practical examples in a lucid manner. The results on metric distortion are novel for this problem. The paper synthesizes a number of existing arguments in a compelling way to generate a clear estimate on the spectral learning rate. 

Weaknesses:
My impression is that the argument in Sec. 5 is not particularly new in the case that the HS norm error is used. But it is not very clear why to perfer the operator norm error. 

Limitations:
Yes, the conclusion captures limitations well. Negative impacts are not very relevant to this paper.

Rating:
7

Confidence:
4

";1
uY4rqdLls9;"REVIEW 
Summary:
The authors introduce a ""dual"" control variate for reducing gradient variance in black-box variational inference in the context of models that admit data subsampling (i.e. exhibit the required conditional independence). The dual control variant is joint in that it simultaneously addresses the two sources of Monte Carlo variance in ELBO approximations: that due to latent variable sampling and that due to data subsampling. The basic idea, in essence, is to leverage linear or quadratic ELBO approximations (which admit closed form evaluation for e.g. gaussian mean field variational families) in conjunction with a running average of gradient estimates for each data point using the variational parameters from the last iteration in which each data point was accessed. Experiments demonstrate that the proposed method can substantially reduce gradient variance (in particular that due to data subsampling, which is often dominant), thus yielding better ELBO optimization (both w.r.t. wallclock time and final ELBOs obtained).

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The main strengths of this submission include the following:
- it addresses a general problem of relatively wide interest in the NeurIPS community (namely, how best to do black-box variational inference) 
- it addresses a particular component of that problem that is often somewhat overlooked compared to other aspects (namely how best to do optimization)
- the experiments are pretty convincing in establishing the efficacy of the method
- the suggested method is technically sound and would appear to be pretty simple to implement
- the discussion in Sec 5 and the variance analysis (appendix B) help the reader conceptually place the proposed control variate alongside other alternatives

Weaknesses:
The main weaknesses of this submission, as I see them, are the following:
- the notation is a bit confusing in some places
- some of the limitations of the method and/or extensions to more general problem settings are either not discussed or are insufficiently discussed

Let me expand on these points:

While the notation for this kind of paper will necessarily be somewhat clunky given the various sources of sampling variability that have to be carefully tracked, I think some improvements are possible. In particular I find the choice of M for the running gradient particularly suboptimal. Since capital N is a positive integer and little n and little m are used to index integers, one might expect that M is also a positive integer. I suggest that M be replaced by something like G(bold $w$) to avoid this confusion and to emphasize that G depends on the ""table"" of $w$s.

The authors consider a generic but still somewhat limited problem specification, in particular they do not consider local latent variables, model learning, or amortization. More discussion of these points would probably be of considerable interest to the reader (for more discussion see below). In addition one of the weaknesses of this method is the potentially large memory requirements, which are O(Nd) where d is the latent dimension. This needs to be *very clearly emphasized*.



Limitations:
As discussed in the weaknesses section, I believe some of the limitations of the method (e.g. w.r.t.~memory requirements and likely trouble with amortization) should be discussed and/or better emphasized.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a new control variate for black-box variational inference. In particular, the proposed ""dual"" control variate attempts to reduce the subsampling noise and Monte Carlo noise at the same time. For this, the paper utilizes an incremental gradient-like scheme. The performance of the new control variate is empirically verified on Bayesian inference tasks with large datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* While control variates have been an active area of research for BBVI, reducing the subsampling noise has certainly been a problem that hasn't been addressed. In fact, the paper shows that conventional control variate solutions do not solve this problem *at all*, despite the fact that subsampling is a major source of variance. 
* The paper motivates the latter point by empirically computing the subsampling noise. Overall, the paper conveys the motivations for the proposed control variate very clearly.
* The proposed control variate based on incremental gradients seems fairly simple to implement, but not trivial. Thus, the proposed strategy has clear technical contribution.
* Empirical evaluations are thorough and adequate to show the superiority of the proposed control variate.

Weaknesses:
* Given that the paper builds on top of incremental gradient methods, which, as typical of the optimization community, comes with a heap of theoretical tools for rigorous analysis. Given this fact, it would have been amazing if the paper also provided some rigorous analysis of the proposed strategy.
* There seems to be some room for improvement in terms of paper organization and notation. The notation can be quite confusing at times, for example, lots of things happen behind innocent-looking superscripts. More on this below.

Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents a method for variance reduction in stochastic gradient estimation in doubly stochastic variational inference 
where there exists two sources of variance: (i) Monte Carlo noise when sampling from the variational distribution and (ii) gradient 
variance due to the minibatch sampling. The authors consider reparametrizable Gaussian variational inference and introduce a control variate that tries to reduce simultaneously both sources of variance. This control variate combines previous ideas, such as a second order Taylor expansion of the function as in [19], and it seems to reduce the variance in the presented experiments.      
  


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is very clearly written and all derivations appear to be correct. It contains also intuitive discussions why the proposed ""coupled"" control variate can be useful as opposed to other control variates that deal separately with each source of variance. 

Certainly variance reduction is a very important topic for stochastic gradient estimation in variational inference, and the paper proposes a potentially useful method.   

The experiments provide many details including running times.  

Weaknesses:
I was not so impressed by the experimental results for two reasons. Firstly the models used are quite small and it would be useful to include e.g. a big neural network. Secondly, I am not sure if the comparison is done in a fair way for methods such the ""naive"" method.  This is because the ""dual method is more expensive and requires more gradient evaluations, such as the ones for the numerical approximation of the Hessian-vector products. Given that computations are dominated by the number of gradient evaluations, a fair comparison should try to match this number across different estimators. For example,  for the ""naive"" estimator someone could increase the minibatch size so that the number of gradient evaluations matches the one of the ""dual"" method.             

Limitations:
The limitations are explained above regarding the current experimental comparison.  

Rating:
5

Confidence:
4

REVIEW 
Summary:
Existing stochastic methods for black-box variational inference only attempt to reduce the noise either due to data subsampling or Monte-Carlo sampling of the expectation. This paper proposed a new ""dual control variate"", which addresses both types of noise at the same time. In experiments, the proposed control variate is shown to perform favorably to existing baselines. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1) The paper disentangles the effects of noise through the data and noise from Monte-Carlo estimation of the expectation in the design of the new dual control variate, which addresses both at the same time. 
2) The proposed control variate is shown to greatly improve performance on the considered examples, at seemingly minimal overhead. If the x-axis in the plots would have been wall clock time, this would be clearer to see. 
 

Weaknesses:
The main weakness of the paper is in my opinion that the experimental evaluation is a bit lacking: 
a) larger experiments would be desirable, see questions;  
b) the plots were hard to read (difficult to distinguish the red and green curves), and could be wrt. wall clock time and not iterations. 

The algorithm looks promising, and I like that it can be used in a ""black-box"" fashion with any optimizer (e.g. Adam); but I believe it is not ready yet in its current state and could require a more thorough experimental evaluation to warrant a clear acceptance. 

Limitations:
All limitations are addressed.

Rating:
5

Confidence:
1

REVIEW 
Summary:
The paper addresses the drawback of the black-box variational inference framework for Bayesian posterior inference by proposing dual control variate that is capable of jointly reducing the variances from both data subsampling and Monte Carlo sampling. The experimental evaluations on various datasets demonstrates reduced variance and improved optimization.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper is fairly written well. The background of the black-box variational inference (BBVI) is nicely described. The doubly-stochastic optimization problem in BBVI's gradient estimation is clearly explained involving two sources of randomness - Monte Carlo sampling from the variational posterior and data subsampling from the full dataset. 

The proposed dual control variate that jointly controls Monte Carlo and subsampling noise in BBVI to create approximations of the target for each datum where the Monte Carlo noise can be integrated exactly, addressing both forms of noise and interactions between them.

Experimental evaluation and visualization of dual estimator. 



Weaknesses:
- Although the idea is novel but its usefulness/impact could have explained well.

- The approximation function for the gradient estimators g_cv and g_dual could have been clarified.

- It is unclear how the noise in Monte Carlo sampling influences the noise in data sampling. Is there a way to measure it?

- What is the role of Beta in eqn(14 - 15). Is it experimentally evaluated?



Limitations:
There is no issue with potential negative societal impact. However, the article does not address limitations (if any).

Rating:
6

Confidence:
3

";0
swNtr6vGqg;"REVIEW 
Summary:
The focal point of this paper is very precise, namely least-squares linear regression under data which need not be independent. Regardless of linearity, when the model is properly specified (realizability, i.e., the expected squared error minimizer is included in the model), martingale-based arguments are well-established in the literature. It is this realizability assumption that the authors remove. They conduct a blocking-based argument; this breaks up the data into blocks which are essentially independent, but takes a hit because the effective sample size is reduced. Their blocking-based argument is done in a careful way, such that the resulting ""noise level"" factor (a variance-like quantity) that appears in excess risk bounds is not excessively inflated by this reduction.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is extremely well-written, with notation and exposition all crystal-clear. The main content is all quite technical in nature, but the key ideas are explained in an intuitive fashion, with appropriate references to the relevant literature upon which this work stands. The paper is centered around the main technical result (Thm 3.1), and is organized to give relevant background to understand the technical context in which this result stands, and to describe the essential points of their proof. In my opinion, the balance between informal and formal content is excellent.

The main result of this work is Theorem 3.1. While there are several technical ""burn-in"" conditions (the sample cannot be too small, relative to the degree of dependence and noise level, etc.), the core result (3.2) is clear, appealing, and to the best of my knowledge fills a valid gap in the theoretical literature for linear least-squares with dependent data.

Weaknesses:
Obviously this is a technical paper with a very particular problem of interest, and thus the overall ""impact"" on the field of machine learning is limited, but the solution provided by the authors to this problem is presented clearly, and the main claims are to the best of my understanding solid.

The only point I had trouble with in terms of clarity was the notion of ""instance-specific"" and ""instance-optimal"" performance guarantees. I know the authors try to spell this out in the first paragraphs of 3.1, but if space allows, I think a more explicit explanation of the ""global"" complexity in previous works would make the ""local"" complexity here a lot more clear.

A couple other small points:
- Since there is some space left, I felt that it would be nice to give $\\widehat{M}$ a definition analogous to $M\_{\\star}$ in (2.1), instead of just giving the form in (2.4) and saying it is the OLS solution. I know this is simple to verify, but if space allows it, such an explicit expression makes it more friendly to a crowd familiar with notions of ERM.
- The last sentence of paragraph 2 in section 1 is repeated.

Limitations:
The technical assumptions are all given explicitly, with plentiful references to the existing literature, so I feel the limitations of this work are quite clear.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper deals with linear regression with dependent ($\beta$-mixing) data. It provides an upper bound of the OLS error in terms of the sample size and the effective dimension of the covariate matrix. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper studies the linear regression with dependent data. The main idea is to decompose the data into blocks, and apply the concentration inequalities. The authors also provide bounds on the burn-in period. The overall results are new, though the idea does not seem to be novel. The presentation of the paper is clear, and I mostly enjoyed reading the paper.

Weaknesses:
The main weakness of this paper is that the approach of combining blocking with concentration is quite traditional. As pointed out by the authors themselves, this idea was carried out in [10] (almost 30 years ago), with subsequent development by Massart, Wu...etc. The main result (Theorem 3.1) is more or less expected in the high dimensional statistics. Moreover, there is no (empirical) experiment for illustration.

Limitations:
NA

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper explores the impact of noise level in linear regression for dependent data by blocking technique, which can accommodate a broad type of dependent structures.  Theoretical justification of the non-asymptotic guarantee and excess risk bound  are provided, imposing any realizable assumptions on the noise.  The paper's insights and conclusions are likely to influence and inspire further research in the field.


Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The authors propose a novel perspective by combining  $\beta$-mixing assumption and blocking technique, leading to a new approach for addressing the dependent data. This innovative combination offers unique insights and contributes to advancing the field in handling such data structures. This paper demonstrates a solid theoretical foundation, characterized by sound reasoning and logical coherence. The non-asymptotic results are precisely analyzed, and potential limitations are appropriately addressed.


Weaknesses:
1. Inadequate comparison with prior work: The paper could benefit from a more thorough comparison with existing literature. Providing a detailed analysis and critique of related works would help situate the proposed approach in the context of prior research. Identifying the limitations of previous methods and explicitly explaining how the proposed approach addresses those limitations would strengthen the argument for the paper's contribution.
2. Lack of clarity in methodology: The paper could benefit from providing more detailed and explicit explanations of the research introduction and methodology. Some sections may be unclear or lack sufficient technical details, making it difficult for readers to understand the implementation nuances. Adding supplementary information, such as mathematical formulations, pseudo-code, or algorithmic details, would greatly improve the clarity of the work.
3. Lack of experimental validation: There conducted no numerical results in the manuscript, which are limited in scope and fail to provide a comprehensive evaluation of the proposed approach. To strengthen the research, the authors should consider adding some numerical experiments  to demonstrate the effectiveness and generalization of their method compared to existing methods. Additionally, providing a comparative analysis with existing methods would further highlight the strengths and weaknesses of the proposed method.

Limitations:
No other major concerns than the ones listed in the weaknesses and questions.
 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies the risk bounds of OLS for linear regression with dependent data. In particular, the label noise is allowed to be non-martingale. It shows that, after a burn-in phase, OLS with dependent data archives a bound of the same order as if the data is iid, provided that the failure probability is moderately small. The proved bound is particularly interesting because it suggests that the leading error does not explicitly depend on the mixing time. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
+ Excellent presentation. 

+  Sharp risk bounds are well-developed for OLS for linear regression with iid data. However, when the data is dependent, the risk achieved by OLS is less clear. Surprisingly, this work proves that, even when the data is dependent, the risk of OLS still recovers that predicted by the central limit theorem and does not rely on the mixing time, given that the sample size exceeds a burn-in requirement and that the failure probability is moderately small. 

+ The proof is decomposed into two neat parts, the first part controls a dependent random walk with the blocking technique and the other part controls the lower tail of the empirical covariance. The proof demonstrates the new ingredients of this work that allows obtaining the improved bound. 

+ Prior works are well discussed. I especially appreciate the comparison with [23], which clarifies that, in the worst case, the bound will depend on the mixing time, but in average cases, the bound does not explicitly depend on the mixing time. 





Weaknesses:
Please see the questions below. 

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper gives finite-sample bounds on the excess risk of ordinary least squares regression in the non-realizable case with dependent ($\beta$-mixing) data. The result asymptotically matches the predictions of the central limit theorem. The dependence on the mixing behaviour of the process is relegated to terms of smaller order in $1/{n}$, where $n$ is the length of the observed sample path, so that, apart from absolute constants, the bound asymptotically coincides with those for independent data.

The technical proof decomposes the excess risk as the norm of the product of a weighted random walk and the inverse covariance matrix (prefiltered with the true covariance). These two factors are bounded separately, in both cases using the blocking technique, which has become a standard method when dealing with $\beta$-mixing processes. How it is avoided, that the mixing times enter the  asymptotically dominant term of the bound, is already explained in the introduction by the example of Bernstein's inequality. The technical details are a major achievement (I did not have time to verify all of it) and relegated to the appendix. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper addresses an important and obvious problem and offers a largely satisfactory solution.

The multiplicative dependence on the mixing times is a major problem which besets many bounds for dependent data. It is a major accomplishment to free the dominant term of the bound from this dependence. The illustrative explanation in section 1.1 (lines 44-57) is very nice.

I did not find any faults, but because of time constraints I could not verify all the material in the appendix. Otherwise I would have rated the soundness with 4 rather than 3.


Weaknesses:
The statement of Theorem 3.1 is somewhat opaque because of the choice of the blocking partition. Presumably the cardinalities of the partition members have to be different to accommodate the non-stationarity of the process, as specified in eq (3.4). 
The theorem would be more transparent if first stated for stationary processes using a homogeneous partition. The more general version could be stated in the supplement.

The most critical ""burn-in"" condition seems to be the second condition in 3.3, because of its dependence on $\sigma^2$ and $\delta$ and in particular if $s=4$. How shall we interpret the slow-down of the ""burn-in"" as $\sigma^2$ becomes small? It seems to me that this condition would merit a more detailed discussion.   

Limitations:
The limitations seem to be adequately addressed.

Rating:
7

Confidence:
3

";1
KexMPvrFgJ;"REVIEW 
Summary:
This paper explained and analysed the issue of unsynchronous RGB and depth measurement under the UAV city modelling scenario, which makes it challenge to incorporate depth supervision for NeRF optimisation under such scenerio. The authors proposed a novel solution to it by modelling the continuous RGB camera trajectory as an implicit time-pose function. Under the prior knowledge that both RGB and depth are generated on the same trajectory, poses of depth images can be queried from the time-pose function. Based on this time-pose function, the authors further proposed a 3-stage optimisation pipeline to train the NeRF model with depth supervision. Qualitative and quantitative results show the proposed method achieves better results compared with previous methods that use RGB inputs only.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. This paper studied a very interesting and important problem of synchronising RGB and depth measurements, especially under large scale UAV scenerios, which posed challenges to incorporating depth supervision for NeRF optimisation under such scenerios. 

2. The authors proposed a novel solution by modelling the continuous trajectory as a time-pose function, and designed a 3-stage optimisation pipeline to leverage the synchronised RGB and depth meaurement for NeRF training.

3. Experimental results showed the effectiveness of proposed methods compared to previous RGB-only methods. Ablation studies also show  naively using unsynchronised depth image could hinder the performance. 

4. The paper is very well written and easy to follow. The storyline and motivation are very clear.

Weaknesses:
1. Although the experimental results in Tab. 1 and 5 showed the proposed method achieves better results than Mega-NeRF and NeRF-W, it's not compeltely fair as the baseline methods only take in RGB images. The authors need to show that the problem could not be easily solved by trivial efforts such as jointly optimising the poses of depth images. For example, in tab. 5 the author showed the results of Mega-NeRF with depth but the joint optimisation was switched off. Also in line 104, the authors mentioned that soft synchronization cannot fully address the misalignment issue, but it would be good to also show how good can all the methods perform with this simple alignment. 

2. Most of the experiments and results are shown in synthetic dataset. Only Fig. 1 and the supplementary video showed results from real-world scenes. It would be good to show more results on real world sequences.

3. It would be good to show some run-time comparison and analysis.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a way to train NeRF with asynchronous RGBD videos. Specifically, three technical contributions have been made:
1. New problem formulation for NeRF training from async RGBD video.
2. Propose a time-pose function to use async RGB and Depth stream, resulting in better pose estimation and NeRF training.
3. Propose a new synthetic dataset for this task.

---
**After rebuttal**: I have read authors' rebuttal and it addresses my concerns.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow.
2. It is novel to formulate the pose estimation issue of an RGBD stream in a time-pose function, which constrains the challenging issue better. Since ground truth poses for RGB stream is known, this paper essentially proposes a novel way to use an async depth stream to 
    1. improve NeRF quality;
    2. refine RGB poses; 
    3. optimise poses for depth images from interpolated RGB poses.
3. Although the method requires poses for the RGB video, which is a strong assumption for un-posed NeRF training, the formulation is still novel and smart to me.

Weaknesses:
In real datasets, I think there should be a self-motion distortion caused by the motion of the mounted drone. I am wondering how would this LiDAR depth distortion affect the performance?

Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose a method to reconstruct aerial scenes with Neural Radiance Fields that are supervised with RGB images and asynchronous depth images. To address the asynchronicity they propose a novel time-pose function that provides a prior to optimise the poses of the depth images. To validate their method a new synthetic dataset is introduced. They outperform one relevant baseline and show that asynchronous depth captures are an issue worth considering in NeRF reconstructions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* S1. Relevancy. Depth supervision is an important method to improve 3D NeRF reconstruction and asynchronous depth has not been considered before. The Problem formulation is novel and interesting with significance in the field of UAV reconstruction.
* S2. Method Idea. The proposed time-pose function is an original approach to exploit the prior that depth and rgb captures lie on the same trajectory and it is shown that jointly optimising the NeRF reconstruction and time-pose function can significantly improve RGB and Depth reconstruction.
* S3. Presentation. The paper is written in a cohesive manner with a clear structure.

Weaknesses:
* W1. Missing baselines for pose optimisation. The authors ablate design decisions for their proposed time-pose function but do not provide baselines for pose optimisation, which is, in this reviewers opinion, their main contribution. A qualitative comparison to methods like BARF (https://github.com/chenhsuanlin/bundle-adjusting-NeRF) or other state-of-the-art methods (https://nope-nerf.active.vision/, https://prunetruong.com/sparf.github.io/) would support the decision to use the time-pose function.
* W2. Questionable assumptions for poses. As far as this reviewer understands the poses for the depth are assumed unknown and only initialised by the time-pose function that was trained on the rgb capture timesteps. This reviewer questions this assumption in general, as in both the synthetic and real-world setting poses for the depth can either be obtained from the simulation or from GPS measurements. A quantitative comparison between GT/GPS poses, BARF optimised poses and the time-pose function would strongly support the authors decision to use an implicit function to represent the drones trajectory. 
* W3. Missing Implementation details for reproducibility. Some important implementation details are missing from both the paper and supplementary, to be precise:
   * The number of images & depth images in the generated and real datasets.
   * Is the time-pose function using positional encoding / fourier features ? This is not clear. 
   * No training parameters are given for the proposed method and baselines. 
* W4. Missing Discussion of related work. Related work in the field of representing signals with MLPs is not discussed (e.g. SIREN) and not considered in the design of the time-pose function.

Limitations:
The authors do not discuss limitations of their method in the paper but include a very small discussion in the supplementary. Overall limitations cannot really be discussed since there is no objective baseline to compare the method to. For broader impact only potential military uses are mentioned, whereas the method also has implications for surveillance. Environmental impacts of large-scale neural network training are also not mentioned.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to include depth supervision in NeRF in the UAV city modeling scenario. The key problem is that the images and depth maps are asynchronous. This paper exploits a prior that RGB-D frames are sampled from the same physical trajectory. It fits a time-pose function to the available RGB cameras and computes depth map cameras by this function and a pre-calibrated pose transform between sensors. Then it trains a NeRF with RGB loss and optimizes it further with RGB-D supervision. A new synthetic dataset is proposed for evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper tackles a new problem in NeRF modeling on UAV datasets. it defines a scenario where RGB and depth frames are asynchronous. 

2. Instead of directly adding new parameters to predict depth camera poses together with the NeRF optimization, it identifies a prior of the relation of RGB poses and depth poses.

3. It designs a network with 1D hash encoding to fit a time-pose function for RGB cameras.

4. It generates a new synthetic dataset to evaluate the proposed method.

5. It designs a new speed loss in pose learning.

Weaknesses:
1. A key contribution of this method is the learned time-pose function. It actually can be seen as an interpolation function to interpolate the RGB poses on depth timestamps. Since camera interpolation is a common practice in 3D software such as Blender, I am curious about whether other simple interpolation methods such as linear interpolation or the interpolation in Blender could get worse or better accuracy.

2. As stated above, on easier datasets, the simple interpolation methods may still get good results. It will be better to construct more challenging datasets to demonstrate the effectiveness of the proposed time-pose function. This situation can be considered when generating synthetic datasets.

3. Directly adding extra parameters to estimate depth poses are also an option as stated in the paper. What is the accuracy like if we directly use this method? It would be not surprising if it gets worse results because the poses are not well initialized.

4. The prior that ""RGB-D frames are actually sampled from the same physical trajectory"" can be explained more in the main paper such as in the Introduction section. This can make us understand clearly that there is a fixed and known pose transformation between the depth and image sensors so we can estimate the depth poses by interpolating image poses. 

Limitations:
The authors addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a pipeline to build depth-supervised neural radiance fields (NeRF) using asynchronous RGB-D sequences. Since the task is novel, the paper also contributes a synthetic dataset and demonstrates that it outperforms certain baselines in the experiments.


Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
## Originality
The task is novel and practical for lots of real-world settings.


Weaknesses:
## Quality
- The most obvious baseline is missing. What if authors simply initialize the depth frames with their (misaligned) poses from the sensor and perform BARF [1] style joint optimization on the depth frames’ poses (note that depth frames can have different poses from the RGB images after optimization)?


- Since the above baseline is missing, it’s hard to understand why the time-pose network is necessary.

- In my opinion, the baselines authors compared against are in the wrong direction. NeRF-W and Mega-NeRF are not solving the problem of inaccurate camera poses. Authors should look into works that try to solve pose estimation and neural radiance fields simultaneously such as BARF, NeRF–, and iNeRF [1, 2, 3].

- Line 156-157’s motivation for using Quaternion since other rotation representations are not continuous is weird. Quaternion is also not continuous and is known to not be the best representation for rotation regression. See this paper [4]


### References
- [1] BARF: Bundle-Adjusting Neural Radiance Fields, Lin et al.
- [2] NeRF--: Neural Radiance Fields Without Known Camera Parameters, Wang et al.
- [3] INeRF: Inverting Neural Radiance Fields for Pose Estimation, Yen-Chen et al.
- [4] On the Continuity of Rotation Representations in Neural Networks, Zhou et al.


Limitations:
Yes

Rating:
3

Confidence:
4

";0
qHzEFxtheD;"REVIEW 
Summary:
The paper studies the sparse dictionary learning and k-means clustering problems, using tools from sketching. Various results are obtained under different settings and assumptions.

The first part of the paper considers lower bounds in the streaming setting. Here, the main technical result, which is a lower bound for k-means, uses a reduction from the communication complexity of the multiparty set-intersection problem. This is a nice set of results that translate prior ideas to k-means clustering problems. A new upper bound is also established under certain input sensitivity assumptions in the random order model.

The second part of the paper gives approximation schemes for both problems. The key technical ingredient is the use of dimensionality reduction. Here, the use of dimensionality reduction follows essentially from prior work on projective clustering.

The final part of the paper considers space complexity in the turnstile model. At the high level, sketching tools are used to discretize the appropriate matrix optimization problem, followed by brute force.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper studies two fundamental important problems. The work introduces several ideas from sketching in this setting.

The authors appear to be very well versed in the related literature.

The paper is well-written and the presentation is quite accessible.


Weaknesses:
In the statements of several of the Theorems, the running time is not given (either at all, or not with enough precision). This makes me think that the algorithms are probably not very practical.

The section on the turnstile model is labeled as ""space complexity"". I find this confusing. Your algorithm has some space complexity and some time complexity, and it would be best to state both clearly.


Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This theoretical paper discusses algorithms and lower bounds regarding time and space complexity for two interesting machine learning problems:

1) Euclidean k-means clustering

2) Sparse dictionary learning


I briefly summarize the results based on the order that they are presented in the main paper (which does not fully agree with the order that is presented in the introduction):

1. Space complexity **lower bounds** for approximate k-means in the **(a)** turnstile streaming, **(b)** row-arrival streaming

2. Upper bound for a special case of k-means, where the so-called sensitivities are bounded, which goes beyond the aforementioned lower bounds.

3. PTAS for dictionary learning and k-means based on random dimension reduction.

4. Space complexity **upper bounds** for the two problems in the turnstile streaming model.

All the results are supported with detailed proofs. Admittedly I only briefly checked very few of the proofs due to the limited time, but from the general impression of the paper I expect them to be robust.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
The major strengths that I can highlight are the following:

1) The problems that are being studied are related to important machine learning problems that have drawn a lot of attention in the past

2) The results that are presented reveal many interesting insights for these two problems, from an algorithmic perspective, and the algorithms communities can benefit from such a detailed analysis.

3) Many results are presented and heavily supported with theoretical analysis (this is mostly a strength, but the amount of work makes it hard to thoroughly review in the limited time that is available)

4) It is evident that there has been a lot of work preparing the paper and it seems that the majority the main claims are robust.

Weaknesses:
The major weakness of the paper that I can mention, unfortunately, is the way that it is written...!

1) There is no clear-cut summary of the results in the introduction. There are many interesting results and improvements, but it takes significant time to identify them. E.g., the order in which the results are presented in the introduction does not agree with the order that the results are presented in the sections thereafter. The first mention of contributions is PTAS, but this only appears in Section 3. Section 2, which precedes PTAS, is about space complexity lower bounds. I spent a lot of time trying to locate results and connect them with each other, which I could have spent in verifying proofs.

2) There are many results in the 20 pages of additional content seem to be new, non-trivial, and seem to be crucial parts of the paper, e.g. Algorithms 1 and 2, the polysolver, .... They should be part of the main paper, it is not ideal that they remain hidden in the Appendix. 

3) There are few things that I think need to be clarified / missing definitions, See ""Questions"".

**Note**: Items 1. and 2. did not affect my score.


Limitations:
I cannot see any potential negative societal impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper considers the well studied $k$-means clustering problem and the $r$-sparse dictionary learning problem. The paper has multiple contributions:

(1) It presents a new approach for obtaining a PTAS for $k$-means clustering which matches the time complexity of previous algorithms for the problems. This approach generalizes to give the first PTAS for the sparse dictionary problem.

(2) Within turnstile streaming algorithms, they consider the setting where the algorithm has to output both the assignments to the clusters/dictionaries as well as the cluster centers or dictionary elements. Previous work, even in the case of the simpler $k$-means have either focused on one or the other so this is a more challenging setting. Omitting logarithmic factors, the paper provides an $O(nr/\varepsilon^2+dk/\varepsilon)$ space algorithm for the $r$-sparse dictionary learning problem with dictionaries of size $k$ and an $O(n/\varepsilon^2+dk/\varepsilon)$ space algorithm for the $k$-center problem. They also present an $O(n)$ space bounded algorithm when the points are inserted in a random order. On the lower bound side, they present an $\Omega(n/\varepsilon+dk/\varepsilon)$ space bound for $k$-means clustering as well as an $\Omega(n/\varepsilon^2)$ bound for algorithms that can estimate the cost for a fixed set of candidate centers.

Technically most interesting seems to be the former lower bound for $k$-means clustering which is via a reduction from the multi-party set disjointness problem.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
I found the paper to be quite strong. It is indeed surprising that the setting where both the assignments and the centers/dictionaries must both be output have only received limited attention. I found the ideas for the lower bound via multi-party set disjointness interesting (they are sketched well in the first 9 pages) and I als think it is nice that they design a PTAS for dictionary learning. I went through a few of the proofs in the appendices but far from everything, so I cannot vouch for correctness. However, the paper is well written and the proofs seem clear. Given that both the $k$-center problem and sparse dictionary learning is of interest to a good chunk of the NeurIPS community, I think the paper should be accepted.

Weaknesses:
It seems that some of the algorithms proposed by the paper might be implementable and it would be nice to see some experiments on their performance.

Limitations:
None as far as I can tell

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents results for the k-means and sparse dictionary problems, both of which ask to summarize an $n$ point data set in $d$ dimensions in terms of $k$ points. In the former we map each point to a center, in the latter, we are allowed sparse linear combinations of points. The paper considers two models, the streaming model (various versions of it) and the ""standard"" model where the goal is to comes up with an algrithm that is polynomial in $n. d$,  but $k, \epsilon$ are treated as constants, and the dependence on these can be arbitrary.  They present both upper and lower bounds. 

Their lower bound results are:
- An $\Omega(n/\epsilon)$ streaming lower bound for $k$ means clustering. This beats the trivial $\Omega(n)$ lower bound but it falls short of the $O(n/\epsilon^2)$ upper bound from JL. The proof is by a reduction from set disjointness, as is standard in streaming. The authors argue that their reduction is delicate and uses the structure of the hard instances from BYJKS'04. 
- An $\Omega(dk/\epsilon)$ lower bound which follows from earlier work by Woodruff.
- They give some other lower bounds for restricted models. 

They also give results on PTASES for both problems. The idea behind both is to reduce the dimensionality of the points using various kinds of sketches. The exact sketches needed for these are chosen with some care. In low dimensions, one can afford a brute-force enumeration or similarly costly algorithm (this general idea goes back to the early work on coresets). They also give some results in the turnstile streaming model, but the results seem to have some caveats about the parameters/solution space.








Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problems considered are important and well-studied in the literature, the results will be of interest to people working in the general area os sketching/streaming.

- I like the fact that they give unified results for $k$-means and the sparse dictionary problems. 

- The results seem to rely on a deep understanding of the prior work in the area, and on using exactly the right tools needed in each setting. 

Weaknesses:
- The paper has too many results, at least some of them rather partial or for rather restricted models. I have a hard time deciding what the main contribution of the paper is. No one result stood out either in terms of the statement, or in terms of new techniques. 

- Some of the results seem a touch incremental, they come from applying prior ideas in a new setting. I realize that knowing what tools are applicable is no mean feat, given the vast literature. But I could not discern too much originality. 

Limitations:
NA

Rating:
5

Confidence:
4

";1
pzc6LnUxYN;"REVIEW 
Summary:
The paper presents an interesting model (so called statemask) to identify critical states for an agent's final reward. The goal of statemask is to find the non-important time steps and randomize their actions without changing the expected total reward of the target agent. A PPO based algorithm is leveraged to formally generate the model. Several numerical examples are shown to demonstrate the merits of the proposed model. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper presents an interesting model (so called statemask) to identify critical states for an agent's final reward. The goal of statemask is to find the non-important time steps and randomize their actions without changing the expected total reward of the target agent. Several numerical examples are shown to demonstrate the merits of the proposed model. 

Weaknesses:
The paper seems claim the method is suitable for all decision making processes. However, for the type of shortest path finding problems, it is questionable that the method will be effective. In fact, the critical elements are not states, but the critical paths. So, it would be interesting to know statemask fits to what kinds of processes which is missing in the current version. 

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on providing an explanation for deep reinforcement learning agents by identifying the important time steps within an episode. The authors propose a module called StateMask, which replaces the original agent's policy with random actions in specific time steps. By preserving the overall episode returns, only non-important time steps are randomized.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
One notable strength of this paper is the intriguing concept of masking actions, which allows for identifying important time steps without altering the learned agent or its learning process.

Weaknesses:
1. The objective Eqn. 2 is problematic. The optimal solution of Eqn. 2 is $\pi=\bar{\pi}$, i.e. $\tilde{\pi}(a_t^e=0|s_t)=1$, where the integration policy $\pi$ degenerate to the target policy $\bar{\pi}$. If StateMask $\tilde{\pi}$ becomes a constant policy, it fails to identify any important time steps.

1. Even if Eqn. 2 cannot be optimized to zero but instead reduced to a small value, StateMask $\tilde{\pi}$ would tend  to predict $a_t^e=0$ in most states but $a_t^e=1$ in a few specific states. Since  $a_t^e=1$ indicates a non-important state, this method can only identify a limited number of non-important states and fails to capture important time steps. Identifying non-important states is not consistent to the major motivation of this paper. 

1. The motivation behind using the absolute error in Eqn. 2 and its surrogate objective in Section 3.2 is not clear. A more straightforward approach for regression tasks would be to minimize the squared error (MSE / L2 loss) rather than the L1 loss, as the L2 loss is differentiable everywhere. 

1. The true optimization challenge is that the optimization problem of StateMask is another reinforcement learning problem. The authors propose a PPO-like surrogate objective which maximize the return under the constrain of minimizing the difference to target policy $\bar{\pi}$. However, it seems not consistent to the major objective Eqn 2.

Limitations:
The authors should refine the objective Eqn 2. Besides, the writing skill needs to be improved.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This submission focuses on explaining which states are important to the agent’s final reward.
By utilizing a mask to learn and assess which actions are critical. When learning the mask, it focuses on the random actions without affecting the agent’s performance. They evaluate on 10 different tasks such as Pong, some scenarios in StarCraft II, and Connect 4. After learning the mask, they provide fidelity scores and show some examples like Pong and Connect 4. With this information from the mask, they utilize it to perform adversarial attacks and correct agent errors by fine-tuning. Their work outperforms among EDGE, lazy MDP, and value-max.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Significance and originality:
How ubiquitous this method can be since as they stated, it does not assume access to the agent’s value function or policy network. Hence it can be utilized by multiple methods as well as individual agents in a multi-agent environment. Figures 4 and 5 are interesting to showcase which time steps are important based on the fidelity scores. These figures showcase a one player game and a two player game showcasing the differences especially in Connect 4. Using the explanations to provide adversarial attacks and correcting agent’s sub-optimal performance is interesting. In Table 1, your implementation outperforms EDGE and others in both performance drop for adversarial attacks and performance gain in patching.

Edit: I have read their rebuttal and I will change the score from a 5 to a 6.

Weaknesses:
Evaluation:
More evaluations among other networks to see how versatile it is. Plus other ablations such as do you vary the amount of time steps for the input, like frame stacking to assess how the time step prioritization could affect among the parameter value for frame stacking.

Clarity:
In the supplementary material when reading it, the networks just said if they were CNN, LSTM, MLP, but were they DQN, A2C, and so on. This is important information to assess with your method if it can learn the different masking for them. Plus it would be interesting as another experiment to see if DQN or Double DQN focus on different time steps.

Related Works:
Related works to reference perturbation methods because in the design rationale what you are mentioning is very similar to what computer vision has done with perturbation methods to understand visual explanations with salience maps. You even use the nomenclature perturb like in the second paragraph of design rationale. For instance to include perturbation computer vision methods like RISE (Petsiuk, Vitali, Abir Das, and Kate Saenko. ""Rise: Randomized input sampling for explanation of black-box models."" arXiv preprint arXiv:1806.07421 (2018).) since it creates perturbation masks and to suggest that there has been work in computer vision using perturbations. Yes, you focus on the time steps but can mention that there has been work that wants to show which pixels are affecting classification. What you are using in your masking approach is still novel for reinforcement learning.

Limitations:
They do mention challenges with their approach such as converge issues. So they did address some limitations. Other negative societal impacts is not an issue for this submission since they are working on the opposite part where they want to understand for reinforcement learning why decisions are being made.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper aims to explain deep RL through identifying the critical states at which the action of policy significantly impacts the final reward performance. The main idea is to learn a state-mask, which is modeled as an additional policy, to determine whether to mask original action output by a random one and minimize the performance difference in the meantime. In practice, the authors adopt a trust-region trick to guarantee the monotonic decrease of performance difference and learn the objective in a PPO style. They further apply the explanation of critical states to do adversarial attack and error patch, which exhibits a better performance than baselines.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The paper is clearly written and the presentation in the evaluation part is good.
- The empirical results and the selected examples show the effectiveness of the proposed method on key state explanation.
- The generated explanation is easily compatible with the downstream tasks, e.g., adversarial attack and defense, which lays a foundation for future work.


Weaknesses:
- The paper is implicitly built upon a restrictive assumption that there are some specific single timesteps/states that contribute to the final reward significantly in every episode. However, there are other cases that it is a series of actions (may be consecutive or not) that mutually influences the total reward, which is more common in complex environments but hard to be captured by this method.

Limitations:
N/A

Rating:
6

Confidence:
3

";1
uv3ge0goPa;"REVIEW 
Summary:
This work introduces a novel and orthogonal approach by exploring the potential of using random weights network as a loss function. The authors have carefully designed the random weights network with theoretical constraints based on mathematical manifolds. To validate the proposed solutions, extensive experiments have been conducted on mainstream image restoration tasks. The results consistently demonstrate the effectiveness of the approach. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
There are several strengths here:
1. This paper presents a pioneering exploration of the potential of using random weights network as a loss function. With a clear motivation and a series of interesting experiments, this study offers valuable insights into the applicability of the proposed approach, potentially shaping the direction of the loss function community.
2. The proposed designs seamlessly integrate into existing methods, resulting in performance improvements. A thorough set of ablation studies provides strong evidence to validate these findings.
3. The paper is well-written and maintains a high level of readability, ensuring that it is easily understandable and accessible to readers.


Weaknesses:
There are several weakness here:
1. To ensure clarity, it is recommended to provide detailed information regarding the experimental settings, including the specific methodologies and procedures employed. This will allow readers to have a clear understanding of how the experiments were conducted.
2. The figures and tables in the paper lack consistent style, indicating the need for a thorough review by the author to identify and correct any errors. Additionally, it would be beneficial to address potential limitations and investigate the extent to which the random weights network can be applied to various tasks or datasets. 


Limitations:
The authors adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper seeks to explore the untapped capabilities of random weights networks as a loss function. Inspired by mathematical manifolds, the authors propose innovative and straightforward solutions for random weights networks based on rigorous mathematical properties. Extensive experimental results across various image restoration tasks validate the efficacy of these solutions, showcasing their plug-and-play nature and ability to enhance model performance while preserving the original model and data configuration as the baseline. The novelty and interest of the idea are noteworthy.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. Innovative approach: The authors propose a novel concept of utilizing a well-designed random weights network as a loss function, offering a plug-and-play solution that leads to remarkable performance improvements when integrated into existing baselines. This approach avoids the need for complex network architecture designs, making it highly appealing in the field of efficiency.
2. Theoretical foundation: The design of the random weights network is derived from rigorous mathematical manifolds, ensuring a solid theoretical basis. Furthermore, the authors have tailored the random sampling strategies to enrich the manifold representation, adding depth to the approach.
3. Comprehensive experiments: The paper provides extensive comparison experiments in both the main paper and the appendix, showcasing the advantages of the proposed flowchart. The inclusion of ablation studies and motivation analysis further strengthens the findings, ensuring convincing evidence of the method's effectiveness.


Weaknesses:
1. In all the tables, the authors have suggested to highlight the best results for a clear illustration. In addition, the more visual comparison is required to show the main body.
2. The authors have performed sufficient ablation studies. However, the corresponding experimental configuration like convolution kernel sizes needs to be detailed.
3. It would be better if the authors have presented more experimental analysis.


Limitations:
The authors adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper introduces the idea that random weight networks can be used as loss functions for training image restoration networks. The paper proposes to use Taylor’s Unfolding Network, Invertible Neural Network, Central Difference Convolution, and Zero-order Filtering as random weight networks. The analysis and ablation studies show the effects of initialization strategy, model architecture, model depth, and model numbers. Experiments on image enhancement, image denoising, and guided image super-resolution validate that the proposed loss functions improve the performance of several existing image restoration methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
+ The idea of using random weight networks as image restoration loss functions is interesting.
+ The quantitative evaluation is performed on several image restoration tasks.


Weaknesses:
- The paper has a major technical flaw. The abstract states that the proposed loss functions do not incur additional training computational cost. This is unreasonable because the gradients of these loss functions require additional computation cost. To be effective, the proposed loss functions must be used in conjunction with a pixel loss and are more complex than the pixel loss. It consumes more GPU memory and time during training. The paper should report the additional training cost or the extra training time.
- The quantitative improvements are not significant. From Table 1 to Table 9, the proposed loss functions have limited impact on the PSNR and SSIM results. For example, MPRNet is a representative denoising method, but its PSNR gain is less than 0.1 dB. In addition, the paper reports MPRNet achieves 39.24 dB on the SIDD dataset, which is far behind the PSNR result of 39.71 dB in the original paper of MPRNet. I suspect that this paper does not train MPRNet to convergence, and it is unreasonable to compare different loss functions without full convergence. As far as I know, NAFNet [a] achieves state-of-the-art 40.30 dB on the SIDD dataset. Are the proposed loss functions applicable to NAFNet?
- Lack of visual results in the main paper. As a paper on image restoration, it is unreasonable that the main paper does not contain any visual results. Moreover, the visual results in the supplementary materials have negligible differences, which suggests the proposed loss functions are ineffective.
- Lack of evaluation on more general image restoration tasks. The paper selects image enhancement, image denoising, and pan-sharpening as the image restoration tasks. Are the proposed loss functions applicable to more general image restoration tasks such as super-resolution? 

[a] Chen et al. “Simple Baselines for Image Restoration”, ECCV, 2022. 


Limitations:
The paper does not mention any limitations of the proposed method. I believe some discussions on the perceptual quality are necessary since the quantitative improvements are limited and the visual results in the supplementary materials have negligible difference.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper explores the notion of using random weight networks as a constraint during the training process for image restoration. This approach aims to encourage the network to learn more robust features and produce better results, addressing the limitations of traditional optimisation methods and deep learning-based methods. By incorporating the random weight network as a constraint, the authors validate the approach towards improving image restoration performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The authors provide sufficient theoretical insights behind the formulation of using a randomly initialised network as an auxiliary loss function during the optimisation process. 

2. The ablation studies are elaborate and cover a wide variety of initialisation configurations and examine its effect on final restoration performance.

Weaknesses:
1. Experimental Setting section is repeated 

2. The proposed approach is similar to [1, 2, 3] and without discussion on differences, the contribution of the proposed work is weak. Specifically identification of different distributions and its impact on final restoration performance. 

3. The authors should discuss the impact of utilising multiple network architectures on the overall training period as well as memory requirements. 

4. The impact of initialisation distribution should be discussed, which is missing. Furthermore in the qualitative results the authors should also provide corresponding input and ground truth images for easier evaluation. 

5. While the authors evaluated the impact of network structures by replacing the CNN with transformer architectures in ablation. Other configurations such as using transformer based restoration networks and implications of using a lightweight optimisation network aren't considered. These ablations are necessary to identify the overall implication of using different strategies during optimisation.


[1] Gallicchio, Claudio, and Simone Scardapane. ""Deep randomized neural networks."" Recent Trends in Learning From Data: Tutorials from the INNS Big Data and Deep Learning Conference (INNSBDDL2019). Springer International Publishing, 2020.

[2] Herrera, Calypso, et al. ""Optimal stopping via randomized neural networks."" arXiv preprint arXiv:2104.13669 (2021).

[3] Tarvainen, Antti, and Harri Valpola. ""Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results."" Advances in neural information processing systems 30 (2017).

Limitations:
The authors have addressed the limitations arising from space but not the limitation of their methodology, which was the original objective.

Rating:
7

Confidence:
4

";1
