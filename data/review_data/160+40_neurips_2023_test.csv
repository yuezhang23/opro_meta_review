id;text;label
tC0r8duG9z;"REVIEW 
Summary:
This paper studies the power of vanilla-SVD algorithm, algorithm without any pre-processing or post-trimming steps, in the symmetric stochastic block model and proves it recovers all clusters in the balanced case, which answers an open question in [Vu18].

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The main contribution of this work lies in its demonstration of the effectiveness of a truly vanilla SVD algorithm in recovering all clusters of the symmetric stochastic block model in the balanced case with high probability theoretically. Some noticeable distinctions from existing works include a truly vanilla SVD algorithm without any additional pre-processing steps and the ability of handling of cluster numbers $k = \omega(1)$. Also, from a technical standpoint, the authors introduce a novel ""polynomial approximation + entrywise analysis"" approach, which simplifies the analysis of eigenspace perturbation by reducing it to the analysis of a simple polynomial under perturbation thus makes the analysis more robust and requires less structure.


Weaknesses:
The presentation of this work could be improved. First, it might be better to explicitly mention in the Abstract and Section 1that the analysis is within the balanced case and not directly extended to other cases. Second, [MZ22b] also claims answering the open question in [Vu18] in the balanced case. The authors point out that the centered-SVD algorithm proposed in [MZ22b] is not truly vanilla, containing a pre-processing centering step which depends on the knowledge of $q$. But it would still be beneficial to compare the condition and bounds in [MZ22b] to have a more complete understanding. Third, adding more background information and necessary theoretical knowledge would make this work more accessible to a broader audience. Lastly, the analysis in Section 4 may be better positioned prior to Section 3.3, which already wraps up the entire analysis, or included within the supplementary material.

Others:
- In Lemma 3.1, it should be $1 - n^{-3}$ rather than $1- n^3$.

Limitations:
There seems to be no discussion of limitations or potential societal impact.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In order to understand the behavior of spectral steps in clustering problems, this paper studies the power of vanilla-SVD algorithm in the SBM. This work shows that vanilla-SVD algorithm recovers all clusters correctly in the symmetric setting. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. To theoretically understand the power of practically successful vanilla spectral algorithms, this paper studies SBM as a preliminary demonstration. Then, some proofs are presented for the proposed Theorem 1.1.
2. This work shows that vanilla algorithms is indeed a clustering algorithm in SSBM for a wide range of parameters.
3. The authors give another analysis on matrix perturbation with random noise.
4. Detailed comparisons with existing analysis for vanilla spectral algorithms in SBM are presented in this paper.

Weaknesses:
1. In real applications, it is common in practice that spectral embeddings obtained by spectral clustering algorithm need a post-processing, e.g., k-means. It is reasonable that spectral embeddings themselves are clustering-friendly. Thus, the contributions of analyzing vanilla spectral methods could be highlighted.

2. From Section 1.3, it can be observed that there is limited work on the vanilla spectral algorithm. Could the authors provide an explanation for the reasons behind analyzing the parameters in the SSBM setting in this paper?

3. From the comparisons, it is apparent that there are some issues with the related theoretical analyses, such as [AFWZ20], [EBW18], [PPV+19], etc. Since these weaknesses are evident, could the authors consider providing quantitative experimental analysis in addition to the theoretical analysis?

4. Between lines 48 and 49, '...can be view as a fixed matrix...' should be corrected to '...viewed...'.

5. The coherence in the review content in the Introduction section is insufficient. The authors seem to focus more on the mathematical advancements rather than considering the theoretical and practical applications of spectral methods in the field of machine learning, especially the related work in both theoretical understandings and practical applications.

6. The argument is verified using a specific setting in a typical model, SSBM,  to demonstrate that vanilla spectral algorithm is powerful and prectically successful. It may not be comprehensive enough.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper investigates the effectiveness of the vanilla-SVD algorithm in the stochastic block model (SBM) and demonstrates that it can accurately recover all clusters in the symmetric setting. The authors address an open question raised by Van Vu in the symmetric setting.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The vanilla algorithms employed in this study are applicable to a wide range of parameters, surpassing previous limitations that only allowed analysis on a constant number of clusters.

2. They also provide a novel analysis on  matrix perturbation with random noise. 


Weaknesses:
1. The paper lacks experimental evaluation to demonstrate the effectiveness of the proposed scheme.

2. The time and space complexity analysis are not compared with previous schemes, which could provide insights into the efficiency of the proposed approach.

3. Lots of proofs are omitted in the main paper (I understand that this is due to the space limit). Maybe the theory conferences like COLT is a better venue for this paper?

Limitations:
Overall, this paper provides some valuable contributions to the study of the vanilla-SVD algorithm in the SBM. However, addressing the weaknesses mentioned above, such as conducting experiments, comparing complexities, and improving readability, would enhance the quality and impact of the research.


Rating:
7

Confidence:
2

REVIEW 
Summary:
The manuscript mention that the paper contributes by providing a theoretical understanding of the power of vanilla spectral algorithms in clustering problems, specifically in the stochastic block model (SBM).  It also presents a novel analysis of matrix perturbation with random noise.  These contributions suggest that the document offers new insights into the application of SVD in clustering problems, particularly in the context of SBM. It focuses on the stochastic block model (SBM), a benchmark for clustering, and treats it as a form of vector clustering. The manuscript proposes and analyzes a vanilla-SVD algorithm for graph clustering and demonstrates its effectiveness in SBM.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main topic of the manuscript is the analysis of the power of Singular Value Decomposition (SVD) in clustering problems. The manuscript discusses the use of dimensionality reduction techniques, specifically PCA and SVD, to improve clustering results in high-dimensional datasets. It explains that classical clustering algorithms like K-means may perform poorly in such datasets due to the curse of dimensionality. Spectral methods like PCA and SVD have been observed to significantly enhance clustering results. The manuscript explores the reasons behind this improvement, including filtering noise from high-dimensional data. It focuses on the stochastic block model (SBM), a benchmark for clustering, and treats it as a form of vector clustering. The manuscript proposes and analyzes a vanilla-SVD algorithm for graph clustering and demonstrates its effectiveness in SBM. The authors present their results, including a clustering algorithm and an analysis of matrix perturbation with random noise. They compare their approach with existing analysis methods and highlight the advantages of their approach. The document concludes with a proof outline and technical contributions, outlining the steps taken to analyze the power of vanilla spectral algorithms for clustering.



Weaknesses:
There are many formulas that make the readability not very strong. Some experiments should be designed to evaluate the significance of the model.

Limitations:
This work should design some experiments to fully illustrate the application value of the model.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper provides rigorous, theory-based evidence that vanilla spectral algorithms (i.e., methods that run SVD on the adjacency matrix without any further processing) succeed in finding many communities in symmetric stochastic block models. In contrast to Davis-Kahan approaches, the authors adopt an analysis similar to [MZ22b], which is inspired by power iteration. The key technical novelty of the authors' method is a new way to study eigenspace perturbation by using a polynomial approximation of the operator that projects a vector onto the space of the first k eigenvectors of the adjacency matrix. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
While there are many community recovery algorithms for SBMs at this point, this paper provides an important analysis of vanilla algorithms for community recovery, showing that pre or post processing steps are unnecessary, thereby validating practical approaches. This is the key strength of the paper, in my view. 

A secondary strength, which is of significance to researchers working on random matrix theory, is the use of the polynomial approximation method for the projection operator. This interestingly allows one to circumvent usage of the Davis-Kahan theorem (which is the standard way to handle eigenspace perturbation), and I expect this technique will be useful in various other settings. 

The paper is also very well written, with clear comparisons to related work, and key contributions highlighted.

Weaknesses:
The main weakness I found was that there is little to no discussion of the condition under which Algorithm 1 recovers communities (Theorem 1.1). At face value, it seems like a similar condition as [Vu18], with a few changes, like $\sigma \sqrt{k}$ replaced by $\sqrt{kp} \log^6 n$. A few questions are:
- Is this change an artifact of the proof, or is it something more fundamental? In most situations, I'd expect the $\sqrt{\log n}$ term to dominate. 
- What are commonly considered regimes for p, q, k in which this threshold is satisfied? For instance, it seems like the regime $p, q = \Theta ( \log (n) / n)$ is not covered by the theorem. What's the minimum scaling of $p, q$ that would work here? And what is the maximum number of communities that can be tolerated? Understanding such qualitative cases would make your results much more interpretable. In a similar vein, it was unclear to me whether the condition you have here is optimal in some sense, or if it's just to prove that vanilla methods succeed in just *some* regime.

Finally, I have a question related to how your work compares to [MZ22b]. 
- It seems that your work and [MZ22b] tackle a similar parameter regime for community recovery. How do the achievability regions compare between their work and yours? In particular, does the vanilla algorithm work in almost the same parameter regime, or is the parameter regime more restricted?

Limitations:
Limitations have been adequately addressed. 

Rating:
7

Confidence:
3

";1
wv79UiY5U7;"REVIEW 
Summary:
This paper focuses on data curation for image captioning. This paper shows that mismatched image-caption pairs do harm to the captioning model. To address this problem, generative models are used. In detail, the BLIP model is used to generate captions based on images, and the Stable Diffusion model is used to create images based on captions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The data curation is an important and effective topic, which could benefit many tasks including visual synthesis, image captioning, language and visual representation, etc.
2. This paper discovers the weak point of captioning datasets, especially for the Flicker30K.
3. It is interesting to use the BLIP model and the Stable Diffusion model to create data for training.

Weaknesses:
1. There are many methods to augment text dates, e.g., adding or editing some words, using synonyms, and changing sentence structure. I think these methods are also worth evaluating.
2. From Table 2, we can see that the BLIP's performance is not significantly affected by the methods proposed in this paper (i.e., Remove, ReplaceCap, and ReplaceImg). For example, the CIDEr of COCO only slightly raises from 132.0 to 133.1.
3. Figure 5 shows that the proposed methods might make the performance worse, especially in the COCO dataset. So I am concerned about the generalization of the proposed methods.

Limitations:
None.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper studies data curation strategies for training image captioning models. Firstly, it identifies the “difficult samples” based on the captioning loss dynamically at the end of each epoch. Subsequently, it introduces three data curation strategies to modify the difficult samples: (1) removal of an image-text pair, (2) replacement of the caption and (3) replacement of the image using text-to-image generative models. The main technical innovation is the third strategy, which is carefully designed in terms of prompt engineering and fine-tuning on the image captioning datasets.

The empirical studies show that the proposed data curation strategies can enhance the performance of the baseline BLIP captioning model. The authors also conduct analysis on the data curation ratio, dynamic versus static curation strategy and the errors of images generated by the stable diffusion model.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The idea of employing text-to-image generative models to curate training data for image captioning is novel and well-motivated.

Weaknesses:
### Effectiveness of the proposal
* According to Table 2, the performance of the third data curation strategy, which is the main technical innovation of this work, is not advantageous compared to the heuristic removal and caption replacement strategies.
* According to Figure 5, all three proposed strategies are sensitive to data curation ratio. Consequently, training the captioning model multiple times is necessary to achieve satisfactory performance, which is less efficient compared to the baseline BLIP model.

### Design of the method
* Identifying the samples to modify based on training loss is questionable. A higher loss does not necessarily imply that the sample is harmful to training. Although Section 5.2 has shown that more errors are identified in images of higher loss, the experimental setup has two issues: (1) The loss is computed over the generated images rather than the real images in the original dataset. (2) The errors are categorized as targeting image generation, rather than image captioning. In other words, an image that possesses imperfect visual quality but aligns well with the caption may not necessarily be considered a noisy training sample for image captioning.

### Missing reference
* An idea similar to the “round-trip captioning evaluation” is already proposed by [1], which generates a caption from the synthesized image and measures the similarity between input text and predicted caption.

### Clarity
* In line 243, it is unclear whether the “model loss” refers to captioning loss or image generation loss.

[1] Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis.


Limitations:
The authors have discussed the limitations of this work from three aspects: (1) Lack of adaptation of the proposal to the pre-training stage. (2) Reliance on pre-trained image understanding and text-to-video generative models. (3) Increase in training time due to the usage of text-to-image generative model. Moreover, a significant limitation of this study is the absence of evidence demonstrating the superior effectiveness of the generated images compared to heuristic data curation strategies.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a data curation model for image captioning. If the loss of a particular image caption pair is high, then either remove the image-caption pair from the training set or replace the caption with a more similar caption or they generate a new image for the difficult caption. The authors demonstrate these strategies help to improve the performance of BLIP caption generation model.


Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The idea is interesting.

Paper shows some positive gains on COCO and FLickr30K.


Weaknesses:
The details of the method are not clear. How to select a replacement caption?

Why one should pick only the high-loss image-caption pairs? Loss may be high due to many other reasons.

Compared to other data augmentation methods in the literature that is also discussed in the related work, what is the novelty?

Why this is a significant finding? I am not sure if this is a significant finding. 

The method is also evaluated using a single model. 

Obtained results are not state-of-the-art.

It is not clear whether such a mechanism will contribute to any state-of-the-art methods in captioning.

Limitations:
Limitations and societal impact are discussed in the paper.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper focuses on improving image captioning by improving the quality of the existing dataset. To this end, this paper proposes three data curation methods: the removal of an image–caption sample; replacing a caption with another caption; and replacing images using a text-to-image generation model. Experimental results demonstrate that models trained with the proposed methods consistently outperform baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.The proposed method is well-motivated, that is to improve the quality of the existing dataset. This paper explores the problem of making better use of existing datasets, which is a very interesting research direction.

2.The authors conduct extensive experiments over these two datasets, where the models trained with the proposed methods outperform baseline methods consistently.

3.The paper is well-written and easy to follow.

Weaknesses:
1.From my understanding, it is risky to judge the quality of the sample based on the loss value. A sample with a large loss value may be a hard sample or a mislabeled sample, so it is risky to judge the sample quality only based on the loss value.

2.In addition to the performance of the model on the test set, the generalization ability of the model is also important. It is not clear whether the proposed method reduces the gap between the training set and the test set or improves the quality of the training set.

3.Lack of necessary theoretical analysis.

Limitations:
yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose an iterative training approach to improve image captioning models.
This approach _refreshes_ the training dataset every epoch with _higher quality_ image-text pairs (authors call it ""data curation"").
Dataset samples with very high training loss are updated -- the real image is replaced with one generated by the Stable Diffusion model.
The authors compare their approach with two baselines: one which removes high-loss samples, and one where the image is replaced by another from the training dataset itself.
Experiments are performed with the BLIP model and two captioning datasets -- COCO and Flickr30K.
Authors also perform an accompanying human study to provide directions for future work.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
This paper has numerous technical strengths:

- The proposed method is conceptually simple and easy to implement.
- The strategy of updating the training dataset with ""better"" samples is very general: it is agnostic to the model architecture and the multi-modal task at hand.
- The writing and presentation quality of the paper is excellent. It contains adequate implementation details to make this work reproducible.
- The experimental setup and ablation study is very meticulous. Tables of results contain experiments that begin with a BLIP baseline, and subsequent rows introduce one change at a time.
- The authors have conducted a human study with sensibly defined failure categories to understand how failure modes of Stable Diffusion can impact captioning performance.


Weaknesses:
Like its technical strengths, this paper also has some shortcomings.
Below I list a few salient concerns with the paper.
I look forward to hearing the authors' response, and I am happy to update my final assessment.

1. **Results do not match with the presented story:**
The main results (`Table 2`) indicate that all considered dataset curation approaches are beneficial over a BLIP baseline that doesn't train on curated data.
However, the main pitch of this paper is to use generative models like Stable Diffusion to replace images (last row),
which in fact performs marginally better or even worse than other curation techniques.
The biggest improvements are generally yielded by ""Remove"" strategy.
I recommend the authors rethink the positioning of motivation and frame it as an exploratory study --
it seems obvious to use generative models for iterative training/distillation and some works already do it for other applications,
but for this task, a practitioner is better off by simply filtering noisy samples altogether.

2. **Captioning metrics appear saturated, maybe overkill for COCO/Flickr:**
The captioning metrics on COCO and Flickr are already saturated,
e.g. decimal improvements are less meaningful for COCO in the range of 130+ CIDEr and 20+ SPICE score.
Since BLIP is already rained with large amounts of data and diverse tasks,
the proposed approach may be an overkill for the tasks considered in this paper.
I suggest the authors rethink other applications where the benefits of this strategy are more prominently observed (see Weakness 5 below).

3. **What if the caption is noisy and can't generate meaningful images?**
An image-text pair may be unaligned if the caption is uninformative,
as frequently encountered in larger web datasets like
[Conceptual Captions](https://arxiv.org/abs/2102.08981),
[YFCC](https://arxiv.org/abs/1503.01817),
[RedCaps](https://arxiv.org/abs/2111.11431), etc.
For instance, captions coming from alt-text may not have any semantic content whatsoever
(e.g. see Figure 2 in [ALIGN paper](https://arxiv.org/abs/2102.05918)) to generate meaningful images.
The proposed approach forces the generative model to create an arbitrary image and ends up adding noise to the training data.
Some selective mechanisms to replace either image or caption may be needed to scale this approach to general image captioning beyond COCO and Flickr30K.

4. **Related work needs more coverage:**
The main focus of this paper is image captioning, hence a broad coverage of prior works on image captioning is necessary.
However, this section only cites a handful of very recent modeling papers.
I suggest the authors begin the discussion with some early image captioning papers like:

  - (Vinyals et al, CVPR 2015) Show and tell: A neural image caption generator
  - (Karpathy and Li, CVPR 2015) Deep visual-semantic alignments for generating image descriptions
  - (Donahue et al, CVPR 2015) Long-term recurrent convolutional networks for visual recognition and description

5. **[Related to 1, 2] Have the authors considered applications others than image captioning?**
What if this curation strategy is used to train general visual representations?
I suggest a CLIP-style contrastive model and/or BLIP/VirTex-style generative model.
The contribution can be strengthened by broadening the scope to various downstream tasks.


Limitations:
The limitations section should be updated if any of the above-mentioned open questions are not within the scope of this paper.

Rating:
3

Confidence:
5

";0
8ox2vrQiTF;"REVIEW 
Summary:
Shows how grid modules of grid cells can emerge as solutions to a self-supervised learning framework, implemented as a recurrent neural network.
They take insights from Continuous Attractor models (velocity dependent weights), Dorrell et al (2023) representation theory (path invariance), and ideas about efficient coding (Sreenivasan & Fiete), and put them all within a single SSL framework, based on three loss functions for RNNs - maximising separation of distinct locations, path invariance and high capacity for encoding locations.   
The simulation results show modules of grid cells are formed. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The single theoretical framework for explaining various aspects of the grid code is a strength

Weaknesses:
The advance on previous work is not so clear, given that each aspect has been presented previously, with representation theory covered by Dorrell et al., nice analysis of the emergence of grid codes in RNNs in Sorscher et al., and coding efficiency in Sreenivasan and Fiete, and using the basline continuous attractor model of RNNs with velocity dependent weights for path integration.  

Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper proposes a computational framework for the emergence of grid cells in the mammalian cortex through self-supervised learning. The learning objective is formulated combining requirements of path independence for location code, error-correcting coding, efficient coding. Validity of the approach is demonstrated through numerical experiments on a recurrent neural network. Resulting cells reproduce important properties of grid cells observed in biology: cells organized into modules with common spatial frequency and orientation; modules exist for a range of frequencies; cells in a single module regularly tile the space. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The paper formulates a principled and biologically meaningful optimization problem and arrives at a representation that manifests important properties of grid cells. 

The role of each component of the objective function is studied experimentally.

Rich future directions outlined in Discussion section.


Weaknesses:
Authors do not discuss relation of their work to the literature arguing for grid cells role in predictive representation (e.g., Stachenfeld et al, 2017; Momannejad 2020). 

The biological plausibility of proposed learning procedure is also not discussed.

Limitations:
See weaknesses

Rating:
8

Confidence:
3

REVIEW 
Summary:
This work reviews some of the issues with existing models of grid cells (cells in mammalian brains that fire when the animal is located at the vertices of a hexagonal grid) and suggests a new model based on recurrent neural networks (RNNs). The model is self-supervised, eliminating the worry that the structure of the readout in a supervised task could influence the outcome. Besides leading generically to grid-like firing, the model also exhibits multiple grid scales organized in modules such that the scale is the same within each module but the phase varies.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
*Originality:*
The paper suggests a way for grid cells to emerge from a self-supervised learning (SSL) paradigm, in contrast to previous work which works mostly in a supervised regime.

*Quality:*
The paper does a good job of surveying prior work and includes a fair amount of simulations.

*Clarity:*
The presentation is generally clear.

*Significance:*
Grid cells are of tremendous interest in neuroscience and there is a considerable volume of work attempting to explain their properties and the reasons behind their existence. This paper provides a novel model for how grid cell may emerge – as a self-supervised means of keeping track of an animal's location in space – and is thus of great interest to neuroscientists.

Weaknesses:
1. The authors present this work as a significant advance over methods based on supervised learning because the latter depend on specific design choices. However, the same seems to be true in the new approach: for instance, while the separation and path-invariance loss are pretty natural, the capacity loss is counter-intuitive, as mentioned even by the authors in the Discussion. Excluding the capacity loss eliminates the multi-scale nature of the solution. Moreover, the emergence of grid cells is sensitive to the parameters used in the loss, as shown in Figure 7. It is thus not immediately obvious that the proposed method requires any less fine tuning to lead to grid cells than prior models.
2. Ideally the code used to run all the simulations would have been included with the supplementary material. It was promised for after acceptance, but this seems hard to justify, since the code exists already, and it could be useful for a thorough evaluation of the paper.
3. Most of the figure panels should be significantly larger – Figure 7 is a particularly bad example. I understand that space is a limiting factor, but some progress can be made by including fewer ratemaps (don't see the need for more than 3x3 or 4x2 examples of each kind). Also ensure that font sizes don't go below 7 or 8 – when they do, it may be better to just remove the text because it is very inconvenient to read.

Limitations:
The authors have adequately discussed limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper shows that recurrent networks trained with a ""self-supervised"" loss leads to units of the internal representations that organize as grid cells. In particular, the paper defines a loss that promotes separations between neural representations encoding different spatial locations, encourages a representation to be invariant to different possible paths taken leading to it's representation, and maximizing the capacity of the representation, and the authors use paired velocities and neural representations as their dataset.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper was very clearly written and organized, with nice visuals that supported the text. Moreover, the paper provided a helpful background of previous research that was relevant to the formulation used in the paper.
- The paper defines a loss that is nicely linked to existing theories for the emergence of grid cells, and shows that this loss, optimized using gradient descent leads emergence of grid cells in artificial recurrent networks.
- The authors performed ablations of the hyperparameters in their loss to show the dependence of their results on the different terms

Weaknesses:
- How robust are the results to other hyperparameters, like batch-size, learning rate, etc?
- While the authors criticize previous work that identified the emergence of grid cells using supervised RNN to specifics of the target function (line 55), the authors do not seem to properly explain how their setup differs, and does not lead to similar implicit assumptions. For example, what is the difference between the velocities being used as a supervised signal (which the authors criticize), versus incoporating implicitly into their dataset and self-supervised loss? Are there similar assumptions with respect to the creation of the dataset (e.g having a sufficient number of examples with overlapping positions)
- Further, the authors assert that ""SSL mitigates the need for large scale supervised data"", but it is unclear to me how different it is to incorporate the velocities as a paired dataset rather than a target variable for a supervised objective. 
- (Minor) The claim in the discussion ""how might ... SSL principles be applied to drive computational neuroscience forward"" seems too general.
- (Minor) Difficult to read text in Figure 7
- (Minor) extra italics on t in line 138

Limitations:
Yes

Rating:
7

Confidence:
3

";1
j2EaW49Rk7;"REVIEW 
Summary:
In this paper, the authors propose a gradient-boosting algorithmic framework for rule ensemble learning, emphasizing the interpretability of produced rule set. Various gradient-boosting algorithms are reviewed in the rule-learning context, and the authors argue that a specific boosting algorithm, called fully corrective orthogonal gradient boosting (FCOGB), is particularly suited for rule boosting. The intuition is that existing additive rule-boosting procedures operate in a strictly greedy fashion - the weight of each added rule is fixed in later iterations. In contrast, FCOGB allows the weights of preceding rules to be adjusted in each later iteration, which may help to reduce the number of required rules (to reach a certain accuracy) and thus the cognitive complexity of the final rule set. Based on FCOGB, the authors derive the stepwise boosting objective function for single rule search, which is similar to existing gradient boosting objectives but with a different regularization term. The overall algorithm looks like the conjugate gradient method - in each iteration, the rule aligning best with the gradient in the orthogonal complement of the subspace spanned by previous rules is added. The authors demonstrate the effectiveness of FCOGB through experimental comparison with existing rule-boosting algorithms on classification, regression, and Poisson regression tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Applying FCOGB to rule learning, to the best of my knowledge, is a novel idea, and the authors provide a comprehensible justification for this choice. Figure 2 is helpful in understanding the difference between FCOGB and existing rule-boosting algorithms.
- How to search the optimal rule in each iteration is especially considered, which is a key step in rule boosting. The authors propose a strategy that exploits the nice structure in the boosting objective function to speed up the bound calculation in branch-and-bound search of optimal rules.
- The proposed algorithm is evaluated on a wide range of datasets and tasks. The authors provide a detailed analysis of the results. Figure 1 clearly shows that FCOGB can achieve a better accuracy-risk trade-off than existing rule-boosting algorithms.


Weaknesses:
- The presentation of the paper can be improved. For example:
  + In the ""Rule Boosting"" section, the ""Gradient boosting"" subsection mixes the description of general gradient boosting and the more specific rule boosting. This makes it hard to understand these objectives for readers who are not familiar with the rule-boosting literature. For example, obj_gb(q) = |g^T q|/||q|| is nonstandard in the general gradient boosting literature. It would be better to separate the general gradient boosting and rule boosting parts.
  + The ""Single rule optimization"" subsection assumes too much prior knowledge about the rule learning literature. I would suggest the authors merge this subsection with the ""4.3 Efficient Implementation"" subsection to make the paper more fluent and self-contained.
  + The authors should provide more details about the proposed algorithm, especially the BnB/beam search of a single rule.
- Lack of comparison with rule induction algorithms based on column generation, e.g., [30] and [b]. In the column generation approach, the weights of all added rules are also adjusted in each iteration when solving the restricted master problem, which is similar to FCOGB. I am interested in how FCOGB compares with this approach experimentally.
- The presentation of the prefix optimization problem is misleading. The authors claim that ""This function can be efficiently computed for many objective functions by pre-sorting the data in time O(n log n)"" in Section 3, but is this true for the objective function obj_{ogb}(q)?  The authors should clarify this point. I cannot immediately see how the optimal solution to (2) under this objective function is contained in the prefix of the data sorted by some (what?) criterion. If this is true, the authors should provide a proof or a reference to support this claim.
- There is a mistake in Lines 233-234.
- Missing references:
  + [a] Jonathan Eckstein, Noam Goldberg. An Improved Branch-and-Bound Method for Maximum Monomial Agreement. INFORMS Journal on Computing, 2012.
  + [b] Jonathan Eckstein, Ai Kagawa, Noam Goldberg. REPR: Rule-Enhanced Penalized Regression. INFORMS Journal on Optimization, 2019.
  + [c] Fan Yang, et al. Learning Interpretable Decision Rule Sets: A Submodular Optimization Approach. NeurIPS 2021.

Limitations:
The limitations of this work are not explicitly discussed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach to gradient boosting of decision rules for interpretable machine learning models. By incorporating a
weight correction step and orthogonal projections, the method maximizes predictive gain per rule.
Their experimental evaluation on various classification, regression, and Poisson regression tasks confirms that the resulting rule learner
enhances the trade-off between comprehensibility and accuracy in the fitted ensemble. Moreover, it maintains a comparable computational
cost to previous branch-and-bound rule learners.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Originality: The paper introduces the first rule boosting algorithm that consistently optimizes the accuracy/complexity trade-off of produced rule sets. This represents a novel contribution to the field.
2. Quality: The research exhibits high quality as it adopts the fully corrective boosting approach, which entails re-optimizing all rule consequents in each boosting round. The study's rigorous algorithm development provides a strong foundation for the research, ensuring
the reliability and robustness of the findings.
3. Clarity: The paper explains the new objective function for selecting individual rule bodies, the corresponding efficient algorithm for cutpoint
search along with some other algorithm details. The clear explanations contribute to the overall clarity of the research.
4. Significance: The research demonstrates significant improvements over previous boosting variants in terms of the risk/complexity tradeoff.
The better risk reduction per rule and the affinity to select simpler rules contribute to the overall significance of the findings. Additionally,
the comparable computational cost to previous approaches adds to the practical relevance of the research.

Weaknesses:
In terms of the compared established methods, SIRUS [1] is the most recent work included in the analysis, published in 2021. However, it is
worth noting that some more recent publications, such as [2,3], are not included in the experiment section.
One limitation of the paper's presentation is the heavy reliance on text and equations, with less emphasis on the use of figures and intuitive
example case studies. This approach may hinder the reader's ability to grasp complex concepts and visualize the practical applications of
the proposed methods. Incorporating more visual aids, such as figures and illustrative examples, could enhance the clarity and accessibility
of the research.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Limitations:
A limitation of the study is that while it includes the most recent work, SIRUS [1], which was published in 2021, it does not incorporate some
more recent publications like [2,3] in the experiment section. This omission limits the comprehensiveness of the analysis and may overlook
potential advancements or alternative approaches introduced in these newer works.
[1] C. Bénard, G. Biau, S. Da Veiga, and E. Scornet. Interpretable random forests via rule extraction. In International Conference on Artificial
Intelligence and Statistics, pages 937–945. PMLR, 2021.
[2] Souza V F, Cicalese F, Laber E, et al. Decision Trees with Short Explainable Rules[J]. Advances in Neural Information Processing
Systems, 2022, 35: 12365-12379.
[3] Calzavara S, Cazzaro L, Lucchese C, et al. Explainable Global Fairness Verification of Tree-Based Classifiers[C]//2023 IEEE Conference
on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2023: 1-17.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents a new algorithm for learning rule ensembles and claims that these are interpretable, but does not present any support.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The proposed method is reasonable, but, in the context of other work in this area, not ground-shaking. The experimental evaluation is done well, but does not touch on interpretability.

Weaknesses:
There is no evidence that the learned rule sets are interpretable.
Rule complexity has not much to do with cognitive complexity.
The efficiency of algorithm is over-stated in the paper.

Limitations:
This is another paper that claims that rule ensembles are interpretable. No evidence is presented to that end, the claim is just derived from the fact that rules are, by themselves, interpretable. However, for example, random forests are well-known to be not interpretable, and they are also just a rule ensemble. In addition, the situation is even worse here, because in a random forest at least each individual rule is interpretable, and may be viewed as an explanation for all the examples it covers. In an additive boosting setting, this property also does not hold, because each rule corrects and refines predictions of previous rules, so rules can no longer be interpreted in isolation, but only in the context of all previous rules. Even a single example can not be easily explained by a gradient-boosted rule set, because one would have to understand the interaction of multiple rules.

The authors use the term ""cognitive complexity"" for something that is essentially the size of a rule set. Again, this is a complete misnomer, as the cognitive effort to parse a rule set does not only depend on the size of the theory. As explained above, there might be dependencies between rules, or rules may be considered in isolation (the latter having a much lower cognitive complexity). There are also factors such as the familiarity with the used concepts. For example, the cognitive effort required to read a page of text in your mother tongue is much lower than the cognitive effort required to read a page in a language that you are just learning, even though both, the content, as well as the syntactic length (essentially the author's measure of cognitive complexity) is the same.

It is a pity that they authors make such unfounded claims about intepretability, where they could simply present their work as an attempt to learn a simpler rule ensemble. As such, the work is reasonable, but also not great break-through. What they essentially propose (following previous work) is to re-optimize all weights once a new rule is added, and build an efficient algorithm around that idea. It gains a little in performance, as can be expected, but it is not great break-through.

The small advantage seems to be bought with an increase in computation time, which the authors interpret as ""in the same order of magnitude"" except for one case, where it is by a factor 26 slower. Actually, it seems to be the case that in most of the datasets, the algorithm is at least a factor of 2 smaller, sometimes worse. 

Minor comments:

Some of the numbers in Table 1 are obviously wrong (e.g., testing risks of 109.5 or 4.115 for XGB).





Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces Fully-Corrective Orthogonal Gradient Boosting (FCOGB), a novel algorithm aimed at facilitating interpretable rule learning. The study contends that existing rule learning algorithms often yield complex models that pose challenges for interpretation. FCOGB addresses this concern by generating simpler and more easily understandable models.
FCOGB is an extension of the widely employed gradient boosting algorithm, utilized for constructing predictive models. It employs a branch-and-bound search algorithm to identify the optimal set of rules that minimize prediction errors.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed method is supported by theoretical justifications and intuitive explanations using figures. Additionally, the paper proposes algorithms with computational complexity analysis to efficiently implement the method, demonstrating practical applicability.

Weaknesses:
Despite an increase in the required training time (takes several times longer computation), the generalization performance does not improve. If this weakness is addressed, I believe it would become a very strong paper.

(Minor comments)
- Despite Figure 2 being referenced on page 5, the figure is actually inserted on page 3.
- The scatters plot in Figure 3 are difficult to interpret due to overlapping points. Please set alpha (transparency).

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a framework of fully corrective orthogonal boosting. The main algorithmic difference here is the objective for each next weak model. It is the cosine between the gradient (which is orthogonal to previous weak learners by construction) and the part of the new model that is orthogonal to previous models. Authors motivate their work by the need of interpretable models, so they restrict themselves to the case of rules as weak learners. Also, they use a variant of b&b algorithm for optimal weak learner search instead of commonly used greedy construction in depth. 

Authors claim that this the paper proposes an algorithm for constructing shorter and more interpretable rules for Gradient Boosting of Decision Rules model. Experiments show that described method outperforms standard implementations of GB in case of using models of low complexity.

Although theoretical part is sound, practical questions are not thoroughly addressed or answered.

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The main part of the paper is well-written, the terms, designations and ideas are clear. The proposed method is sound, reasonable and well described. The idea of orthogonal rule search in conjunction with fully-corrective goosting looks good. The theoretical part is described very well, the main formulations are correct, and the obtained contributions look important and are novel to the best of my knowledge. The proposed algorithm is justified and has the potential to compete with SOTA in the outlined formulation that refer to ""cognitive complexity"".

The main part of the paper is well-written. The terms, designations, and ideas are clear. 

The only point I did not buy is the Poisson loss defined in line 109, in my opinion, incorrectly (or unclear), because, formally, from that definition, its minimum is at $f(x_i)=0$ independently on $y_i$.

Weaknesses:
I have the following concerns about the research direction itself. Claimed advantage of ensembles of rules over ensembles of trees is their human interpretability. However, I cannot agree that the decisions a rule ensemble makes can be treated as interpretable. Particularly, I argue that in the domains where interpretation is important summation of even two terms is usually not interpretable for humans. Most critical decisions in such domains like medicine and justice, partly science and risk management are usually based on several binary factors, not a sum of dozens of rules. Where ensembles of rules are really used in practice?

Second, I am disappointed that the term ""cognitive complexity"" was left without any background. I would expect references to some papers using this metric or explicit statement that this way to estimate models' complexity is originally proposed in the current paper. Futhermore, I would expect some consideration of actual research in psychology domain that address the problem of cognitive complexity of calculations.

For example, we can see in ""Human knowledge models: Learning applied knowledge from the data."" Plos one, 2022, by E. Dudyrev et al., that a human decision is usually based on:
-	Boolean operators: OR, AND, NOT, and thresholded Boolean SUM (arithmetic sum of
Boolean variables, compared to an integer threshold)
-	At most four (Boolean) variables, where each variable is used at most once\

These ideas are rather far from the concept of sums of dozens of rules

See also:

Lemonidis C., “Mental Computation and Estimation: Implications for mathematics education research, teaching and learning”, 2015,

Marois R et al, ""Capacity limits of information processing in the brain,"" Trends in cognitive sciences, vol. 9, no. 6, pp. 296–305, 2005

Nys J. et al, ""Complex Mental Arithmetic: The Contribution of the Number Sense,"" Canadian journal of experimental psychology, vol. 64, no. 3, pp. 215–220, 2010.

At last, but not least, the experimental part spoils the impression of the work and requires improvements:

- First of all, I see no hyperparameter tuning step description (e.g. regularization terms for XGB, number of boosting rounds, length of decision rules) in the section on experiments. Are there any hyperparameters which may have a significant impact on the performance of FCOGB? Where they left ""defaulted"" or were they were tuned by a separate step of an algorithm?

- In the beginning of Section 5, it is mentioned that you use only 5 runs for each dataset with < 50 cognitive complexity (CC) limit. But then I see averaging over complexities between 1 and 50 in the description. What does it mean? I suppose that CC may alter in different runs but it is limited to 50, is it true? Or did authors perform exhaustive search of all possible CCs and averaged over them? If the first is true, I have a doubt that different model may have had different mean CC values, so that it is not quite fair comparison results. Is the second is true, then it is unclear why such an averaging can prove something

- It would be interesting to see the dynamics of quality with respect to increasing CC. In particular, some graphs that plots quality vs CC to see which algorithm uses the CC limit more effectively.

- It may be useful to provide comparison with other variants of fully-corrective boosting implementations since the quality gain may origin from described by this scheme only

- Time limitations should be discussed more in terms of time per CC point and pareto curves (time to achieve the desired quality)

- How should we interpret relatively low quality for regression problems?

- This paper addresses interpretability of trained decision rules, so it would be profitable to demonstrate a difference in the simplicity of interpretation for FCOGB rules and, e.g., XGB rules

Limitations:
I do not see any particular limitation of the proposed work

Rating:
7

Confidence:
4

";0
H15KtcyHvn;"REVIEW 
Summary:
The paper investigates the algorithmic complexity of training fully connected ReLU neural networks. It is shown that perfectly fitting two-dimensional data by even a one-hidden layer network with two output neurons is $\exists\mathbb{R}$-complete, namely it is as hard as finding the roots of a multivariate polynomial with integer coefficients.

The paper is well-composed and easy to read, and the findings are interesting even though they are somewhat limited as elaborated in the strengths/weaknesses sections. Overall, my opinion of this paper is positive and I believe it merits acceptance.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- Very well-written and easy to follow. It is evident that the authors have put a lot of effort into composing this paper and thoroughly discussing its results, impact and drawbacks.

- The results in this paper are interesting and may suggest that training neural networks is strictly harder (assuming a somewhat acceptable complexity theory hypothesis) than problem in NP. This indicates that certain practices make the worst-case complexity of real-world problems much harder, but the problems are nevertheless solved efficiently. As motivation for future work, it is interesting to study how changing these practices might affect the efficacy in which neural networks are being trained.

Weaknesses:
- The paper studies the problem of perfectly fitting the data, while in practice we are interested in getting the training error to become sufficiently small.

- The paper assumes that the learning problem requires classifying at least 13 classes that are two dimensional and are not linearly dependent. While I agree with the authors' claim that the gap between 3 and 13 classes seems immaterial, I still find the two-dimensional output requirement very limiting. This is due to the fact that we can always re-encode the target classes in a different manner which circumvents $\exists\mathbb{R}$-completeness. E.g., we can just encode the classes using the naturals $1,2,\ldots$. The authors point out that a common practice is to use $k$-dimensional standard unit vectors to encode $k$ different classes which may still imply $\exists\mathbb{R}$-completeness, but the study of this is left to future work. To summarize this weakness, the paper cannot currently rule out the possibility that common practices circumvent $\exists\mathbb{R}$-completeness entirely.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proves ER-completeness of training (empirical risk minimization) a fully connected two-layer ReLU network with two inputs and two outputs. The result contributes to the understanding of theoretical complexity of neural network training at an important, fundamental level. The authors also did a good job explaining the intricacies of the achieved type of hardness result with respect to implications and applicability in the broader neural network training context. The true main, and (unavoidably) very technical part of the paper, i.e., the actual proofs, are delegated to a ""supplementary material"" document that is twice as long as the ""main"" paper. 

-- update: I have read and acknowledged all other reviews and the authors rebuttals, see discussion. --

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
The result of this paper constitutes and important and fundamental contribution to the understanding of the complexity of training neural networks (to global optimality). Besides the main result itself, the paper's strength lie in its clarity of presentation and the discussion of context, limitations and implications.

Weaknesses:
It appears to have become common practice to submit papers to NeurIPS (and ICML) whose actual, main content is put in a separate ""Supplementary Material"" document whose length far exceeds that of the supposed main paper. Unfortunately, this paper is no exception. I consider this a weakness because this format bears the danger of the formally most important parts of the work not being reviewed thoroughly due to the short review period and high review load of reviewers at these conferences. I cannot exclude myself from this -- I simply did not have the time to rigorously check all the details in the long supplementary document, and therefore cannot give a definitive answer regarding the proofs' correctness beyond ""believing"" everything appears to be in order. In this regard, I cannot help but wonder if a full journal paper would not be the better way to publish results that simply do not fit into the 9-page limit...

Limitations:
The authors adequately commented on applicability/meaning of their result and, thus, its inherent limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper shows that training a neural network with two input neurons and two output neurons is ER complete where ER stands for the existential theory of the reals.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A new idea (reduction) to prove hardness result. The paper is generally well written and provides a good cover of the (dense) related work.

Weaknesses:
In the end, the results in the paper point more to the limitation of current computational hardness results than to the limitation of deep learning. The task of finding the exact minimizer and the inability to prove hardness results for the approximate case diminish the relevance of the results to modern ML. Personally, I'm not a fan of ER but I think the reductions could find uses elsewhere. 
The paper is somewhere between ""accept"" to ""weak accept"" and I'm willing to raise the score if the authors provide convincing responses. 

There are several issues with the writing such as:
1) Definition 2 is very cumbersome. Consider shortening it as it done in the Blum Rivest paper. 
2) ""However, when using gradient descent
 we usually do not get any guarantees on the quality of the obtained solutions or on the time it takes to
 compute them [30]"". This is misleading as there are many (proven) results about GD finding the global minimum. [30] is one example one GD does not work well, but there are many other results for which it does work well such as ""Globally optimal gradient descent for a convnet with gaussian inputs"".
3) ""Thus it would be very desirable to use other methods, like SAT- or mixed-integer
programming solvers that are reliable and widely used in practice for many NP-complete problems.""
What does ""reliably"" mean here? I'm sure that there are bad examples for these heuristics just as there are for GD.
4) The elephant in the room which the authors largely ignore is the huge success of gradient-based optimization in training neural networks with tens of millions of neurons.
5) Definition 1 is strange. Consider using the standard matrix notation for neural networks. 
6) There seems to be a redundancy between the average case analysis paragraph and the ""Let us stress"" paragraph.
7) I would put still more effort in explaining the ideas behind the main reduction and exemplifying it with a small example. 

Limitations:
I think one of the biggest issues is not addressing the success of gradient-based optimization in the paper. 
The authors might want to talk more about improper learning which are typically used and make the learning task easier. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper strengthens a result by Abrahamsen, Kleist and Miltzow [Neurips 21] by showing that empirical minimization problem of two-layer neural network is \exist R-hard. They further show that arbitrary algebraic number is required as optimal weights even with rational data points. The example include a ReLU neural network with two inputs and two outputs. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper is well-written and the related works have been discussed in great details.
- The reduction of training two-layer neural network to ETR-INV is novel.
- The construction of inversion gadget in appendix B.3.4 is quite interesting to learn.

Weaknesses:
- Definition 1 which defines neural networks from a directed acyclic graph seems to be redundant as the paper is devoted to analyze a two-layer fully connected neural networks, whose structure is much simpler.
- The structure of the paper can be improved. The majority of the papers is to introduce definitions and discussing related works, while the main proof idea only starts at page 7. It would be better if the authors can elaborate on the proof idea. From my perspective, the main theoretical contribution in this paper is the reduction of training two-layer neural network to ETR-INV, while the details how they are achieved are largely left in the appendix. 

Limitations:
Yes.

Rating:
5

Confidence:
2

";1
N0KwVdaaaJ;"REVIEW 
Summary:
The paper theoretically analyses the sample complexity of CNNs as compared to fully connected networks. The authors find that while fully connected networks require quadratic samples with input dimension, the local connections of CNNs reduce the sample complexity to linear, while weight sharing further reduces sample complexity to logarithmic.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
**Originality**

As the authors point out, separating the impact of local connections vs weight sharing in CNNs is not a new idea; nevertheless, as far as I am aware, no previous works have shown theoretical sample complexity results for CNNs vs LCNs vs FCNs.

**Quality**

Technically speaking, the paper appears solid. The results and proof look correct as far as I can tell. The appendix is quite comprehensive.

**Clarity**

The paper is generally well-written. The notation is appropriately introduced and concepts are adequately explained.

**Significance**

I think this paper may be significant to the specific subfield that studies the sample complexity of specific model architectures.

Weaknesses:
Overall, I'm unfortunately not too convinced about the significance of this work given that there is already work analyzing at least empirically the sample complexity benefit of weight sharing vs locality (Xiao & Pennington 2022 as the authors point out, but also Poggio et al. 2016). Certainly, it seems that the theoretical result proved by the authors is new. On the other hand, it would benefit the paper to validate these theoretical results with experimental evidence, or at least point to prior experiments that line up with the authors' theory.


Minor comment: Using similar-looking notation for orthogonal groups and big O is not ideal.

Limitations:
Limitations are adequately addressed; the authors clearly state the assumptions of their theory. No negative potential societal impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors study inductive biases in deep CNNs. They analyse approximation abilities of these networks and compare the learning performances of CNNs for regression with those of locally-connected networks (LCNs) and fully-connected networks. They show that CNNs of depth O(\log d) is sufficient for achieving universality. They also present some analysis for CNNs in approximating functions with sparse variable structures. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The result that CNNs of depth O(\log d) with exponentially increasing channels is sufficient for achieving universality is very interesting. It shows the power of CNNs and channels in approximating functions. The example presented in Proposition 3.2 about the lower bound for the universality is nice. The comparison between CNNs and LCNs on learning with bounds O(\log^2 d) and \Omega (d) is simulating. Though the methods used by the authors are not new, the obtained results are valuable and insightful and can be used to explain the efficiency of deep learning algorithms induced by structures of deep neural networks. 

Analysing structured deep neural networks in approximating and learning functions with sparse variable structures is an important topic in deep learning theory. 

Weaknesses:
The topic of analysing structured deep neural networks in approximating and learning functions with sparse variable structures has been studied in the community of deep learning theory. the related literature should be mentioned in the paper. The work of Mhaskar and Poggio (Analysis and Applications 2016) with fully connected neural networks is based on a known sparse variable structure while the recent work of Mao, Shi, and Zhou (Analysis and Applications 2023) with CNNs is for unknown sparse variable structures. Another paper of Chui, Lin, Zhang, and Zhou (IEEE NNLS 2020) is about learning of spatially sparse functions. 

Limitations:
The CNNs with exponentially increasing channels might be restrictive. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper provides a fundamental analysis of the superior performance of CNNs, specifically focusing on their downsampling and multichanneling capabilities. Moreover, it establishes evidence with theoretical guarantees that CNNs exhibit efficient learning of sparse functions compared to FCNs, attributing this efficiency to the weight sharing and locality properties inherent in CNNs.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper presents a comprehensive and rigorous theoretical analysis of the intrinsic inductive biases linked to multichanneling, downsampling, weight sharing, and locality within CNNs. It introduces a new theoretical framework that characterizes the crucial role of multichanneling and downsampling in empowering deep CNNs to efficiently capture long-range dependencies within the target function, yielding valuable insights into their operational mechanisms. Furthermore, leveraging these theoretical insights, the paper substantiates the superior performance of CNNs compared to FCNs, providing further support for their efficacy.

Weaknesses:
1. This paper primarily concentrates on exploring the expressive power of Convolutional Neural Networks (CNNs) and argues that a CNN with a depth on the order of $\log d$ possesses the capacity to learn spatial information within the data. However, it lacks the convergence and generalization analysis, i.e., whether CNNs with a layer depth in the order of $\log d$ can effectively learn spatial information using SGD-based algorithms remains uncertain.

2. This paper lacks empirical experiments, particularly those using real data, to provide concrete evidence in support of its theoretical findings.

3. Recent observations suggest that Transformers are more efficient in learning spatial information compared to CNNs, and CNNs are not as widely utilized in current practices. In light of these trends, it would be valuable for the paper to offer insights or considerations pertaining to the Transformer model, further exploring its strengths and potential advantages in learning spatial information.

Limitations:
Yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the inductive bias of deep convolutional neural networks (CNNs) in three distinct ways. With $d$ the input dimensions, it is shown that (i) depth $O(\log{d})$ is sufficient to achieve universality, i.e. the capacity to approximate any function of $d$ variables, when there is downsampling between layers; (ii) all sparse functions can be learned with $O(\log^2{d})$ samples, close to the optimal $O(\log{d})$; (iii) both weight sharing and locality of the connection are crucial for achieving the nearly-optimal sample complexity.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Our theoretical understanding of deep CNNs is still very limited and this paper has the potential to provide a solid contribution to the topic. The explanation of how multichannelling and downsampling conspire to achieve universality with $O(\log{d})$ layers is nice and intuitive. The result of Theorem 4.6 on the sample complexity of learning sparse functions is novel and interesting. The separation results start from a known idea  (using the symmetry of an algorithm to unveil its limitations) but give significantly new insights into the separate roles of weight sharing and locality.

Weaknesses:
Overall, I believe this paper would be a valid contribution to the community but there are some issues that I would like to see addressed before recommending publication. 

I like section 3 and I believe it is a valuable addition to the literature, but I would downplay the claim that all previous results required a depth of $\log{d}$, as currently written in the abstract. The idea that sufficiently deep CNNs achieve the same expressivity as fully-connected networks (hence they are universal) has been there for a while and the difference between $d$ and $\log{d}$ is solely due to the amount of stride included in the architecture. The case considered in this paper, where the stride equals the filter size, is commonly referred to as *nonoverlapping patches*---the authors should say it explicitly at least in section 2.1---and it has been studied in several other works, including some cited in this paper such as  Cagnetta et al. (2022) and some that are not cited but should be such as Poggio et al. (2016), *'Why and When Can Deep – but Not Shallow – Networks Avoid the Curse of Dimensionality: a Review'*. Both these works imply that deep CNNs with nonoverlapping patches and depth $\log{d}$, so that the post-activations of the last hidden layer look at the whole input, are universal.

There is also something about theorem 4.6 that I think requires further explanation. As far as I understand, theorem 4.6 uses lemma 4.4 together with the generalisation properties of one-hidden layer neural networks (theorem 4 from E et al. (2022)). If this is true, then I do not understand why one needs a deep CNN to achieve sample complexity $O(\log^2{d})$: the result of lemma 4.4, i.e. that there exist a set of parameters such that the $L$-th layer outputs the right-hand side of Eq. (4), is also applicable to the post-activations of a  one-hidden-layer fully-connected network. I should remark that my concern might be due to a misunderstanding of the proof idea, but if that is the case I will be happy to raise mi mark after rebuttal.

Finally, the symmetry-based lowe bounds like those of theorems 5.4 and 5.7 can usually be explained very well at an intuitive level. I think not having such intuitive explanations is a weakness of this manuscript.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
7

Confidence:
4

";1
J0Pvvxspmz;"REVIEW 
Summary:
This paper proposes a new pruning approach to train a uniform 1×N sparse structured network from scratch, dubbed as SUBP. SUBP pruned blocks according to block angular redundancy. The blocks are pruned in a uniform manner to make full use of multithread parallelism computation. During the training process, the pruned blocks can be regrown through importance sampling. They conduct experiments across various CNN architectures to show the superiority of  SUBP.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper is well-written and its motivation is reasonable.
- Code seems to be available, and it'll be helpful if the authors could open-source the work if the paper is accepted.
- They deploy the pruned model in both the arm platform and x86 platform to get actual inference speed. These results well-validate that uniform pruning methods are faster than non-uniform pruning methods.

Weaknesses:
-  The authors are encouraged to conduct experiments on other tasks, not just classification.
-  The authors only report the FLOPs in Table 2, the authors are encouraged to report the training time.
-  Compared with method CHEX, the improvements in accuracy are limited.

Limitations:
As discussed in Section 5, there are still missing experiments, including applying  1×N sparsity on other types of DNNs like RNN, transformer, and other tasks including object detection, natural language processing, etc.. The generalization on downstream tasks still needs to be verified.

Rating:
6

Confidence:
5

REVIEW 
Summary:
SUBP trains a uniform 1×N sparse CNNs from scratch, with three characteristics: (1) a periodic block pruning and regrowing technique via importance sampling, (2) a pruning criterion based on angular redundancy across blocks,  and (3) a uniform 1×N sparse pattern for multithreading acceleration. The authors also provide latency results on hardware platforms.

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
1. The method achieves competitive performance against many existing methods. 

2. SUBP trains a uniform 1×N sparse CNNs from scratch.

Weaknesses:
1. Novelty concern. This paper uses a prune-and-grow idea to train a sparse model from scratch, which is been well-studied by many papers in sparse training [1-2]. DMCP and CHEX also don’t require pre-training. Work [3] studied how to prune the network from scratch with a grow-and-prune methodology. Besides, the 1xN pruning technique needs compiler optimization (like TVM) to achieve real inference acceleration. Therefore, the acceleration part of this work highly relies on TVM.

2. The performance improvements are marginal. In Table 1, it’s hard to tell if SUBP is better than CHEX. More important, CHEX uses channel pruning, which could achieve a faster acceleration rate when the pruning ratio is the same. From my point of view, CHEX is basically faster and has the same performance in terms of accuracy. DMCP (channel pruning) is only trained 150 epochs, which is also hard to tell if SUBP has better performance (69.0%,150 epochs vs 69.7%, 250 epochs). Overall, when compared with many SOTA methods, SUBP could not achieve a faster acceleration rate or has an obviously better performance in terms of accuracy. Table 2 has the same problem with many outdated baselines. The authors should provide error bars when the accuracy improvements are marginal.
 
3. The paper is written poorly. Sections 1 and 3 should be organized better, and section 3 is not clearly written. Table 1 first appeared on Page 6 (section 3) and was first mentioned at the end of Page 7 (section 4). Table 2 has an error with vertical lines exceeding the table. Also, the font size of the figures in this paper is very small, especially in Figure 2. All these things make this paper hard to read.

4. There is no ablation study to show the effectiveness of each components of SUBP.

[1] Rigging the Lottery: Making All Tickets Winners, ICML 2020.

[2] MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge, Neurips 2021.

[3] Effective Model Sparsification by Scheduled Grow-and-Prune Methods, ICLR 2022.

Limitations:
Please refer to “Weaknesses"" for limitations. The authors adequately addressed the potential negative societal impact of their work.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper proposes a new fine-grained structured pruning method, which can train from scratch to reduce the training cost and prunes the weights in a uniform manner to handle the load imbalance problem. During pruning, the method uses block angular redundancy as the criterion to discard the redundant blocks and allows pruned blocks to regrow to the network based on the importance sampling to search for a better sparse pattern. Experiments show that the SUBP method can achieve a significant accuracy improvement compared with other methods under the same FLOPs constraint.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written.
- The proposed SUBP method can greatly improve the model accuracy than kernel pruning methods under a similar pruning ratio.
- The authors propose uniform pruning to handle the load imbalance problem and show the real speedup on the CPU to evaluate the effectiveness of this method.


Weaknesses:
- Experiments are insufficient. The authors only evaluate the proposed method on the classification task. It is better to discuss the generalization of the method and evaluate on different tasks.
- The paper lacks a comparison of accuracy with the non-uniform block pruning method. Figure 4 shows that the speedup of the uniform 1xN method is limited. If there is an accuracy drop, I wonder if it is worth adding the uniform constraint at the cost of such a model accuracy loss.
- Experiments only compare with the kernel pruning methods and lack comparisons with other block pruning methods, e.g. PatDNN [1] and 1xN pruning [2].
- This paper only focuses on CPU, but GPU on mobile is also important. For example, the implementation of the algorithms related to autonomous driving currently heavily relies on Orin GPUs. It is better to analyze the efficiency of this method on GPU, and the uniform pruning method should achieve better speedups.

[1] Wei Niu, et al. PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-based Weight Pruning, ASPLOS, 2020.  
[2] Mingbao Lin, et al. 1xN Pattern for Pruning Convolutional Neural Networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

Limitations:
The author discussed the limitation of the method and the societal impacts. Other limitations of my concern can refer to the weaknesses.

Rating:
5

Confidence:
5

REVIEW 
Summary:
Structural weight pruning methods have gain popularity due to the hardware friendly pattern. This work improves 1xN pruning with prune-and-grow and uniform block pruning. Evaluation shows outperforming accuracy results at similar levels of FLOPs than prior work.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper presents a new contribution on block-wise pruning with soft (prune-and-grow) in channel-wise uniformly.
- The evaluation shows potential significance of the proposed block pruning method in achieve accuracy with reduced FLOPs.

Weaknesses:
- The paper could be improve in clarity and sufficient implemantion details to help readability. 

Limitations:
Not adequate. From the evaluation, the proposed method, among many other pruning work, significantly increase the training epochs. The negative impacts from more training costs need to be discussed.  

Rating:
6

Confidence:
4

";1
eTHawKFT4h;"REVIEW 
Summary:
This paper provides a theoretical framework based on generalized variational inference [1] and Wasserstein gradient flows (WGF) for analyzing deep ensemble methods and their regularized versions. The authors demonstrate that deep ensembles and other variational Bayesian methods can be cast as instances of an infinite dimensional variational inference problem and the WGF of different instantiations of a free energy functional. The authors additionally use their theoretical framework to derive a new algorithm for generating samples from a target distribution.

[1] Knoblauch, Jeremias, Jack Jewson, and Theodoros Damoulas. ""An optimization-centric view on Bayes’ rule: Reviewing and generalizing variational inference."" Journal of Machine Learning Research 23.132 (2022): 1-109.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is well-organized and written, and the benefits of the unifying theoretical framework are compelling. The discussion of how deep ensemble methods can be viewed through the lens of WGF and the use of this lens to prove theoretical guarantees on the limiting behavior of particle estimations is useful and insightful. This work also holds the promise of deriving new algorithms, as demonstrated by the deep repulsive Langevin ensembles presented in Section 4.3.


Weaknesses:
#### **Experiments section is difficult to follow**
While the details corresponding to the various figures in Section 5 are fully provided in Appendix G, this section is currently difficult to follow as a stand-alone section in the main paper. Without (even high level) details on the experimental setup and the general motivation of each experiment it is difficult to dive right into the results and Figures as they are currently presented. I recommend moving some details from Appendix G into the main text and providing the context for each experiment before diving into the results.

---

#### **Motivation for convexification is unclear**
While the authors prove that convexity of the infinite-dimensional variational form of the learning objective guarantees uniqueness of a minimizer, this is somewhat disconnected from the presented goal of optimizing $\ell(\theta)$ via probabilistic lifting. For example, in footnote 1 on page 2, the authors argue that the unregularized variational objective has non-unique optimum. However, the local optima all have equivalent values of the objective and are simply weighted averages of equivalent optima of $\ell$, hence it is not clear why uniqueness is a desiderata here.

Additionally, Figure 2 in Section 5 demonstrates how deep ensembles (DE) do not converge to $Q^*$. However, although deep Langevin ensembles (DLE) and deep repulsive Langevin ensembles (DRLE) provable converge $Q^*_{DLE}$ and $Q^*_{DRLE}$, respectively, these optimal distributions are also not equal to $Q^*$.

Hence a clearer exposition as to why regularized optima are preferred to the unregularized ones is needed.

---

#### **Motivation for DRLE is lacking**
While DRE / DRLE is indeed interesting as a new algorithm that can be derived from the presented theoretical framework, it would be great if the authors also provided some intuition / motivation as to why MMD is perhaps a better suited divergence regularizer than KL. 


Limitations:
Authors can potentially elaborate on the future directions of this work, specifically around analyzing the approximation errors of approximating WGF with finite number of particles over a finite time horizon.

Rating:
8

Confidence:
2

REVIEW 
Summary:
The paper established theoretical connections between ensembling an old and established method of deriving uncertainty estimates They use theory from iteraction of particles  in a thermodynamic system to generalise and connect seemingly different ways of ensembling and Variational Bayes(Inference) methods. This is done by formulating the original non-convex optimization problem ubiqitious in ML and stats as infinite dimensional convex optimization problem in the space of probability measures. The addition of a regularization quantity ensures the strict convexity of the problem and by choosing different forms of this quantity lead to derivation of various inference algorithms.   

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. The paper is well written, theory heavy and addresses important topic of deep ensembling and its connection with variational Bayes methods
2. I am not so good with theory, but the theorems and equations looked ok to me without obvious mistakes.
3. Although this is a theory paper, the theoretical claims are well supported by the experiments and where they are not the authors they explain it well.
4. The distinction between IDGVI and FDGVI is well drawn out and explained. Also the limitations with FDGVI that the approximation family is limited by construction serves as a motivation for using IDGVI methods.

Weaknesses:
1. There is a lot of content that has been compressed in 9 pages which can be challenging for a reader, and a journal might have been more appropriate for this work.

Limitations:
The limitations or practical challenges with the derived inference algorithms can be addressed. How practical is the result from Theorem 2, is it something that will only work asymptotically or will this work practically and if so how fast ? 

Rating:
8

Confidence:
2

REVIEW 
Summary:
The authors propose to unify existing theory on Bayesian (variational) inference (VI) by addressing a generalized objective, which is obtained from standard parameterized loss minimization by “probabilistic lifting” (re-casting in a space of probability measures over the parameter) and “convexification” (ensuring the existence of a global minimizer by regularization), with infinite-dimensional gradient flows in 2-Wasserstein space. A general recipe is provided to implement such a Wasserstein gradient flow (WGF) via an energy objective and a system of interacting particles. In the key contribution of the paper, the authors study WGF with different types of regularization–most notably, the unregularized version corresponding to deep ensembles (DE). It is shown that DE do not conduct a Bayesian learning procedure and systematically fail to generate samples from the optimal distribution, yet perform competitively thanks to the flexibility of the infinite-dimensional inference they realize (as opposed to, e.g., classical parametric VI).

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
* [S1] **Unifying framework**. After much discussion in the past few years, the authors are–to the best of my knowledge–the first to establish a comprehensive theory that encompasses (finite-dimensional) VI and DE.
* [S2] **Clarity**. Despite the rather abstract subject, the authors present a coherent and easy-to-follow sequence of arguments. Complexity is strictly limited to the necessary extent.
* [S3] **Rigor**. Mathematical concepts and notation are sound. Extensive proofs and/or references to prior work underline every claim (though I did not check every proof in detail).


Weaknesses:
* [W1] **Analysis of DE behavior** (see Questions)
  * It is not entirely clear if the paper studies arbitrary variants of DE or only a very narrowly defined version (see Q5). 
  * The authors conjecture that the number of samples being vastly smaller than the number of local minima is responsible for D(R)LE not outperforming DE consistently and point to Fig. 4. This evidence seems rather anecdotal and could benefit from a more detailed investigation. Also see Q7--Q8.
* [W2] **Omissions in notation**. While the notation is consistent and comprehensible overall, the authors tend to omit integration domains, objects of differentiation etc. (e.g., Eq. 1, l. 150, l. 177, l. 179). With the shifting of integration spaces and various gradients involved, it would be helpful to be as explicit as possible in this regard.


Limitations:
Given that the main contribution is a unifying framework for existing theories, this point doesn’t apply as usual. However, the authors should state more clearly that the evidence shown in Section 5 for findings in Section 4 is quite limited.

Rating:
8

Confidence:
3

REVIEW 
Summary:
To improve the accuracy of the uncertainty quantification, the authors aim to provide a mathematically rigorous link between Bayesian inference, Variational Bayes methods and ensemble methods. In this work, methods s.a. variational inference, Langevin sampling and deep ensembles can be seen as particular cases of an infinite-dimensional regularised optimization problem formulated via Wasserstein gradient flows. They also provide a novel inference algorithm based on MMD and gradient descent in infinite dimensions plus regularisation.

The procedure takes place by reframing the usual finite-dimensional loss function problem into an infinite-dimensional one. This is done rewriting the original optimization problem using an infinite-dimensional problem over the set of probability measures $\mathcal{P}(R^J)$ and introducing a strictly convex regulariser to induce a unique global solution. This solution is assumed to not be too different from the solution to the original problem, which is controlled by a reference measure $P$. This leads to an interpretation of many different inference setups as particular cases of the optimization problem proposed. Therefore, this approach results as a combination of the proposals in Knoblauch et al. (2022) and Ambrosio et al. (2005).  



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Good idea, could be interesting to the community were it proven in some other contexts. 

* The formulation is clear and elegant thanks to the gradient flows and the usage of Wasserstein space. The usage of the thremodynamical formulation of free energy is very attractive as well.


Weaknesses:
* Altough the proposal is interesting and elegant, I think the experimental part of the paper does not provide enough evidence of the benefits related to this framework change. Results such as those present in Figure 3 could, in principle, be rivaled by previous methods s.a. [1] and [4], neither of which are discussed here. The authors maybe could provide a stronger motivation in this regard, and maybe try to encompass these other methods inside their framework.

* Some literature relevant to the topic at hand seems to be missing from the discussion, or at least should be discussed more thoroughly:
  
  *  Regarding the definition of infinite-dimensional GVI methods, I consider that other methods based on samples are left out and should be considered, such as [1,2,3]. These works may seem specially relevant due to the interest in implicitly-defined target $Q^*$, and in particular those that make use of the function-space formulation s.a. [1] or [4].

  * I think finite-dimensional GVI methods are misrepresented as they can be much more expressive than the selection made in Section 2.2 may lead to believe. I consider that this point should be addressed, and the discussion must be readjusted accordingly in order to highlight the benefits of the proposed approach without relying on this fact. As examples of this matter, please see references [4,5,6]. 

* The writing can be generally improved, since the paper can be at times hard to follow. This is just a consequence of the amount of information provided, which is a positive point, although in sections 3 and 4 could be polished further.

* (minor) The presentation could be improved, for example, by convering images to formulas s.a. in Figure 1 or the layout on the final page. 

* (minor) Since Wasserstein spaces are such a crucial point of this, I would suggest devoting a bit more time to explain the basics of the concept in the main text itself and not fully depend on the sources.

(_References included in the ""**Limitations**"" section_)

Limitations:

* Since the Bayesian framework is abandoned, I fear there are no guarantees about the properties for the distributions obtained in the same sense as with Bayesian inference. Although can be somewhat justified by results, a lot more work is needed in this regard in methods that rely on this extensions (which is a problem for this paper, although definitely not exclusive to it). 

* The paper is centred on theoretical developments, and as such, the theoretical discussion and argumentation is really interesting. However, and although it is not the core of the paper, the experimental phase leaves a lot to be desired in terms of justifying why this formulation change is needed. 

--- 
**References**:

[1] Rodrı́guez-Santana, S., Zaldivar, B., & Hernandez-Lobato, D. (2022, June). Function-space Inference with Sparse Implicit Processes. In International Conference on Machine Learning (pp. 18723-18740). PMLR.

[2] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. ""Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks."" International Conference on Machine Learning. PMLR, 2017.

[3] Santana, S. R., & Hernández-Lobato, D. (2022). Adversarial α-divergence minimization for Bayesian approximate inference. Neurocomputing, 471, 260-274.

[4] Ma, C., Li, Y., and Hernández-Lobato, J. M. (2019). “Variational implicit processes”. In: International Conference on Machine Learning, pp. 4222–4233.

[5] Sun, S., Zhang, G., Shi, J., and Grosse, R. (2019). “Functional variational Bayesian neural networks”. In: International Conference on Learning Representations.
 
[6] Ma, C., & Hernández-Lobato, J. M. (2021). Functional variational inference based on stochastic process generators. Advances in Neural Information Processing Systems, 34, 21795-21807.

[7] Deng, Z., Zhou, F., & Zhu, J. (2022). Accelerated Linearized Laplace Approximation for Bayesian Deep Learning. Advances in Neural Information Processing Systems, 35, 2695-2708.

[8] Antorán, J., Janz, D., Allingham, J. U., Daxberger, E., Barbano, R. R., Nalisnick, E., & Hernández-Lobato, J. M. (2022, June). Adapting the linearised laplace model evidence for modern deep learning. In International Conference on Machine Learning (pp. 796-821). PMLR.

[9] Gneiting, T., & Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477), 359-378.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper offers a viewpoint on deep ensembles as a (unregularized) Wasserstein gradient flow in the space of probability measures. This viewpoint enables new algorithms for deep ensembles (Langevin and repulsive via MMD), which are evaluated on some small datasets.  

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The paper is technically sound and well-written. Overall it was easy to follow.
2) While many similar ideas have been floating around in the literature, the precise presented view on deep ensembles seems novel, and I found Theorem 1 to be interesting. 
3) While experiments on larger neural networks are missing, the effect of the proposed algorithms is clearly demonstrated in some controlled experiments and small data sets. 

Weaknesses:
1) Perhaps the main weakness of the paper is the lack of a comparison of the new methods on large neural networks. 

2) Many of the introduced tools (convexification via probabilistic lifting, Bayes with general divergence function, Wasserstein flows, etc.) are well-known.  But I believe Theorem 1 and Theorem 2 offer some new insights (in case they are really correct, see Questions). 

Limitations:
All limitations are addressed.

Rating:
7

Confidence:
4

";1
iT9MOAZqsb;"REVIEW 
Summary:
The authors proposed a new theoretical framework based on the mean field theory to analyse adversarial training from several perspectives, such as the upper bounds of adversarial loss, the time evolution of weight variance and adversarially trainable conditions. Besides the theoretical analysis, the authors conducted several experiments verifying the proposed theoretical framework. 
Generally speaking, the proposed theoretical framework provides a new perspective to analyse adversarial training and is highly versatile and even can extend to other training methods.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is organised well and easy to follow. 
The proposed theoretical framework seems inspiring and intriguing and gives a new perspective to analyse adversarial training from several aspects, which may serve as a good guidance for future work. Besides the proposed theory, verification experiments were also conducted to prove its effectiveness further.



Weaknesses:
1.	The verification experiments were only conducted on the easy dataset (MNIST); it may strengthen the findings if additional experiments are conducted on more challenging datasets.
2.	 Including a more systematic evaluation of the results may be beneficial, such as adversarial loss in residual networks vs. training steps for normally or adversarially training. 
3.	Several conclusions match the previous works; it could be more convincing if the reference could be given in the main content, such as ‘’the square sum of the weights in Ineq. (9) suggests that adversarial training exhibits a weight regularisation effect,’’ is consistent with [1], ‘’to achieve high capacity in adversarial training, it is necessary to increase not only the number of layers L but also the width N to keep L^2/N constant’’ is somehow consistent with [2].
4.	The authors mentioned in Line 260 that ‘’This result suggests that residual networks are better suited for adversarial training. However, one of the previous studies indicated that residual networks are more vulnerable to transfer attacks [54] than vanilla networks. ’’ However, the proposed theoretical framework did not explain such a transfer attack phenomenon. Could the authors explain or give more comments about this?
5. in Equation 3 and 4, it seems that the minimize part is not shown, perhaps it would be more reasonable to change the form of min-max in equation 3 and 4.

[1] A unified gradient regularization family for adversarial examples, in: IEEE International Conference on Data Mining (ICDM), 2015.
[2] Do wider neural networks really help adversarial robustness? Advances in Neural Information Processing Systems, 34.


Limitations:
The authors have clearly discussed the limitations of the proposed framework, such as, ‘’some theorems begin to diverge from the actual behaviour‘’ and ’’ the mean field theory assumes infinite network width, which is practically infeasible’’.
From my point of view, this article does not involve any potential negative social impact. 


Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors propose a mean field based framework for theoretically analyzing the training dynamics of adversarial training for MLP and residual networks with ReLU non-linearities. Based on this framework, the authors provide tight bounds on the adversarial loss (squared loss between clean and adversarial example output) and investigate trainability of MLP and residual networks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- While I am not very familiar with related work on mean field theory for (adversarially) training networks, the proposed framework seems to address limitations of prior work quite elegantly.
- The paper includes quite a number of interesting results using the proposed framework (loss bounds, training dynamics, trainability, impact of width).
- Formulation includes MLP and residual network with a class of “ReLU-like” non-linearities; is also includes adversarial training with clean loss (as e.g. done by TRADES).
- I feel the framework can be quite insightful and helpful for future work in understanding or improving adversarial training.
- Many claims are empirically supported on MNIST.

Weaknesses:
My main concern about the paper is that it is incredibly dense (due to the number of included claims) and its structure does not make checking the proofs and claims easy. Even though I invested significantly more time in this review compared to other reviews, I was unable to fully follow all derivations and proofs. I believe this is mainly due to the extremely convoluted way of presenting the theoretical results across main paper and appendix. Unfortunately, I feel that this also makes me less enthusiastic about the results. Here are some more detailed comments and suggestions:
- Starting with 4.2, the reader is somewhat forced to jump to the appendix at least once just to see that the authors simply reformulate the ReLU using its derivative and D for J and a. This trend continues throughout the paper and appendix. Especially in the appendix, one is forced to jump around a lot just to follow 2-3 sentences.
- The proofs are structured starting with simple lemmata and building up to the actual theorems. This is generally fine, but again, the referencing is overdone. For Thm 4.1 in Appendix E, there are 9 pages of derivation with >20 individual lemmata. So if I want to follow the proof of Thm 4.1, I am forced to go through many but not all of these lemmeta. Many have proofs of only 1-2 lines, but reference 2+ other lemmata or remarks and I need to remember the numbers or jump back and forth 2+ times just to read a single sentence. I feel the root cause for this is that many of the results are over-generalized and compartmentalized too much. I appreciate the thorough job of the authors in establishing many of the independence results, but as a reviewer and reader my #1 interest is following Thm 4.1, nothing more and nothing less. Everything that complicates this job is – in my opinion – bad for the paper. For me, the ideal solution would be a separate, easier to follow section for the proof of Thm 4.1 - even if it restates many of the lemmata and remarks, and moving the other results to a separate section for the (very very) interested reader.
- The main paper includes so many results that there is basically no discussion of each individual result. Often it feels that every sentence refers to some additional result in the appendix. I think for me, and many readers, actually discussing the results informally, in words, and taking more time and space to introduce the required notation would be more beneficial than including the current amount of results. I would prefer to have fewer results well-described and the remaining ones being in the appendix.
- Empirical results are discussed twice – after the corresponding theorems as well as in Section 6 – the space of the latter could be used to address one of the points above.

Comments and questions unrelated to structure and writing:
- In the introduction, contribution (d) is unclear to me – what theoretical result does it refer to, the trainability?
- The use of “probabilistic properties” is a bit unclear until the discussion of training dynamics. It would be helpful if the meaning would be detailed earlier in the paper.
- In 3.1, why is having $P^{in}$ and $P^{out}$ important, i.e., why do we need these fixed layers?
- Usually, the adversarial loss is also cross-entropy or something similar. While I saw papers arguing for a squared loss, TRADES does not use it AFAIK. Instead, the common setting is adversarial cross-entropy loss only, combined with clean cross-entropy loss or clean cross-entropy + KL as in TRADES. This makes me ask how assuming an adversarial cross-entropy loss would impact the results? Can similar results be derived?
- In 4, the assumption of independence is also unclear. I feel making this more explicit, e.g., by informally providing a short result on independence from the appendix, could be useful.
- The authors also highlight broader applicability; I am wondering if the authors derived similar results for standard training as reference? Or has this been done in previous work with other frameworks? This also related to the statement in l280.
- In 5.3 l261, I can’t follow how transfer attacks are relevant here? Transfer attacks should be weaker than the general attack modeled in the paper …

Conclusion:
I think that the paper has many interesting contributions and will be valued by the NeurIPS community. However, as I was not able to follow all derivations in a reasonable time, I will closely follow what the other reviewers have to say about the theoretical results. Also, I believe that the paper in its current form would have better fitted a long-format journal. For NeurIPS, I hope the authors invest some time in simplifying the main paper and restructuring the appendix. I think the current format will limit the audience of the paper to those very familiar with related work or willing to invest hours jumping back and forth between main paper and appendix. Some restructuring could really make this paper more accessible to the broad audience at NeurIPS.

Limitations:
See weaknesses.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper provides a mean field analysis on relu networks for adversarial training. The main insight is that networks without residual connections are not most likely inevitably suffer from gradient explosion or vanishing and thus are not adversarially trainable, unlike vanilla network training.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Analyzing adversarial training performance is an important problem both theoretically.

The insight from the analysis that adversarial training vanilla relu networks is more likely to suffer from gradient explosion/vanishing is an interesting insight.

Weaknesses:
The verification of the theorems might need more effort. E.g., Figure 4 is showing accuracy of vanilla network, it would be helpful to also show the curves for residual networks.



Limitations:
Limitations are well discussed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The theoretical understanding of adversarial training is an important and valuable topic. This work proposes a new theoretical framework for this based on mean field theory. With the proposed framework, the authors analyze the properties of adversarial training from multiple aspects, including the upper bounds of adversarial loss, the time evolution of weight variance, the adversarial trainable conditions, and the degradation of network capacity. These results could be helpful for the understanding of adversarial training and inspire more efforts on this topic. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. It proposes a new framework to analyze adversarial training theoretically based on mean field theory. 
2. Based on the proposed framework, it presents several theoretical results for adversarial training.
3. The proposed framework and the presented theoretical results are non-trivial and helpful for the understanding of adversarial training.

Weaknesses:
It studies several different adversarial training characteristics in the main paper. Is there any correlation between these different characteristics？ Why do we choose these aspects for analysis? Further, is it possible to provide a global diagram to better see which properties can be analyzed and which cannot be analyzed at present based on the proposed framework?

Limitations:
yes

Rating:
7

Confidence:
3

";1
GCY9C43A4L;"REVIEW 
Summary:
The authors propose using task information to modify a metric that the prediction model uses for training and include task information. This way, they encourage the model to generate more accurate predictions specifically for inputs relevant to the downstream task.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 - The manuscript is well-structured, and the subject of research is relevant
 - The authors provide an introduction referencing a comprehensive set of related work

Weaknesses:
We have not identified strong weaknesses in this paper.

Limitations:
The authors have identified and acknowledged limitations of the proposed methods. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose an approach to loss re-shaping for a supervised prediction model with the goal of improving the performance of a larger system that performs a downstream task using the predictions of the prediction model. Such shaping is necessary because reducing loss on the prediction task may only roughly correspond to improved performance on the downstream task. The authors show that their approach mostly outperforms standard maximum likelihood estimation and achieves similar performance to existing baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
TaskMet is an interesting method that enables injection of knowledge about downstream tasks without ignoring the potentially useful training data for the base prediction task.

Weaknesses:
The clarity of the presentation could be significantly improved. For example, the distinction between ""task learning"" and ""prediction"" is not clear to readers who are not very familiar with the terminology; these are interchangeable synonyms in many fields. The descriptions of the benchmark tasks are also not very accessible to those not already familiar with them (what are ""resources"" in the Cubic problem?).

The experimental results in section 5.2 are relatively weak; TaskMet does not show improved performance in any of the three benchmark tasks. Although the average performance across tasks is slightly better than existing approaches, it's not clear if TaskMet outperforms the baselines if they are well-tuned (and the authors note that hyperparameter tuning for TaskMet is key).

In section 5.3 (the model-based RL experiments), the results are stronger. However, the setting is somewhat contrived, in that the authors study either a) a large number of artificial noise dimensions added to the state, which is likely to improve TaskMet's performance because it is specifically designed to rescale the loss of each dimension and b) when the network is unrealistically small (3 hidden units). Although the results in these settings are positive, they do not decisively show the benefit of the proposed method in real-world settings. Further, the baselines considered are either only using the prediction task loss (MSE) or only using the downstream loss (OMD). It's not clear if TaskMet outperforms other methods that utilize both objectives.

Some related work could be added, particularly work in meta-learning loss functions, but this is not critical.

The advantages of TaskMet re: interpretability are not actually validated experimentally, other than on a synthetic problem showing that TaskMet learns to ignore dimensions of the targets that are artificial pure noise.

Limitations:
Yes, although addressing the differences in computational requirements (if any) would be helpful.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel method, called TaskMet, for learning a metric that can be used to improve the performance of prediction models on downstream tasks. The key idea is to use the task loss to guide the learning of the metric. This is done by using a gradient-based approach to optimize the metric parameters. The authors show that TaskMet can improve the performance of prediction models on a variety of tasks, including portfolio optimization, budget allocation, and model-based reinforcement learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The paper is well-written and easy to follow.
+ Motivations are clear and reasonable.
+ The proposed method is novel and has the potential to be widely applicable.
+ The authors provide extensive experimental results to support their claims.

Weaknesses:
It is not clear how sensitive the results are to the choice of hyperparameters. Please provide a more detailed analysis of the hyperparameter sensitivity.

Limitations:
Authors adequately addressed the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a method for end-to-end learning of metrics to train prediction models. It proposes a concept of task-driven metric learning. The main idea is to let the model focus on task-relevant features and dimensions in the prediction space. In addition, the resulting prediction model can be more interpretable, as the proposed method uses metric learning as a preconditioning step for gradient-based model training. Experiments have shown the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed idea of only modifying the metric that the prediction model uses for training is quite interesting. It enables the model to focus more on task-relevant features and dimensions without explicitly altering the optimal prediction model itself.  Hence, it allows the prediction model trained on its original prediction problem while being valuable to the downstream tasks.

- The results are promising. The proposed method has outperformed the other baseline approaches.

Weaknesses:
I'm not an expert in this field.
- The writing of the methodology is a bit difficult to understand. It would be great if the writing can be polished with more illustrations.

Limitations:
The paper has provided the limitations of the proposed method.

Rating:
7

Confidence:
2

";1
H9hWlfMT6O;"REVIEW 
Summary:
This paper presents a 4-bit training method mainly to speed up the training process. In the forward propagation, Hadamard quantizer is proposed to suppress the outliers whereas structure sparsity and bit splitting are used for quantization of gradients. It has been shown that the proposed training method can successfully train transformer models across different datasets with some performance degradation w.r.t. the baseline.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
-- The main strength of this paper is theoretical analysis of their proposed quantization method for both forward and backward steps.
-- Use of different models and tasks for evaluation purposes is another strength of this paper. 

Weaknesses:
-- The main concern of mine is the limited speedup of this paper. The main goal of quantized training in this paper is to speed up the computations and not to reduce the memory. However, the authors have only shown up to 35% speedup w.r.t. the baseline (FP16). First, there is no comparison with Int8 training speed. It's not clear why I should choose Int4 over Int8 while I know the accuracy of Int8 is better than Int4. Second, the speedup amounts are only measured across specific configurations which are not the ones used for evaluation purposes in Table 1. Measuring the training time for the tasks listed in Table 1 can make the contribution of this paper clear.
-- Fig. 4 and Fig. 5 are hard to read. Again, why not simply reporting the speedup for each task in Table 1.
-- I have a hard time understanding Table 3. What does ""epoch 1-5"" mean? Which dataset was used for the measurement? What is the accuracy performance?

Limitations:
NA

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose 4-bit training methods for Transformers. It first proposes a Hadamard quantizer with activation outliers issue being solved by Hadamard Transformation. Optimization for backpropagation is proposed by leveraging gradient sparsity with bit splitting and score sampling. The proposed 4-bit training methods achieve competitive accuracy on NLP, Machine translation, and Image classification.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* Achieves competitive or superior accuracy compared with existing works on 4-bit training.
* Optimize backpropagation with hardware-efficient structural sparsity.
* Hardware-friendly INT4 training method is runnable on GPUs with up to 2.2 times faster than the FP16 MM.

Weaknesses:
* The activation outlier handling in Transformers is not new, as in SmoothQuant. And the comparison is lacking in the paper. Are those techniques not able to be used in 4-bit training? And what is the efficiency comparison between those methods when handling outlier activations?
* Could you elaborate more on why the proposed methods cannot be applied to Conv layers? What is the bottleneck?

Limitations:
* Conv cannot be supported
* INT4 training on extremely large models remains to be an open question.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The research paper presents a framework for training transformer-based neural networks using 4-bit integers, offering enhanced computational and memory efficiency. 
The proposed method includes a dedicated Hadamard quantizer for forward propagation, designed to suppress outliers which typically cause degradation of model accuracy. The Hadamard quantizer works by transforming the activation matrix, spreading outlier information across nearby entries in the matrix, and reducing the numerical range of outliers.
In backpropagation, the authors exploit the structural sparsity of activation gradients. They note that a few token gradients are extremely large, while most are much smaller, even below the quantization residuals of larger gradients. To make efficient use of computational resources, the authors propose a technique called bit splitting. This technique divides each token's gradient into two parts: the higher 4 bits and the lower 4 bits. They then identify the most informative gradients using a technique known as leverage score sampling, an importance sampling technique in the field of RandNLA.
The combination of these techniques results in an algorithm that employs 4-bit integer MMs for all linear operations in transformers. This algorithm has been evaluated and found to perform competitively across a wide range of tasks, including natural language understanding, question answering, machine translation, and image classification.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Unlike many other low-precision training methods, this proposed method does not require custom numerical formats and is compatible with contemporary hardware, such as GPUs. This opens the potential for widespread adoption and usability.
- The proposed method demonstrated competitive or superior accuracy across a range of tasks, compared to existing 4-bit training methods. This indicates the practical effectiveness of the method in diverse applications.
- The authors have developed specialized quantization methods for both forward and backpropagation. These techniques manage outliers and leverage the structural sparsity of activation gradients, maintaining model accuracy despite the ultra-low numerical precision.

Weaknesses:
My main concern is that the paper chose to compare the performance and computation complexity of their approach with FP16 rather than examining its efficiency against INT8 or other INT4 alternatives. This could potentially limit the depth and breadth of the performance analysis.

The key contributions of this research are primarily focused on the implementation of a Hadamard quantizer and bit splitting techniques. Both techniques leverage INT4 matrix multiplication (MM). However, the HQ-MM has the 4 steps procedure to leverage the efficiency of INT4 computation and the bit-splitting technique deploys two INT4 MMs as a replacement for one INT8 MM. This can potentially induce more overhead than simply utilizing INT8, particularly considering that INT4 does not straightforwardly translate to half the computational, memory access, or energy costs associated with INT8 computations.

While the aim of utilizing INT4 computations is to increase efficiency, it's crucial to recognize that the benefits of moving from INT8 to INT4 are not always linear. According to a report by NVIDIA, INT4, in an optimal scenario involving Convolutional Neural Network (CNN) inference in image classification tasks, resulting in a throughput improvement of approximately 59% with a negligible accuracy loss (around 1%) on NVIDIA T4. Meanwhile, on TITAN RTX, the speedup was about 52%. These improvements, although significant, don't necessarily equate to halving the computational resources or doubling the performance, and this is also applied to the transformer-based models scenario of this paper.

Hence, a more thorough comparative study involving INT8 and other INT4 counterparts could potentially provide a more nuanced and comprehensive understanding of the computational advantages and trade-offs of the proposed techniques. Additionally, if there are any misinterpretations or inaccuracies in my understanding of the paper, I would appreciate it if you could provide corrections to ensure my perspective aligns accurately with the content and intentions of the research.


Limitations:
Implementing advanced quantization techniques such as the Hadamard quantizer and bit splitting may be technically challenging and potentially difficult to integrate into existing neural network training pipelines.
The method exhibits limitations in its scalability, as it is not equipped to support exceedingly large models like OPT-175B. This underscores a broader, unresolved issue in the realm of machine learning, where even the application of INT8 training to these substantial models remains an open-ended problem.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper deals with 4-bit training (fine-tuning) of transformers. For forward propagation, a Hadamard quantizer is proposed to solve the problem of outliers. For backpropagation, the authors leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling
techniques to quantize gradients accurately. Experiments show up to 35.1% speedup for 4-bit training with 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed Hadamard quantizer is simple while useful for transformer quantization with outliers.
2. The authors illustrate the structural sparsity of gradients and propose bit splitting (BS), which splits a full-precision matrix as higher and lower 4 bits. The similar idea is used in quantization for inference. This is the first to use it for training.
3. The experiments are extensive. The training speed on GPUs are reported.

Weaknesses:
1. The proposed method introduce extra FP16 computations. The BS with selection needs data rearrangement, which could slow down the speed.
2. The title is somewhat over-claim. The experiments are mainly about fine-tuning (FT). It seems that the proposed method works for fine-tuning (FT) of a converged model. For pre-training (PT), there is large accuracy loss. 
3. The speed-up of up to 35.1% is misleading. It is better to use the average speed-up in the abstract.

Limitations:
NA

Rating:
6

Confidence:
5

";1
cMUBkkTrMo;"REVIEW 
Summary:
This paper simultaneously addresses the label imbalance problem and uncertainty qualification capability in regression.
The authors propose to enhance the reweighting technique dealing with the imbalance problem in (Yang et al., 2021) to be applicable to VAE and combine the method with the output distribution and the corresponding loss in (Amini et al., 2020), which provides uncertainty qualification capability.
Experimental results on several real-world datasets demonstrate that the proposed method performs better than state-of-the-art imbalanced regression methods in terms of both accuracy and uncertainty estimation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
### Originality:
- The authors simultaneously address the label imbalance problem and uncertainty estimation capability in regression, which is a novel problem setting.

### Quality:
- The combination of the reweighting technique in (Yang et al., 2021) and the output distribution and the corresponding loss in (Amini et al., 2020) is non-trivial and works well.

- The authors showed the superiority of the proposed method experimentally in terms of both accuracy and uncertainty estimation, where they used multiple public datasets.

### Clarity:
- The presentation is clear.

Weaknesses:
- Good combination of the SOTA, but the originality can be limited because of that.

Limitations:
They discussed that the exact computation of variance of the variances is challenging in Section 5.3.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose a variational regression model for imbalanced data, which (1) borrows data with similar regression labels for variational distribution (neighboring and identically distributed: N.I.D.) and (2) utilize the conjugate distributions to impose probabilistic reweighting on the imbalanced data to give better uncertainty estimation. Experiments show the proposed model achieves the SOTA on several datasets.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- Imbalanced regression is a research area that is yet to be explored, although its real-world application is important.

- In addition, the proposed model can provide uncertainty of the prediction, which is important for real-world applications.

- The uncertainty estimation on imbalanced datasets is also an interesting field but is yet to be explored.

- Overall, the present topic is very relevant in the community. I encourage the authors to develop and explore this direction more, in view of the nice performance of the model.

- The performance of the proposed model is excellent. Both of the accuracy and uncertainty estimation outperform the SOTA models.

- The present paper is easy to follow and well-written. I enjoyed reading it. I found some nice ""road signs"" for the readers to follow the logic and story.

Weaknesses:
- The code for reproduction is not available. I would like to see and use your code. I could not find any other major weakness.

- See Questions below.

Limitations:
Limitations are included in Section 5.3.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposes a variational imbalanced regression model by taking the Neighboring and Identically Distributed (N.I.D.) assumption to solve both imbalanced regression and uncertainty estimation problems. Experiments on four imbalanced datasets demonstrate the effectiveness of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1) Compared with imbalanced classification,  imbalanced regression is underexplored and also an important topic.
2) The Neighboring and Identically Distributed (N.I.D.) assumption seems more reasonable than the Independent and Identically Distributed (I.I.D.) assumption.
3) The proposed method improves not only the performance of few-shot region, but also the performance of many-shot region.

Weaknesses:
1) The description of some parts of the paper is not clear, e.g., how to define the neighboring labels?  What is the detailed formulation of the importance weights in line 237?
2) Some statements in the paper are somewhat subjective and lack support, e.g., in line 42-43, the authors claim that ""This allows the negative log likelihood to naturally put more focus on the minority data"", but I do not find any ""naturally"" thing, the important weights are mainly determined by the kernel functions, which needs manually selection; in line 273, the authors claim that ""Such dual representation is more robust to noise"", but they neither list any references nor conduct any experiments to support it.

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this work, authors recognize that although the existing regression models for the imbalanced datasets have been mainly developed to improve the prediction accuracy, they overlooked the quality of the uncertainty estimation. In this context, authors propose a deep probabilistic regression framework, to improve the uncertainty estimation performance as well by combining the idea of [1] and [2]. 

Specifically, authors first consider multiple bins, splitted on the range of labels to get statistic of the labels. Then, they revise the latent statistic of VAE for the imbalanced datasets, by smoothing these features based on the statistic on each bins and then applying probabilistic whitening and recoloring procedure. Next, they use the revised latent features to get the posterior distribution of NIG distribution, which acts as the pseudo counts that alleviates the issue of imbalanced sets. Last, they employ these parameters for prediction and training.   

Empirically, authors demonstrate that the proposed approach can improve the performance of the prediction accuracy and uncertainty estimation on various datasets.

[1] Delving into Deep Imbalanced Regression - ICML 21

[2] Deep Evidential Regression - NeurIPS 20

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* This work extends the DIR of [1], as the probabilistic model, to estimate the uncertainty.

* This work considers to use the NIG posterior distribution, updated by the statistic of the stochastic latent feature. This seems to help balance the latent features of the imbalanced labels and thus yield the credible uncertainty for the imbalanced datasets. I believe that this is a novel part of this work as comparing [1] and [2].

[1] Delving into Deep Imbalanced Regression - ICML 21

[2] Deep Evidential Regression - NeurIPS 20

Weaknesses:
* Absence of ablation study
> In experiment section, it seems that the proposed method improves the prediction accuracy and uncertainty estimation. However, the current work does not investigate (1) whether each trick of the proposed method, such as the use of the stochastic latent feature (VAE) and use of the posterior distribution (NIG), is effective and (2) whether the proposed method is consistent up to the number of bins. 

* Less explanation on why the evidential regression model is used along with DIR, instead of using other BNNs.
> In general, BNNs is widely used to estimate the uncertainty. I believe that applying the BNNs with DIR could be a direct way to solve the targeted problem. However, author takes the evidential regression approach, without explaining the reason or its motivation. This seems to less persuade why the proposed method is reasonable. If authors provide the motivation of this approach or can demonstrate that the proposed method could outperform the results of the DIR, obtained by BNNs or other approach for uncertainty estimation, I believe that the contribution of this work would be more clear and strong.

Limitations:
See above Weaknesses and Questions.

Rating:
6

Confidence:
3

";1
19AgWnmyoV;"REVIEW 
Summary:
This paper considers the problem of instructing goal-conditioned RL agents to follow specifications expressed in Linear Temporal Logic (LTL) formulae. The proposed method works as follows. First, construct a Buchi automaton from the LTL specification, which is then converted to a directed graph representation. Then, use a weighted-graph search algorithm to solve for a high-level plan that satisfies the LTL specification, utilizing the value function of the goal-conditioned agent as a surrogate of the difficulties of achieving each goal. Finally, execute the high-level plan using the goal-conditioned agent. The proposed method is evaluated on three benchmark environments: LetterWorld, ZoneEnv, and Ant-16rooms. The method is compared with two baselines for learning LTL satisfying policies. It is shown to outperform the two baselines, as well as generalize better on out-of-distribution tasks.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
This paper focuses on a promising direction and targets an important problem of the field: how to learn/search policies that can generalize to complex, compositional task specifications, while using less or no additional training on the new tasks. The problem formulation of LTL specifications is a fruitful step towards this general direction, therefore of great potential significance. 

The proposed method of using Buchi automaton and weighted graph search (using value functions as weights to measure difficulty of achieving each goal) to solve high-level plans for LTL tasks is technically interesting and novel to my knowledge. It also makes sense intuitively and seems to be a good method for this problem. 

Weaknesses:
In my view, there are several improvements that needs to be made in terms of experimental settings and expositions of the paper before it is ready to publish here. 

- The proposed method considers the setting where low-level policies to achieve each goal are given, and targets the problem of how to solve for a high-level plan that can satisfy the LTL task specification. In this case, the baselines should be alternative methods for computing a high-level plan, with the same assumption that the low-level policies are given. Then the experiments can show how good the proposed method is in solving the problem it targets. It seems that the current baselines do not operate on the same premise (i.e., given low-level policies for each goal, how to solve high-level plan). 
- The writing of the paper could be improved to help with clarity. For example, it would be helpful to briefly introduce how the proposed method works and summarize the experimental results in the introduction section. There are also a few typos in the paper, e.g., line 70 “white zones”. 


Limitations:
I did not find a discussion on the limitations in this paper. One possible aspect for discussion is what to do when the task cannot be divided into high-level LTL solving and low-level goal achieving. For example, the case where how to achieve each goal is context dependent.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a new technique for multi-task RL when the tasks are specified using a high-level language (LTL in this case). The approach involves identifying a set of skills corresponding to a set of reachability and safety objectives and training policies for them. While training these policies (which are represented using a single goal-conditioned policy), a separate value function is trained to measure, for every pair of goals, the expected return for the task of reaching one goal from another. Then, given an LTL formula, a subtask graph structure is constructed which is used to compute a high-level plan for performing the task using the learned skills. Experimental results suggest that the proposed approach outperforms a state-of-the-art method for multi-task RL (for LTL tasks) and is better suited for multi-task performance than another compositional approach which is designed for single-task RL.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The ability to learn a set of skills that can be used to perform a wide range of long-horizon tasks specified using LTL is very useful. This proposed approach is a simple and natural way to achieve this.

- Although the idea of planning over a graph structure in order to perform a complex temporal task is not new, the paper provides a way to achieve this for all of LTL (rather than a subset of LTL considered in prior work) and furthermore applies the idea to multi-task RL. 

- The experimental results look promising and show that the proposed approach can be used in a wide range of environments to solve complex tasks without further training (after training the goal-conditioned policy)

Weaknesses:
- The main weakness, in my opinion, is that the approach doesn't seem to be general enough to handle all of LTL as claimed by the authors. For instance, the LTL task is eventually reduced to following a single path (with a loop at the end) in the automaton graph. But it might not be optimal to follow a single path and one might have to use different high-level strategies from different states of the MDP. Furthermore, the high-level plan is computed using the trained value function which does not consider the ability to stay safe and avoid triggering alternate transitions when measuring the ability to trigger a specific transition in the automaton. The heuristics for handling such avoidance constraints during test time seems reasonable but it is a bit ad-hoc and it is unclear why it is good in general (an ablation study might improve the paper).
- The clarity of the paper could be improved. Many assumptions are made throughout the paper (such as transitions using conjunctive predicates and goals being disjoint). It appears that some of these assumptions can be removed. A clearer presentation could help mitigate doubts about what assumptions are necessary.

Limitations:
As mentioned in weaknesses, there are some limitations to the proposed approach which should be discussed in the paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a method to transfer learned or planned goal-directed skills in domains to novel tasks represented by linear temporal logic within the same domain. The key idea of this paper is to train goal conditioned policies to achieve (and avoid) Boolean goals, and to compose them temporally to achieve temporal logic goals. 

The paper proposes to first convert the automaton corresponding to the LTL specification into a Buchi automaton, which is then converted into a directed graph with a target state. The algorithm also estimates the cost-to-go heuristic at each node to estimate edge traversal costs, and finally combines an optimal graph search along with learned goal conditioned policies to achieve the temporal logic goals. 

The authors primarily benchmark against LTL2Action where the specification is embedded into a feature vector using a graph neural network, and this embedded latent feature vector is used alongside a state-feature vector in a through a Deep-RL algorithm to compute the final policy. The authors demonstrate that their approach beats LTL2Action on range of randomly sampled tasks, and two specific avoidance tasks.  

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
**Sound problem definition**: The authors are correct in their statement that with competent goal-conditioned policies available to the agent, the agent can solve temporal LTL tasks through composition of these policies. There has been a lot of recent interest in this approach, and the authors have demonstrated that in two navigational environments that have been utilized in research on RL + temporal goals. There are however some issues with the assumptions made by the authors as I describe in the following sections.

**Evaluations**: The authors might have chosen just two navigational domains, but have focused on evaluating over a wide range of temporal logic formulas. Such evaluations are much more valuable in the space of RL for temporal tasks as is considered in this paper.

**Originality and significance**: Prior work suffers from lack of transferrability to all novel tasks as the library of learned skills is inadequate to transfer to all possible tasks. The authors propose to pre-train a goal conditioned skills that should offer more coverage of the logical transition-space. The idea is well demonstrated in the zones and letters environment in the paper, but there are some additional concerns that I highlight in the next section.

Overall, the idea of composing pre-learned skills to novel task scenarios is not original in and of itself. But the combination presented in this paper is novel to the best of my knowledge. However, there are elements of similarity with prior work that have not been addressed adequately 

Weaknesses:
**Positioning in context of prior work**: The core idea of skill reuse is not entirely novel. There are prior works addressing this [1],[2],[3] that appear to be missing from discussion. Infact both these works handle a wider variety of logical composition for the transition edges which appear to not be considered by this paper. While the core idea in this paper is distinct, it deserves to be discussed in context of these works that appear to handle the problem with greater generality.

**Correctness concerns**: The authors claim that their approach is applicable to all $\omega$-regular automata. However their approach appears to have a strong reliance on a single unique accepting state, in all the examples that they test on. Generalized Buchi acceptance condition requires that the system visit atleast one state in each accepting set infinitely often, and the approach described here is incompatible with such a specification. An example of this would be the patrolling task $\square \diamond a \wedge \square \diamond b$. Here an accepting run would require the agent to visit both $a$, and $b$ infinitely often, but this would not be discoverable by the graph search algorithm described here.

A second correctness concern relates to the type of edge transitions that can be accomplished by the goal conditioned policy. In general an edge transition in automata is described by the self-transition edge that is maintained until the transition trigerring truth evaluation is reached. The goal conditioned policy implicitly assumes that the trigger transition is reached as the first distinct transition in the truth values of the propositions. This might be true for navigational tasks, where each state has at most one proposition true, but may not be true in general when not all propositions are controlled by the agent, or even in cases where simultaneous satisfaction of multiple propositions might be required. For instance the specification $\diamond(a \wedge b)$. [2] would appropriately identify this specification as unsatisfiable, and abort task execution, whereas the behavior of the system proposed in this paper is uncertain. In particular, none of the goal conditioned policies are applicable to this outcome. In contrast [1] can compose the policies logically to satisfy the specification if it is indeed satisfiable. If we study the type of transitions occuring within automata, then there are many such edge cases pertaining to self-transition, and simultaneous truth value changes that cannot be handled by this approach. These limitations must be explicitly acknowledged in the submission.

**Difficulty of training goal-conditioned policies**: This approach relies on having a good goal conditioned policy to perform the task. However, this is in general a challenging problem, and I am not aware of any works that have managed to train competent goal conditioned policies beyond grid world domains that achieve good coverage over all possible logical goals. I would appreciate if the authors add some text to evaluate the quality of goal conditioned policies before using them for this algorithm, or point to works that address this issue.

[1] - Nangue Tasse, G., James, S. and Rosman, B., 2020. A boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems, 33, pp.9497-9507.

[2] - Liu, J.X., Shah, A., Rosen, E., Konidaris, G. and Tellex, S., 2022. Skill transfer for temporally-extended task specifications. arXiv preprint arXiv:2206.05096.

[3] - Xu, D. and Fekri, F., 2022. Generalizing LTL Instructions via Future Dependent Options. arXiv preprint arXiv:2212.04576.

Limitations:
I do not believe that the limitations are adequately identified and acknowledged. Please refer to the weakness and questions section.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of learning to solve a linear temporal logic
(LTL) tasks in a Markov Decision Process (MDP). Given a fixed Markov Decision
Process, this is done by:

1. Pre-training a goal-conditioned policy to solve a uniform sampling of reach-avoid tasks,
   where goals correspond to atomic propositions.
1. The input LTL sentence is translated into a Buchi automata.
1. The Buchi automata is transformed into a weighted graph.
   - Weights are determined using the value function of the pre-trained policy.
1. A path is generated by solving a sequence of shortest path problems.

The approach is then experimentally validated against the LTL2Action method using 
the prior works domain and concept class.


---- update ----

After re-reading and being pointed to Appendix section D, it seems my major concerns are accounted for. What remains is the question of how to incorporate this into the main text. As such I am increasing my score to erring toward accept.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The approach tackles an important problem. Namely, learning to solve sparse
tasks represented in a formal specification language defined over infinite runs
of the system.

Weaknesses:
1. The proposed approach is ultimately heuristic and is susceptible to being
   ""catastrophically myopic."" In particular, the greedy sequence of shortest
   path problems is necessarily biased toward solutions that work for finite
   horizons, but says nothing about the infinite time behavior, e.g., the ""lassos""
   generated by sequence of shortest path queries. It is not hard to imagine
   constructing an adversarial Buchi automata that uses a sequence of easy
   to reach accepting states to lead the agent into a long term bad position.

1. The paper claims to address infinite horizon specifications, but then
   compares against a regular language benchmark. This undercuts the stated
   motivation. For example, all of the base-line problems have a finite
   accepting prefix, e.g., as opposed to G(x -> F y).

1. The approach should be compared to hierarchical, meta RL, and compositional
   RL works. For example, the graph approach seems very similar to [1] but adapted
   to goal conditioned policies.
   
[1] Jothimurugan, Kishor, Rajeev Alur, and Osbert Bastani. ""A composable specification language for reinforcement learning tasks."" Advances in Neural Information Processing Systems 32 (2019).

Limitations:
See weakness 1.

Rating:
5

Confidence:
4

";1
TcG8jhOPdv;"REVIEW 
Summary:
The paper evaluates robustness of multi-exit language models against specifically perturbed datapoints that induce adversarial slowdown and controbutes to the literature on availability attacks. The paper targets language models, as opposed to the vision models that were explored in the existing literature. The paper presents WAFFLE and attack that forces the early exists to be avoided and ultimately slows down the computation. The paper then explores the constracted examples and finds that they appear slightly more out of distribution. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Interesting important setting

Weaknesses:
+ Unclear performance with respect to the related work

Limitations:
-

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper considers adversarial slowdown attacks on multi-exit text classification models based on BERT. They propose an attack, Waffle, which adapts text adversarial example attacks to a slowdown objective. They measure the susceptibility of multi-exit models for GLUE to their attack, evaluate attack transferability, analyze their generated adversarial text, and discuss mitigations for this attack.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The evaluation is quite broad in the classification tasks considered, multi-exit models used, and baselines. I also appreciated the transferability analysis and the ""UAP"".

I was interested to see the linguistic analysis of attacks. The linguistic markers mentioned seem plausible and I like the qualitative analysis in Table 3. However, I would like to also see some quantitative analysis of this property as well.

The paper is the first I am aware of to consider adversarial slowdown on text classifiers. This is a natural problem and may be of interest as text models become increasingly large.

Weaknesses:
Reading the paper, I was surprised that I never saw an experiment's running time measured, since this is the motivation for the attack. I think this is a pretty important consideration especially for defenses. If the running time of a defense (especially ChatGPT) is higher than the actual slowdown, there's not any point in applying the countermeasure (this is never discussed in the subsection).

The attack often creates incoherent text, as seen in Table 3. This seems to be a feature of text attacks, rather than a limitation specifically of the Waffle attack. However, it seems that the attack could be overfitting to a specific ""distance metric"" for the attack. Using some other distance functions, such as token/character edit distance, character replacement, as supported in TextAttack may also be useful to understand the generality of the attack.

ChatGPT is likely to have data leakage here, making it a bad scientific baseline. I would encourage at least a discussion of this. This may be one reason why the more standard grammar checking tools are unable to correct the attacks.

Limitations:
The running time of experiments is never measured, which is a limitation that is not mentioned in the paper. 

Data leakage in ChatGPT may also be a factor in the countermeasures.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This papper proposes WAFFLE, a slowdown attack to generate natural adversarial text bypassing early-exits. 
Empirical results show the robustness of multi-exit language models against adversarial slowdown.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

1.The paper is well-written and easy to follow.

2.The evaluation is comprehensive.


Weaknesses:

1.It seems reference[1] does similar slowdown attacks, but [1] works on computer vision domain. What are the differences between WAFFLE and [1]?

2.Does WAFFLE still work on non-transformer based architectures, such as LSTM?



[1] Hong, S., Kaya, Y., Modoranu, I.V. and Dumitras, T., 2020, October. A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference. In International Conference on Learning Representations.


Limitations:
The authors adequately addressed the limitations.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper evaluates the robustness of multi-exit language models against adversarial slowdown. The authors propose a slowdown attack that generates natural adversarial text to bypass early-exit points. They conduct a comprehensive evaluation of three multi-exit mechanisms using the GLUE benchmark and demonstrate that their attack significantly reduces the computational savings provided by these mechanisms in both white-box and black-box settings. The study reveals that more complex mechanisms are more vulnerable to adversarial slowdown. Adversarial training is found to be ineffective in countering the slowdown attack, while input sanitization with a conversational model like ChatGPT can effectively remove perturbations. The paper concludes by emphasizing the need for future research in developing efficient and robust multi-exit models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Proposed the first slow-down attack on language models.
- Evaluate the proposed methods on different architectures (i.e., early-exit mechanisms)
- Demonstrate the effectiveness of the methods in different threat models/attack settings (i.e., black-box, white-box)
- Analysis on generated adversarial examples is conducted to provide further insights into the vulnerability of the model
- Mitigation and defense methods are discussed to show that sanitization methods are more effective compared to robust training method.

Weaknesses:
- Since the slow-down attack has been demonstrated in vision task, the challenge of adapting it to language model is not clear.
- The proposed slow-down objective seems trivial in terms of novelty.

Limitations:
The authors did not have a section regarding the limitation discussion.

Rating:
6

Confidence:
4

";1
b4Tr8NWTDt;"REVIEW 
Summary:
The paper addresses the problem of multi-agent RL by using learned world models over multiple potential opponent policies. The technique uses a Dyna-style algorithm to train the core policy with a combination of experiences generated through a world model and experiences playing against opponents. One evaluation demonstrates the world models benefits from training on data from multiple distinct policies. A second evaluation compares ways to use experiences generated from a world model to train a policy, showing pretraining on purely generated experiences is effective to warm-start a policy. An ablation study compares the proposed Dyna-PSRO model to vanilla PSRO on three MARL games, showing improvements over the PSRO model.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
## originality

Modest. The paper extends existing lines of work on MARL and world models, specifically studying the question of the policy diversity for training the world model.

## quality

Modest. The core results (figure 5) show clear improvements over PSRO. This is limited to a small number of games and the games themselves are relatively simple game domains.

The paper does a good job of breaking down specific claims to isolated experiments.

## clarity

Low. It was difficult to interpret many of the figures (questions and suggestions below). Generally the results of each experiment were hard to understand and would benefit from a single clear statement of the core outcomes in each section.

## significance

Low. The core audience of this work is researchers in MARL and particularly those considering world models as a solution.

Weaknesses:
The experimental results are promising, but would benefit from expansion. There are a few experiments that would help:
1. More games from MeltingPot. I hate asking simply for ""more"", but in this case it would help to show how well the agents perform on a wide variety of tasks. The results would help clarify where DynaPSRO benefits and may reveal limitations or areas for improvement. The wider set of results would give others confidence in the generality of the improvements gained by planning against diverse other agents.
1. More complex games. Consider more complex environments from PettingZoo (https://pettingzoo.farama.org/) or SMAC (https://github.com/oxwhirl/smac) that would highlight the potential of these algorithms in more compelx scenarios. This would help address the point that world models can become unstable and the value of strategic diversity in scenarios that support a much wider array of behaviors.
1. Scaling experiments. For example, when do prediction improvements level off when adding more policies? The experiments only examine adding 2 policies, which is a sparse sample of the space of strategies for most games.

The evaluations would benefit from other baselines to compare. What other algorithms could be used aside PSRO?

The full evaluation (last experiment) would benefit from a set of ablation studies. This could easily replace some of the planning experiments as the ablations would examine similar capabilities. I ask for ablations as these will be more convincing that the parts of Dyna being used add benefit over PSRO.


Limitations:
Yes. The work is focused on integrating world models into game playing agents and recognizes the preliminary nature of this work along with the potential risks introduced by using a simulation (world model) for decision making in real world applications.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to PSRO algorithms, where a world model of the environment is learned concurrently to the iterative PSRO strategy expansion. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors are right that the problem of having to re-learn policies from scratch is a large problem in the PSRO literature. Therefore, the idea to co-learn a world model alongside the expansion of the empirical game, in so taking advantage of the diversity of experience created by agents with slightly different best-response targets is an interesting approach to this problem. To the best of my knowledge this is also a novel solution to this problem.  
- I really like the presentation of the paper, and in general I think the authors do a very good job in terms of analysing the different moving parts of the framework in a reasonable manner.


Weaknesses:
I have a few concerns with the paper, however none of these necessarily game-changing in my evaluation of the work.

- I think the greatest misgiving I have with this work is that the related work seems to miss quite a large collection of PSRO papers that probably deserve mentioning. PSRO-style algorithms is a fairly small research area and I am surprised that the authors fail to make mention to many variants. In particular, as there is a section on strategic diversity itself in the paper, it seems odd that the authors have failed to comment on the line of works on diversity-based PSRO frameworks. For example, [1], [2], [3], [4], [5] are all diversity PSRO approaches. It also fails to place itself in the literature involving PSRO algorithms that attempt to speed up convergence times such as [6], [7].

- Furthermore, I was additionally surprised at the lack of comparison to NeuPL [8] which is another population-based framework attempting to similarly deal with the best ways to transfer information between agents in the population. 

- I do not necessarily believe that the authors need to benchmark against all of the approaches that I have listed. I do however believe the paper still needs work in terms of placing itself within the current literature on PSRO and other population-based frameworks.

- Based on the above, my score is set at a borderline accept. However, I am willing to revise this upwards upon seeing a better framing of this work in the current literature.

REFERENCES  
[1] Policy Space Diversity for Non-Transitive Games - Yao et al. 2023  
[2] Open-ended learning in symmetric zero-sum games - Balduzzi et al. 2019  
[3] Modelling behavioural diversity for learning in open-ended games - Perez-Nieves et al. 2021  
[4] Towards unifying behavioural and response diversity for open-ended learning in zero-sum games - Liu et al. 2021  
[5] A unified diversity measure for multi agent reinforcement learning - Liu et al. 2022  
[6] Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games - McAleer et al. 2020  
[7] Neural auto-curricula in two-player zero-sum games - Feng et al. 2021  
[8] NeuPL: Neural Population Learning - Liu et al. 2022  



Limitations:
I think the authors actively engage with the limitations of the work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors consider learning world models for deep reinforcement learning in combination with the construction of empirical games through PSRO. They first show that world models benefit from training on a diverse set of strategy profiles as can be generated through PSRO meta-game solvers. They then empirically show that PSRO best responses can enjoy sample efficiency benefits by training with simulated world model experience. Finally, they present Dyna-PSRO, in which PSRO best responses make use of a world model trained on all available experiences collected thus far in a run of the PSRO algorithm. Dyna-PSRO provides lower-regret solutions with higher sample efficiency than PSRO without a world model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well written and presented, and the experiments are well designed. 
- World models are seeing increased use in the RL community, and PSRO is one of the more practical and general methods currently available for finding approximate game solutions. This paper provides insights on how to properly combine the two and make improvements to PSRO's sample efficiency, which is one of its largest issues.
- The proposed Dyna-PSRO method is sound.
- While many implementation details are not present in the main paper, the appendix describes these details thoroughly.

Weaknesses:
It would have been nice to see how current high-performing world model methods such as Dreamer, which employs latent state spaces [1,2] might perform with the same approach. It's not immediately clear if experiments like in section 3.3.2 would have had the same outcome.

[1] Hafner, Danijar, et al. ""Dream to Control: Learning Behaviors by Latent Imagination."" International Conference on Learning Representations. 2019.

[2] Hafner, Danijar, et al. ""Mastering diverse domains through world models."" arXiv preprint arXiv:2301.04104 (2023).

Limitations:
All limitations have been adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper describes combining two things: training a world model of a game, and doing Policy Space Response Oracles (PSRO) on the game.

Doing PSRO involves getting a lot of episodes from the game (episodes are used to train the RL best-responses, and also to estimate the payoffs of the empirical game). The novel algorithm in this work (Dyna-PSRO) can be thought of as a modification of PSRO where those episodes are **also** used to train a world model (which is essentially a learned simulator of the game engine). Then, the world model is used to improve the training process of the best-response policies. 

Through experimental results, the authors show that this improved training process (based on Dyna) can cause the best-response learners to learn a stronger policy than the normal method when using the same amount of interactions with the real game environment. It does this by training the policy using trajectories from the world model (in addition to the usual trajectories from the real game environment), and by equipping the agents with one-step lookahead planning during training.

This paper showcases experiments on the Dyna-PSRO algorithm in three games, and Dyna-PSRO outperforms PSRO in all three, as measured by an approximation of NashConv.

The paper also shows experiments to measure the quality of the learned world model, to test the hypothesis that Dyna-PSRO results in a good world model.

I think the paper has some flaws* in its current form, but the core work of the paper is good, the charts are beautiful, and the results are strong.

---

*Edit: many of the abovementioned flaws were addressed during the rebuttal/discussion period.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Overall, the paper is well-executed.
    - It is well-written and polished.
    - There has clearly been a lot of time and effort put into the engineering and writing of this paper. There are 4 sets of thorough experiments in the main paper, and more in the appendix.
    - The figures are extremely readable.
- The results concerning the performance of the best-response policies are impressive.
- The effort will surely be helpful to future researchers: the research directions of (1) improving the efficiency of PSRO response calculations and (2) training better world models should continue to flourish, and the work presented in this paper contributes to both.
    - The research direction seems natural, especially in the direction of using world models to improve the performance of PSRO.


Weaknesses:
I think the paper could be better in explaining or hypothesizing the ""why"" for a lot of things, even if just qualitatively.

I think the paper states some conclusions too strongly.

I think some things are not explained well enough and are confusing to the reader (at least, to me):

- SumRegret metric:
    - It's really not clear from the main paper how SumRegret works (even though it is explained in the appendix). This could be clarified by defining the terms ""method"" and ""combined game"".
    - Also, I would feel a lot better if I saw results measured by an alternative metric, where the deviation set is the set of **all** policies. The $max_{\pi_i \in \bar{\Pi}_i}$ could then be approximated by just training one more response policy (as if doing one last epoch of PSRO). This seems like it would be a more accurate approximation of the Nash Conv. Is there any reason to use the metric in the paper instead of this?
- Empirical Game Solution not described
    - Since the settings here are general-sum, it's probably important to specify what solution concept is used for the meta-strategies in the main paper (even though it is included in the appendix).
- Experiments in Section 3.1 Strategic Diversity
    - Looking purely at Figure 2, the conclusion ""Overall, these results support the claim that strategic diversity enhances the training of world models"" does not ring true to me. For example, there are three world models which perform better on the metric (accuracy) used in Figure 2 than the most diverse one, for Observations.
    - Even if I look in the appendix at E.1, there doesn't seem to be significant evidence to support the conclusion: multiple world models have similar recall scores than the most diverse one, and the one trained without the random policy seems to have better scores.
    - I would be interested in seeing the cross-entropy loss instead of (or in addition to) the accuracy.
- The discussion of the Decision-Time Planning results (3.2.2) seems incomplete:
    - ""**The main outcome of these experiments is the observation that multi-faceted planning is unlikely to harm a response calculation,** and has a potentially large benefit when applied effectively. These results support the claim that world models offer the potential to improve response calculation through decision-time planning."" (emphasis mine)
    - However, Figure 4 does show that decision-time planning causes the response to be *worse*: The solid blue line (top) has no decision time planning, and the dashed gray line (second from the top) has decision-time planning, and performs worse.
    - It would be nice if there was some discussion about this, perhaps an intuitive/qualitative reason why this is.
- Dyna-PSRO results need more details (Figure 5):
    - For each experiment, how many policies (iterations of PSRO) were there?
    - Does each policy train for a fixed number of steps, or until some measure of convergence is reached?
- Was ""policy"" vs. ""strategy"" ever defined like this before? In my opinion, we shouldn't define these terms like this, because they are usually considered synonymous. The terms I'm familiar with are ""policy"" or ""strategy"" for the former, and ""meta-policy"" or ""meta-strategy"" or ""meta-strategy distribution"" for the latter. Just my opinion!
- I was very confused by the definition of World Model while reading the paper.
    - Even after reading it through entirely, I was under the impression that each player had their own world model, and that it implicitly modeled the actions of the opponent.
    - If one misses the bold notation of the definition of agent world model from line 137 to 141, it's easy to think that this is the case, especially since the phrasing is that ""the agent learns and uses a ... world model"" (instead of, say, ""the agents learn and use a ... world model).
    - On one hand, the formal definition given for an ""agent world model"" is technically accurate, and I am just dumb. On the other hand, I suspect many of us are dumb, and will be similarly confused upon reading the paper. (Also, I'm not **that** dumb: it's really hard to tell that the O and A are bolded!) (Also, even if some of us are not dumb, we are likely lazy and will gloss over the explanation that boldface means joint.) This is all to say that I would suggest explicitly stating that the world model takes as input an observation and action from **each** player, and returns an observation and reward to each player. And that the world model does NOT model the actions of any player.
- The bolding is nice, but it would be less confusing to **also** say ""strategy profile"" or ""joint strategy"" anytime this is meant instead of just ""strategy"" and something bold, as it's very easy to miss or forget what something bold means (plus, it seems incorrect to call a strategy profile a strategy). Also, maybe emphasize that sentence that explains what boldface means, so that readers don't miss it?
    - For example in line 774 and 775 of the appendix:
        - ""This is typically not tractable, but instead draws are taken from a dataset generated from play of a behavioral strategy **σ**. And the performance of the world model is measured under a target strategy **σ∗**""
        - should be ""strategy profile"" and ""target strategy profile""
    - and same in Line 159 and 160 of the main paper, and throughout section 3.1


Limitations:
Limitations addressed

Rating:
6

Confidence:
4

";0
VtkGvGcGe3;"REVIEW 
Summary:
This paper describes a test battery to test for the emergence of cognitive maps and planning abilities in LLM. The tests are based on existing cogsci experiments converted into text prompts. For example, to test for a planning ability, the prompt first describes an apartment layout, and then asks the model to plan a path through several rooms to retrieve something. The authors show that LLM are poor at completing such tasks.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
I think the adaptation of cogsci planning experiments to text prompts is a great idea. There needs to be more tests like this to map the scope of LLM capabilities.

The authors evaluate multiple prompts from the same generative model and compute statistics of LLM responses. This approach stands in contrast to many arguments for emergent intelligence in LLM, which derive from anecdotal examples. It is important to do actual experiments with LLM.

Weaknesses:
This is a presentation issue, but it affects my ability to understand and evaluate the paper. The writing is unedited. There are a lot of unnecessarily long sentences that can be condensed for pragmatics and readability. The grammar and punctuation are weird, like this might be an initial draft. I had to read the same content multiple times, and missed important details throughout the paper. 

Figures are odd, seem to be just thrown in together from unrelated bits. For example Figure 1 shows examples of graphs A-E. Are these all the graphs used in Experiment 1? If so, this is not stated. Weirdly, each graph is shown in a different pictorial style. I'm sorry, but this looks like someone just downloaded different types of graphs from google search, panel E has a random red outline around it.

Experiment design is not given. I can not check the statistics without the experiment design. What are these degrees of freedom? It looks like multiple regression models were built? I think the intention is a good one, but it is not clear what was done

Limitations:
I can not make out what was done exactly, but am happy to read the next draft. The experiments and statistical analysis need to be described so that people could reproduce them.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper evaluates LLMs on a set of tasks that could be solved by cognitive maps, such as goal-oriented planning, or incorporating shortcuts. The work finds that LLMs generally perform poorly at these tasks, and their performance is affected by features such as graph sparsity. 

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
* The paper is admirably thorough with experiments and analyses:
    - Assessing a range of LLMs (including varying parameters such as temperature).
    - Evaluating across a range of different graph structures, task paradigms, etc.
    - Creating new stimuli to avoid dataset contamination. 
    - Performing regression analyses to determine how different features contribute to model performance, and describing the failure modes observed.
    - These thorough results and analyses suggest that the conclusions are likely to roughly generalize to some extent, and help the reader to understand which features will affect performance.
* The results are interesting, there are a variety of patterns that could be investigated further.

Weaknesses:
My primary concern is that the overall framing of the paper is misleading. In particular, the work is motivated with references to the cognitive and neuroscience literature on cognitive maps. This work performs  draw strong conclusions from its experiments such as ""no evidence for understanding cognitive maps or planning."" Are conclusions such as these justified?

A key issue in analyzing AI in comparison to human or animal capabilities is determining where a performance failure originates: is it a lack of an underlying capability (such as the ability to form a cognitive map), or a more superficial performance issue? Several recent papers have emphasized this point from a cognitive science perspective, and argued that it is essential to ensure fair comparisons between AI and natural intelligence to draw accurate conclusions (https://www.pnas.org/doi/abs/10.1073/pnas.1905334117; for LLMs specifically see: https://arxiv.org/abs/2210.15303).

In that context, it's worth noting that the animal and human experiments cited involved a great deal more experience before the map was tested than the present experiments do. For example, Tolman's latent learning experiments involved the rats fully exploring the maze for multiple days before they were tested with a food reward at the end (and even then, performance continued to improve well after the rewards were first introduced). Or Schapiro's temporal community structure paper involved half an hour of exposure to transitions from the graph; that is, thousands of transitions from a graph with only 15 nodes. This a much denser sampling of experience than the current LLM experiments afford, and it is quite possible that some degree of repeated experience contributes to the ability of natural intelligences to form a cognitive map. The difference in learning conditions is briefly mentioned to in the limitations, but is quickly dismissed; however, the difference in experimental conditions is a fundamental challenge to concluding that LLMs are failing to form cognitive maps like animals/humans do.

Likewise, it is typical in cognitive science to report comparisons to chance-level performance, and an underlying ability is usually inferred from better-than-chance performance, even if that performance is imperfect. For example, in Tolman & colleague's studies, the rats continued to improve for several days after the reward was introduced (that is, their paths were not optimal on the first test), and the rats were clearly stitching together trajectories that they had observed (since there was only a single route through the maze, everything else lead to dead ends), but we still interpret their performance as showing latent learning. It would be useful to report chance-level performance across all conditions, and, in the case that a model performs better than chance across a broad range of conditions, that would suggest some underlying ability, even if it is imperfect. For example, GPT-4's performance seems reasonably high across most conditions in Table 2 (though certainly imperfect).

In addition, explicit comparisons to human performance on these tasks (presented exactly as they are presented to the language model), would strengthen the claim that language models are failing in a fundamental way that humans or animals would not.

These points seem critical to the overall framing of the paper, and also to much of the discussion. I therefore think the paper would be substantially improved by:
1) providing a more nuanced framing of the very interesting results, that suggests they emphasize some limitations of planning in LLMs, without making overly strong claims such as ""no emergent planning"" or ""no evidence of cognitive maps""
2) Providing explicit comparisons to chance-level (and ideally human) performance to help contextualize the results.

Limitations:
See weaknesses; I believe that the limitations of the experimental paradigm not matching the inspiration are not fully discussed, and that more generally the conclusions are not fully supported by the experiments presented.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents CogEval, a set of best practices ported from cognitive science on how to do behavioral evaluations. The authors also transcribe new tasks from human reinforcement learning and planning into text, such that LLMs can be tested on them. On these tasks, the authors do not observe evidence for an emergent capability for planning in large language models.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- The presentation of CogEval is clear and potentially useful for the ML community. I’m excited to use CogEval in my own work.
- Thorough and thoughtful discussion.
- Exhaustive experimental setup—a feature perhaps enabled by the principled CogEval framework!
- The paper is lucid overall and easy to read.

Weaknesses:
There’s some mild overselling in the paper, given the empirical results, which only demonstrate the failure of an existence proof.  Defining the conditions under which we can declare definitively that there is no emergent planning. However, given that we have not rigorously defined these conditions, for example, “No Emergent Planning” in the title seems too strong. 


Limitations:
The authors have adequately addressed limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes an evaluation of large language models with respect to their ability to solve problems that require use of latent cognitive maps. Evaluation focuses on different underlying graph structures, and the influence of chain-of-thought inference on performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* I really liked the very clear outline of the motivation of the research design in the introduction of Section 2.
* The experiments are extremely thorough, and the goal of designing experiments with statistical robustness in mind is good.
* Good discussion around the capabilities and limitations of LLMs

Weaknesses:
Some of the presentation could be refined:
* The description of Figure 1 is somewhat difficult to understand. References to future aspects of the paper (e.g., ""Experiment 3"") are undefined, which makes it more difficult to understand.
* The figures / tables should appear closer to where they are referenced in the text.
* I'd suggest reordering 2.1, so that the tasks (i.e., maze learning) are described before the experimental setup.
* Some details could use more context. e.g., what is temperature? What are the graph structures A/B/C... etc? 
* The discussion on BFS/DFS prompting should be moved to the experimental setup section (2.3). I also didn't quite understand the distinction between these two; an example would help.
* Formatting of Figure 3 can be improved
* What is ""dialogue"" referring to throughout the paper? I don't believe an actual multi-turn dialogue is taking place during the evaluation
* Text of Table 2 is really tiny

Limitations:
Yes

Rating:
7

Confidence:
3

";1
bbbbbov4Xu;"REVIEW 
Summary:
This paper aims to solve the task of 3D instance segmentation by leveraging pre-trained 2D instance segmentation models. The authors propose a novel approach to lift 2D segments to 3D via a neural field. This idea is not completely new [38, 51], but the authors propose a contrastive loss that replaces the Hungarian-algorithm-based loss used in [38]. Moreover, the authors propose a synthetic dataset where the number of objects can be controlled, and show that the Hungarian-algorithm-based loss slows down substantially as the number of objects increase. Finally, the authors augment the contrastive loss with a momentum-teacher component (similar to [16]).

_[16] Olivier J Hénaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman,
João Carreira, and Relja Arandjelović. Object discovery and representation networks. In Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part
XXVII, pages 123–143. Springer, 2022._

_[38] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulò, Norman Müller, Matthias Nießner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. arXiv.cs, abs/2212.09802,
2022._

_[51] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi S. M. Sajjadi, Etienne Pot,
Andrea Tagliasacchi, and Daniel Duckworth. NeSF: Neural semantic fields for generalizable semantic
segmentation of 3D scenes. arXiv.cs, abs/2111.13260, 2021_

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
S1) The proposed scheme to deal with instance label ambiguity is novel.

S2) The paper is mostly well-written.

S3) The authors show that the proposed contrastive momentum-teacher loss gives good performance and is computationally lighter than the Hungarian-algorithm-based loss.

S4) The problem of adapting 2D instance segmentation methods to 3D is useful to real-world applications.

Weaknesses:
W1) The notation has some issues, is confusing in some places, and deviates a bit from prior work. 
- It is not common to let $y$ represent a mapping. It is common to say that $f$ is a mapping such that $y = f(x)$. Same goes for $Y$. The label $y(u)$ is sometimes referred to as a label and sometimes as a function, which is a bit confusing.
- I think $u$ should be the location of a pixel, rather than the actual pixel (which would be in $\mathbb{R}^3$).
- $Y(x)$ is not presented as a function of $x$, but as a function of $u$.
- $\Theta$ is introduced as both a mapping $\Theta: \mathbb{R}^2\rightarrow \mathbb{R}^D$ and $\Theta: \mathbb{R}^3\rightarrow \mathbb{R}^D$.
- Also $\rho$, $c$, and $\Theta$ are introduced as both mappings and actual elements of the codomain of the mapping.
- Usually, $x$ and $d$ are used for 3D location and ray direction. While it is not a crime to change notation, I cannot see a reason for replacing $d$ with $v$.
- Equation (1) does not link to the input to the nerf ($x$ and $v$). The link can only be found in the text in l128-l129. It would be easier to understand if $R$ was actually described as a function of $u$.

W2) The name Slow-Fast is confusing.
- The loss component name slow-fast could have been named _student-teacher_ or something with _exponential moving average_. Currently, the name makes it easy to confuse with Slow-Fast [1001].

W3) The prior work [38] also adapts 2D instance segmentation models to 3D instance segmentation using NeRFs. The proposed approach replaces the matching-based loss and the ""slow-fast""-component seems necessary for good performance. Is the comparison to [38] completely fair? Except for the proposed changes, are all other things equal, e.g., the underlying instance 2D segmentation approach, NeRF architecture, or training scheme? I find this difficult to tell from the paper text. It is clear that the proposed approach yields good performance and trains faster than [38], but it is not clear whether this is purely do to the proposed changes.

Minor remarks
- ""much smaller"" on l62 should probably be removed.


_[1001] Feichtenhofer, Christoph, et al. ""Slowfast networks for video recognition."" Proceedings of the IEEE/CVF international conference on computer vision. 2019._

Limitations:
The authors provide a discussion that clarifies some important aspects, for instance that the proposed approach supports only static scenes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel Contrastive Lift method for 2D segment lifting to 3D reconstruction and instance segmentation. The authors fuse multiview representations obtained from pre-trained 2D models into a unified 3D neural field. They propose a scalable slow-fast clustering objective function that enables segmenting without an upper bound on the number of objects. Additionally, a new semi-realistic dataset is created to evaluate the proposed method, which demonstrates superior performance compared to previous state-of-the-art approaches on both public datasets and the newly introduced dataset.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The research motivation and key challenges are generally well illustrated and summarized. 
2. The newly constructed framework and dataset fits well within the 3D vision field, particularly the flexible and scalable design for lifting 2D segments to 3D.
3. The proposed method is technically sound and achieves SOTA performance on the newly proposed dataset and challenging scenes on public datasets.

Weaknesses:
1. The paper lacks a discussion on the complexity of the proposed method and its potential impact.
2. Ablations on different 2D segmenters are not included in the paper.
3. The analysis of the ablations and their corresponding results is insufficient.

Limitations:
Please refere to Weaknesses.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper utilizes contrastive learning for lifting 2D segments to 3D and fuses the learned embedding by means of a neural field representation, namely Contrastive Lift. The authors further propose a slow-fast clustering objective function, which makes the method scalable for scenes with a large number of objects. To further validate the ability of the method, this paper also introduces a new dataset, Messi Rooms, which includes up to 500 objects as a benchmark for instance segmentation with a large number of objects. The experiments show that the proposed approach outperforms former SOTA on ScanNet, Hypersim, Replica, and Messy Rooms.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The proposed approach employs a low-dimensional Euclidean embedding to represent a 3D instance. The dimensionality D is far less than the number of objects L, making the approach more efficient and easily extended to larger numbers of objects. Most importantly, using the 3D instance embedding implicitly ensures multi-view consistency. It avoids the assignment problem that exists in Panoptic Lifting [43].

- Using contrastive learning and the clustering strategy makes the proposed approach independent of the number of objects, which is more suitable for scenes with a large number of objects.

- The slow-fast contrastive learning is scalable for different object numbers and stabilizes the training phase. And the proposed concentration loss ensures the concentration of embeddings within the same cluster. It improves the more complete instance segmentation results.

- The proposed semi-realistic dataset, Messy Rooms, provides a novel benchmark for testing the performance on scenes with large object numbers. 


Weaknesses:
- In Tab. 1 and Tab.2, the metric used for evaluation only uses PQ^scene^, which is cherry-picked. In Panoptic Lifting [43], mIoU and PSNR are also used for evaluation. A comprehensive comparison according to different metrics should be included.

- For semantic segmentation, Contrastive Lift should append a new branch to predict the semantic labels and be supervised by the segment consistency loss in [43]. I suppose that the model should be trained specifically for semantic segmentation. Panoptic Lifting can predict semantic and instance labels simultaneously. I am curious about whether the additional supervision on semantic labels would influence the instance segmentation results. Please explain it. 

Limitations:
The authors have addressed some limitations in Sec. 6. There is no negative social impact in this work. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The proposed method tries to solve the problem of reconstructing a 3D scene together with the underlying instance segmentation. Prior work required either GT tracking data or concurrent work a less efficient way to assign instances. From a set of images a Neural Radiance Field is reconstructed together with a feature field, that represents an embedding of the instance. Instance embeddings are guided by a contrastive loss function, that pushes embeddings that are projected into pixels from the same segment in a 2D segmentation mask closer and embeddings projecting into different masks apart. To improve the stability of the training, the authors propose an additional loss with a jointly trained slowly-updated instance embedding field updated with a moving average over the parameters of the faster field instead of SGD.
Instances are later computed by clustering embeddings, which is supported by the third loss term, which uses an average embedding from the slowly updated field to penalize the difference for the fast-field predictions. Specific values in the embedding vector are assigned to semantic classes for semantic segmentation and are directly supervised with the 2D semantic maps.  
Additionally, the authors propose a novel dataset with up to 500 objects for evaluating future 3D instance segmentation 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
So far 3D instance segmentation methods, such as Neural Panoptic Fields (referenced by the authors as well) required a tracking algorithm or GT tracking data to reconstruct instance labels of the 3D scene and this method presents a light, optimization-based approach, that directly learns an instance embedding from semantic maps through alignment. 

The authors describe their method in a way that is understandable and design choices, especially for the loss function and the learning paradigm are reasonable and justified by ablation and additional experiments.

In general, this is a well-designed method that leverages the current state of the art and adds interesting new components to allow a jointly learning of the radiance field, instance and semantic embedding.

A big plus of the presented method is the additional dataset with up to 500 objects and the submission of the code, that allows the reproducibility of the results.


Weaknesses:
While the presented evaluation of the proposed dataset shows a clear advantage over state-of-the-art methods on existing and their own synthetic dataset, as well as ScanNet, methods like Panoptic Fields have also shown results on complex outdoor scenes, such as the KITTI dataset. Therefore an evaluation in such a complex outdoor setting would further strengthen the paper and usability in future work.

Limitations:
The authors address most of the limitations I can think of and ablate the use of different methods they rely on, such as the clustering method in the supplement.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies the 3D object instance segmentation inside a 3D NeRF space. Specifically, to train the model, conservative loss between features generated by slow and fast NeRF models are computed to 1) maximize the feature distance between different semantic regions, 2) minimize the feature distance within the same semantic regions. Also, a dataset is introduced namely Messy Room, which consists of rendering real captured objects from Google Scanned Objects. Experiments on the dataset show a reasonable improvement in comparison to baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposed contrastive learning for 3D semantic segmentation on NeRF is elegant and novel. I would believe such a structure is considered to be better in comparison to previous works that directly output the semantic labels from the NeRF network. Also, the introduced messy rooms dataset is believed to be useful for the community, despite the dataset is partially synthetic. The overall performance improvements from the baseline of the proposed method are not huge but still reasonable.

Weaknesses:
1) The proposed method is only evaluated on the half-synthetic dataset with small objects on the table. It is necessary to evaluate the proposed method on some real images, at least qualitatively.

2) The performance of the segmentations before lifting is not reported in the experiment section, it is unclear how the proposed method improves from the 2D segmentation.

3) The contrastive training pipeline is partially similar to this work [1], it would be better to include it in discussion.

[1] Bai, Y., Wang, A., Kortylewski, A., & Yuille, A. (2020). Coke: Localized contrastive learning for robust keypoint detection. arXiv preprint arXiv:2009.14115.

Limitations:
The authors include and discuss limitations of this work. There seems no potential negative societal impact.

Rating:
7

Confidence:
4

";1
VgQw8zXrH8;"REVIEW 
Summary:
This paper presents Uni-ControlNet, a model that aims to enhance Text-to-Image diffusion techniques by allowing the concurrent use of multiple local and global controls. It only requires two additional adapters, regardless of the number of controls used, and circumvents the necessity of training from scratch. The authors assert that Uni-ControlNet performs favorably in terms of controllability, generation quality, and composability.  

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method allows for the simultaneous utilization of different local and global controls within one model, making it flexible and composable.

- It eliminates the need for training from scratch, reducing costs and making it suitable for real-world deployment.

Weaknesses:
-	Limited Novelty and Contribution: The paper's primary contributions are its condition injection strategy and training approach. However, the condition injection strategy appears to be derived from SPADE, and the training strategy seems to be based primarily on empirical evidence, without providing much novel insight or theoretical explanation.

-	Insufficient Detail in Discussion: The description of the training strategy and the inference process, stated as major contributions, are not sufficiently clear. It remains unclear how the authors handle other conditions when only one condition is being utilized. It seems problematic to set the local conditions' values to zero with the intent of rendering them empty. Moreover, it seems that Uni-ControlNet cannot handle multiple conditions of the same type, as suggested in Figure 2 of the Supplementary Materials.

-	Incomplete Comparisons in Experiments: The experimental comparison does not seem comprehensive. For instance, it's known that Stable Diffusion 2.1 unclip can also accept CLIP image embeddings as inputs, like global condition in this paper. Additionally, ControlNet can accommodate multiple conditions. Furthermore, the quantitative results do not appear to be particularly impressive - with the method achieving the best result in only 4 out of 8 instances in Table 2, and only 2 out of 8 in Table 1.  Same for CLIP score in Supplementary Materials.


Limitations:
yes.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes Uni-ControlNet that leverages lightweight local and global adapters to enable precise controls over pre-trained T2I diffusion models. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well written and organized.
2. The idea of local/global adapter to achieve all-in-one control is reasonable and interesting.
3. The results seem good.



Weaknesses:
Since this paper is easy to follow and self-consistent, I have only a few minor questions:

1. Please compare the training costs of Uni-ControlNet with those of other methods (T2IAdapter, ControlNet).
2. I notice that the different conditions are concatenated as inputs to the adapter. If we want to add other control conditions, does the adapter need to be retrained?

Overall, although this work has some limitations, I think it meets the bar of NeurIPS.

Limitations:
The authors have adequately addressed the limitations. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposed a method to do controlleble t2i generation from a pretrained diffusion model. The main contribution is that they only have two adapters one local (e.g., edge map, keypoint etc) and one global (e.g., image). For local, they use the controlnet, but concatenate conditions as input. For global, they extract image feature and treat it as text tokens. 



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The idea is simple and writing is clear 

Weaknesses:
There are several weakness for this paper:

1, the technique novelty is incremental. Although, they tried something such as SPADE like injecting information for local branch and combine image and text tokens for global etc, but they are very straightforward. 

2, missing baseline GLIGEN [Li et al, CVPR, 2023] which also supports conditions studied in this paper. 

3, one more weakness for this paper is missing evaluation for condition correspondence. They only reported FID as a metric, which only reflects image quality. But they should also study how well the generated images are corresponded with input conditions. For example, in GLIGEN, they use mask-rcnn to detect keypoints from generated images and compare with input keypoint, so that we can know how well the model following the input. I understand that for certain conditions such as edge map, maybe it is hard to evaluate, but as least for keypoint, semantic map, depth map, it is easy to come up with some metrics.

 




Limitations:
See weakness 





====================================================================================

They addressed my main concern which is they only evaluated image quality, but not controllability.
I strongly encourage them to add results table in the rebuttal to their paper, thus will be served as a baseline for future controllable image generation work. 

Based on this point, I am willing to raise my score despite that I feel novelty is a bit weak   

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes Uni-ControlNet for the simultaneous utilization of various local controls and global controls within a single model in a flexible and composable manner. This is achieved by fine-tuning of two additional adapters on top of pre-trained text-to-image diffusion models, eliminating the significant cost of training from scratch. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper propose a new framework that leverages lightweight adapters to enable precise controls in a single model. 

Weaknesses:
1.	The training sets for the different models in table 2 are not the same. It raises the question of fairness in comparisons between the models. 

2.	The author should compare with the simple baseline Multi-controlnet: https://huggingface.co/blog/controlnet.


Limitations:
yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors proposed Uni-ControlNet, a novel approach that allows for the simultaneous utilization of different local controls and global controls. It uses two additional adapters (local and global) and injects their outputs into the frozen pretrained diffusion models, and only the parameters in adapters need training. 

Through both quantitative and qualitative comparisons, Uni-ControlNet demonstrates its superiority over existing methods in terms of controllability, generation quality, and composability. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. By training with multiple conditions simultaneously, Uni-controlnet is able to perform various kinds of control with only one model. 
2. Uni-controlnet only adds 2 adapters, which is efficient in both training and inference. 
3. By concatenating clip image embedding with text embedding (condition), the method can control the style of generated image. 

Weaknesses:
1. The clarification of the dataset construction is unclear. For example, is the skeleton/sketches generated by model, or manually collected? If it's automatically generated by models, the performance will be bounded by the accuracy of those models, and may suffer from distribution gaps if the control map  are painted by human during inference. Otherwise, it's very hard to anotate such a complex dataset.  
2. Insufficient ablation study. As mentioned in L14, ""Uni-ControlNet only necessitates a constant number (i.e., 2) of adapters, regardless of the number of local or global controls used."" Is it because of the structure design? If so, a normal ControlNet with multiple controls trained together should be compared with. 

Limitations:
see weakness and questions. 

Rating:
7

Confidence:
4

";1
JYUN0vYjh9;"REVIEW 
Summary:
This paper deals with the privacy-preserving action recognition.

They propose a meta privacy-preserving action recognition (MPPAR) framework based on the concept of meta learning to improve both generalisation abilities w.r.t. novel privacy attributes & privacy attack models in a unified manner.

They simulate train/test shifts by constructing disjoint support/query sets w.r.t. the privacy attributes and attack models.

They conduct extensive experiments to show the effectiveness & generalisation of the proposed framework vs. existing methods.



Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
+ Overall the paper is well-written, technical sound and well-organised.

+ The authors conduct enough experiments to show the effectiveness of the proposed model.

+ There are some nice visualisations and comparisons provided in both main submission and the supplementary material.



Weaknesses:
- Formulas: In the approach section, the author gives a detailed description of every mathematical detail, which is worth encouraging, but neglects the main idea and intention behind the module design. To be honest, I am lost in a large number of mathematical symbols. I don't know why these modules are designed in this way and what the structure does.

- It is suggested to have a notation section for the maths symbols used in the paper to make them clearer to readers. For example, regular fonts are scalars; vectors are denoted by lowercase boldface letters, matrices by the uppercase boldface, etc. 

- Figures: The research paper should use sufficient figures to show the details of the model, module details and experimental parts. In this paper, the author only draws one figure, and all the other details are contained in either texts or mathematical representations. This is a noteworthy shortcoming.

- Description: the author uses too many explanations in the text. An excellent paper should reduce the proportion of auxiliary parts, make the organisation of paper clearer enough to readers. English expression should be improved.

- Some typos and grammar mistakes, full stop is missing in Line 450, etc.


Limitations:
Limitations of the proposed model is not provided in the submission.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel privacy-preserving action recognition (PPAR) framework built around the MAML meta-learning framework by Finn et al.. The goal of PPAR is to train an anonymization model that is able to anonymize video data so that 1) an action recognition model can still predict the depicted action in the video and 2) the anonymized video does not reveal sensitive information like the visual appearance of a person to the adversary. The approach consists of three steps, virtual training, virtual testing, and meta-optimization. During the virtual training, the model is trained on a support set and then tested on a disjoint query set. The meta-optimization is then performed based on the testing loss. The proposed approach is experimentally evaluated on multiple datasets and compared to various previous defense approaches for PPAR.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper is generally well-written and mostly easy to understand. Figure 1 helps to understand the overall approach. Particularly the task is introduced in a way that also people with basic machine learning knowledge should be able to understand it.
- The meta-learning approach adds an interesting new avenue to PPAR research and offers a practical defense against future attack approaches.
- The evaluation is extensive, and the results are promising. Since I am not an expert in the PPAR domain, I cannot tell if important related work has been ignored. But the paper investigates four different settings, which seems convincing.

Weaknesses:
- The font size in the figures and tables is a bit small. To improve readability, the font size should be increased in these cases. Also, the space after Fig. 1 should be increased. 
- Some parts of section 4 could be more succinct
- The Metrics, particularly cMAP, should be formally introduced. 
- The paper can be improved by adding a technical limitations section, discussing, e.g., the additional training time, data requirements, etc.

Limitations:
Technical limitations are not discussed in the paper. I do not expect negative societal impact, so it is ok for me that it is not discussed in the main paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Considering that current privacy-preserving action recognition are difficult to cope with novel privacy attribute and privacy attack model, this paper propose a Meta Privacy-Preserving Action Recognition (MPPAR) framework to improve generalization abilities. Specifically,  inspred by meta learning, the authors construct support set for virtual training, and query set for virtual testing. Then authors design virtual training and testing scheme to drive the model learn more generalizable knowledge that can improve the model generalization capability.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. Based on meta-learning, the authors design virtual training and virtual testing processes, thereby improving the generalization of privacy-preserving action recognition.
2. The proposed method effectively defends against the privacy attack model on the basis of maintaining the accuracy of action recognition.

Weaknesses:
1.  The description of the task is not clear, making it difficult to understand the mission objectives. To protect the privacy of individuals in the video and identify the action behavior, why not directly convert the RGB video into a skeleton sequence？It is well known that actions can already be well identified by skeletal sequences alone, and other information other than the actions of the actor can be removed from the video.
2. The description of the experimental setup is not clear enough. For the Privacy-Preserving Action Recognition task, authors  use the data that contains 7 privacy attributes (i.e., color, gender, complete face, partial face, semi-nudity, personal relationships, social relationships) for training, and use the data that contains other 7 privacy attributes (i.e., hair color, race, sports, age, weight, landmark, tattoo) for testing. Why did authors choose these attributes for training and testing?
3.  The authors used the C3D model for action recognition, but the recognition accuracy of the original videos in Table 1、2 and 3, was inconsistent. The original video should be unencrypted data, why they have different recognition accuracy under the same recognition model？


Limitations:
Authors should verify the proposed method in more complex action data.

Rating:
5

Confidence:
3

REVIEW 
Summary:
To deal with novel privacy attributes and novel privacy attack models that are unavailable during the training phase, this paper applies the meta learning to the privacy-preserving action recognition. As a result, this work improves both generalization abilities above (i.e., generalize to novel privacy attributes and novel privacy attack models) in a unified manner.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.	This paper is well-written, well-organized and easy to understand. The arguments and conclusions are clear and explicit.
2.	This paper proposes a solution to address a important and interesting problem, i.e., preventing privacy leakage while maintaining action clues for activation recognition.


Weaknesses:
1.	This paper claims that the proposed method is driven to learn more generalizable knowledge and can help remove potentially unseen privacy attributes (and defend against unknown privacy attackers). Although the authors provide some theoretical understanding about the objective function in the appendix, are there any theoretical guarantees about the generalization of the proposed method?
2.	This paper claims that guide anonymization models to learn such generalizable knowledge is challenging, but is seems like can be directly solved by employing the meta learning. This makes this paper appear to utilize meta-learning for action recognition, which limits the novelty. What are the challenges in applying meta-learning?
3.	More comparison methods from the last two years need to be compared to verify the effectiveness and generalization of the proposed method.
4.	It would have been better if the authors could provide the standard deviation of the experimental results.


Limitations:
None.

Rating:
5

Confidence:
3

";1
Lg1ODJGGiI;"REVIEW 
Summary:
The paper introduces a new class of Bayesian nonparametric  models by extending neural processes. The fundamental idea is to create a Markov chain of stochastic processes, culminating in a flexible enough stochastic process.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The paper introduces a simple but a clever modification of vanilla neural processes. The idea allows for more expressive representation as demonstrated by the experiments. I believe the idea is significant enough for it to be of interest to the community.

Weaknesses:
The paper uses an unclear notation for probabilistic notions. It’s not just that this choice is unconventional, forcing the reader to keep a mental map of how things are redefined, but also that it needlessly complicates exposition. For example, what is $p$? It is ubiquitous in the paper but is never clearly defined. This makes the discussion too hand-wavy — what is $p_{x_{1:n}}(y_{1:n})$? Is it the distribution function of the random vector $(F(x_1), \ldots, F(x_n))$? If so, what even is $y_{1:n}$? Is the density function of the random vector $(F(x_1), \ldots, F(x_n))$? What is $f$? Since $\mathcal X$ is simply the indexing set, and for real-valued random variables we should have $\mathcal Y = \mathbb R$, is $f$ simply a real-valued function defined on the indexing set? The notation for transition operators is also very troublesome. My point in raising such questions is that I would have preferred if the authors used standard measure-theoretic notation (it is standard for a good reason) to write much of the paper (I really like the treatment in Ghosal and van der Vaart’s book).

Limitations:
Not applicable.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes Markov neural process, a new class of stochastic processes. The work is based on prior art on neural process, which was published in 2018. The paper is reasonably written. The effectiveness of the new method is very clear from the experimentations. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed markov neural process adds more expressiveness to the traditional neural process, which potentially can open the door to new applications;
2. The theoretical analysis appears sounds
2. The proposed method appears to be competitive when compared to the baselines.

Weaknesses:
1. The explanation of the overall MNP is hard to follow. Fig 2 is crucial to explain the overall structure, plus the training and inference flow. But it's unclear. For example, the three type of the arrows in Fig 2(b) aren't explained well.
2. The experiments are impressive in the tables. But the manuscript didn't provide any intuitive explanation why MNP works better. In the contextual bandit example, MNP shows significant improvement over NP when $\delta = 0.99$. Some explanation the reason or an ablation study with respect to the number of markov stages is needed for the readers to get a better feeling.

Limitations:
No potential negative societal impact. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper attempts to improve the flexibility of Neural Processes without sacrificing exchangeability and consistency properties of classical stochastic processes. The paper proposes a diffusion-like iterative refinement of the predicted values, however, ensuring exchangeability and consistency.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Theoretically ensures exchangeability and consistency.
- Performs better than the main baseline in this category i.e., NPs.

Weaknesses:
I discuss this in the Questions section.

Limitations:
Yes, the paper discusses the limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents Markov Neural Processes (MNPs), a novel class of Stochastic Processes (SPs) that leverage neural networks for data modeling. MNPs enhance the flexibility and expressivity of the Neural Processes (NPs) framework without sacrificing consistency or imposing restrictions. The proposed iterative construction is fully generative and maintains consistency under conditioning. MNPs demonstrate superior performance over baseline models across various tasks.

The primary contribution is the MNP model, which employs a neural process as the transition operator, thereby adding flexibility to the stochastic process. This method increases expressivity while preserving the fully generative nature and consistency of NPs. The conditional consistency defined in this paper plays a crucial role in preventing uncertainty mismatches.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
he model is theoretically proven to satisfy the valid conditions. The experiments are well-designed to demonstrate the model's better expressivity, advantage over traditional models on non-Gaussian data, and better performance on scientific problems. The method has high theoretical novelty, especially in the use of Markov operators that lifts the Gaussian restrictions of the previous models.

Weaknesses:
The experiments are run on a few small-scale synthetic datasets. The model's performance would be better evaluated if there are experiments on a larger variety of, especially large real-world, datasets. Besides, experiments that evaluates the time consumption among the models would be helpful. 



Limitations:
Yes. 

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper introduces Markov Neural Processes (MNPs), a new variant of Stochastic Processes (SP). The paper points out that predictive SP models that construct context-to-target mappings no longer satisfy conditional consistency property and also are not fully generative. An expressive NP variant called MNP is proposed, the basis of which is stacking sequences of Markov transition operators in the function space. The paper adapts the definitions of consistency and exchangeability to MNPs, and show that they are consistent and exchangeable by construction. The paper shows the potential of MNPs in a variety of benchmarks from 1D function regression to geological inference.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper introduces the framework from a very generic point of view, and also provides context in the specific choices made for the experiments. For instance, the usage of instance-wise conditional normalizing flows to parameterize the MNP is very interesting; as flows are universal approximators, this gives the MNPs high flexibility. Similarly, the usage of permutation-invariant neural networks is also interesting.

The idea is simple but powerful, and the exposition of the idea is very neat.


Weaknesses:
The empirical evaluation of the paper is somewhat limited. More empirical results may attract readers to this paper - for instance, few-shot image classification results would be interesting to see. Results in more large-scale datasets will also be useful.


Limitations:
The training of MNPs is computationally intensive as the authors state; this limitation can be addressed in future works.


Rating:
8

Confidence:
4

";1
OZ7aImD4uQ;"REVIEW 
Summary:
In this paper, the author performed a large-scale psychophysical experiment to see whether scales improve mechanistic interpretability. They found that there is no scaling effect for interpretability; the latest large models do not provide better mechanistic interpretability than older models. The dataset with user response is publicized so that it can be used in future studies that aim to optimize interpretability.

I think the paper is novel in that it answers a new question that was not explored in this way. However, since it lacks new theoretical results, substantial methods, or findings, I believe the NeurIPS datasets and benchmarks track would be a more suitable venue for the paper.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The approach to answering the question of whether scales affect mechanistic interpretability via large-scale psychological experiments is novel.
2. The experiment was performed on various axes such as model sizes, dataset sizes, and model architectures. 
3. The large-scale psychological experiment dataset would be a valuable resource for further research
4. analyzed the dataset using multiple ways.
5. The paper was well written.

Weaknesses:
1. The paper does not provide substantially new methods or findings.

Limitations:
1. I am not sure whether the hypothesis that scaling improves mechanistic interpretability is a common perception. If not, the utility of the dataset would be limited. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents an empirical evaluation of how interpretable the individual neurons in different models are, with specific focus on how increasing model scale effects this. This is done via large scale human experiments via amazon mechanical turk, and they find no evidence that larger models would be more interpretable, instead having weak evidence towards large models being less interpretable. They also release their evaluation data to help create automatic evaluation in the future via models to predict interpretability to a human.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Sound experimental setup and statistical analysis
- Studying an important problem
- Larger scale than previous experiments, finding additional evidence towards previous findings such as feature visualizations being less informative than natural images, and robust models having more informative feature visualizations
- Clearly written
- Good discussion of related work

Weaknesses:
I think the experimental setup could be improved in a few ways:
- I think 84 neurons per model, while bigger than previous might not be enough, especially if wanting to compare between layers in a single model, as this leaves less than 2 neurons per layer for ResNet-50 for example, which is not large enough sample size. This might also be part of the reason why most findings lack statistical significance. 
- While the costs make it harder to scale, could have been better to narrow down focus i.e. look at fewer models/layers within these models or use fewer evaluators per neuron.
- Top-5% and top-15% activations for medium and large are too big of a jump in my opinion, top-15% rarely has any meaningful pattern for a neuron. Would be more interesting to see closer results like in the top-1% range

Finally results are not surprising, I wasn't expecting scale alone or the other features studied here to have a significant effect on the interpretability. This reduces the impact of the work but it is still good information to have confirmed.

Limitations:
Yes, good discussion. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
 This paper starts with the motivation of  a recent growing field of mechanistic interpretability, in which the goal is to attempt to reverse engineering a given neural network (or possibly any function). The main hypothesis is that since recent models have grown in size in terms of number of samples and number of parameters, if models perform more human-like task, the extracted features might become more interpretable. The authors then perform a large-scale human psycho/meta-physical study, in which they ask participants which of the two presented group of images are more aligned with a selected neuron’s response. The correctness of the response then serves as a metric for how interpretable a neural network is, in which they find neural networks from recent years (2017-2023), despite being larger in parameters and datasets size, are not mechanistically interpretable, which means study subjects are unable to identify the correct group of images. Last but not least, the authors release the data collected from participants as a dataset, to motivate the need for an automated formula/system to promote interpretability within any trained model. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Overall, the paper is well-written and its contents are well-organized. I did not run into trouble understanding the paper. 

Interestingly, the work is heavily influenced by Olah 2017, which looks into activation maps of individual neurons of a neural network and find “object detectors” such as “dog neurons.” It is also somewhat parallel to works done back in 1950s, when neuroscientists would probe neurons in monkey visual cortex and find “edge” detectors from simple/complex cells. Hence I believe this experiment, although somewhat similar to what previous has done in terms of methodology, is an important contribution to the field of mechanistic interpretability, and to the entire field of interpretability/study of neural networks on a unit level in general. Although the connection to neuroscience in mentioned in the related work, personally I would suggest adding a short sentence to the introduction regarding its parallel to neuroscience. 

The setup of the experiment is also reasonable and sound. Authors have sufficiently considered the potential biases of the experiment and have addressed them accordingly in the design of the experiment. The experiment is also done with a wide range of models (both CNNs and ViTs) and datasets of different scales. I believe the experiments are well thought out and done. 


Weaknesses:
In this work, the authors constructed the hypothesis “If models make more human-like decisions, this might hint at a closer alignment between the extracted features and human perception.” While ImageNet classification is often considered standard for benchmarking, it is hard to argue that better models of predictive performance/larger models implies more human-like decisions. A better benchmark would have been comparing models that can perform multiple tasks, which humans do. An alternative solution would be to perform the same experiment on architectures, but that would obviously induce a much higher financial cost. I think the authors can better address why ImageNet is an indicative task for human-like decision in their Method or Introduction. 

Small typo. In Training Details on OpenReview website, authors wrote: “we do not train but onyl evaluate models”, onyl -> only

But other than the above, there are no major weakness in this work.  


Limitations:
The authors have adequately addressed the limitations in their Limitation section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors investigate whether features associated with neurons in vision models become more or less interpretable as these models are scaled up. They utilize mechanical turk and a forced association test to see whether human workers are able to clearly associate exemplar images (or synthetic feature activations) to strongly vs. weakly activating sets of input images for given neurons. They find that across multiple scales and classes of vision models, there is no significant increase in interpretability when increasing scale. They provide a number of additional findings on the shortcomings of feature visualizations, and trends (or lack thereof) in interpretability as a function of neuron specificity and the location of neurons within a network. In addition to these interesting findings, they release an annotated dataset of neurons and their associated intepretabilities which should prove a great boon to mechanistic interpretability researchers more generally.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Overall this paper provides a meaningful contribution to the field of interpretability research; both through its own investigations and through the accompanying release of the IMI Dataset. 

## Originality and Significance
The psychoanalytic approach toward addressing questions about interpretability has thus far been relatively underexplored, and is used to great effect in this work. The findings themselves seem general enough to guide future research on interpretability methods for vision models, but the release of the associated dataset may be more impactful still, as it will allow researchers with fewer resources to work on superior algorithmic approaches for discovering features / Interpretable neurons.

The additional experiments carried out in App. B.4 are also very interesting, as these undermine some intuitions prevalent in the community regarding which kinds of simple flags are likely to indicate interpretable neurons (though the annotations don’t explicitly measure the levels of polysemanticity, thus allowing this to remain as a potential confounder).

## Quality and Clarity
The methodology and findings are mostly well-presented, and only minor changes are suggested (see Weaknesses)


Weaknesses:
The methodological approach seems comprehensive and well-suited towards addressing the questions of interest. I have no major criticisms.

The two minor criticisms are on:
* **Claims** - the paper repeatedly makes claims of the form “mechanistic interpretability of larger models is not better”, which is supposed to be short-hand for “the interpretability of features associated with neurons in larger vision models is not better”. I think this amounts to an overstatement of the valid claim without being any more concise, as reframing “feature interpretability” to “mechanistic interpretability” is a generalization (there are other forms of mechanistic interpretability which may be unaffected by these findings, such as circuit discovery). Collapsing “larger vision models” to “larger models” is fine, given that this is obvious. Additionally, one might wish to argue that “feature interpretability” is too broad, but the field’s current definition of features is quite loose, and the paper acknowledges issues around superposition and synthetic feature generation, so this is fine.

* **Presentation** - The paper is very well presented for the most part. The only part that was slightly difficult to parse was Fig.2 - here I found the examples in the appendices more illuminating (partly due to a nitpick regarding the caption, mentioned below). I don’t think there would be any loss of generality if this figure were replaced with one of these examples from the appendix (as the caption explicitly states that the query may be a natural exemplar or synthetic visualization)

## Nitpicks:
* 74: -> “was their usefulness .. experimentally quantified”
* 114: do you mean “higher density of interpretable units”?
* Fig 2 Caption: It was slightly unclear at first that “two extremely activating” meant one extremely positively, and one extremely negatively activating; this made the following “pick the more positively activating query” slightly confusing.
* Fig 3 Caption: The caption is split into part “A” and “B”, but these are not labelled pictographically. Changing to “left” and “right” would be easiest.
* 147 - “keep” -> “kept”, as latter part of sentence is past tense
* 174 - “paradigm shift” seems quite strong
* 300 - The footnote referencing superposition should be places before the period


Limitations:
Limitations are thoroughly discussed.

Rating:
7

Confidence:
5

";1
lRG11M91dx;"REVIEW 
Summary:
This paper proposes a so-called functional-group-based diffusion generative model, namely D3FG, to generate molecules with realistic substructures conditioned on the protein binding sites. D3FG represents the protein-ligand docking system as a fragment-based system. The molecule fragments are molecule substructures and the protein fragments are amino acids. Thus, the original protein-ligand docking system is reduced to a coarsen graph with molecule substructures and amino acids as nodes. The molecule substructures are connected with heavy atoms as linkers. Thus, the denoising and diffusion processes are defined with molecule substructure translation probabilities. Although this idea is interesting, the reviewer is concerned about the paper presentation, literature reviewer, and marginal performance improvement.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Representing the protein-molecule docking system as the coarsened graph reduces the computation expenses of the diffusion generative models. Recently, this scheme has drawn much attention and there are some related work has already shown the efficiency of this scheme. Moreover, molecule elaboration is also related to ligand optimization, which aim to optimize the ligand efficiency by modifying it structure.

Weaknesses:
1. Limited Novelty and Insufficient literature review. This paper represents the fragment-based method for ligand generation and claims no prior work employs the fragment-based generation scheme. However, there are already some methods that employ fragment-based generation. For example, Deepfrag [1], FLAG [2], and SQUID [3] are all fragment-based generative methods for target-aware molecule design. It is necessary to discuss the difference and highlight the contributions. 

2. The paper presentation needs to be improved. For example, what is the intuition of the definition of absorbing state in Line 105? What is the connection between the BERT-style objective and the denoising objective in Eq.4? What are the different levels of nodes in Section 4?

3. Definition of ""functional group"". The functional group often refers to the molecule substructures that contribute most to the chemical properties. However, as shown in Section 5.1, the functional group in this paper refers to the most frequent substructures. Mining the most frequent substructures for molecule generation is not new and has been employed in [2,4].

4. Marginal empirical improvement. Moreover, there is only marginal empirical improvement over the baseline methods as shown in Table 4.

5. Unfair comparison. Since the proposed method already employs the most frequent substructures for model training, it is unfair to choose the ""'Ratio’ of the top ten functional groups with the highest frequency"" as an evaluation metric in Table 1.


[1]. Deepfrag: a deep convolutional neural network for fragment-based lead optimization. Chemical Science.

[2]. Molecule generation for target protein binding with structural motif. ICLR 2023.

[3]. Equivariant shape-conditioned generation of 3d molecules for ligand-based drug design. ICLR 2023.

[4]. Molecule Generation by Principal Subgraph Mining and Assembling. NeurIPS 2022.



Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, a functional-group-based diffusion model called D3FG is proposed to generate molecules in 3D for target protein binding. Two generation schemes including joint and two-stage generation schemes are formulated. 

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
1. The paper is well-written and easy to follow.
2. It is the first functional-group-based diffusion model for structure-based drug design.

Weaknesses:
1.  The performance improvement over previous methods is limited. For example, in table.2, TargetDiff achieves the lowest JS divergence on most bond distances. In Table. 4, TargetDiff achieves the best performance on the vina score. In Figure. 3 the D3FG does not show an advantage over other baselines with respect to atom type distribution.
2. The evaluations are not comprehensive. For example, besides divergence between bond distances, bond angles and dihedral angles should be evaluated. The influence of the size of the functional group set is not explored. 
3. The technical contribution is limited. The application of diffusion models for drug design is not new. It seems the required techniques are proposed in previous works including TargetDiff [7], DiffSBDD [8], and DiffAB [52]. 
4. Some related works are not discussed. For example, some previous methods also leverage functional groups or motifs for molecule design: JT-VAE [1], PS-VAE [2], and FLAG [3].

[1] Jin et al., Junction tree variational autoencoder for molecular graph generation, ICML 2018
[2] Kong et al., Molecule generation by principal subgraph mining and assembling, NeurIPS 2022
[3] Zhang et al., Molecule generation for target protein binding with structural motifs, ICLR 2023

Limitations:
Yes, the authors have adequately discussed the limitations.

Rating:
4

Confidence:
5

REVIEW 
Summary:
Pocket-specific molecule generation has received considerable attention in recent years, and the authors propose a functional-group-based diffusion model to address this task. The model considers the generation of complete molecules as the assembly of functional groups (fragments) and atoms that connect these functional groups. They employ a diffusion-based generative scheme to determine the coordinates, orientation, and types of these fragments. The experiments demonstrate the satisfactory generation performance of the model and its potential applications in molecule elaboration.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written, and the narrative is clear and concise.
2. The authors address many important details, including the determination of functional groups, handling chirality, and generating heterogeneous graphs.
3. Overall, while the idea of decomposing molecules is not novel, the authors provide a robust model and conduct experiments to validate its superiority.

Weaknesses:
1. In the Method section, it would be beneficial to provide an overview of the entire method before delving into the specifics of each part.
2. The aromatic elements C:C and C:N in Table 2 could be written in lowercase to maintain consistency with convention.
3. It would be beneficial to include more visualizations (2D/3D) of generated molecules.

Limitations:
The authors have addressed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes to generate 3D molecules using functional groups and linker atoms in a diffusion model. Using functional groups as building blocks help the model to generate realistic local structures. In the proposed diffusion model, the atom/functional group type, coordinates and orientation are predicted.  In addition, the authors experimented with two strategies, joint and two-stage, to generate heterogenous graph, which contains functional groups and linker atoms. The performance indicates the proposed method achieves SOTA performance in structure-based drug design task. 

This paper also proposed a new molecular elaboration task and curated a dataset for this task. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper implement functional groups in diffusion-based structure-based drug design model for the first time. 
2. The performance indicates using functional groups as building blocks can improve QED and SA, which have been a challenge for previous diffusion models. 
3. This paper proposes a new task, molecule elaboration, for structure-based drug design and provides a dataset for it. 

Weaknesses:
The paper proposed a new molecule elaboration task, and some existing baselines (e.g. Pocket2Mol[1], 3DSBDD[2], TargetDiff[3] and DiffSBDD[4]) and can be easily adapted for this task. Currently, there is no performance comparison for molecule elaboration task, and it is recommended for the authors to evaluate their model with existing baselines which can be used for this task. 


[1] Peng, Xingang, et al. ""Pocket2mol: Efficient molecular sampling based on 3d protein pockets."" ICML 2022.

[2]Luo, Shitong, et al. ""A 3D generative model for structure-based drug design."" NeurIPS 2021.

[3] Guan, Jiaqi, et al. ""3d equivariant diffusion for target-aware molecule generation and affinity prediction."" ICLR 2023. 

[4]Schneuing, Arne, et al. ""Structure-based drug design with equivariant diffusion models."" arXiv preprint arXiv:2210.13695 (2022).

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents a novel method for generating 3D molecules that bind to specific protein pockets based on a functional-group-based diffusion model (D3FG). The model decomposes molecules into functional groups and linkers and generates their types, positions, and orientations gradually through a denoising process. The model uses equivariant graph neural networks to parameterize the denoisers and ensure the roto-translational invariance of the molecule distribution. The paper also introduces a new task of molecule elaboration, which aims to modify existing molecules based on the fragment hotspot maps of the protein pockets. The paper evaluates the model on the CrossDocked2020 dataset and shows that it can generate molecules with realistic structures, competitive binding affinity, and good drug properties. The paper also demonstrates that the model can perform molecule elaboration and generate molecules with higher affinity than the reference molecules. The paper claims that D3FG is a novel and effective method for structure-based drug design that leverages the pharmacological information of functional groups.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper proposes a functional-group-based diffusion model for pocket-specific molecule generation and elaboration called D3FG. The method decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. The two kinds of components can form complicated fragments that enhance ligand-protein interactions. In the experiments, the authors claim the method can generate molecules with more realistic 3D structures, competitive affinities toward the protein targets, and better drug properties. The paper is original in its approach to generating molecules given the pockets’ structures of target proteins and its use of functional groups as basic components instead of atoms. The paper is clear in its description of the method and its results. 

The paper is well-written and clearly describes the proposed method, its implementation details, and its evaluation results. The authors provide sufficient background information on related work and explain how their method differs from prior methods. The authors also provide detailed explanations of the model's components, such as the functional group decomposition, equivariant graph neural networks, and molecule elaboration. The authors use appropriate visualizations to illustrate their method's outputs and compare them with reference molecules. 

The paper's originality lies in its novel approach to generating 3D molecules that can bind to specific protein pockets by leveraging functional groups' pharmacological information. The authors show their method can generate molecules with realistic structures, competitive binding affinity, and good drug properties. The paper's significance lies in its potential to improve structure-based drug design by enabling the generation of novel molecules that can bind to specific protein pockets with high affinity.

Weaknesses:
The paper's main weakness is that it does not provide a detailed comparison with prior methods for generating 3D molecules that can bind to specific protein pockets. The authors briefly mention some related work, but they do not provide a comprehensive comparison of their method with prior methods in terms of performance, efficiency, and scalability. The authors also do not provide a detailed analysis of the limitations of their method, such as the types of molecules it may not be able to generate or the types of protein pockets that it may not be able to bind to. 

Another weakness of the paper is that it does not provide a detailed analysis of the interpretability and explainability of its method. The authors briefly mention some visualizations and explanations of their method's outputs. Still, they do not provide a systematic analysis of how their method's components contribute to its performance and how they can be interpreted in terms of pharmacological properties. 

To improve the paper, the authors could perform a more comprehensive comparison with prior methods for generating 3D molecules that can bind to specific protein pockets, including both quantitative and qualitative analyses. The authors could also perform a more detailed analysis of the limitations of their method and how they can be addressed in future work. Finally, the authors could perform a more systematic analysis of the interpretability and explainability of their method, including sensitivity analyses, feature importance analyses, and pharmacophore analyses.

Limitations:
The authors have not adequately addressed the limitations and potential negative societal impact of their work. The authors only briefly mention some limitations of their method in the conclusion section, but they do not provide a detailed discussion of how these limitations affect their results and how they can be overcome in future work. The authors also do not discuss any potential negative societal impact of their work, such as the ethical, legal, or environmental implications of generating novel molecules that can bind to specific protein pockets.

Rating:
5

Confidence:
4

";1
0rVXQEeFEL;"REVIEW 
Summary:
The paper introduces TPSR, a Transformer-based Planning strategy for Symbolic Regression. TPSR incorporates Monte Carlo Tree Search into the transformer decoding process, enabling the integration of non-differentiable feedback such as accuracy and complexity. Experimental results show that TPSR outperforms existing methods in terms of fitting-complexity trade-off, extrapolation abilities, and robustness to noise.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and easy to understand.
- The idea of enhancing large scale pre-trained Transformers with improved search capablities is very promising in the context of symbolic regression
- The model shows good performance both compared to the E2E baseline and the GP methods.

Weaknesses:
- My main concern is about the novelty of the approach. A very similar idea has been recently investigated in [1] where the authors also combine MCTS with pre-trained Transformers. I would be grateful if the authors could clarify any eventual differences between the two approaches.
- The impact of $\lambda$ seems quite significant in your experiements. However, it is not clear to me how one should select it in practice.


[1] Kamienny, Pierre-Alexandre, Guillaume Lample, and Marco Virgolin. ""Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search."" (2023).

Limitations:
n/a

Rating:
6

Confidence:
4

REVIEW 
Summary:
Authors propose a transformer-based planning (using MCTS) strategy to solve symbol regression task. Different from traditional decoding method, the new method is able to integrate non-differentiable feedback into the transformer-based process of equation generation. Experiments demonstrate the significent performance. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Distilling symbolic equation from noisy data is intractable. Recent progress is achieved by training neural networks to generate candidate symbolic expressions, which is really promising.  

This work combines the Monte Carlo Tree Search and pretrained transformer-based symbol regression model for equation generation. Compared with Genetic programming method, the new approach not only leverages pre-trained priors, but also considers feedbacks during the generation process. 

Weaknesses:
There is not much initiality in the new method. It demonstrates a new application for a combination of two existing methods. 

Limitations:
Monte Carlo Tree Search is statistical, and Pre-trained transformer is trained through data, the integration of the two methods is still within the traditional paradigm of machine learning, so, may not work well for out-of distribution data. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes to incorporate  Monte Carlo Tree Search (MCTS) on top of pretrained transformer-based SR models to guide equation sequence generation. This is to address the challenges where existing methods purely rely on the pretrained transformer’s output and without accounting for external performance requirement. In MCTS, the authors develop a reward function to encourage the balance between fitting accuracy and regulating complexity for the SR generation. Also, the caching tricks are employed to improve the implementation efficiency. SR benchmark datasets are used to demonstrate the improved performance of the proposed method over the state-of-the-art.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Including performance feedback in the pipeline of generation of SR equation generation from pre-trained transformer-based SR models is well-motivated.  To achieve so, this paper proposes including MCTS as the decoder in this pipeline, and imparting    the external requirement, via a reward function in MCTS, to eventually improve the performance of equation generation. The extensive experiments and baseline comparison clearly show the effectiveness of the proposed method, in terms of fitting-complexity trade-off, extrapolation abilities, and robustness to noise. 

The presentation of techniques is clear, and the evaluation in my opinion is solid. Overall, this paper makes a good contribution in the SR field.

Weaknesses:
I only have two comments:

- In Equ. (1), how to select $\beta(s)$? It would be better to show its effect on the performance in the ablation study as well.
- Currently, the method still relies on a pre-trained transformer SR model. The authors could give some perspective about how (or if it is possible) the MCTS can also be incorporated in the transformer training (or fine-tuning) process.
-A typo in line 151: trnasformer--> transformer

Limitations:
See my comments in the above section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This submission proposes a neural network-based approach to symbolic regression (SR), namely generating equations as sequences. It leverages the power of pretrained SR transformer models and the MCTS algorithm to tradeoff the fitting accuracy and equation complexity. Experimental results on the SRBench and the In-domain Synthetic datasets demonstrate that the proposed approach outperforms the backbone E2E transformer model.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Soundness:
The techniques employed in the proposed approach are sound. The approach is able to use any non-differentiable target function to guide the training of a neural model for symbolic regression. Experiments demonstrate that it outperforms a state-of-the-art transformer model which is used as the backbone in the proposed approach, indicating that the implementation of the proposed approach is likely to be correct.

Presentation:
The submission is in general well written and organized, easy to follow.

Weaknesses:
Presentation:
There is a minor issue on the term single-instance symbolic regression introduced in Related Work. According to the description about the therein algorithms GP, RL, GP+RL and MCTS, the difference between them and the proposed approach mainly lies in not employing pretrained knowledge. Thus, the term single-instance is strange and cannot tell the true difference from the proposed approach.

Contribution:
The proposed approach seems to be a combination of the E2E transformer model [18] and the MCTS framework [26]. Although the transformer model can be replaced with other neural network-based models, the contributions beyond [18] and [26] are not significant. Moreover, the current evaluation cannot confirm that either the proposed approach is a general framework for enhancing any neural network-based model for symbolic regression, or the approach achieves the truly state-of-the-art performance. For the former confirmation, the authors need to compare multiple implementations having different backbone models with the original backbone models. For the latter confirmation, the authors need to compare the proposed approach with more state-of-the-art solutions such as [30] and [31].


Limitations:
As far as I can see, the authors have adequately addressed the limitations through sufficient discussions in the supplemented material.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposed to combine pretrained Symbolic Regression models with MCTS procedure to improve SR performance without finetuning the pretrained models. Experiments are conducted to demonstrate the improved performance the proposal.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Proposed a new MCTS based decoding procedure to improve pretrained SR models performance without finetuning the models.
2. The paper is clearly written and easy to follow in texts. 
The experiments clearly demonstrates the performance improving over the baselines and the E2E models' decoding.

Weaknesses:
1. A few key building blocks needs to be summarized from the literature to be a self-contained paper, e.g. how the datasets are embedded. 
2. The methodology contribution is minor as  only MCTS is introduced on top of SR models although experimental performance improvement is observed. Although this is a valuable contribution it might not meet the bar for NEURIPS.

Limitations:
NA

Rating:
5

Confidence:
4

";1
MWQjqtV1z4;"REVIEW 
Summary:
This paper studies the infinite-horizon restless bandits problem and proposed a simulation-based framework, i.e., Follow-the-Virtual-Advice, to leverage single armed policy to solve a multi-armed problem, which gets rid of the difficult-to-verify condition, i.e., the uniform global attractor property. Removing this pre-condition is a significant improvement for RMABs. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper studies the infinite-horizon restless bandits problem and proposed a simulation-based framework, i.e., Follow-the-Virtual-Advice, to leverage single armed policy to solve a multi-armed problem, which gets rid of the difficult-to-verify condition, i.e., the uniform global attractor property. Removing this pre-condition is a significant improvement for RMABs. 

Weaknesses:
1. The arguments on FTVA in Section 3.3 look to be a bit hands-waving since there is no clear evidence to support their claims. Based on the reviewer’s understandings, those arguments are possibly wrong.  For example, in lines 181-184, the authors claim that even if the initial virtual state and real state of an arm are different, they will become identical in finite time by chance under mild assumptions in Section 4.1. However, Assumption 1 relies on the “observation” in lines 233-238, which is not true. All the transitions are stochastic, not deterministic. How could we guarantee that $S(t+1) =\hat{S}(t+1)$ if $S(t)$ is different with $\hat{S}(t)$?  If Assumption 1 fails, the argument in lines 183-184 does not hold. The arguments in lines 185-189 are also not true. Even when the virtual process can always satisfy the budget constraint, how do we guarantee that the real process following the virtual actions does not violate the budget constraints. 

2. The example in lines 194-215 is also not fully supported by evidence. The single-agent policy defined in eq. (8) is stationary stochastic policy, and agent selects actions according to certain probabilities. What do you mean by the preferred action at each state? Though this particular example may only have one action at each state, there is no preferred action in general. How to guarantee the descriptions in lines 207-209 to be true?  This example shows that the real and virtual process has different state distributions, which contradicts Section 3.3. See comment 1.

3. A very important related work is missing. [Ghosh22] also gets rid of the global attractor assumption and considers a much more challenging setting with heterogeneous arms and multi-actions.  An outstanding limitation of this work is that all arms must be the same. 

Ghosh, A., Nagaraj, D., Jain, M., & Tambe, M. (2022). Indexability is Not Enough for Whittle: Improved, Near-Optimal Algorithms for Restless Bandits. arXiv preprint arXiv:2211.00112.

4. The references cited in this paper are not precise. For example, as far as the reviewer knows, [Ver16] does not consider the single-armed problem. Hence, the argument in lines 140-141 are not precise.  Another example is in lines 123-124. The reviewer is not aware of the meaning of these sentences. How are they related to the RMAB problem in eqs. (1)-(2)?


Limitations:
no negative societal impact. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper presents an algorithm providing an asymptotically optimal policy to solve restless bandits with N arms when N goes to infinity without using classical assumptions on the expected deterministic dynamics (uniform global attractor).


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:


1. The idea to couple the actual trajectory under the relaxed policy with a virtual one, sampled independently for each bandit is a very nice idea.

2. Removing the UGAP assumption is also a strong point, although this is not the only paper going in this direction (see for example Zhang X, Frazier PI (2022) Near-optimality for infinite-horizon restless bandits with many arms. arXiv preprint arXiv:2203.15853).


3. The paper makes a good literature review and a good  use of previous work in this domain.

Weaknesses:
1. The discussion on the synchronization assumption, and especially the sufficient conditions could be made more precise. The link with UGAP is also missing.
Also there is a gap between the synchronization of each arm with the synchronization of the whole bandit (see for example the PhD thesis of Kimang Khun). 

2. The fact the algorithm is based on independent sampling of each arm makes the \sqrt{N} bound tight, wuth little hope for improvement, while other approaches based on the fluid dynamics could still work with UGAP (using Cezaro averaging for example) with a higher chance to have a better convergence rate.

3. The comparison with previous algorithms is a little unfair, especially because of initial conditions.

Limitations:
I did not see any limitations.


Rating:
7

Confidence:
4

REVIEW 
Summary:
In the late 80s and early 90s, Whittle formalized the restless multi-armed bandit framework and Weber & Weiss proved that Whittle’s index policy, which solves a relaxed version of the optimization problem, is asymptotically optimal under conditions of indexability. Since then, RMAB problems have attracted much attention for their wide application in diverse areas. This paper contributes a new technique that is not an index policy. Instead, the simulation framework requires a new “synchronization assumption” for the discretized time case. The contributions of this paper are purely theoretical, including a new algorithmic approach (Follow-the-Virtual-Advice) and optimality gap bounds.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
This paper introduces an original technique for solving two-action RMAB problems in both discretized and continuous time spaces, and provides bounds on conversion loss and gaps in optimality for both. It is clear that the only assumption that must be enforced in the discrete case is the new “synchronization assumption”. There is a nice discussion about this assumption and when it holds in the appendix for novice readers. The submission appears to be technically sound, and claims are well supported by theoretical analysis.


Weaknesses:
After decades of essentially little-to-no change in the state-of-the-art techniques that are deployed in practice, it is wonderful to see new approaches. The authors should be made aware of [1], which aims to address the same problem and also does so in a new, novel way.

I am surprised by the use of “uniform global attractor property” rather than “indexability” throughout the paper. With regards to clarity, I encourage the authors to include more explanations to inform the reader. 

The contributions of this paper are purely theoretical. Open-source code as well as empirical results comparing FTVA against other state-of-the-art implementations would be helpful for the research community.

[1] Abheek Ghosh, Dheeraj Nagaraj, Manish Jain, and Milind Tambe. 2023. Indexability is Not Enough for Whittle: Improved, Near-Optimal Algorithms for Restless Bandits. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (AAMAS '23). International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 1294–1302.


Limitations:
Yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studied, for both the discrete-time and continuous-time settings, the average-reward restless bandit problem without the uniform global attractor property. They proposed the simulation-based policies Follow-the-Virtual-Advice (FTVA) and FTVA-CT, which converts the optimal single-armed policy into the N-armed constrained policy by simulating a virtual process that follows the single-armed policy, and then by letting the real process act closely to the virtual process but satisfying the constraints. The policies provide vanishing performance loss.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Novelties: In this reviewer's knowledge, this paper is the first paper to provide an algorithm for the average reward constraint RB problem without uniform global attractor property, which is a property that is hard to verify. The proposed FTVA policy is quite straightforward and also easy to understand. The idea of using virtual processes in this problem seems original to me.

Presentation: The paper is well-written, with precise language and clear structure. The paper is clear and easy to understand. And there are a lot of intuitive explanations.

Significance: The paper provides the first algorithm for the average reward constraint RB problem without uniform global attractor property.

Weaknesses:
The following assumption is very strong and important to this policy: the author assumed that all the arms with the same state and action have the same reward. And this policy seems very hard to extend to the case where the arms have different reward functions. (More comments in Questions.)

The paper feels incomplete, which prevents a clear conclusion at the end.

The simulation is not sufficient, the author only showed the average reward comparison for different policies, but I would also want to know how is the action look like for different policies or is there any intuition in it. 

The paper presentation is too high-level and has missing details. The author claimed at first, but didn’t explain later, how the FTVA policy can be implemented in a distributed manner. The author claimed the algorithm can run at a linear computational cost but didn’t explain how, later.

Limitations:
There is no negative societal impact. Discussion on limitations is not extensive.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies the restless bandit problem. In this setting the agent controls a set of $N$ arms modelled as Markov Decision Processes with two actions (active, inactive). The MDPs are coupled with a constraint that forces the agent to maintain a fixed arm activation rate. The authors propose a novel algorithm for this problem that does not use the commonly used uniform global attractor property (UGAP). The key insight of the approach is to transform a learned single arm policy to a $N$ armed policy. While this may lead to a discrepancy between the learned policy and what is actually played because of the constraint, the proposed algorithm aims to bring these two states closer together. The authors provide convergence guarantees and rates for both discrete and continuous time variants of the algorithm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
To the best of my knowledge, the proposed algorithm is novel. The authors also presented very clearly the setup of the problem which is especially important given that many members of the community may not be familiar. While I did not follow all the technical details in this work, the synchronization assumption seems like a significantly weaker assumption than UGAP, indicating this a very strong contribution. On top of that, the way the algorithm solves the problem is itself highly non trivial.  

Weaknesses:
I think clarifying a little bit more the importance of synchronization when we are using the constraint would be very helpful. I will try to outline my understanding in case it is helpful in improving the presentation:

* In Section 4.1, based on the stated assumptions it is clear that achieving synchronization once is sufficient to achieve synchronization forever. But for the restless bandit setting it seems that the real and virtual states synchronize and desynchronize in a loop. Indeed the states start synchronized and some arms may immediately get desynchronized (become bad in the terminology of Theorem 1). 
* I understand that the the SA assumption helps us understand desynchronization is resolved in finite time. But if synchronization lasts only a single round, then the arms will still be bad most of the time. So SA alone is not sufficient.
* Based on the proof sketch of Theorem 1, it seems like desynchronization is inherently rare for large $N$ so this is why SA is sufficient. Intuitively, SA is only needed to bound the effect of a single good-bad-good arm loop.

If my understanding is indeed correct, it would be helpful to make this more explicit somewhere in the text, preferably in the context of the example of Section 3.3.

Limitations:
All limitations are addressed.

Rating:
8

Confidence:
2

REVIEW 
Summary:
This paper studies the problem of optimizing a policy for restless bandits in the infinite horizon case. A new algorithm is proposed that can convert a single-arm policy into an N-arm policy using virtual trajectories of the states and actions of the single-arm policy. The main idea is to generate virtual actions using the single-arm policy and then attempt to match these virtual actions with real actions that match the budget constraint. The algorithm is shown to have suboptimality that degrades with N. Both discrete time and continuous time versions are given. One of the main contributions is that this paper forgoes the UGAP condition.

This is an emergency review. I am not familiar enough with this literature to definitively judge the significance and novelty.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper appears to tackle an important problem in the literature broadly (restless bandits) and provides a potential solution to optimize policies with vanishing suboptimality as N increases. 

Achieving this without the UGAP condition that appears in prior work is interesting.

Overall the paper is well written and generally easy to understand. The examples and organization of the paper help convey these results.

Weaknesses:
The main weakness appears to be that the entire premise hinges on the synchronization assumption that the real states and virtual states will eventually match, if the same actions are played, after which point they will remain identical. This seems reasonable in some settings, but a more clear contrast with conditions in other papers would be helpful.

In general, it would be helpful to better understand the significance of this change of assumptions and how this might translate into practice.

Limitations:
No negative societal impact, but it would be helpful to have more discussion of limitations.

Rating:
6

Confidence:
2

";1
URI2aAQiQC;"REVIEW 
Summary:
This paper proposes SpikeBERT, a spiking-based BERT model for text classification. It employs LIF spiking neurons an surrogate gradients for backpropagation. The training method consists of a two-stage distillation process (pre-training + task-specific). The experiments conducted on different benchmarks for English and Chinese languages show that the proposed SpikeBERT achieves higher accuracy than prior spike-based language models and lower energy consumption than the (non-spiking) BERT.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The proposed method is novel and relevant to the community.
2.	The technical sections are described clearly.
3.	The experiments provide good-quality results.


Weaknesses:
1.	The design decisions are not discussed in detail.
2.	Unlike non-spiking BERT, there are scalability issues when increasing the depth.


Limitations:
The limitations have not been discussed by the authors, but there are no major limitations in this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents SpikeBERT, an improved version of the Spikformer spiking transformer model for language tasks. SpikeBERT utilizes a two-stage knowledge distillation method that combines pre-training with BERT and fine-tuning with task-specific data. Experimental results show that SpikeBERT outperforms state-of-the-art spiking neural networks and achieves comparable results to BERT on text classification tasks for English and Chinese, while consuming significantly less energy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1 The writing is commendable.
2 The proposed approach is captivating and innovative, and I believe it will generate significant interest within the machine learning community.
3 The results are promising, particularly in achieving comparable performance to BERT in text classification.

Weaknesses:
1 Although SpikeBERT significantly reduces energy consumption during inference, the two-stage knowledge distillation process may introduce additional costs. It would be helpful if the authors could provide insights on this matter.

2 Considering other model architectures such as the OPT model families, does SpikeBERT possess the potential to be extended to these models? It would be interesting to explore the applicability of SpikeBERT beyond the BERT architecture.

3 I am curious about the evaluations of SpikeBERT on additional tasks such as GLUE, RACE, and SQuAD. It would provide valuable insights into the model's performance across a broader range of language processing tasks.

Limitations:
No limitations

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents SpikeBERT, an implementation of BERT-based models on a Spiking Neural Network (SNN) architecture, motivated by theoretical energy efficiency benefits.

The paper presents the transformer architecture and a two-stage distillation approach which first distills a general purpose BERT model into a general purpose SpikeBERT, then finetunes the SpikeBERT by distilling a finetuned BERT model.

The approach is evaluated on several text classification tasks on English and Chinese datasets, resulting in accuracies lower but comparable than a standard finetuned BERT classifier.

A theoretical energy efficiency improvement is calculated, reporting improvements, which however I find dubious (see weaknesses)

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- Interesting work on an unusual architecture

Weaknesses:
I am especially concerned about the claims of improved energy efficiency, which serve as the main motivation of the paper.
Starting from the introduction, where the author claim: ""However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference.""
It is clearly not true that ""it requires too much computational power and energy to train and deploy state-of-the-art ANN models"" since these models are in fact trained and deployed.

More concerning is the theoretical energy comparison of SpikeBERT and BERT (Section 4.4 and Appendix C), where the authors compare FLOPs for BERT and SOPs (spiking operations) for SpikeBERT, multiply by theoretical energy costs and declare SpikeBERT the winner. The theoretical energy costs seem to be copied from other papers, and following the citation chain they seem to come from Yao et al. 2022 ""Attention Spiking Neural Networks"" where they are computed using data from Horowitz 2014 ""1.1 computing’s energy problem (and what we
can do about it)"", with the assumption of 32-bit floating point operations on 45nm hardware. Modern GPUs use 7nm hardware, and inference is often done with 8-bit floating point operation or less, therefore I wonder whether these number are obsolete.


Limitations:
Yes

Rating:
3

Confidence:
3

REVIEW 
Summary:
This work develops SpikeBERT, which extends Spikformer to perform language processing tasks, and proposes a two-stage knowledge distillation method for better training it. Experiments validate the improved accuracy of SpikeBERT over previous SNNs and the improved efficiency over vanilla BERT.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. This work is the first transformer-based SNNs for language processing tasks.

2. Experiments validate the achieved efficiency improvement over vanilla BERT.

Weaknesses:
I have the following concerns about this work:

1. The novelty and technical contributions of this work are limited: the modifications from Spikformer to SpikeBERT are minor, and similar distillation schemes have been widely studied and adopted in efficient BERT works, e.g., TinyBERT, TernaryBERT, FastBERT, DistilBERT, etc. It is hard to tell the key technical contribution of this work.

2. The experimental validation is insufficient: It only validates the improved efficiency over vanilla BERT while the aforementioned efficient BERT variants are not benchmarked, making it not clear whether SpikeBERT is a practical efficient BERT option. In addition, the theoretical power is not enough for indicating the real-device efficiency and on-device measurement is highly desirable for benchmarking the aforementioned efficient BERT variants.

3. This work may violate the formatting regulations, i.e., it adds an extra appendix in the main manuscript. In addition, the citation format seems to not follow the official template.

Limitations:
This work may violate the formatting regulations, i.e., it adds an extra appendix in the main manuscript.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors have proposed SpikeBERT which is an energy efficient Spiking Neural Networks(SNN) for natural language representation. The architectural design for SpikeBERT is inspired from Spikformer which is an SNN for computer vision, with the following major changes:
	- Spiking Patch Splitting for images is replaced by embedding layer to encode words/tokens into vectors.
	- BatchNorm replaced by LayerNorm
	- Shape of Spiking Self Attention is changed to adapt to language tasks by changing its size based on length on input instead of dimensionality of hidden layers.

The model is trained using a two-step knowledge distillation approach:
	- General purpose: The model is trained using embedding and intermediate hidden layer representations of BERT on unlabeled natural language texts.
Task specific: The model is tuned using task specific logits from a fine-tuned BERT model.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
	- The paper is well written and easy to follow. The authors provide the necessary background on SNNs including the advantages and challenges in training them.
	- The motivation is clear and the energy consumption presented in the results section helps convey the same.
	- The metrics indicate that the proposed approach outperforms the previous spiking network based baseline developed for natural language understanding and standard SNN training mechanism using surrogate gradients.
	- The authors present a thorough ablation analysis for all the contributions presented in the paper.


Weaknesses:
	- The novelty of the paper is limited:
		○ The architecture is mostly derived from Spikformer: the usage of word embeddings instead of image patches, and using layer normalization in transformer  is a standard approach.
		○ The two-step knowledge distillation approach has been used widely in the past for distilling BERT and GPT style transformer models to smaller/specific architectures.
		○ The usage of hidden layer representations in the distillation process is also a standard practice for BERT style models.
		○ Most of the formulations and approaches related to spiking neurons, its derivatives and feature transformations are adapted from previous work.


Limitations:
	- The datasets used for evaluation seem limited. With models like BERT, it's a standard approach to present results on all GLUE tasks or at least the ones such as MNLI as they are considered to be good & reliable indicators of model quality.


Rating:
4

Confidence:
3

";0
9NzC3PjpAt;"REVIEW 
Summary:
The authors present a new reinforcement learning objective, dubbed Reinforcement Learning from Human Gain (RLHG), that explicitly incorporates an understanding of human performance with an intervention into the objective function. They show that training with this added component improves outcomes in a MOBA, both on the overall objective of winning and on subobjective related to satisfaction.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
They present an important research problem (ML systems working as collaborators) and make a good attempt at overcoming it.
They did experiments with human participants.
The domain they trained models in is non-trivial and showing results shows good ability


Weaknesses:
I don't believe the main claim. The authors say that by explicitly adding their more complex models of human wants/behaviours they can get better performance, but they don't compare against a model that attempts to optimize for those things directly. A perfect RL agent that has correct values for the different objectives should be able to learn the optimal policy without using their proposed more complicated methods. They only test against a model that is trained to win, and a model that is trained on short term value optimization. How does the method compare to another model trained with similar data and objectives? The lack of comparison makes this feel like they shows that PPO and deep learning work, not that there new methods are better.

Limitations:
The authors do not explain why their decomposition of the loss/training loop is _theoretically_ better than any other, they only provided some _empirical_ evidence to suggest it.
The authors only discuss the RL literature in their framing/discussion. In other domains, such as recomender systems, the issue of over optimizing for a single metric to the detriment of other objectives is a major area of research. Consider looking over some data science papers from KDD as a starting point

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method called Reinforcement Learning from Human Gain (RLHG) to effectively enhance human goal-achievement abilities in collaborative tasks with known human goals. The paper evaluates the RLHG agent in the widely popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting experiments in both simulated environments and real-world human-agent tests.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The problem setting considered by the paper is tightly connected to some real-world problems (e.g., assistive agents in MOBA games). 
Experiments are performed in a real-world application (Honor of King).

Weaknesses:
It is difficult to evaluate the major contribution of the paper (the two-step training process) because
1) the major contribution of the paper, as the authors claimed in the paper, is orthogonal to many of the complications in the paper (e.g., multiple agents, multiple goals, partial observability). These complications are not contributions and they make it hard to understand the contribution of the paper clearly. Maybe the authors added them because their experiments are in Honor of Kings?
2) probably because the paper focuses too much on these complications, the paper fails to explain why RLHG is a good idea and provides clear evidence. For example, why do the authors propose estimating primitive human performance rather than primitive human+agent performance? What is making estimating primitive human performance helpful? Can this idea be used in environments without humans?
3) the writing of the paper is unsatisfying. I was completely lost when reading the paper. Please see Questions for my questions about the paper.
4) in experiments, the new method achieved a worse winning rate compared with the baseline method. I can understand this performance drop given that there are improvements regarding other metrics. What should I learn from this indeterminate result?

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose Reinforcement Learning from Human Gain, an RL algorithm that explicitly optimizes for enhancing human abilities in cooperative human-AI settings. Given a predefined set of human goals, the main approach first learns a value network to estimate the primitive human performance at achieving said goals. Then, a secondary Gain network is trained to estimate the enhancement the human return receives under interactions with the cooperative agent. The cooperative policy is trained with a combination of a traditional agent value network and the proposed human gain-based value network. The authors test the RLHG framework in a cooperative game and find that across experiments with real humans, the RLHG agent is preferred over an agent without the human gain objective, despite having a lower overall win rate.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem setting is interesting and relevant, and the proposed solution of optimizing for human gain is intuitive — it makes sense that a cooperative agent should account for improvements in human behaviours, rather than having the agent directly optimize for its own reward or what it perceives human rewards to be (which may reduce human enjoyment and overall autonomy in a task, as noted by the authors).
- The experiments use a complex multiagent task and test both human models and real human participants. The results show a significant improvement in human preference for the RLHG agent across the predefined goals as well as various subjective metrics.

Weaknesses:
- The method seems sensitive to the choice of partner $\pi$ while collecting the primitive human episodes. If the initial partner is already very good, will its success be attributed to the human? And vice versa -- if the partner is very bad, would that lead to a false representation of the base human skill? The paper would be strengthened with additional studies on how sensitive the method is to different pretrained policies.
- The method relies on knowing the set of human goals beforehand. It would be interesting to have additional analyses of how the method is affected in the case where some of the goals are missing or misspecified, which is more representative of a real-world scenario.

Limitations:
The authors adequately discuss the limitations of this work. Additional discussion on societal impacts would be helpful, as these agents are trained to interact with people directly.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on the fine-tuning of a pre-trained agent to assist and enhance the performance of a given human model in achieving specific goals. The authors assume access to a human model and a pre-trained agent. The authors propose a two-step approach.

1. The human model's initial performance is determined by training a value network to estimate its effectiveness in goal attainment using episodes generated through joint execution with the agent.

2. The is trained agent to learn effective behaviors for enhancing the human model's performance using a gain network that estimates the improvement in human return when compared to the initial performance.

The algorithm is evaluated on a Multi-player Online Battle Arena (MOBA) game.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The idea of developing algorithms to assist humans to solve tasks is interesting and of great practical interest, and this paper makes headway in this direction. The authors conduct extensive experiments which includes evaluating using real players to test their algorithm in a game.

Weaknesses:

1. The algorithmic contribution in this paper appears to be relatively modest, as it mainly builds upon the existing Proximal Policy Optimization (PPO) approach. The authors introduce a gain function, which essentially computes an advantage by comparing it to another state-dependent baseline ( $V_\phi(s)$)
 
2. The assumption of having a human model is justifiable; however, the strong reliance on assuming knowledge of human goals, in my opinion, limits the direct applicability of this research (as acknowledged in the limitations section).

3. There is a lot of notational ambiguity in the paper (Section 2.2), which makes reading a little hard. For example, 
 
3a. Advantage function generally depends on state/observation and action. In this setting, the Advantage is independent of both. 

3b. Is V value of a state or the infinite horizon discounted reward? It is unclear as its used in both contexts. 

3c. G is used as return-to-go, which should be a state dependent function. 

4. I do not understand the exact need for the gain network. What would happen if in line 12 of the algorithm, you drop the - Gain(s) part? This essentially means that you are computing advantage with respect to the human primitive baseline. 

5. The authors have invested significant effort and computational resources in conducting their experiments, making it extremely challenging to reproduce or recreate such experiments due to the demanding compute requirements. Although this does not diminish the value of their work, it would be beneficial if the authors could incorporate simpler environments into their experimental setup. This addition would aid in evaluating the algorithm's performance and further validate its quality.

Limitations:
The authors discuss limitations and future work. 

Rating:
6

Confidence:
3

";0
oDtyJt5JLk;"REVIEW 
Summary:
The authors present directional diffusion model (DDM) to learn graph / node representation. Compared to vanilla diffusion based representation learning techniques, DDM adds a batch based mean and variance, as well as preserving direction of diffusion. The authors then demonstrate that their model perform better than other models on representating learning by experimenting on various dataset and compare with other methods.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
- It is a nice finding by the authors that compared to diffusion model for generative modeling, diffusion model for representation learning does not require sampling from final distribution, thus we don't need to know the limiting behavior of the diffusion process.
- The experiments supports the authors' claim on the performance of the model.

Weaknesses:
- The authors should include a brief introduction on how diffusion models work on representation learning

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper offers a good study on anisotropic and directional structure learning of the diffusion model. The authors first conduct a statistical analysis emphasizing the importance of anisotropic structure in graph dataset and demonstrate the disadvantage of the vanilla diffusion model through signal-to-noise ratio. Later on, they propose their work, which is a new pipeline considering preserving the characteristic of data in the diffusion process. Its idea is simple: adding directional diffusion on data, preserving more information than vanilla diffusion. Its model is simple, leveraging 4 layers GNNs and one layer MLP, it achieves improvement in several benchmarks. The idea is elegant.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
Good idea! The work not only offers inspiration to the graph learning community, but it also contributes to the diffusion model. The two constraints added in the work are elegant and proper.

Weaknesses:
The writing is not so well, especially the formula part. There are too many mistakes there. Their performances aren't satisfying, but I believe the underlying problem doesn't come from the pipeline, it comes from the simple model. There aren't convergence proofs for their learning architecture. Noticed that the diffusion model, which stems from the quasi-static process, possesses a good property of converging. I hope to see proof of the effectiveness of the constraints adding to the original process.

Limitations:
1. They haven't provided limitations for their work. 
2. Though the idea is inspirational, the writing is not good, and there are too many writing mistake.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a method named Directional Diffusion Model (DDM) for unsupervised representation learning, targeting applications in graph and node classification. The model's performance is evaluated on various benchmark datasets and compared to both unsupervised and supervised models. The results demonstrate that DDM can effectively learn meaningful graph- or node-level representations. Furthermore, the paper offers some exploration of how different types of noise impact the learning process.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors present empirical evaluations of DDM on multiple benchmark datasets, which validate the effectiveness of the proposed method to some extent.

2. The paper includes an investigation into the impact of different types of noise, which adds depth to the analysis and understanding of the proposed method.

Weaknesses:
1. The SVD visualizations could be more insightful. It's observed that the 2D projections computed from graph datasets appear biased and are predominantly on the right plane. However, the methodology behind these visualizations remains unclear. Are they based on singular values? Further explanation would be beneficial.

2.  The Signal-to-Noise Ratio (SNR) plots in the supplementary material show comparable results under white noise and directional noise. This implies that the DDM may not offer significant advantages on these datasets. The authors should provide some clarification.

3. The paper is primarily empirical and lacks theoretical foundations, which might limit its generalizability.

4.  The algorithm snippet doesn't seem to provide sufficient implementation details, which might impede attempts to reproduce the study.

Limitations:
The authors did not explicitly state any limitations of their study. However, the identified weaknesses could be viewed as potential limitations. These include the lack of theoretical foundations and potential issues in reproducibility due to insufficient implementation details. Additionally, the paper could benefit from a clear discussion of the limitations of DDM in handling specific scenarios or types of data.


Rating:
5

Confidence:
5

REVIEW 
Summary:
This study proposed adding directional noise on learning anisotropic graphs using diffusion models. The study adds new perspectives in exploring the anisotropic structures in graph data. The numerical results are promising to support the authors' ideas.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I find this is an interesting paper - the rational is convincing, and the authors performed extensive experiments to support the idea. The discussions on the noise and ablation studies provide further insights into understanding the usefulness of adding directional noises. The paper provides a valuable perspective in understanding diffusion models for graph learning.

Weaknesses:
The authors mainly prove the utility of the proposed approach through experiments; there is a lack of theorectical proof. 

Limitations:
The authors have discussed several future directions that can be used to improve the current model.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work proposes a class of diffusion models to improve the accuracy of graph representation learning. The model incorporates both data-dependent and anisotropic noise in the forward noising process, by scaling its magnitude and direction based on each coordinate of the data. This structured noise maintains the signal present in the data over longer time windows than using standard isotropic white noise during the forward noising process. The authors show that this model improves upon state-of-the-art methods on a large collection of benchmark datasets. Moreover, they perform an ablation study to understand the effect of the two proposed modification to the noise process.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The authors visually explain and empirically demonstrate the effect of using non-isotropic noise in the forward diffusion process to improve classification tasks. These noise processes provide clear intuition why this is preferred to white noise for these tasks.
- The authors propose a novel strategy for extracting graph representations based on time-dependent denoiser that combines graph neural networks and UNet architectures.
- The application of diffusion models to graph representation learning is novel and the new model is shown to yield superior results to existing algorithms.

Weaknesses:
It would be great for the authors to comment and compare with other non-isotropic noise processes that have been proposed for diffusion models for sampling and generative modeling. Some examples outside of the graph representation learning context are: 
* Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models, Vikram Voleti, Christopher Pal, Adam Oberman, 2022
* Blurring Diffusion Models, Emiel Hoogeboom, Tim Salimans, 2023

In addition to the changes in the noise process, the authors propose a specific architecture for the denoiser, and selection of representative features from their hidden layers. This architecture could be relevant on its own to extract representations of the dataset, without the denoising time components. Do any of the compared methods investigate how this denoiser architecture, without the diffusion model, would perform for representation learning. This might be helpful as an additional ablation study to see the effect of the architecture. 

The metrics used to evaluate their experiments can be described in more detail. The values for the results in Tables 1,2,3 were not clearly mentioned in the caption or the main text. Mathematical equations may also be helpful to precisely describe the accuracy measurements in Figure 5. 

The authors comment that the word vectors often exhibit greater anisotropy, which yields superior performance in node classification tasks. It would be great if the authors could quantify this to validate this claim that the performance improves with more anisotropy.

Limitations:
The authors show improved results on almost all datasets. It would be great if the authors could comment on what leads to the similar performance on the Ogbn-arxiv dataset. Is it because the data is more isotropic in this case? When do the authors expect the algorithm to under-perform for the evaluated tasks?

In addition to the questions above, it would be good to address these minor comments:
- Define the acronym for their proposed framework, DDM
- Include standard errors for the results in Table 3
- Explain why $f_\theta$ depends on $A$ in equation (4). It is sufficient to highlight that this is the structure of a GNN, which may be helpful for some readers.
- Clarify in Figure 4 that the directional noise is only added to $X_0$ and not $A$.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors address the gap in unsupervised graph representation learning by exploring the use of diffusion models. They propose directional diffusion models that incorporate data-dependent, anisotropic, and directional noises in the forward diffusion process to better handle anisotropic structures in graphs. Experiments on publicly available datasets showcase the superiority of their models over state-of-the-art baselines, demonstrating their effectiveness in capturing meaningful graph representations. Overall, the paper presents a compelling approach that contributes to the advancement of unsupervised graph representation learning.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. **Motivation**

    The introduction is well-motivated, providing a thorough explanation of the challenge and task at hand. The authors go beyond textual descriptions and use simple visualizations on both real and synthetic data to demonstrate their points effectively. This approach enhances the clarity and understanding of the presented research, making it accessible to a wider audience.
&nbsp;

2. **Method**

    The authors present a straightforward yet effective solution for incorporating directional noise into node embeddings. This approach effectively addresses the challenge posed by anisotropic structures in various domains, including graphs and potentially text data. Their proposed method demonstrates promising results in handling directional noise and enhancing the quality of node and graph embeddings.
&nbsp;

3. **Architecture**

    I appreciate the authors intention to adapt the well-known and effective U-Net architecture from the image domain to the graph domain. The incorporation of skip connections in the U-Net is particularly relevant for denoising tasks. This thoughtful adaptation enhances the model's ability to handle graph-related denoising effectively.
&nbsp;

4. **Experiments**

    The authors conduct a comprehensive comparison with numerous baselines across multiple datasets. Furthermore, their evaluation settings, which involves 10-fold cross-validation with standard deviation after five runs, are robust and reliable. This rigorous evaluation methodology ensures the validity and statistical significance of their results.

Weaknesses:
1. **missing releted work**

    There are existing works in the intersection of graphs and diffusions are missing, contradicting the authors statement ""To the best of our knowledge, there have been no works for diffusion-model-based graph representation learning."". Some for example:
    - Niu, Chenhao, et al. ""Permutation invariant graph generation via score-based generative modeling."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
    - Xu, Minkai, et al. ""Geodiff: A geometric diffusion model for molecular conformation generation."" International Conference on Learning Representations, 2022.
    - Vignac, Clement, et al. ""Digress: Discrete denoising diffusion for graph generation."" International Conference on Learning Representations, 2023.

    Furthermore, some simple techniques can be applied to create a smoother SNR curves over the different diffusion steps. For instance:
    - Chen, Ting. ""On the importance of noise scheduling for diffusion models."" arXiv preprint arXiv:2301.10972 (2023).

    The same problem was presented in it over the image domain (on high-resolution images), and the solution was to use different noise schedulers. Why not simply try this trick?
&nbsp;

2. **missing details**

   There are missing details in the paper that makes it hard to fully understand and reproduce the papers results. For example:
    - ""µ and σ are calculated using graphs within the batch.” -- what is done during inference? it is EMA over what was been seen in training time? does it handle only batch inference?
    - They paper did not state how exactly the entire graph-level representation is obtained, it is sum/mean of all node representations?
    - Algorithm 2 in appendix, line 6: what is exactly $A_t$? should it be just $A$?

Limitations:
The authors address the limitations of their work, and support their claims with experiments and ablations.

Rating:
7

Confidence:
4

";1
X6mwdEVYvc;"REVIEW 
Summary:
This work considers the convergence of discrete time interacting particle systems to their respective continuous time limits (i.e, McKean-Vlasov type equations) under general assumptions which are applicable to varied contexts like neural networks, kinetic theory, game theory and sampling algorithms.  The finite particle + finite step size algorithms are considered to be stochastic approximations of the mean field limit and convergence is analyzed in terms of dynamical systems theory.  This work considers the notion of weak asymptotic pseudo-trajectory to show that the stochastic approximations are close to mean field limit under general conditions. These are then applied to various specific contexts like neural networks and SVGD to derive convergence bounds. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The generality of the assumptions and the framework is the main contribution of this work. This enables the authors to derive several useful results in varied domains under a common framework. This can be a useful tool in establishing convergence to mean field limits in new problems without requiring elementary analysis. 

Weaknesses:
1. The notation in the algorithmic template is extremely confusing. Shortening $b(x,\mu)$ to just $b(x)$ makes it very confusing. Population level SA is also a bad terminology since it confuses the reader about whether this is the mean-field limit (i.e, $n \to \infty$) or not (because the mean field limit is often referred to as the population limit).

2. WAPT as the notion of convergence requires more justification. This is so since the continuous process begins at $X_t$, the $t$-th time instant of the discrete time process and the uniform convergence is established as $t \to \infty$. What if the initial deviation in the stochastic approximation ensures that $X_t$ itself is not likely to be reached by PSDE ? 

3. Assumption 5 is a bit non-standard. Also, I think there is a typo here. Assumption (11) is not satisfied for any decreasing step size sequence since $\gamma_{k+1}/\gamma_{k+2} > 1$. Please clarify and state what exact step sizes are allowed.

4. Under specific settings, much stronger results can be derived for convergence when the algorithm is designed specially or under specific assumptions like logarithmic sobolev inequalities (even with finite particles and constant step sizes). This framework precludes such analyses.  (See [A1,A2]). I am not very well versed in the game theory or kinetic theory literature, so I will abstain from commenting on these results. 

[A1]  Convergence of mean-field Langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction.

[A2]  Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation


Limitations:
The authors have a good discussion on the applicability of their work. I think they should discuss the drawbacks of WAPT better. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper analyses discretisations of mean-field type SDEs arising in several areas of machine learning. The main contribution is a convergence result (Theorem 1) stating that under appropriate conditions on the drift and diffusion coefficients, the discretised dynamics convergence in 2-Wasserstein distance, in an infinite time horizon limit, to the continuous-time system of interacting particles. Using the classical uniform propagation of chaos for interacting particle systems to show convergence to the mean field dynamics. Applications to Two-Layer NNs training by SGD, Stein Variational Gradient Descent, Two-Player Zero-sum Continuous Games and Kinetic Equations are presented.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
As outlined by the authors, SDEs of mean-field type arise in several areas of machine learning and statistics as continuous-time and infinite-number-of-particle limits of discrete stochastic difference equations. Therefore, a convergence analysis of the discrete schemes to the their continuous-time counterparts is of high relevance and importance within the modern machine learning landscape. The paper is well-written and clear, its structure is easy to follow. The four examples on Two-Layer NNs, Stein Variational Gradient Descent, Two-Player Zero-sum Continuous Games and on Kinetic Equations nicely demonstrate the applicability of the main result (Theorem 1) to several topics/areas of modern machine learning.

Weaknesses:
There is a rich body of literature on SDE discretisation scheme for McKean-Vlasov SDEs and interacting particle systems [1, 2, 3], that is completely ignored by the authors. The results of these papers concern convergence of Euler-Marayama and/or Milstein type numerical schemes to the limiting mean-field equations when the step size of the solver goes to zero and the number of particles goes to infinity. As far as I see, the main difference in the anlyses is in the notion of convergence in time: the authors consider as convergence criterion the *Wasserstein asymptotic pseudotrajectory* (WAPT), which is a large time behaviour from dynamical systems theory, while in the aforementioned series of paper the convergence is in terms of discretisation step, which is more classical in (numerical) stochastic analysis. Albeit the two notions of convergences are different, I think an in-depth discussion and comparison between the two is required.

I invite the authors to initiate a conversation on the above during the rebuttal period. If a rigorous and fair comparison/discussion is eventually presented, I will happily increase my rating.

**References**

[1] Bao, Jianhai, et al. ""First-order convergence of Milstein schemes for McKean–Vlasov equations and interacting particle systems."" Proceedings of the Royal Society A 477.2245 (2021): 20200258.

[2] Reisinger, Christoph, and Wolfgang Stockinger. ""An adaptive Euler–Maruyama scheme for McKean–Vlasov SDEs with super-linear growth and application to the mean-field FitzHugh–Nagumo model."" Journal of Computational and Applied Mathematics 400 (2022): 113725.

[3] Leobacher, Gunther, Christoph Reisinger, and Wolfgang Stockinger. ""Well-posedness and numerical schemes for one-dimensional McKean–Vlasov equations and interacting particle systems with discontinuous drift."" BIT Numerical Mathematics 62.4 (2022): 1505-1549.

Limitations:
The authors adequately addressed limitations of their contribution and discussed future work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper develops a theoretical mathematical framework to characterize the convergence properties of discrete particle systems to their mean-field limit.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
The mathematical theory in this paper is beyond my scope, but it appears to be mathematically sound. The paper is well-written.

Weaknesses:
I believe that this paper would benefit from some applied tests/results to show practical relevance. For example, how would it help training a GAN? Do the theoretical convergence results help actual training? How do the results compare to actual training? Are the bounds tight relative to actual convergence?

Limitations:
Limitations do not seem to be explicitly addressed by the authors.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper fills a theoretical gap between the application of ideas interacting particle systems to algorithms in machine learning--algorithms, like SGVD, that are almost always realized as discrete-time routines with a finite number of particles--and the substantial existing body of theoretical work on finite particle systems with continuous dynamics. These latter works have yielded valuable insights about, e.g., the training process of two-layer neural networks, algorithm design for approximate Bayesian inference, or the nature of equilibria in games. However, they have not rigorously established the convergence of discrete-time to continuous. This paper establishes that convergence via Benaïm and Hirsch's notion of a Wasserstein asymptotic pseudo-trajectory (WAPT), which gives a measure of asymptotic closeness (in the Wasserstein-2 sense) between two stochastic processes. Specifically, via Theorem 1, the convergence of the family of (discrete-time) stochastic approximation algorithms (SAA) is reduced to its continuous time counterpart. Combining Theorem 1 with existing results in the literature yields the conclusion that the empirical distribution of particles following the discrete-time SAA converges to the mean-field solution, as desired.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The central originality of this paper lies in its adaptation of WAPT to solve an open problem in the mean-field theory of discrete-time IPS in machine learning.
Broadly, I found the clarity and elegance of the mathematical exposition to be exceptionally good. The paper persuasively argues for the importance of a rigorous theory of convergence, and smoothly introduces concepts and definitions needed for understanding Theorem 1, while re-orienting the reader by summarizing previous results at effective moments. After stating the Theorem, the applications of the theory to two-layer NNs, SGVD, games, and kinetic equations were clear and enlightening.
The significance of the presented comprehensive framework is high, as the future directions section makes clear.

Weaknesses:
I found the paper without major weaknesses. In terms of the overall presentation of results, I was surprised to see interacting particle systems (IPS) as the frame for this theory rather than simply continuous-time Markov jump processes. I can see the value in specializing to interacting particle systems, but some readers may be deeply acquainted with continuous Markov jump processes and be largely unaware of the IPS literature. Bringing that connection onto the screen by mentioning it in the technical background may help orient readers with a more general stochastic process background.

In the same vein, a brief mention of the relationship to multi-agent systems could be of value in ensuring that this work reaches the wide readership for which its theory is relevant.

Limitations:
The authors have adequately address the limitations of their work. I see no potential negative societal impacts.

Rating:
9

Confidence:
3

";1
YsZTDcIQwQ;"REVIEW 
Summary:
In this work, the authors propose a Spatial-Temporal Diversification Network (STDN) for video domain generalization. First, they intrdouce a spatial grouping method to summarize the spatial clues in each frame. Then,  they further build up spatial-temporal relations in a multi-scale manner. Finally, they show the effectiveness of the method via different experiments of video domain generalization.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The domain generalization problem is important for video understanding in practice.
2. The paper is written in a good structure, basically.
3. The experiments somehow show the effectiveness of the design.

Weaknesses:
1 Design.

I am not quite convincing by the proposed design in the paper. Bascially, the spatial grouping (or clustering) or spatial-temporal relation modeling are not particularly designed for domain generalization. It could be used for the traditional video classification problem without any difficulty. Why are these designs important for domain generalization?  

2 Experiment.

2.1 The setting is not quite challenging, actually. The data sets bascially belong to the same domain. It would be interesting to see the cross domain setting, like action recognition from dark videos in UG2+ Challenge.

2.2 It would be interesting to show the results of traditional video classification setting on the popular benchmarks, like Kinetics400 or Something-Something V1 or V2. 

Limitations:
Please see the weakness section.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this manuscript, the authors proposed a novel Spatial-Temporal Diversification Network (STDN) for video domain generalization.  More precisely, the proposed method introduces the Spatial Grouping Module and Spatial=Temporal Relation Module to discover various groups of spatial cues within individual frames and to model spatial-temporal dependencies. Experimental results on three benchmarks show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written and well-organized. 

The proposed method achieves state-of-the-art results across three benchmarks.

The proposed method is straightforward and interesting. 

Weaknesses:
The Temporal Relation Module is derived from the work cited as [6]. It would be beneficial for the authors to acknowledge this in their manuscript.

In my opinion, the proposed spatial grounding module is similar to spatial attention and the KNN model. The authors are suggested to conduct an ablation study to compare these methods for a more thorough analysis.

The proposed method is founded on Temporal Segment Networks TSN). However, the comparative methods use ensembles with various backbones. It would be advisable for the authors to replicate this approach for a fairer comparison.


Limitations:
N.A.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper addresses the problem of video domain generalization for classification task. The core idea of the paper is to enhance the diversity in class-correlated cues both in spatial and temporal dimensions with the assumption that in this diverse pool, it is more likely to capture the domain-generalizable features. To capture diversity in spatial dimensions, a spatial grouping module is proposed which forms K integrated features representing K groups of features by aggregating spatial features in each group. Two different entropy-based losses are used to ensure diversity in spatial cues. Next, the paper learns spatial relation features by sampling the integrated feature from different space scales. These spatial relation features, limited to space only, are then leveraged to learn temporal relation features. To improve the effectiveness of temporal relation features, a relation discrimination loss is also used to avoid collapse of learned temporal relation feature. The overall loss for optimisation is composed of task-specific loss, two entropy losses and a relation discrimination loss. Experiments have been conducted on three different datasets and the results claim to achieve better performance than the existing method and other image-based DG baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1) Generalizing to novel domains for video modality is an important and challenging task and it carries several applications in real-world. Also, not much work has been done for video domain generalization.

2) The idea of spatial-temporal relation feature is interesting to capture the diverse class-correlated cues in search of domain-invariant features in the video data.

3) Results claim to demonstrate better performance against competing methods in all three datasets, including EPIC-kitchens-DG, UCF-HMDB, and Jester-DG.

4) Ablation studies show the clear performance contribution of different components in the proposed method.

Weaknesses:
1) The analyses of spatial grouping using the tSNE in Fig. 6 is not very convincing. It is not very clear how the claim that the spatial grouping does the feature group of spatial features is justified in this diagram. It is important to better understand either by visualizing or some other quantification measures that what is the clustering ability of spatial grouping mechanism.

2) It is not very clear how the diverse class-correlated cues in space and time, for which the spatial-temporal diversity module is developed,  are domain-invariant information that the paper claims to extract (L:6). Fig. 6 uses Grad-cam visualization to show the attention heatmaps but it is very general and can be applicable to any classification task.

3) In table 1, the improvement from UCF to HMDB is not very encouraging over VideoDG [13]? The paper doesn’t discuss any potential reasons for this. In fact, the the performance improvement from UCF to HMDB is less than than VideoDG [13] if Mixstyle [67], which is an off-the-shelf component,  is not used in the overall framework.

4) What is the performance of the method when only using MixStyle [67] for the datasets used in Table 3?

Limitations:
The supplementary material mentions that the work doesn't consider the multi-modal nature of video data as it contains different modalities such as, RGB data, optical flow, and audio.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes Spatial-Temporal Diversification Network (STDN) for Video Domain Generalization (VDN). VDN is a new problem which is similar to video domain adaptation, but more challenging due to no unlabeled videos from target domain is provided. STDN is mainly designed into two modules: Spatial Grouping (soft-clustering) and Spatial-Temporal Relation (similar idea as TRN [6]). Experiments are done on 3 different benchmarks: UCF-HMDB, EPIC-Kitchens-DG, Jester-DG with good improvements over baselines. Written presentation is clear and mostly easy to read. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The motivation of the proposed method is clearly presented and experimental results are solid, i.e., good improvements over baselines.
* Various ablations, qualitative analysis provide better understanding about the proposed method.
* Written presentation is clear and easy to read and understand.

Weaknesses:
* Missing a direct comparison with [14], even though [14] may use additional audio modality, an attempt to compare with [14] may make the paper more solid.

* Although the problem of general domain generalization has been studied recently, the video domain generalization is less explored, which can be either good (this paper and a few other [13,14] are among the first ones) or bad (the problem is too small with limited impact).
 

Limitations:
The reviewer does not foresee any potential negative social impact of this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents STDN, a spatio-temporal diversification network designed for domain generalization. It introduces a spatial grouping module that effectively groups features from individual frames across different spatial frames. Additionally, a spatio-temporal relation module is proposed to model spatial-temporal correlations at multiple scales. The experiments demonstrate the network's good performance on three benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well written and easy to follow
- The results on four setup is comprehensive (I do have concerns on the results, pleas also read the next section)


Weaknesses:

Regarding the methodology:
- The author asserts that domain-specific cues are crucial for achieving good generalization (L43-45). Subsequently, the paper states that spatial grouping is employed to enhance the diversity of spatial modeling. It is necessary to provide further justification as to why this diversity aids in learning domain-specific features instead of introducing noise.
- The term ""domain-specific feature"" is frequently used in this paper; however, it is never explicitly defined. Moreover, the results fail to substantiate that the proposed method effectively learns these domain-specific features. Taking the basketball example into consideration, the backboard can be regarded as the domain-specific feature within the training set due to its construction, and the basketball itself might not be the case as you can kick the basketball, so what is the domain specific feature? should it be data driven or manually defined? When the paper claims that the proposed method can learn more representative features, supporting evidence must be provided. Currently, aside from visualization (which will be discussed later), there is a lack of evidence to support the theory that the proposed method effectively learns domain-specific features. 
- It is important to note that improved results do not necessarily demonstrate that the proposed method resolves the domain generalization problem. For instance, if a stronger backbone were employed, significantly better performance could be achieved under the same experimental conditions; however, this would not imply that the stronger backbone more effectively addresses the domain generalization issue. To this end, the most straight forward experiments would be comparing against the baselines use I3D backbone, as I3D learns spatio-teamporal feature without grouping and multi-scale.

Concerning the results:
- Firstly, it should be noted that the works listed for comparison in Table 1 are incomplete. The authors could easily find numerous works on UCF-HMDB that exhibit considerably better performance.
- Given the substantial emphasis placed on spatial modeling and spatio-temporal modeling, it is crucial to compare the proposed method against previous works that utilize 3D backbones, such as CoMiX (refer to https://arxiv.org/pdf/2110.15128.pdf), which demonstrates superior performance. Additionally, it is important to include the set of baselines cited by this paper and establish a fair comparison, such as using ResNet101 as the backbone.
- It is worth pointing out that there may be instances where Grad-CAM highlights the ""domain-specific"" features, yet the network makes incorrect classifications. Thus, Grad-CAM alone cannot serve as conclusive evidence for improved domain generalization.

Limitations:
None

Rating:
5

Confidence:
4

";1
Og2HCj3V1I;"REVIEW 
Summary:
Paper proposes an evaluation metric for generative models which compare the distributions of real and generated images using a predefined set of attributes, or pairwise occurrences of attributes. The advantage of these metrics over the previous work is that they provide explicit visibility of which aspects contribute to the final metric value.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea for the proposed metric is novel – it uses language-image models to measure alignment of distributions of real and generated images given a set of text attributes. Additionally, the metric is highly customizable for downstream tasks because users may define their own set of attributes that are of interest to the specific task and drop irrelevant attributes.

Weaknesses:
Failure modes of the metric are not discussed (or limitations of language-image models and how they affect the metric). 

Consider a scenario where there are two models, A and B, with the same SaKLD. Model A produces essentially perfect alignment on all attributes other than one which fails dramatically, causing a large spike in SaKLD histogram, and this attribute is the only contributing to the final score. Model B on the other hand performs poorly across all attributes but averaging over attributes yields the same SaKLD score as the model A. In this scenario SaKLD would potentially not agree with human judgment, since failing in a single attribute might not be visible when inspecting large image grids. Can this kind of scenario occur in practice, and if it can, what would be your recommendation for the user of the metric in that case?

Fig. 4 shows that SaKLD and PaKLD are dominated by few attributes of attribute pairs. Is this usually the case in practice? Fig. 5 (b) also indicates that adding new attributes contribute to the metric with diminishing strength. This might be misleading for the user of the metric. Intuitively, adding a large set of attributes should correspond to more thorough evaluation of the model, however, this might not be the case if few attributes are dominating the final value of the metric.

The empirical effectiveness of the attribute based metric is not fully demonstrated. The authors advocated for an interpretable metric but unfortunately end up comparing modern generative models using single scalar numbers (as the existing metrics do), instead of taking advantage of the interpretability of the metric and showing a more fine-grained analysis of the models.


Limitations:
The authors adequately addressed the limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new metric to evaluate the quality and diversity of generated images based on
interpretable embeddings. To obtain the interpretable embeddings for selected attributes, the cluster
centers of the encodings of two separate encoders, one for images and the other for the text attributes
are calculated and the interpretable embeddings are the difference between the encodings of each
image and attribute and their respective cluster centers. The interpretable embeddings represent the
direction in which a particular embedding lies with respect to it’s cluster center. The CLIPScore
between the interpretable embeddings of an image and an attribute is calculated and is named as
’Directional CLIPScore’, since the interpretable embeddings represent the ’direction’.
The authors have proposed two metrics : 1. SaKLD - to quatify how closely the attribute distri-
bution in generated images matches with that of training images. 2. PaKLD - to quantify correlations
between different attributes. The KL divergence between the probability density functions of the
Directional CLIPScores for all the images in the train data and generated data.
Using SaKLD, the KL divergence between the attribute distribution in training and generated
images is calculated. PaKLD calculates the KL divergence similar to SaKLD, except that the presence
of a pair of attributes is required in the training and generated images.


Soundness:
1

Presentation:
3

Contribution:
1

Strengths:
The motivation for the idea is good and has a huge potential impact for improving evaluation of generative models.

Weaknesses:
- Although, the motivation for developing this metric is valid, the overall methodology and the
experimental results are not convincing for the use of this metric. Also, the experiments performed
are inadequate and do not sufficiently justify how well the metric performs compared to the previously
proposed metrics. Additionally, previous metrics can directly evaluate quality of generation based
on the generated images alone, but this metric heavily relies on the attributes in the form of text
descriptions. Thus, it limits the applicability and generalizability of this metric.
- The proposed metrics use CLIPScore (which already exists) for interpretable embeddings and then
applies KL Divergence for the PDFs of the ClipScores of images and attributes, thus, showing limited
novelty.
- Most of the paper is easy to follow but some important parts like, how is the center of text attributes
calculated, results from table 1 (what does accuracy stand for) etc. are a bit ambiguous. There is a
lot of scope to improve the technical soundness of the paper. Although some of the popular metrics
1are mentioned, there has been a lot of work in the generative modelling domain which the literature
survey must cover. The proposed methodology is not very sound and is not well supported with the
experimental setup. The results are also not sufficiently explained. Diversity is the main motivation
for the paper as it is mentioned in the abstract but any theoretical or empirical work to support it is
completely missing.
- Motivation and methodology including the steps to evaluate generated samples is understandable but
some parts are ambiguous (please refer above comments).
- Based on the current state of the experiments, the contributions don’t seem to be significant as there
is not enough validation to support the claims.

 typos:
- Line 43 : Instead of ”Figure 1 (b)”, it should be Figure 1 (a)
- Line 168 : Section 3.3 First letters of all the words in the heading must be capital
Other remarks :
- Line 56 : Instead of ”If the model lacks essential attributes” the following sounds technically
correct ”if model lacks ”representation” of essential attributes”.
- Line 56-58 : This claim does not seem to be correct.
- Line 147 : Link to the specified figure is missing
- Figure 2 is never refered to in the text, is it an unnecessary figure?
- Line 253 : Generated set has non-smiling men and non-smiling women. But Figure 4 caption
says otherwise

Limitations:
limitations have been addressed

Rating:
4

Confidence:
2

REVIEW 
Summary:
The embedding space used for calculating FID is computed with Inception V3 which is trained for image classification as the target task. This means it is more likely to capture discriminative features, raising doubts about its ability to effectively evaluate generative models. There is also a need to devise a new evaluation metric that can interpret underlying factors. This paper introduces a method called Directional CLIP Score (DCS) to properly evaluate this. The pre-trained CLIP is used as the embedding space. In particular, the paper proposes ""Single attribute KL div (SaKLD)"" to measure single attribute alignment and ""Paired attribute KL div (PaKLD)"" to measure multiple attribute alignment as new metrics. This paper provides some insightful measurement result using the proposed metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The effort to evaluate generative models from various perspectives through the introduction of such metrics can be considered novel and a contribution. Especially considering the current issues such as the bias in stable diffusion [1], the proposal of such metrics can bring benefits to the field from the perspective of trustworthy AI.

Weaknesses:
It appears to be an intrinsic limitation that a significant number of samples (50k) are still required to obtain stable results. Nonetheless, thanks to the reported findings, we can gain more insights, and I appreciate that. 
Remaining concerns are written in [Questions] section.

Limitations:
It seems that there are no particular specific limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This is a very interesting research work. The main contribution of this paper is to consider the attribute information in the original training data when evaluating the quality of images generated by the model. There are two benefits to this approach: 1) determining whether the model can correctly imitate the distribution of the training data; 2) explaining which attributes the model does not perform well on. In implementing this idea, the authors found that directly calculating the CLIPScore between the image representation and the text representation of the attribute does not yield distinctive results. Therefore, they proposed Directional CLIPScore (DSC). The main idea of this approach is to move the reference point for calculating vector similarity to a more reasonable point. At the same time, they proposed two methods to apply DSC, one is Single attribute KL Divergence, and the other is Paired attribute KL Divergence (PaKLD) considering the combination of attributes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. I think this research question is very interesting. It is very meaningful to use the attribute information of the dataset to help evaluate the quality of the model's generation.
2. The Directional CLIPScore (DSC) proposed by the authors is very concise and appears to be quite effective in the case study.

Weaknesses:
1. In Section 3.3, I agree with the extraction of attributes using the BLIP and manual annotation methods. However, one method of extracting attributes is to generate an attribute list with GPT and then filter it with training data. I think the attribute list generated by GPT in advance may bring biases. The core of this paper is mainly to study the correlation between the model and the training data. However, if the list generated by GPT is not the most representative attribute, the results may be biased.
2. The experimental part in Sections 5.1-5.3 does not seem very convincing. The authors mainly verify that the proposed method can indeed be consistent with some expected experimental designs, but there is a lack of more convincing quantitative indicators to show that their proposed evaluation metrics are better than those proposed by others previous research works. I would prefer to see the authors analyze the correlation coefficient between their proposed evaluation metrics and human evaluation, as well as whether their evaluation metrics have improved in terms of correlation coefficient compared to previous evaluation indicators. This is my biggest concern for this work.


Limitations:
see weakness

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes two new metrics allowing to measure and explain the diversity of a generated set of images w.r.t a training set. Instead of the usual distributional distances relying upon an embedding space from a pre-trained model, these metrics rely on a set of textual attributes. The similarity between an image and an attribute is computed using their representation in a common semantic space, via the CLIP model - vectors are shifted using a centre of training images/attributes to make similarity scores more meaningful. Several ways to obtain attributes (Captioning, User-based or GPT-based) are investigated. The usefulness of the metrics is tested with an experiment injecting images correlated with target attributes in data, an experiment aiming to detect a specific attribute relationship in a curated dataset, and in a comparison of several generative models. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- This paper proposes an interesting approach, aiming at using the common semantic space between images and text proposed by CLIP to measure attribute and image relatedness, in order to provide interpretable representations of generated images and measure attribute-based metrics between a reference and generated dataset; such an application seems relatively original to me. 
- The two first experiments demonstrate well the usefulness of the approach, able to detect a shift when the data has been curated by human, based on which attributes. 

Weaknesses:
- The presentation of the paper could be improved upon. This include the writing, and the readability of the figures, as well as the quantity of information provided. This last points concerns mainly the presentation and framing of the problem of interpretability of representations, as well as the presentation and motivation of the experimental settings. 
- In particular, the paper lacks related work on concept-based representation for interpretability of images. While building metrics dedicated to generative model seems new to me, there are based on an idea which has been explored extensively before. See for example ""Concept Whitening for Interpretable Image Recognition, Chen et al, 2020"". 
- The paper focuses on a narrow choice of methods to generate attributes, which, to me, should be one of the key experimental investigation of the paper. Notably, the previous literature explores using different kind of attributes, coming from existing data (for example, ""Interpretable Basis Decomposition for Visual Explanation, Zhou et al, 2018"") or to be learnt (""A Framework to Learn with Interpretation"", Parekh et al, 2021). The authors only (very shortly) argue about the number of needed attributes.
- The toy experiments seem relevant but are very fastly presented and should be expanded upon. The remaining experiments are too short to be convincing and only focus on a handful of models. 

Limitations:
- Previous distributional metrics are several times referred as relying on external models in your paper. However, the attributes that you use also rely on external models (except the USER one, of course - but in this case, the computing of DCS still relies on a captioning model). How would you address this issue ? 

Rating:
6

Confidence:
3

";0
Ki6DqBXss4;"REVIEW 
Summary:
The general scope is online classifier learning under online label shift where the key assumption is that the class conditionals $Q(x|y)$ are invariant. The paper focuses on both unsupervised and supervised problems, which they coined as Unsupervised Online Label Shift (UOLS) and Supervised Online Label Shift (SOLS).

Key previous works in the UOLS problem [76, 8] setup a regret function and adapt the initial classifier via minimising the regret function over time.  [76] optimises the classifier via reweighting the predicted distribution $\hat{q}_t$. [8] retrains the classifier at every step. However, both suffer from convexity issues of the losses. This paper integrates the best of both [76] and [8] in a single framework. On top of that they avoid any previous convexity issue by reducing the online label shift problem to the problem of online regression of the class marginal distribution function. Tight theoretical bounds under the usual Lipschitz condition are provided.

The second part of the paper is an extension of the approach from the UOLS problem to the SOLS problem. The main contribution here is that theoretical bounds in (weighted) Empirical Risk Minimization are generalised to the SOLS context. Other contributions involve in making online model training faster without compromising the theoretical bounds too much.

Finally, there is an extensive empirical study based on well known datasets. However, in many of the datasets used for the experiments, the label shift dynamics are simulated.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is very well written and very detailed. The authors were very clear of their contributions and limitations of the proposed approaches.

In my view, the key novelty of the paper is at reformulating the online label shift problem as the problem of online regression of the marginal distribution function, which previous works have overlooked. Everything else in the paper develops from that idea in naturally. This key novelty is clever because the new problem can be solved efficiently with existing tools.

The provided bounds appear to be quite tight in terms of complexity, although I am not sure whether in practice the coefficient of the highest order term plays a significant role. Proofs in the appendices are simple applications of probabilistic tools, concentration theories and a few key results in online learning theories. I was able to follow without any major trouble.

The finding in appendix G.6 is interesting, which provides an evidence that retraining does not work better than reweighting, despite an additional computational cost for retraining.

Weaknesses:
Although well acknowledged in the paper, the fact that the approach, like [8], requires memory that scales linearly with time is quite a concern from a practical point of view. It would be good to have some quantification results regarding memory usage in the experiments.

Although the paper addresses online label shift, among 6 datasets used in the experiments for UOLS and SOLS, 5 of them use simulated label shift data. Only SHL contains real-world label shift data.

I understand that the scope of the paper is online label shift where $Q_t(x|y)$ is unchanged over time. Still I find this scope is rather restrictive compared to real-world data. In the data collection business that I have been involved, data are usually collected from many sources, with large differences in collection periods and frequencies of data being collected per source. With external factors like that it is hard to assume that $Q_t(x|y)$ is unchanged over time. The SHL dataset used in section 5 seems to be the only dataset in which the label shift part of the data is real. However, given that it was collected by only 3 participants, it is hard to justify whether the dataset reflects what happens in the real world.

Perhaps the authors could try with some recent consumer-related datasets (e.g. clothes, food) that contain both pre-Covid and post-Covid periods? During the Covid-19 pandemic, so many shops closed, leading to a large-scale gradual decrement of data that can be collected. Since 2022, many shops have reopened, leading to a gradual increment of collected data.

Maybe a typo. However, on line 112, why does $\delta$ not play a role in the big $\tilde{O}$ function?



Limitations:
I do not see any potential negative societal impact of the work.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The authors study the important problem of learning in an online setting.
In particular, they focus on two important problems:
- Online Unsupervised learning with Label Shift (UOLS)
- Online Supervised Learning with Label shift (SOLS)
In both cases, they assume, as in previous work, that the distribution of the samples can change at each step, however, the marginal distribution of the covariate X  given the label Y is the same. Their algorithm is adaptive and does not require knowledge of the distribution drift. They show that their results on dynamic regret are (almost) optimal in a minimax sense. Unlike previous work, they do not require a convexity assumption on the family of the losses, which is an important technical contribution. They corroborate their theoretical findings with experiments.





Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Main strengths:

- The paper is very well written, easy to follow, and the results are sound. The assumptions used are discussed and well-motivated.

- They provide a major technical contribution in dropping the convexity assumption of the previous work. 

- They show the (almost) tightness of their results in a minimax sense.

- They show an interesting algorithm to reduce the number of training steps in the SOLS setting so that they do not need to train a new model at each step. This is a very interesting result, as this is a common problem of many previous works of learning with distribution drift.

- Their theoretical findings are corroborated by extensive experiments. Moreover, I appreciate the effort of the authors to address many problems related to how to obtain the optimal minimax regret efficiently in practice. (Appendix D).




Weaknesses:
The results are limited to changes in the probability of obtaining an element of a certain class (label shift). This is only a specific setting of learning with distribution drift. Their technique looks limited to this specific setting, as it revolves around estimating (regression) the class probabilities. It does not seem straightforward to extend this work or use the results of this work for the more general problem of learning with generic distribution drift. 

The experiments are only run with respect to data where synthetic drift is introduced.

Limitations:
The authors discuss limitations, assumptions, and possible future work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
this paper applies techniques from online regression oracles to tackle the problems of unsupervised and supervised label shift, where the marginal probabilities of the labels can be changed over time by an adversary. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
the main contributions include:
C1 an algorithm that adapts to the changing labels without knowing the parameter V_T
C2 an algorithm whose online learning assumptions are verified
C3 an algorithm compatible with black box access to the learner, to enable deep neural networks and decision trees to be used as learners
experiments illustrate the proposed algorithm is promising

the paper is well written and extensive experiments are performed with many baselines

Weaknesses:
some minor issues:

1 figure 2 does not explain what the uncertainty bands are. I assume these are std's or are they standard errors? similairly for the tables. figure 2c misses erroprbars or are they not visible? 

2 the appendix is extremely long, I did not have time to go over all the details, and I am inclined to suggest a full paper would be better reviewed at a journal. I thought its a shame that fig. 3 (appendix) which shows interesting trends is not included in the main body. 

Limitations:
none

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper delves into the supervised and unsupervised online label shift problem, which focuses on adapting to changing class marginals using online data. The authors propose new algorithms that convert the adaptation problem into an online regression task, guaranteeing the minimax optimal dynamic regret. These methods leverage online regression oracles to track the shifting proportions. Evaluations of various online label shift scenarios show improved performances of the proposed methods.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The manuscript is neatly structured and clearly drafted.

2. The authors have comprehensively explained both the problem setup and their proposed solutions.

3. The theoretical foundations elaborated in the manuscript appear to be solid. Importantly, the authors extend the concept of non-stationary online learning to scenarios encompassing non-convex loss functions, albeit under a Lipschitzness assumption. This extension significantly broadens the applicability of non-stationary online learning, notably in the context of neural networks or other sophisticated prediction models.

Weaknesses:
While I am mainly positive about the paper. However, there remain some issues that may need to be further addressed for a more compelling submission:

1. The usage of the term ""sample efficient"" in both the abstract and experimental sections appears confusing to me. Sample efficiency is typically understood in a theoretical context. However, the proposed method does not theoretically demonstrate sample efficiency (consistent with previous methods). Moreover, the experiments merely evaluate the size of the hold-out offline labeled data, hardly serving as evidence for ""sample efficiency."" Consequently, the paper does not sufficiently convince me of this.
2. It is commendable that, unlike its predecessors, the proposed method does not hinge on the convexity of the functions (line 37). Nevertheless, given the importance of the Lipschitz constant in this paper, questions arise regarding the control of the Lipschitz constant G in complex models such as Deep Neural Networks (DNNs) and decision trees. For instance, decision trees, lacking continuity, presumably possess an infinite G.
3. In Theorem 6 (line 834), a simple union bound is utilized to derive the lower bound of the online label shift problem, resulting in a term $|\mathcal{H}|$ in the lower bound. Yet, can this bound accommodate complex scenarios where $\mathcal{H}$ is the DNNs or decision trees, given that $|\mathcal{H}|$ would be infinite in such instances?
4. Based on the points raised in Weaknesses 1, 2, and 3, the paper's title, ""Online Label Shift: Optimal Dynamic Regret with Non-convex Implementation"", appears somewhat overclaimed to me:
    * In Remark 10, the authors acknowledge that Algorithm 4 necessitates storing and reprocessing all past data in each iteration, which contradicts the fundamental principles of online learning. Consequently, the term ""Practical Algorithms"" in the title seems overclaimed to me.
    * A more fitting title, such as ""Online Label Shift: Optimal Dynamic Regret with Non-convex Implementation,"" could be considered, given the paper's principal contribution lies in addressing non-convexity. Such a title could potentially draw more attention from readers.
5. The authors assert (line 55) that their approach achieves the best-of-both worlds of previous works. However, the meaning of ""worlds"" is unclear. Could the authors elucidate this further?
6. The theoretical exposition is presented in a complicated manner and is rather daunting to read. It would potentially be more friendly to a broader audience if the authors simplified and condensed the theoretical content.
7. Within the methodology section, the discrepancy between estimated and actual label marginals is converted into a 2-norm gap, which subsequent methods seek to optimize directly. However, in the context of label shift problems, is this direct optimization of the 2-norm (square loss) between distributions appropriate? Is the 2-norm excessively susceptible to out-of-distribution (OOD) data? Perhaps, alternative measures like KL-divergence or cross-entropy could be more suitable?
8. Some other relevant works related to online distribution shift should be considered to be explored and discussed, such as:

    * Kumar et al., Understanding Self-Training for Gradual Domain Adaptation;
    * Zhang et al., Adapting to Continuous Covariate Shift via Online Density Ratio Estimation;
    * Zhou et al., Online Continual Adaptation with Active Self-Training.
9. I wonder what are the ""NA""s mean in the experimental tables?
10. I am unsure about the FLH and Alligator algorithms (line 193). Could the authors briefly introduce these algorithms and explain their function?
11. line 196: The averaging within ""intervals"" xxx, can the author detail explain what the ""intervals"" is? and how to divide these intervals.
12. The paper can be further refined to enhance readability, and there are several typographical errors:
     * line 31: Wu et al. [76] only controls -> Wu et al. [76] only control
     * line 35: However their approach based on -> However, their approach is based on
     * line 82: the extend of distribution drift -> the extent of distribution drift.
     * line 94: i.e. -> i.e.,
     * line 96: respectively -> , respectively
     * line 314: i.e. -> i.e.,
     * line 1055: $\alpha_t = 1-\alpha_{t-1}$.
     * line 1079: to better performance -> to a better performance

Limitations:
As delineated in the Weakness section, the authors could provide a more detailed elucidation of the methodologies employed, and potentially consider revising the title to garner a broader impact. I am open to adjusting the evaluation score if the authors address my concerns.

Rating:
7

Confidence:
4

";1
aZ44Na3l9p;"REVIEW 
Summary:
This paper exams whether existing deep multi-instance learning algorithms are indeed ""multi-instance learning"". Specifically, it proposes a unit test for multi-instance learning (MIL) algorithms. The goal of this test is to examine whether an MIL algorithm satisfies the multi-instance assumption. Two widely-accepted MIL assumptions are tested, the standard MIL assumption, and the threshold assumption. The results show that all attention-based MIL algorithms do not pass the test. However, CausalMIL which is proposed in last year NeurIPS, and mi-Net which is a basic deep extension from mi-SVM, are the only deep MIL algorithms that passed the test. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The motivation of this work is great. As the test results have shown, all deep MIL algorithms based on attention do not respect the standard MIL assumption. Although the results are not theoreticall surprising as attention performs a weighted average, it is nice to have a paper that formally identify the problem with good experiment support.

2. The proposed unit test is model-agnostic. Therefore, any existing or future MIL algorithms can be tested. 

3. The MIL algorithms tested in this work is representative of the current state-of-the-art in classic and deep MIL algorithms.

Weaknesses:
1. The writing, formatting, presentation of this work need significant improvement. See my detailed comments for more.

2. The discussion on the implication of the test results should be expanded. There should be more discussion on how the algorithms that passed the test can be better explored in applications such as histopathology image classification, and the algorithms that failed the test should be treated cautiously in downstream applications. 


Limitations:
The authors can further improve this work by adding limitations on there exists other MIL assupmtions that this work does not test. See the assumptions mentioned in:
Foulds and Frank, A review of multi-instance learning assupmtions. 2010.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work proposes a set of algorithmic unit tests to verify whether multiple instance learning (MIL) models adhere to underlying MIL assumptions. The standard assumption is that a bag of instances is positive if and only if at least one of the instances in the bag is positive, otherwise it is negative (binary classification). The negative instances are considered null (background) instances, meaning the only causal link between instances and bag label is the occurrence of positive instances, and thus models should only be using the presence of positive instances for decision-making. Through the use of three algorithmic unit tests, it is demonstrated that existing MIL models do not adhere to this strict assumption, and instead utilise information from the null instances in their decision-making (i.e., to make a negative bag prediction). The tests are applied to a set of non-deep MIL models (such as SVM models) and deep MIL models.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**Originality**
1. The work is unique in its exploration of whether MIL models adhere to the underlying MIL assumptions that fit the constraints of their problems.
2. The proposed tests are a novel way of constructing train and test datasets where the assumptions hold but it is possible to detect if the model is learning rules that do not adhere to the assumption. 

**Quality**
1. The deep models are well chosen. Many pieces of new research build upon these models, so the assumption is that if the original models violate the MIL assumptions, then the more advanced models will also be in violation.
2. The results of the tests are easy to interpret - it is obvious which models are violating which tests.

**Clarity**
1. The majority of the paper is easy to read and follow.
2. A strong combination of mathematical notation and algorithmic blocks are used to convey the tests.
3. The results are clearly presented and well discussed.

**Significance**
1. The findings that MIL models that are widely utilised in research do not actually adhere to the implicit MIL assumptions they are meant to follow is an interesting result.

Weaknesses:
**Quality**
1. The way concept classes are defined in the algorithmic tests is a little convoluted. For example, in the presence assumption (test 1), two positive class indicators are used: $\mathcal{N}(0,3)$ and $\mathcal{N}(1,1)$, but these belong to the same concept. It is not stated why two positive class indicators are required and why these belong to the same concept class. This appears to unnecessarily complicates the dataset generation process.

**Clarity**
1. $x$ is used in most cases to represent an instance, but Section 3 (lines 156 and 157) use $z$ instead of $x$. I don't see any reason why this is the case - I think it would aid understanding to use $x$ or $x_i$ instead.
2. Line 184 - I believe ""... a testing distribution AUC of < 0.05"" should be 0.5 not 0.05. 
3. The use of $t$ in Section 3 to describe the ""number of items"" is a little misleading as $t_k$ is also used for thresholds in Sections 3.2 and 3.3. Furthermore, I assume number of items in this case is the number of instances per bag? In this case, it may be easier to utilise the notation of $n$ for number of instances per bag as defined elsewhere in the work.
4. The number of instances per bag in the various unit tests is not clear. There are various parameters ($t$, $b$, $k$) used differently in three algorithmic tests that are sometimes undefined. It would be useful to have a summary of the distribution of bags sizes and witness rates for the train/test datasets for each algorithmic test. Please also see questions below.
5. In Section 4, it is mentioned that there are 100,000 training samples - I assume this is the number of bags, but making this explicit may aid clarity.
6. I think extra clarity is needed in the instance generation notation to show that the instance has $d$ dimensions, for example $x \sim \mathcal{I}(\mathcal{N}(a, b), d)$. This would overcome the need to ""abuse notation"" in line 202 and make the algorithmic blocks clearer - at the moment if viewed in isolation it appears the items being added to the bags are scalar values rather than $d$-dimensional instances.
7. It would be useful to have a summary table with simple ticks and crosses to state which models pass which algorithmic tests. At the moment it is difficult to get a high-level summary of the findings (having to scroll between three different tables).
8. Typo on line 438 - ""onlye"".

**Significance**
1. The fact that some models do not adhere to the underlying MIL assumptions is an interesting finding. However, the actual implications of models not following these assumptions is not extensively discussed. The impact of the work would be more significant if real world examples can be given that describe why not adhering these assumptions is a problem. My main argument against this work is that even though some models do not adhere to all assumptions, they still perform well on datasets they are applied to in the literature, so why does failing the unit tests matter? What implications does this have for designing/using MIL models going forward?

Limitations:
Limitations and areas for future work are not clearly listed.

Suggested limitations:
1. Algorithm 2 does not have negative bags that contain neither of the $c_1$ or $c_2$ concept classes (i.e., negative bags have either one instance from $c_1$ or one instance from $c_2$). This may impact what the models are learning and is not discussed.
2. The work discusses multiple underlying concept classes, but only uses binary bag labels (positive and negative bags). Some MIL datasets have multiple positive classes, see the SIVAL and Four MNIST-Bags datasets discussed in [1]. In these datasets, null (background) instances still exist, but the different concept classes for instances relate to different positive classes. An extension to this style of problem is not discussed in this work - would the proposed algorithmic unit tests be able to scale to these datasets?

I think the work is mostly robust and the findings are interesting, but additional clarity and discussion of the impacts is required to push me towards acceptance.

[1] Early, Joseph, Christine Evers, and Sarvapali Ramchurn. ""Model Agnostic Interpretability for Multiple Instance Learning."" ICLR (2022).

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates whether multiple-instance learning (MIL) models actually respect the constraints of MIL problems.

This paper defines MIL problems as classification of bag-of-instances, such that the bag is only classified positive if any instance is classified positive (or a more complex rule based on positive classes of the instances, but crucially, not on negative instances, cf Eq 1 in the paper). Therefore, contrary to classification problems, there is an asymmetry between positive and negative classes in MIL problems.

The authors design tests to check whether proposed MIL models structurally enforce this constraint. More specifically, they design 1 test for the presence MIL, a more complex test for threshold-MIL, and a test to check that for the latter test, no degenerate rule is found. Each test is based on synthetic data, and are designed to fool models not enforcing the MIL constraint.

Experimental results demonstrate that many deep MIL models actually do not enforce the MIL constraint as understood by the authors, except for 2 of them (Table 1).

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- Well written paper, congrats to the authors for the clarity
- The methodology is sound
- The experimental results are compelling

Weaknesses:
- Limited impact: this paper shows that what the community calls ""MIL models"" is closer to ""set-of-instances classifiers"". I think it does not mean that they are irrelevant in some problems; but it is true that if there are issues at stake, one cannot trust these models for enforcing the MIL constraints. To sum up, the main impact of the paper for the community would be to clarify what ""MIL"" actually means.

Limitations:
Yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper deals with multiple instance learning (MIL), where a collection of items is considered in a bag/collection, whereby presence of certain items in the bag implies a positive label for the whole collection, and otherwise the collection has a negative label. The paper discusses prior MIL methods that do not respect implicit MIL assumptions (i.e., learning a degenerate solution), and proceeds to develop algorithmic unit tests to check if a model satisfies those. Their main contribution is:

1. Designing a unified framework for checking MIL assumptions by creating synthetic datasets that check for one or more of the implicit MIL assumptions
2. Running experiments to show how often algorithms fail these tests despite being the most recent ones.
3. Arguing that not passing these tests means a model is not performing MIL correctly, however, passing these tests does not imply certification.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The problem setting and idea is generally interesting. The work seems novel with detailed experimentation.

Weaknesses:
I believe the paper’s writing/organization can be improved. For example:
1. Lines 51-66 discusses the paper’s sections. It would strengthen the paper if instead the paper discusses at least a few of the algorithmic unit tests/MIL assumptions that would give the reader a better understanding of what the paper is trying to do. Reading the introduction does not give specific information like this. Also highlighting how an example popular MIL method fail an important MIL assumption would also make the introduction much more intriguing.
2. Also, **there is no mention of reproducibility in the whole paper other than the title and related work**. I am genuinely curious about how reproducibility is related to the algorithmic unit tests here, or what “reproducibility” means in this context. some clarification from the authors would be important.
3. The paper is missing a “preliminaries” section, and it is not self-contained. It ought to contain a section describing the problem setup of MIL.

Also I think the theorems are rather straightforward and do not contain any particularly interesting insight. They can be seen as **tautological** with the constructed tests.


Limitations:
N/A

Rating:
6

Confidence:
3

";1
VpGFHmI7e5;"REVIEW 
Summary:
This paper proposed an efficien training technique for Vision Transformers, called Patch n’ Pack. Specifically, it packs multiple images of various input resolutions into a single sequence as a batch exmaple. Furthermore, based on the modified architecture, the authors proposed NaViT. By combining Patch n’ Pack and NaViT, the authors conducted extensive experiments on JFT-4B datasets as well as a few downstream tasks. Overall, the method achieves great effciency for pretraining, and the model achieves better accuracy for differnet image resolutions at inference time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Adapting general-purpose Transformers into different input image resolutions is a fundamental research problem. Therefore, the technique that proposed in this paper is important to the community. And it works pretty well on both pretraining and downstream tasks.
2. The experiments are comprehensive. The efficiency gain during pretraining is impressive.
3. This paper is easy to follow. The overall presentation is of great quality.

Weaknesses:
1. Packing examples into a single sequence during training is not new in the literature. However, it's technically new for ViT training.
2. Despite the performance, pretraining on JFT-4B is very expensive, making it difficult for the subsequent works to follow up and compare with. It would be better for the authors to include experiments of pretraining on ImageNet-1K.
3. It is not clear how the memory cost would be under the proposed Patch n’ Pack.


Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on adapting the computer vision model to flexible usage. The authors stand from the ViT architecture and exploit its flexible sequence-based modeling to enable arbitrary resolutions and aspect ratios. The proposed NaViT could benefit the downstream tasks of object detection, image, and video classification. Evaluations on typical ViT tasks show the performance on different downstream tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Exploiting the flexible sequence-based modeling of ViT models is interesting. This paper uses a simple but effective idea to make the image preprocessing match arbitrary resolutions and aspect ratios. The idea is motivated by convincing preliminary experiments. The performance evaluation also presents useful insights into utilizing NaViT’s property.

Weaknesses:
The architecture design of NaViT and its essential components to extract visual features could be introduced to help the readers better understand the techniques.

Although the authors claim the proposed NaViT can be applied to different downstream tasks, the experiments are mainly based on high-level tasks of classification and detection. I am interested in NaViT’s generalization to more low-level tasks, such as super-resolution with arbitrary scales and pixel-level segmentation.

The additional overhead of introducing NaViT as the image preprocessing could be reported.

Limitations:
None.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This passage discusses the common practice of resizing images to a fixed resolution before processing them with computer vision models, which is not optimal. The author introduces a new model called NaViT (Native Resolution ViT) that takes advantage of flexible sequence-based modeling and allows for processing inputs of arbitrary resolutions and aspect ratios with adaptive positional embeddings. NaViT uses sequence packing and token drop which improves training efficiency for large-scale supervised and contrastive image-text pretraining. The author believes that NaViT represents a promising direction for ViTs and offers a departure from the standard input and modeling pipeline used by most computer vision models, which rely on CNN-designed approaches.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The authors present a simple method that significantly enhances the training efficiency of the vanilla ViT model, as evidenced by the results displayed in Figure 1. The observed improvements are noteworthy and suggest potential for practical application. 
- The authors also make a compelling argument, supported by Figure 3, that the conventional practice of resizing or padding images to a fixed size, which has been historically associated with convolutional neural networks, is flawed. Specifically, the authors demonstrate that both resizing and padding can lead to suboptimal performance and inefficiency, respectively. 
- While the overall algorithms are relatively straightforward, they are clearly depicted in Figure 2.

Weaknesses:
- The authors have presented a comprehensive set of experiments to demonstrate their results. However, I would like to raise a concern regarding the absence of comparison with the related works discussed in Section 4. This omission makes it challenging to evaluate the actual improvements over the baseline methods in a fair manner.

- To address this issue, I strongly encourage the authors to provide a detailed discussion on the primary contributions of their proposed methods. It appears that the example packing technique has already been thoroughly discussed in Efficient Sequence Packing [1], and one could simply replace the word tokens with image patches to form the proposed method. Moreover, apart from the example packing, the main difference between ""Patch n'Pack"" and Pix2struct [2] seems to rely solely on the construction of positional embedding. Additionally, it is worth noting that the most recent work [3] also aims at mix-resolution tokenization. While the proposed method may have some differences from existing works, the current manuscript fails to clearly establish the novelty of this paper.

### Reference:
- [1] Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. Submitted to ICLR2022
- [2] Pix2struct: Screenshot parsing as pretraining for visual language understanding. Submitted to ICLR2023
- [3] Vision Transformers with Mixed-Resolution Tokenization. CVPR2023w

Limitations:
I found no potential negative societal impact of their work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose to use example packing to train ViTs, where training examples of various lengths are packed into a single sequence. This requires a few straightforward architecture changes, including modified attention masking, pooling, and positional embeddings. This scheme allows for some interesting ideas, such as variable image resolutions during training, variable token drop rates, and adaptive inference time computation.

Extensive experiments in the paper show that NaViT results in more efficient training and finetuning (in terms of TPU hours) and a better compute-performance tradeoff at inference time. Further analysis shows that mixed resolution training is beneficial to model performance, that the time-varying token drop ratio allowed by the model can improve results when the correct schedule is used, and that the proposed factorized positional embeddings can offer very good generalization to aspect ratios and resolutions unseen during training. Finally, additional experiments show promising behavior with respect to calibration, fairness, object detection, and video classification.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Overall, this is a very comprehensive paper. The number of experiments and the aspects of the proposed models performance evaluated is impressive. The authors touch on a large number of relevant properties of the model, including calibration stability, positional embedding ablations, and out of distribution performance. Additionally, results are quite positive, showing promising results on image classification, object detection, video classification, and training efficiency. Finally, the appendix is thorough, and gives enough details to accurately reproduce all experiments from what I can see.

Weaknesses:
According to appendix section B.1, classification experiments seem to be ""compute-matched."" Are there any experiments analyzing the asymptotic performance between ViT and NaViT, or can the authors discuss this? The experiments in the paper seem inconclusive on this point. 

The question of compute matching also applies to downstream experiments. For example, in the fairness analysis, are compute matched ViT and NaViT being compared? If so then the conclusion from Appendix H that ""native image resolution improves the performance of fairness signal annotation"" may not hold.

NaViT seems to benefit from training at the original aspect ratio, and at variable resolutions. However, could either of these benefits be achieved through scale and crop data augmentation? What data augmentation is used for ViT? Additionally, I was under the impression that  image stretching or shearing could be useful as a data augmentation. Could the authors please discuss this?

Where are the contrastive results? I might have overlooked something, but I can't find zero-shot imagenet or COCO image-text retrieval results as discussed on line 154.

Minor typos:
- Figure 2 typo: ""image 2"" is repeated twice in the ""data processing"" part
- Figure 9 is before Figure 7 and 8
- It seems that a lot of (if not all) appendix section references are incorrect. Examples of these typos occur in line 152, line 158, line 339, and line 120.

Limitations:
I think the ""limitations"" section from the appendix could be more forthcoming and candid. It mostly just says ""NaViT"" is a great idea, but we didn't get to apply it to all the tasks we wanted to. It could benefit from a more honest discussion of the method's limitations, such as compute overhead from packing or additional architectural changes needed to support example packing.

Rating:
6

Confidence:
3

";1
NpyZkaEEun;"REVIEW 
Summary:
The authors give novel algorithms to learn the structure of a Bayes net from noisy data.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality:
Work seems original.

Quality:
Quality is high, writing-wise and content-wise; see questions.

Clarity:
Writing is clear; see questions.

Significance:
Topic is significant.

Weaknesses:
No significant issues detected; see questions.

Limitations:
Yes.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The authors propose methods for learning Bayesian network skeletons based on discrete data when the data is corrupted by arbitrary noise.

The authors show guarantees on discovering the correct skeleton in terms of the required sample size. Some empirical experiments are also presented.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ rigorous mathematical derivations

Weaknesses:
- very dense mathematical derivations
- limited and inconclusive empirical results

Limitations:
ok

Rating:
5

Confidence:
1

REVIEW 
Summary:
In this paper, the authors revisit the problem of structure learning of Bayesian networks. Here, we are interested to learn the underlying DAG structure that encodes the conditional independence properties of a Bayes net from random samples. A number of algorithms have been proposed to solve this problem over the years which can be broadly classified into two groups: score-based and constraint-based. Despite a lot of research activity over the last several decades the general problem is not yet known to be solvable in polynomial time and several approaches are shown to be NP-hard. 

The authors of this paper start with an approach first proposed by Bank and Honorio in 2020: to reduce this discrete structure learning problem to a continuous regression problem based on encoding schemes and surrogate parameters. The idea is that once cast as a regression problem, techniques from structure learning in continuous models such as regression can be employed. The main contribution of this paper is a distributionally robust optimization (DRO) method that builds upon the aforesaid framework to handle noisy data. In this proposed method, the best structure is searched among a ball of certain radius around the empirical distribution that minimizes the regression cost. Specifically, balls according to the Wasserstein distance and KL divergence are considered. The authors give two theorems for each of the above two distance functions under certain assumptions. They further verify the effectiveness of their methods by experimenting with benchmark real and synthetic datasets. 

 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The problem considered in extremely important from a theoretical and practical standpoint. The approach followed by the authors to reduce it to a regression problem as in the previous work could be promising for certain datasets.

Weaknesses:
A major weakness of the paper is that a number of assumptions are made in order to derive the results, which looks ad-hoc and too restrictive in my opinion. See page 4 of the paper for these assumptions. See also line number 271. 

I could not help but notice several vague statements throughout the paper, a few of which are listed below. 

Line number 224-233 the NP-hardness argument is very vague. Looks like you are using a brute-force approach, if so, the algorithms wont scale beyond 15 nodes. 

Line number 240-241: the citation of Chen and Paschalidis [2018] here is for the first time and sudden. What question are they solving? How is it related to your problem? 

Unfortunately, I am not convinced from the experimental results that the proposed algorithms are overall performing better than the existing methods. Many of the entries in table 1 show that other methods are performing superior to the proposed algorithms. 

Finally, I found the paper very hard to follow, maybe it reads more complicated than it actually is. The author may try to see whether writing it in a simpler manner with small examples makes the paper more readable or not. 


Limitations:
None

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper presents a method for learning the structure of discrete Bayesian networks from potentially corrupted data. It utilizes distributionally robust optimization and regression, optimizing the worst-case risk over a set of distributions. The approach works with general categorical variables without assuming specific conditions and provides efficient algorithms. Non-asymptotic guarantees for successful structure learning are derived, and numerical experiments validate its effectiveness.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Originality:
The paper addresses the challenge of missing data in distributionally robust skeleton learning. This aspect adds a unique and valuable contribution to the field.

Quality:
The quality of writing in the paper is commendable. The ideas and concepts are presented effectively, and the paper demonstrates a strong grasp of the subject matter.

Clarity:
The paper's clarity is notable, as it is organized in a logical manner where each section builds upon the previous ones. The flow of information is well-structured, making it easy for readers to follow the paper's content.

Significance:
The task of building discrete Bayesian networks from potentially corrupted data holds significant importance, as it has wide-ranging applications in various fields. By addressing the challenge of distributionally robust skeleton learning, the paper contributes to the advancement of techniques that can handle data uncertainties and enable more accurate modeling and inference in real-world scenarios. The significance of this research lies in its potential to enhance decision-making processes, predictive modeling, and knowledge discovery across diverse domains.

Weaknesses:
- The author's assertion that they had limited space to relate their work to the existing literature is understandable. However, it is crucial to provide a clear positioning of their own work within the broader research landscape. Emphasizing the novelty and distinguishing features of their approach will strengthen the paper's contribution and demonstrate its unique value.

- The paper lacks clarity on whether cross-validation or hyperparameter tuning was performed during the experiments. It would be beneficial to address this issue explicitly to ensure the robustness of the results. Additionally, to further improve the reliability of the findings, it is recommended to conduct a rigorous statistical test, such as the corrected Student's t-test proposed by Nadeau and Bengio [1], using 15 hold-out folds. Incorporating such statistical analyses will enhance the credibility and validity of the experimental results.

[1] https://link.springer.com/article/10.1023/A:1024068626366

Limitations:
- The proposed method accounts for the effect of outliers by optimizing the worst-case risk over a family of distributions within bounded Wasserstein distance or KL divergence. However, the extent to which the method can effectively handle outliers and their impact on the learned network structure should be further explored and evaluated.
- The proposed approach is designed for general categorical random variables without assuming faithfulness, an ordinal relationship, or a specific form of conditional distribution. While this makes the approach flexible, it may also limit its applicability to certain domains or types of data. Investigating the generalizability of the approach to a broader range of variable types or distributions could be a potential limitation.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper considers the problem of learning the exact skeleton of general discrete Bayesian networks from potentially corrupted data. The estimator optimizes the worst case risk over a family of distributions within bounded Wasserstein distance and KL divergence to the empirical distribution. Under mild conditions, the paper presents non-asymptotic guarantees for successful structure learning with logarithmic sample complexities for bounded degree graphs. Numerical studies on synthetic and real datasets validates the performance of the estimator.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
 

Weaknesses:
 

Limitations:
 

Rating:
8

Confidence:
4

";1
bXvmnpCMmq;"REVIEW 
Summary:
The authors propose a provably exact and an approximative method for calculating permutation matrices requiring much lower memory than previous approaches. The algorithms are based on Kissing numbers and describe row relationships as their cosine value. This allows for representing the problem with significantly fewer values than the baseline n*n permutation matrix representation. Additionally, the authors propose a relaxation of this exact algorithm, utilizing the SoftMax operator, that alleviates the issues with the optimization for large problems. The authors also show a number of applications where their method is applicable. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I believe this is a pretty strong work with a very general and useful contribution. Strengths in more details:
- A provably exact algorithm that significantly reduces the memory consumption of general permutation matrices.
- Additionally, a relaxation is proposed that has better convergence properties in optimization.
- The authors demonstrate on a number of applications that their method is applicable and useful. 

Weaknesses:
First, I wanted to give a strong accept, but the experiments left me slightly in doubt. 
My main problem is the following: The experiments focus a little bit too much on the application side (which is amazing), but miss to do a general comparison to baselines and the proposed methods. I would have liked to see comparisons to SOTA algorithms (related works mention many) on finding permutation matrices, showing run-time, memory, and errors. While the exact algorithm will probably have 0 error, some other methods might have a little bit bigger while being orders of magnitude faster. Also, to my understanding, the exact algorithm breaks down with the problem size, when the SoftMax version is used, which is not exact anymore. Also, it would be good to see the breaking point of the exact algorithm where it does not converge anymore. In brief, a thorough comparison to the SOTA is missing.  

I still like the paper, but without such comparison (considering the theoretical value), I do not give a strong accept. If the authors can provide such an analysis to understand the trade-offs in their rebuttal, I consider improving the rating.


Limitations:
Discussed.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposes a formulation for representing high-dimensional permutation matrices. The basic idea is to express the n x n permutation matrix as an elementwise non-linear function of a product of low-rank matrices V,W (n x m). The authors introduce the kissing number theory which gives the minimum number 'm' for which such a decomposition is provably possible and then show intuitive proofs that in scenarios when the decomposition is valid - their construction is appropriate. In addition, they also promote two well-known non-linearities (ReLU and SoftMax) which can be used in the optimization process for the representation.

Results are demonstrated on two synthetic and one recent 3D shape-matching scenario. Broadly the experiments make an impression of a significantly reduced memory consumption and the ability to recover permutations. In the shape-matching example, the proposed formulation also slightly improves on the previous baseline. 

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
- The core idea of this paper (low-rank representation of permutations with nonlinearities) is *very* interesting and potentially has a very wide impact in scenarios where matching between pointsets is a crucial problem (linear assignment, quadratic assignment etc)
- Overall the writing of this paper is very good, and the background and related material on the kissing number theory has been introduced and explained in a way interesting way.  
- The benefits of the proposed construction are (1.) strong memory reduction in representing permutation matrices (2.) An accompanying optimization scheme that allows for recovering permutations

Weaknesses:
- Broadly, I felt the experimental section is very rudimentary. There are very few comparisons to conceptual baselines: namely using stochastic matrices, optimal transport, Hungarian algorithm and nearest neighbor-like methods. It is not clear from the experiments whether the obtained solutions are: not just some permutations, but the *correct* permutations. 

- More specifically, for the point cloud example in section 4.2, what is the accuracy of the recovered transformation $\Theta$ as a function of n? How do other conceptual baselines compare in this example both in terms of accuracy and memory complexity? 

- Despite the experiments on Marin et.al, perhaps a simpler and more convincing demonstration would be to use the proposed permutation representation in either or all of [43], [30], etc. where a linear/quadratic assignment is solved and then compare with the original methods (perhaps yielding an improvement in memory with a comparable or better accuracy)  


Limitations:
I do not think this paper has any direct negative societal impact. Please see the weaknesses for technical limitations. 

Overall, this paper has a very interesting idea and a clever conceptual message on representing permutations. My biggest concern is whether the proposed construction is impactful in terms of ease of optimization and acceptable accuracies for the multitude of shape-matching problems that this can be applied to. Given the lack of comparisons to conceptual baselines (i.e. not in terms of a state-of-the art shape matching paper, but even simply comparing with previous permutation representations like stochastic matrices, or spectral decompositions - in any framework), I am inclined towards a weak reject at this point. I can be convinced of the gains in memory complexity of the proposed representation but am not yet sold on its applicability and accuracy. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel approach for representing permutation matrices with low-rank matrix factorisation. The method employs Kissing number theory to find the minimum rank necessary to represent the target matrix. This is often quite a bit lower than the rank of the original matrix, so allows for a more efficient memory representation. The paper also shows how the approach can be used in practice in various relevant problems, such as point cloud alignment or shape matching.

Edit: I have read the rebuttal, and given the scores from other reviewers and I would like to keep my score.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
* The work deals with a meaningful problem, and produces an elegant solution with wide ranging applicability.
* The authors demonstrate the performance of their approach in various problems, demonstrating similar / better accuracy with a significant improvement in memory requirement.
* The paper is very well written and easy to read.
* The experiments are relevant.
* The formulation is principled and includes several relevant proofs.

Overall I believe this is a perfect NeurIPS paper. While I have little knowledge of Kissing number theory, the paper solves a very relevant problem in an elegant way, and demonstrates impressive and wide-ranging practical applicability in multiple domains. I therefore strongly believe the work should be accepted.

Weaknesses:
I have very little negative notes about the work, though, I could say that the ablation section could be expanded, e.g. by seeing how increasing the stochastic training k affects speed and accuracy.

Limitations:
These are addressed at the end of the paper.

Rating:
9

Confidence:
4

REVIEW 
Summary:
This work addresses the problem of estimating large permutation matrices by approximating them with a low-rank factorization follow by a nonlinear mapping, thereby reducing the storage complexity from O(n^2) to O(n).

The main contribution of the paper is i) a theoretical derivation and proof of the minimal required rank of the factorization matrices, and ii) the use of a nonlinearity, such that the full rank permutation matrix can be restored *exactly*. i) is based on the Kissing number (or bounds on it), while the nonlinearity in ii) can be a ReLu. Importantly, the possibility to exactly represent any permutation matrix is in strong contrast to direct low-rank factorizations which can only approximate the permutation matrix (since unable to recover the full rank).

The authors propose practical solutions for the optimization: A smooth version of the nonlinearity (softmax), and an optimization scheme (inspired by stochastic optimization; gradients are considering a row of the factorization matrices at a time) that approximates the full objective, but never requires to build the full permutation matrix, thus fully leveraging the compact representation and resulting storage savings.

Experimental results are extensive and demonstrate the applicability of the method on point cloud alignment, linear and quadratic assignment problems, and shape matching.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
**S1** Proposition 1 and Eq. (7) represent a novel contribution. The insight that a non-linear mapping of a matrix factorization is able to recover the full rank permutation matrix is a strong contribution and is of interest for the wider community.

**S2** The proof of the minimal require factorization matrix rank via the Kissing number is an interesting theoretical contribution. Also practically it provides a clear guidance on the required size of factorization matrices such that the exact permutation matrix can be recovered.

**S3** The proposed stochastic optimization in Sec. 4.1 provides a practical algorithm for optimizing the permutation matrix, while leveraging the compact representation (the full permutation matrix is never built, but only computed element-wise at a time). The non-smoothness of the ReLU is addressed by soft-max function with a controllable temperature parameter that allows to approximate the exact solution with desired accuracy over the course of the optimization procedure.

**S4** Experimental results are performed across different application domains and demonstrate the usefulness of the proposed approach.

Weaknesses:
None, but I'm also not an expert in the area. I'm especially not knowledgeable about related work and thus can not judge the novelty of the proposed approach.

Limitations:
-

Rating:
8

Confidence:
2

REVIEW 
Summary:
This paper proposes a novel way to decompose a permutation matrix into two low-rank matrices so that a significant amount of  space can be saved to store a permutation. Then the authors further implement such decomposition to solve practical tasks of point cloud alignment, assignment problem and shape matching. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Overall, the paper is well-written and clearly explains the heuristics behind the method. The method of using Kissing numbers and non-linearity to perform low-rank decomposition of a permutation matrix is very interesting and can possibly inspire future research. Based on Fig 5, the method indeed provides memory saving for supervised learning. 

Weaknesses:
The most troubling weakness of this method lies in its value of application. Specifically, I have the following concerns:

1. The method introduces two complex problems to optimization: bi-variable structure and non-linearity. As mentioned by the authors, this requires devising a non-trivial, problem-specific adaptation for each problem. 

2. To avoid forming an $n\times n$ matrix, the authors propose to only optimize over a handful of entries, which basically requires knowing the ground truth permutation (or assuming some sparse structures as in LAP). This leads to a very limited set of application scenarios.


Limitations:
The limitations of the work are mostly mentioned in the weakness section. The authors indeed addressed them but it is still quite confusing to read for the first time.

Rating:
5

Confidence:
3

";1
fifeeUmV4Z;"REVIEW 
Summary:
This paper introduces a novel approach to tackle the challenge of noisy label learning through a generative framework. Firstly, it presents a new model optimization technique that establishes a direct association between the data and clean labels. Secondly, the generative model is implicitly estimated by leveraging a discriminative model, thereby eliminating the need for training a separate generative model and enhancing efficiency. Thirdly, the paper proposes an informative label prior inspired by partial label learning, which serves as a supervision signal for noisy label learning. Extensive experiments conducted on various noisy-label benchmarks demonstrate that the proposed generative model achieves state-of-the-art results. Remarkably, it achieves these results while maintaining a comparable computational complexity to discriminative models.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- This paper introduces an informative prior for the latent clean label in noisy label learning, which is interesting.
- Experimental results show the effectiveness of the proposed method.


Weaknesses:
1. My concern is whether it is reasonable to build a generative noisy-label learning model, which only assumes that Y causes X. The previous work, where the latent feature Z and Y cause X, seems more reasonable.
2. From the perspective of the algorithm, it seems unnecessary to limit the algorithm to image data, while the notation at Line 138, Page 4 and the experiments focus on the image data, which is required to be explained or conduct more experiments on other kinds of datasets.
3. Some symbols need to be explained. For example, at Eq.(9), Page 5, $y_i(j)$ and $p_i(j)$ is very confusing. So is $|\mathcal{Y}|$ at Eq.(12), Page 5.
4. Some paragraphs, especially for the approach, need to be polished up to better explain how to address the proposed three issues in the abstract. 


Limitations:
The authors analyze the limitations in the conclusion of the paper. It is suggested that more explanations should be made on the reasonability of their generative model. Besides, some paragraphs need to be polished up.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper discusses the solution of learning with noisy labels by directly optimizing P(X|Y) relying on associating the data with clean labels directly. A informative label prior is derived with experimental results on several benchmark datasets.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The author derive a solution for generative noisy label learning under some very strong and unrealistic assumptions.

Weaknesses:
(1)	The paper is very difficult to understand partially because the method is not well motivated. For example, there are many generative model based noisy label learning methods such as
(i)	Label-Noise Robust Generative Adversarial Networks, CVPR 2019
(ii)	From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model, ICML 2022.
Moreover, the approaches that donot directly optimize the P(X|Y) usually because it is not tractable under general conditions. This paper is adding many strong but unrealistic assumptions to directly optimize P(X|Y) may not sound like a reasonable approach. Namely the contribution point 1 is not clear.
(2)	I have strong doubt about the theoretical soundness of the paper. For instance, in the equation (10), why the clean label prior can be defined as the linear combination of the three terms \tilde{y}_i, c(i) and u_{i}(j), as these three terms may not be mutually exclusive and they may overlap with each other. 

(3)	Another issue is in the equation (12), the authors said “ the label ui is obtained by sampling from a uniform distribution of all possible labels proportionally to its probability of representing a noisy-label sample”, this assumption is way too strong and cannot be true. As a matter of fact, there are very few cases that the label distributions are uniform. Frequently, the noisy label distributions are highly imbalanced. Please take a look at the reference [ii] in ICML 2022, the usual way to assume noisy label distribution is multinomial instead of uniform distribution.

(4)	The contribution is not clear. Besides the contribution 1 is not really a contribution, in the second point of their claimed contribution, they said “Our generative model is implicitly estimated with a discriminative model, making it computationally more efficient than previous generative approaches.”
This is also not valid as there are many generative adversarial network based noisy labeling which applied both generative model and discriminative model and let them collaborate with each other (for instance, the reference [i]) Thus, the second point is also very ordinary and not novel as well.



Limitations:
As mentioned before, as the contribution of this paper is not clear. Both of the points (1) and (2) are actually done by previous work with a wider scope and less strict assumption.

In addition to that, assuming the noisy label distribution to be uniform is too restrictive, making the solution has little usage.

Last but not least, the solution is derived based on very strong and unrealistic assumption such as equations (10) and (12), making the experimental results not convincing.

In summary, the paper is too far away from the level of a Neurips paper.

Rating:
1

Confidence:
4

REVIEW 
Summary:
This paper focuses on improving the efficiency of the generative model in the context of learning noisy labels. To achieve this, the authors first introduce a generative framework whose loglikelihood given a variational posterior can be extended into a label transition term and two KL-divergence terms. Then, the authors demonstrate that the optima of one of the KL-divergence terms could be used to transform a discriminative model into an implicit generative model without extra computation cost. 

To further improve the performance of the proposed framework, the authors also present an informative label prior which combines benefits from both the high coverage (sample from Categorical distribution) and the low uncertainty (sample from Uniform distribution) as well as the information contains in the noisy labels.

The proposed work has two main contributions: 1) derive a KL-divergence term from the generative model that allows a discriminative model to be transformed into an implicit generative model, which guarantees the performance of a generative model and the efficiency of a discriminative model at the same time for learning noisy labels; 2) propose a novel clean label prior which allows a tradeoff between the label coverage and uncertainty. Although the second contribution is not as novel in terms of its simplicity, the visualization of its coverage and uncertainty enhances its impact.

The results of the wide range of experiments on benchmark datasets for noisy labels demonstrate the effectiveness of the proposed framework. The ablation studies are also carefully designed and conducted to validate the contribution of each component of the framework. 

###########################################################################

######################### Post Rebuttal ######################################

###########################################################################

The authors have addressed all of my concerns. I am happy to raise my score from 6 to 7.



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-presented and easy to follow, with detailed descriptions of each component. The ablation of the framework and the analysis for the proposed clean prior are well performed. The performance gains on CIFAR benchmarks are significant, especially when the instance-dependent noise ratio is high. This indicates that the proposed model could indeed improve the model's robustness against noisy labels.

Minimizing the KL-divergence term KLD( q(y|x) || p(x|y)p(y) ) to estimate the generative model parameters using discriminative model parameters is novel. In addition, the contribution regarding the clean label prior makes this paper a valuable addition to the community.


Weaknesses:
Minor:

1. The authors should elaborate more on how the modeling P(X|Y) contributes to the informativeness of noisy labels in the introduction part.

2. Please check the references. Some of them are incomplete (e.g., no journal or conference name for [37]).

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
Most previous works address learning with noisy labels with discriminative models while this paper takes the generative approach which maximizes directly on associated data and clean labels. This generative model is implicitly estimated with a discriminative model, making it computationally more efficient. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper successfully addresses several issues that might exist for approaches rely on generative models e.g. challenging to train and tend to use uninformative clean label priors.

- The experiments are extensive with results on many datasets with both realistic and synthetic label noise. All results show the effectiveness of the proposed method.

Weaknesses:
- The method is not well motivated. Even though most previous methods often adopt discriminative models and generative models are less discussed, it is still very hard for the reviewer to understand why generative is better than discriminative models for noisy label problems. 

- The author argues that the small loss hypothesis offers little guarantee of successfully selecting clean-label samples, however, this hypothesis is very related to the early-learning phenomenon in which the clean labels tend to fit earlier in the training than the noisy labels. This paper still uses this early learning to estimate the clean label prior. 

- Figure 2 is difficult to read and it does not make understanding the method any easier, the presentation of the paper should be significantly improved. 

Limitations:
The authors adequately addressed the limitations.

Rating:
5

Confidence:
5

";0
8hKCNVqrlf;"REVIEW 
Summary:
This paper considered the problem of computing the projection robust Wasserstein distance between two discrete probability measures. By formulating this problem as an optimization problem over the product space of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints, the authors proposed the so-called Riemannian exponential augmented Lagrangian method (REALM) to solve them. Further, the convergence of REALM was given. For solving the subproblems in REALM, the authors designed the so-called inexact Riemannian Barzilai-Borwein method with Sinkhorn iteration (iRBBS), where stepsizes are adaptively chosen. As the authors claimed, the complexity of iRBBS to attain an $\epsilon$-stationary point of the original PRW distance problem matches the best known iteration complexity result.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Clearly, this paper generalized some results of the references [26,32]. To the reviewer's best understanding, the core idea is to find feasible points that satisfy the first-order necessary conditions of problem (6) or (11). To solve a three-block optimization problem, the paper transformed it as alternatively minimizing the Stiefel manifold variable $U$ and the Euclidean variables $\alpha$ and $\beta$, where the later can be solved by the well-established inexact gradient methods. Overall, this paper is well-written and mathematically solid.

Weaknesses:
(1) Some mathematical details/arguments are missing (please point out if they are added in the supplementary material). For examples, Line 115: why the minimizer $(x^*,y^*)$ of the problem (9) must satisfy the relationship $y^*=\eta\log(\Vert\zeta_\eta(x^*,11^T)\Vert_1)$? Line 124, how to calculate the gradient of $\mathcal{L}_{\eta_k}(x,\pi^k)$?

(2) It seems that the update of $\theta_{t}$ in Algorithm 2 is missing because $\theta_{t+1}$ appears in the inexactness criterion (19b). Please correct me if not.

(3) The paper may require some ablation studies in the numerical experiments. The authors claimed REALM always outperforms the Riemannian exponential penalty approach since it could avoid too small penalty parameters in many cases (Lines 70-71). What's the range for ``too small"" penalty parameters? And which cases?

Limitations:
Yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper reformulates the projection robust Wasserstein distance as an optimization problem over the product of the Stiefel manifolds and a subset of a Euclidean space. A Riemannian exponential augmented Lagrangian method (REALM) is proposed to solve this problem. The proposed method is empirically more stable than the existing Riemannian exponential penalty-based approach. A Riemannian BB method with Sinkhorn iteration (iRBBS) is used for the subproblem. The iteration complexity of iRBBS is given. Numerically, the proposed method outperforms the existing methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) A method (REALM) is proposed and its global convergence is given.
(2) An inexact Riemannian BB method with Sinkhorn is developed and its iteration complexity is derived. Such a result is interesting by itself. 
(3) From the numerical comparison, the proposed method is most efficient.


Weaknesses:
The existing methods have iteration complexities for the corresponding algorithms. This paper only gives the iteration complexity for the subproblem.


Limitations:
The authors pointed out an important limitation in the theoretical analysis, which is the lack of a lower bound of \eta_k.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors first reformulate the computation of the PRW distance as an optimization problem over the Cartesian product of the Stiefel manifold and the Euclidean space with additional nonlinear inequality constraints. And then they also propose a Riemannian exponential augmented Lagrangian method (REALM) for solving the problem


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.	The authors propose a Riemannian exponential augmented Lagrangian method (REALM) method to efficiently and faithfully compute the PRW distance and establish the global convergence of REALM in the sense that any limit point of the sequence generated by the algorithm is a stationary point of the original problem

2.	To efficiently solve the subproblem in REALM, the authors also propose a novel and practical algorithm, namely, the inexact Riemannian Barzilai-Borwein (BB) method with Sinkhorn iteration (iRBBS).


Weaknesses:
1.	The authors should provide a proof sketch for their main theoretical analysis in this paper.

2.	The authors should add more experimental results to verify their theoretical results and their algorithms.


Limitations:
See Weaknesses

Rating:
6

Confidence:
1

REVIEW 
Summary:
This work proposes a new method, called REALM, to compute the projection robust Wasserstein (PRW) distance. 

The method REALM is an extension of the exponential augmented Lagrangian method to the Riemannian space. 

The convergence of REALM is established. 

To solve a subproblem during REALM, this work proposes an inexact Riemannian Barzilai-Borwein method with Sinkhorn iteration (iRBBS). 

The complexity rate of iRBBS to solve the subproblem, whose solution is an $(\epsilon_1,\epsilon_2)$-stationary point of the PRW problem, matches the existing works in the literature. 

This work claims the robustness in terms of the parameter-tuning in their proposed algorithms. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This work proposes a new method to compute the PRW distance with solid theoretical guarantees. 

Weaknesses:
The complexity rate matches the existing rate. The idea of reformulating the PRW distance computing problem into a Riemannian optimization setting is not new, e.g. Huang et al. 2021. The claimed easier parameter-tuning part seems to need more elaboration, either from the perspective of theory, or from numerical evidences. 

Limitations:
In the Euclidean space, some existing works consider the boundedness of the penalty parameters, e.g., see Echebest et al. 2015. While the additional (Riemannian, inequality) constraints might make situation totally different, I was wondering if those existing literature could shed light on further investigating the parameters settings. 

Rating:
5

Confidence:
1

REVIEW 
Summary:
this paper proposed a Riemannian Exponential Augmented Lagrangian Method for solving the projection robust wasserstein distance problem. the authors claimed two contributions compared with the previous works: 1) the proposed algorithm is much more stable as \eta needs to be small in previous works. 2) a Riemannian Barzilai-Borwein method was proposed to adaptively fine tune the step size. numerical experiments compared with previous works were reported.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. the author proposed to do multiple sinkhorn steps and one riemannian gradient step in each iteration because sinkhorn steps are much more cheaper. this idea makes sense and the author provided convergence guarantee for the proposed algorithm.
2. numerical experiments show the advantages of the proposed algorithm compared with RBCD.

Weaknesses:
1. the main concern is that the author claimed that the proposed algorithm is numerically more stable than RGAS/RBCD because both of them require \eta to be very small. however, I'm not convinced by simply saying ""Based on the knowledge that the exponential ALM is usually more stable than the exponential penalty approach ... "". are there any systematic study to support this claim? the proposed ALM requires the penalty parameter \eta to be exponentially decreasing. such an \eta appears in the denominator of an exponential term (e.g. eq 7). wouldn't this cause the same numerically instability issue? besides, in the proposed algorithm, the author applies the sinkhorn iteration, which will naturally introduces the numerical issues?

2. the writing of this paper needs to be further improved. 

Limitations:
see the weakness.

Rating:
5

Confidence:
4

";1
0x2Ou3xHbH;"REVIEW 
Summary:
This paper considers the problem of online learning in time-varying games under different setups. Specifically, the authors consider the case where all the players apply optimistic gradient descent (OGD) algorithm with a certain choice of learning rate. The main results that in the two-player bilinear game setup, the sum of squared duality gap is bounded by $O(1+V_{\epsilon-NE}+V_A)$, recovering the best-known result in the stationary game setup. This result is based on the bounded second-order path length of the learning dynamic and the important observation that sum of dynamic regret with respect to (approximated) Nash is (almost) non-negative. Next, they extend this result to the strongly convex-concave game with multiple steps and obtain similar duality gap guarantees. The author also consider the potential game and general-sum game setup with guarantees on the duality gap and CEgap bound respectively. Experiments are also done to support their theoretical results. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The problem considered in this paper is important and the authors show that the classic OGD algorithm achieves desirable average duality-gap and other equilibrium-related gap bound with provable guarantees.
- The authors also do experiments on time-varying potential games and time-varying zero-sum games to verify their obtained NEgap bound.

Weaknesses:
The main concern is the novelty of this paper. Specifically, this paper shares similarity to the results in [60] in many perspectives, although the authors explain in many places in the paper that how their results are different from [60].
- One of the main lemma Lemma A.1 is the same as the DRVU property shown in [60].
- Property A.3 is very similar to Eq.(32) shown in [60].
- Example shown in Proposition 3.2 is almost the same as the example shown in Appendix C, case 2 in [60].
- From the technical perspective, I feel like the analysis is very similar to the one shown in [60]. While [60] does not consider the approximated NE path-length, it is not hard to extend their analysis to the approximated NE path-length by replacing P_T with \epsilon-NE path-length +T\epsilon. Also, the boundedness of the second-order dynamic is also shown in Lemma 16/18 in [60].
- In addition, as mentioned by the authors, there are parameter-tuning issues in achieving better individual regret guarantees in bilinear and  strongly convex-concave games, which is also handled by the meta-base structure proposed in [60].

The authors also derive results for general-sum games and potential games. Given Property 3.8 that the sum of regret with respect to CE is positive, it seems that the average CEgap bound is also not very hard to obtain given the bounded second-order dynamic of OGD.

Limitations:
See Weakness section.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this work the authors consider no-regret learning in multiagent games where the underlying game varies across different rounds. They study several classes of games and various learning algorithms that the agents can use. Naturally, the results they obtained are parametrized by variation measures of the underlying game that the agents participate in. To be more precise:
* For time varying zero-sum games, they focus on the setting where both of the agents are using optimistic gradient descent (OGD),  which is a variant of gradient descent that puts a bias on more recent rounds of the game. Interestingly, they show that almost all iterates of OGD are approximate Nash equilibria provided that some variation measures related to changes in the set of approximate equilibria of the games and the underlying payoff matrices are $o(T)$.
* Then, they consider sequences of games where the games in each round are strongly convex-concave. For this class of games, they are able to show a similar result as above, but under weaker variation conditions for the underlying sequence of games.
* Finally, they consider time-varying general-sum games. Naturally, since Nash equilibria are not tractable in this setting, they consider convergence to correlated equilibria. They prove similar results as above, but now the variation measure they use is related to the set of correlated equilibria of the game.
Their results have implications to other settings as well such as meta-learning and dynamic regret guarantees in static games.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The paper studies a very natural problem and provides strong results under various settings of interest, which also have implications in other settings, as I mentioned in the summary.
* For the most part, the paper is easy to follow and the authors have done a good job placing their work in the literature. 
* The authors are not trying to oversell the proof technique, which is heavily inspired by prior works, but uses some natural and clever modifications. For example, instead of letting the variation measure to depend on variations of the set of exact Nash equilibria which would make the problem very difficult to handle since this set is very sensitive to any changes of the payoff matrices, the authors consider variations of the set of approximate Nash equilibria, which behaves much nicer. Since the results are strong and general, I don't think that the authors should be penalized for the fact that the proof techniques are not very novel.


Weaknesses:
* Some parts of the paper might be a bit hard to follow for non-experts, especially in Section 1.1. For example, the MVI property and the RVU bound were not defined. I think the authors could make the transition to this section a bit smoother, although I understand that the space limitations are making it trickier.
* Even though the variation measures the authors use are intuitive and it makes sense that the regret should scale with these quantities, there are no lower bounds to show the extent to which these results are optimal.


Some minor comments:
* In Proposition 3.10, it might be useful to state which dynamic benchmark you consider for their dynamic regret bound.
* With this bibliography style it is a bit hard to keep track of the references, although I understand that it saves some valuable space.

Limitations:
n/a

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies learning dynamics in games that change over time. This is a similar setting to [60], but while [60] focuses on regret guarantees, this paper focuses on iterate convergence to NE.

The main result states that for bilinear zero-sum games, running optimistic OGD guarantees that,
$$
\sum_{t=1}^T EqGap_t = O(V_{NE-\epsilon}^T + V_A^T),
$$
where $EqGap_t$ is the NE gap (i.e., the difference between the player utility and best response), $V_{NE-\epsilon}^T$ is the variation of $\epsilon$-approximate NEs of the games $+\epsilon T$ (in fact they allow different $\epsilon$ for each $T$), and $V_A^T$ measures the variation of the game matrices. $V_{NE-\epsilon}^T$ can be much smaller compared to the variation of the exact NEs.

The paper continues by providing variation-dependent bound for strongly convex games. Next, they provide a bound on the sum of NE gaps for general sum-potential games (which depends on some notion of variation of the potential function), as well as bound on the sum of CE gaps in general games. Finally, they present results for *dynamic* regret in static games.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper provides a set of interesting results. These include,
- Various results on the sum of NE gaps. Specifically, I find the notion of $V_{NE-\epsilon}^T$ very elegant, and indeed, it seems like a much more reasonable notion for characterizing the complexity of time-varying-games instance.
- Bounds on the dynamic regret in static games - even though this is a basic question, according to the authors these are the first results that show $\sqrt{T}$ dynamic regret in games (they also show $\log T$ dynamic regret under a stronger feedback model)


Weaknesses:
- The main text lacks proofs/proof sketches. So it is impossible to understand the main ideas and techniques, even at a high level, without diving into the full technical proofs in the appendix (+ it makes it hard to evaluate the technical contribution of the paper).

- In several places, it is quite hard to follow the text. Specifically, section 1.1 is rather technical, given that it is part of the intro. In addition, the section on general games that starts in line 287 was not sufficiently clear to me. For example, the authors mention that *""there exist matrices
$A_1, . . . , A_n$, with each matrix $A_i$ depending solely on the utility of Player i...""* but what are exactly these matrices represent? What does the value of the optimization problem in (3) represents? Why does *""incorporating the 0 vector will be useful""*? and why does there exist $\mu^\star$ that satisfy the condition in line 300?

- Lack of comparison to previous work: what is the result of [30] and how is it compars to your result in ""meta-learning""? How does Corollary 3.4(2) compares to the result in [60] (except for the difference between $V_{NE-\epsilon}^T$ and $V_{NE}^T$)

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper studies optimistic gradient descent for time-varying games.  Authors prove convergence bounds for zero sum games involving the first order variation of approximate nash equilibrium, which can be significantly tigher than variation bounds involving exact nash equilibria and second order bounds in payoff matrices. The paper also includes refined convergence bounds involving second-order variation for strongly-convex-concave games.  The results also have implications for meta-learning, where games are repeated many times.

The authors also extend results to time-varying general-sum multiplayer games with correlated equilibria, extending exisiting meta-learning similarity measures to general sum games and proving new single-player regret bounds. Techniques are applied to static games, improving our understanding of dynamic regret.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Nonstationarity is an important and challenging area of study for learning in games, and is under-explored.

Bounds involving approximate nash equilibrium variation can be significantly tighter than existing bounds involving variation in exact nash equilibria.

The results for general sum games solving two independent open problems.

Ideas provide improved understanding of dynamic regret for static games, including both positive and negative result.

The paper is well written, including a variety of results while still providing technical exposition on insights behind the proofs and contextualizing the result.

Weaknesses:
Observation 3.11 requires two point feedback, so it's less clear this is a significant improvement.

The paper could probably be a bit more self contained.  The paper borrows ideas from [60], like a dynamic RVU bound but it is hard to follow without additional context.

Limitations:
Yes.

Rating:
7

Confidence:
3

";1
uvdJgFFzby;"REVIEW 
Summary:
The paper introduced a novel inference strategy for transformer models that focuses on inference efficiency. Instead of retaining all context tokens throughout the entire inference process, they gradually eliminate tokens as they move deeper into the layers. To determine which tokens to drop, they trained some small linear layers to predict the remaining relevant context. Their experiments revealed that approximately 80% of the tokens could be safely discarded without much adverse impact on downstream task performance or perplexity. As a result, this approach significantly reduces the computational resources required for inference when the context length exceeds 500 tokens.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The paper demonstrates excellent writing with a clear flow of ideas.
- The authors introduce a unique and innovative data structure that enables batch operations involving masked tokens.
- The experimental results indicate that in scenarios where long context (> 500 tokens) is involved in the inference process, the models can safely drop up to 80% of the tokens without any impact on perplexity.

Weaknesses:
- **The choice of downstream tasks raises questions:** The downstream tasks evaluated in Figure 5 primarily involve small context sizes, and Figure 7 indicates that a smaller context leads to reduced throughput compared to the standard dense model. It would be more persuasive to demonstrate that task performance remains intact when longer contexts are required, while simultaneously achieving gains in inference efficiency.
- **Insufficient experiments with stronger base models:** It appears that the proposed method inevitably leads to performance degradation for larger and more capable base models, as evident from both Figure 5 and 7. This is likely because stronger base models are better at utilizing contexts, and additional contextual information enhances performance. To strengthen the paper's argument, the authors should include more results using larger and stronger base models (>1.5B parameters), such as LLaMA, Pythia, or even OPT.
- **Evaluating generation quality:** Though the authors perform evaluations on language modeling with perplexity, it does not necessarily align with generation quality. If would be helpful if the authors could further provide evidence that dropping context tokens does not affect generation quality.

Limitations:
The authors mentioned the limitation of the working being exclusively tested on autoregressive language models, and specifically GPT2 model family. The paper would be stronger if the author can show positive results on stronger base models. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes an efficient and interpretable context-based dynamic pruning method. They use additional query / key layers to generate dynamic attention masks and sparsify self-attention maps. They also introduce a sparse sigmoid and regularization term to control the sparsity. In experiments, they demonstrate the lowest performance degradation compared to previous static attention pruning methods at the same level of sparsity. The results of throughput and speed analysis show that the proposed method can achieve additional inference efficiency with minimal performance loss. They analyze the distribution of remaining contexts with respect to part of speech and the depth of the layer. They also demonstrate that the proposed method dynamically prune attention based on contexts through context switch experiments.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The proposed method is well-motivated and easy to follow.
- The proposed method efficiently increases interpretability and sparsity with minimal performance drop compared to previous research.
- They provide an extensive analysis of the relationship between context length, throughput, and speed in various parameter sizes.
- Based on the proposed pruning method, they devise efficient batched data structures for the optimized computation.

Weaknesses:
- Quantitative comparison of throughput and speed with local/sparse attention would contribute to a comprehensive understanding.
- Further qualitative study of interpretability (Fig. 8) varying the sparsity level (gamma) would provide additional intuitive observations.

Limitations:
As written in the Limitations section, scalability studies on larger language models (>7B) would provide further insights into the dynamic pruning method.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors propose a modified dynamic masking operation to the traditional multi-headed attention in transformers in order to allow models to learn to drop tokens at specific layers during training. In order to facilitate learning, they use a sparse sigmoid that is annealed to interpolate from a traditional sigmoid up to a step function in the infinite limit. They show that this dynamic sparse attention achieves lower perplexity at the same sparseness levels compared to other modified attention mechanisms, as well as better zero-shot accuracy and throughput. The proposed model shows promise for improving the efficiency and overcoming the quadratic time complexity of traditional dense transformers.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
Improving the efficiency of vanilla dense transformers is an important problem, and being able to learn the sparsification operation rather than setting a static prior is an interesting direction. The proposed method is simple and computationally efficient, and the manuscript is well written and clear. The results demonstrate marked improvements over other sparse attention mechanisms at similar sparsity levels as well as increased throughput.

Weaknesses:
The main concerns I have are with how well this adaptive sparsity can be used when training from scratch and on long range dependency benchmarks such as Long Range Arena [1]. Since the experiments are initialized from GPT-2, all dense information is present, the model simply has to learn which information it can safely ignore. However, when training from scratch this optimization problem becomes much more difficult since falling into a local optimum with respect to pruning early on can reduce the model’s effectiveness in the future as the $\alpha$ annealing increases towards a step function. It would be interesting to see results (even on small models) on how much utilizing this mechanism during from scratch training affects perplexity, given the same compute budget. 

[1]  Long Range Arena: A Benchmark for Efficient Transformers. 2020. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
Given the trend in large language models, it is a pretty important problem to search to efficient architectures. In this direction is the line of work to make the attention component efficient by introducing sparsity in the attention block and allowing every token to attend only a subset of the previous tokens.

The paper presents usage of Adaptively Sparse Attention whereby
1) The network learns to drop parts of the context no longer required.
2) The tokens dropped at every layer are independent, and different set of tokens might be chosen to be persisted at every layer.
3) The network is trined using sparse sigmoid functions that introduce sparsity during training itself in contrast to some works that have tried to introduce sparsity only during inference.

Experiments show that this methodology can lead to pretty strong performance (with minimal perplexity loss), even with 80% sparsity.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The work presented in the paper is pretty innovative and impactful in this direction as finding efficient transformer architectures is key to sustain and grow the research and production usage of these large language models.

The model successfully exploits the fact that the attention matrix in these models are pretty sparse and additionally encourages that with sparse sigmoid-like function to mask out some tokens.

Experiments show pretty strong performance even with 80% sparsity in the network.

Weaknesses:
Given that the model is performing pretty well in terms of perplexity even with huge sparsity, it would be interesting to perform an analysis of whether there is a class of NLP tasks the is significantly affected (like knowledge intensive tasks?), or perhaps translation or similar tasks where the structure of the input sentence matters if the model is focusing to forget stopwords very early.


Limitations:
None discussed in paper, and nothing important that I can think of.

Rating:
7

Confidence:
4

";1
RJpAz15D0S;"REVIEW 
Summary:
The authours proposed a metric called `idealized runtime` to evaluate the inference efficiency among different LLMs as if they were run on the same standard hardware/software system. The main contributions can be summarized as following:

* the `idealized runtime` for a specific query with prompt length `p` and output length `o` on a target LLM can be collected by running the LLM on standard hardware/software (e.g., A100 and Megatron containers) **if target LLM's architecture is known/opensourced**
* the `idealized runtime` of transformer models can be modeled as a function linear to `o` and piecewise linear to `p` **if context window size is much smaller than embedding size of the model**. So with a few queries with different `p` and `o`, the coefficients of this function can be extracted by fitting the runtime of these queries to the function.
* with the fitted function, `idealized runtime` of queries can be estimated without running them exhaustively, so that the capacity of model (e.g., accuracy on a dataset/benchmark) can be compared against the pre-calculated/estimated `idealized runtime` of all the queries in the dataset to tradeoff/review scaling effect of LLMs.
* for closed models with api access, the authors proposed an alternative `denoised runtime` to approximate their idealized runtime.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
* The authors pointed out the scaling law of LLMs should not just focus on accuracy vs FLOPs/Model size but accuracy vs inference latency/cost/power as well, which is a valuable point in LLM production.
* The authors proposed a simple way to estimate the idealized runtime of queries on standard hardware/software systems given prompt length and (projected) output length.

Weaknesses:
* The inference latency of a well known (open sourced) LLM architecture can be easily estimated by total ops of the model (e.g., those calculated in line 105/115) and divided by hardware effective (typically around 50% for LLMs on A100, https://arxiv.org/pdf/2104.04473.pdf or BLOOM paper) FLOP/s. The authors only mentioned about the drawback of this proxy briefly in line 300-308, without evidences based on target LLMs. Thus the significance of approximating these latencies via fitting instead of analytical calculation is quesitonable.

* The work makes too many assumptions that devalue its practice,
  1. context length << embedding size. This is true when the common context lenght is only 2K, but new development on LLMs have pushed them to 32K(GPT4)/64K (MPT)/100K(Claude), so this assumption and therefore the linear function approximation is no longer valid. With long context/prompt length, the analytical equation is probably more accurate.
  2. the `idealized runtime` is collected on standard Megatron container assuming model architectures are known. This limits the effectiveness of the work to open sourced models with efficient (distributed) implementations. 
  3. the work assumes the pretrained/finetuned model is the final service/inference model. However in practice, the service model can be quantized/sparsified/distilled, so apple-to-apple inference latency comparision on pretrained models doesn't direclty translate to service model and the gap is typically huge, e.g., as shown by FIG3, the variance in idealized-runtime vs denoised-runtime is significant.

* The presentation of the work is not clear and sometimes repetitive/redudant.
  1. using FIG3 again as example, what's the diff of the two subplots in (a)/(b), it is not clarified and seems just different scales in axis. And as mentioned above, this FIG suggests for a couple models, the denoised-runtime is smaller than idealized-runtime, which is not a good evidence that idealized-runtime is a good approximation of roofline runtime.
  2. A lot of the equations are just rewrite/modifications to a simple linear function, and probably can be omitted or summarized in a table.

* Some of the conclusions are drawn without evidence, e.g., in line 257, the authors claimed ""This suggests that scale alone does not predict model capabilities."" Scaling law should be studied for the same model by varying model/dataset size. Comparing different model families with drastically different in pretraining dataset size/quality can not lead to any conclusion over model capacity over model size or inference latency.

* Overall the work is oversimplified by the assumptions above to a linear regression problem, which can be analytically calculated (and is commonly adopted, e.g., like Palm2 tech report) and becomes trivial to the audience of NeurIPS.

Limitations:
The limitations are mainly the assumptions the authors made, as mentioned in weakness. The authors either didn't discuss them or without referring to SOTA LLM developments.

Rating:
2

Confidence:
4

REVIEW 
Summary:
The authors proposed idealized runtime, a metric for measuring inference efficiency in LLMs, which measures the performance of models as if they were executed on a given hardware and software platform. Idealized runtime efficiently can be extended to estimate the idealized energy and dollar cost. The metric also takes into account the amount of hardware required to perform inference on a given model. Using these metrics, the authors compare ten state-of-the-art LLMs to demonstrate inference efficiency-capability tradeoffs. Overall, the authors aim to provide a more accurate and fair comparison of LLMs across different providers and to shed light on the tradeoffs between inference efficiency and capability.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The paper presentation is clear and the metric proposed is intuitive to understand.

2. The empirical study is thorough and examines the LLM inference efficiency from various perspectives.

3. The empirical findings demonstrate that the proposed metric can largely capture the efficiency-capability tradeoff in LLM inference.

Weaknesses:
1. The models on different tasks and metrics exhibit vastly different Pareto Frontiers (Figure 9). The cause of this variance hasn't been fully explored and explained in the work. Can these datasets/tasks properly evaluate the capabilities of the LLMs or is the variance due to something else?

Limitations:
The authors may further discuss the potential negative societal impact of their work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper aims to better understand the tradeoff between the inference efficiency and capability of LLM. It comes up with a metric called “idealized runtime” to fairly evaluate the inference efficiency of several popular LLMs, which is designed to factor out the irrelevant noises such as hardware, code stack, network latency and so on. With this metric, the authors analyised the efficiency and capability of LLMs and got some inpsiring insights.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
First of all, I found the motivation of this paper very realistic and helpful to the community. We have been seeing lots of efforts made by the industry and academia to reduce the cost of the inference of LLMs, but how to compare those models from different parties fairly is yet to be figured out. This paper would be an inspiring exploration to this end. Besides, the analysis regarding the LLM inference is thorough and clear. 

Weaknesses:
- Some technical details are kind of missing, which I will specify in the Question section.
- The concept of “idealized runtime” could be a bit unpractical because it requires one to implement and deploy the LLM on a very specific hardware and software setting, which may involve quite amount of proper engineering efforts.

Limitations:
The potential social impact is not discussed in the main paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
""This paper introduces a range of metrics designed to compare the tradeoffs between inference efficiency and capability of autoaggressive Language Model (LLM) based on Transformers. Alongside the raw run-time metric, which can be influenced by infrastructure optimization and performance variance, the paper proposes additional metrics, namely idealized runtime, denoised runtime, idealized dollar cost, and idealized energy cost. These metrics enable a comprehensive comparison of various LLMs, even when they are served through different black-box APIs and hardware/software implementations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Originality: The paper introduces a novel analytical approach to estimate the inference efficiency of autoregressive Transformer models, overcoming challenges posed by models accessible only through black-box APIs and running on diverse hardware and software. This distinct proposal stands apart from previous analytical models designed for non LLMs. Moreover, it addresses the limitations associated with other proxies, such as model size and FLOPs.

* Quality and clarity: The proposal is presented with a rigorous analytical description and validated through empirical verification. The paper's structure is well-organized, and the content is conveyed in a clear and concise manner.

* Significance: The metrics, encompassing both the newly proposed and conventional ones, are effectively employed to compare the tradeoff between capability and efficiency. This comparison leads to several noteworthy insights, elaborated upon in Section 5.4.

Weaknesses:
There are a few minor formatting issues that require attention. For instance, in line 153, the notation of $o^2$ is confusing. The ""2"" is intended to refer to the footer, but it appears as if it denotes an exponent of 2.

Limitations:
As mentioned in Section 3.1.1, the analytical model does not account for models with very large context windows (> 10,000); however, it is worth noting that these models are now becoming available.

Rating:
7

Confidence:
3

REVIEW 
Summary:
LLMs are dominating the NLP space at the moment, but efficient inference run-time and cost estimations have not been explicitly defined and tested. Raw run-times are not particularly useful across multiple services providing LLMs, as there is typically high variance overhead via the presence of black box prompt APIs. This paper proposes a few ways of estimating inference costs that attempt to provide an upper bound to the costs incurred by model computation. These metrics are demonstrated to be generally accurate and are cheap enough to be easily implemented, and the authors attempt to use them to generate new insights concerning various LLMs and generally large models across multiple services.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Very relevant, as LLMs are dominating conversations in the NLP space. Paper as a whole does a good job of making a case for the necessity of their work. Methodologies to estimate a separation of overhead costs from core model computation are inherently useful. Most, if not all, assumptions are fairly setup and seem correct.
2. The paper is very well-written. The problem setup is executed well and the initially proposed solution is very easy to follow. Explanations are mostly succinct, and most sections read as cogent. 
3. The results look to be generally correct and explanations related to them are succinct. The results are made more relevant by the scale of the experiments, rendering the paper somewhat unique. The applied denoising factor estimation technique seems to roughly work in providing an upper bound for expected model computation time. 


Weaknesses:
1. Some areas of the paper are a bit overexplained for this venue. Readers and reviewers of NeurIPS should, for example, be very familiar with autoregressive frameworks and how they generally work, or the forward pass design of generative AI in general. 
2. Several early conclusions are mostly obvious. A lot of time is spent on specifying a procedure to estimate the number of floating point operations (and subsequently runtime based on throughput), but that isn't really all that interesting or new. For example, LLMs having roughly linear costs with respect to the size of their input (their quadratic attention costs are dwarfed) is probably well understood by your audience. Empirical proof related to this likely shouldn't be core to the paper.
3. Unclear if particularly new insights were generated from the results. There were largely no clear winners in cost-capability tradeoff analyses, there is no comparison to other evaluation techniques in the core body of the paper, and insights into when models fall or do not fall on the Pareto frontier of a given task do not seem initially useful. 

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a new metric that attempts to more fairly evaluate machine learning model inference runtime performance cost, which the authors refer to as idealized runtime. The authors argue that the black-box nature of many large language models (LLMs) does not necessarily provide accurate estimates of inference cost using raw runtime results alone, due to several external factors (e.g., hardware used, compute resource contention, etc.). Thus, a goal of idealized runtime is to create a fair(er) inference evaluation metric for LLMs.

The authors analyze 10 state-of-the-art LLMs using their idealized runtime metric and discuss the efficiency tradeoffs between them.


Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
- The paper is well-written
- The research topic of creating a somewhat universal metric to measure spatial and temporal overhead of LLMs on inference is relevant and timely
- The authors’ intuitions on focusing on compute operations (e.g., floating point instructions) and performing an inference runtime calculation that is not overtly computationally intense, is a good foundations for such research



Weaknesses:
While I agree with the authors that this topic is certainly worth deeply investigating, I have too many open concerns to recommend it for acceptance (at this time). Below I list two areas that I believe would need to be addressed before can be reasonably considered for publication.

- floating point operation overhead (a critical paper weakness): in Section 3.1.1 the authors discuss some of the concrete details used for their idealized runtime equation. Of note is the focus on deriving expressions “for the number of floating-point operations required for each of the two steps, and then use these to derive expressions for runtime.” While the number of floating point operations required to perform a single forward pass of inference is quite likely to an important datum for estimating inference runtime overhead, it is (in my opinion) just the first of many (perhaps dozens?) of factors that may be required to make an accurate estimation of inference overhead. There are several reasons for this, I list a couple below (the below list is not exhaustive). 

(1) Not all floating point operations are spatially or temporally equivalent (e.g., a floating point ceiling or floor operation is notably less expensive than even a mathematical divide operation due to the execution “trap” that must exist for the divide, but not for the floor/ceil). Moreover, some floating point operations are orders of magnitude different in overheads (e.g., an FP divide is likely 10x-100x faster than a gather-scatter vector FP operation). To attempt to quantify all FP ops in a single computational class is logically unsound.

(2) The spatial and computational complexity of FP operations between different hardware compute classes varies significantly (often times more than 10x-100x). For example, GPUs can handle up to their channel size limit of FP data and continue to emit a fixed computation time for equivalent class FP operations. This is due to GPUs inherent SIMD properties. CPUs, on the other hand, emit approximately linearly increasing FP operation overhead as FP operation count increases. On the other hand, if the GPU channel size is exceeded even by a single datum, the GPU execution time may increase by 2x or more because it will generally require a second issuance of GPU block execution. Yet, CPUs only suffer roughly additive execution overhead when additional datum are added for FP operations. This means there is generally an inversely proportional runtime cost associated with FP operations and data between CPUs and GPUs. The paper seems to only focus on GPUs, which, in commercial practice, tends not to be representative (or at a minimum highly specialized) of which compute is used for inference.

- References: of the 50 or so references included by the authors, it appears that only around 10 of them are from peer-reviewed publications. The other 40 or so are from arxiv and websites. Moreover, the arxiv references are not necessarily recent (some date back over 5 years ago). The lack of peer-reviewed citation raises questions about the validity of substantiation as well as the authors’ understanding of the domain. A core problem with citing non-peer-reviewed published work is the lack of a fixed written artifact. Arxiv papers can have many versions as can website/webpages (which version are the authors referring to and how would a reviewer know this a priori?). I strongly encourage the authors to perform a more rigorous literature review and to substantiate their claims principally through peer-reviewed scientific research. It is a tall order to expect a reviewer to personally review / validate every non-peer-reviewed research paper to help verify its soundness and thus its appropriateness when used to substantiate the authors claims. Yet, not doing so would seem to imply that reviewers would need to implicitly “trust” the quality of such non-peer-reviewed artifacts. This seems antithetical to the evidence-based scientific research that is published at NeurIPS.


Limitations:
The authors seem to only focus on inference runtime overhead associated with GPUs (and a single class of GPU, as a reference point, Nvidia A100) and no other form of hardware compute. However, many other types of hardware are used to perform inference. Some of them are: CPUs, FPGAs, TPUs, CGRAs, etc. Moreover, each of these hardware classes have subclasses within that result in different temporal overhead and power footprint costs. None of these factors seem to be considered in the authors runtime equation.

Rating:
2

Confidence:
4

";1
eJZ5vJEaaa;"REVIEW 
Summary:

This paper focuses on formalizing what class of planning problems can be solved by a relational neural networks. It aims to bridge the gap between the expressivity of relational neural networks and the complexity of planning problems. To do this, a serialized goal regression rule and search algorithm is defined. Three classes of planning problems are defined based on the serialized goal regression search and show these classes can provide network parameters for relational neural network. Experiments with two planning domains show the empirical implications of the results.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:

* The paper provides a novel approach to quantifying the complexity of planning problems
* As far as I am aware, this is the first paper to analyze what kind of planning problems can be solved by neural networks.
* The formal analysis can aid in better evaluations of the neural approaches proposed for planning problems.


Weaknesses:

* Some of the notations are not clear (See questions for details). 


Limitations:
 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper explores the application of relational neural networks, seen as circuits, in representing policies for discrete planning problems. Specifically, it introduces a circuit complexity analysis that categorizes planning problems into three classes based on how circuit width and depth grow, by establishing connections with serialized goal regression search (S-GRS), which is a method for generating plans by working backwards from goals. In essence, the main contribution of the paper is that it combines previously introduced planning problem complexity related quantifications (width, depth) with circuit/policy complexity analysis to analyze the resource/representation requirements of policies. This analysis helps to better understand the capabilities of relational neural networks in solving planning problems, which in turn helps designing them for policy learning. By analyzing goal-conditioned policies for such planning problems, upper bounds on the circuit complexity of such policies are provided as a function of the problem's regression width. The experimental results support the theoretical formulations and demonstrate the practical applicability of the proposed approach for relatively small sized/simpler discrete planning problems. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Originality: The paper proposes a new perspective for circuit complexity analysis for relational neural networks (GNNs, transformers), and thus what they can compute (i.e., their limits). 

- Quality: The proposed method is supported by theoretical analysis and proofs. A small set of experiments were conducted to validate the approach.

- Clarity: The order of presentation is mostly nice, even though the terminology used throughout the paper sometimes makes it hard to follow. 

- Significance: Helpful for constructing relational neural networks for planning problems, and the proposed formulation can also help investigate solutions to planning problems for future works in this area. If the problems analyzed had a broader scope, it would have increased the overall impact of the paper.


Weaknesses:
- Lack of a convincing set of quantitative results: Proposed methods might be tested on different networks and problems (only two different problems and networks are presented). 

- A broader discussion of the applicability of the proposed analysis and formulation on different planning problems, especially to real-world (e.g., robotics) scenarios, would help convince the reader about the impact of the paper. 

- Related work section might be placed earlier (e.g., after introduction) for better placing this work w.r.t. the literature. 

- Supplementary material helps clarify some points, due to page limit it is hard to integrate this information but for clarity there might be more reference to it within the main text.


Limitations:
- Limitations of the work have been barely touched upon on the paper. 

- The discussion on applying this formulation to continuous planning problems, e.g., in robotics, can be extended. 

- In general, an evaluation and discussion on the type of problems that the proposed formulation is not applicable to would be nice.


Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper discusses the expressivity of classical relational architectures and its implication for solving planning problems. In particular, the authors identify characteristics of the environments that make planning hard. They prove a theorem that bounds the size of a network sufficient to solve the problem of planning perfectly, under some assumptions on the underlying domain. Finally, they show experimental results in two simple environments.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is very clear and easy to follow. It's very convenient that you provide intuitive explanations of the derived theorems and explain them in an example environment. The motivation is clearly stated in the introduction. The problem is interesting and deserves a study. The presented results are novel and provide a deeper understanding of the capabilities of widely used architectures.

Weaknesses:
In my opinion, the greatest weakness of this paper is an insufficient discussion of the connection between the formulated theory and practical applications. I understand that the contribution of the work is mostly theoretical. Anyway, it would be nice to show how the presented theory should influence my design choices. For instance, I suggest providing a list of environments with a short explanation of their properties (finite/unbounded depth, serializability, width, theoretical bounds values, implications, etc.), even without experiments, even in the appendix.

Limitations:
The limitations are briefly discussed in the conclusion. Negative societal impact is not an issue with this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents a mapping from tractable but incomplete algorithm classical planning into circuits that can be represented by a class of neural networks called relational neural networks RelNN including transformers architecture and graph neural networks. The paper is divided into two parts.

The first part presents an extension of Chen and Gomez [CG2007] that introduced a notion of width, in turn, related to the more practical notion of width proposed by Lipovetzky and Geffner [LG2012]. The change is about using regression search –not a big change given that as a theoretical algorithm– while keeping the same idea of keeping track of a reduced context.

The second part considers how to map policies based on those algorithms –given a set of regression rules– into a RelNN. The paper discusses the size of the required network in different cases. The best scenario is when the size of the NN doesn't grow linear with the size of the problems. Using the ideas of the first part, the paper shows how can that be expressed as a RelNN. Preliminary empirical results over a set of problems suggest a MLP is learning a policy that matches the ideas of the paper.   

Soundness:
4

Presentation:
2

Contribution:
2

Strengths:
- Innovative combination of tractable planning with the expressivity of some NN. Serialization is a suggestive idea for a tractable fragment for planning.
- The idea of 'regression rule selector' might be hard to use in an effective symbolic algorithm. Framing it as learning is an innovative idea.


Weaknesses:
- The paper offers little insight into how depth and breadth behave in different problems. In particular, it's not clear how the complexity of the rules is related to that. Perhaps a better title for the current manuscript would be ""Relational Neural Network can Solve some Planning Problems"", as the paper does not help to identify what problems can be solved except by trial-and-error. That might be a good contribution, but then the tone of the paper might be different.
- The paper doesn't clarify whether the goal of the approach is to solve a single problem or problems from a domain.
	- For instance, the width of [LG2012] refers to problems, but then they discuss width across instances of a problem.
	- This might be easily fixed in the next discussion.
- Experiments cannot be replicated as we lack details.


Limitations:
This is a very interesting line of work combining results in different areas. The theoretical arguments focus on the existence of a bound, there is little formal study or intuition of how the complexity grows across different problems. For instance, if conditions are free FOL, for a fixed number of variables and objects, then the resulting circuits can have many different sizes.

In general many intuitions are hidden in the proofs ideas and not in the main text. That's a possible reasonable decision, but sometimes the key intuitions are hidden there and not connected with the overall picture.

Lacking intuitions, the empirical study offer some light in this direction, but a reader in NeurIPS might need further details on how this was done.

Finally, I think this would be a better paper with a more clear distinction between problems and domains. Previous work using the notion of width tends to discuss it across a class of problems. In that setting, providing or obtaining a set of rules is more clear that for solving a single problem. The set of rules might be very different for subsets of problems. (For instance, blocks world instances where all the blocks start at the table). That framing would allow to establish connections with work in learning for generalized planning. 

For that reason and others, this one is a very relevant reference:
Learning Sketches for Decomposing Planning Problems into Subproblems of Bounded Width. Dominik Drexler, Jendrik Seipp, Hector Geffner.
ICAPS 2022.
https://arxiv.org/abs/2203.14852 

All in all, I learned a lot from the paper. That's a positive sign.

Minor point(s):
- why can serialization be incomplete? Perhaps explaining this might be useful: https://en.wikipedia.org/wiki/Sussman_anomaly


Rating:
6

Confidence:
3

";1
tJ88RBqupo;"REVIEW 
Summary:
This paper contributes a new model evaluation framework called 3S-Testing, which uses (conditional) deep generative models to create synthetic test sets. To provide uncertainty estimations, 3S uses deep generative ensemble method. It is empirically confirmed that the better performance on small subgroups (with real data), and distributional shifted data. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The motivation is clear by targeting the subdominant class. Using generative models (and generative ensembles) to evaluate model is an encouraging attempt, especially for uncertainty estimation. The experiment section is clearly organized and easily understood.

Weaknesses:
$\bullet$ The major concern is to evaluate a model, more uncertified models are introduced even though they are used for uncertainty estimation. The author(s) gave reasoning from line 155 to 161. The first is reasonable, while the second is not convincing. For example, in figure 2, the samples of green star class are sparse in the test set by nature. Generating samples of green star class becomes (b) is a “bias” introduced by the generative model. 

$\bullet$ 4.2 is hard to read. While the intention was to keep the discussion for shift as general, more and more notations and assumptions for simplifications are continuously added, sometimes without explanation (see below in questions).


Limitations:
Fully discussed with explanation. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose to use synthetic data to evaluate models, especially under distribution shifts or in areas of the input space with low coverage. The authors use CTGAN to empirically validate their idea, and apply it to tabular data.

The paper is a resubmission from ICLR 2023 ( https://openreview.net/forum?id=J7CTp-jNyJ ).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: While the general idea of using synthetic data for evaluation does not appear new, to the best of my knowledge this is the first thorough evaluation of the idea for tabular data. (I went out of my way to find published studies on this, and the only works I found were on relatively low-quality journals, or very different in scope).

Quality: Experiments were done on 6 datasets of relatively small scale. This gives a first idea that the idea might work, but mention two possible extensions under ""Weaknesses"".

Clarity: the paper clarity was fine.

Significance: I think this is an important topic that deserves more study, the results of which will be of interest primarily to practitioners, but might also encourage further research on the topic.

Weaknesses:
* The paper mostly focuses on 6 small datasets. While the approach itself is probably interesting in cases where data is scarce, it would be interesting to see this applied to harder datasets that have more than just a couple of dozen features, or where there are millions of samples involved. I personally remain unconvinced the method scales to larger data, and would appreciate if the authors could report results on larger datasets.

* The authors write ""* ""an end-user only has access to a single draw of Dtest,f . e.g., we might incorrectly overestimate 265 the true performance of minorities. The use of synthetic data solves this."". I'd like to challenge that statement: I think it's very typical in real life (especially given the relatively small data sets that are the focus in the publication) to at least use cross validation to get multiple test sets, potentially even Leave-1-Out CV. This should give a much better estimate of the loss landscape. I'd appreciate if the authors could add such experiments as baselines.

* It would be nice if the authors gave an indication of run times of the various methods they compare too, as this might be useful to practictioners. E.g. They mention in Appendix section C that the GAN was trained with a fairly large hyperparameter search, which I expect is not cheap.

Limitations:
The authors mention limiations in passing, but have not gone out of their way to find out failure modes of their method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes to use synthetic test data to improve the estimation of model performance for tabular datasets when insufficient test data is available. Their approach of generating synthetic test data conditioned on subgroups improves performance estimation for underrepresented subgroups and can accurately estimate the model performance under distributional shift towards the underrepresented subgroups. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well-written, and the details are covered by the Appendix. 

They have done a good job in showcasing the advantage of their synthetic strategy.


Weaknesses:
Their proposed method is limited to the tabular data and data synthesis has been looked at in prior studies, but I think this is a good paper with nice experiments. 

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose the utilization of synthetic data for evaluating models and introduce an automated suite of synthetic data generators called 3S. 3S offers two key advantages: it enables reliable and detailed evaluation, and it measures model sensitivity to distributional shifts. The paper explores different scenarios involving 3S, providing valuable insights into the application of synthetic data for model evaluation.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper exhibits a clear and easily comprehensible writing style. The authors conduct a comprehensive examination of relevant literature, effectively summarizing its advantages.
2. The issue investigated in this paper, namely the utilization of synthetic data for evaluating models, is both intriguing and significant.
3. The authors effectively discuss various use cases of 3S, offering insights into the practical utilization of synthetic data for model evaluation.







Weaknesses:
1. Further discussion on method limitation is needed. 


Limitations:
As mentioned, further discussion on method limitation is needed. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a model evaluation framework, generating the synthetic test to mitigate the challenges of model evaluation with limited real test sets, such as unbalanced subgroups and distribution shifts.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea of using synthetic data to improve the testing and evaluation of machine learning models is impressive. 

This paper analyzes clearly and reasonably the failure of real test data and its corresponding challenge for reliable model evaluation.


Weaknesses:
**1. Counterintuition.** Although the empirical benefits of syntenic data are observed in the experimental parts, the inequation of Line 155 is not evaluated theoretically. More precise statements are needed here. 

**2. Overfitting of deep generative models.** With limited test data in the small subgroup, deep generative models tend to perform overfitting. In this case, the learned manifold could have a huge gap from the real manifold. 

It could be better to show more visualizations in Figure 2. In detail, if we change the test samples in the green subgroup, what will happen when we compare synthetic manifolds and the real manifold?

**3. Failure case.** As a model evaluation framework with synthetic data, which makes a lot of sense in the real world, this paper lacks failure cases to show the limitations of their works. 


Limitations:
Please check the weakness.

Rating:
6

Confidence:
3

";1
CWdxHxVAGG;"REVIEW 
Summary:
The authors consider 3 rejection models in the OOD detection setting and establish theorems about the optimal rules for these models. Due to the commonly used metrics that may partially only focus on either OOD detection or misclassification, they proposed a double-score method to consider both aspects.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The authors extend the rejection in the non-OOD setting to the OOD detection setting.
* The 3 different rejection models share similar optimal strategies, and those strategies outperform other baselines.
* Since previous works only focus on either OOD detection or misclassification, the authors proposed a novel double-score method to consider both aspects. 


Weaknesses:
* Now that we know all the distributions, including OOD and ID, why put effort into eq 3? Can’t we just treat augmented data of OOD + ID as the new collection with $K + 1$ ""inlier"" classes, then follow the regular rejection analysis under the closed-world assumption?

* The optimal rules heavily rely on the clear information (the distribution in the theorem or estimated from the sample) on OOD data. For example, the prior $\pi$ in the bounded precision-recall rejection model, the conditional probability (of OOD) in the bounded TPR-FPR rejection model, or even both in the cost-based model. However, it is challenging to access OOD data and estimate these probabilities in real-world applications within an open-world setting. This raises concerns about the practical guidance provided by these optimal rules.

* I have a reservation about the constraints in equation 8 or the setup of problem 1. In practice, it is more likely the threshold or the selective function is determined by only one constraint (even in your proof of theorem 2, there is nowhere about the constraint $\rho(c)$). For example, once the $\lambda$ is determined by one constraint, it does not necessarily satisfy another one, which is similar to the trade-off between the type I and type II errors or Neyman-Pearson classification. A similar argument applies to Problem 2.

* Optimal rules: the authors claim the optimal rules related to the Bayes classifier (lines 126, 170), but from theorems, it holds under the condition that the given $(h, c)$ is optimal. If we cannot find the optimal $h$, can we still say the Bayes classifier is optimal?


Limitations:
See my concerns and questions above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper provides a unified viewpoint of reject model evaluation metrics under OOD-ness. Despite different performance metrics being used in practice, they can all be factorized into both the misclassification prediction on ID datapoints as well as the discrimination performance between ID and OOD. The experimental section provides some evidence that this proposed two score decomposition can provide competitive OOD detection and input rejection performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- OOD detection and classifying with a reject option are both timely and important issues in trustworthy machine learning. ML algorithms need to be aware of their limitations and signal if they are deployed on previously unseen data.
- The unified framework / viewpoint makes sense and yields an interesting decomposition result showing that the optimal uncertainty score is a function of both misclassification prediction on ID datapoints as well as distinction performance between ID and OOD. Although this appears like an intuitive desiderata for a prediction uncertainty score, explicitly tying it back to the different rejection models seems new.

Weaknesses:
- The paper assumes that the OOD distribution is known. This is a strong assumption and severely limits the applicability of the approach in practical settings where knowing or explicitly estimating this distribution is often prohibitive.
- Even though the paper does provide a nice unification of three performance measures for reject option models in an OOD setup (cost-based rejection, TPR-FPR rejection, precision-recall rejection), these metrics are not new and routinely used in existing applications. To me it seems like this kind of unified viewpoint would be better suited for a survey-style paper.
- Section 3 could have been motivated better. To me, it appears a bit sudden without much justification or transition from the previous section.
- Section 3.1 could have significantly profited from a figure showcasing the 1D Gaussian example for added intuition.
- While the optimal uncertainty score is unified across error models, the experimental section showing the efficacy of the double score method is very lackluster. Details about models, training methods, and hyper-parameters area all missing (and also not documented in the appendix). Results are also based on a single run which makes it impossible to judge the statistical significance of the presented results. Moreover, the considered datasets do not all share the same sample shapes (e.g. MNIST vs CIFAR-10) which makes me wonder how OOD scores were obtained here in the first place. Real life shifts, like from the WILDS dataset collection would have been a better place to assess OOD-ness of samples. I am not convinced by this evaluation.

### Post-rebuttal

I have read the author's rebuttal and found that although many of my concerns were adequately addressed, the experimental documentation issue remained unaddressed. I increased my score only slightly as a result.

Limitations:
Weaknesses of the approach are not adequately discussed in the paper. See weaknesses and questions above.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes three reject option models and introduces double-score OOD methods that consistently outperform state-of-the-art methods. The authors also propose novel evaluation metrics for comprehensive and reliable assessment of OOD methods. The proposed metrics simultaneously evaluate the classification performance on the accepted ID samples and guarantee the performance of the OOD/ID discriminator, either via constraints in TPR-FPR or Precision-Recall pair. The authors argue that setting these extra parameters is better than using the existing metrics that provide incomplete if used separately, or inconsistent, if used in combination, view of the evaluated methods.  Overall, the paper's contributions provide a significant improvement in OOD detection and evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes three reject option models for OOD setups, which extend the standard reject option models. These models define the notion of an optimal OOD selective classifier and establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. This is an original and creative approach to the problem of OOD detection, and the proposed models are well-motivated and clearly explained. The paper introduces double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The paper proposes novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models. These metrics provide a comprehensive and reliable assessment of OOD methods. 

Overall, the paper is well-written and easy to follow, with clear explanations of the proposed models, methods, and evaluation metrics. 

Weaknesses:
One weakness of the paper in the experimental results is that the dataset used in the experiments is relatively small, and the proposed methods do not show significant advantages over the baseline. This raises questions about the generalizability of the proposed methods to larger and more diverse datasets. Additionally, the paper could benefit from a more detailed analysis of the limitations of the proposed methods and how they can be improved in future work.

Limitations:
The authors did not explicitly discuss the limitation of the proposed method.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper addresses out-of-distribution detection (ID/OOD discrimination) and misclassification detection (selection classification). The authors introduce double-score OOD methods that leverage uncertainty scores from OOD detector and misclassification detector. For evaluation metric, this paper proposes to use PR curve.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1) Existing works mainly focus on OOD detection, which this paper simultaneously considers OOD and selection classification. This is more practical and useful for high risk applications.

(2) Theoretical analysis of OOD detection and selective classification is valuable.

Weaknesses:
(1) The analysis of Bayes-optimal OOD selective classifier as well as the Bayes-optimal misclassification selective classifier has also been investigated in [1]. The authors are suggested to demonstrate the difference.

(2) The experiments are insufficient, only evaluating on cifar-10 and mnist. Results on CIFAR-100 is valuable.

(3) What is the advantage of PR curve over AURC (risk-coverage) curve [2] for evaluating rejection model?

[1] Narasimhan, H., Menon, A. K., Jitkrittum, W., & Kumar, S. (2023). Learning to reject meets OOD detection: Are all abstentions created equal?. arXiv preprint arXiv:2301.12386.

[2] Geifman, Y., & El-Yaniv, R. (2017). Selective classification for deep neural networks. Advances in neural information processing systems, 30.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a formal analysis of three distinct models for classifiers with a reject option in the presence of out-of-distribution inputs at test time. All three models, viz. 1) cost-based rejection, 2) bounded TPR-FPR, and 3) bounded precision-recall, share the same form of optimal selective classifier (see Section 2.4). 

This selective classifier consists of the Bayes in-distribution classifier $h_B(x)$ (which is optimal for a given in-distribution), and a selection function score that is a linear combination of the conditional risk $r(x)$ and the likelihood-ratio of OOD to ID $g(x)$. Based on this analysis, the authors point out the limitations of current OOD detection methods which only have a single score, and instead propose double-score OOD detection methods which can focus on both mis-classification detection and ID/OOD separation.

Using a concrete synthetic example, they discuss the limitations of existing metrics such as AUROC and AUPR in evaluating selective classifiers in the OOD setting. They propose a novel metric (one each for the bounded TPR-FPR and bounded precision-recall models) which calculates the selective risk, subject to a given minimum TPR and maximum FPR (or a given minimum recall and minimum precision). The proposed metric is shown to be better at capturing the overall performance of selective classifiers in the OOD setting.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The proposed reject option models and evaluation metrics are developed systematically and connected well with summary sections. Found the paper to be well written. 

2. The analysis builds upon the prior work [6] which deals with reject option models but not in the OOD setting. In this work, they consider rejecting inputs due to both mis-classification and being OOD.

3. Although the proposed work makes a strong assumption of a known OOD distribution, the analysis is useful to draw insights about the need for a double-score OOD detection method, and also to highlight the shortcomings of existing evaluation metrics.

[6] https://www.jmlr.org/papers/volume24/21-0048/21-0048.pdf

Weaknesses:
1. The analysis and proposed new metrics (parts of the new metrics like the FPR and precision) depend upon knowledge of the OOD distribution. This is a strong assumption for practical settings. 

2. For the proposed double-score OODD method, it seems to me that we need access to OOD data in order to set the hyper-parameter $\mu$ in the combined score $s_r(x) + \mu s_g(x)$. The authors should clarify if this is set based on validation data from a different OOD distribution than the test data.

3. Minor: it is a bit tedious to keep track of all the notations needed for the analysis.

**Update after author-reviewer discussions:** \
I have read the authors rebuttal. The paper lacks sufficient discussion on the experiments. Not enough details are provided in the appendix as well. Hence, I decrease my rating to 6.

Limitations:
Some limitations of the proposed work are mentioned in Section 3.5. Another limitation to include is the fact that the proposed analysis and novel metrics depend on the OOD distribution which is usually unknown. One could use a validation set (as in the paper) with a mix of in-distribution and auxiliary OOD data, but the distribution of auxiliary OOD data in the validation and test sets should be different. 

Negative societal impacts has not been discussed, but may not be applicable here.

Rating:
6

Confidence:
3

";0
1cY5WLTN0k;"REVIEW 
Summary:
Designing neural PDE sovler using deep neural networks is a challenging task for which several solutions have been proposed in the literature using for instance networks that encode the initial conditions or physics informed neural networks.

The authors propose to use Monte Carlo methods to train neural PDE solver for the solution of a general convection-diffusion equation.
Using Feynman-Kacformula, the authors derive a loss function that can be used to learn a mapping that can simulate the target fields using the input parameters and the initial condition. 

They propose a theoretical guarantee on the solution provided by the Monte Carlo solver and the paper illustrates the performance of the proposed method with a  one dimensional differential equation and a 2-dimensional Navier-Stokes equation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes a Monte Carlo based PDE solver strained via Monte Carlo approximation which can  handle  coarse time steps better than existing alternatives. 

The proposed method does not require many particles in the numerical experiment and is computationally efficient in the settings explored.

Under some assumptions, the authors propose an upper bound on the error explicit in some hyperparameters of the approach in the case of a  convection diffusion equation.


Weaknesses:
Theorem 1 is obtained under several assumptions. 
These assumptions should be discussed more, are they restrictive or common assumptions in the PDE literature ?

The claim that the approach is efficient even with few samples seems correct in the proposed experiments. However, these experiments are in dimension 1 and 2 and Monte Carlo methods can be cumbersome in high dimensional settings without tunning. 
The authors could detail the explicit advice or theoretical guarantees we have for the error with respect to M.

The authors only explore one discretization scheme (Euler), the scheme used could have an impact on the performance of the method, can this be discussed ?

Limitations:
The authors provide several research perspectives for this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose an unsupervised neural technique for solving PDEs based on the classical correspondence between (parabolic) partial differential equations (PDE) and stochastic differential equations (SDE) as given by the Feynman-Kac formula. Specifically, they propose minimizing the error between the neural approximation's deterministic prediction at timestep t+1 and the expected prediction of the neural approximation over particles that have evolved up to time t stochastically according to the Feynman-Kac SDE representation of the given PDE.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
Incorporating Feynman-Kac into the PINN framework is an interesting idea.

Weaknesses:
# Poor numerics
The experiments do not justify the claims, e.g. that long rollouts are more stable using this method. To prove this, at the very least the authors need to present results where the trajectories actually exhibit turbulence. Then, an ablation with the multi-scale framework is required.

# Poor presentation
The authors cannot expect the reader to be familiar with Feynman-Kac and need to give explanations in plain English of the significance of this result. Then, the equations in the main paper should aim to clarify this further, not give a comprehensive mathematical presentation. For example, the inclusion of the forcing function  distracts from the main result which is using time reversal to obtain an SDE that moves in the right time direction for equations that are most often solved using neural networks, e.g. ones with an initial condition, not a final condition.

Furthermore, only the most experienced reads will walk away from this paper with a clear idea of how to implement the proposed algorithm. The emphasis in the paper should be to give the reader something to implement and try, not a theoretical proof.

Finally, there are a number of tricks that are not included in the initial idea and are not sufficiently explained. For example, the Fourier interpolation. The multi-scale framework makes the model non-parametric which is a significant departure from previous work and from the presentation of this paper as a Monte-Carlo approximation to PDEs.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors present MCNP, a new unsupervised training loss for surrogate simulation networks. This loss is based on the link between stochastic processes and PDEs, sampling one-step Brownian motion to estimate the PDE solution. The learned network takes an initial state and the target simulation time to compute the state at that time in one pass. For longer periods, multiple NNs are trained, one for each sub-interval.

Soundness:
2

Presentation:
4

Contribution:
4

Strengths:
The paper is generally well-written and relatively easy to understand. It includes a good overview over related work.

The paper includes both theoretical and numerical results. The method is derived using the Feynman-Kac formula and the authors show how the errors of PSM and MCM scale when given an incorrect input state, such as predicted by a neural network.

A total of five numerical experiments are performed, covering a large range of simulation configurations. The paper contains an ablation study, giving some insight into the impacts of various parts of the MCNP method.

Weaknesses:
While the experiments are varied in the tested configurations, all but on experiment consider simple diffusion equations, some of which can be solved analytically.

The paper does not show any simulation trajectories and gives no insight into how the various tested methods behave in their experiments. Instead, only the final losses are reported. This makes it hard to determine the cause of improvement from the numerical results. I strongly recommend documenting your observations in the appendix.

The Navier-Stokes experiment seems to be mostly forcing-driven with all initial states ending in a similar configuration. A different forcing, such as Kolmogorov flow, would result in a much wider range of trajectories.

The paper does not give details as to how the numerical simulation (PSM) of the Navier-Stokes experiment was performed. Please describe the simulator in more detail if you implemented it yourself.

The source code is not part of the submission and the authors have not declared their intent to make it public. I strongly recommend doing so, especially if the employed CFD solver was implemented from scratch.

Minor:

* Eq. 3: Please indicate the x-dependence of xi
* Fig 1 caption: The formulas are missing a factor of 1/M
* L184: You mention that cutting the gradients prevents numerical instabilities but ignore the positive effects that gradient backpropagation can have.
* You refer to Eq.13 as a convection-diffusion equation but it does not contain a convection term.
* L199: The notation Δ ₜ u is confusing since Δ is already in use.
* L216: The argument that label noise helps MCNP perform coarser time steps seems unfounded to me. This requires an explanation.
* L237: By lattices, you probably mean frames or time steps?
* L268 and Eq. 17: You can simplify the forcing to a single sine term.
* Figure 2 is never referenced.
* Figure 2 shows the vorticity, correct? Please specify in the caption.

Limitations:
The limitations are adequately discussed.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors propose Monte Carlo Neural PDE Solver (MCNP Solver) which leverages the Feynman-Kac formula to train neural PDE solvers in an unsupervised manner.

I'm willing to revise my score based on the rebuttal from the authors to the questions that I raised below.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* That paper addresses and interesting problem: learning the neural operator in an unsupervised way. 
* The authors propose practical enhancements to their method such as one-step rollout, Fourier Interpolation and the use of a multi-scale framework.

Weaknesses:
* Limited set of experiments, only two cases: 1d diffusion and 2d Navier-Stokes.

Limitations:
* The limitations that I see are encapsulated on the questions that I raised above.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper proposes a new physics informed neural network based solver that utilizes the connection between PDEs and SPDEs. This is achieved through the Feynman-Kac formula and applies to a large class of PDEs. It comes with a bound on the error at each step in the rollout. The results are compared with multiple supervised and unsupervised PDE solvers on a number of 1D and 2D equations.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: To the best of my knowledge, the paper is original in combining the Feynman-Kac-based approach with a neural operator architecture. The connections to existing work that relies on the Feynman-Kac formula as well as other PINN/NO approaches are discussed in detail in the main paper and the appendix.

Quality: The work is thorough in discussing existing literature and presenting the methodology. The theoretical result gives some intuition relating to how the proposed method scales as compared to the classical solver. 

Significance: The methodology combines a number of existing techniques (Feynman-Kac formulation, neural operators, Fourier interpolation) to achieve some improvement in specific setups, e.g. where the solution of the PDE is rapidly varying in space and/or time.





Weaknesses:
Clarity: The quality of the writing could be improved, particularly in the abstract/introduction. The rest of the paper is detailed enough in describing the experiments and discussing the results. 

Significance: The proposed approach seems to give an advantage only in specific situations. While this is acknowledged in the paper, it could be beneficial to discuss specific applications where such oscillatory conditions occur.



Limitations:
The limitations and extent to which this method gives an advantage over existing approaches are discussed in detail in the final section of the paper. 

Rating:
5

Confidence:
4

";0
SlXKgBPMPn;"REVIEW 
Summary:
This work proposes another auto-regressive-based graph generative model similar to GRAN. The authors propose a hierarchical generation scheme to un-coarse a graph level by level. In each non-leaf level, the abstract graph is weighted both in nodes and edges. A node represents a community, and its weight represents how many edges should be inside the community. An edge is the ""connection"" between two communities, and its weight represents how many edges should exist between the two communities. The weights of each community are generated through a stick-breaking process. And the number of communities is automatically decided by it. The structure within the community is generated using an AR model. Then the edges between communities are generated using GNN.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The assumption makes sense, and the model decomposition is quite convincing.
2. This method can indeed improve generation efficiency by only auto-regressively generating the diagonal blocks of the adjacency matrix and using GNN (which has O(M) runtime) to predict the off-block entries.
3. The method is simple and straightforward.

Weaknesses:
see questions below

Limitations:
1. one possible limitation is the edge independency of the model.
2. The model has made a strong assumption that the graph should have a community structure, while the experiment datasets are relatively small and may not have such a structure.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes HiGen a hierarchical generative graph model. The model consists of a clustering process (Louvain), followed by a GNN model (GraphGPS) to estimate probabilities. The generative process is separated by communities and bipartite sub-graphs.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper seems original. The proposition of a new model is always an important contribution. Even though the new model is the combination of a clustering process and GNN. The combination of both ideas is interesting. 

The theoretical quality of the demonstrations is good. Most of them seem fine and no errors were observed during the revision. 

Parts of the papers are quite clear. Figure 1 really helps to understand the main idea of the paper. However, there is room for improvement.

The significance of the paper is high, it seems that this new model is able to reproduce the mean of the distribution quite correctly in comparison to other state-of-the-art methods, as it is shown in the results.

Weaknesses:
The state of the art can be improved. The paper mentions ""there exists no data-driven generative models specifically designed for generic graphs that can effectively incorporate hierarchical structure."". Neville et al. focused on this type of work, generating several papers related to hierarchical graph models (doi.org/10.1145/3161885, doi.org/10.1145/2939672.2939808, doi.org/10.1007/s10618-018-0566-x). 

Parts of the paper are closed related to mKPGM (doi.org/10.1145/3161885). In both cases there is a hierarchical structure, both have the idea of a super-node at the higher level, and the sampling process is also based on a multinomial distribution (doi.org/10.1007/s10618-018-0566-x). Please take a look at the sampling process proposed, because it has similarities to the proposition of this paper, and the authors claimed to sample a network with billions of edges in less than two minutes.

The paper must state its main contribution. In the beginning, it seems to be the model, but after reading the paper, it seems to be the sampling process. Unfortunately, both of them have different issues.

If the main contribution is the model, then the paper should improve the modeling of the main network and be fairly compared in the experiment section against other baselines (not just the mean of the distribution). The main models consider $\ell$ hierarchies, but just two are applied. It is also not clear how the final probabilities are obtained. 

If the main contribution is the sampling process, there are some issues too. The time complexity of the generative model claims to be O(n_c \log n), but this is not demonstrated. The results of the paper are focused on the modeling of networks, not the sampling process. For example, there are no empirical results about the time complexity, and the largest networks have some thousand nodes, rather than millions.

I understand that the papers follow the experimental setup and evaluation metrics of Liao et al. However, this methodology must be stated in the main paper, otherwise, the experiments of the main paper are not reproducible. 

The results of Table 1 are difficult to read because of the lack of explanations. There are no details on the separation of the data in the main paper. I understand that this is explained in the supplementary material (80% for training and 20% for testing), but it must be considered in the main paper too. Moreover, I do not know if the values are the average over the 20% of the testing graphs or, if you just considered it as a single distribution. In the first case, please add the standard deviation, to see if the difference at statistically significant. 

Section 5 claims: ""The results demonstrate that HiGen effectively captures graph statistics"". Considering that, generally speaking, MMD estimates the distance between the means of two distributions, I suggest you change it to ""The results demonstrate that HiGen effectively captures the mean of the graph statistics"". Given the use of MMD, you can not determine if the other part of the distribution are correctly estimated. 

The conclusions state that HiGen ""enables scaling up graph generative models to large and complex graphs"" but this is not demonstrated.

Limitations:
No, the authors did not consider the limitations of the proposed model. For suggestions, please check weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduces an innovative hierarchical method for graph generation, which employs multiple levels of graph coarsening. This approach begins with the first level, representing the most coarse graph, and progressively expands nodes and edges to form new communities and connections between the newly created nodes. At each level, nodes serve as communities for the subsequent level, and the edge weights, including both inter-community edges and self-loops, dictate the total number of edges within each community in the final graph. Consideration of independence among the generation processes of inter and intra-community edges, conditioned on the graph and edge weights from previous levels enables parallel execution of the steps, resulting in acceleration of the generation process.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper effectively utilizes hierarchical clustering to enhance the graph generation process, capitalizing on the benefits of this technique.
2. By introducing parallelization in generating distinct clusters at each level, the paper successfully minimizes the number of sequential steps required.
3. The experimental results presented in the paper demonstrate improvements across multiple datasets.
4. Paper for the most part is well-written and easy to follow.

Weaknesses:
1. In lines 35-36 paper mentions that this work is the first hierarchical method for generic graphs. I believe [1] is also a hierarchal method for graph generation. I understand that methods are significantly different but still it would be more accurate to highlight the unique aspects of the proposed method and consider including a comparison between the two approaches.
2. The time complexity analysis provided in the paper focuses solely on the sequential steps, neglecting to consider the computational requirements. It would be valuable to compare the overall computational workload, particularly since the proposed method utilizes the GraphGPS approach, which has a time complexity of $O(n^2)$, in contrast to conventional GNN methods with a complexity of $O(n+m)$. Including such a comparison would provide a more comprehensive analysis.
3. The paper lacks a study examining the distribution of community sizes during the generation process across different datasets. Addressing this limitation by investigating and reporting the distribution of community sizes would enhance the understanding of the method's behavior and its adaptability to various datasets.
4. The paper uses a more advanced GNN compared to methods like GRAN, raising the question of how much of the observed progress is solely due to the change in the GNN architecture. Conducting an ablation study specifically focused on the GNN architecture used would provide valuable insights into its individual contribution to the overall performance of the method.
5. The evaluation metrics commonly employed for graph generative models have their limitations, as discussed in [1] and [2]. It is important to consider these limitations in the evaluation process. The paper mentions the use of random GNNs as an alternative evaluation method, but this approach is only used in the appendix for a few experiments. I would suggest using this in the main body and comparing all models using this metric. [Additionally/Optionally, there are two more recent approaches, one based on contrastive training and another one based on Ricci curvatures that could be incorporated for evaluation purposes.]

[1] Shirzad, H., Hajimirsadeghi, H., Abdi, A. H., & Mori, G. (2022, May). TD-gen: Graph generation using tree decomposition. In International Conference on Artificial Intelligence and Statistics (pp. 5518-5537). PMLR.

[2] O'Bray, Leslie, et al. ""Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions."" arXiv preprint arXiv:2106.01098 (2021).

Limitations:
Considering the assumed independence among the clusters and the cross-edges connecting them, it is evident that there exist certain graph distributions which the model may struggle to learn.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a graph generative model that is analogously structured as the inverse process of graph pooling, where the model first split a single node into a metagraph. This metagraph is further partitioned by utilizing a multinomial scheme, which allows for the division of nodes and edges into intra-community and inter-community connections. The proposed model's performance is evaluated on several benchmarks using various metrics, demonstrating state-of-the-art results.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
1. The approach of initially generating a graph's skeleton and subsequently refining its details is a novel and intuitively logical motivation for the proposed model.

2. The proposed methods have demonstrated state-of-the-art performance on some widely adopted benchmarks.

Weaknesses:
1. Some important technical aspects in the paper may require additional clarification or more detailed elaboration. Here are the major concerns regarding specific aspects:

    (1) Can the authors please provide more information on the loss function utilized in the model?

    (2) How is the weight on level 0 determined during model inference?

    (3) On line 188, the node embedding matrix $\mathbf{h}_{\hat{C}}$ is referenced without being defined. Could the authors please explain how this matrix is generated from the node and edge embeddings of prior levels?

    (4) Could the authors please elaborate on how the graph neural network (GNN) is utilized throughout the entire process?

2. The utilization of notations in the paper has resulted in a significant amount of confusion. There are two main issues that need to be addressed:

    (1) Inconsistent notations caused by reusing the same symbols: One notable example is the letter ""t"" used at lines 187-188, which has multiple interpretations. In $\hat{C}_{i,t}^l$, ""t"" represents the ""t-th"" step in the stick-breaking process. In $h{(t, s)}$, ""t"" denotes the node that is associated with community ""i"". Moreover, when referring to the node matrix size as ""$t \times d_h$"", it indicates the total number of nodes in community ""i"". These varying interpretations of the same notation can lead to confusion and should be clearly distinguished or explained consistently throughout the paper.

    (2) Notations used without being defined: An example is the ""r"" symbol in Figure 1 (c). Although it is assumed to represent the acronym for ""remaining (edges),"" its precise definition is not explicitly provided in the paper. To enhance clarity, it would be beneficial to define such notations explicitly or provide a glossary of symbols and their corresponding definitions.

3. In the paper, the specific method for determining the number of mutually exclusive events (i.e., the edges split from the same parent node) when modeling the partition weights using a multinomial distribution is not explicitly mentioned. This aspect requires further clarification or explanation. The paper should provide details on how the number of events is determined, whether it is considered a fixed parameter based on the model's architecture or if it is treated as a latent variable to be inferred during the training process.

Limitations:
1. Please refer to questions 2 & 3.

2. The model is built upon the assumption that the graph contains underlying communities. While this assumption can aid in generating higher-quality graphs with evident community structures, it may come at the expense of the generation quality for graphs where the community structures are less apparent. It would be intriguing to explore how the quality of generated graphs varies with changes in graph modularity or other community metrics.

Rating:
5

Confidence:
3

";0
oO1IreC6Sd;"REVIEW 
Summary:
The paper presents a method for integrating hard constraints, represented by a linear operator, into neural field basis functions. This is achieved by learning kernel functions as basis functions at specific constraint points. Through experimentation, the paper provides evidence to show the effectiveness of the proposed method in comparison to unconstrained neural fields across diverse practical tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Originality: The paper introduces a novel approach by directly applying linear operator constraints to the basis functions constructed with neural field functions and learning a linear representation. Also, the weights are found by applying a solver to a linear system, which is nice and removes some optimization problems when the weights should also be learned.
Quality: The results obtained in the paper demonstrate the effectiveness of the proposed method across various tasks. I appreciate the efforts in comparing different common implementations of neural field bases.
Clarity: The paper effectively explains the reasoning behind critical implementation choices, such as the selection of basis functions and the choice of kernel.
Significance: The proposed approach addresses the significant challenge of the application of explicit hard constraints for neural fields.

Weaknesses:
- As a reader, I found it challenging to comprehend the training procedure without delving into the code. Therefore, providing a comprehensive explanation would facilitate understanding. This would make it easier to grasp the methodology.
- Furthermore, sharing more details about how regularization is applied would provide valuable insights into the approach. Explaining the specific methods used for regularization and their impact on the model's performance would enhance the clarity and comprehensiveness of the paper.

Limitations:
The authors have addressed the limitations of the work in the Summary section.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose a method to enforce hard constraint points on neural fields. Instead of a single black-box coordinate network predicting the field values, this work uses neural networks to learn basis functions which are then combined in a linear transformation. Given enough basis functions, the weights of this linear transformation can be found by solving a system of linear equations.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and easy to understand. The authors have included their source code which seems reasonably well organized.

The method itself is simple and guarantees that constraint points are not violated. It outperforms existing methods on the MERL BRDF dataset.

The paper includes a varied selection of experiments to validate the method, and some ablation studies have been performed and reported in the appendix.


Weaknesses:
The paper contributes little in terms of theory. While the derivation is simple enough, it would have been nice to see a theoretical argument for why this method converges faster than unconstrained training, at least for special cases.

The constraints seem to be applicable only to single points, which is not clear from the abstract. Consequently, initial states and boundary conditions cannot properly be handled using this method.

Experiments 4.1 and 4.4 are synthetic and very simple. Only experiments 4.2 and 4.3 measure real-world performance.

The paper introduces six basis functions in chapter 3.2 but they are never compared against each other in the experiments. The authors simply employ different basis functions for different experiments. This makes it hard to judge which one to use for any given problem. Comparisons against state-of-the-art is also sparse. Only the BRDF fitting experiment compares to related work.

The paper does not include learning curves for the various experiments. Figure 5 is somewhat related to learning curves, but the quantitative evaluation of the training process is severely lacking. Training time is not discussed in the paper either. Learning curves against wall-clock time would be appreciated.

Minor:
* L11: The claim in “Our approaches are demonstrated in a wide range of real-world applications.” seems a bit of a stretch. It’s more like one to two.
* L41: Missing citation for the statement about inequality constraints.
* L72-78: The review of previous work is missing stream functions for divergence-free fields (e.g. Deep fluids, 2019) and other conservation properties (e.g. Guaranteed Conservation of Momentum for Learning Particle-based Fluid Dynamics, 2022). Hamiltonian and Lagrangian networks have also been used with conserved properties in mind.
* The citations in the main text do not have hyperlinks to the references at the end.
* The hybrid kernel basis is poorly explained in 3.2.
* Footnote 2 is not explained. It is unclear what kind of LaTeX expressions can be specified.
* Chapter 5 is more outlook than summary
* A.2 and Figure 5: Which experiment does this belong to?


Limitations:
While some advantages and disadvantages of various basis functions are mentioned, the authors do not provide a limitations section and only lightly touch on general limitations of the method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
A broad range of problems can be formulated as linearly constrained problems, e.g., learning material appearance, interpolatory reconstruction, solving linear PDE, etc. In order to solve linearly constrained optimization problems, this paper developed a novel hard constraint method that builds upon neural fields and differentiable linear solver, naming constrained neural fields (CNF). 

**Contribution**:
1. A novel methodology CNF is proposed, specifically, linear equality constraints are transformed into a linear system, i.e., eq. (3)-(4);
2. Then both weights of neural fields, $\beta_i$, and learnable parameters, $\theta$, of neural fields (see eq. (2)) can be learned from gradient descent of the objective function in eq. (1) given a differentiable linear solver is applied to eq. (3)-(4);
3. Hyper kernel basis is proposed, which is benchmarked with various basis functions (see section 3.2) and demonstrates advantages such as stable conditional number throughout training (Fig. 5);
4. In experiments, 4 examples from very different background are solved by CNF with superior performance, showing the potential of CNF as a general learning framework for linearly constrained problem.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
**Originality**: CNF is a novel method of solving linearly constrained problem by implementing neural fields based on differentiable linear solver.

**Quality**: The paper shows that CNF can solve various problems with high performance.

**Clarity**: The methodology is clearly presented. Worthy of mentioning, the analysis of conditioning of the matrix due to different kernel methods (see Appendix A) is quite convincing about why the authors believe Gaussian kernel with hybrid kernel basis (see eq. (9)) is the best choice.

**Significance**: This work can be applied to a broad range of linearly constrained problems. 

Weaknesses:
The work relies on differentiable linear solvers, and therefore the most suitable problem for CNF is a linearly constrained problem. In section 5, the authors discussed that CNF can be applied to nonlinear problems given a differentiable solver. However, such a differentiable solver to nonlinear problems is in general not easy to get. Hence, CNF is currently limited to linear problems.

Limitations:
N.A.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors look at enforcing hard constraints on neural fields. Here, the problem formulation is to take continuous coordinates as input and predict the solution on these points as output. The neural field is represented as a linear sum of basis functions, and specifically, variants of a neural kernel function. The constraints must be linear operators which are then satisfied via using a linear solver. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- Enforcing constraints more precisely on neural fields could improve prediction performance, and trying to do this via harder constraints seems promising.

- The method utilizes flexible representations of neural fields (i.e., neural kernel fields; as well as other representations of the basis functions) to enforce the relevant constraints. 

Weaknesses:
- The evaluation metrics are limited, and it is hard to contextualize these results with respect to other neural field methods. For example 1, the authors compare to other neural representation methods, but what about comparing to “soft constraint” approaches as well? It would be helpful to know how speed vs accuracy compares when enforcing the constraint is a softer way. 

- There is no discussion of the speed and training time to implement this hard constraint approach. It seems like it would be expensive to do a linear solve on these matrix systems, greatly hindering efficiency. More details about the linear solver would be helpful (as well as how the authors treat the system as fully differentiable).

- One major limitation of this approach is that only linear operators are used as constraints (thereby being able to utilize eon 3). The practicality of this method seems especially useful when the operator is non-linear.

- It would be helpful to describe the problem formulation for each problem (inputs/outputs, form of constraint, what is being minimized, etc.), as this is not always clear from the paper. 

Limitations:
- Limitations are not discussed (besides that this approach does not work for non-linear operators), but it seems like this approach would also be difficult to scale up for larger systems because of the expensive linear solves on larger systems.

Rating:
4

Confidence:
4

";1
zpVCITHknd;"REVIEW 
Summary:
The paper proposes to use the recently published model reassembly technique (NeurIPS 2022) to obtain personalized models through federated learning. At each round, the server collects the current models from the clients and uses reassembly to generate new candidate models, potentially training some stitching layers using a public datasets. Then the closest candidate model (again similarity is evaluated through the public dataset) is sent back to the client who can (later) train its own model distilling knowledge from the candidate model. By using the public dataset only for a few selected operations, the proposed scheme should be more robust to deviations from the training dataset and the public one.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The idea to use reassembly for federated learning is a novel one to the best of my knowledge 
* Experimental results are promising and the robustness to the choice of the public dataset is definitely an important plus of the proposed approach.


Weaknesses:
* In the proposed scheme, the server does not keep historical aggregate information about the training, as for example it does under FedAvg by storing the last version of the shared model.  Historical information is rather kept at the client, which at each round performs a local training with knowledge distillation from the candidate model selected by the server during the previous communication round to which the client participated.  As a consequence the method does not seem suited for large-scale cross-device settings where clients may be selected only a few times. The authors have considered clients' sampling rates between 1/3 and 1/10. I expect performance to decrease significantly for lower rates.
* The proposed solution requires the server to maintain the identity of the each client (to be able to send back the relevant candidate model). This prevents the applicability of privacy-preserving techniques like secure aggregation. Note that there are other personalized approaches which do not have this constraints (e.g., Ditto, FedEM,...)
* Computational overhead. If I understood correctly, the server needs to train the stitching part for every possible client/candidate-model pair, i.e., to train in total BM models, which poses a significant load on the server. 
* Complexity of the proposed solution. It would have been good to perform an ablation study to evaluate if all pFedHR steps are really needed. For example, what if the clients' models are directly compared and the closest one is sent as candidate model to the client without performing any reassembly and stitching?
* The candidate model can be more complex than the client's model. There is then an implicit assumption that, while the client has selected a given model size for example on the basis of its computational and memory capabilities, it is still able to use a more complex model for knowledge distillation at training time
* The comparison with the previous literature is not always clear. Two examples:
	1. a limitation of previous literature would be that ""the averaging process significantly diminishes the characteristics of individual local models"" I found this sentence too vague.
	2. ""however, FedDF trains a global model with different settings compared with our approach.""  Again, this is too vague, what is the difference with FedDF in a few words?
* compute
	* the authors checked the compute checkbox but I was not able to find any information about computation in the paper or in the supplementary material
* reproducibility
	* while the code is provided there is no readme file about how to use it and how to reproduce the results in the paper.
* minors:
	* footnote numbers should go after punctuation marks
	* report the number of clients for table 4
	* typos: bettwen and cadidates 


Limitations:
I think the paper should have discussed the following limitations (see corresponding weaknesses above)
* performance under low clients' sampling rate
* the server keeps track of clients' update
* Computational overhead
* The clients need to work with models more complex than its own.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors designed a method to train a model with FL when clients have heterogeneous model architectures. They designed a model reassembly technique that stitches together parts of DNNs. pFedHR also creates personalised models for each client without requiring server-side data or explicit human guidance. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Heterogeneous FL is an important problem 
- The paper is well-written and easy to understand
- The authors compared their approach with number of existing algorithms 

Weaknesses:
- The reason to stitch together heterogeneous architectures is not very well motivated. There are many other approaches that aim to learn personalised model for a set of clients with heterogeneous capabilities. For example FjORD [https://arxiv.org/abs/2102.13451]  train a number of subnets using adaptive dropout, HeteroFL [https://arxiv.org/abs/2010.01264] is another model where a superset is trained and submodes are used to address heterogeneity in FL, FedRolex [https://arxiv.org/abs/2212.01548], or [https://arxiv.org/abs/2210.16105] all use weight sharing to address similar tasks.

The authors should discuss what is the main advantage/motivation of having heterogeneous architectures stitched together over other approaches where the computational complexity of a model can be scaled up/down, and possibly compare with some of the above methods. 

- Most experiments were done with few clients (N=12 up to 100). This is very small and might be unrealistic in real-=world applications where we might have million of clients participating in FL. It would be great to show how things scale up. 

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors introduce a technique that tackles the challenge of enabling collaboration among client models with different network structures in federated learning. Unlike traditional knowledge distillation (KD)-based approaches, the proposed model involves dividing the heterogeneous models into distinct parts and subsequently reassembling them. This unique reassembling approach fosters model diversity, facilitating personalized local training at each client. The proposed technique alleviates the adverse impact of utilizing public datasets on the server. By providing experimental results, the authors establish the effectiveness of their proposed model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Overall, the paper is well-written and easy to follow. The motivations behind the research are clearly articulated, and the experimental results presented are both sufficient and convincing. 

- The introduction of the novel reassembling technique to federated learning is a significant contribution. This approach opens up new avenues for achieving model personalization, which has the potential to inspire further exploration by researchers in the field.

- The authors' inclusion of an empirical analysis that explores the negative impact of utilizing heterogeneous public data on the server is a notable and valuable aspect of this work. Moreover, the proposed reassembling solution effectively mitigates this issue, providing a practical and effective resolution.

Weaknesses:
- In the experiments, the authors use four self-designed CNN-based models for saving computational resources. However, there are some lightweight models, such as MobileNets, used in previous FL work. Incorporating these lightweight models would enhance the comprehensiveness of the experimental evaluation and provide a broader perspective on the proposed approach.
- Figure 1 is a little hard to read, and I suggest the authors use one setting as an example to demonstrate the challenge of using heterogeneous public data and put other results in Section 4.4.
- Adding a readme file to the source code package would be a useful addition.

Limitations:
The authers have provided some discussions.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The authors design a novel approach to make it possible for clients equipped with different model structures to cooperate in the federated learning framework. Specifically, the server will reassemble models into different parts and assemble them together. After that, they propose a similarity-based approach to match the most fitted model with the clients’ models and distribute them back to the clients respectively. To emphasize the advantages over the KD-based approaches, the authors discuss different perspectives of using public datasets. Finally, they address the effectiveness of the algorithm using experiment results under the IID and non-IID setting compared with other baselines.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. This paper presents a new way to achieve personalized federated learning using heterogeneous model reassembly, which is significantly different from existing work.
2. The authors aim to design a new model to alleviate the issue of performance drop caused by introducing public data on the server, especially when its data distribution is different from that of clients. 
3. The authors conduct extensive experiments on different settings, including 12 clients, 100 clients, IID, non-IID, and public data with labels and without labels. The experimental results demonstrate the effectiveness of the proposed model.

Weaknesses:
1. The training of stitching layers is not significantly clear. Are they trained with the other parts of the networks or trained separately? The authors can provide a description of how the networks are trained after they are stitched together.
2. Since the generated candidates may change for different runs, are the results averaged by multiple runs?
3. The font size used in Figure 1 is too small.

Limitations:
The authors discussed the limitations of the proposed work.

Rating:
7

Confidence:
4

";1
9VqMaSjf7U;"REVIEW 
Summary:
This paper proposes a diffusion-based method for fMRI-guided image synthesis and claims that it can identify the selectivity of various ROIs in the visual cortex. The motivation of this work is well-defined and is of vital importance to the computational neuroscience field. While the method in this work is intuitive and lacks machine learning depth.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Strengths: 
* Interesting scientific field and question of neuroscience.
* The paper is well-written and has a smooth and concrete flow.
* The experiments and visualizations are in good condition and sufficient.

Weaknesses:
Weaknesses

* The method used in this paper is heuristic and lacks technical depth. 
    * Generating or reconstructing images from fMRI is not a new idea, as has been proposed in [1], etc.
    * CLIP is an off-the-shelf large-scale pre-trained model, performing conditional diffusion-based generation with embedding from CLIP or Stable Diffusion is stereotyped [2,3].
    * In Section 3.3, the domain-specific techniques you designed for this specific task are just some first-order tricks (i.e., linear combination of S, euler approximation). Maybe these tricks are effective enough, but that's overly empirical.
* The experimental section is empirical and lacks theoretical analysis. Your analysis of the fMRI encoder with ROIs is interesting, but little focus had been put on what's the scientific relationship of neuroscience and diffusion model.



[1] https://www.biorxiv.org/content/10.1101/2022.11.18.517004v3

[2] https://arxiv.org/abs/2112.10752

[3] https://arxiv.org/pdf/2208.01618.pdf

Limitations:
To my knowledge and understanding, there is no potential negative societal impact of this work. Other limitations please see “Weaknesses”.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents a new algorithm to guide a diffusion model to decode maximally activating images for particular voxel subregions of human fMRI using an encoding model trained to predict brain activity from images. The algorithm identifies stereotypic features for defined ROIs, such as faces or food items. The authors evaluate the specificity of the reconstructed image with CLIP zero-shot classification and human evaluation. They also cluster the encoding model weights and find that the clusters result in perceivable semantic categories in the reconstructed images. This algorithm allows for more fine-grained analysis of preferences across the visual system. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
There have been many papers that reconstruct images from brain activity. However, all of them require some form of retraining the diffusion model. This approach only needs an encoding model that can be trained independently. 

The extensive human evaluation is great. While this goes beyond what I would ask for a NeurIPS paper, I think it would be great to also verify them by showing them back to human subjects. 

Weaknesses:
The authors show that the reconstructed images possess stable properties that can be identified by humans and by a CLIP network. However, they don't show that the images also do what they are supposed to do, which is activate a particular subnetwork. While human experiments probably go beyond the scope of this NeurIPS paper, one way to do that would be to take **another** encoding model for the same neural activity, take it as a proxy for the brain and show the images to that. 


Limitations:
Limitations are discussed. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This study proposes BrainDIVE, a system for synthesizing optimal stimuli for any given region of interest in the brain. The model combines a pretrained latent diffusion model for image generation with a linear “brain encoder” trained to map CLIP feature vectors onto the corresponding brain activity. At test time, a gradient-based optimization iteratively produces an image that maximizes the activity of a predefined set of voxels. The technique is validated on well-known ROIs, producing the expected images. It can also highlight subtle differences between ROIs selective to the same broad category, or functional subdivisions of existing ROIs. The functional hypotheses are validated by human subjects evaluating the properties of the generated pictures. The technique appears simple but can provide meaningful information for neuroscience studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The method can retrieve subtle differences between ROIs selective for the same class (e.g. OFA and FFA).
* The method can highlight functional differences between sub-clusters of existing ROIS (e.g. food clusters)
* The qualitative observations are validated by behavioral evaluations from human subjects. 


Weaknesses:
* The Related works section tends to overstate the novelty of the technique. It only mentions NeuroGen as prior work, whereas other prior studies had also attempted to generate optimal stimuli for specific ROIs: for instance, Ratan Murty et al (2021), Ozcelik et al (2022), Ozcelik et al (2023). The latter also used a diffusion model for image generation, as done here. Furthermore, you seem to be aware of at least some of these studies, since you criticized one of them in your Methods section for using a “hand-derived” prior. It would be much better to properly acknowledge all these studies in the Related works section, and clarify your method’s advantages/disadvantages (e.g. no hand-derived prior/time-consuming iterative method).

Limitations:
Limitations are properly acknowledged.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces Brain Diffusion for Visual Exploration (BrainDiVE) that aimed at exploring the fine-grained functional organization of the human visual cortex. Motivated by the limitations of previous studies that relied on researcher-crafted stimuli, BrainDiVE leverages generative deep learning models trained on large-scale image datasets and brain activation data from fMRI recordings. The proposed method uses brain maps as guidance to synthesize diverse and realistic images, enabling data-driven exploration of semantic preferences across visual cortical regions. By applying BrainDiVE to category-selective voxels and individual ROIs, the authors demonstrate its ability to capture semantic selectivity and identify subtle differences in response properties within specific brain networks. It is also shown that BrainDiVE identifies novel functional subdivisions within existing ROIs, highlighting its potential for providing new insights into the human visual system's functional properties.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper's methodology shows promise in capturing semantic selectivity and identifying fine-grained functional distinctions within visual cortical regions. Also, the paper's potential significance lies in applying BrainDiVE to understand the fine-grained functional organisation of the human visual cortex. 
By providing insights into category selectivity, response properties, and sub-regional divisions, the paper opens avenues for further exploratory neuroscience studies. 
To achieve this, the authors perform the experiments in the manuscript are extensive, covering:
1. the semantic specificity of the method by decoding images from task fMRI and literature-obtained ROIs
2. compared the abstraction of face representation in the brain by comparing images decoded from two regions, the fusiform face area (FFA) and the occipital face area (OFA). 
3. Use the method to extend knowledge of brain function by finding subdivisions in known areas in the cases of food decoding.
The authors describe the experimental setting clearly at the beginning of section 4 for all cases.
The experiments show that the manuscript has a good balance between the combination of known methodological approaches and addressing interesting questions in neuroscience. Specifically, the authors perform a commendable effort in obtaining quantitative results using human evaluators (cf Tables 3 and 4).
Finally, the authors show through results evidence that their method is a window into understanding region-specificity hypotheses of brain regions which are knowingly involved in different aspects of visual processing.

Weaknesses:
The paper could benefit from comparing its results with previous contributions [e.g. 9 and 10] to highlight its novelty and contributions in relation to existing methods. 
The methodology part lacks clarity, particularly in explaining key components like the diffusion model architecture and brain-guided synthesis—further clarifying the role of the image-to-brain encoder in influencing the denoising process during inference.
The generalisability and reliability of results are hard to asses through a small dataset, specifically just 10 subjects. 
Furthermore, the paper lacks clarity on how the sub-divisions of the visual cortex are being verified or validated. Whether the results are specific to the analysed regions or the algorithm is being too biased by the experimental condition is not clear through the experiments. For instance, what would happen if the authors try to decode an area not specific to visual processing? An example of this would be using subsections of the orbitofrontal gyrus or other brain areas not expected to perform well in reconstructing visual stimuli.

As a small point authors should review the presentation of images. Figure 1 shows mostly best-case scenarios which are then not as good in Figure 4 (for instance the case of images generated from face voxels). This might bias readers. Second, some details, such as figure 4 appearing before figure 3 make reading the paper confusing.

Limitations:
Authors have not addressed the negative societal impact of their work, but this can be fixed by adding specific text in the Discussion section. The authors don't mention the demographic characteristics of the small human subject database that they have used not mention any ethical concerns related to decoding images from brain activations. This is fixable so authors should address it.

Rating:
7

Confidence:
4

";1
td6xbEOPLr;"REVIEW 
Summary:
This paper proposes to attack different fairness definitions for a variety of graph learning models. The task is formulated as a bi-level optimization problem, which is solved in a meta learning manner. The major advantages of the proposed framework include 1) it is feasible to any fairness notion and graph learning model, 2) it can support continuous and discrete perturbation on the graph topology. Experiments demonstrate the attack efficacy and classification utility of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed framework provides a good coverage for multiple types of (differentiable) fairness notions (e.g., statistical parity, individual fairness), and graph learning models (e.g., non-parametric, parametric);
2. Extensive experiments are conducted, considering possible fairness defenses (FairGNN and InFoRM-GNN) and transferability.

Weaknesses:
1. The clarity and rationale of methodology can be improved: there are inconsistent definitions (of the budget constraint), and the realization of budget constraint in the optimization procedure is not validated. See detailed questions;
2. The IID assumption for kernel density estimation may not hold in graph data;
3. Important baseline should be compared (a modified version of DICE based on the sensitive group); and more related works should be discussed.

Limitations:
The authors provide reasonable discussion about the limitations of this work (i.e., other fairness notions, space efficiency).

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose an attacking framework called FATE. Existing research in algorithmic fairness aims to prevent bias amplification but neglects fairness attacks. This paper fills this gap by formulating the fairness attack problem as a bi-level optimization and introducing a meta-learning-based attack framework. The authors present two instantiated examples, demonstrating the expressive power of the framework in terms of statistical parity and individual fairness, and validate the model's capability of attacking fairness through experimental verification. The paper contributes by providing insights into adversarial robustness and the design of robust and fair graph learning models.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well-structured and easy to follow.
2. The originality of the article deserves emphasis as it formulates the fairness attack on graph data as a bi-level optimization problem. This novel approach contributes to understanding the resilience of graph learning models to adversarial attacks on fairness.
3. The experimental results show that the proposed method is capable of attacking fairness without decreasing too much on accuracy.

Weaknesses:
The motivation provided is somehow insufficient in persuading me. The authors state that “an institution that applies the graph learning models are often utility-maximizing” (line 82), which I totally agree with, but then concludes that “minimizing the task-specific loss function … for deceptive fairness attacks” (line 88). While I do agree that pursuing utility will lead to a preference for models with superior performance, and if the objective of the attack is to deceive victims into selecting an unfair learning model, then it does make sense to enhance the utility of the malicious model. But it’s not the case in this article, where the attack’s aim is to poison the graph data. Unlike models, we don't have much discretion when it comes to the data, and it is exceedingly challenging for me to envision a real-life scenario wherein an institution would discard the data due to unsatisfactory performance, as a more practical solution is data cleansing or a more capable model. Consequently, my concern is that, is it truly necessary to maintain the utility for “deception”, leaving aside that I’m not convinced that preserving the utility is necessary for successful deception. Please provide a more detailed explanation.
 
The experimental findings reveal the limitations of the attack's effectiveness. Although I agree that FATE is capable of attacking fairness and is relatively more stable, in 2 out of 3 datasets, FATE exhibits incompetence in reducing statistical parity compared to FA-GNN. Although the authors argue that the victim model achieved the best performance under the attack FATE, I’m skeptical about the cost-effectiveness of this trade-off.

Limitations:
Yes, the authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel framework named Fate, which is capable of attacking any fairness definition on any graph learning model, as long as the corresponding bias function and the task-specific loss function are differentiable. Fate is equipped with the ability for either continuous or discretized poisoning attacks on the graph topology.studies. The paper provides insights into the adversarial robustness of fair graph learning and sheds light on designing robust and fair graph learning in future studies. The empirical evaluation on three benchmark datasets shows that Fate consistently succeeds in fairness attacks while being the most deceptive (achieving the highest micro Fl score) on semi-supervised node classification.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1) This paper addresses an important problem in graph learning — fairness attacks. While previous work in the field focused on ensuring that bias in not perpetuated or amplified during the learning process, the proposed framework, FATE, allows for the study of adversarial attacks on fairness.

(2) FATE, a meta-learning based framework, is versatile and can be used to attack different fairness definitions and graph learning models.

(3)The experimental evaluation shows that the proposed framework can successfully attack statistical parity and individual fairness on real-world datasets with the ability for poisoning attacks on both graph topology and node features while maintaining the utility on the downstream task.

(4) This article is well-written and well-organized. It starts with an introduction that highlights the importance of fair graph learning and the need for resilience against adversarial attacks. Then it provides some background information and defines the problem of fairness attacks in graph learning. The paper then proposes the Fate framework as a solution to this problem, providing a detailed explanation of its design and mechanism. It also presents experimental results to evaluate the efficacy of Fate. Finally, the paper concludes with a summary of its contributions and future research directions. Overall, the writing logic is clear and easy to follow.

(5) The paper provides detailed information on how they implemented the proposed framework, including the optimization process, the selection of the bias function and the task-specific loss function, and the hyperparameter tuning process. Additionally, they provide a detailed description of their experimental setup, including the datasets used, the graph learning models, the evaluation metrics, and the implementation details of the Fate framework and other baselines. The authors also provide a thorough evaluation of the proposed framework through extensive experiments and analysis.


Weaknesses:
(1) One weakness of this paper could be the limited evaluation of the framework on only three benchmark datasets and one task (semi-supervised node classification).
 Further evaluations on various graph learning tasks and datasets could provide more insights into the effectiveness and generalizability of the proposed framework.

(2) The proposed Fate framework may not be effective in attacking other fairness definitions beyond statistical parity and individual fairness, and it may not work well on graph learning models with very large graphs. 

(3) The paper assumes that the attacker has access to sensitive attributes of all nodes, which may not always be feasible in real-world scenarios.


Limitations:
see comments above

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a novel approach for introducing fairness attacks in graph learning, which is impressive. To address this issue, the article proposes an attack framework for graphs and conducts experiments on the classic GCN model. Compared to two baseline methods, DICE and FA-GNN, the proposed method is more effective in attacking graph neural networks.

The strengths of the paper include:

1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

The weaknesses of the paper includes:

1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

Weaknesses:
1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Limitations:
The article does not mention the limitations of the work. More discussions are required.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper studies an interesting problem, attacking fairness on GNN. Specifically, the authors aim to amplify the unfairness while maintaining the performance of the downstream tasks. They propose a bi-level optimization scheme with a meta-gradient poisoning attack to achieve this goal. Experiments on both statistic fairness and individual fairness show the effectiveness of the proposed method. Overall, good problem and a solid framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	Fairness attack on graph learning is an interesting problem, this paper gives an intuitive problem definition, which can extend to other bi-level optimization goal attacking scenarios.
2.	Given the bi-level optimization goal, the authors design a meta-gradient graph poisoning attack and corresponding Low-level and High-level loss functions.
3.	Experiments on statistical fairness and individual fairness demonstrate the effectiveness of the proposed method.


Weaknesses:
1. It will be more interesting if the authors provide more experiments on group fairness and Rawls fairness. How to attack some specific groups, such as best/worst accuracy group fairness.

Limitations:
None

Rating:
6

Confidence:
3

";0
xOJUmwwlJc;"REVIEW 
Summary:
This paper quantifies and proposes a mitigation for a phenomenon in DNN training, where more 'unusual' examples (here defined as having a higher average distance to its K=10 nearest neighbors) are generally more miscalibrated across a range of models and tasks. The authors propose a new proximity-aware calibration metric, PIECE, and demonstrate that it can capture calibration issues that are not necessarily captured by the standard ECE metric (and, in fact, always P>= ECE). Additionally, the authors propose a mitigation, ProCal. This mitigation is presented in two variations, one for continuous and one for discrete confidence. These work by adjusting the uncalibrated probability score based on the model's miscalibration on examples with that average distance.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is very clearly written, with a logical flow and good explanations of all steps taken. I particularly enjoyed the motivation via experiments on existing models leading to theory-driven proposals leading to further experimental verification. I also appreciated the through ablation study and the extra attention given to OOD examples.

In addition, the original miscalibration problem that the paper brings up seems to me to be important and relevant, and is well-motivated. As I am not an expert in miscalibration mitigation techniques I cannot fully comment on the novelty aspect of the work, though I have not seen this 'atypicality' concern brought up explicitly before; it also dovetails neatly with other research in atypical examples.

The proposed metric and mitigations, while very simple, are logical and effective, which I hope will lead to their adoption.



Weaknesses:
I felt that the 'atypical' (high $D(X)$) examples could have been better characterized. In particular, the authors relate these examples to underrepresented categories in datasets (eg, Black people in health datasets), but this is not necessarily so. It does not seem incredible to have a scenario where a data is divided into two clusters, where the smallest cluster is nevertheless very tight, and so has a smaller average $D(X)$.

It is not really clear what the ProCAL method refers to. It seems like it mostly splits into two methods each of which have their own name. Maybe it would be better to call them ProCAL-C and ProCAL-D  (for continuous and discrete)?

The analysis of ProCAL effectiveness seems a little sparse in claiming ""our method consistently improves the calibration""... My interpretation of the data would be that ProCAL is very helpful in conjunction with raw confidence as well as some of the other methods with higher PIECE scores (TS variants), but not as effective for methods such as IR and MIR, where it seems to hurt as much as help. I don't think that this disqualifies the paper, but a more thorough explanation and analysis of this would be appropriate.

(minor) in section 3.1, what is the sensitivity of two points having the 'same' confidence?
(minor) It is not clear why the definition of D(X) contains an exponent, rather than the simple average of the ten smallest distances.


Limitations:
The limitations are adequately addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the prevalence of proximity bias in calibration, i.e. the rate of miscalibration on samples that are far away from their nearest neighbors in the data (""low proximity""). The authors empirically show that this type of miscalibration is present across many models, and propose a new post-training calibration procedure for mitigating it. Their approach shows significant empirical improvements over standard post-training calibration approaches.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- **Originality:** The empirical investigation and proposed methodology in the paper is quite novel, as I am not aware of prior work that has studied this type of bias in calibration (although others have studied subgroup calibration). 
- **Quality:** The claims in the paper are technically sound, and the experiments exploring proximity bias are extensive (more than 500 pretrained models considered, and various calibration baselines).
- **Clarity:** Overall, the paper is well-written and well-organized, with motivating experiments and intuitive definitions. However, I believe the presentation of the calibration algorithm in the paper could be improved (detailed further in weaknesses).
- **Significance:** The idea of low proximity samples introduced and analyzed by the authors seems quite significant, as these samples can correspond to underrepresented populations in the data.

Weaknesses:
- **Algorithm Details:** The weakest part of the paper in my view is the lack of detail in Section 5.1. This part of the paper would be significantly improved by including something akin to the pseudo-code algorithm in the appendix. There are several questions that arise when reading this part: what does one do after estimating the posterior probability conditional on prediction and proximity (in the algorithm I can see that this is just the output on the test point)? Do we compute proximity only with respect to the test data? What type of KDE is used (i.e. kernel, bandwidth, etc.)? 
- **Implementation Details:** In addition to algorithm details, some parts of the experimental setup could also be made clearer. How are ECE and ACE computed (i.e. binning scheme)? Do you set aside calibration data for the scaling methods (TS, ETS, PTS, PTSK) in addition to the set aside data for ProCal?

Limitations:
The authors appropriately discuss limitations, but it might be helpful to include some bits from the appendix (particularly regarding efficiency) in the main body.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The article focuses on the problem of uncertainty quantification in classification.
Calibration provides some guarantees on the estimated class probabilities on average. However, subgroups can still be miscalibrated. The article first aims to characterize these subgroup miscalibrations through proximity levels of the samples. It claims that a classifier, even calibrated, tends to be underconfident on high-proximity samples and overconfident on low-proximity samples. To measure this effect, it defines a proximity-informed ECE. Then, it proposes a recalibration framework based on this proximity-informed measure. Finally, it benchmarks the proposed method on numerous datasets and models.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The problem is well presented and motivated. Figure 1 is pedagogical and helps the comprehension of the problem.
* The idea of characterizing subgroup miscalibrations through proximity is interesting, and refining uncertainty estimates is a good direction.
* The proposed framework is versatile: it can work as a stand-alone or combined with standard calibration techniques. It provides two versions: binning-based and continuous.
* The experimental study is substantial.
  * Datasets are large-scale, numerous, and multimodal: ImageNet, Yahoo-Topics, iNaturalist, ImageNet-LT, MultiNLI, ImageNet-C.
  * The article studies numerous models, e.g. 504 pre-trained models on ImageNet.
  * It compares many standard calibration methods, both scaling-based and histogram-based: temperature scaling, ensemble temperature scaling, parameterized temperature scaling, histogram binning, isotonic regression, and multi-isotonic regression.
* The experimental study provides substantial evidence.
  * It reveals proximity bias in most of the 504 pre-trained networks on ImageNet (72% according to a Wilcoxon rank-sum test).
  * The proposed method consistently improves over standard calibration methods.
  * The time overhead of the method is small, with an increase of 1.17% in inference runtime.
* Completeness of the study: It reveals the proximity bias, proposes a metric to measure it, a recalibration procedure to address it, and substantial experiments showing consistent improvements.

Weaknesses:
No major weaknesses.

Limitations:
The authors discuss the following limitations:
* The proposed recalibration technique needs to maintain the calibration set during inference to compute the proximity of the new points.
* Maintaining the calibration set for inference may challenge the method when memory is limited.
* Focus limited to the closed-set multi-class classification problem.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This work addresses the problem of proximity bias and confidence calibration by performing a comprehensive empirical study of various pretrained ImageNet models. The empirical findings provide insights on persistence of proximity bias even after performing calibration using existing post-hoc calibration algorithms. To mitiagte proximity bias and improve confidence calibration based on sample proximity, the paper proposes PROCAL algorithm that can be used as a plug-and-play method combined with existing calibration approaches. Further, proximity-informed expected calibration error metric is introduced to quantify the effectiveness of calibration algorithms in mitigating proximity bias. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- A comprehensive study of pretrained ImageNet models involving various neural network architectures on their model calibration and proximity bias evaluation. The empirical study is performed on image classification tasks (under balanced, long-tail, and distribution-shift settings) and text classification tasks. 
- Experimental evaluation if thorough using various calibration metrics. 
- The paper provides many interesting observations from the empirical study related to proximity bias and model calibration, which is an important area of study under long-tailed data distribution settings.

Weaknesses:
- The presentation of experimental results and writeup of Experiments Section 6 can be improvised, the findings for many of the questions are pointed to the Appendix without any brief details in the manuscript. I understand this is due to page limitation, but I would suggest the authors to focus on the results that are presented in main manuscript, or include the brief info in the manuscript at least.


Limitations:
The authors have addressed the limitations of their work in the manuscript.

Rating:
6

Confidence:
3

";1
Q3CRHnttxW;"REVIEW 
Summary:
The paper presents two primary contributions for the causal bandit problem with unobserved confounders, general graphs, and discrete RVs.
1) a lower bound on the regret any algorithm can obtain for this problem. 
2) an upper bound on the regret of a newly proposed algorithm that is able to generalize across interventions (unlike an algorithm such as vanilla UCB) using allocation matching. 

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
- This is the first causal bandit algorithm I've seen that has a regret guaratnee for the cumulative regret setting and general graphs (besides [20], which focuses only on reducing the number of arms rather than using structure to generalize across arms). I think the community will find this very interesting. 
- Generally well written prose.
- There is not much to add regarding the strengths: the paper is generally clearly presented and the result is a novel solution to a open problem that the community has shown a good amount of interest in. I have not gone through the supplementary material so I cannot comment on the correctness of the proofs. 

Weaknesses:
- Experiments are synthetic (fairly common in these causal bandit papers in fairness) but also on extremely small graphs. It would be good to understand how performance varies with graph size and the size of the intervention set. 
- It would be nice to see a discussion in words of how the regret bound scales with various quantities- graph size, intervention set size. The bound as presented is not one of the easiest in terms of extracting this kind of qualitative information. 
- Some prose was clunky and I recall a few typos. The text should be run through Grammarly or a similar tool. 

Limitations:
- In the related work the nonparametric setting studied here is presented as a benefit but it should also be mentioned that the fully discrete setting here can be limiting compared to works that study continuous observations (it is also not really clear that the main ideas described here can be easily extended to those settings). 
- The computational intensity of the algorithm, especially depending on graph size and intervention set size, should be discussed. 
- Line 338 ""it is shown that SCM-AAM has superior empirical performance compared to existing baseline algorithms"". I think this is potentially misleading since the baseline compared to is from one of the earliest casual bandit papers and likely not the strongest among published baselines on the graphs used. I will not insist on comparing against every or any other causal bandit algorithm published since then but I think the claim could be made more moderate, rather than implying that some kind of SOTA empirical performance has been shown. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the structural causal bandits in which the causal graph contains latent confounders. The paper contains a lower-bound on attainable regret as well as an algorithm that achieves instance-dependent upper bound.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper studies an established problem, building primarily on [20] and the idea of intervening on POMIS. The paper develops a number of theoretical results on attainable regret.


Weaknesses:
The paper is quite dense and could benefit from a more clear exposition on its theoretical contributions. For instance, this may include close-form or approximate values on the various factors listed in L228, L291.

To underscore the advantage of SCM-AAM and to establish novelty, it seems that one has to establish SCM-AAM is better than the algorithm developed in [20]. The comparison part L293-303 (particularly L298-L301) could benefit from greater clarity on the precise benefit of this paper's algorithm over that of [20].  It would be nice to see a particular factor, say in a particular causal bandit instance where it is clear the algorithm attains better regret. It seems like graphs in Figure 2 are promising candidates, as it is shown empirically that SCM-AAM does better.

Limitations:
Seems fine.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors present a method for studying structural causal bandits with unobserved confounders. They show that their method, which unlike the baseline (POMIS), includes two tuning parameters, has superior empirical performance on three synthetic experiments (three synthetic SCMs). 

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
The main review can be found in the Questions section.

Weaknesses:
The main review can be found in the Questions section.

Limitations:
The main review can be found in the Questions section.

I think the title is a bit misleading; plenty of studies have considered causal bandits with unobserved confounders (Lee and Bareinboim being a pair of authors who have published a number of works in this area). Why do you draw attention to this aspect of your method? Would be interested to hear your thoughts on this.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper considers online learning in a structural causal bandit whose stochastic environment is govened by causal relations represented by an SCM. The paper gives a logarithmic asymptotic regret lower bound characterized by an optimization problem, and inspires on that, provides an algorithm to untilize the causal structure and accelerate learning process. Numerical experiments illustrate that the proposed algorithm outperforms traditional online learning bandit algorithms.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Writing. The paper has a clear organization of the relative background, concerning Structural Causal Model, Causal Bandit, and Interventional Distribution Tuples in Section 2-3. The presentation is good and easy to follow.
2. Originality. The paper studies online decision-making algorithms for causal bandits. The lower bound result extends from similar results in the structured bandit [1], and so as corresponding algorithm.
3. Quality. The writing is of high quality, and theoretical results and experiments are enough to support the algorithm. 

[1] R. Combes, S. Magureanu, and A. Proutiere. Minimal exploration in structured stochastic bandits. Advances in Neural Information Processing Systems, 30, 2017.

Weaknesses:
1. The whole paper seems to extend the methodology and algorithm in [1] to causal bandit settings. It is better if the author could highlight extra contributions of this paper.

[1] R. Combes, S. Magureanu, and A. Proutiere. Minimal exploration in structured stochastic bandits. Advances in Neural Information Processing Systems, 30, 2017.

Limitations:
See above.

Rating:
6

Confidence:
2

";1
Eewh7sl0Xj;"REVIEW 
Summary:
The present paper offers a Toeplitz matrix architecture which can handle sequence modeling. The architecture comes in two flavors. The first flavor is a fast version that is most useful for bi-directional tasks. It speeds up previous Toeplitz networks by using an interpolation and low rank approximation scheme in its setup. The second is a Fourier based model that appears to offer advantages in causal tasks. Benefits are shown in terms of the speed of training and some marginal benefits in performance.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The main strength of this paper is a noticeable speed up in an alternative architecture to transformers. I am a fan of papers that look at ways to speed up sequence modeling. The Curren paper presents a nice idea. More specifically:
- The approximations to the TNN appear to be effective in experiments and result in a faster network
- Approximations do not seem to deteriorate performance and may offer some added performance boosts
- The changes to the architecture are grounded in some theory 


Weaknesses:

**Disclaimer**: I am not an NLP expert and am more focused on the theory side. I have significant concerns about the theory in this paper, but feel the experiments and techniques are solid enough to potentially overcome that issue. Furthermore, none of the theorems are crucial to the crux of the paper, and if any are wrong, they can be removed. For this reason, I placed a borderline accept rating for now, but I believe this will need to be confirmed by people who are closer to the experimental side of the literature and can assess the experiments in a more rigorous fashion. 

\
Broader comments:
- Most of my larger concerns revolve around the theory and proofs in this project listed below.
- Reading through many times, I could not understand what was gained in the “causal training” proposal in section 3.1. First, it seems the parameters are changed to live completely in the Fourier regime. This change was made to obtain “an alternate causal speedup"", but I don’t see where that speedup arises. Following the steps, the main change seems to take the algorithm to do an FFT on the $n$-dimensional space resulting in runtime of $O(n \log n)$ which is worse than before. Also, changing the algorithm to work in Fourier space introduces a different implicit bias that I’m not sure is desired. For example, the $\lambda$ parameter controlling the decay is not controlled here. I also have many questions about this approach which I’ve left below.
- Experiments in table 1 don’t appear to offer much improvement especially considering the added parameters. I would also ask the authors to include citations to the models or results compared to in this table so it is easier to see what is being compared to. 

\
Theory comments:
- Theorem about ReLU MLPs being $d$-piecewise linear has assumptions missing or is just wrong. If the authors are implying that any ReLU network from $\mathbb{R} \to \mathbb{R}^d$ has $d$ pieces or contiguous linear regions, this is clearly wrong. MLPs are universal approximators so this is clearly false. If the authors are saying that this only holds for an MLP with a single hidden layer of width at most $d$ then this may be correct. But I don’t see this assumption made anywhere.
- Theorem 2 has a few confusing elements from my end. First, it is hard to parse. There are many variables and factors like condition number that it is hard to know the scaling of. Second, the bound doesn’t appear to be all that good. The error grows at least linear in $n$ and depends on other factors like singular values or the nystrom error that may also be badly bounded. I suppose the authors would argue that it is exponentially small in the degree of interpolation $N$, but this degree would have to grow at least logarithmically in $n$ to counteract the $n$ factor. This would result in a runtime essentially equivalent to just doing FFTs on the whole space. Third, the error in interpolation is an unusual thing to even bound in my opinion. The weights are updated with this interpolation taken into account. In other words, the algorithm learns a weight matrix with parameters contained in this interpolation. 
- Definition 2 and 3 present the discrete time Fourier transform, but in practice only the DFT of the matrix form is ever used. The resulting statements, regardless of their correctness, do not seem to apply to the setting in practice. Unless I am missing something.
- Related to the above, I cannot see why Theorem 3 and 4 are correct. Similar to my previous statements, MLPs are universal approximations so they can output any possible function. How can that statement hold true? 

\
Small:
- Simply having a % label on the y axis of Fig 1B is confusing. Percentage relative to what? Also, what does 20% speed-up mean; i.e. that it ran in 20% less time? Simply having this number on the first page can be confusing without the context added.
- Fig 1A and 1B also appear to be different size fonts.
- Line 150: I think the dense-case runtime is only a factor of $r$ worse so $O(nr  + r \log r)$ and not $O(nr^2 + r \log r)$ unless I’m missing something.
- For someone outside of the NLP community, section 3.1 needed more motivation and formality. Some details about what “causal masking”, “causal kernel”, and the sequential nature of the data would be helpful.
- To follow easier, it would be good to define what the role of N is in section 4.1 (i.e., number of interpolating points)

\
Formatting:
- Hyperref links seem to be broken
- Line 189: sentence is a run-on and hard to follow

\
Finally, to add ideas not for criticism, but instead for completing the paper and offering new ideas, there is a wealth of literature on related techniques that could be useful here, or at the very least cited. For example, there are sparse Fourier transforms that can offer speed-ups beyond the $O(n \log n)$ that the paper aims to improve on, e.g., [1]. Since matrices are low-rank and/or sparse, this could be a more direct way to get the speed-ups desired. Second, there are a number of papers on optimizing structured matrices like Toeplitz matrices, e.g. [2]. In the sequence modeling specifically, there have been a lot of papers on unitary networks for example [3], one paper which actually uses low rank approximations in its implementation [4]. Third, low rank approximations have also been used to speed up other architectures like conv-nets, [5-6]


\
**References:** \
[1] Hassanieh, Haitham, et al. ""Simple and practical algorithm for sparse Fourier transform."" Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2012.\
[2] Kochurov, Max, Rasul Karimov, and Serge Kozlukov. ""Geoopt: Riemannian optimization in pytorch."" arXiv preprint arXiv:2005.02819 (2020).\
[3] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. ""Unitary evolution recurrent neural networks."" International conference on machine learning. PMLR, 2016.\
[4] Kiani, Bobak, et al. ""projUNN: efficient method for training deep networks with unitary matrices."" arXiv preprint arXiv:2203.05483 (2022).\
[5] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\
[6] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.



Limitations:
There is a very brief discussion of limitations in the conclusion, though I feel this could be expanded. I would also appreciate some context for this work in relation to other works in NLP and how it fits into the broader landscape of NLP architectures. For someone like me not in the community, this would be useful to better understand its limitations from a practical perspective.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors of the paper propose two modifications of a recently published alternative to attention mechanism, Toeplitz Neural Operator (TNO), which constitutes the most important part of Toeplitz Neural Networks. The application of TNO is the multiplication of the input sequence by a Toeplitz matrix. Parameters of this Toeplitz matrix are given by a lightweight feed-forward network, called Relative Position Encoder (RPE). The first proposed modification, called SKI-TNN, represents a learned Toeplitz matrix as a sum of a sparse and low-rank matrix. Thus, its multiplication by a vector has the complexity of O(nr^2 + r log r) instead of O(n log n), where r is the rank of a second summand. However, this modification can speed up only the task of bidirectional modeling. In order to speed up the causal modeling (such as autoregressive language modeling), authors view the TNO as an application of kernel to a vector. Then, they train the RPE to model the real part of the Fourier transform of this kernel. The imaginary part is then computed via the Hilbert transform of the real part. This modification does not change the asymptotic complexity, but achieves empirical speed up.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The article explores an important topic of speeding up the token mixing part of a general sequence modeling pipeline. Nowadays, this topic is highly relevant because of its applications in the field of NLP. The article builds off of a very recent paper [1].
2. The article creatively combines together a large body of previous work. It uses the ideas of TNN, SKI, FFT, Hilbert transform, Nyström approximation, fast causal masking, etc. 
3. The experimental results show that the proposed modifications do indeed speed up the original TNN.
4. Overall, the presentation style is mathematically strict and to the point. The formulae in section 3.2.1 and in Appendix are sufficiently well-explained. Both modifications proposed in the paper are succinctly defined in Algorithm 1 and Algorithm 2. This helps the reader significantly to understand the main ideas.
5. In section 3.2.1, the authors specified not only the theoretical complexity of their modification, but also the practical limitations they meet when implementing it, and specified the practical complexity as well as theoretical.
6. The theory on the smoothness in Fourier Domain is supported with experimental visualizations
[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023.


Weaknesses:
1. The main claim of the article is the speedup achieved by the proposed modifications. The only results supporting this claim are Fig. 1 and some percentages in the text (in section 5.1). Fig. 1 shows the performance on specific tasks from the LRA benchmark. Firstly, it is not clear for which task the baseline (TNN)  is evaluated. Secondly, the choice of the tasks shown on the graph is questionable. The hardest task from the LRA benchmark, Pathfinder-X, is not shown. As for the speedups mentioned in the text: it would be better to put them all into a separate table. Moreover, it would be interesting to see a speed comparison in the form of a table similar to Table 5 from the TNN article.
2. In section 4.2, theoretical results on the choice of activations are presented. Several possible improvement ideas. The ablation study with experimental results for different activation types would be of interest. Moreover, the graphs showing the decay rate for randomly initialized networks might be improved. It would be better to leave only the lowest and highest lines and show the average line in between. Also, if you compare the rate of convergence to some baseline rate (e.g. exponential), plotting it would be appropriate. In addition, it seems to be not quite fair to compare the decay rates of trained and untrained networks.
3. While the overall presentation style is to the point, as mentioned earlier, it would help the reader if the abstract, introduction and related work were more general. Both abstract and introduction may be hard to read for an unprepared reader, as they contain too much mathematical details and not enough motivation. Moreover, some paragraphs of the introduction repeat the abstract almost word for word, while rephrasing would help the reader to understand the ideas more deeply. The related work section should describe either the ideas of mentioned papers or their connection to your paper more clearly.
4. The section 3.2.2 is a bit obscure. “Inverse time warp” is not a common term, and it is not described in the section.


Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents several techniques to speed up Toeplitz neural networks (TNNs). In particular, TNNs use convolution of length n (sequence length of the input) and so scales as O(n log n), and TNNs have many calls to the MLP that generate relative positional encoding (RPE) and decay bias. To reduce the time of convolution, for bi-directional modeling the paper proposes to approximate the Toeplitz matrix as a sum of a short convolution and a low-rank matrix, which results in O(n + r log r) complexity where r is the rank of the approximation. For uni-directional model (e.g. auto-regressive modeling), the paper proposes to parameterize the convolution directly in frequency domain and uses the Hilbert transform to obtain the imaginary part from the real part of the filter to ensure causality. The approximation error is then analyzed. Validation on language model (Wikitext-103) and long-range benchmark (LRA) show that the approximation lead to some speedup (10-15%) and the quality stays around the same.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The idea of using asymmetric Nystrom to approximate the Toeplitz matrix is quite clever. This allows a decomposition into a sparse and a low-rank component, which leads to asymptotically faster algorithm in the case of bi-directional modeling.

2. While uni-directional modeling prevents the Nystrom technique due to causal masking, parameterizing the filters directly in the frequency domain is able to overcome this challenge. While this is not asymptotically faster, it avoids one inverse FFT per layer and leads to some speedup.

Weaknesses:
1. Unclear what problem the paper is trying to address, and how it is motivated.
The intro starts out with Toeplitz neural networks, and the paper aims to make it faster. However, it's not clear why we want to make these faster, and what we would enable if we make these faster. Are they being used in very large-scale tasks? Are they being scaled to very long sequences?
While the technical contributions are solid, it's not clear to me why the paper chose to tackle this problem.

2. Unclear what the technical challenges are. 
- The paper mention that they want to avoid O(n log n) computation. But in practice O(n log n) isn't very slow, especially on GPUs. FFTs are pretty much bounded by memory bandwidth, and they take only 2-3 times as long as any pointwise operation. So if the goal is to speed up TNNs, then it makes more sense to have an efficient implementation, rather that using algorithms that faster asymptotically (O(n + r log r)) but is slower than a hardware-friendly algorithm (line 150, where using matmul with O(n r^2 + r log r) is faster). 
- The paper mentioned ""many calls to the RPE"". Why is this a problem? Showing a profile of how much each operation is taking will be much more convincing. That would motivate the approaches in the paper much better.
Without knowing how long each operations in TNNs are taking, how do we know that we're solving the right problem?

3. Lack of detailed speed benchmark. Given the goal is to speed up TNNs, I would have expected one of the main results to be speed benchmarks, across different sequence lengths, on different devices, to show the tradeoff. In the main paper, speed is only reported in Figure 1b, which is end-to-end speed for a particular sequence length (512).
How do we know that we're close to the maximum speed on these devices (GPU)? Or are we still far from optimal? When we speed up convolution and RPE, what is the remaining bottlenecks.
Having these would make the paper stronger.

At sequence length 512, an optimized implementation of attention (e.g. FlashAttention) is likely faster than FFT and the method in this paper. This is my impression as the Hyena paper [1] reports that TNNs are not faster than FlashAttention until sequence length > 4k.

[1] Hyena Hierarchy: Towards Larger Convolutional Language Models. Poli et al. 2023.

Limitations:
Not necessary.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes to reduce the computational complexity of Toeplitz neural networks. TNNs are a new form of network for sequence modeling that reduces space complexity of the attention matrix to allow for longer sequences.TNN model consists of a stack of Gated Toeplitz Units (GTU) that includes TNO (Toeplitz Neural Operator) that does token mixing with relative positioning. Then, GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO). 
The paper addresses the TNNs efficiency limitations: 1) super-linear computational complexity 2) many calls to the RPE: for each layer, one call per relative position. Thus, the paper proposes to reduce both the complexity of sequence modeling and of the relative positional encoder. The work proposes solutions by means of both the Structured Kernel Interpolation (SKI) [2] and working with frequency domains.
It does so through:
–	Approximating Toeplitz matrix using low-rank approximation and replacing the RPE MLP with linear interpolation and using Structured Kernel Interpolation.
(for O(n) complexity, that is use linear interpolation over a small set of inducing points to avoid the MLP entirely   -using an inverse time warp to handle extrapolation to time points not observed during training)
–	Causal training, SKI does not bring benefits, so instead they eliminate explicit decay bias by working in the frequency domain, using Hilbert transform (to force causality) and also use some smoothness
–	 For the bidirectional case, they eliminate the FFT applied to the kernels.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The work includes a number of solutions to improve TNN speed-up (addressing RPE MLP, the FFT, and the decay bias).

RPE is a neural network to obtain relative position embedding to obtain entries in Toeplitz matrices. These entries could be evaluated with stationary non-SPD kernel which is a good idea.

So first decomposing Toeplitz matrix and then using interpolation for MLP is a comprehensive pipeline.

The theory part makes the arguments more sound, and the explanations in supplementary materials are fairly abundant.

The experiments on LRA show good predictive performance on long range data and on wikitext some speed-ups.


Weaknesses:
The major paper of the paper talks about the SKI to accelerate the TNNs but in experiments SKI is only shown in the LRA experiment and does worse than both TNN and FD-TNN

As mentioned in the paper, doing sparse-dense multiplication in practice can be slower than dense-dense matrix multiplication (but that is only part of the potential speed-up)


Limitations:
Yes.

Rating:
5

Confidence:
3

";0
rUldfB4SPT;"REVIEW 
Summary:
This work incorporates images into point cloud pretraining since images contain richer semantic information. Instead of adopting the back propagation strategy which can not handle the misalignment between camera and LiDAR, this paper leverages the neural rendering technique to injecting the semantics into the representation learning process.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
(1) This paper is well-written and presents a clear and concise idea.
(2) The motivation of leveraging the neural rendering to overcome the mismatch of pixel and point is compelling.
(3) The experimental results demonstrate consistent improvement on different datasets with varying baselines. And the ablation studies validate the contribution of each component in the pipeline.

Weaknesses:
(1) The technical contribution is limited. For example, the BEV conditioned semantic neural rendering strategy, and the masking strategy have been studied in previous works.
(2) Although the occlusion problem can be alleviated by assigning a reduced weight to occluded points, the rendering process will introduce additional noise since it assigns multiple semantic labels to each point, leading to the semantic ambiguity.
(3) The rendering process allow every points in the ray receive the gradients which leads to many irrelevant points being optimized.
(4) The authors claims that the neural rendering is superior to the point-to-pixel projection, but there is no performance comparison between these two methods in the experiments. I think the latter may also bring an obvious improvement since the inambiguous projections.
(5) This work takes the semantic labels as supervision for neural rendering. I think it would be better to use the 2D feature vectors as the supervision.

Limitations:
The author address the limitations.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper investigates weakly-supervised representation learning for outdoor LiDAR point clouds. To start, the authors point out that the inherent incompleteness of outdoor LiDAR points would reduce the effectiveness of self-supervised representation learning approaches. To mitigate this, the authors propose to use synchronized images as additional signals to supervise the representation learning process. Observing the lack of color information in the point cloud and the mismatch between points and pixels due to occlusion, the authors propose to use neural rendering of pixel semantics as a pre-text task to circumvent the two problems. Specifically, with a slight modification to NEUS’s weighting function, the authors build an implicit neural representation on top of the BEV LiDAR features, positing that good BEV LiDAR features for implicit neural representations could be good representations for any downstream recognition tasks. Extensive experiments have been conducted to show the effectiveness of the approach.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
- Novelty - Using neural rendering as a pre-text task for point cloud representation learning is a novel idea the reviewer has never seen in the literature. 
- Beautiful figures - Figure 1,2,3 are aesthetically pleasing, with figure 2 clearly illustrating the core problems in representation learning for outdoor LiDAR point clouds. 
- Strong performance improvement over baselines that do not use images as additional signals. 
- Thorough experiments - Experiments thoroughly demonstrate the strength of the approach.

Weaknesses:
Overall, this is a strong paper but the reviewer has to point out three weaknesses — two related to presentation and the other related to baselines. 

- (Presentation 1) One of the contributions of the paper is to show that images could be valuable signals for pre-training representation. However, it is unclear which baselines in table 1 and 2 actually use additional images as signals. Also, it is unclear what type of signals are being used (i.e. raw pixel values vs pixel semantics). 
- (Baseline) Another contribution of the paper is to show that pixel semantics is a useful signal for pre-training representation. Although the proposed approach did not converge when trying to render raw pixel value (line 101-103), SLidR (table 2) is a baseline that actually leverages color information to form superpixel for pre-training. Given the big difference between the no-pretrain variants for SLidR and ours, it is difficult to judge whether pixel-value is a weaker signal or there is some other difference (such as model architecture or optimization recipes) that is causing SLidR to underperform PRED. 
- (Presentation 2): Since using pixel semantics is part of the core contributions, it is important for the authors to acknowledge/mention whether there are any overlaps between the pixel semantic label space and the downstream task label space. Without this, it is difficult to tell whether the proposed approach is a self-supervised approach or a weakly-supervised approach.

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper proposed a novel point cloud pre-training framework, PRED, which leverages the semantic information consistency between the LiDAR point clouds and the camera images to improve the point cloud pre-training performance. The author proposed (1) a novel semantic rendering module for decoding the semantics from the BEV feature maps and (2) a point-wise masking mechanism to alleviate the reconstruction ambiguity.

The proposed pre-training method demonstrates superior performance according to the experiments section.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
(1), To the best of my knowledge, the proposed point cloud pre-training framework with semantic rendering is novel and reasonable.
(2), point cloud pre-training is an important task for academia and industry.
(3), According to the experiment section, the proposed framework PRED has achieved superior performance on multiple benchmarks.

Weaknesses:
(1), Dealing with occlusion is claimed as one of this paper's major contributions and advantages. However, why the occluded points will be allocated a lower weight is not illustrated clearly in the paper. The author could add more presentation, analysis, visualization, and evaluation. 
(2), The pre-trained semantic model is required for the proposed approach. This point should be marked and compared with other methods in Tables 1 and 2.

Limitations:
(1) Is there anything special or novel for handling occlusion compared to [46]? If not, then this point should not be highlighted.
(2) How would the proposed method perform if the 2D segmenter fails?

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work proposes a new pretraining algorithm for outdoor 3D perception tasks, where images are utilized to provide comprehensive semantic information. The main algorithm is to leverage the semantics of images for supervision through neural rendering. The authors also apply point-wise masking with a high mask ratio to further enhance performance. The pretraining brings notable performance gains on multiple benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The authors provide a novel insight into the exploitation of image semantics by combing off-the-shelf image segmenter and neural rendering, which I think is well-motivated. 
2.	Extensive experiments on multiple benchmarks demonstrate the effectiveness of the proposed method.
3.	The paper is well-written and easy to follow. The illustrations are straightforward and helpful.

Weaknesses:
1. I acknowledge the motivation and the method design of this paper. Different from neural rendering for RGB as NeRF does, the high-level idea of rendering semantics from point-cloud is quite novel. However, I have heavy concerns about the use of Cityscape pre-trained segmenter as it is not general enough, e.g., Cityscape pre-trained segmenter can perform well on nuscenes and Once but can't on Waymo. Despite the authors explaining the reason Waymo experiments do not perform that well on Line101 of the appendix, I think the poor generalizability of pre-trained segmenter maybe also a reason. So I suggest replacing the pre-trained segmenter with SAM.

2. This work shares a similar spirit to Ponder [1]. Despite [1] for indoor scenes, the paper aims at outdoor scenes, the authors should carefully discuss the intrinsic differences. 

[1] Ponder: point-cloud pre-training via neural rendering.

Limitations:
Please refer above.

Rating:
8

Confidence:
5

REVIEW 
Summary:
1. The paper proposes PRED, a novel pre-training framework for outdoor point clouds that leverages image semantics through neural rendering. The paper addresses the challenges of incompleteness and occlusion in point clouds, which are common in outdoor LiDAR datasets for autonomous driving.
2. The paper uses an encoder-decoder architecture to extract a BEV feature map from the point cloud and render semantic maps from image views, supervised by image segmentation and depth estimation.
3. The paper incorporates point-wise masking with a high mask ratio (95%) to enhance the pre-training performance and avoid losing semantics of small objects.
4. The paper conducts extensive experiments on nuScenes and ONCE datasets, showing that PRED significantly improves various baselines and state-of-the-art methods on 3D object detection and BEV map segmentation tasks.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The paper presents a novel pre-training framework for outdoor point clouds that integrates image semantics through neural rendering. This is a creative and effective way to address the incompleteness and occlusion issues in point clouds, which are often overlooked by previous pre-training methods. The paper also introduces point-wise masking with a high mask ratio, which is different from the conventional patch-wise or group-wise masking strategies and preserves more semantics of small objects.
2. The paper is technically sound and well-motivated. The paper provides a clear and detailed description of the proposed method, including the encoder-decoder architecture, the semantic rendering pipeline, the loss functions, and the masking strategy. The paper also conducts extensive experiments on two large-scale datasets, nuScenes and ONCE, and compares with various baselines and state-of-the-art methods on 3D object detection and BEV map segmentation tasks. The paper reports significant improvements over previous methods, demonstrating the effectiveness and generality of the proposed framework.

Weaknesses:
1. The paper does not conduct ablation studies on the choice of image segmentation model and its impact on the pre-training performance. The paper uses DeepLabv3 as the image segmenter but does not justify or evaluate this choice. It is unclear how the quality and accuracy of the image segmentation model affect the semantic rendering and supervision.
2. This paper does not report the computational cost or time complexity of the pre-training framework. Semantic rendering involves sampling and aggregating points along multiple rays for each pixel, which may be computationally expensive and memory-intensive. It would be helpful to provide some statistics or benchmarks on the pre-training speed and resource consumption.

Limitations:
Please see the comments on 'weaknesses'.

Rating:
5

Confidence:
3

";1
hCdqDkA25J;"REVIEW 
Summary:
This work proposes optimization algorithms that achieve optimal convergence rate and reproducibility for the settings of convex optimization and convex-concave minimax settings. This work settled some of the open questions from the previous work, and extend the results to the minimax setting.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Clear presentation.
- Novel technical contributions.
- Strong theoretical results and cleanly written proofs.

Weaknesses:
- Overall, the presentation is very clear, and the results are rigorous, and I didn't see any major weaknesses.
- Some details about Inexact-EG would have been helpful since it seems like a new method developed in this work. In particular, is it a direct extension of Devolder et al. to the minimax setting?


Limitations:
- They discussed them well in the conclusion section.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The authors proposed and studied optimization algorithms in both the convex and the convex-concave minimax setting, where both the criteria of convergence and reproducibility are measured. The authors provided upper bounds on these criteria for the proposed algorithms that matches nearly all of the lower bounds. 

While I'm not an expert on this subject (and I leave the judgement of correctness to other experts in this area), I believe this work has reached satisfying results, and I would recommend accept. 

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1. The upper bounds on convergence and reproducibility matches nearly all of the known lower bounds. 
2. The presentation of the work is quite clean and readable. 

Weaknesses:
N/A 

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
The authors investigate the reproducibility problem of algorithms that solve convex and convex-concave minimax optimization problems.  They propose new methods with better reproducibility guarantees while maintaining the theoretical state-of-the-art convergence rates.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
*I want to acknowledge that I got this paper for review **after the deadline**, so I didn't have much time to check every detail, especially the proof.*

The idea of using regularization is good. It leads to new theoretical state-of-the-art convergence rates and reproducibility guarantees. This is a solid contribution to the NeurIPS community.



Weaknesses:
1. The new method requires $\epsilon$ and the distance $D$ between the starting point $x_0$ and $x^*.$ (e.g. Theorem 3.3.) I am not sure that the previous methods in Table 1 need these parameters. Do the authors discuss these important limitations?
2. Algorithm 1 is a well-known method in the optimization community. For instance, see https://arxiv.org/pdf/1603.05642.pdf. It is called ""regularization technique"" or ""regularization reduction."" I believe that the authors should cite the previous works that consider Algorithm 1.
3. Wrong citation [55] in Theorem 3.3. The paper [55] doesn't provide an analysis of AGD for strongly convex functions. It is better to cite any of Nesterov's books.

Limitations:
.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper considers the problem of ensuring reproducibility in convex optimization. It builds on a recent framework for understanding reproducibility initiated by Ahn et al. The paper considers both minimization and minimax optimization (the latter being a new setting investigated in this work). The key results of the paper are the following:

1. Minimization problems: The paper shows an improvement in the convergence-reproducibility tradeoffs under inexact initialization and inexact gradients compared to the results of Ahn et al. For inexact initialization, the paper shows that an L2 regularized version of AGD simultaneously obtains optimal convergence and optimal reproducibility. For inexact gradients, the same algorithm obtains sub-optimal reproducibility but with optimal convergence.

2. Minimax optimization: L2 regularized versions of existing algorithms achieve optimal reproducibility and near-optimal gradient complexity. Similar to Ahn et al., SGD attains optimal reproducibility and convergence under a stochastic gradient oracle.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The problem of developing reproducible optimization algorithms is well-motivated and relevant. Various empirical studies have shown that randomness in initialization, training, data augmentation and numerical instabilities can lead to models which make significantly different predictions on test points.

2. The paper demonstrates a valuable algorithmic principle of using L2 regularization to ensure reproducibility. All the results in the paper (apart from those for a stochastic gradient oracle) are obtained by incorporating L2 regularization into prior algorithms. The results strongly suggest that there could be deeper connections between stability and reproducibility, since L2 regularization is also a similarly useful techniques for ensuring algorithmic stability. Investigating this could be an interesting direction of future work.

3. The paper improves stronger bounds across a number of settings compared to prior work. It also demonstrates that suspected instability issues of AGD are not a barrier to obtaining reproducibility guarantees for it. The paper also broadens the study of reproducibility to minimax optimization, with a similar message for it.

Weaknesses:
1. The writing of the paper is decent overall, but could do with some improvements. The paper does not motivate reproducibility adequately on its own. Some of the comments seem to concern reproducibility in science broadly rather than the particular issues which concern reproducibility in modern ML. Since the paper has a lot of different results, some more intuition behind the specific bounds that are obtained could be useful. For example, the paper could comment on the reproducibility obtained in different settings and why these bounds arise from the algorithm.

2. Though I can understand that there is not too much space given the number of results, some intuition for the technical ideas which go into the bounds would be good as well. In particular, what are the main ideas behind extending reproducibility to minimax optimization? Does the intuition for why L2 regularization works for reproducibility in minimization mostly carry over and give the bounds for minimax optimization?

Limitations:
These are discussed adequately.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of reproducibility in convex optimization. The notion of reproducibility, borrowed from prior work, measures the ""stability"" of the output of a procedure under noisy initialization or gradient computation. For the smooth convex setting, they design an algorithm based on running accelerated gradient descent on a regularized objective, which achieves optimal reproducibility and convergence rate. This answers an open question from prior work. The authors further extend their results to the minmax optimization setting deriving many new results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Reproducibility has become an important topic in modern machine learning. Since (convex) optimization is the dominant algorithmic paradigm for modern ML, it is imporant to formulate and study reproducibility in optimization. The topic of the paper thus is important and timely.

2. The paper obtains the optimal bounds on convergence and reproducibility for the smooth convex setting, something which prior work conjectured to be unattainable. This is an important contribution. Further, they managed to also get optimal, and non-trivial rates for the minmax optimization, a setting which has received considerable attention lately.

 3. The paper is well written. Granted that it covers a lot of algorithms and results, the writing is to the point and the flow of ideas is natural.

Weaknesses:
1. About the definition of reproducibility: since this is a new field, I presume that the community has not yet agreed upon a definition. However, it seems to me that the only paper using the definition in this paper is the prior work of Ahn et al. Does adhering to this definition indeed reflect reprodubility in practice, in some sense? Even in optimization settings, there are other sources of instability not accounted for in the analysis, for instance, truncation and rounding due to the finite precision. Is adhering to reproducility (say with respect to initialization) and disregarding potential numerical instability arising in other steps give some meaningful in practice?

2. Related to the above, some experiments demonstrating usefulness of the framework would strengthen the paper. In the current version, there are no experiments.

3. The underlying idea is very simple and has been used many a times in (related) prior works -- regularization makes the problem strongly convex and thus aids leasds to (various forms of) stability. Nonetheless, the authors build on this to provide non-trivial bounds for many settings.

4. Some technical details, which I presume are in prior work,  are not covered in the main text. Something that confused me is how to define  ""optimal reroducibility"", which is referred many a times in the paper. Some text explaining it, perhaps in the preliminaries will be helpful.

5. What if we consider an inexact initialization as well as inexact grdient, with the same $\delta$ say -- is it possible to say something about this from the algorithms proposed?

6. The authors analyze a number of algorithms in the minmax setting, as a result this part of the paper looks rather dense with Thm statements.  Some organization of what is to come will help the reader. From my understanding, Alg3, Inexact Proximal Point Method, strictly improves over all others? If yes, this should be conveyed early on in this section.

Limitations:
The work is limited to a certain notion of reproducility, used only in one prior work, and thus which may or may not be the definition as the area matures. Further, even though the authors identify two sources of instability: initialization and gradient computation, they are studied separately. A unified analysis can perhaps reflect more about the practical aspects.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors introduce a novel algorithmic framework that can achieve near optimal convergence while preserving optimal reproducibility, for minimizing smooth convex objectives and minimax optimization of convex-concave objectives. Here, reproducibility under inexact initialization oracles, inexact deterministic gradient oracles, and inexact stochastic gradient oracles are considered. The key idea is to optimize a regularized strongly convex objective ( or Strongly convex - strongly concave objective for minimax optimization) using a given base algorithm, and bounding the error introduced by the regularization. The authors derive convergence and reproducibility guarantees that match or improve existing guarantees for different base algorithms applied to the proposed framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper discuss about theoretically improving the trade-off between optimal convergence and optimal reproducibility of algorithms, which is an important emerging research area.

* The paper is fairly easy to read, and the methods and results are presented in a clear manner.

* Using the proposed algorithmic framework, the authors show, contrary to what was previously believed, that accelerated gradient descent (AGD) method can achieve near optimal convergence preserving optimal reproducibility, which seems like a non-trivial and interesting result.

Weaknesses:
* The paper covers only convex (convex-concave) objective minimization (minimax optimization), which prevents the results of this paper being applied to many applications where reproducibility is a challenge (e.g. reinforcement learning) as mentioned in the introduction of the paper.

* The paper considers a constrained optimization setting for minimax optimization, while most applications of optimization, such as machine learning, will be deployed in an unconstrained setting. This might again prevent these results being applied to many practical applications.

* The dependence of the convergence bounds on the diameter of $\mathcal{X}$ and $\mathcal{Y}$ in Theorems 4.4. and 4.6 makes the corresponding bounds too loose when $\mathcal{X}$ and $\mathcal{Y}$ are significantly large.

Minor comments

* Using $x^*_{r’}$ to denote  $\underset{x\in\mathcal{X}}{\operatorname{argmax}}\\{ F(x) + (r/2) \Vert x - x_0'  \Vert^2\\}$, it might suggest $x^*_{r’}$ corresponds to the optimum when $r’$ is used as the regularization parameter, which can be a bit confusing.

* Introduction of Assumptions 3.1 and 4.1 seems abrupt, and some discussion on the assumption (e.g. their implications and how these assumptions compare with prior work) seems missing.

Limitations:
This work only considers convex (convex-concave) setting, and some convergence results contain logarithmic factors, which are recognized by the authors in the conclusion. In addition to these, this work considers a constrained minimax optimization setting, which might limit the applicability of the corresponding results in practical applications.

Rating:
7

Confidence:
3

";1
5sV53leJCv;"REVIEW 
Summary:
This paper proposes a new training method for greedy layer-wise or module-wise training of neural networks, which is compelling in constrained and on-device settings where memory is limited and suffers from a stagnation problem. Experimental results show that their method improves the accuracy of module-wise training of various architectures.

-------After Rebuttal-------

I thank the authors for the answers to my comments and questions. Most of my concerns are properly addressed. Therefore I raise my score.

While I still think the word ""early overfitting"" in the paper should be carefully used. I agree with the phenomenon that vanilla module-wise training performs very well in the early layers but stagnates and gets overtaken later. While it can be due to the insufficient mutual information between the learned features and the inputs, instead of the early layer overfitting. Generally, I think the early modules containing fewer learnable parameters are difficult to overfit the large-scale dataset, such as ImageNet.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Overall, the idea is new and the proposed method is theoretically sound.
2. The paper is well-written and easy to follow. 

Weaknesses:
On Line 34, the authors claim that the early modules can overfit and learn more discriminative features than end-to-end training, destroying task-relevant information, and deeper modules don’t improve the test accuracy significantly, or even degrade it. However, the experimental results in this paper are not sufficient enough to support this assumption, especially for the overfitting problem.

As the listed experiments are all conducted on small or medium-scale datasets, the overfitting problem might happen for the early layers. While, for the large-scale dataset, such as ImageNet, the early layers are more likely to be under-fitting due to the insufficient model 
capacity of the first few layers. 

Limitations:
See weaknesses and questions.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper explores the module-wise training of neural networks via the Minimizing Movement Scheme. This approach aims to overcome the stagnation problem often encountered in layer-wise training, leading to improved accuracy and reduced memory usage. The authors compare their results with those of other methods and demonstrate the effectiveness of their approach on various architectures, including ResNets, Transformers, and VGG.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The introduced methodology effectively addresses the frequently observed stagnation issue associated with layer-wise training. As a result, there is a noteworthy enhancement in model accuracy along with a commendable reduction in memory usage.

2. Establishing a connection between neural network training and optimal transport theory is not only intellectually stimulating, but it also presents intriguing theoretical insights.

3. Generally, the manuscript is well-structured and clearly articulated. However, some convoluted notations used in Section 2.2 could potentially be simplified for improved reader comprehension.




Weaknesses:
1. The paper is currently missing a detailed convergence analysis. It's vital to elucidate whether the overall training process will indeed converge in order to reinforce the reliability and robustness of the proposed approach.

2. The study could benefit from a more comprehensive ablation analysis, specifically, an exploration on the impact of variations in the regularization penalty. This would further validate the robustness of the model and provide additional insights into its behavior under different conditions.

Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposed a new regularization for module-wise training via the distance of the input and output of the module.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The proposed regularization is quite straightforward and easy to apply.
2. The paper connects the proposed regularization with optimal transport via theoretical analysis.
3. Some experimental results are good, like adding more modules on STL-10.

Weaknesses:
1. Although the author shows the connection between the proposed regularization and optimal transport, it does not provide insights regarding convergence. Whether the proposed regularization helps the convergence of the multi-module training might be more interesting for larger audiences.
2. Authors argue that 'residual connections are already biased towards small displacements.' However, residual connections are not toward small values for early layers, as shown in [28]. As a result, how does the proposed method control each module's regularization strength? If they are set to the same value, then the regularization may be harmful to early layers. In addition, the selection of the regularization strength becomes more complex when increasing the number of modules K.
3. The improvements over the previous method are only obvious on STL-10, and STL-10 is not a large dataset. In addition, larger datasets and models are omitted. I think memory saving will be more meaningful for larger datasets and models. As a result, the scalability of the proposed method is not examined.

Limitations:
Limitations are provided.

Rating:
6

Confidence:
3

REVIEW 
Summary:
TRGL offers a promising approach to module-wise training that addresses the stagnation problem, improves accuracy, and reduces memory usage in constrained and on-device settings. The method introduces a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. By minimizing the kinetic energy of modules along with the training loss, TRGL encourages modules to change their inputs as little as possible, preserving task-relevant information.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
(1) TRGL significantly reduces memory usage compared to end-to-end training, ranging from 10% to 60% less memory. This makes it particularly suitable for constrained settings and on-device training where memory resources are limited.

(2) The authors provide theoretical analysis, proving that TRGL leads to more regular and stable greedy modules that progressively minimize the loss.

(3) TRGL can be applied to various network architectures, especially those using residual connections such as ResNets and vision transformers.

Weaknesses:
(1) TRGL introduces an additional regularization term to the training objective, which increases the computational complexity of the training process. The calculation of the kinetic energy and its incorporation into the loss function may require additional computational resources, leading to longer training times. How about the actual training time compared to other methods?

(2) what is VanGL? is it just a module-wise training without the regularization terms?

(3) In my opinion, the performance gain is marginal compared  to vanilla VanGL.

(4) The principle of finding Tau is vague.

Limitations:
This increased complexity may result in longer training times and higher computational requirements, which could be a limitation in resource-constrained environments or when training large-scale models.

Rating:
5

Confidence:
3

";1
1B6YKnHYBb;"REVIEW 
Summary:
This paper proposes a method named MolRL-MGPT for drug molecular generation.
Concretely, GPT-based agents are used to iteratively generate candidate compounds, and a special reward signal is adpoted to encourage agents to explore in diverse directions.
The experiments on GuacaMol benchmark show the superiority of the method.


Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
1.	An effective method for De novo Drug Design is proposed.
2.	A special reward signal is proposed to promote the diversity of the agents.


Weaknesses:
It is not appropriate to call the proposed method a “RL-based” method though the score function can be treated as the “reward”. RL follows Markov decision process and aims to maximize the accumulated return, but it seems that the goal in this paper is to maximize the final score. Meanwhile, there is no RL objective function in this paper. (I suggest to change the RL-related statement to another, and this will not affect my rating.)
1.	Eq.3 aims to make each agent get a fix improvement which neglect the difficulty of different generation timesteps.
2.	Eq.4 forcing the different agents obtain different scores, but different score is not equal to the different explore direction.
3.	It seems that the agents are treated as independent units without sharing their experience. Since the different agents cooperate with each other and aim to achieve a common goal, different kinds of correlation need to be considered.


Limitations:
No potential negative societal impact.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper creates a multi-agent reinforcement learning approach to promote diversity in the search space of small molecules during molecular optimization.  Because of the complicated nature of early stage drug discovery research, the diversity is useful during early work in the identification and validation of small molecules, however previous RL methods did not manage to address diversity in a satisfactory way.  The algorithm is rather simple and employs pre-trained GPT agents that speak the language of molecular SMILES. Importantly, the algorithm manages to outperform previous state of the art methods on a number of public benchmarks.  


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The use of multiple agents in this small molecule optimization setting is original and proves to be very useful.  The explicit diversification by equation (4) ends up creating a stronger model than previous attempts that used RL or other deep learning methodologies, and importantly, it outperforms (even if only barely) the graph Genetic Algorithm, a significantly simpler algorithm that puts other methods to shame in several benchmarks.  The paper demonstrates the increase in diversity of the designs with the increasing number of agents.  The paper is written in a clear fashion, and the main result it significant.  The authors provide code.


Weaknesses:
A minor weakness of this method is that the performance of the code is somewhat slow, which is somewhat understandable given the slow individual query of the GPT models and the challenges of the additional RL-related operations.  I agree with the overall attitude of the authors that this particular concern is not very important.  The performance of the graphGA algorithm is nearly identical to that of the new method, despite its extreme simplicity and efficiency.  On the other hand the lack of pre-training is a limitation of graphGA, at least in principle.


Limitations:
This work has no negative societal impacts.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel multi-agent reinforcement learning algorithm with agents parameterized with a pre-trained GPT architecture for de novo drug design.  The authors propose a modified objective function with an intrinsic reward inspired bonus to encourage diversity between agents and also propose to use a constraint to keep the fine-tuned agents close to the pre-trained agents.  The authors evaluate their algorithm on Guacamol, by generating a number of inhibitors for SARS-CoV2 targets.  They also perform ablations on their method with the GSK3$\beta$ and JNK3 maximization tasks.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Some strengths of this paper are:

- An intrinsic reward like term added to the agents' loss which encourages diversity.
- The method performs favorably to the other methods compared against on Guacamol benchmark.
- It performs comprehensive ablations on the GSK3$\beta$ and JNK3 maximization tasks

Weaknesses:
My main complaint about this paper is that I am not convinced of the algorithm's superiority over rivaling methods based upon the experiments section.  I feel that the paper has both missed some necessary baseline methods and that the experiments as they stand are insufficient in demonstrating the paper's main claim that its method leads to improved diversity over other methods.  I also have concerns about missing related work in multi-agent RL in which there is already a body of literature on encouraging diversity among agents, as well as other competing methods which have been applied to molecular drug generation such as diffusion models and GFlowNets.  I also have concerns regarding this paper's reproducibility.  I will go over these concerns one by one.

## Experiments section
1. The main claim of this paper is that its approach leads to superior diversity, but I did not see any experiments _comparing_ the diversity of molecules it generates to molecules generated by other methods.  Indeed, there were experiments looking at the diversity of molecules generated by their method, but it was only their method.  There isn't an indication whether their main claim of improved diversity is true if there is no comparison to other methods.
2. While the generated molecules do look diverse to my non-chemist eye, I would like to see more generated molecules and critically some measurement of diversity of the generated molecules to be convinced.  Also, these results were on only one seed which is insufficient.  At minimum there should be three seeds, ideally quite a bit more (see [here](https://ai.googleblog.com/2021/11/rliable-towards-reliable-evaluation.html)).
3. There are missing baselines in the experiments.  The authors should have compared to an existing LLM molecule generation method such as MolGPT, but this was missing from their experiments.  There are other methods for encouraging diversity for molecule design with RL-inspired machinery such as GFlowNets (https://arxiv.org/abs/2106.04399) that should be compared against.  Also, it would have been nice to see a comparison with one of the recent works on diffusion for molecular design (e.g., https://arxiv.org/pdf/2203.17003.pdf, https://arxiv.org/pdf/2305.01140.pdf), or at minimum a compelling reason for why not to compare to these methods.
4. In the Guacamol experiments it seems experiments were run over one seed.  This is insufficient.
5. A more minor comment: it's hard to understand the significance of the Guacamol task when the tasks are ordered 1-20 without context of what the tasks actually are.

## Missing references
1. I mentioned in the last point, but it would have been nice to see some discussion of other methods such as GFlowNets or diffusion models which also try to encourage diversity in molecular design.
2. There is already a body of literature on encouraging diversity in multi-agent RL, but I did not see references to this literature.  Some representative papers may be https://arxiv.org/abs/2106.02195 and https://openreview.net/forum?id=H-6iczs__Ro.

## Reproducibility
1. All experiments seem to have been run with one seed.  There is no way to know if the results would hold with more seeds.
2. There is no listing of the hyperparameters used or the hyperparameter tuning methods used (or values tried if a grid search).
3. There is no (anonymized) submitted code available to verify or reproduce the authors' claims.

## Clarity issues
1. In the section explaining the loss function, the indexing used is rather confusing.  E.g., the authors use a loss $L_1$ which seems fixed, then also a loss $L_k$ which seems to index the different agents (so what about when $k=1$?).

Limitations:
The main and most important limitation of this work is that I do not know whether the proposed method actually is competitive with rival methods due to some missing experiments, baselines, and insufficient reproducibility.

Rating:
4

Confidence:
3

";1
r8LYNleLf9;"REVIEW 
Summary:
This paper starts from an interesting viewpoint that the synthetic of the previous ZSQ method usually fails to model the similar text feature like real data. As a result, the authors suggest retaining the texture feature. At first, they synthesize calibration images with a LAWS texture feature energy preserve loss. Then, the calibration images are used to provide mean and variance centers for each class, which is then used to guide the generator to synthesize samples. At last, they proposed using a mix-up strategy to further augment the synthetic data.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1, The viewpoint is interesting and persuasive. Also, the results do demonstrate the effectiveness of their method.
2, Sec 5.1 provide many meaningful discussion, which makes this paper more convincing.


Weaknesses:
1, To be honest, the writing of this paper is not good enough and I get lost in many parts. 

For example, Line 171, ‘Q always faces linear decisions when inferencing’ is very strange. I can't understand the meaning behind it.

Line 166, where µ_l (\bar(x)| \bar(y) = k) and σ_l (\bar(x)| \bar(y) = k) should be the calibrating mean and variance center since calibration set C = {( \bar(x), \bar(y))} in Line 160.

2, Why do the authors want to use a calibration center in Step2: Synthetic samples generation? The paper lacks some discussion.

3, The principle behind Calibration set generation and Synthetic samples generation is not clear. Why do you still need Synthetic samples, even though you already have Calibration set. I know such a way could give a better performance. The authors should provide some ablation study when only using Calibration set.

4, The role of Eq6. The K set only selects top-k texture elements that account for more than 80%. However, the images are randomly initialized from Gaussian noise. I am wondering if the distribution of the K set keeps the same for each synthetic image. For example, R5R5 is very fit for Gaussian noise and is selected for each image.


Limitations:
The authors do provide a discussion about the limitations.

Rating:
6

Confidence:
5

REVIEW 
Summary:
TexQ is a novel zero-shot quantization (ZSQ) method that addresses the limitations of conventional synthetic samples in retaining texture feature distributions. It achieves state-of-the-art results in ultra-low bit width quantization, with a significant accuracy increase compared to existing methods on ImageNet.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Strengths are shown below:

1. This paper tried to exploit the important direction for inevitable privacy and security.
2. The performance on the benchmark datasets is promising.
3. The paper is well-organized.

Weaknesses:
1. Maybe the structures used are insufficient with ResNet-18 and MobileNet-V2. What about the most commonly used ResNet-50?
2. Maybe the datasets verified are insufficient with Cifar and ImageNet. What about the others tasks, such as CoCo of detection.

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
This work proposes TexQ, which targets keeping the texture information of the synthetic samples of zero-shot quantization. The texture feature energy distribution calibration method is applied to the synthesized samples, and mixup knowledge distillation is introduced to improve the diversity of the synthetic samples. Extensive experiments of ResNet, MobileNet on CIFAR, and ImageNet datasets prove the effectiveness of this method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The paper is well-organized and easy to follow. Fig.3 is an excellent visualization of the proposed system architecture.

* To measure the texture feature distribution for the input sample of quantization is novel. The comparison (Fig. 4/5) of synthetic samples and natural images is insightful and may help future research on the ZSQ.

* The experiment results are convincing and high accuracy (especially for 3-bit) demonstrates the effectiveness of TexQ.

Weaknesses:
* The introduction of the concept ""LAWS texture feature energy"" needs to be improved. The example given in Eq.5 is not straightforward. Visualization of features extracted using $E_5, S_5, W_5, R_5$ would be better. Why these ""texture"" feature is important to retain should also be included more in the discussion.
* There are too many artifact weighting coefficients $\alpha_i, i \in [1,5]$ to balance different loss function. The author claims in Sec. 4.1 that they ""empirically"" select the value. An ablation study or sensitivity analysis on these parameters would be better to prove that the proposed loss is solid and that better performance does not come from a grid search of the parameters.
* The method is only validated on CNNs. The experiment results on Vision Transformer would be a plus.

Limitations:
The author includes a discussion of the problems to be solved in Section 5.2, which is great. One crucial limitation I would like to raise is that this work is built upon the concept of ""texture"" and is **only meaningful for visual samples**. The application to other tasks with different modality (language, speech, etc.) would be limited.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper points out that there is a strong dependency between the performance of CNN and the texture feature of the dataset. For extending this concept to the quantization field, the paper adopts calibration samples which are trained with manually designed texture filters. In addition to synthetic samples which are generally used in ZSQ works (generated by a network that is trained with Batch Normalization layers’ statistics of a full precision model), the paper exploits calibration samples to quantize a model without the original dataset. Furthermore, the paper applies mixup data augmentation to improve the quantized model’s performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper is well-motivated and easy to follow.

- The paper adopts a concept of texture feature in Zero-shot quantization for the first time.

- The paper demonstrates the proposed method well with several formulas and figures.

Weaknesses:
- It seems costly that generate calibration samples along with synthetic samples for quantizing a neural network.

typo:
- In the 215th row, presnted -> presented

Limitations:
Previous ZSQ works which exploit Batch Normalization layers' statistics for generating synthetic samples are hard to apply transformer-based models.

It is worth analyzing that the proposed method can be applied to those models.

Rating:
6

Confidence:
4

REVIEW 
Summary:
They suggested a zero-shot quantization method to retain the detailed texture feature distribution and introduced the mixup knowledge distillation module to diversify synthetic samples for finetuning

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
They identified the new feature required when generating synthetic data for quantization.

Weaknesses:
They should compare their work with MixMix [1], Genie[2] and KW[3].

The authors only empirically showed their superiority. i.e. lt lacks explanations of intuitive or mathematic. The author should give more reasons.

The image they generated showed a little bit of poor quality to argue that it has captured the texture feature distributions. please see the synthetic images in [1], [2], [3].

[1] Li, Yuhang, et al. ""Mixmix: All you need for data-free compression are feature and data mixing."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[2] Jeon, Yongkweon, Chungman Lee, and Ho-young Kim. ""Genie: Show Me the Data for Quantization."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

[3] Haroush, Matan, et al. ""The knowledge within: Methods for data-free model compression."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.

Limitations:
It lacks a literature survey.

Rating:
5

Confidence:
5

";1
1kZx7JiuA2;"REVIEW 
Summary:
The authors propose Implicit Transfer Operator (ITO) learning, which aims to learn a surrogate of a molecular dynamics (MD) simulation process. Since standard MD simulations integrate Newton's equations of motion numerically, small integration time-steps are necessary, making simulations costly when processes on long time-scales (requiring many integration steps) are of interest. The proposed ITO method learns to simulate the dynamics across multiple time-scales, allowing to study processes on long time-scales more efficiently by using large time steps. It is also shown that ITO models are able to learn surrogate dynamics using partial observations, which is useful e.g. in the context of coarse-graining.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The presented method has strong theoretical foundations and the underlying theory is well-described.

Weaknesses:
While the premise of the paper is interesting, the results and contributions are not particularly impressive:
1. ChiroPaiNN is an extremely simple modification of the existing PaiNN architecture. So simple in fact, that I find it hard to argue this is a separate contribution (the only modification seems to be an added cross product between vector features, followed by a scalar product).
2. The authors write that their models show quantitative agreement with dynamic and stationary observables, which is not backed by the results shown in Table 2. Free energy differences of folding have relative errors in excess of 400% and also absolute errors are extremely large with over 3 kT in some cases. Mean first passage times have similarly large errors. These results cast doubt on the practical usefulness of the proposed method.

Limitations:
Limitations with respect to transferability and scalability of the proposed method and the accuracy of the surrogate dynamics model are mentioned. In some cases it is unclear how the limitations can be reconciled in the future (e.g. the requirement of a closed-form expression for target path probabilities). Potential societal impacts of the work are not discussed, which however, I find acceptable considering the topic of the manuscript. A discussion of societal impacts would probably appear contrived.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper develops a conditional diffusion model to sample unnormalized densities - e.g., the Boltzmann distribution of molecules to replace molecular dynamics simulations. Given a molecule structure, noise is added to it and the diffusion model denoises this distribution to the distribution generated by MD. The diffusion model is not only conditioned on diffusion time, but also on MD time, such that a single model can be used to make larger or smaller MD steps.

The method is not generalizable - training data needs to be collected via the expensive MD that the method aims to replace. The paper is transparent about this. Experiments are performed on a toy system, alanine dipeptide, and 5 fast folding proteins.

The paper has strong contributions but is very unpolished. I think it can be substantially improved in the rebuttal, especially by addressing my first two content concerns in the weaknesses.

I thank the authors for any time they take to answer my questions!

Soundness:
4

Presentation:
1

Contribution:
2

Strengths:
### Strengths

1. The authors tackle an important and impactful task (sampling Boltzmann distributions of molecules).
2. The idea is clever. Since it is hard to sample the Boltzmann distribution of a molecule unconditionally, the authors instead construct a noisy distribution that still shares structure with the bolztmann distribution such that the generative model has an easier job at producing a sample from the less complicated conditional distribution instead of the unconditional one. **************************************************Notably this was already done by TimeWarp************************************************** but the authors bring diffusion models to this task which seem better fit for it (if no reweighting is done).
3. It is an interesting hypothesis that training with multiple MD times improves stability and would perform better than a single MD timestep size. Furthermore, this is justified and illustrated with interesting transfer operator theory. I think there is great value in bringing this theory to the attention of the ML for MD and boltzmann generator community.
4. The model is self consistent. The experiments set up for this are ideal and illustrate the self consistency very well.

Weaknesses:
The first three content concerns are by far the most relevant concerns

### Content concerns (approximately in order of importance)

1. The main claim (as I see it), that training with multiple MD times helps, is not sufficiently investigated:
    1. Why do you only show results for the Muller Potential for this? I do not see why not to evaluate it for all systems or at least for some molecule instead of only the toy system.
    2. Why do you only show VAMP2 score gaps and not kl divergences?
    3. What about 1000 lag in Table 1?
2. What is the comparison in wallclock time for sampling the whole distribution compared to MD? I would expect to see e.g., a plot of how the KL divergence of some ovservable goes down in MD vs. with your method in wall-clock time. Running diffusion models also takes quite some time I imagine, so is this not one of the main things that need to be investigated? Am I missing this information somewhere in the paper?
3. The paper claims to be reproducible, but there is no code provided.
4. Is there related work on ML for transfer operators (e.g., by Anima Anandkumar) that should be explained in the related work? I am not familiar with the field - just a potential pointer.


### Presentation Concerns

I think the presentation has to be improved and currently suffers from missing information, information relegated to the appendix, distraction with irrelevant material, and unnecessarily complicated explanations. The first two concerns are the most relevant.

1. Are we obfuscating the method unnecessarily with math to make it seem more sophisticated? I think it would be tremendously helpful if the transfer operator theory is explained and justifies correctness, but the explanation of what is actually done in the end can be vastly simplified and made much more transparent by just stating that it is now a diffusion model that is conditioned on the MD timestep as well and can therefore make larger or smaller MD timesteps during inference.
2. The “novel” architecture explanation is unnecessary and it is really not needed to claim this as a contribution instead of just focusing on the clearly good contributions and the point of the paper (to me at least) in using multiple MD times and a conditional diffusion model to replace MD. These are two novel **************actual************** contributions. How about just saying you use Painn and there is a little tweak similar to what you can do with e.g. e3nn to deal with chirality that the reader can look up in the appendix? 
3. (would take quite some rewriting I suppose): I think the presentation might benefit a lot from distinguishing between e.g., an “MD SDE” and a “Diffusion SDE”. Then you can distinguish between an MD time and a diffusion time and say that in the text next to your superscript, subscript notation. 
4. Figure 3 does not show any visible difference between fixed and stochastic lag? My recommendation would be to put it in the appendix and use the space for something useful.
5. I would recommend not talking about a special coarse grained approach in the contributions introduction and abstract. I went into the paper thinking you do something interesting with coarse graining.
6. You talk about nested sampling vs. direct sampling. While it is clear to me what is meant, this is not explained anywhere. A simple sentence would suffice.
7. More explicitly point out the dependence of the added noise on the size of the MD timestep size.
8. It should be mentioned in the main text that the number if diffusion ODE steps is the same independent of the size of N.
9. Maybe fix all the typos especially in the related works section for the next/camera-ready version :sweatsmile:
10. The appendix pointer for Algorithm 1 is wrong.

Limitations:
The paper is transparent with the main limitation, that there is currently no capability to generalize. I think the extent to which this is made clear could go further though: point out that the model is only **********************overfitting********************** on a distribution and that there is **********no ""real-world use value"" (e.g., speeding up MD) in its current form.**********

The paper also points out the important difference to e.g. Timewarp that there is no direct way to do reweighting and get exact convergence to the boltzmann distribution in the limit. However, in the discussion it is made to seem like obtaining exact likelihoods from a diffusion model is very much feasible as well and would not come with a large amount of difficulty.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents implicit transfer operator (ITO) learning for the simulation in molecular dynamics. Their approach adopts the SE3 equivariant MPNN architecture (ChiroPaiNN) to parameterize the transition kernels in the denoising diffusion probabilistic model. The method displays a decent performance on several all-atom molecular simulation tasks with only coarse-grained representations.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The problem is very relevant and the motivation is clear. Most notably, the method takes SE3-equivariance into consideration, which I think is a very good practice to combine state-of-the-art generative models with the simulation of molecular dynamics.
- The paper presents a good balance between the introduction, methodology, and experiments. It is an interesting paper to read.
- Illustrative examples are shown to demonstrate the effectiveness of their approach with detailed implementation details.

Weaknesses:
- The theory part concerning transfer operators is chaotic. Eq. (3) has a typo in it ($\rho$ should be $f$). And in Appendix A.2, I don't think Eq. (17) should be correct if the transfer operator is defined as in (16). It seems they mix up the definition in Eq. (16) with that defined in Eq. (5) of [1]. 
- The experiments seem sound but it is very hard for me to evaluate the effectiveness of their approach since there is a lack of comparisons. As listed in the related works section by the authors, the idea of transfer operator surrogates has already been commonly used in molecular modeling as well as deep generative Markov state models. I believe there would be state-of-the-art methods other than all-atom MD simulations, to which they are comparing.
- As I mentioned before, the use of an SE3 equivariant message-passing neural network seems one of the main contributions of this work. Following the last point, I would suggest at least one experiment that could explicitly demonstrate how and why this architecture is beneficial to the diffusion model. Will the results be much worsened if we remove this SE3-equivariance from the architecture, given a similar size of the NN?

[1] Prinz, Jan-Hendrik, et al. ""Markov models of molecular kinetics: Generation and validation."" The Journal of chemical physics 134.17 (2011).

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
The proposed approach aims to learn molecular dynamics via an implicit transfer operator framework that can perform modeling at multiple time scales. The framework is using diffusion model with SE3 equivariant architecture. The approach has shown capability in stable and self-consistent modeling at multiple time and space resolutions for molecular dynamic modeling. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- a novel framework for multi-scale simulation for molecular dynamics, combined with the SE3 network for stable, long-term results. 
- demonstrate good experimental results on multiple MD datasets and achieve an order of magnitude speedup
- work on an important task of molecular dynamic modeling 
- use diffusion network for exact likelihood evaluation 
- honest and clear notes on limitations - appreciated 

Weaknesses:
- in terms of presentation, it'd be nice to make each figure self-contained in terms of points to make and the meaning of the notations.
- 

Limitations:
- this paper may be intense for people noting working on MD. it'd be nice to include more descriptions in captions. 
- I am curious how it can be generalized to other dynamics like some other physics system that is also multiscale. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
In this work, the authors proposed a framework that combines SE(3)-equivariant MPNN and conditional DDPM, called Implicit Transfer Operator (ITO) learning, as a method to efficiently sample observales from MD simulation trajectories. Such a framework is validated on Muller-Brown potential data generated by the authors, as well as the commonly used alanine dipeptide and fast-folding protein dataset generated using MD simulation. ITO learning framework is shown to bear the potential of accelerating or surrogate MD simulation.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. It is a nice effort to formulate the MD trajectory using the ITO such that the sampling can be learned by the conditional DDPM.

2. Proposed a SE(3) version of the PaiNN.

Weaknesses:
#### Major
My major concerns are related to the experiment and results sections of the manuscript.
1. In Figure 4, the $\phi$-$\psi$ plot or Ramachandran plot is often colored by the free energy of the system (e.g. [Köhler et al. 2023](https://pubs.acs.org/doi/epdf/10.1021/acs.jctc.3c00016), [Marloes et al. 2023](https://arxiv.org/abs/2302.00600) and many more), which is directly related to the probability of the state given Boltzmann distribution. From the current Figure 4, the physical property of the system, energy, is not directly visualized. 

2. Another issue with Figure 4 is the large bin size of the 2D histogram. The current binsize of 2D histogram is about 0.4 rad, which makes it really hard for readers to understnad the performance of the SE3-ITO model. Moreover, the MD simulation data are represented in the form of 2D histogram while the model sampled data are in the form of contours. I would highly recommend that the authors to show the MD data in separate subfigures with the same form of the model sampled data. If 2D histogram is to be used, a much smaller bin size should be used for clarity.

3. Although the effort of proposing a SE(3)-equivariant version of PaiNN (ChiroPaiNN) should be recognized, the necessity of CPaiNN is not well estabilished in the manuscript. As the author mentioned in the manuscript, there is no parity change during MD simulation. I am wondering if there will be significant difference in accuracy if the original PaiNN is used in the ITO learning framework. Assuming an SE(3)-equivariant model is absolutely necessary, the authors have not shown any comparison between CPaiNN and other established SE(3)-equivariant model such as the [SE(3)-Transformer](https://arxiv.org/abs/2006.10503). Such a benchmark can definitely help to improve the manuscript.

4. For the fast-folding protein experiments, CG-SE3-ITO model is compared with MD data. In Table 2, the error of the model is significantly higher for proteins with more residues or $C_{\alpha}$ atoms. Yet, the discussion about the high error is limited in the manuscript. The purpose of coarse graining is to achieve relatively high accuracy with large system. Underwhelming accuracy on large system impairs the applicability of the SE3-ITO model on coarse graining.


#### Minor
5. Line 65, the probability is written as $p_{\tau} (x_{t+\tau} | x_{t})$, which is inconsistent with the notation in the dynamics observables equation ($p_{\tau} (x_{t+\Delta t} | x_{t})$) in line 64.

Limitations:
Yes, limitations are addressed.

Rating:
5

Confidence:
4

";1
w79RtqIyoM;"REVIEW 
Summary:
This paper proposes an approach of Compositional Sculpting for iterative generative models, including GFlowNets and defusion models.
The model uses classifier guidance to sample from the target posterior distribution composed of pre-trained base models.
The paper also proposes a training algorithm for the classifier.
The approach is validated by empirical analyses of an image dataset and molecular generation.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The method is general enough for different GFlowNets and defusion models.

- The empirical results validate the method.

Weaknesses:
(1) It would be convincing to have more complicated experiments, especially for image data.
Colored MNIST might be too small and simple.

(2) More clarity might be helpful for the following points.

(2A) What is the model in line 193? ""Under the model we have introduced the variables y_1, ... y_n are dependent given a state s in S, but, are independent given a terminal state x in X.""

(2B) I am confused while reading line 209. The sampling scheme is 1) sample y from its prior 2) sample tau given y. But the following sentence says sampling y given tau.

Limitations:
The paper has a limitation section.
It seems not to have a dedicated section for social impact.

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper describes a way in which, given sequential samplers from multiple probability distributions, a combination of the samplers can be used to sample from a composition of the distributions. To be precise, the sequential samplers are either GFlowNets or diffusion models, the combination of samplers is a weighted combination of action distributions at each intermediate sampling step, and the composition of distributions can be defined by simple soft conjunction and set difference operators. This is demonstrated in toy illustratory experiments and multiobjective molecule synthesis (GFlowNet) and MNIST with digit class and colour attributes (diffusion).

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
From the perspective of someone who works on both GFlowNets and diffusion models, this is a very well-written paper. 
- The text reads naturally and the right amount of detail is given in the main text. There is a good choice of illustrations to help the reader.
- The composition of multiple GFlowNets has not been considered before and could be useful, especially in multiobjective problems. The unifying perspective on classifier guidance is also an advantage (but see below).
- Code is provided, a nice addition to the paper.
- I checked the GFlowNet-related math and believe it to be sound.

Weaknesses:
- Line 234, 255, 634, 643, maybe others: typo ""GFLowNet""
- It would be good to explain why / state as a subclaim that (8) is a policy (i.e., sums to 1 over $s'$), which is not actually obvious from the definition. 
  - It relies on the fact that $p(y|s) = \sum_{s'}p(y|s')p(s'|s)$, which follows from conditional independence of $y$ (a function of the final state) and $s$ given $s'$. That is a consequence of the Markov property in GFlowNets. A note should be made about this.
  - The equality may not actually hold in practice, when $p(y|-)$ is a trained classifier, so (8) may not exactly sum to 1. What do you do in this case (in the experiments)?
- The results on molecule generation raise a few questions:
  - The reward exponent was set to $\beta=32$ or $\beta=96$, which is far larger than in past work, where it was at most 16. Why was such a choice made? This is suspicious, since convergence and mode collapse issues worsen at low temperatures.
  - Related, with such high exponents, one wonders about mode coverage in the learned distributions. Have you considered the in-sample diversity of the generated molecules (e.g., as measured by average Tanimoto similarity or  diverse top-k metrics)?
- On related work:
  - There is no substantial discussion of related work in the main text, even though there is a large body of work on compositional generation and classifier guidance with diffusion models (e.g., the many papers cited in the second paragraph of the introduction).
  - In the Appendix, the connection with [23] is discussed. The proposed method can easily turn a collection of classifiers for different objectives into a classifier for any convex combination of the objectives. It would be interesting to empirically compare this with conditioning of the model on the linear scalarization weights used in [23].
  - The paper would be stronger if more explicit unifying connections were made between guidance in diffusion models and in GFlowNets. Note that diffusion models in a fixed time discretization are actually GFlowNets of a certain structure (cf. ""A theory of continuous generative flow networks"" [arXiv:2301.12594, ICML 2023] and ""Unifying generative models with GFlowNets and beyond"" [arXiv:2209.02606]). Classifier guidance using the gradient of $p_t(y|x_t)$ should be the continuous-time limit of equation (8).

Limitations:
Yes

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper studies the problem of composing independently trained generative processes of diffusion-based generative models and GFlowNets. The paper considers a setting where one has access to $m$ pre-trained samplers for $\{p_i(x)\}_{i=1}^m$, and the goal is to obtain a sampler which corresponds to a composition of these processes. Specifically, the authors consider two ways of composing the processes, namely harmonic mean: where the likelihood of the composition is high only where the component processes have high likelihood and contrast The authors frame this as sampling from a conditional distribution $p(x|y)$, where y is an observation which denotes the index of the process a sample is generated from. This results in a procedure analogous to classifier guidance which is popular in the diffusion literature. The authors show how the classifier guidance results in sampling from the desired composition. A critical component of this procedure is learning the classifier $p(y|s)$, for which the authors propose a MLE based procedure using trajectories from the base model. The authors first validate their approach on some synthetic tasks on a 2D grid, followed by experiments on small molecule generation with GFlowNets and colored MNIST with diffusion. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The paper studies an interesting question - which is relevant to the community. In particular, methods to leverage pre-trained models for various downstream tasks are becoming increasingly important with the growing adoption of pre-trained models for various domains. 
* Since similar classifier guidance approaches have been studied extensively in the literature on diffusion models, the novelty is relatively limited. Nonetheless, there are several technical aspects of the approach such as the classifier training scheme that are novel (to the best of my knowledge). 
* The proposed method is relatively simple conceptually, and in terms of implementation. 
* The experiments are well designed, and the results are quite promising, albeit with some caveats I mention below. 
* The paper overall is quite well written and easy to follow. I also appreciate the authors including the code with the submission. 

Weaknesses:
* As the authors discuss in Section 3, their theoretical analysis is analogous to classifier guidance in diffusion models. On the other hand, [1] establishes equivalence between GFlowNets and diffusion models. As a results, it seems to me that the insights provided by the theoretical analysis aren’t particularly novel even though the path to achieving them was different (which could be seen as useful on it’s own)
* The experiments on diffusion are limited to a simple coloured MNIST task, with no baselines from the classifier-guided diffusion literature. 
* The central aspect of the approach is learning the classifiers, however there is no analysis on the classifiers - e.g. how accurate are the classifiers? what is the effect of the classifier performance on the results of the composition? How does the training of the classifier compare to simply training a GFlowNet from scratch in terms of runtime? 
* (Minor) A simple baseline that is missing in the experiments is training a GFlowNet with the appropriate composition from scratch. 

[1] Unifying Generative Models with GFlowNets and Beyond. Zhang et al. 2022. arXiv:2209.02606

Limitations:
The authors already discuss some limitations of the approach - effect of the classifier as well as the quality of the underlying models. I would also add limited evaluation and lack of comparison to existing approaches. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes a method to compose multiple iterative generative models, i.e., either multiple GFlowNets or multiple diffusion models. The idea starts out with a mixture model over the generative models. Then, one can construct a categorical distribution over the generative models that tells us which model a sample originated from. By adapting classifier guidance to GFlowNets, the proposed method can compose multiple models in a way that allows both emphasizing or de-emphasizing specific models by treating the different generative models as different classes. On a diverse set of (toyish) experiments the method is shown to be effective for both GFlowNets and diffusion models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method is very interesting. In particular, the part where the question ""which model was this sampled from"" is treated as a classification task for the purpose of compositional generation.

The paper addresses a very important problem with high impact. 

The presentation is easy to follow and the text is well-written.

 

Weaknesses:
The experiments clearly demonstrate the effectiveness and versatility of the proposed method. Though, the experiments are limited to toyish settings and I believe more complicated settings would greatly enhance the impact of this work.

Limitations:
The main limitation is inherited from classifier guidance: the need to train a classifier on intermediate states. This is already mentioned in the paper.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The current paper focuses on the challenge of composition generation from pretrained generative models, with a specific focus on GFlowNets and Diffusion models. In comparison to prior literature, two novel compositionality operations are introduced for generating samples that are simultaneously likely according to two generative models or likely per a subset and unlikely per the remaining models. This is a strict generalization of operations introduced in prior work on composition of energy-based models. Practically, the operations are instantiated via a framework motivated by classifier guidance in diffusion models. Experiments are conducted on a molecule generation application and a colored MNIST problem.

Rebuttals acknowledgment: I had a good view of the paper before rebuttals and the authors' response to my questions was fair. I continue to keep my score accordingly.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
I really like this work! The contributions are simple and straightforward, but very interesting. The formalization, generalized operations, and relation to classifier guidance were exciting to read through. The authors also appropriately acknowledge the limitations of their work, specifically the need for sufficiently strong component models.

Weaknesses:
- My biggest apprehension is limited experimental investigation, which would raise the quality of this paper quite strongly in my opinion. I do not hold this to be a strong weakness though.

Limitations:
- A clear note of what is meant by compositionality would help in this work, since the term is extremely overloaded. The experiments currently reported focus on what would be called systematicity or systematic generalization in my opinion (see Hupkes et al., ""Compositionality decomposed""), but other valuable forms of compositionality, e.g., productive generalization, will arguably require some ""chaining"" operator that allows prior generated states to be fed into the model for generating the next state. Since the work focuses on GFlowNets and diffusion models, where a notion of sequential generation of intermediate states is present, arguably authors can use their defined operations to perform productive generalization as well? I would appreciate if the scope of this paper is clearly discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a method to combine sequential generative models, in this case GFlowNets, so as to create new distributions from base models. This is done by training classifiers that are then used to guide sampling. The method is tested on a simple grid and a molecular domain (emulating the problem of the paper that introduced GFlowNet), as well as on MNIST with diffusion.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is moderately easy to read, although some of its results were not immediately clear to me so I spent quite some time doodling on paper to convince myself that the propositions were reasonable.

What the paper proposes and seems to be able to achieve empirically is very interesting. Combining generative models in the ways shown here could be an amazing multiplier of large pretrained models.

Weaknesses:
Generally the paper is not making a good job of convincing the readers that the proposed method should work at the theory level, and that the effort of combining distributions by training a classifier is worth it (compared to retraining a generative model).

Limitations:
The authors have addressed some of the limitations of their work.

Rating:
6

Confidence:
4

";1
8YN62t19AW;"REVIEW 
Summary:
The paper proposes a unified discretization framework for the differential equation (DE) approach to convex optimization. The DE approach relates optimization methods to continuous DEs with Lyapunov functionals, providing intuitive insights and convergence rate estimates. However, there has been a lack of a general and consistent way to transition back to discrete optimization methods. The paper introduces the concept of ""weak discrete gradient"" (wDG), which consolidates the conditions required for discrete versions of gradients in the DE approach arguments. The authors define abstract optimization methods using wDG and demonstrate that many existing optimization methods and their convergence rates can be derived as special cases of their setting.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
Comprehensive Approach: The paper addresses an important gap in the DE approach to convex optimization by providing a unified discretization framework. It consolidates the conditions required for discrete versions of gradients, simplifying the overall analysis.

Clarity: The pedagogical approach of the paper is appreciated. Most of the theory is constructive and all new aspect are clearly presented.

Abstract Optimization Methods: The introduction of abstract optimization methods using wDG allows for a simpler view and analysis of existing optimization methods. It provides a systematic framework for deriving convergence rate estimates.

Potential for new methods: The framework allows for the development of new optimization methods by combining known DEs with wDG. It opens up opportunities for researchers to explore novel approaches in the optimization field.

Weaknesses:
Knowing the accelerated gradient flow: The biggest problem of the approach presented in this paper is the requirement that one needs to know the accelerated gradient flow to derive its discretization - accelerated gradient flow that has been derived from an accelerated algorithm. Hence, the practicability of the approach is limited as one requires to know already an accelerated method before deriving another one.

Case-Specific Discrete Arguments: The authors acknowledge the need for adjustment and optimization in the construction of optimization methods: while the framework consolidates the case-specific discrete arguments in the definition of wDG, there may still be some complexity involved in finding new pratical wDGs for different methods.

Limitations:
- Non-Universal Applicability: The paper acknowledges that certain optimization methods, such as the Douglas-Rachford splitting method and methods based on Runge-Kutta numerical methods, may not fit into the current framework. The applicability of wDG to these methods remains an open question.

- Practicability: While the paper indeed overcome some limitation from previous work in this vein, given the previous points in the Weakness section, it seems the approach overcomplicate the design of new accelerated methods in other settings.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper focuses on the design of convex optimization schemes based on a general discretization framework applied to differential equations (DE). The authors heavily rely on Lyapunov based inequalities to provide convergence rates. 
The authors build a systematic framework on top of the one proposed by Su, Boyd and Candès, allowing to provide an automatized analysis, thanks to the concept of weak discrete gradient (wDG). This weaker notion of DG allows one to overcome the assumptions needed for discrete versions of gradients in the DE approach. The resulting convergence rates turn out to be competitive, even though they might not be optimal.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The analysis of a large class of convex optimization algorithms can now be performed in an automatized fashion. Many existing optimization methods together with their rate estimates, can be retrieved as special cases of the authors' method. This offers a simpler perspective.

The presentation of the paper is great, the authors make an effort to avoid loosing the reader into too many technicalities. Not being an expert in such convex optimization schemes, it was still enjoyable to follow most of the details. The paper is well-written.


Weaknesses:
The estimated rates are not always optimal as demonstrated by the authors (some sub-cases of Theorem 5.5 for strongly convex functions), which indicates that there are still specific efforts to be pursued for certain algorithms.


Limitations:
There is a clear section dedicated to limitations with an explicit list:

- some methods do not fall into the authors' framework, e.g., Douglas–Rachford splitting method 
- adjustment (for instance of time-stepping) could be improved for wDG
- the obtained rates are not always optimal

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers unconstrained convex optimization problems, studied from the angle of their close relation with differential equations. Specifically, the authors propose a framework for translating results from continuous time methods to their discrete time counterparts. They propose Discrete Gradients (a technical tool from Numerical Analysis) as a means for achieving this, and suitably adapt the concept to the convex optimization scenario (via weak Discrete Gradients). As such, the complexity of showing convergence for DE discretizations is mostly transferred to finding the appropriate wDG. The authors then show how this framework can recover known convex optimization results for some important methods in the class.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper follows in a line of work studying discrete gradient methods as optimization schemes. While the central concept of Discrete Gradients is not novel, its weak variant used in this paper and the unifying framework built around it are, to the best of my knowledge. The studied topic is of considerable interest to the optimization community, and the tools introduced in this work are welcome additions. The paper is written in a crisp and clear style (making for a very pleasant read), and the authors critically discuss their results in (mostly) adequate detail. 

Weaknesses:
- Reference [1] seems to have quite some overlap in terms of results and considered classes of functions. In this light, a more detailed comparison is warranted in terms of e.g., assumptions -- especially on stepsize values, type of results, classes of functions, and techniques.
- The usage of the ""free"" iterate z (introduced in Def. 4.1) when devising the discrete accelerated schemes is opaque and would benefit from further discussion. 

* Minor (typos and the like):
	- Line 315: of in 
	- Line 315: the Hessian
	- you use (i), (ii) in Thm 4.2, but on page 4 line 131 you use the same notation for other concepts, which allows for confusion


[1] Ehrhardt, M. J., Riis, E. S., Ringholm, T., and Schönlieb, C.-B. (2018). A geometric integration approach to smooth optimisation: Foundations of the discrete gradient method. arXiv preprint, arXiv:1805.06444.

Limitations:
The authors address the limitations in a separate and very clearly written section and go into adequate detail describing them. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a family of oracles called wDG (weak discrete gradient) verifying (8).
This family constraint (8) has been created in order to make proof works in the discrete setting and has been inspired by the observation of what happens in the continuous setting.
As expected, authors propose results obtained running classical algorithms replacing the gradient by any of those oracles. This new framework allows including many popular algorithms using different oracles such as proximal operator.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is concise and clear. The problem is motivated, and well explained.
The new framework is fairly large and authors detailed examples of classical algorithms that are included in their framework.

Weaknesses:
- It might be because I did not know this line of work, but based on the title, I thought of a completely different result, on how, using gradients on the current iterate, efficiently discretize an ODE. Finally, the proposed framework seems to be more about the nature of the oracle itself, not the algorithm, and some of them (prox, average gradients, ...) are not always accessible.

- Thm 2.3: From this thm, it seems that the convergence rate can be arbitrarily good. Authors should discuss this here and mention that discretization error unables to converge faster than the proven lower bound $1/t^2$.

$\underline{\text{Typos:}}$

- l.23: « strong convex » -> strongly
- l.164: « in of »
- eq 10: I think \beta and \gamma have been permuted
- Thm 5.2: « Let $\bar{\nabla} f$ be a wDG of $f$ […] let f be […] » -> « Let f be … let $\bar{\nabla} f$ be …. »

Limitations:
NA

Rating:
6

Confidence:
3

";1
thPI8hrA4V;"REVIEW 
Summary:
This work proposes GlyphControl for visual text generation by augmenting textual prompt with additional glyph conditional information. A benchmark of LAION-OCR is built for evaluating this model.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
++The task of visual text generation is interesting.

++Good results are shown in experiments.

++A new visual text generation benchmark is introduced.


Weaknesses:
--The main concern is the limited technical contribution. The whole architecture of GlyphControl is a simple extension of ControlNet by using additional control of glyph images. But the idea of glyph images has been validated in GlyphDraw [13] for the same task.  

--I am curious by the claim of “outperforms DeepFloyd IF and Stable Diffusion in terms of 55 OCR accuracy and CLIP score while saving the number of parameters by more than 3x”. As shown in Table 1, the parameter number of GlyphControl is significantly larger than Stable Diffusion v2.0.  

--Why not train GlyphControl on the common SD 2.1, which has the similar number of parameters and clearly outperforms SD 2.0.  

--More competitive baselines should be included for comparison (GlyphDraw [13], SD XL and Midjourney).  

--As in GlyphDraw [13], it is necessary to report the FID values in performance comparison.


Limitations:
None

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes an approach to generating images with visual text. The main approach consists of a generalized version of ControlNet with rendered text as control guidance. To facilitate the training and evaluation, a benchmark dataset called LAION-OCR is proposed. Experiments show that the proposed approach outperforms competitors such as DeepFloyd IF and SD. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Generating visual text accurately has been a challenging issue in the field. This work provides a simple yet effective approach to this challenge by elegantly extending ControlNet with rendered text as control. I am convinced that having rendered text as input makes a lot of sense in this context and would greatly help generate accurate text.
- In addition, having rendered text also allows control over the positioning, font size, which would be of great help in practice.
- Experiments show that the proposed approach significantly outperforms competitors while showing compelling visual results.
- Overall, I believe this work is a nice addition to the current research landscape of text-to-image generation.

Weaknesses:
It would help to add some failure cases analysis in order to better understand when models may fail.

Limitations:
Limitations not well discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper addresses the development of diffusion-based text-to-image generative models for generating coherent visual text. They propose GlyphControl, which augments textual prompts with glyph conditional information to encode shape details and improve accuracy. They introduce the LAION-OCR benchmark dataset and evaluate GlyphControl's effectiveness using OCR-based metrics and CLIP scores, demonstrating its superior performance over the DeepFloyd IF approach in empirical evaluations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The strengths of this paper lie in the proposal of GlyphControl, a glyph-conditional text-to-image generation model. Additionally, the introduction of the LAION-OCR benchmark dataset and the ability to customize and control the content, locations, and sizes of generated visual text demonstrate the paper's practical contributions to the field of visual text generation.

Weaknesses:

When it comes to text generation, information such as font size and style is crucial, and it would be beneficial for the authors to conduct experimental analysis on this aspect.

The authors' decision to remove images with more than 5 bounding boxes lacks clarity. Considering rich-text images as a valuable corpus and only focusing on a limited number of OCR images may limit the model's ability to generate rich-text images.

While the authors utilize BLIP-2 captions, which may not consistently describe both the image content and OCR text information, it would be interesting to see how the authors address the challenge of generating captions that consider both aspects.

Although this method generates accurate text, it raises curiosity about its effectiveness when dealing with a large amount of text or small font sizes, such as generating a paragraph rather than a simple phrase or word.

In addition to text generation, is it possible to modify the text within an image based on given text information, such as changing the color and position of specific words?

When dealing with a large amount of text, such as 500 words, ensuring text generation quality becomes crucial, as it requires high-resolution images. Additionally, ensuring efficiency in generating such images is also an important consideration.

Many texts may not need to be generated as they can be obtained through text rendering. Have the authors attempted this approach, based on text rendering?

Limitations:
The main concern of this paper revolves around the quantity of text. Starting from the database, the authors control the number of OCR boxes, which, to some extent, limits the generation of rich-text images.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper add glyphcontrol to diffusion model by adding controlnet that takes rendered whiteboard images as inputs. Texts in the whiteboard images are extracted by OCR engine during training and then rendered by glyph renderer. During inference, glyph renderer renders the whiteboard images based on the instructions and feed the images using glyph controlnet. They also provide LAION-OCR benchmark by filtering LAION dataset using OCR systems.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The incorporation of whiteboard images generated by the glyph renderer into diffusion models for glyph generation, using controlnet, is a reasonable and easy to understand idea. The paper effectively demonstrates the successful synthesis of images with clean glyphs, surpassing the performance of the baselines, and the glyphs can be controlled easily. They also provide LAION-OCR dataset that is useful for OCR-related research.

Weaknesses:
The proposed method requires additional training. The method may be seen as an extension of ControlNet and may appear to involve the combination of multiple modules, which could be seen as engineering work.

Limitations:
The paper does not explicitly describe their limitations. It would be nice the author provides limitations that can be solved for future works.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, a glyph-conditional text-to-image generation model named GlyphControl is proposed for visual text generation. In addition, the authors introduce a visual text generation benchmark named LAION-OCR by filtering the LAION-2B-en. The results show that method of this paper outperforms DeepFloyd IF and Stable Diffusion in terms of OCR accuracy and CLIP score.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1.	The visualization results shown in this paper are impressing. The images presented in the paper show that the text of the generated text region is accurate at both the character level and the word level.
2.	The introduction of LAION-OCR dataset is a modest contribution to the realm of NLP and text image generation.


Weaknesses:
1.	The contribution is limited. The whole model is the same as ControlNet. The LAION-OCR dataset is just filtered out from an open-source dataset LAION-2B-en, which is not collected by the authors.
2.	The results of the quantitative experiment indicate that the accuracy achieved using this method is merely 40%/26%, which to some extent suggests that the visualized outcomes have been carefully selected. More results need to be displayed.
3.	The comparison is unfair. The compared methods do not prioritize text generation.


Limitations:
The manageable elements of users are still constrained. Users are unable to select the color or the font.

Rating:
4

Confidence:
5

";1
SAzaC8f3cM;"REVIEW 
Summary:
The paper tackles the task of graph anomaly detection. It proposes a multi-view subgraph information bottleneck framework that is used to introduce SIGNET, a self-interpretable graph anomaly detection method. The method leverages the dual hypergraph transformation to obtain two views of the same graph, which are subsequently used for a mutual information maximization objective. The resulting model can then be employed to detect anomalous samples in the dataset and provides an implicit explainability mechanism that can highlight salient regions of the graph.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is well-written, and the ideas presented are exposed in an easy-to-follow manner.

- The authors present compelling arguments for their design choices, an ablation study, and the derivation of the loss function in the Supplementary materials.

- The Supplementary materials contain the code for the proposed approach and various details about the experimental setup, such as the hyperparameter pool used during hyperparameter selection.

- The quantitative results are compelling, and the proposed method obtains the best results in most cases. Adequate baselines were selected for the experiments, containing both neural approaches and more classical AD methods, such as the OC-SVM with a WL kernel.

Weaknesses:
- The paper focuses on presenting an explainability/interpretability mechanism instead of an anomaly detection pipeline with an explainability mechanism built in. The usage of the dual hypergraph view in the context of training an anomaly detector is novel in its own right. The main results for anomaly detection in Table 2 are also good. An explainability mechanism is a very important addition, but I would prefer it not to be the paper's primary focus. The strong emphasis on explainability made me think that the authors were not confident in the anomaly detection model, which in my eyes, should be pitched as the main strength of the paper.

- The qualitative analysis of the explanations is somewhat lacking. A more detailed caption for Fig. 3, which also explains what the highlighted nodes and edges represent, would improve readability. It looks to me that the model highlights the motifs in both the normal and anomaly scenarios, but I would have expected it to highlight the motif in the anomalous graphs more. I would also like more qualitative results, especially on real-world datasets. Figure 3 is overall somewhat confusing and makes me feel like the examples might be cherry-picked.

- As far as I'm aware, the NX-AUC and EX-AUC metrics are not very common. Please consider expanding their meaning in the main paper or the Supplementary materials.

- I don't particularly like the ""cartoon""-style fonts used in Fig. 1 and Fig. 2., please consider some alternatives. However, this is a subjective stylistic opinion and won't affect my final score for the paper. 

- I have not seen any mention of hyperparameter selection for the baselines. Some classical methods (such as the OC-SVM) are particularly sensitive to hyperparameters. Please consider adding a discussion regarding this.

- It would be insightful if the paper would further discuss potential limitations and negative results.

Limitations:
- The authors have addressed their limitations regarding the inability to use ground-truth labels when training their model, making it entirely unsupervised. Expanding the discussion of the limitations, potentially with negative qualitative results, would be beneficial. Discussing FP/FN samples obtained by picking some anomaly threshold would also be interesting since the model could provide explanations.


Rating:
6

Confidence:
5

REVIEW 
Summary:
Inspired by the multi-view information bottleneck and dual hypergraph transformation, this paper proposed a self-explainable anomalous graph detection method called SIGNET. The method has three key designs in the model training stage: 1) Dual hypergraph-based view construction, 2) Bottleneck subgraph extraction, and 3) Cross-view maximization. Anomalous graph detection and graph rationale extraction will be achieved through maximizing the mutual information between the representations of graph rationale extracted from the original graph view and hyper-graph view. During inference, the anomaly score of a test graph is determined by the negative mutual information between the representations of its two graph rationales from different views.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The problem studied in this paper is important and interesting. It is essential to understand the subgraphs make a graph anomalous. 
2. This paper provides a clear summary of the challenges of self-interpretable GLAD and why existing explainers for GNNs cannot be applied to graph-level anomaly detection.
3. This paper is well-structured. 


Weaknesses:
1. Despite having a clear framework, it is still hard to get the motivations behind employing multi-view information bottleneck (MIB) and dual hypergraph transformation (DHT) to solve self-interpretable graph-level anomaly detection.  Much like GAST [1], authors draw inspiration from information theory to extract subgraphs as the explanation of model prediction, but they fail to adequately discuss the underlying intuition or theoretical basis for how MIB and DHT specifically address graph-level anomaly detection. 
2. The dataset used in experiments is relatively small-scale, and time complexity analysis is lacking. If there are difficulties in conducting experiments on large-scale graph data, time complexity analysis is necessary.
3. More related work about anomaly detection and explainers of anomaly detection is expected. 

[1] Miao et al. Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism, ICML 2022.



Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
To overcome the disadvantage of existing anomaly detection methods that fail to provide meaningful explanations for the predictions, this paper proposes SIGNET to (1) detects anomalous graphs and (2) generate informative explanations. To achieve this, the paper devises a multi-view subgraph information bottleneck framework to extract the informative subgraphs as explanations. Empirical results on 16 datasets verify the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The investigated problem is novel and important, as robustness and interpretability are the key sub-areas of trustworthy graph learning.
- The paper is easy to follow, with generally clear writing and illustration.
- The proposed SIGNET is technically solid, with good and competitive empirical results.
- Some preliminary analyses in terms of information theory are conducted.

Weaknesses:
- The technical contributions are neutral. The proposed MSIB framework seems to be a combination of information bottleneck and graph multi-view learning, while the novelty and difficulty are not clear.
- The answers to ""RQ1: Can SIGNET provide informative explanations for the detection results"" are not convincing enough, which should be the key contribution of the paper. The reasons are as follows.
- The two compared GNN explainers, GNNExplainer and PGExplainer, are not up-to-date. The latest and state-of-the-art explainers, e.g., GSAT (ICML'22) [22], should also be considered and discussed. 
- Note that GSAT is also derived from the information bottleneck, which shares a similar design as the proposed SIGNET. I would suggest the paper discuss more the connection and differences.
- Besides, directly combining GNN explainers and anomaly detection methods, e.g., OCGIN-GE, can be sub-optimal, as shown in Table 1. The paper should explain more about the baseline settings, that is, why such a combination is reasonable, but the results show that it does not work well in most cases.
- The few cases shown in Figure 3 are insufficient, which are not promising and convincing enough. It seems that SIGNET learns to capture the same (similar) functional sub-graph (or motif) for both normal and anomaly samples. I would suggest the paper show more cases and provide an in-depth analysis, which will add more value to the main contribution, i.e., interpretability.

Limitations:
Please refer to the above Weaknesses and Questions. 

I would consider raising my score if the above questions are well answered.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies graph-level anomaly detection (GLAD), which aims to find the anomalous graphs. To construct an explainable GLAD under an unsupervised manner, the authors first proposed a multi-view subgraph information bottleneck (MSIB) framework and then introduce a dual hypergraph as a supplemental view of the original graph. The core contribution of the paper is the designing of a self-explainable GLAD model. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. Explainable AI is important for many real-world applications that highlight interpretability and security. The proposed framework is explainable by the model itself (rather than post-hoc explainers). 

1. Using two different and distinguishable views to train the MSIB framework for graph-level anomaly detection is reasonable and sound. The authors also used cross-view MI maximization for estimating MI between two compact subgraph representations. 

Weaknesses:
1. The authors stated ""this is the first attempt to study the explainability problem for anomaly detection on graph-structured data"". However, there are works on node-level graph anomaly detection, both supervised and unsupervised and self-supervised. Better use ""graph-level anomaly detection"" here. 

1. The proposed model is a little heavy and introduces non-trivial computational overhead. The trade-off between scalability and interpretability (there are also other efficient methods for explainable graph-level representation learning) should be considered. Complexity and run-time analysis should be reflected in the paper -- given the overhead of computing on multiple subgraphs. 

1. Statistics of all datasets should be provided, e.g., avg. number of nodes, density of the graph.

1. All methods on GLAD seem to be unstable and less robust on the detection performance (with extremely high std). The evaluation of the effectiveness of the proposed method (RQ2 and RQ3) seems to be trivial and ""boring"". I suggest the authors to emphasize more on the RQ1.


Limitations:
Authors slightly discussed the model limitations in the Conclusion section. More discussions w.r.t. complexity and dataset-type applicability are needed. 

Rating:
6

Confidence:
4

";1
ztDxO15N7f;"REVIEW 
Summary:
The paper offers a new perspective on the problem of role extraction while defining node roles based on the ideas of equitable partitions and and graph-isomorphism tests, the Weisfeiler-Leman algorithm. The paper studies two associated optimization problems (cost functions) inspired by graph isomorphism testing and provides theoretical guarantees for their solutions.  To validate the approach, a novel ""role-infused partition benchmark"" is introduced, enabling the generation of networks where nodes possess different roles in a stochastic manner.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
(+) The paper offers a new perspective on node role extraction, focusing on equitable partitions and providing a principled stochastic notion of node roles. This approach adds to the existing methods in the field and offers a new way to define and quantify node roles in complex networks.

(+) The paper presents a quantitative definition of node roles based on equitable partitions, allowing for the numerical measurement of deviation from an exact equivalence. This provides a more nuanced understanding of node roles compared to traditional definitions.

(+) The technical aspects of the paper are thorough and detailed. Also, the technical details aeem to be correct. 

(+) Numerical experiments show the effectiveness and superiority of the proposed method over a graph neural network, GIN, for three different tasks. 

Weaknesses:
(-) While the paper briefly mentions different categories of node role extraction approaches (graphlet-based, walk-based, and matrix-factorization-based), it does not provide a detailed comparison or analysis of how the proposed equitable partitions approach compares to these existing methods. A more rigorous comparative analysis, including performance metrics and evaluation on benchmark datasets, would strengthen the paper's contribution and demonstrate the advantages of the proposed approach.

(-)  The idea of Equitable partitions for node role discovery is interesting. However, the details regarding ""why this approach"" makes sense is missing. There can be other ways too? Why this method should work? At a conceptual level, a more rigorous explanation related to the definition of roles based on equitable partition is missing. I think this is crucially important. In other words, the paper focuses on providing details of ""designing"" and approach based on EP, while leaving details of ""why this is appropriate""

(-) A more thorough literature review might be needed. For instance, the following paper provides a nice simple algorithm for computing equitable partitions. (I am not sure it is better or worse; however it might be handy to have a look at it.)
    
- Zhang et al., ""Upper and Lower Bounds for Controllable Subspaces of Networks of Diffusively Coupled Agents,"" IEEE Transactions on Automatic control, 2014.

Also, there is some recent work on counting subgraphs (that may define node roles based on the structural attributes of the graph), for instance, 

- Hassan et al., ""Computing Graph Descriptors on Edge Streams."" ACM Transactions on Knowledge Discovery from Data, 2023.

(-) The paper introduces a family of cost functions to assess the quality of a role partitioning. However, it does not thoroughly discuss the selection, design, or properties of these cost functions. Providing a more in-depth exploration and analysis of the different cost functions, their properties, and how they relate to the problem of node role extraction would enhance the technical understanding of the proposed approach.

(-) It is unclear how the ground truth for the sampled adjacency matrices is computed  in Experiment 1 of Section 5. Moreover, GIN is a relatively old graph neural network. There are recent methods that show better results on several downstream tasks and could have been considered for baseline comparisons. Some of these works include the work of Bouritsas et al, and Ramp\'a\v{s}ek et al, below.

- Bouritsas et al., ""Improving graph neural network expressivity via subgraph isomorphism counting,"" IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022 Feb 24;45(1):657-68.

- Ramp\'a\v{s}ek, et al., ""Recipe for a general, powerful, scalable graph transformer,"" Advances in Neural Information Processing Systems. 2022.

Limitations:
The limitations and possibly approaches to tackle these limitations are presented in the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents a novel perspective on the problem of node role extraction in complex networks, highlighting its distinctions from community detection. The authors propose a definition of node roles and introduce two optimization problems based on graph-isomorphism tests, the Weisfeiler-Leman algorithm, and equitable partitions. Theoretical guarantees are provided, and the approach is validated using a newly introduced ""role-infused partition benchmark"" that allows for stochastic assignment of different roles to nodes in sampled networks. The findings contribute to network analysis, graph mining, and the development of reduced order models for dynamical processes on networks.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The research topic, role discovery in graphs, is very important and is worth studying, especially compared to the community detection problem, role discovery is also important but lacks attention.

- The design of the model based on equitable partitions is technically sound and theoretically guaranteed.

- Experimental results on several datasets from different perspectives show the effectiveness of the proposed method.

Weaknesses:
- Baseline selection. Some relevant and/or important baselines have not been compared.

- Downstream tasks. The proposed method is not tested on widely used downstream tasks to show its effectiveness.

- Dataset. There are some more widely used datasets for role discovery that are not used in this paper.

Limitations:
No potential negative societal impact.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper considers a relaxed definition of the coarsest equitable partition (CEP), which equals the final partition of Weisfeiler-Leman or the color refinement algorithm in the original version. The authors allow to specify the number of cells of the partition and derive a related optimization problem. From this, an algorithm for role discovery is derived, outperforming techniques such as role2vec on real-world graphs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The concept of structural roles is closely related to the Weisfeiler-Leman algorithm and fundamental for graph learning.
* The relaxation of the final WL partition with a fixed number of cells is novel.
* The RIP model for graph generation is innovative.

Weaknesses:
* Algorithm 1 is not analyzed sufficiently. It would be interesting to have approximation guarantees. Moreover, it would be nice to investigate the case where $k$ equals the number of cells in the cEP. I believe that the current version would not guarantee that the cEP is found since the cluster method obtains only $X$ as an argument neglecting $H$, which should be required to guarantee this. Is this correct?
* Several other methods compute relaxed WL partitions without allowing to specify $k$ and are discussed in section 2.1. These could also be used in the experimental comparison to assess the advantages of the proposed method.

Limitations:
Limitations are sufficiently discussed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose the notion of equitable partitions from graph isomorphism literature in order to partition the nodes of a network according to their structural roles. They study two optimization problems for approximately recovering such equitable partitions. Analogous to the SBM model, the authors devise the RIP (role-infused partition) stochastic model and validate their findings on this model. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is very well-written. The proposed cost functions for computing approximate equitable partitions are very well-motivated and natural. Approximate versions of the Weisfeiler-Leman algorithm is an important question which deserves much study. The proposed RIP model is a very satisfcatory benchmark for role discovery.

Weaknesses:
The experimental work is inadequate in order to understand the impact of the proposed formalism to real-world data (Section 5, Experiment 3). The considered datasets are too few, specialized and small-sized in order to deduce that approximate equitable partitions (and approximate Weisfeiler-Leman) is at the core of role discovery.  

Limitations:
Yes.

Rating:
6

Confidence:
3

";1
VNyKBipt91;"REVIEW 
Summary:
This paper proposes a novel Bayesian personalized federated learning approach using meta-variational dropout. The proposed approach employs a shared hypernetwork to predict the client-dependent dropout rates for each model parameter, enabling effective model personalization and adaptation in the limited non-i.i.d. data environment. The effectiveness of this approach is demonstrated empirically on various FL datasets, including CIFAR-10, CIFAR-100, FEMINIST, and CelebA and multi-domain FL datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is clearly written and well organized.

2. The proposed method in the paper is well-motivated and technically correct.

3. The Bayesian approach to FL is interesting and seems suitable for personalization. The introduced MetaVD seems to work well in practice.

4. The method is extensively tested on a variety of FL datasets.



Weaknesses:
1. The discussion with related work needs to include other Bayesian treatments for personalized FL, e.g. [1], [2]

2. The experiments lack a comparison with relevant methods mentioned above [1-2], as well as the baseline in [3].

[1] Personalized Federated Learning via Variational Bayesian Inference. ICML 2022.

[2] Fedpop: A Bayesian approach for Personalised Federated Learning. NeurIPS 2022.

[3] Personalized Federated Learning using Hypernetworks. ICML 2021.

Limitations:
NA.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper introduces a novel approach called meta-variational dropout (MetaVD) for addressing challenges in federated learning (FL). Traditional FL faces issues such as model overfitting and divergence of local models due to non-i.i.d. data across clients. MetaVD leverages Bayesian meta-learning to predict client-specific dropout rates using a hyper network. Extensive experiments conducted on various FL datasets demonstrate that MetaVD achieves good performance.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The proposed approach is novel.
2. Authors performed experiments on a variety of tasks and datasets and showed promising results.
3. The proposed method showed consistent improvements over the considered baselines with a good margin.

Weaknesses:
A primary weakness is that the authors have not compared their results with existing state-of-the-art personalized FL algorithms, e.g., [1,2] and the related work/baselines referenced within them.

Other notes and requests for clarification:
1. In L66, where the authors discuss the convergence guarantees of the original FL algorithm, it is important to provide a citation.
2. The authors have written about the “Challenges of FL” in Section 2. They should briefly describe how the proposed approach addresses these challenges.
3. Figure 1 requires additional details to aid understanding. The authors should clarify the meaning of the ""x"" in the modules of the figure and explain how it illustrates the difference in aggregation between MetaVD and FedAvg.
4. In Table 3, the signs of ""+"" and ""-"" should be reversed to ensure accurate representation.
5. Table 4 should explicitly mention that the results presented are for out-of-distribution (OOD) clients to provide a clear context for the findings.
6. Figure 1 is not referenced or discussed in the paper. It should be either referred to in the main text or removed if it does not contribute significantly to the paper's content.
7. Additional information regarding the computation of gradients for the hypernetwork parameters should be included.
8. The authors should provide an explanation as to why the PerFedAvg+MetaVD+DP combination in Table 7 leads to improved performance despite dropping almost 80% of the parameters.
9. Regarding equation 5, the authors should elaborate on the implications of the inverse dependence of the aggregation weights on the square of the model weight. This information would enhance the understanding of the aggregation process and its impact on the final results.


[1] “Fusion of Global and Local Knowledge for Personalized Federated Learning”, TMLR 2023.

[2] “FedALA: Adaptive Local Aggregation for Personalized Federated Learning”.

Limitations:
The authors haven’t discussed the limitations of their work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The submission proposed a novel Bayesian meta-learning approach metaVD for federated learning. metaVD learns to predict client-dependent dropout rates via a hypernetwork, helping address the model personalization and limited non-i.i.d. data problems. At the same time, metaVD compressed the model parameter, alleviated the overfitting and reduced the communication costs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
MetaVD encompasses a new posterior aggregation strategy to consolidate local models into a global one. In addition, MetaVD predicts the dropout rates of parameters via a hypernetwork, enabling parameter compression. This not only allievates the overfitting problem but also reduces the communication costs of exchanging model parameters.

Weaknesses:
In general, this article is written in a fluent, simple, and easily understandable manner. However, there are two main issues that need to be addressed:

1. The proposed method in the article is straightforward and intuitive, but many aspects lack theoretical guarantees and analysis. 

2. In the past years, Bayesian federated learning has made significant progress, but many relevant works have not been discussed or compared in the article. 

3. if my understanding is right, the propsoed method has the risk of data leakage.

Limitations:
The authors discussed the limitations of their work, but they did not address the potential negative societal impact it may have.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a new federated learning (FL) framework to address the various issues of FL. Specifically, this framework (a) leverages Bayesian FL to address the issue of non i.i.d. data among clients, (b) instantiates its BFL model with a variational dropout posterior to efficiently handle large amount of clients, and (c) applies meta learning adaptation strategies (e.g., MAML and Reptile) at client-sides to personalize the local models. The paper demonstrates the performance of this approach on several vision datasets, with ablation studies to confirm the effectiveness of proposed components.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
The proposed method makes sense and addresses various relevant problems in the field of FL. I think the paper did an excellent job combining various ideas, and presenting them as one coherent framework. In addition, the empirical results positively improve over existing baselines, which suggests strong practical merits. I also like that the paper is really rigorous with conducting ablation studies.

Weaknesses:
The most apparent weakness of this paper is the lack of technical novelty. Bayesian FL, Variational Dropout, and Meta-learning are all very well-known strategies. While I am not opposed to a creative combination of ideas, such idea should have a scientific merit that outweighs the sum of its individual components. Here, each component is behaving exactly as expected and there seems to be no technical challenge in terms of integrating them. I think the novel adjustment here is the hypernetwork (although I am not sure if this strategy has been previously adopted in the context of variational dropout --- e.g., hierarchical prior is sort of similar, but not exactly the same).

Some other weaknesses:
- Lack of error bar/standard deviation in any of the reported results.
- I would prefer some sort of performance vs. communication rounds to demonstrate convergence. Result tables are acceptable, but very uninformative.
- It was previously claimed that the hypernetwork helps with large number of clients, especially when some of them have limited data. I think this should be supported by an ablation study showing the effect of increasing no. clients/ decreasing client data (the current ablation study only compares MetaVD to NaiveVD/EnsembleVD on the default client setting).

Some other minor issues:
- Various typos (e.g., FEMINIST, line 187; Hetrogenity, table 2 header ...)

Limitations:
The authors have discussed some limitations of the work in the conclusion. I think the method is purely theoretical and there is no potential negative societal impact.

Rating:
6

Confidence:
4

";1
NibgkUin5n;"REVIEW 
Summary:
This paper tackles semi-supervised medical image segmentation over multiple domains, generalizing unsupervised domain adaptation and semi-supervised learning. A multiple branch encoder-decoder architecture with a shared encoder is proposed. The first branch is a diffusion model with V-Net architecture trained over labeled images and their noisy labels, where the encoder aims to extract domain-invariant features. The second decoder is trained over labeled images to combat class-imbalance by reweighting the loss with respect to Population Stability Index. The third branch is trained over unlabeled images with pseudolabels, where pseudolabels are ensembles of probability maps predicted by the other two decoders. To account for the sparse predictions of diffusion model decoder, its probability predictions are reparamatrized via Gumbel-Softmax prior to ensembling into pseudolabels. Each branch is trained over a combination of cross-entropy and dice loss, with input images and labels varying with respect to the branch. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Adopting diffusion models for generalized semi-supervised segmentation and domain adaptation is novel. Abundant experiments over multiple tasks and datasets demonstrate the benefit of the proposed method, particularly for unsupervised domain adaptation. 


Weaknesses:
Unsupervised/semi-supervised segmentation and domain adaptation in medical imaging are well-explored domains with several missing related works:

Gu et al. “ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation”, 2022

Xia et al. “Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation”, 2020

Bian et al. “DDA-Net: Unsupervised cross-modality medical image segmentation via dual domain adaptation”, 2021

Perone et al. “Unsupervised domain adaptation for medical imaging segmentation with self-ensembling”, 2019

Shin et al. “COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training”, 2022

Bateson et al. “Source-Relaxed Domain Adaptation for Image Segmentation”, 2021

Liu et al. “S-CUDA: Self-cleansing unsupervised domain adaptation for medical image segmentation”, 2021

Liu et al. “ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training”, 2022

Particularly, Liu et al. 2022 propose decoupled semi-supervised training and domain adaptation branches, a similar motivation to the proposed method. The authors should highlight their novelty against these methods that are not discussed. 


Limitations:
Limitations are discussed

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper addressed both the semi-supervised issue and the domain adaptation issue for medical image segmentation. The proposed method incorporated the diffusion model and used a consistency-based regularization for training. Performance on several public datasets is good for the segmentation tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. Applying a diffusion model to reduce the domain gaps is reasonable and achieves performance gains according to the results.
2. Extensive experiments and sufficient comparisons.
3. The motivation is sound. Considering both tasks together is now the research attention.

Weaknesses:
1. There are some recent papers considering the two tasks as well. See:
[1] Bai Y, Chen D, Li Q, et al. Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 11514-11524.
[2] Wu H, Li X, Lin Y, et al. Compete to Win: Enhancing Pseudo Labels for Barely-supervised Medical Image Segmentation[J]. IEEE Transactions on Medical Imaging, 2023.
Therefore, the problem definition is new.

2. It's a bit over-claimed to unify four tasks together. For example, this paper does not provide unique designs for imbalanced learning. Do not make the claims too broad.
3. The performance is not SOTA, especially on the popular LA dataset, see [1]. Moreover, about the comparison, it is weird that there are different compared methods for different tasks.
4. Regarding the main task (semi-supervised learning), the technical insight is not clear. The domain gap should be a critical issue here. Please elaborate more on the take-home insights rather than tricks.

Limitations:
See the above weaknesses

Rating:
4

Confidence:
5

REVIEW 
Summary:
This work proposes a generic framework for semi-supervised learning (SSL) in medical image segmentation. They identify two obstacles that prevent the applicability of SSL in real-world scenarios. Then they propose Aggregating & Decoupling to alleviate the issues under several settings, including class imbalanced SSL, UDA, and SemiDG. They show the proposed method can mitigate overfitting to labeled data, specific domains, and classes across four settings and many datasets. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-structured and easy to follow. The proposed method is generalizable to four different settings, enhancing the applicability of SSL in real-world scenarios. The presentation is very clear, especially the introduction, where the obstacles, and motivations are clearly stated. 

Weaknesses:
My biggest concern about this work is the effectiveness in more generic applications, e.g., standard computer vision benchmarks. I appreciate the authors identifying obstacles that hurt the performance of SSL in real-world applications. However, what is the relationship between the proposed approach and medical image segmentation? Is any proposed component tailored to deal with the issues with medical images? In my eye, the proposed framework is generic and should be working widely well in many applications instead of only focusing on medical images. It would be great if the authors could verify the effectiveness of the proposed methods in CV benchmarks instead of only medical imaging applications if the proposed method is not designed or tailored to medical settings. I would consider re-rating if the authors response to my concern. 

Limitations:
Yes. 

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes an Aggregating & Decoupling framework, which unifies the SSL, Class Imbalanced SSL, UDA, and SemiDG for volumetric medical image segmentation with one generic framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper explores a generic framework for the SSL, Class Imbalanced SSL, UDA, and SemiDG of volumetric medical image segmentation.

Weaknesses:
The description of the method is not clear.

Limitations:
The authors have analyzed the limitations of the proposed method.



Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents a novel framework for semi-supervised medical segmentation that seeks to address three different scenarios simultaneously: lack of labelled data (standard semi-supervised learning -- SSL), domain shifts (unsupervised domain adaptation -- UDA) and generalization to new, unseen domains (semi-supervised domain generalization -- SemiDG). Toward this goal, they combine several strategies in their framework: sampling-based data augmentation, diffusion V-Net for learning domain invariant representations, difficulty-aware supervised training to generate class-unbiased pseudo-labels, and a reparameterize and smooth (RS) technique based on on the Gumbel softmax & ensembling strategy to generate high-quality pseudo-labels. The proposed approach is tested on four different datasets and tasks: LASeg for the SS taskL, Synapse for class imbalanced SSL, MMWHS for UDA, and M&Ms for SemiDG. Results show the method to achieve a similar or better performance on all tasks compared to SOTA methods.




Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposed framework brings together three different medical image segmentation scenarios involving unlabelled data: SSL, UDA and SemiDG. This bridges a gap in the literature, as existing approaches typically focus on a single of these scenarios.

* Authors do a good job in the Introduction (Figs 1-3) to situate their method with respect to existing approaches and highlight the limitations of these approaches. 

* Experiments to evaluate the proposed method are comprehensive. The proposed method is evaluated in four different scenarios and, fr each one, compared against the SOTA. The experimental validation also includes several relevant ablation studies that demonstrate the usefulness of the method's different components.

* Results show the method to achieve SOTA performance on all tasks. These results are particularly impressive for the UDA task on the MMWHS (MR to CT) where the method achieves a performance on par with supervised training (more on this later).

Weaknesses:
* From my understanding, the technical novelty of the work comes from the combination of several existing techniques to address different problems in SSL. While some results are good, the proposed method feels heavily-engineered and complex.

* Experiments do not fully support the main claims of the paper. For example, current experiments employ different baselines to compare the method in the SSL, UDA and SemiDG settings, instead of testing the same approaches (or some of them) across these settings. Hence, the claim that current approaches can only handle one setting is not truly validated. Likewise, the claim that having separate branches for labelled and unlabelled is not well validated in the ablation study on architectures since all tested models implement this idea (as I understand)  

*  In terms of results, the method yields more limited improvements for the SSL and SemiDG settings. It is unclear if these are statistically significant.

Limitations:
The discussion of limitations could be expanded (only the training time of the diffusion model is mentioned). Any failure cases?



Rating:
5

Confidence:
4

";1
s7xWeJQACI;"REVIEW 
Summary:

This work re-examines a well-known technique in the NLP literature, called continued pre-training, that can be utilized to enhance the performance of language models on downstream tasks (in this case, for classification and regression tasks).
The authors have revealed that the conventional approach to continued pre-training is ineffective when applied to sentence-pair tasks and prompt-based fine-tuning settings.
Based on their findings, the authors propose a simple method that involves training on the masking language modeling objective while augmenting the text input with prompts.
This proposed approach has demonstrated its effectiveness across a range of tasks and configurations, suggesting that it can be an effective choice for natural language understanding tasks when using Transformer encoder-based models.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The proposed method is technically simple and effective.
- The work begins with a compelling finding that challenges the effectiveness of the conventional approach, which was previously regarded as effective.
- The authors made efforts to provide analysis from multiple perspectives, aiming to convince readers of the effectiveness of the proposed approach.



Weaknesses:
- Although the paper offers a detailed explanation and analysis, there are still ambiguous points that need to be clarified for a better understanding by the readers. Let me ask about this point in the following Questions section.
- There is room for improving the grammar and fluency of the paper's writing.
- I would like to see a thorough analysis of the factors that potentially contribute to the success of the proposed method. For instance, it would be valuable to explore the significance of including labels in continued fine-tuning (although partially considered in Section 4.4) and investigate why sentence-pair tasks do not benefit from the conventional TAPT approach.




Limitations:
The evaluation primarily focuses on the utilization of a single model, specifically RoBERTa-large, for relatively straightforward downstream tasks such as classification. These tasks are considered comparatively easier to solve.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a new approach to pre-training language models called Prompt-based Continued Pre-training (PCP). PCP combines the idea of instruction tuning with conventional continued pre-training. The authors argue that PCP can improve the performance of prompt-based fine-tuning on a variety of natural language processing tasks.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper presents a clear comparison of the proposed PCP with the conventional continued pre-training. The evidence on benchmark datasets is strong to support the claim that PCP performs better on these tasks.
2. The paper is well-written and easy to understand. The authors do a good job of explaining the technical details of the proposed approach, and they provide clear and concise summaries of the experimental results.
3. The approach is simple and easy to adopt.


Weaknesses:
1. It is not well understood why conventional continued pretraining (TAPT) is doing so poorly on sentence-pair tasks.
2. Because PCP makes use of templated prompts to align continued pretraining and fine-tuning, with additional pseudo-labels. It is unclear whether the gain is due to prompt alignment or the additional data augmentation from pseudo-labels. Further ablation is necessary.
3. It is unclear how well the proposed approach generalizes to other types of pretraining objectives such as language modeling.


Limitations:
Generalization of the approach to other pretraining objectives is unclear.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper explores the problem of continued pre-training on task-related text. The authors discovered that conventional continued pre-training methods may not be very effective and can even have a negative impact on fine-tuning performance. To address this, they introduce prompt-based continued pre-training. The approach involves generating pseudo labels on unlabeled data using a fine-tuned model and constructing a prompt-based pre-training corpus by applying templates to the pseudo labeled dataset. The researchers then utilize this corpus, which incorporates prompt information, for continued pre-training and prompt-based fine-tuning. Experimental evaluations conducted on various datasets validate the effectiveness of the proposed approach, as it achieves significant improvements over the baseline methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The researchers identify the limitations of conventional continued pre-training on task-related text and propose an innovative solution.
* The experimental results provide strong evidence for the effectiveness of the proposed approach.

Weaknesses:
No major weaknesses have been identified in this paper.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper makes a contribution by studying how to adapt pre-trained models to downstream tasks. The authors identify the limitations of TAPTs and show when they do not work well. They then propose PCP, a better algorithm that can adapt a pre-trained model to a target task. PCP is shown to be more effective than TAPTs on a variety of tasks.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides an intriguing analysis of when TAPT style is effective for fine-tuning. TAPT is not very effective on sentence-pair tasks or prompt-based fine-tuning, particularly.

Weaknesses:
The paper does a good job of pointing out the weaknesses of TAPT, but it is less clear why PCP works better. The authors do not need to know exactly why it works, but some hypotheses would be helpful. For example, why does PCP-style pretraining work better for prompt-based fine-tuning? Is it because the ""pretraining"" is more similar to ""fine-tuning"" in PCP? If so, how can we understand why PCP also works for sentence pair tasks?

The presentation of the paper could also be improved. Figure 2 is not very easy to understand, and the overall flow of the paper is not as clear as it could be.

Overall, the paper could be improved by providing more clarity and explanation.

Limitations:
The paper only applies on text classifications tasks, ignoring many text generation tasks.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The proposed method (CPC) is built on top of pseudo-labeling and continued pre-training via masked language modeling. This method provides an alternative way to use pseudo-labeled data before fine-tuning the model on downstream tasks. It improves the TAPT method and other semi-supervised methods for text classification.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The novelty of this paper lies in using pseudo-labeled data for masked language modeling. This idea is simple, yet effective. Even though pseudo-labeling and continued MLM training are not new, the way to combine these two is new.
- Extensive experiments on a large number of text classification tasks show the effectiveness of the method.


Weaknesses:
- One of the motivations (mentioned in the introduction) is that TAPT does not work well on sentence pair classification. However, it’s still unclear why TAPT does not work well on sentence pair classification while CPC can address this issue.
- Although CPC works empirically better than TAPT, the intuition of CPC is still unclear. Why should we continue pre-training on pseudo-labeled data instead of unlabeled data in TAPT?


Limitations:
NA

Rating:
5

Confidence:
4

";1
mLe63bAYc7;"REVIEW 
Summary:
This paper studies the robustness certification for equivariant tasks. Specifically, the adversarial robustness with group equivariance is defined in terms of  the input and output distance. Then equivariance-preserving randomized smoothing is introduced as well as various smoothing schemes. Experiments show that robustness guarantees can be obtained via equivariance preserving randomized smoothing.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The motivation of studying robustness with equivariance is clear and the problem is well formulated.

- The theory part looks sound and detailed and the paper is well organized.

Weaknesses:
- There are not enough baselines to show that the equivariance can improve the provable robustness as claimed on line 8 in the abstract. It is expected to compare with some previous works to back up the claim that prior work underestimates the robustness as line 399 states.

- If I understand correctly,  the permutation invariant task is a special case of equivariant task as the experiment of point cloud classification shows. Therefore, transformation specific robustness work [1, 2, 3, 4] can be involved like image/point cloud classification against 2D/3D rotation or translation. It will be of great interest to discuss the connection between these resolvable/non-resolvable transformations and group equivariance.

- Each experiment needs more justifications for choosing different smoothing schemes and measure types. 

- The notations in section 3 regarding measure-theoretic randomized smoothing are a bit confusing and not easy to follow. It is better to explain each domain notation ahead and explicitly.


---

[1] Li, Linyi, et al. ""Tss: Transformation-specific smoothing for robustness certification."" *Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security*. 2021.

[2] Chu, Wenda, Linyi Li, and Bo Li. ""Tpc: Transformation-specific smoothing for point cloud models."" *International Conference on Machine Learning*. PMLR, 2022.

[3] Hu, Hanjiang, et al. ""Robustness Certification of Visual Perception Models via Camera Motion Smoothing."" *Conference on Robot Learning*. PMLR, 2022.

[4] Hao, Zhongkai, et al. ""Gsmooth: Certified robustness against semantic transformations via generalized randomized smoothing."" *International Conference on Machine Learning*. PMLR, 2022.

Limitations:
Yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper investigates adversarial robustness for group equivariant tasks. To this end, the authors propose a novel notion of adversarial robustness:
An attacker aims to find a perturbation $x'$ ""close"" (up to some distance constraint in the input space) to a natural input $x$. For this perturbation, they aim to find the worst-case perturbation within the input constraint and over the group of symmetries over the input. By a model equivariant to the underlying group, this reduces to classical adversarial robustness.

Further, the paper shows that randomized smoothing, a popular framework for making models provably adversarially robust, (and many of its variants) can fulfill this notion of robustness when the underlying model, distribution, measure, and smoothing scheme are equivariant.
Several instantiations of this approach are showcased on a variety of tasks, including graphs, point clouds, and molecules.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- Well written and easy to read
- The proposed definition for adversarial robustness is a) novel, b) a natural extension of the classical definition, and c) well motivated
- The section on the different variants of randomized smoothing and their relation to the problem is extensive
- I appreciate the combination of semantic preserving change (here the group action) and perceptual noise in the set of admissible perturbations and think this is a promising direction for the field.

Weaknesses:
- No investigation of the robustness notion directly, i.e., no attacks against undefended models where performed to showcase the issue
- Some further disambiguation from related work might be helpful to the reader (see below)


Limitations:
Limitations are adequately discussed in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, it propose  equivariance-perserving randomized smoothing to provide the robustness of the GNN model. The proof demonstrates the provably robustness of the proposed smoothing method. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The theoretic guarantee of this paper is strong. And the proposed random smoothing methods can be applied on multiple tasks including the point cloud classification and the force field prediction.

Weaknesses:
1. The experiment of Figure 4 can contain more geometric GNN such as Spherenet (https://arxiv.org/abs/2102.05013), and GemNet,(https://arxiv.org/abs/2106.08903) to demonstrate.
2. The math part of this paper is a little hard to follow. Maybe some some figures or concrete descriptions on how to apply the proposed smooth techniques can help people understand better.

Limitations:
Is there any experiments showing the performance of model when no smoothing techniques are used? I think it can provide the reason why such smoothing technique is needed.

Rating:
5

Confidence:
1

REVIEW 
Summary:
Authors note that the definition of adversarial robustness needs to be adjusted in cases when the problem is equivalent - for example, graph perturbations that are large in terms of the number of removed/added edges might actually be small if we take into account graph isomorphism (eg if a large graph perturbation ends up not change the graph topology much). Effectively, authors propose (Proposition 1) to change to notion of the distance between inputs x and x’ in the definition of adversarial robustness to equal the minimal distance between x and all elements of the equivalence class of x’ (eg all permutations of x’ if the input domain is graphs, or all translations and rotations of x’ if the problem is translation and rotation equivariant = use set registration distance between x and x’ to define adversarial robustness of the translation+rotation equivariant task). Then authors argue that symmetries in the output domain should also be accounted for (Def 1) which is often hard, but can be skipped entirely (Proposition 2) if the model itself is equivariant by construction. Then authors argue that the notion of smoothing distribution can (and should) take into account the updated notion of distance between points and that this definition allows that to theoretically justify several existing equivariant smoothing schemes and define more generalized notions of robustness (e.g. Eq 2). Authors note that in many cases computing such distances is computationally hard (e.g. NP-hard when working with permutation equivariance groups). Authors measure their updated equivariance-aware adversarial robustness of PointNet and DGCNN on permutation-invariant problem of ModelNet40 classification, of DimeNet++ on rigid-transformation-equivariant regression on MD17, and on graph convolutional DNN on permutation-equivariant node classification with several notions of graph edit distance (weight of deletion/insertion). 

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The paper is relatively easy to follow and sound. The motivation makes sense. Authors provide a formal generalization of the intuitive notions of how to measure distances between inputs in problems that have natural symmetries and having a shared language might be useful as a reference for future research.

Weaknesses:
My main concern with this paper is that it proposes a shared general formalization for the intuitive procedure that people have been already using extensively across many tasks with symmetries (eg rigid-aligned error in pose estimation), but this formalization that does not appear to bring much value beyond defining “shared language” and identifying “generalized knobs” that one can tweak to get slightly varied definitions of adversarial robustness. While reading the paper, I was hoping that authors would provide some kind of surprising practically useful result that makes use of their generalized definitions (eg a general algorithm that lets us compute such distances efficiently), but that did not happen - all presented results appear to be mathematizations of observations we already take for granted (eg if the model is equivariant by construction, there’s no need to invert its outputs because they are already “canonical”). One good example of a paper that worked with symmetries, introduced some simple new general definitions, and proved something surprising and practically useful from that point is “Training Generative Adversarial Networks with Limited Data” by Karras et al (see Appendix C).

I might be wrong, so I would appreciate input from other reviewers.


Limitations:
Authors acknowledge that their approach is applicable only to equivariant models.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper presents several contributions to the field of adversarial robustness for group equivariant tasks. The authors propose a sound concept of adversarial robustness for these tasks and demonstrate that using equivariant models can facilitate achieving provable robustness. They also prove that various randomized smoothing approaches preserve equivariance and extend existing robustness certificates for graph and node classification from $l_0$ perturbations to graph edit distance perturbations with user-specified costs. The paper underscores the importance of reevaluating adversarial robustness in equivariant tasks for future research in robust and geometric machine learning. Extensive experiments were conducted in various settings to validate the claims of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper presents a novel definition of adversarial robustness for group equivariant tasks, which is a significant contribution to the field. The $\epsilon-\delta$ definition is interesting.
- The paper provides theoretical proofs for the soundness of their proposed notion.
- The proposed method is applicable to a wide range of tasks, including graphs, point clouds, molecules, and more.
- The paper includes experimental results that validate the theoretical claims, providing empirical evidence for the effectiveness of the proposed method.

Weaknesses:
I think this paper is innovative and sound in general, and just want to list a few things to address below:
- While the proposed definition of robustness in this paper specifically tailors toward group equivariance tasks, I am wondering how it can address the limitations and problems with the previously used definition. In figure 1 and figure 2, the authors give two examples related to graph isomorphism and rotation – how would the flaws in the old notion of robustness cause issues in applications related to those scenarios? For instance, one potential setting of graph node classification attacks is making imperceptible insertions/deletions to the recommendation graphs (example attacks in [1] and [2]). How will this new notion of robustness affect the consideration of attacks when grounded to those specific tasks/applications?

[1] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2847–2856, 2018.   
[2] Daniel Zügner and Stephan Günnemann. Adversarial attacks on graph neural networks via meta learning. arXiv preprint arXiv:1902.08412, 2019.

Limitations:
The authors adequately addressed the limitations.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper provides a new insight into adversarial robustness for equivariant tasks (like graph classification) where we need to separate the perturbations that occur according the allowable transformations (that actually form a group) - and where we'd like to have the same output disregarding the norm of perturbation - and harmful perturbations, where we need to care about its norm.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The novel work stating the very important question: what is the objective for the adversarial robustness for inputs which are equivariant according to some group of transformations?
Although the task seems very vague, the authors answered it in a very elegant manner by:
- introducing the new distance $\hat{d}_{in}$ which should be invariant w.r.t. to the group transformations
- introducing two properties of this new distance (upper bound as usual distance $d_{in}$ + being the max of such distances)
- and finally proving the Proposition 1 that shows that $\hat{d}_{in}$ is actually min among usual distances w.r.t. group transformations for one of its input.
Based on it, the authors came to a definition of adversarial robustness for group equivariant tasks, which can be shrunk to the usual definition of adversarial robustness when we don't have the group properties, or compatible with other group-invariant distances like Hausdorff and Chamfer ones.
Moreover, they proved the following (Proposition 2): if a model is equivariant w.r.t. to the group transformations, then the new definition it is equivalent to being robust in the usual sense. As a result, we just need to prove the usual robustness (for which we have a number of techniques like Certified Smoothing) for an equivariant model.

Weaknesses:
During the discussion in ""Related Work / Transformation-specific robustness"" (line 88), and even more seriously during ""Product measures"" (lines 287-288), it makes sense to mention the work [1] where the interesting usecase was studied:
- multiplicative group of the ""gamma correction"" image transformations
- Rayleigh distribution to serve as a smoothing distribution
Would be interesting to know how it fits into the proposed framework.

Additionally, how the proposed framework is dealing with interpolation error (e.g., [1], [2]) which is very non-trivial and is out of group properties, is not mentioned in the paper.


[1] Muravev, Nikita, and Aleksandr Petiushko. ""Certified robustness via randomized smoothing over multiplicative parameters of input transformations."" Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, 3366-3372, 2022.
[2] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations via randomized smoothing. Advances in Neural information processing systems, 33: 8404–8417, 2020.

Limitations:
According to the text, there is a statement: ""prior work drastically underestimates the strength of the adversary and actually proves robustness for significantly larger sets of perturbed inputs"" (e.g., lines 398-400).
Is there any :
- empirical proof for it?
- theoretical consideration reasons for it?

Probably the answer was somehow blurred inside the text, would love to hear the clear explanation for it.

Rating:
7

Confidence:
4

";1
XkcufOcgUc;"REVIEW 
Summary:
This work proposes a structure-free graph condensation paradigm to distill a large-scale graph into a small-scale graph node set without explicit graph structures. The node attributes of the obtained small-scale condensed graph-free data could encode topology structure information. And the condensed node set could serve as a substitution to replace the large-scale graph for training GNNs and achieves comparable performance on the test set.
The method contains two techniques, (1) the parameter matching schema under imitation learning, and (2) a dynamic evaluation schema with a GNTK score. The experimental results could verify its claims and show performance effectiveness.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
S1-[originality]: This work's most remarkable part is its structure-free condensation paradigm, which removes the graph structure by encoding it into node attributes, so that the obtained condensed graph-free data only contains a set of informative nodes. I think this paper would inspire many interesting questions for future researches and topics, when only use a set of nodes to represent the whole large-scale graph for training but obtain comparable test results.

S2-[clarity]: Overall, the question descriptions, contributions, techniques, and experimental results of this paper are described clearly and soundly. 

first, the concept of structure-free graph condensation and its relevant scenarios have been clearly defined and exemplified.

second, the contributions and techniques including the training trajectory matching with online GNN parameters and the GNTK-based dynamic score are clear and sound. 
This paper could ease three-level optimization to a bi-level optimization without condensed graph structures, which is a novel and interesting structure-free graph condensation pattern. 
The GNTK-based dynamic evaluation score enables the closed-form solution of GNN evaluation and sounds novel to me.

third, the experimental results compared with the whole graph dataset training results could support this paper's arguments. An ablation study verifies the effectiveness of the structure-free paradigm. There is another interesting part that the results across different GNN architectures show surprisingly good generalization ability in terms of the condensed node set. This is an inspirable result and deserves future explorations.

S3-[significance]: this work on graph condensation is an interesting problem to reduce the effects brought by large-graph data scale and quantity, and it might be helpful for reducing calculations in real-world applications.


Weaknesses:
There are only some technical details that are not well presented here.

W1: how to choose the condensation ratios like Cora with 0.9% and Citeseer with 1.3%? More details and explanations should be involved.

W2: how to deal with the condensed graph labels Y? Is it generated according to the original large-scale graph's classes and each class examples?

W3: are the student steps and teacher steps empirical hyperparameters or learnable parameters that need to be optimized in the condensation process? More details should be given. Besides, this work provides some interesting results, and more discussions of experimental findings should be given, for instance, why the synthesized graph-free data has good generalization ability.

Limitations:
Yes, the authors have mentioned limitations, no negative social impact.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies an interesting problem. The authors proposes a new paradigm for reducing the size of large-scale graphs without explicit graph structures. The proposed SFGC, encodes topology structure information into node attributes in synthesized graph-free data. Extensive experiments demonstrate the effectiveness of the proposed method compred with existing graph condensation methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper proposes a novel SFGC approach.
- This paper studies the effectiveness and generalization ability issues in graph condensation.
- This paper provides theoretical illustrations of the proposed structure-free graph condensation paradigm from the views of statistical learning and information flow, respectively.

Weaknesses:
- The work fails to provide a clear motivation for why it is important or required to reduce the size of big graphs lacking explicit graph structures. 
- The paper could benefit from a more thorough discussion of the limitations and potential future directions of the proposed approach.
- Without having access to the source code, it is challenging to reproduce the results. The experimental setting is described, however it would be beneficial to have access to the source code to guarantee that the findings can be reproducible.

If the main concern about the reproducibility is solved, I am willing to increase the score.


Limitations:
The authors could benefit from a more thorough discussion of the limitations and potential negative societal impacts of their work, as well as potential ways to mitigate these issues. 

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents a new method to condense a training graph into a smaller number of disconnected nodes, such that a GNN trained on these nodes performs similar to one trained on the original graph at test time.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
**Originality.** Structure-free condensation has been reported before (GCOND-X), but the proposed method to do so is novel.

**Quality.** The overall quality of the work is good, including the presented techniques, results, tables and plots.

**Clarity.** The paper is mostly clear.

**Significance.** Graph condensation can be very useful under the right application scenarios. I don't see any special significance of aiming for structure-free condensation, except that the method ends up giving better results (e.g. due to easier optimization, as discussed in the paper).

Weaknesses:
The paper is not particularly easy to follow.

GCOND-X is not discussed explicitly in the related works, even though it is a structure-free graph condensation method.

Hyper-parameter details have not been provided.

Limitations:
A discussion of limitations is missing. An obvious limitation is that structure-free condensation won't work under ego- and neighbor-embedding separation [1] which is an effective recommendation for heterophilic datasets.

[1] Zhu et. al. ""Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs."" NeurIPS 2020

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper studies the problem of reducing the size of a large graph dataset while preserving task-relevant information. 
It introduces a new methodology to distill large-scale real-world graphs into smaller synthetic graph node sets by disregarding graph structures to create condensed graph-free data. The approach involves two key components: a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data, and a graph neural feature score metric for evaluating the quality of condensed graph-free data dynamically. Extensive experiments have demonstrated the efficiency and effectiveness of the proposed method.



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper addresses an interesting problem in the field of graph condensation, which has significant practical implications.
2. The proposed SFGC methodology exhibits commendable performance and generalization across various graph neural network (GNN) architectures.
3. The utilization of the Graph Neural Tangent Kernel (GNTK) to avoid iterative training of GNNs adds an interesting aspect to the paper.



Weaknesses:

Suggestions for Improvement:
1. To further strengthen the paper, it is recommended to demonstrate the benefits of SFGC in practical applications such as neural architecture search, privacy protection, adversarial robustness, or continual learning. Including at least one of these applications would greatly enhance the paper's significance.
2. It would be of interest to clarify how SFGC can benefit neural architecture search for GNNs since it does not generate graph structures, which are essential for GNNs, and different GNNs may require distinct operations over the graph structure.
3. While Figure A2 provides a comparison of the running time between GCN and GNTK, it would be valuable to include a detailed complexity analysis of both methods concerning the number of nodes. 
 * Specifically, elaborate on the quadratic complexity of GNTK due to the pairwise kernel matrix calculations and the matrix inversion operation. 
  * I guess that is also why on Reddit (r=0.05%) GCN and GNTK exhibit similar running times. What if we further increase r to 0.1%?  
4. It would be beneficial to include an empirical comparison with DosCond, as it also aims to accelerate the graph condensation process, to provide a comprehensive evaluation of SFGC.


Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a structure-free graph condensation method designed to distill large-scale graphs into small-scale graph-free data while preserving comparable expressiveness. The proposed method, named SFGC, achieves this by condensing the graph topology into an identity matrix, effectively embedding the structure information into the node attributes. To effectively imitate the GNN training process, SFGC employs a training trajectory meta-matching scheme. Additionally, a graph neural feature scoring technique is used, which dynamically evaluates the quality and relevance of the synthetic graph-free data.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Graph condensation is a crucial research area with many real-world applications. The authors propose a new “structure-free” graph condensation method. 
- They provide convincing experimental results and comprehensive discussions overall.
- The idea of using GNTK-based graph neural feature score metric is interesting and effective. 
- The paper is clearly written and easy to follow. Supplementary materials also offer valuable additional information regarding the model and its performance behaviors. 


Weaknesses:
- The authors claim that this is the first work that distills large-scale graphs to small-scale synthetic graph-free data, but the previous work GCOND-X also appears to perform a similar task of distilling large graphs to small-scale graph-free data. Further clarification on this would be needed. 
- It appears that the experiments do not precisely determine the extent to which different aspects of the model contribute to performance improvement. 
- The effectiveness of the “structure-free paradigm” in SFCG should also be convincingly demonstrated. In Section 3.2, a graph structure is generated from condensed node features and compared with SFCG. However, since the graph structure is already included in the node features, inputting the graph structure and condensed node features into the GNN model again could cause over-smoothing. Therefore, it is unclear whether the performance improvement over existing models is due to the structure-free paradigm or appears to be due to over-smoothing.


Limitations:
The authors do not include a discussion on the potential limitations of this study or offer insights into possible future research directions.

- An important consideration is that if SFGC necessitates the pre-training of a GNN on a large-scale graph to obtain a pre-trained training trajectory, it implies that real-world large-scale data must be initially trained. While this process is distinct from the graph condensation pipeline, it nonetheless introduces challenges associated with intensive computational demands. Addressing and suggesting solutions for this could provide valuable directions for future studies.

- I find the potential of employing different GNN models as the condensation network intriguing. As the method of incorporating graph structural information into node features heavily relies on the characteristics of the condensation model, using a different GNN model could potentially alter the properties of the condensed graph and impact the final performance. Therefore, additional experiments with various condensation networks and test networks would offer valuable insights.



Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a graph dataset condensing algorithm with a main idea of creating a new format of graph representation that does not explicitly include edge information. The authors suggest a new graph kernel and training algorithm with trajectory for online gradient to achieve graph condensation. Experiments demonstrate that the suggested algorithm (named SFGC) outperforms other baselines that condense the graph with explicit edge information in terms of node classification. The authors also showcase the performance of SFGC in terms of generalization ability and empirical learning time efficiency.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
One of the main strengths of this paper is introducing an interesting method for compressing graph datasets. The idea of removing edge information explicitly seems strong, but it appears plausible and well-founded, akin to non-negative matrix factorization with constraints of non-negativity. The simplicity of the idea allows other researchers to easily adapt it to their own graph-related research. Furthermore, the authors present numerous experimental results demonstrating that the suggested framework outperforms the baseline models.


Weaknesses:
I have some questions, so I hope to listen to the answers from the authors.

It is hard to see Figure 3.

Limitations:
It is challenging to find the limitation section. Could you please indicate the part where the limitations of the proposed method are discussed?

Rating:
6

Confidence:
2

";1
QwvaqV48fB;"REVIEW 
Summary:
In this paper, the authors investigate a phenomenon appearing in positive and unlabeled data learning. i.e. predictive trends is different for positive and negative classes in binary classification task. Based on this observation, the authors propose a holistic approach to capture the predictive trends of different instances. Extensive experiments on real-world and synthetic data sets validate the effectiveness of proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The phenomenon appearing in positive and unlabeled data learning is interesting, i.e. predictive trends is different for positive and negative classes in binary classification task.
(2) Extensive experiments on real-world and synthetic data sets validate the effectiveness of proposed method.

Weaknesses:
However, there still exists several shortcomings to be overcome as follows:
(1) In experiments, the comparing algorithms are not presented.
(2) In Figure 3, what does it mean ""No trend""? The trend score is a constant or not?
(3) By using trend score, the problem of unlabeled data classification is transformed as threshold selection, which requires that the trend score exhibits obvious difference for these two classes.  Although the authors leverage Fisher Criterion to induce the positive and negative classes, as shown in Figure 1, the predictive trend on CIFAR10 is not completely consistent with the assumption. How does the author view this issue?

=====after rebuttal=====
The authors' responses have well addressed most of my concerns of this paper. I would like to raise my overall score from 6 to 7 and recommend acceptance on this paper.

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper focuses on addressing the positive and unlabeled learning problem. The authors make an observation that a simple resampling method can yield strong early-stage performance, which has been overlooked in previous literature. Building on this insight, the paper proposes a trend detection measure to tackle the PU learning problem. The proposed method is supported by both theoretical and empirical results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This work studies an important problem, and it is well-written and easy to follow.
2. The motivation and proposed method are clearly described. It's definitely a new way to approach the positive and unlabeled learning problem from the perspective of trend detection.
3. The proposed PU learning approach requires no explicit computation of the class prior and avoids the need for additional tuning efforts and assumptions.
4. The experiments are well-conducted, and comprehensively compared to recent SOTA methods. Ablations and sensitivity analysis are also abundant.

Overall, the paper presents a novel insight for the positive and unlabeled learning problem , and the contribution is definitely relevant and significant.

Weaknesses:
1. Although it is mentioned that the proposal can be applied to more machine learning topics, further explanation should be included.
2. Can other clustering methods could yield better empirical performance compared with Algorithm 1 

Limitations:
This is an algorithmic work, and the authors have addressed some limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors investigate the topic of positive and unlabeled learning, and solve it from an interesting perspective, namely trend score. The story begins with the empirical observations of different temporal output score trends of positive instances, unlabeled negative instances, and unlabeled positive instances. Based on the observations, the authors propose to compute the trend score under the temporal point process, and use the trend scores to distinguish positive and negative from unlabeled instances. The authors conduct numbers of experiments, and the results show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 The idea is novel for PU learning.
- The paper is easy-to-follow.
- Extensive empirical results.


Weaknesses:
Please refer to the following Questions section.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper addresses the positive and unlabeled learning problem and presents interesting and meaningful observations. Inspired by the observations, the authors propose an innovative approach that effectively discriminates between positive and negative examples within unlabeled data. The performance of this approach is validated on both synthetic and real-world datasets, further confirming its superiority.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- (Clarity) The paper is well organized and clearly written,  the figures are also informative and well-designed.
- (Novelty) The paper is thought-provoking! Handling the PUL problem from the perspective of predicting trends is highly innovative. Furthermore, the proposed techniques are also novel as far as I can tell. 
- (Quality) The paper is of good quality in my opinion. The algorithm is well designed for identifying trend prediction (via Trend Score) and clustering unlabeled data (via the improved Fisher Criterion). The technical details are all correct as far as I can tell. The empirical evaluation is also comprehensive, covering 6 datasets, including synthetic and real-world datasets, and comparing against popular baselines.
- (Significance) The proposed techniques are simple, easy to implement and experimentally highly effective, making the algorithm a strong, potentially impactful baseline for future researchers & practitioners to use and/or improve upon.

Weaknesses:
1. It would be appropriate to discuss why there can be significantly different prediction trends for positive and negative examples in unlabeled data.
2. Both Table 1 and Table 2 lack the experiments on STL10, although you mentioned in Table 2 that the reason is ""the true labels of unlabeled data are not available in STL10."" However, a better choice would have been to provide an explanation for the absence of this experiment from the first occurrence.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposed an interesting idea of using the trend detection technique for positive and unlabeled learning (PUL). The core problem of PUL is thus reformulated as recognizing trends in these scores. A new TPP-inspired measure is then utilized to solve the task. The idea of the paper is interesting; it is well-motivated and well-supported with a range of experiments from fraud detection and computer vision tasks.

-----------------------
After rebuttal: I am satisfied with the rebuttal and would like to increase my score.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed technique is well motivated and clearly distinguished from prior works.
2. The PUL model achieves good performance in the early training stage is an interesting phenomenon and worth studying.
3. The proposed method is technically sound and it demonstrates a strong performance against both two-step methods and popular reweighting strategies. Plus, comprehensive experimental results on benchmark datasets clearly demonstrate the phenonmenon and the effectiveness of the proposed method.

Weaknesses:
1. The paper attributes the performance downgrade after the early stage success to the label noise problem and the data imbalance problem. However, it is unclear whether all existing reweighting or resampling methods can achieve the same state-of-the-art (SOTA) test set performance in the early training stage and witness a quick performance degradation.
2. std should be included in the experimental results such as those demonstrated in Figure 1.

Limitations:
Yes

Rating:
7

Confidence:
4

";1
sw2Y0sirtM;"REVIEW 
Summary:
Neural population decoding refers to inferring the behavioral output of organisms from a recording of a population of neuronal recordings. This paper introduces a transformer architecture to improve the accuracy of decoding. The efficiency of the system is increased by representing input spike trains with event-based tokens and by switching to a latent representation (to decrease the size of attention matrices).

In the presence of behavioral data, the model can identify new neurons (units) that are potentially obtained from different experiments. This is done by freezing the parameters of the network during gradient descent except for the encoding of the new units. This is a novel and interesting approach although it depends on the presence of behavioral recordings.

The method is tested on multiple experiments where neural activity recordings are obtained from motor cortical regions of monkeys while the monkeys are engaged in tasks involving arm movements.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- A novel framework for neural population decoding
- Efficient representation of the recordings via event-based tokenization
- Identification of units from new experiments in a way that is consistent with previous experiments. i.e., embedding a new unit such that the embedding location is consistent with the locations of other units in previous experiments whose activities/roles are similar to the new unit.
- Use of multiple experiments done in different labs to demonstrate the method.
- The inference accuracy of the method seems to improve significantly upon strong baselines.

Weaknesses:
- I oppose alluding to 'foundation models', which have become popular in natural language processing. Importantly, such models are trained in an unsupervised manner and demonstrate emergent abilities to solve multiple downstream tasks. The proposed architecture depends strongly on the presence of joint behavioral recordings. Similarly, it wouldn't make sense to apply this trained model to recordings from, say, the visual cortex.

- [line 165] Are commands given via natural language? If not, please provide mathematical description instead (or in addition).

- One simple experiment to probe the fidelity of ""unit identification"" could be: (i) take a recording, learn embeddings of units, etc. (ii) shuffle the order of units and repeat the experiment by adding rows to Embed as described in text. (iii) Compare and report the similarity between the two embeddings.


Limitations:
Yes.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Deep learning and transformer models have shown great promise in identifying structure from large datasets. With recent advancements in neural recording methods, it is now possible to generate rich and heterogeneous recordings from large populations of neurons across multiple brain regions and experimental conditions. The proposed work bridges this gap by introducing a new approach to modeling neural dynamics across these recordings. The authors leverage transformer models by tokenizing neural activity, preserving the temporal structure by treating each spike as a discrete token. This strategy allows training across multiple sessions and individuals. The success of the approach was extensively tested in several datasets across multiple labs, brain regions, and tasks, demonstrating increased decoding performance compared to alternative methods.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The paper is presented clearly and is technically sound. The method was tested and shown to work well across multiple datasets. The proposed strategy leverages the success of transformer models and applies it to neuroscience by introducing a novel way to tokenize neural activity. Extracting shared variability across multiple datasets is crucial for neuroscience and biomedical applications, and this approach could provide a framework for analyzing these emerging datasets. The authors demonstrated the success of the approach when trained on multiple neural recordings and tasks, with the ability to pool data across sessions and animals. Decoding results showed that the method outperforms alternative approaches, including lesion versions of the introduced model, when tested within and across sessions and animals. They also investigated the impact of different architectures and numbers of parameters on performance. Importantly, they showed that minimal fine-tuning allows the model to improve decoding performance on other tasks. The proposed strategy could lay the foundation for studying unifying principles of neural computation.

Weaknesses:
However, it should be noted that this strategy heavily relies on having access to multiple large neural recordings, which may not be feasible for most basic neuroscience research where datasets are often limited. While the authors tested the method across multiple brain regions and tasks, all of them were motor tasks in motor-related areas. While this choice likely facilitated the identification of shared structure and enabled transfer learning, it would be even more intriguing if this approach could be applied across categorically different tasks, such as sensory encoding and decision-making, spanning from sensory areas to higher cortical regions. This could potentially help identify universal coding principles of neural computation as well as task-specific coding principles related to motor function.

Limitations:
The authors mention some limitations and future work in their paper. However, it would be important to acknowledge the computational costs, training times, and data demands associated with the proposed method. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel method for developing models that can predict the activity of neural populations by learning from data recorded across different sessions, tasks, and animals. The method is uses a custom tokenization procedure for spikes and a deep neural network architecture based on PerceiverIO. The paper presents the core method, then describes a technique to reuse a large pre-trained model to learn efficiently to predict a new neuronal population/condition, and finally presents a series of validation experiments.



Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- the proposed technique is technically solid, elegant, and performs well in the experiments.

- this technique addresses a real need in neuroscience labs, making the paper potentially very high impact.

- I found the scaling analysis (lines 237-257) particularly instructive, and especially relevant because for a new method like this one the reader may wonder what type of performance they may expect on their own data, as a function of the architectural choices they would have to take when deploying the method. This is of course a very hard thing to assess correctly, but this type of analysis is a great starting point.

- the paper is very well written.


Weaknesses:
- I have noticed a few minor issues (mostly related to clarity and plots) in section 3.4. See below under ""questions"".

- The discussion of related literature seems to imply that all spike train analysis has always been done by binning spikes. It would be good to include at least a cursory mention of kernel methods for spikes and other binless measures: see for instance Paiva, A.R.C., Park, I., Príncipe, J.C., 2010. A comparison of binless spike train measures. Neural Comput & Applic 19, 405–419.


Limitations:
Limitation of the present work are appropriately discussed. I see no potential issue with societal impact.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors describe a novel method for the task of neural decoding: using the time series of activity recorded from a population of neurons to predict the activity of scientifically relevant target variables. The describe their approach, called POYO, based upon the tokenization of spike data, the application of transformer based models to this data to generate pre-trained models, and the use of these pre-trained models for neural decoding. They demonstrate that this approach allows for accurate generalization across different sessions of neural recordings from the same animal, and across different animals. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The authors present a novel approach for the decoding of neural data, that appears promising in its ability to generalize over different animals and experimental sessions, which is a major problem in neuroscientific data analysis. The performance of their method is impressive, especially in the setting of pretrained models that are finetuned to novel datasets. The ability to combine various datasets in the way described here has the potential to be impactful. 

Weaknesses:
My main concerns are with regard to scientific rigor, and the scope of the claims made in this work.
- Generalization across brain regions. I believe there needs to be a more thorough discussion of the role of brain regions in tasks where POYO is asked to generalize across individuals with recordings from different brain areas. To what degree will region-specific neural activity impact the ability of POYO to generalize when performing few shot learning? Concretely, what would behavioral decoding results in Table 2 look like if they are broken out per individual brain areas? 
- Lines 37-39: ""Overall, this lack of correspondence across recordings and channels complicates the integration of information from different experiments and individuals, ultimately hampering efforts to construct a unified perspective on population-level interactions and dynamics in the brain.""It is not clear to me how POYO will address these challenges. While it is clear that POYO allows for highly performant decoding, the lack of recurrent structure within POYO makes it difficult to understand how this model would assist these challenges. Perhaps analysis of the attention mechanism could provide some answers to this question, but that point is not discussed in this work. 
- Comparison with existing baselines. In Table 1, it is unclear to me why the performance of AutoLFADS + Linear only provided for the NLB datasets. It would be useful to see the performance of this model on all datasets, as it is a more powerful and commonly used decoding model than the others provided. Comparison with other transformer based neural decoding approaches like NDT (Ye and Pandarinath 2021) would also provide a better perspective on the value of the methodological advances proposed in this particular work. 

I am open to reconsidering my score if my concerns here are addressed. 

Limitations:
As noted by the authors, generalizing this framework across other brain regions (as well as many other conditions) is a key limitation. In general, I have reservations about the use of the phrase ""foundation model"" without qualification in the context of neural decoding. Is the suggestion of this work that neuroscientists should aim to build a general purpose model that performs decoding regardless of model organism, brain area, or recording modality? While the results of this study are impressive, I do not believe that such claims are justified, even as future work. 

Rating:
6

Confidence:
4

";1
GGIA1p9fDT;"REVIEW 
Summary:
The paper introduces a new method for training RNNs that allows fast convergence which could make realtime parameter update possible.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- The method conceptually is simple and intuitive.
- It tackles an important problem which is the time consuming nature and computing intensivity of ML models
- It achieves impressive results through simple hacks
- One possible use of this is also quick experimentation with RNN models
- Outlier detection is included

Weaknesses:
- The methods section is easy to understand until the end of setion 2.3 where there are many equations with variables that are not defined either in the main text or in the SM (any equation in the main text has to be defined very explicitly in the main text). 
- To evaluate the value of this implementation on the proposed application, I would appreciate a comparison of the speed of computation to the sampling rates of data collection and intervention delivery.
- I would have appreciated results experimenting on real data and comparing neural dynamics computed via this method compared to other methods

Limitations:
NA

Rating:
8

Confidence:
4

REVIEW 
Summary:
The motivation behind this paper is to replace FORCE/RLS fitting of biologically-flavored RNNs to data by a faster and more robust alternative training scheme. This is accomplished by 1) altering the training objective to a cross-entropy loss; 2) using an approximate pre-computable Hessian to facilitate the use of Newton steps; 3) use of a suitable fixed point initialization; 4) using teacher forcing (training only one-step losses); 5) employing ADMM to enforce constraints (e.g., no self-excitation). Experiments show improvements to various metrics, including accuracy of recovered weights, fits to observed activity, and runtime. 

The stated goal of the project is to produce a method fast enough for real-time analysis, though this work focuses only on weight inference and fitting accuracy of the firing rates. Still these models are increasingly used in the field to fit neural data, so better algorithms for doing so are needed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Good algorithm performance as assessed along multiple dimensions (speed, inference accuracy, reconstruction accuracy).
- Thoughtful comparisons to other baseline models.
- Emphasis on real-time and streaming settings.
- Experiments targeting model mismatch.
- ADMM approach allows for many different types of potential constraints.

Weaknesses:
- Addresses some key sources of potential model error but not others (e.g., correlated noise, wrong nonlinearity, mismatch between network timescale and measurement timescale for some data types).
- No experiments on real data. Even if accuracy of weight recovery is impossible to assess, reconstruction accuracy would be.

Limitations:
1. Equation (1) is not the usual form for the equations defining these networks (e.g., refs 22, 24). Typically, the leak term is on the voltage variable, not the firing rate, and the nonlinear transfer is from voltage to firing rate, not input current to effective drive. It looks to me as if this choice is essential to make the cross-entropy convex in $\theta$. If so, this should be discussed, since it's a qualitatively minor change that has large implications for the feasibility of the approach.

2. As noted above, it seems to me there are five distinct technical pieces, and it is not always clear what is doing the heavy lifting. For instance, Figure 3 (and S3) seems to indicate that initialization in these models is doing a huge amount of work, which is unsurprising but not well emphasized in the text. Better delineation in the main text of which choices are most important to the results would be helpful.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper proposes a convex loss for training data constrained recurrent neural networks (dRNNs). This is achieved by performing the optimization in the $d_{t,i}$ space instead of $r_{t,i}$ space (as defined in the paper) and using cross-entropy instead of $l_2$ loss. The authors discuss that this choice makes the optimization convex, and using a weighting scheme they show that a pre-computed inverse Hessian (using the data correlation) makes the optimization much faster than original BPTT or FORCE algorithms. Results are shown on synthetic chaotic rate networks under model misspecification as well as noise and subsampling. Better accuracies and faster runtimes suggest that CoRNN has desirable properties compared to baselines.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* Developing tools for online experimentation and targeted manipulation of brain circuits is a significant problem. Given the current experimental advances in the neuroscience field these tools are in crucial demand to help us understand geometrical and computational properties of the brain networks. This makes the problem considered by this paper timely and significant.

* Arguably one of the main bottlenecks of this online experimentation is fitting/inference time complexity. This paper takes a step towards speeding up the optimization by convexifying the loss and pre-computing objects to avoid additional computational burden.

* The motivation is clear and the presentation of the paper is logical, understandable, and easy to follow.

Weaknesses:
Below I've included a list of questions and concerns. In summary, I believe that the experimental part of the paper can be improved by a large margin. Specifically since the method is pitched as a general purpose optimization framework it's important to present results on more complex synthetic datasets as well as real neuroscience dataset where the performance of the model is well investigated under various parameter changes. In addition, I think the discussion section can be further improved by being more upfront about the limitations of the work and paths for future work.

> 81 extraction, especially with thousands of neurons, requires GPUs, leaving only the CPU for dRNNs.

* I'm a little confused by this repeating statement, can we not use multiple GPUs? Is this a constraint for the specific hardware you're using or a more fundamental constraint?

* Eq. 1: correct me if I'm wrong but most of the theoretical neuroscience work is done on the equation where the nonlinearity is applied pre-summation $\tau \frac{dr_i}{dt} = -r_i + \sum w_{ij} \phi(r_j)$. Although from a model capacity perspective, the two models are shown to have similar capacities the one I included here is more biologically relevant. Can this be implemented in your approach and can you include some results on this alternative model? 

* Related to this, the framework seems to heavily rely on the specific instantiation of the network dynamics. For example, given a different form, we cannot immediately consider the loss form of $d_{t,i}$ and use its linear+sigmoid form. This seems to be an important limitation and some discussion on how to extend this to other forms of ODE would be helpful.


> 114 $d_{t,i} = \tanh(\hat{z})$

* Should it be $\hat{z}_{t,i}$?


* Why is (5) convex with respect to $\theta$? Is this a trivial fact that the authors leave it unexplained?

* Equation 7 is playing a crucial role in the optimization, can the authors bring the proof to the main text instead of SM?

* Based on (S4), we can in principle use other nonlinearities and a corresponding weighting function $c_{t,i}$ right?


**Figures**

* Fig. 1 seems to be over-promising. Although in principle dRNNs can lead to the applications that are discussed but those are not shown in the current work. Unless those applications are not shown in real data/simulations I recommend rewording/having a more concise Fig. 1 that focuses on the contributions. Usually, Fig. 1 is a schematic of what's done in the paper, not an idealistic scenario that can be done in future studies.

* Fig. 3: The colors are very hard to see, specifically because the error bars are too small, can the authors change the formatting to make it easier to read? After reading this paper two times I'm still unsure what ""teacher enforcing"" means in this context and how it's achieved in various percentages. Instead of these two parameter configurations, can the authors show these as line plots when changing the parameters more continuously on a certain range that includes the performance extremes? Related to this, are the claims true about this specific instantiation of the network or does it hold if we change the noise model? For example, intuitively, cross-entropy loss corresponds to the log probability of a categorical distribution, and as the authors show it provides a better surrogate for the data generated from normal distribution. What if we change the noise model to something like a Poisson or some noise model that's not independent across different time points?

* Does the result hold if we change the dynamical regime of the network? In neuroscience, previous work has hypothesized various network mechanisms such as limit cycles, multi-stability, input-driven networks, transitions between multiple attractors, etc. Is it possible to show results on these other regimes to ensure that the presented optimization framework is not beneficial for a particular regime? Can the authors include experiments with varying the dimension of the chaotic attractor or a more complex data-generating model as opposed to a simple chaotic (low-d) rate network? 

* Fig. 4B: The result is a little surprising for me, how can the accuracy go down for other methods? Aren't all of these methods guaranteed to converge to a local minimum? Are the authors controlling for the initialization of the parameters?

* Are the times reported in Fig. 4 CPU times or GPU times? 

* Fig. 5: In (A) there are these weird bumps/spikes in the network output. It looks like the errors are happening in a space that's not within the null space of the output. Whereas my experience with the flip-flop task is that BPTT errors don't look weird like this. Is this due to the specific architecture chosen or is this a result of the convex loss/optimization framework?

* Fig. 7: miss-matched -> mismatched

* I'm again surprised that the network can account for the common inputs/unobserved nodes. I think it's an established fact that by just using observational data no method can generically recover the weights in the presence of common inputs and unobserved nodes. This might be the result of the specific data-generating model that's used here. A discussion on this would be helpful to point to the limitations of the work.

* Although the work is motivated by neuroscience data and experiments, no experimental data is included in the paper and that to me is the biggest weakness of the paper. Brain data is much more complex and heterogenous and the noise model, nonlinearity, dynamical regimes, and other factors make it harder to build a robust estimator that generalizes to test trials/test time points. I would assume that various loss functions have different properties that make them more suitable to specific datasets and it's hard for me to believe that a single loss can improve loss on all datasets. Unless results on more complex datasets and simulations are shown it's hard to decide whether the presented framework is preferred to the traditional ones.

Limitations:
See weaknesses section

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors introduce a method to produce a digital twin of a biological neural network that can be inferred from measurements of neuronal activity.  The major advantage of the model is that it can be trained in real time to gain the basic dynamics of neural population. In the future this could allow advance HRI applications, like linking brain activity to controlling prosthesis.

The digital twin is trained using a randomised neural network that represents a neural population. It is used as a teacher network for student network that has a specific structure, designed in a way this is allows approximate convexification of the optimisation problem, enabling a ridiculously easy training of the network that can scale to large neural populations.  The computational cost of performing the analysis of the biological neural network scales linearly with its size.

The system is evaluated by a test, where a random neural network is initialised repeatedly with Gaussian random weights in a range that can produce also chaotic self-sustained signalling. The is assumed to provide a large enough variation for a neural signal generator


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The real time, fast learning capability of the suggested architecture. The convexificaiton, as a technology. has definitely uses and application potential beyond the domain of the paper.

Excellent figures to clarify the process.


Weaknesses:
The reasoning about the enforced use of CPU does not sound convincing to me. See the question part.

Limitations:
NA

Rating:
8

Confidence:
4

";1
SCsJFNcSHQ;"REVIEW 
Summary:
By adding various elaborations to the state-of-the-art Markov chain Monte Carlo inference algorithm, this paper achieves an efficient and effective Bayesian network inference algorithm. Bayesian networks are one of the main tools of machine learning with a long history. In general, their learning is known to be a computationally hard problem, but efficient inference algorithms based on Markov chain Monte Carlo have been actively studied. Section 2 describes the problem setup of a linear functional Bayesian network with a normally distributed weight matrix as a concrete example. Section 3.1 introduces the baseline algorithm as a state-of-the-art inference algorithm for Bayesian networks, applying the method of Liang et al. [2022]  for Bayesian variable selection problems to Bayesian networks. In Sections 3.2 through 3.4, the authors' further elaborations are carefully described step by step. Section 3.2 shows how to effectively adjust the neighborhood of the proposals, inspired by Liang et al. [2022]  for Bayesian variable selection problems. Section 3.3 introduces an effective device for (nested) sequential sampling of neighborhoods to eliminate problems that can arise in the DAG estimation problem, but not in Bayesian variable selection. Section 3.4 presents a method for properly restricting the neighborhoods, which tend to be enormous.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- This paper is a solid proposal for possible reasonable improvements in the Bayesian network inference problem, with broad coverage of the latest developments in the surrounding fields of Bayesian machine learning.
- The text is very detailed so that a wide variety of readers (from beginners to experts) can follow the history and the latest developments in the field.
- The code is provided in such a way that the proposed algorithm can be easily followed up by subsequent research, which is a very significant contribution to the field.

Weaknesses:
The candidate weaknesses listed below are based on questions I had during my initial peer review. As my misconceptions are resolved, they may cease to be weaknesses.

- The paper is somewhat unclear in its claims about the mixing time analysis (or empirical observation) of the proposed MCMC algorithm, although there are several mentions of it (e.g., Line 13, 288, 365).

Limitations:
As discussed in the above Questions, I am not sure the (theoretical) guarantee of mixing time of the proposed MCMC algorithm. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes PARNI-DAG, a new MCMC method for sampling Directed Acyclic Graphs (DAGs) that can be used for the problem of structure learning under observational data. PARNI-DAG builds on top of PARNI, and similarly uses locally informed, adaptive random neighborhood with an efficient point-wise implementation, but introduces additional improvements include: pre-tune sampler parameters using a skeleton graph derived from other methods, augmenting the search neighborhood with an edge-reversal move, and neighborhood thinning to improve computational efficiency. Experiments on some toy datasets demonstrate advantage over existing baselines.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
The proposed method seems to be an effective adaptation of PARNI to do Bayesian learning on DAGs, and outperfoms a few (classical) baseline methods on a few toy benchmarks.

Weaknesses:
I am not an expert in this field, but it seems to me this paper is largely adapting the existing PARNI method for Bayesian variable selection for DAG learning, and combining various techniques that are already present in the current literature on top of PARNI. It would be helpful if the authors can make a more compelling case for the contributions of the paper over existing work.

For example, in L62-L72 contribution, it seems up until L69 it is just describing what PARNI already has. The procedure for pre-tune sampler parameters and do warm start also seem like a trivial application of exsiting method. In Section 3 titled ""The novel PARNI-DAG"" proposal, the entire Section 3.1 seems to be just a recap of wht PARNI already has. Section 3.2 is mostly a recap of Kuipers et al. [2022]. Section 3.3 introduces the reversal neighborhood but this is also done in for example partition MCMC. Section 3.4 also seems like a simple adaptation of the thinning procedure that is already present in PARNI. In fact a large part of Section 3 seems to belong to background review rather than a description of a novel method. It would be helpful if the authors can reorganze this way and also clearly state what exactly is the novel contribution of PARNI-DAG.

Limitations:
The authors did not discuss limitations

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper presents a Markov chain Monte Carlo (MCMC) sampler adapted from previous PARNI sampler, called PARNI-DAG, which is designed for Bayesian structure learning under observational data. The authors assume causal sufficiency and target the posterior distribution on Directed Acyclic Graphs (DAGs). The proposed PARNI-DAG mainly relies on the PARNI sampler and modify it to suite the purpose of structure learning, including 1. warm-start of neighbourhood sampling probability 2. reversal neighbourhood step and 3. adaptive neighbourhood skipping probability.

The main contributions of the paper are

1. A warm-start procedure of the sampler's parameters that exploits skeleton graphs derived through constraint-based or scoring-based algorithms, ensuring better scalability and mixing property with the number of nodes.

2. A reverse step to avoid getting trapped in the local mode.

3. An adaptive skipping probability such that not all intermediate neighbourhood sampling steps are executed. 

Empirically, the author demonstrates the advantage of PARNI-DAG using real-world examples, which shows advantages compared to the previous MCMC methods. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
## Originality
The originality of the paper lies in the development of the PARNI-DAG algorithm, which combines the Point-wise Adaptive Random Neighborhood Informed (PARNI) proposal with new features specifically designed for structure learning in the space of Directed Acyclic Graphs (DAGs). The proposed algorithm addresses the challenges of mixing and convergence in high-dimensional settings, which is not adequately addressed by existing MCMC methods for structure learning. This work does not provides a completely new sampling algorithm, instead, modify the existing approaches to suite the purpose of structure learning. 

## Clarity
The paper is logically structured with clear presentation of the modifications made to the PARNI proposal. The authors have made efforts to provide intuitive explanations and motivate their choices in the development of the PARNI-DAG algorithm. The paper is easy to follow, and the appendices provide additional details on the derivations and calculations.

## Significance
The significance of the paper lies in its potential impact on the field of structure learning and causal discovery. The PARNI-DAG algorithm aims to address the challenges of mixing and convergence in high-dimensional settings. The algorithm's improved performance over existing MCMC methods for structure learning should make it a reasonable contribution to the field but there are some limitations, which I will elaborate in the following.

Weaknesses:
## Empirical experiments:
While the experimental results demonstrate the advantages of PARNI-DAG over existing MCMC methods, the experiments primarily focus on the comparison with MCMC based approach. However, in the literature review, the author also mentioned several structure learning approach. Although the main claim of this paper is the improvement over existing MCMC, it is still beneficial to include a comparison to state-of-the-art Bayesian structure learning approach like [1,2,3].

[1] Cundy, C., Grover, A., & Ermon, S. (2021). Bcd nets: Scalable variational approaches for bayesian causal discovery. Advances in Neural Information Processing Systems, 34, 7095-7110.
[2] Lorch, L., Rothfuss, J., Schölkopf, B., & Krause, A. (2021). Dibs: Differentiable bayesian structure learning. Advances in Neural Information Processing Systems, 34, 24111-24123.
[3] Geffner, T., Antoran, J., Foster, A., Gong, W., Ma, C., Kiciman, E., ... & Zhang, C. (2022). Deep end-to-end causal inference. arXiv preprint arXiv:2202.02195.

## Limitation with linear model
The proposed PARNI-DAG mainly targets at the linear model with Gaussian assumptions because the marginal probability are needed. For general nonlinear model, such integration is not tractable, which rendering the following PARNI-DAG proposal invalid. However, nonlinearity is everywhere in the real-world applications. To make the paper stronger, the author should discuss the implication of using linear model or demonstrates that linearity assumption does not harm the performances too much. This can be achieved by comparing some of the previous mentioned baselines [1,2,3]

## Computational complexity
Although the proposed PARNI-DAG method uses many modifications to reduce the computation cost, this approach relies on the MH step to correct the bias. It is known that MH step scales linearly with the number of datapoints, which can be a huge computational bottleneck for large dataset. I suggests the author should explicitly discuss the computational complexity or limitations, and potential approach to remove this constraints. 

Limitations:
The author does not explicitly write a limitation section, I have made suggestions to include one like computational challenges and limitations of using linear model. 

Rating:
5

Confidence:
4

";1
tF7W8ai8J3;"REVIEW 
Summary:
This work aims to address the challenges of imbalanced data in FL. To this end, the authors propose to optimize AUC score. Some experiments are conducted to verify the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper is easy to follow. The notations are well-defined. The studied problem is promising.

Weaknesses:
This work confuses me a lot.

The authors believe that data imbalance is a crucial issue under the FL scenario, which is consistent with my understanding. The problem has motivated the data-split fashion, i.e., Latent Dirichlet Sampling [1]. Moreover, many excellent works have demonstrated the effectiveness of their efforts in mitigating data heterogeneity [2, 3]. However, I cannot find these works in this work. This indicates that the authors may overlook some advanced methods in this field.

[1] Measuring the effects of non-identical data distribution for federated visual classification
[2] Federated optimization in heterogeneous networks
[3] SCAFFOLD: Stochastic controlled averaging for federated learning


Limitations:
cf. Weaknesses

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper firstly studies the federated compositional AUC maximization problem, which includes both the local and global imbalanced distributions, and proposes the momentum-based algorithm LocalSCGDAM to solve this problem. The SOTA convergence rates are established and various experiments are used to evaluate the proposed algorithms. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Novelty: This paper is the first work to consider the federated compositional AUC maximization problem. Data heterogeneity is a key problem in federated learning and many works focus on it.  The Federated compositional AUC maximization problem is more challenging because it considers both the local and global imbalanced distributions.

Quality: It proposes a new algorithm to solve the problem. The structure of the algorithm is clear.  Both theoretical analysis and experimental verification are provided. The theoretical analysis is very complete.

Clarity: The paper is organized well and it is easy to follow. 



Weaknesses:
1. More motivation should be introduced. It includes why 1) AUC is significant and 2) why FL AUC should be considered.

2. In the experiments, all samples are set to 0.1. Discussion about different imbalanced data settings is welcomed. 




Limitations:
No

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper proposes a new federated learning algorithm to address the class imbalance problem. Instead of using cross-entropy loss functions, the proposed algorithm directly optimizes the AUC score by solving a federated stochastic compositional minimux optimization problem. Specifically, the paper proposes to employ the local stochastic gradient with momentum to update the local model parameters. Experiments show that the proposed algorithm achieves better model performance than the other baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper considers a setting where the global distribution is also class-imbalanced, which is interesting.

2. The paper provides a theoretical analysis of the convergence of the proposed algorithm.


Weaknesses:
1. There are many FL studies that try to address non-IID challenge in FL. However, the baselines seem to focus on different optimization methods and lack SOTA FL studies that address the non-IID data (e.g, [1][2]). 

[1] Addressing class imbalance in federated learning
[2] No fear of heterogeneity: Classifier calibration for federated learning with non-iid data

2. The theoretical analysis requires assumptions on the outer-level and inner-level functions. I’m not sure how realistic these assumptions are.

3. The algorithm estimates the inner-level function in local training. It may not be applicable in the client sampling setting, where a client may be selected after many rounds.



Limitations:
One potential limitation is that the algorithm may not work well in the client sampling setting.

Rating:
5

Confidence:
3

";1
B5LpWAaBVA;"REVIEW 
Summary:
This paper introduces an extension to model-free RL that has its roots in non-stochastic control. The main idea is to adjust a nominal control of a base policy (from e.g. a RL algorithm) by a control signal that can be computed from estimates of the process noise. As a main contribution, the authors present three distinct methods to compute those noise estimates.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Control theory provides us with many ideas that can be exploited and possibly transferred to model-free RL algorithms. Thus, I think it is important to think about how such ideas can be utilized and adapted to make RL-algorithms more reliable, sample-efficient etc.
- The ideas are formally justified and the derivations appear to be mostly correct. For LDS, the authors provide convergence proofs of their method.
- The paper is well structured and well written. The contributions are clearly set out and the structure of the paper allows the reader to follow along easily.
- The authors provide example code for some of their experiments.

Weaknesses:
- The results suggest, that PD1 produces similar results to the baseline and that PD2 was only able to really outperform its baseline on one task (noisy hopper). The only PD-estimator that consistently beat its baseline was PD3, which uses a simulator to compute the disturbance signal. Now, the question arises, whether this is still a model-free algorithm or not, since we would need this model also during inference. The paper would benefit from more evidence, that at least PD2 provides stronger performance than the baseline. Furthermore, it would be interesting to see if you could use a learned model to compute the disturbance and how this would affect performance. 
- In the definition of PD(1), in Line 169, the term on the right-hand side is called ""gradient of a TD error."". I do not think that this is an accurate statement. Recall that the TD-Error is defined as
$$c(x_t, u_t) + \gamma V_\pi(x_{t+1}) - V_\pi(x_t).$$
From the definition of $Q$ and $V$ and $\mathbf{\hat w_t}$, we can derive
$$
\begin{align*} 
&\gamma V_\pi(f(x_t, u) + w_t) - (Q_\pi(x_t, u) - c(x_t, u)) \\
&=\gamma V_\pi(\hat x_{t+1}) - (\underbrace{c(x_t, u) + \gamma \mathbb E_{x_{t+1}}[V_\pi(x_{t+1})]}_{=Q(x_t, u)} - c(x_t, u)) \\
&= \gamma (V_\pi(\hat x_{t+1}) -  \mathbb E_{x_{t+1}}[V_\pi(x_{t+1})]),
\end{align*}
$$
which I would not call a TD error (it does not measure the consistency of the value function over timesteps). It appears to me that it is more like a measure between the expected value of the next state and the value of the predicted next state.
- In the evaluation, the plots are kind of hard to read. I would recommend to increase the font size for the axis/legend, especially for Fig. 3.

Limitations:
The authors did not dicsuss the limitations of their approach.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This work introduces the notion of disturbance-based policies for model-free reinforcement learning, as opposed to traditional state-based policies. The disturbances capture unmodeled deviations from observed dynamics. In the model-free setting, since these disturbances are not known to the learner, the paper proposes three signals which can be used as pseudo-disturbances. These include the gradient of the TD error, the difference between auxiliary value functions of consecutive states, and the difference between the observed state and a simulated state. These three signals each require different assumptions and have their own advantages and disadvantages. These can recover the true perturbations up to linear transformations assuming time-invariant linear dynamical systems and a linear policy. A new algorithm, MF-GPC, is introduced which can adapt existing RL methods under this framework. Notably, this approach has sublinear regret bound under certain assumptions. Experiments on noisy versions of OpenAI gym environments demonstrate the effectiveness of the proposed method. 

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The paper presents a novel paradigm for model-free reinforcement learning based on unmodeled disturbances. There is an existing line of work on disturbance-based techniques which employ model-based control. However, this paper is the first work to extend this approach to the model-free setting, to the best of my knowledge. 

The use of various signals as pseudo-disturbances is an original idea and the three proposed variants seem sound. The mathematical guarantees that these pseudo-disturbances are linear transformations of the true disturbance for linear systems, as well as the regret bound for MF-GPC, are valuable contributions and strengthen the quality of the paper. The concepts introduced in the paper are presented with sufficient clarity.

Weaknesses:
The practical applicability of the proposed framework raises some concerns. Among the three proposed pseudo-disturbances, PD3 requires an accurate simulator which is in most cases not available. PD2 can be applied to specific systems where there are additional signals available from the environment. PD1 seems to be the one most generally applicable to typical RL environments and dynamical systems, however it is unreliable and the empirical results demonstrate the ineffectiveness of this signal. See questions for detailed comments.

Another consideration is that the analysis considers linear dynamical systems and the regret bounds and the guarantees require further assumptions. While such constraints are typical when performing rigorous analysis, it does raise the question of how generally applicable this framework is to various other types of systems.

Limitations:
The paper does not address the limitations of the proposed framework. As mentioned in my review, the main limitation seems to be the usefulness of the pseudo-disturbances to generic environments and systems. In addition, the analysis is limited to linear dynamical systems, and it is not clear how this framework extends to other types of systems. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work considers model-free RL in the setting with additive disturbances in the environments' forward dynamics. In particular, it focuses on ""disturbance-based policy"" which adds a correction policy to the vanilla state-based policy. Because the disturbances are unknown in model-free RL, the correction policy instead conditions on ""pseudo-disturbance"" which are proxies of the actual disturbance. The paper proposes and analyzes 3 pseudo-disturbances that are feasible to compute in a model-free RL setting. The main algorithm trains the disturbance-based policy in a typical model-free RL training loop and is independent of the base RL algorithm. The authors provide theoretic guarantees of the method under linear settings. Empirical results show that the algorithm brings substantial improvement in a collection of noisy control tasks.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The problem setting is well-motivated and clearly formulated.
2. New concepts are nicely explained.
3. The different choices of pseudo-disturbance make sense intuitively and are well supported for their properties in the linear case.
4. Empirical results show significant benefits from the algorithm.

Weaknesses:
1. The authors mention potential adversarial disturbance but empirical results are shown in environments with uniform additive noise. The paper would be stronger with experiments containing adversarial disturbances.
2. The gym experiments are done with only one baseline method. Max-entropy RL algorithms like Soft Actor-critic might be more robust to noises in the environment. I would be more convinced if the method is compared with a few more state-of-the-art methods.

Limitations:
The authors cover the limitation of each pseudo-disturbance. They are also upfront about PD1 not performing well in RL.

Rating:
6

Confidence:
3

";1
CbsJ53LdKc;"REVIEW 
Summary:
This paper explores the ability of a LLM to engage in roleplay, and how being prompted to do so influences the models' generations. In particular, they focus on exploring how models prompted to behave like children seem to do so accurately using comparisons from the human developmental psychology literature.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
**Originality:** This paper is not particularly original, but that is not something I count against it. Well executed experimental evaluations of ""common knowledge"" are an essential part of science.

**Clarity:** The paper is clearly and compellingly written.

**Significance:** It is difficult to assess the significance of this paper due to the concerns about the experimental results I identify below. If the authors' claims are correct then the paper would be highly significant.

Weaknesses:
**Over-Claiming**

The paper makes several false or overstated claims about its results and those of others. For example, it characterizes its relationship with the existing psychology literature as follows:

> Importantly, LLMs impersonating older participants generate higher average rewards (β = 0.15, p < .001), thereby replicating a general pattern found in the developmental literature [59].
> 
>  Lastly, we analyze how regression weights of the probit-regression were influenced by the age group the LLM is impersonating. Figure 2 (bottom right) reveals that LLMs pretending to be older explored their environment less (β = −0.06, p < .001) and exploited more (β = 0.05, p < .001). This pattern is in line with several results from the psychological literature which also found that children explore their environment more [60]. These results suggest that impersonating LLMs can recover human-like developmental stages of exploration.

These results are not obviously consistent with the claims made in [59] and [60]. For example, [60] finds ""... the two groups of children did not differ significantly in their extent of generalization"" and ""... exploration did not reliably differ by age group."" The only age group studied in this paper that would be considered an adult by [60] is the 20-year olds, and its not at all clear that the effects in this paper are qualitatively or quantitatively similar to the ones in [60]. For [59], I am unable to identify which claims specifically the authors think that they reproduce. These problems are exacerbated by the fact that the authors do not release the raw data for me to analyze.

The authors claim ""Vicuna is competitive with proprietary services such as ChatGPT,"" however according to their own source users prefer the results from ChatGPT to Vicuna over 60% of the time. Additionally, when looking at how likely a user is to find the two models substitutable, there is consistently a gap of around 10% in how often users prefer ChatGPT over Model M compared to how often they prefer Vicuna over Model M for each third Model M in the benchmark.

The authors claim

> For both LLMs, in both datasets, we observe that with increasing age, the complexity of the vocabulary and the attributes of the mentioned objects increases. A 2-year-old persona talks about the sound the bird or the car makes, the shapes of the wings or wheels, and the emotions attached to seeing or riding it. A 4-year-old persona interestingly mentions experiences seeing the bird or the car more distinctly. A 7-year-old persona starts using more complicated adjective phrases, e.g. can drive on rough roads and outside places, whereas a 13-year-old persona takes it one step further, e.g. brownish-gray body with distinctive rusty colored markings. Finally, a 20-year-old persona makes a more complete description of the object including where the bird is found or what the car is mainly used for. This is in line with [68] where the authors show that given the same length of text, smaller children use less diverse vocabulary, repeat a lot, and focus on non-academic vocabulary.

**Missing Citations**

[Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm](https://arxiv.org/abs/2102.07350) was one of the earliest papers exploring personas in LLMs. It introduces a ""Master Translator"" prompt and finds that performance on translation tasks improves when using it. The same authors also wrote [Role-Play with Large Language Models](https://arxiv.org/abs/2305.16367), which is likely simultaneous with this paper and not something I judge the authors negative for omitting. However I think that the paper would be enhanced by drawing on some of the content in the paper.

The authors touch on Text-to-Image and VLMs, but largely omit reference to the extensive literature on using persona-like prompts to steer image generation. Most relevant is [VQGAN-CLIP](https://arxiv.org/abs/2204.08583) which explicitly discusses this phenomenon in terms very similar to the submitted paper, including prompting the model to generate ""a child's drawing of an X."" Other papers such as [GLIDE](https://arxiv.org/abs/2112.10741) and [ImaGen](https://arxiv.org/abs/2205.11487) also prompt the models to impersonate specific artists and/or their styles.

Limitations:
The authors did adequately.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper tries to understand whether LLMs can take on different personas when they do in-context generation. They have tested different settings: In a two-armed bandit setting, they ask LLMs to act as different-aged people; in a reasoning task, LLMs are asked to behave as different-level experts; and in a vision-and-language task, multiple roles have been set to the LLMs. The authors find while impersonation can improve model performance, such methods also reveal the biases in LLMs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Test their hypothesis across different domains and with various persona aspects. 
- They have set the persona specific to each task, e.g, expertise-based persona for reasoning ability analysis.
- Well-presented results.

Weaknesses:
- LLMs seem to be sensitive to prompts [1]. And I'm wondering how stable the conclusion will be. 
- I'm curious to what extent the LLMs align with human behavior. For example, in the two-armed bandit task, the authors observe ""LLMs could reproduce human-like developmental stages of exploration behavior"". And I'd be curious how humans (from different ages) perform the same task. Similar to other tasks.

[1] Arora, Simran, et al. ""Ask me anything: A simple strategy for prompting language models."" arXiv preprint arXiv:2210.02441 (2022).

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
Language models can play different roles using prompting techniques (e.g., adding ""you are a 3-year-old boy"" at the end of the prompt). This paper investigates whether roles in the prompt affect language model performances on three tasks: multi-armed bandit, language-based reasoning, and visual category description. The paper found that: 
1. Language models exploit more in multi-armed bandit tasks when prompted to be older. 
2. Language models perform better when prompted to be task experts. 
3. Language models describe objects better when prompted to be older. For certain object categories, language models show biases (e.g., describe cars better when prompted to be a man instead of a woman).

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper focuses on an interesting and novel topic: how role-playing (impersonation) affects language model performances on downstream tasks. This is a less-known research question in the NLP community.
- The finding about performance differences when describing certain object categories, such as cars, when prompting language models to be different genders provides a novel perspective to understand biases in language models and our society.
- The paper is generally well-written and can be easily understood. However, the writing should be more focused. For example, in the first paragraph of the introduction, I did not get any connection between Min et al. found prompting LLM with random labels will not degrade model performance and your research topic impersonation. Similarly, there is no need to introduce CLIP training when introducing the visual category description task.

Weaknesses:
- While the topic is to study how different roles affect language model performances, from experimental results, ages have the most significant impact on model performances, and other factors have much more minor effects on model performances. This seems obvious- it is easy to understand older people can exploit more in multi-armed bandit tasks and describe object categories more clearly. However, the most exciting finding, where gender/race can affect model performance on tasks, is not well studied. They only study two object categories from the visual object description task, and the performance difference between genders/races is much smaller and insignificant based on the plots. More systematic studies on language model biases during impersonation should significantly improve the work.

- Experimental results can become more solid using more prompt variations and model families. Language models are well-known to be sensitive to prompts; the authors should mention how robust their results are when changing some words in the prompt. Moreover, more model families besides Vicuna are appreciated, especially on bias experiments. Will the finding about gender bias hold for different model families?

- Besides the tasks with deterministic labels, it would also be interesting to show how impersonation affects task performances on non-deterministic tasks, such as social opinions or morality tasks. Since the major goal of impersonation is to simulate different people, it would be more interesting and convincing to show that language models indeed reflect these group opinions on non-deterministic tasks when prompted to be them.

Limitations:
See weaknesses. I'm happy to increase my rating if these are well addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a comprehensive evaluation on the downstream impact of impersonation with the use of LLM and multimodal model (specifically CLIP). There is a couple of interesting finding that LLM, when impersonated, aligns to some behavior carried by the people in the respective norm. This happens in ages, races, genders, and degree of expertises. The finding seems to suggest that one can exploit such phenomenon to make a large model perform better on some downstream tasks. This paper also suggest that such exploitation should be concerned. The arguments are of conflict, but also interesting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper introduced three types of evaluation using recent LLM and multimodal CLIP to probe LLM's downstream performances: exploration-exploitation, reasoning (MMLU), and shallow multimodal understanding. There is an very interesting finding that impersonation aligns to different behavior and downstream performances on these benchmarks. An immediate takeaway is that it gives an ""interface"" to easily boost downstream performance with a discovered stereotype to impersonate. The respective concern raised in this paper is also of value to the community. In general, I am happy to see such argument and findings to appear in the conference, as long as the argument goes well.

Weaknesses:
A few downsides:
- On one hand, this paper seems to suggest one can easy explore stereotypes to impersonate and use them to improve some downstream performances. On the other hand, this strategy is of special concern, and I believe, should be avoided. ***It's unclear what this paper's take is.*** A very simple and immediate counter argument to this paper is that, such exploitation aggregates the existing stereotypical biases in the real world. And if this concern is valid, then this paper shouldn't advertise it as a strength.

- Not clear if the observation is robust against prompt variance. The only place seems to have robust eval is the two-armed bandit experiment.

- The age-related evaluation is narrow. A more comprehensive range should go beyond age 20. This can be useful to probe for potential age biases and have a more comprehensive picture about the age ""mimicking"" phenomenon.

Limitations:
Implicitly or explicitly, this paper suggests the exploitation of stereotypes to improve model performances on downstream tasks.

Rating:
6

Confidence:
3

";1
BT03V9Re9a;"REVIEW 
Summary:
This paper presents a distillation technique called EmbedDistill for Information Retrieval, aiming to align the pooled representations of various IR models. The focus of this study revolves around two distillation paradigms: DE-to-DE distillation and CE-to-DE distillation. The former involves straightforward embedding matching, while the latter utilizes special tokens for alignment.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. Embedding matching is a practical method for distillation. This work pioneers the application of feature mapping to the distillation of Information Retrieval (IR) models, contributing to the existing body of knowledge in this area.
2. The experimental findings presented in this study reveal certain positive outcomes when applying EmbedDistill to IR models.


Weaknesses:
1. The concept of embedding matching lacks novelty and has been extensively explored in prior studies [1, 2, 3]. Pooling techniques are also commonly employed to handle the matching of intermediate features.
2. The theoretical proof, while complex, does not significantly contribute to understanding the proposed method, as it is essentially a standard feature mapping approach already described in previous works.
3. There appears to be no direct correlation between the term ""Geometric"" and the proposed method. If a straightforward matching technique can be considered a geometric algorithm, it implies that most feature-based KD methods could also be classified as geometric. 

[1] Jiao, Xiaoqi, et al. ""Tinybert: Distilling bert for natural language understanding."" arXiv preprint arXiv:1909.10351 (2019).  
[2] Romero, Adriana, et al. ""Fitnets: Hints for thin deep nets."" arXiv preprint arXiv:1412.6550 (2014).  
[3] Hofstätter, Sebastian, et al. ""Improving efficient neural ranking models with cross-architecture knowledge distillation."" arXiv preprint arXiv:2010.02666 (2020).  
	

Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper proposes EmbedDistill, a novel distillation approach for IR that directly aligns the embeddings from the student and teacher. The authors conduct a theoretical analysis of the teacher-student generalization gap, strengthening the importance of embedding alignment and suggesting the use of asymmetric encoders. The architecture can be directly applied in DE to DE distillation. For CE to DE distillation, the authors propose dual-pooling to obtain document and query embeddings from CE.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:


- The authors show theoretically that embedding matching can close the teacher-student generalization gap in IR setting, and support the use of asymmetric conﬁgurations.

Weaknesses:


- The authors claim that EmbedDistill can be used for CE to DE distillation, but the empirical results show poor performance. It only improves the score distillation method from 33.3 to 33.7 (+0.4), while the teacher score is 37.0. This raises doubts about the effectiveness of EmbedDistill in CE to DE distillation. EmbedDistill appears to only work in DE to DE distillation, which is an easier scenario where embedding matching is trivial. This significantly weakens the contribution of this work, as EmbedDistill can’t learn from CE, the stronger architecture.
    - One possible reason for this is that the embeddings extracted from a dual-pooled teacher may not be good. Since the query and document embeddings are concatenated in the re-ranker, the model can simply make the representations of the two close to each other when the query and document are relevant, without properly distributing the representations as required in the DE retrieval scenario. This poses a major challenge for embedding matching in CE to DE distillation, and the authors do not seem to address it adequately.
- There are errors and inconsistencies in the formulas presented in the paper and the appendix. For instance, in line 130, $R\left(s^{\mathrm{s}}, s^{\mathrm{t}} ; \mathcal{S}_n\right)$ is not defined (should it be the empirical risk?). In the proof of Lemma C.3, the authors use $K_Q$ and $K_D$ in Eq. (25) but use $K$ elsewhere. The last line of Eq. (26) is incorrect; it should be $=\mathbb{E}\left[\left(1-y-\left(1-\sigma\left(F(q)^{\top} G(d)\right)\right)\right) f(q)^{\top} g(d)\right]$. The authors should thoroughly check all formulas and correct any errors.
- The document needs improvement in terms of writing quality.
    - Section 5, which covers the result analysis, is difficult to follow.
        - The authors use too many parentheses to add additional explanations, which interrupts the reading flow.
        - The captions of the tables contain too much information.
        - The teacher performance is presented in the content of the table of NQ test and BEIR, but is described in the caption of the table of NQ dev and MS MARCO dev, which can confuse the reader.
    - The author uses too much italics in the introduction, which obscures the key points of the paper.

Limitations:
As pointed out in the Weaknesses, EmbedDistill does not seem to work in CE to DE distillation, which limits the contribution.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to learn dense neural IR models by distilling not only the teacher scores but also by aligning the learned representations.

The paper has a theoretic part that motivate the actual embedding distillation with two propositions. The first one shows that the teacher-student gap is bounded by an expression where some terms are the difference between teacher/student embeddings. The second bounds one of term of the expression by leveraging the $u$-covering number of function classes for teacher and student.

Experiments show that using a regularization based on embeddings improve results in two settings: dense to dense and cross-encoder to dense distillation. In the latter case, the authors propose a simple but original approach to get a document and query representation.

The experiments are conducted on the Natural Questions and MS-Marco datasets. 



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This modification of the distillation procedure can easily be applied to all dense model, so the impact of the paper might be important in changing the mainstream procedure.

There is also a nice result showing that just using embedding alignment (without the other part of the distillation loss) words pretty well.

The mathematics justifying the approach are also interesting, by integrating the distance between teacher and student embeddings in the picture, but the bound do not seem to be of practical interest (apart from motivating the approach) and thus might not need to be that central in the paper (esp. proposition 3.2).

Weaknesses:
In the experiments, the training based on query generation should not be the last model modification – it should rather the query/document embedding part, since query generation is not the core method proposed by the authors. This would also allow to see what exactly is brought by aligning the embeddings.

The cross-encoder based distillation (which is the main one used by dense models) is quite disappointing in term of performance (even the best CE model performs worse than the best dense model, which should not be the case)… and suggests that to properly train a dense model with their method, one needs a very well trained dense models (but still, distilling to a lower-capacity model is interesting in that case, although the difference between the two procedures is not discussed or experimented with.

The discussion 196-206 is not really needed (it is quite obvious that aligning with the `[CLS]` does not make sense). 

Limitations:
The limitation section is quite generic apart from the first sentence that could be more discussed. Also, the fact the CE distillation is not that performant is (at least for me) a major limitation (since cross-encoders are so much powerful), but it is also a challenge for this type of method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposed to use embedding matching task for knowledge distillation for IR: unlike in traditional knowledge distillation where the matching score of query and document is used, the embedding matching tasks try to align the embedding representation of query and document. It works both for dual-encoder model (DE) and cross-encoder model (CE). The student model learned achieves 1/10th of size while retraining 95-97% of the teacher performance.

This work presented a theoretical analysis of the teacher-student generalization gap for IR settings. The student DE model proposed has an asymmetric configuration where the query encoder is smaller than document encoder, and the later is frozen during knowledge distillation. This could reduces inference latency at query time.

To validate the effective of the proposed method, experiments on common benchmark for IR is used. The model achieves competitive results on Natural Questions, MSMARCO and BEIR (zero-shot IR benchmark).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* In-depth theoretical analysis of the teacher-student generalization gap in IR models, which inspired the embedding alignment methods.
* Extensive experimental results to validate the proposed method: it achieves competitive results on both Natural Questions, MSMARCO datasets and also the BAIR datasets in zero-shot cases. 
* Overall well written and easy to follow.


Weaknesses:
Overall, the technical contribution presented in this work isn’t super strong in my opinion. Knowledge Distillation is known to be effective for IR, both for DE and CE models. The embedding alignment techniques has also be explored in other settings. Although specific technical challenges needed to be addressed in applying embedding alignment for knowledge distillation of IR models, it does not seem to be groundbreaking.



Limitations:
The authors addressed the limitations of the work where it only studied transformer-based models (but expect it to work for other models such as MLP). It only studied IR in text and did not extend to multi-modal cases. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The present study introduces a new technique, EmbedDistill, designed to transfer knowledge from large-scale neural networks to smaller versions for information retrieval (IR) purposes. This approach utilizes the relative geometric connections between queries and documents, as learned by the more extensive teacher model, to synchronize the representations of both teacher and student models. Furthermore, it scrutinizes the data manifold to diminish disparities between the student and teacher, particularly in areas where the training data is scant. The findings suggest that this proposed method can effectively distill both dual-encoder and cross-encoder teacher models into 1/10th size asymmetric students, maintaining 95-97% of the original teacher's performance. The study's key contributions consist of a groundbreaking geometric distillation technique, a unique query creation method for enhancing distillation quality, and practical results underlining the success of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper presents a new approach to distilling knowledge from strong neural models to weaker ones for information retrieval. The paper makes several contributions that are noteworthy.

- The paper's approach is original in that it leverages the relative geometry among queries and documents learned by the large teacher model to align the representations of the teacher and student models. 
- The paper is of good quality, with a well-defined problem statement, clear methodology, and adequate experimental evaluation. The paper's contributions are remarkable in that they provide a new approach to distilling knowledge from strong neural models to weaker ones for information retrieval. The authors provide a detailed analysis of the proposed approach and compare it with existing methods. The experiments are well-designed and the results are presented in a clear and concise manner.
- The paper is well-written, well-organized, and easy to follow. The authors provide clear explanations of the proposed approach and the experimental setup.


Weaknesses:
Although there are several distillation methods proposed in this paper, the core idea of this paper is to distill representation, via L2 distance, from a strong representation model to smaller ones. And the derivation of the representation is also straightforward and brute-forcing, i.e., CLS representation or mean pooling over corresponding text. 

The backbone of the retriever is too weak, which cannot verify the generality of the proposed methodology. That is, it’s required a stronger student model to check if the validity of proposed methodology will be mitigated with stronger students. Meantime, the neural retrievers and rankers used in this work is relatively out-of-date. It’s recommended to leverage more state-of-the-art retrievers/rankers to make the experiments more convincing. 


Limitations:
Yes

Rating:
5

Confidence:
5

";0
K3BMejPSyQ;"REVIEW 
Summary:
The performative prediction problem targets real-world application using deployed models, and this method  studies the optimiazation method for such a scenario. Since directly optimiazing the non-convex objective is challenging, this paper utilizes weighted Markovian samples of states to estimate the gradient and update with designed timescale step size.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is in general well-written.
2. The motivation is clear for tackling the targeted problem.
3. The theoretical derivation and understanding are well-presented.
4. The authors conduct various experiments settings to examine the the proposed method.

Weaknesses:
1. The experiments are conducted on toy examples and illustrate the efficacy of the method. The question would arise for applying the method for more challenging task setting. Also, how does the method perform in comparison with other strategies that also fit with such tasks?

2. What the limitation of the proposed method? As it targets online deployed models and involves the Markov Chain sampling, what is the computational cost?

Limitations:
Please see the weaknesses and questions part.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes a derivative-free two-time scale  for performative prediction problem. The two-timescale facilitates a faster accumulation of samples to compute a gradient with smaller bias. For smooth nonconvex objective, the work proves a iteration complexity of $O(\epsilon^{-3})$ for the $\epsilon$-staitonary point. They illustrate results via numerical experiments.

Soundness:
4

Presentation:
2

Contribution:
1

Strengths:
1. This paper deals with an important problem: derivative-free optimization of nonconvex function under performative prediction setup. 

2. The convergence rate seems reasonable. 

3. The experiments are well-motivated.

Weaknesses:
1. **The applications are shown for squared loss. But it is not a bounded loss and does not satisfy Assumption 3.2. This leads me to believe that Assumption 3.2 is made for the convenience of theoretical analysis.**

In fact, **one of the major challenges in the decision-dependent noise or state-dependent Markov noise setup is that the algorithm is often not stable** for unconstrained optimization (see [1,2] below). Assumption 3.2 helps to avoid this issue.

**So Assumption 3.2 is crucial for Lemma E.3 and Lemma B.i (i=1,2,3,4). Then the theoretical analysis of the algorithm does not apply to squared loss which is one of the most popular loss functions.**

2. The proof techniques required are pretty similar to Wu et al., 2020 ([3] below). In fact, **the main result is almost same as Corollary 4.9 of Wu et al., 2020. It's just that Corollary 4.9 of Wu et al., 2020 is wrapped in the cover of reinforcement learning but the algorithm, underlying proof techniques, and result are same.**  Wu et al., 2020 does achieve a faster rate of $\tilde{O}(\epsilon^{-2.5})$ although I guess here the poor rate is due to derivative-free algorithm. 


[1] Liang, Faming. ""Trajectory averaging for stochastic approximation MCMC algorithms."" (2010): 2823-2856.

[2] Andrieu, Christophe, Éric Moulines, and Pierre Priouret. ""Stability of stochastic approximation under verifiable conditions."" SIAM Journal on control and optimization 44, no. 1 (2005): 283-312.

[3] Wu, Yue Frank, Weitong Zhang, Pan Xu, and Quanquan Gu. ""A finite-time analysis of two time-scale actor-critic methods."" Advances in Neural Information Processing Systems 33 (2020): 17617-17628.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies performative prediction when the data is stateful, in particular generated via a controlled Markov chain, and the learner's loss is possibly nonconvex. The paper develops a two-timescale derivative-free optimization algorithm for this setting and shows a O(1/eps^3) sample complexity for finding a point with squared gradient norm at most eps.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
There aren't many convergence results in performative prediction for nonconvex settings, so this is one strength. The stateful setting has been studied before in several papers, but this paper studies this setting under quite a bit of generality. The treatment of the stateful setting in this paper is probably my favorite treatment of the setting in the literature. The paper is very clearly written and easy to follow, also providing intuition for the ideas behind the analysis, which I appreciated.

Weaknesses:
There are some limitations to the conceptual novelty, in the sense that a similar algorithm has been studied outside of performative prediction, and the controlled Markov chain model for the distribution map has been studied. It would be good to be more precise about the differences to the recent works in performative prediction studying the stateful setting (bottom of page 2).

The problem is perhaps arguably a bit niche since it goes away if the performativity is not stateful, which is the most commonly studied observation model in performative prediction. In that case, a simple application of the Flaxman et al. ""gradient descent without a gradient"" algorithm suffices. The issue in this paper is that we can't sample from the stationary distribution directly so a naive application of the Flaxman et al. algorithm doesn't work. All this being said, the stateful setting is very well-motivated and deserves its own analyses, and this paper gives a clever solution.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper is on performative prediction i.e. a setting where the data distribution changes in response to the predictions of a learned model. The prototypical example of such a setting is where a learned model is used to make loan decisions, and then people or companies adjust their behavior based on knowledge of the learned model in order to secure more favorable loan terms. This paper extends prior work on performative prediction to the setting where the data distribution follows a controlled Markov process, where the control is given by the predictive model. While this setting has been studied before, this paper removes previous structural assumptions on (1) the loss function used to evaluate the model and (2) the dependence of the data distribution on model. Notably the loss function is not assumed to be convex, and so convergence is established to a stationary point (i.e. a point where the magnitude of the gradient is small).
The algorithm designed is based on standard methods for derivative-free optimization, with a two-timescale step-size modification that updates model parameters more slowly than the generation of gradient estimates in order to deal with high variance in the gradient estimator.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The extension of algorithms for performative prediction to the setting without convexity of the loss expands the range of techniques for such problems. The paper clearly discusses the differences with prior work and gives a concise and intuitive overview of the modifications required to make derivative-free optimization work for this setting.

Weaknesses:
While I do appreciate the generality of extending algorithms for performative prediction beyond convex losses (as well as beyond the mixture dominance assumption on the data distribution), there are a couple of limitations that arise from assuming so little about the loss and data distribution.

1. The algorithm converges to a stationary point of the performative risk, rather than to a point that approximately minimizes the preformative risk. Of course this is necessary in the completely unstructured setting considered here, but it is clearly a weaker statement than e.g. the initial work of Perdomo et. al. that showed (for strongly convex losses) convergence of repeated risk minimization to a point that is close to an actual minimizer of the performative risk. This paper should state more clearly that such a strong result cannot be hoped for in the setting considered. In particular, the claim on line 43 seems somewhat misleading as written.

2. Removing the convexity assumption seems to come at the cost of introducing an additional assumption. In particular, Assumption 3.3 requires that the distribution map $\Pi_{\theta}$ is Lipschitz with respect to the total variation distance, rather than the Wasserstein 1 distance used in prior work. This is a **much stronger** assumption in many natural settings e.g. the mean of $n$-independent random $\pm 1$ valued variables has a distribution that converges in Wasserstein 1 distance to the Gaussian distribution at a rate of $\frac{1}{\sqrt{n}}$, but has the maximum possible total variation distance of 1 from the standard Gaussian distribution.

3. The algorithm has a slower convergence rate than those in prior work, as the authors show is necessary in this highly general setting. The original paper introducing performative prediction shows that repeated risk minimization converges at a linear rate, and thus allows us to leverage whatever fast optimization method fits the particular problem for each risk minimization step. In contrast, the algorithm in this paper requires us to essentially sample many, many random perturbations of the model and then deploy/evaluate each one in order to obtain gradient estimates, making it highly impractical for real-world problems.

To summarize, the generality of the setting has several drawbacks in terms of the solution quality, performance of the algorithm, and most notably the additional assumption described in (2) above. While some of these drawbacks are provably necessary (again with the notable exception of the issue in (2)), taken together they suggest that perhaps this complete lack of structure is not a particularly good model of the types of problems for which performative prediction is interesting.


Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper considers the performative prediction problem, where the goal of learner is to optimize the expectation of the known loss function over a decision-dependent unknown data distribution that evolves according to an underlying controlled Markov chain. Authors presents a stochastic derivative-free optimization algorithm $DFO(\lambda)$ that achieves $O(d^2/\varepsilon^3)$ sample complexity using gradient accumulation mechanism and two-timescale diminishing step-sizes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The presented algorithm achieves a $O(1/\varepsilon^3)$ sample complexity under a challenging setting of Markovian data.
- The other possible types of gradient estimator were considered that creates a more complete picture in the presented setup.
- The presentation of the paper is clear.

Weaknesses:
- No presented lower bound for this problem, thus it is not clear is the presented rates improbable or not.

Limitations:
This is a theoretical paper that does not need to address the potential societal impact.


Rating:
6

Confidence:
2

";0
aIUnoHuENG;"REVIEW 
Summary:
This work studies the problem of sparse linear regression under the statistical model where the examples are drawn as zero-mean Gaussians with covariance matrix $\Sigma$ and each response is a t-sparse linear combination of the examples plus i.i.d. Gaussian noise. While classical results establish that the LASSO can computationally efficiently recover a good solution v* with a nearly optimal number of samples when the covariance matrix $\Sigma$ is well-conditioned, guarantees beyond this simple setting are substantially lacking, with a few notable exceptions. In general, not much is known beyond a brute-force approach of trying all $\binom{n}{t}$ sparsity patterns, which requires $n^t$ time. This work shows that even if $\Sigma$ is ill-conditioned, if Sigma is well-conditioned after removing the top or bottom O(1) eigenvalues (i.e. a notion of “robust” well-condtionedness), then this running time can be improved to roughly $f(t) \cdot n^3$. 

When there are a small number of tiny eigenvalues, the main problem is the existence of sparse linear dependencies among the variables. The algorithmic approach to address this problem is then to iteratively peel off a small number of these variables at a time (IterativePeeling()). This procedure gives a construction of a small dictionary of vectors such that any t-sparse vector can be written as a linear combination of this dictionary with coefficients bounded in L1 (Lemma 2.9), which in turn implies that the LASSO identifies a good solution v in the sense of bounded excess risk. The authors interpret the technique of applying LASSO to an augmented dictionary as “feature adaptation”. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
* Sparse linear regression is a widely studied yet notoriously difficult problem, and this work identifies a very natural class of inputs for which positive results can be obtained. 
* I find the discussion in Section 2.1 on the proposal of (t, alpha) dictionaries as a canonical abstraction of all existing approaches to sparse linear regression to be very interesting and valuable. It helps concentrate research efforts on this problem to this more structured approach, and I believe it may prove to be influential in following works on sparse linear regression.

Weaknesses:
* It seems like like there are no bounds on the sparsity of the approximate solution v that is outputted by this algorithm, probably due to the fact that the feature adaptation step augments the feature set with dense linear combinations of the existing variables (please let me know if I have misunderstood). Thus, the measure of the “sparsity” of this algorithm lies in the small sample complexity, rather than its ability to output a sparse solution. Thus, the guarantees of this algorithm are tightly linked to the statistical setting of sparse linear regression, and it may be difficult to adapt these techniques to the problem of outputting a good sparse solution to linear regression. However, I do find this limitation very interesting, and I wonder if there are gaps between the performance of these algorithms if one is required to output a sparse solution. Such separations are known in certain sparse recovery settings (see, e.g., https://arxiv.org/abs/1110.4414). 

Limitations:
The authors have provided adequate discussion of limitations

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper introduces an algorithm to solve sparse regression when the covariates are generated from a normal distribution with ill-conditioned covariance matrix, i.e. outlayer eigenvalues. The algorithm is based on feature augmentation where, meaning that the covariates are completed with well-chosen vectors, and is designed to provably achieve near optimal sample complexity in the studied framework.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Regarding presentation, the paper is clearly written and presented. Each theoretical result is explained and justified, which makes the paper easy to follow. Regarding the content, the idea of augmenting the features by taking the data distribution into account before solving the Lasso seems new. The paper provides interesting theoretical insights on this method, while maintaining a computational perspective, for instance when explaining the design of the dictionary.

Weaknesses:
While the main contributions of the paper are theoretical, it would have been interesting to get more experimental details on the algorithm. In particular, numerical illustrations of the behavior with respect to the conditioning of the covariance matrix (number of outlayers, gap between largest and smallest eigenvalues, ...), and of the robustness with respect to standard algorithms like Basis Pursuit (for instance in ""phase transitions"" between well and ill-conditioned scenarios), may broaden the audience and the impact of the contributions.

Limitations:
The limitations are discussed by the authors.

-----------

After reading the other reviews and the rebuttal, I increased my rating.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper presents an innovative polynomial-time algorithm for sparse linear regression in the correlated random design setting. The algorithm adapts the Lasso technique to effectively tolerate a limited number of approximate dependencies, resulting in both computational and statistical efficiency for covariance matrices with a few ""outlier"" eigenvalues. The proposed method is part of a more extensive framework of feature adaptation for sparse linear regression with ill-conditioned covariates and offers the first polynomial-factor improvement over brute-force search for constant sparsity and arbitrary covariance.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. The paper contributes a novel algorithm for sparse linear regression, adeptly adapting the Lasso to accommodate a small number of approximate dependencies.

2. The proposed algorithm exhibits both computational and statistical efficiency for covariance matrices with a few ""outlier"" eigenvalues, providing a substantial advancement over existing methods in this context.

Weaknesses:
1. The paper assumes constant sparsity for simplicity, but it is essential to explore the impact of this assumption on the main results.

2. To more convincingly demonstrate the algorithm's computational and statistical efficiency, the paper would benefit from the inclusion of supplementary numerical simulations.


Limitations:
Yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the correlated random design setting, where the covariates are drawn from a multivariate Gaussian, and seeks an estimator with small excess risk. This work provides a polynomial-time algorithm that, given Σ, automatically adapts the Lasso to tolerate a small number of approximate dependencies, and achieves near-optimal sample complexity for constant sparsity and if Σ has few “outlier” eigenvalues.

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
Sparse linear regression is a fundamental problem in high-dimensional statistics.
This paper studies a polynomial-time algorithm that automatically adapts the Lasso to tolerate a small number of approximate dependencies. In theoretical analysis, this work achieves near-optimal sample complexity for constant sparsity. The proposed algorithm fits into a broader framework of feature adaptation for sparse linear regression with ill-conditioned covariates.

Weaknesses:
1.If this work can provide experimental verification of the superiority of the proposed algorithm, it will be more convincing. For example, compare with the related work to verify the performance of the proposed algorithm in terms of time complexity and accuracy.

2.In figure 1, when the number of samples is greater than 100, is the standard deviation of adapted BP algorithm zero?

3.The presentation of references is not standardized, such as:

[38] Sara Van De Geer. On tight bounds for the lasso. Journal of Machine Learning Research, 19:46, 2018.

[22] Jonathan Kelner, Frederic Koehler, Raghu Meka, and Ankur Moitra. Learning some popular gaussian graphical models without condition number bounds. In Proceedings of Neural Information Processing Systems (NeurIPS), 2020. 

4.The organization and presentation of this paper can be further improved.

Limitations:
N/A

Rating:
5

Confidence:
4

";1
pJbEXBBN88;"REVIEW 
Summary:
The paper investigates adversarial robustness of trained ReLU neural networks when data lies within a linear subspace.  For the theoretical part, which is the bulk of the paper, the networks have two layers and only the first layer is trained.  Then the key observation is that the assumption on the data causes the training to change only the projections of the first-layer weight vectors to the linear subspace, i.e. their components in the orthogonal subspace remain as at initialisation.  Based on this, the authors prove several results, including a lower bound on the projection to the orthogonal subspace of the gradient of the network output at any point in the linear subspace, existence of a direction in the orthogonal subspace for universal adversarial perturbations, and that either reducing the initialisation scale or using L2 regularisation can reduce the norm of the projections to the orthogonal subspace of the gradients (in the latter case, the projections of the first-layer weight vectors to the orthogonal subspace are changed during training only by the L2 regulariser).  The results contains some assumptions, most notably for the universal adversarial perturbation, the width of the network has to be roughly at most a constant fraction of the input dimension.  The theory is supplemented by experiments on small examples in dimensions 2 and 3; in them all layers are trained and a network with five layers is also considered, which suggests that it may be possible to extend the theoretical results beyond the assumptions in the paper.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is a nice mostly theoretical follow-up to the mostly empirical ""The Dimpled Manifold Model of Adversarial Examples in Machine Learning"" by Shamir, Melamed, and BenShmuel (CoRR abs/2106.10151), which in particular brings in the considerations of the effects of initialisation scale and L2 regularisation.  The theorems are proved in detail in the appendix, with helpful sketches provided in the main.  The experiments and the resulting figures aid the intuition, and suggest directions for future theoretical work.


Weaknesses:
The proofs of the theoretical results are perhaps not surprising or difficult.

I believe the code for the experiments is relatively simple and essentially given by the text in Appendix E, however it was not submitted with the supplementary materials.


Limitations:
The assumption of small network width in relation to the input dimension is restrictive, although a similar assumption featured in the related paper by Daniely and Shacham (NeurIPS 2020).


Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the vulnerability of two-layer ReLU networks under adversarial attack when the data is in a low-dimensional manifold. The paper also observes that adding L2 regularization in clean training also improves the adversarial robustness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The intuition of this paper is interesting, with a high quality and a good clarity. Although existing literature considers that the vulnerability of clean training is caused by the off-manifold attack, this is the first one to study the theoretical perspective in detail.

Weaknesses:
(1) My major concern is that this paper mainly studies the side off the data manifold, but not the side on the data manifold. Some claims need more justifications. For example, in the abstract, the authors mention that ""the standard gradient methods lead to non-robust neural networks, .. and are susceptible to small adversarial L2 perturbations in these directions"". To claim that the perturbation is ""small"", the authors need to provide a result on what is the smallest on-manifold attack strength which results in an incorrect prediction. It is essential to make comparison in order to claim a quantity is ""small"", i.e., while Theorem 4.1 presents the L2 norm of the off-manifold gradient, what is the on-manifold gradient? 

(2) For the numerical experiments, please also share the code or report the observations when the training iteration T is large enough so that the loss converges to zero. In the paper

Ba, J., Erdogdu, M., Suzuki, T., Wu, D., & Zhang, T. (2020, April). Generalization of two-layer neural networks: An asymptotic viewpoint. In International conference on learning representations.

it is observed that training a neural network in regression in their setting is the same as a penalized linear regression, i.e., the training already introduces some sort of regularization. It would be great if the authors of this submission can provide the code or some more experimental results to support the claims in this submission.

Due to concern (1) and (2), my current score is 5. I like the intuition of this paper so I will consider raising my score if these major concerns can be addressed properly. For (1), a full theoretical proof may takes too much time. A brief justification would be helpful enough.

Some other comments:

(3) While Theorem 6.2 provides justifications on how L2 regularization improves adversarial robustness, it would be helpful if the authors can also provide the convergence and the generalization performance of the regularized neural network, which may potentially implies a trade-off between clean performance and adversarial robustness.


Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focus on the data lies on a low dimensional data manifold (linear subspace P). There’re no additional assumptions on the number of data and their structure (orthogonality). The paper considers the perturbation on P^\orth space. The paper claims that standard gradient descent leads to non-robust neural network; while decreasing the initialization scale, adding L2 regularizer can make the trained network more robust.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Studying the adversarial perturbation on the low-dimensional subspace is an interesting problem. 
2. The paper is well-written and easy to follow. The mathematical derivation seems sound.


Weaknesses:
1. This paper specifically assumes the dataset lies on a linear subspace, and the perturbation lies in the orthogonal direction of the subspace. The analysis of thm4.1 still relies on part of the weight being unchanged after training, and the gradient lower bound depends on the unchanged weights. To my understanding, such analysis still relies on random initialization property and therefore not much different compared to previous work, which might previous work analysis also holds under this paper’s low dimensionality assumption. To that end, the perturbation lies in the orthogonal direction of the subspace this constraint is trying to bypass the gradient update algorithm. In other words, does it mean that theorem 4.1 still holds even if you consider adversarial training?

2. Although the author motivates the idea by saying real-world dataset mostly lie in low-dimensional subspace, the experiments are based on extremely simple synthetic data. I’m wondering whether similar results hold for a simple dataset like MNIST.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper shows that on two-layer neural networks trained using data which lie on a low dimensional linear subspace that standard gradient methods lead to non-robust neural networks, that networks which have large gradients in directions orthogonal to the data subspace, and are susceptible to small adversarial L2-perturbations in these directions. The paper shows that decreasing the initialization scale of the training algorithm and adding L2 regularization, can make the trained network more robust to adversarial perturbations orthogonal to the data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and organized, and is innovative and original. The structure of the paper is clear and rigorous.

Weaknesses:
The experiments are insufficient, the number of data points used to evaluate the proposed method is few.

Limitations:
The number of data points is few, it's better to evaluate on more data points.

Rating:
6

Confidence:
4

";1
waXoG35kbb;"REVIEW 
Summary:
The paper attempts to elucidate the theoretical reasoning for the benefits seen in score matching. The author proposes a family of exponential distributions that can efficiently compute the score matching loss while having a comparable statistical efficiency to that of maximum likelihood. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper is well written/organized and easy to follow. The author does a good job introducing related theoretical information.
- Equations are extensively described with a thorough description.

Weaknesses:
- There are no experimental results. It would be nice if there were some machine learning models that were optimized with score matching and ML and compared with each other. 

Limitations:
The author has adequately addressed limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper provides an example of fitting exponential family models for which score matching 
and MLE are both statistically efficient, but MLE is computationally hard to optimize. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The strength is in the construction of an example to showcase the benefit of score matching 
over MLE. 

Weaknesses:
Several aspects  could be improved. 

1. First, for the computational lower bound, the points to evaluation loss and gradient is worst case. 
When one has samples from such distribution, is solving MLE still computationally hard? 

2. Although the aim of this paper is to provide examples to advocate for score matching, but for the 
distribution family proposed in the paper, are there simple (or simpler) estimators that are both computationally and 
statistically efficient? 

3. In the paper, the analysis of the statistical performance is quite crude, which makes it hard to see what's 
the statistical cost one needs to pay when computation is the restriction. Is there proper computation-statistic 
tradeoff for the model considered in the paper? 

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors describe an exponential family where the sufficient statistic $T(x)$ contains all non-constant monomials of degree $\leq d$, the background density is $h(x) = \exp(-\sum_{i=1}^n x_i^{d+1})$, and the parameter $\theta$ is constrained to have infinity norm bounded by $B$. Using a reduction from $3\mathsf{SAT}$, they show that under this exponential family, finding the MLE is NP hard, and thus unless $\mathsf{NP} = \mathsf{RP}$, that it would take time exponential in the random vector dimension $n$ to compute the MLE. 

They also show that the MLE has asymptotic sample efficiency $(nB)^{O(d^3)}$. They show that score matching also has asymptotic sample efficiency $(nB)^{O(d^3)}$. Unlike the MLE, score matching can be solved in time polynomial in the dimension of $\theta$, which if $d$ is considered constant, is polynomial in $n$. This is because the objective corresponding to score matching is convex in $\theta$.

This is a concrete example of a situation where score matching has a provable advantage over the MLE (same asymptotic sample efficiency, but one needs exponential time unless $\mathsf{NP} = \mathsf{RP}$ and the other is polynomial time).

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- clear contribution
- relevant related work is discussed
- this is a significant result (rigorous justification for potential benefit of score matching over MLE)

Weaknesses:
No glaring weaknesses, but the paper was a bit difficult to follow; I found it hard to connect the math-heavy theorem/lemma statements together. For example, I did not understand how Lemmas 4.2 and 4.3 built toward Lemma 4.4 on my initial read. 

Typos spotted:
- Equation under line 172, $\|\theta\|_2^2$ should be $\|\theta^*\|_2^2$?
- Line 482, $f''(x) = 56\gamma x^6 - 2\beta + 6\beta x^2$ should be $f''(x) = 56\gamma x^6 - 4\beta + 12\beta x^2$ I think, but this doesn't affect anything
- Lemma A.6, the $(2/e)^n$ should be replaced with $(e/2)^n$
- Math at the bottom of page 16, second line, you want to minimize the exponent so instead of $-BM(BM)^{-d}$ it should be $-BM(BM)^{-1}$ right? I don't think this affects anything though since you bound it by $-1$ regardless
- Math under line 519, I'm not sure that $(n+\ell)\log(2L) + 2 + n\log(BM) \leq \frac14 L^{d+1}$, unless some lower bound on $d$ is assumed? Like I'm not sure if that holds for $d=1$?
- Lemma B.3, bound on Laplacian in the Lemma statement doesn't seem to match the derivation (exponent of 2d vs 4d)
- Line 564 - this appears to be claiming that $\sum_{j=0}^k \binom{k}{j}^2 = 2^k$, which is false? I think you could bound it by $2^{2k}$ though by Cauchy-Schwarz, so it probably doesn't matter, just changes some constants down the line.
- Line 573 - $B_\iota \to W_\iota$
- Line 583 - $\alpha_i \to d_i$

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors present a mathematical setting where Score Matching (SM) method has more statistical benefits than Maximum Likelihood (ML) technique, when estimating a parameterized probability distribution $p_\theta \in P(\mathbb{R}^n)$ known up to a normalizing constant $Z_\theta$. In particular, they describe an explicit exponential family of distributions $F$ for which SM loss $L_{SM}$ is efficient to compute, with same statistical efficiency as ML loss $L_{ML}$ (Theorem 2 and 3), while $L_{ML}$ is shown to be intractable in polynomial time depending on the parameters of this family (Theorem 1). Given an odd integer $d$ and $B>0$, any distribution $p_\theta \in F$ is notably defined by (i) its vector of sufficient statistics, which consists of all monomials in $x^1,..., x_n$ of at least degree $1$ and at most degree $d$, and (ii) its parameter $\theta$, which lies in the $\ell_\infty$-ball with radius $B$. This work is the first one to put in perspective the statistical efficiency of SM and ML for a large family of continuous probability distributions. 


Three main theoretical results are stated here. In Theorem 1, the authors prove that, for any $p_\theta \in F$ (with $d=7$) and any set of $N$ independent samples from $p_\theta$, it is NP-hard (in $n$ and $N$) to provide an accurate approximation of $L_{ML}(\theta)$ and $\nabla L_{ML}(\theta)$. This result comes from the difficulty to approximate $Z_\theta$, which is necessary to compute $L_{ML}(\theta)$, while it does not appear in $L_{SM}(\theta)$. In Theorem 2, they derive an upper bound of the $\ell_2$-error between $\theta$ and its ML estimator obtained via $N$ samples, in the limit where $N\to \infty$, for any $p_\theta \in F$. Their proof relies on the asymptotic result given by [1] and consists of lower bounding the smallest eigenvalue of the Fisher information matrix of $p_\theta$. In Theorem 3, they derive the same upper bound in the case of the SM estimator for any $p_\theta \in F$, by invoking the asymptotic result from [2] and bounding the Poincaré constant of $p_\theta$. Combined together, Theorems 2 and 3 show that ML and SM techniques roundly have the same statistical efficiency.

[1] Asymptotic statistics, Van der Vaart, 2000.

[2] Statistical Efficiency of Score Matching: The View from Isoperimetry, Koehler et al., 2022.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- This work provides a fair comparison between statistical efficiency for SM and ML technique, which is of primary interest for the community of machine learning.
- Although Theorems 2 and 3 rely on important theoretical results, their results are not straightforward to obtain, and their proofs are original and clear to understand.

Weaknesses:
Although the theoretical results presented here are interesting in their own, they should be combined with numerical experiments, which should illustrate the trade-off between these two methods in practice, depending on the parameters of the exponential family.

Limitations:
In this work, the asymptotic theory is elaborated on the fact that the MLE and SM estimators can be computed exactly from a collection of samples. However, to compare these two techniques for practical purpose, one should include approximation error on the estimator.

Rating:
6

Confidence:
3

";1
nF6X3u0FaA;"REVIEW 
Summary:
This paper improves the state-of-the-art in synchrony based object centric learning methods. The current SoTA is the Complex-valued AutoEncoder (CAE). But CAE is unable to handle even the Tetrominoes dataset. The paper presents CAE++ and CtCAE.

CAE++ = CAE + minor architectural improvements

CtCAE = CAE++ + Contrastive loss term.

Experiments show that CAE++ and CtCAE are the first synchrony based methods that can handle colour images, and CtCAE can handle up to 6 objects per image.

The contrastive loss term in CtCAE is intended to make the object clusters well separated in phase space. This is then evaluated in Table 2 by measuring inter-cluster and intra-cluster distances. The rest of the paper measures ARI and FG-ARI on Tetrominoes, dSprites, and CLEVR datasets. Ablation experiments are used to justify the design choices.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Although this line of work is niche, it is promising and engages scientific curiosity among many readers. The paper advances this line of work closer to the overall SoTA among object centric learning methods; although there is still a large gap here.

The method is easy to understand and the paper is well written.

The experiments cover most design choices and compare against CAE, the obvious baseline. The experiments also address important questions such as the robustness across varying number of objects and generalization to more objects. Error bars are included in all tables.

Weaknesses:
Please incorporate the following minor improvements
- In line 175, it is claimed that performance is closely matched at 32x32. This is not true for dSprites ARI-FULL is 0.90 vs 0.68. I believe it may be sufficient to say that the methods are ordered the same way across resolutions.
- In Table 2, please indicate in row 1 whether higher is better (up arrow) or down is better (lower arrow) to aid interpretation. In some cases numbers have been bolded despite overlapping error bars. Are these statistically significant?
- In lines 299-305, consider citing Odin (https://arxiv.org/abs/2203.08777) which does pixel level BYOL followed by kmeans to get objects. It is kind of a contrastive object centric method (although mean-teacher distillation is probably the right word for BYOL).
- Some background explanation about the encoder and decoder and where the kmeans happens to get masks would make it even easier to understand the method, for readers not familiar with CAE.

The main drawback is that slot based methods (DINOSAUR, https://openreview.net/forum?id=b9tUk-f_aG) are now working on real world data whereas Synchrony based methods are yet to make the break into textured images. But I think this paper is still relevant and that future developments in this line of work may catch-up to Slot based methods.

Limitations:
The authors have adequately addressed the weakness although the empirical gap between slot based and synchrony based methods should be made clearer in my opinion.

I do not see any potential negative societal impact of this work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
CAEs hold potential to address several issues associated with slot-based representation. They may provide flexible object granularity, enable the extraction of part-whole hierarchies as needed, and provide faster training speeds. However, testing of the original CAE was confined to grayscale images and a small number of objects (2-3). This paper explores the question: how can we scale up CAEs beyond this initial scope?

For this, the paper takes the following approach:
1. Discovers certain architectural modifications that make original CAE perform much better on more complex datasets, in this case, Tetris, CLEVR, and dSprites.
2. However, noting that this is not enough, the paper proposes a novel contrastive loss, which improves the segmentation performance even further when added to the original reconstruction loss of CAE.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
1. Useful ablations show that architectural optimizations can be important to achieve the full potential of CAE. A good thing about some of these optimizations is that they actually relax and don’t complicate the original architecture e.g., by removing sigmoid activation.
2. Empirically show higher storage capacity in terms of the number of objects than CAE.
3. Paper is largely well-written.

Weaknesses:
1. While it is interesting that contrastive loss helps the segmentation performance, it somehow does not provide me new insights about the CAE framework itself. For instance, it doesn’t illuminate why separability was poor in the original CAE in the first place. As such, I am a bit worried that the proposed solution addresses a symptom rather than the root cause.
2. Line 215: *“The results in Table 2 indicate that indeed, intra-cluster distance (the 216 last column) is smaller for CtCAE than CAE++ for every dataset, especially for the most difficult ones”* It is actually difficult to conclude this from Table 2 considering the large standard deviations of these measurements. My suggestion would be to tone down this claim and/or confine this result to the appendix. Regarding the separability results more broadly, it is not clear whether separability in the representation space (as defined in this paper) is actually important with respect to the things we may eventually care about e.g., downstream performance or other tasks.
3. (Optional) It would be good to also report performances of some slot-based baselines, not for strict comparison, but to help the reader position the current performance in a more complete perspective. I see a line mentioned about it in the conclusion but without a pointer to a supporting result.

Limitations:
Yes, the paper discusses the limitations relative to current slot-based methods, but one should also consider the potential benefits of the CAE family of models.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a collection of small modifications of the Complex-valued AutoEncoder architecture leading to improved reconstruction/segmentation performance on simple RGB datasets; a contrastive learning objective is introduced to further improve the performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper's story is easy to follow, and the motivation is clearly stated.
- Architectural choices are justified through ablation studies.
- Experimental results are averaged over multiple random seeds.

Weaknesses:
- The captions of figures/tables could be improved to make it easier for readers to understand what is displayed. For example, Figure 1 is hard to understand without reading the relevant section.
- Table 1: While I see that this is a comparatively new line of research on new approaches for object-centric learning, I'd still appreciate it if the authors compared to state-of-the-art models so that readers get a feeling for how far the gap still is/how small it has become.

Limitations:
While the authors mention the strongest limitation, a more extensive discussion of this approach's limitations is missing. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes several improvements over the Complex AutoEncoder (CAE) to scale it to more complex multi-object datasets. The CAE is a synchrony-based model that learns object-centric representations by injecting complex-valued activations into every layer of its architecture, but it is only applicable to grayscale images with up to 3 objects. In this paper, two modifications to the original architecture and a novel auxiliary loss function are proposed that make the CAE applicable to RGB datasets with more than 3 objects.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
Overall, the paper is very well written and easy to follow. The introduction provides a good overview over the relevant research and a convincing motivation for the proposed approach. The methods section describes the proposed improvements and novel contrastive loss clearly. The results section is well-structured, and the related works section embeds the proposed research well into the existing research landscape.

The proposed contrastive loss seems like an elegant solution towards improving the ""separability"" of objects in the phase space. The complex-valued features contain two separate types of information (feature presence in the magnitudes and feature binding in the phases), and the proposed contrastive loss smartly utilizes this separation.

The provided ablation studies provide interesting insights into the proposed model improvements and design choices.

Weaknesses:
1) The main problem with the paper is that it seems to evaluate the models incorrectly. As described in section C.1 - ""RGB Images"" of the paper proposing the CAE [1], it cannot simply be applied to RGB images due to its evaluation method. During evaluation of the CAE, as well as in the proposed approach, features with small magnitudes are masked out, as they may exhibit increasingly random orientation values. However, this rescaling may lead to a trivial separation of objects in RGB images. For an image containing a red and a blue object, for example, the reconstruction will assign small magnitudes to the color channels that are inactive for the respective objects. By masking out values with small magnitudes, objects will subsequently be separated based on their underlying color values, rather than their separation as learned by the model. As far as I can tell, the paper does not describe any mechanism that would prevent the evaluation from doing this. I believe this effect is even visible in the qualitative examples. In Figure 3, the phase image of the CtCAE model on the dSprites example does not seem to clearly separate the purple and orange objects. The mask, however, separates them perfectly. In my eyes, this is only possible if the evaluation procedure makes use of additional (color) information besides the pure phase output. Overall, this makes it unclear whether the presented results are valid.

2) Eq. 6: When measuring the cosine distance between the phase feature vectors, extra care would be necessary to take into account their circular nature. Otherwise, two phase vectors $a=(0,0,0)$ and $b=(2 \pi, 2 \pi, 2 \pi)$ would have a large cosine distance to one another even though they represent the same angles. Has this been taken into account?

3) It would be interesting to see an evaluation of how the proposed improvements influence the separation of objects in the latent space of the model. Ultimately, the goal of object discovery is to create object-centric representations within the model, but the paper currently only evaluates the separation at the output.

Minor points:
- line 25: not all slot-based method work with iterative attention procedures
- Personally, I would move the CAE overview into a separate Backgrounds section
- line 90: $m_\psi$ can technically not be called a magnitude, as it may contain negative values.
- Eq. (6): Is there no exp() applied to the cosine distances, such that the equation would correspond to a Softmax?
- Using a dimensionality of 32x32 for Tetrominoes changes the shapes of the objects. While at the original dimension, all objects are made up from perfect squares, these squares get squeezed randomly when resizing the images. Thus, it would be better to make use of the original input dimension.



---

[1] Löwe, S., Lippe, P., Rudolph, M., & Welling, M. (2022). Complex-valued autoencoders for object discovery. arXiv preprint arXiv:2204.02075.

Limitations:
Limitations are adequately addressed.

Rating:
6

Confidence:
4

";1
wRhLd65bDt;"REVIEW 
Summary:
This paper proposes to improve diffusion-based image synthesis by explicitly reinforcing each point to predict its neighborhood context during training, without extra cost at inference. To reduce computation/time complexity of context decoding the authors propose efficient large context decoding adopting Wasserstein distance to characterize the distribution reconstruction loss. The method is applicable to both discrete and continuous diffusion backbones and achieves new SOTA text-to-image generation on MS-COCO with FID 6.21

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow. 
2. The method of explicitly reinforcing each point to predict its neighborhood context for diffusion models is well-motivated with effective designs to reduce substantial computation complexity for large-context neighborhood reconstruction.
3. The method is proven effective in boosting FID scores on MS-COCO text-to-image synthesis for both continuous (eDiff-I) and discrete diffusion (VQ-Diffusion) backbones.

Weaknesses:
1. The main results are on text-to-image synthesis and image inpainting. It would be good to add unconditional generation results. 
2. The method emphasizes on diffusion with better neighbouring context, leading to generations ""semantically better consistent with the text prompts""(L233), ""prommising cross-modal semantic understanding"" (L234), ""can synthesize more complex objects and scenes"" (L236-237). I don't think the claims are well-justified: e.g. Fig. 2 and Fig. 3 only presents results of the proposed method without any comparison to warrant the aforementioned conclusions. More analysis and evidence of ""better semantics"" are required other than the overall FID score.
3. Some notations are misleading, e.g. L 114-116, for h_i (h_{t-1}), the subscript is used to indicate both spatial and time; x_t is not defined in the main text. 

Limitations:
No.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes to improve diffusion based image generative training objectives by adding context prediction loss. The motivation of predicting context comes from other non-diffusion based models like semantic segmentation and representation learning. To mitigate the complexity of predicting large per pixel neighbourhood context, the author further models the context as a probability distribution using Wasserstein distance. Experiments show the proposed model achieves new SoTA generation on MSCOCO for both discrete and continuous diffusion models.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The introduction is well-written and the motivation of predicting context in the diffusion based model is easy to follow.

The presentation of the method including the loss derivation and the training pipeline is easy to understand.

The authors conduct intensive experiments showing the proposed context prediction loss can be used on various DM models, achieving SoTA performance on MSCOCO FID and inpainting tasks.


Weaknesses:
The paper lacks training and implementation details. For example, the text-to-image experiment uses T5 encoder as the text encoder, but did not mention architecture details and training details.

One big motivation to model context as a probability distribution is to improve the training efficiency. As shown in Figure 6, feature matching has lower throughput compared to distribution matching, but it has better FID. I think an important baseline is missing – sampling based feature matching, i.e. use the same number of random samples as proposed in the distribution matching, 9 instead of the full neighbourhoods features for context prediction.


Limitations:
There is no limitation/future work discussion in the paper

And there is also no training / implementation details in the paper, which cause concerns on reproducibility. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents ConPreDiff, a method introduced to improve the performance of diffusion models by preserving the neighborhood context of predicted pixels/features. They achieve this by predicting the neighborhood context during the diffusion generation process. To simplify the modeling complexity, they propose predicting distributions instead of directly reconstructing the neighborhood. The method's effectiveness is demonstrated through extensive experiments on unconditional image generation, text-to-image generation, and image inpainting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The idea is intuitive and easy to understand. 
* The proposed method is general and can be easily applied to recent diffusion models.
* The performance of the proposed method is very impressive.

Weaknesses:
* Recent diffusion models use UNet backbone, which stacks many convolutional and self-attention layers. Thus, it has a large receptive field. Additionally, LDM also has a decoder, which also has a decent receptive field. Therefore, I am confused about the paper's main claim that the point-wise reconstruction neglects to fully preserve the local context.
* There are no visual comparisons of the proposed method and baselines. I am not sure if ConPreDiff can really be more local-context consistent compared to other methods.
* The authors need to discuss the additional training cost. Besides, they also need to provide the additional parameters they use for the context prediction.

Limitations:
The authors did not discuss their limitations and societal impact in the paper. 


Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes an idea of context prediction to boost difussion-based image generation.
The core idea is that in each step of diffusion, after the denoised point is generated, neighborhood context prediction is performed.
In particular, to maintain the spatial orders of the neighborhood, a permutation invariant loss is used for optimization by replacing the context prediction with neighborhood distribution prediction. Performance improvement against standard diffusion models were presented. in experiments.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The idea is very interesting and sound.
From an image denoising point of view, neighborhood info is commonly used, so it's a natural extension of diffusion-denoising models.
2. Performance improvement showed in experiments are promising. 

Weaknesses:
The proposed approach probably takes longer to train. Can you discuss from that perspective?

Limitations:
Similar with text2image papers.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper is proposing context-aware Diffusion Models. They make the models learn the context information by setting up auxiliary networks to estimate the neighbor distributions from the estimated denoised sample from Diffusion Models. The benefit of this approach is that additional cost from the auxiliary networks are not applied during the sampling. Both quantitative and qualitative experiments are reported.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Motivation is agreeable.
- Good writing.
- Reasonable method for motivation.
- Experiments are done well.

Weaknesses:
Specifics of the weaknesses of this paper are written below as questions and limitations, but I believe most of them can be resolved during the rebuttal. I will increase my rating if my concerns can be resolved.

Limitations:
- The most important part of the paper would be “context prediction” term in Eq . 3. It’s motivation is understandable, but it is not interpreted in terms of optimizing variational bound of negative log likelihood. Is it just “additional” term? or can it be interpreted as a term playing a certain role in maximizing ELBO or likelihood?
- This paper is proposing to 1. estimate the neighborhood distribution (instead of directly estimating the neighborhood) and 2. minimize Wasstertein distance as a core objective. Though the authors made an attempt to justify the design choice, I believe it could be compared as a sort of ablation study, which might strengthen the proposed method. --- I found Fig.6 and the first concern is resolved.
- I believe training time needs to be compared together in Fig. 7.
- Although performance improvement is shown by quantitaive experiments, it is not straightforward how the ""context prediction"" makes model performance better (qualitatively).

Rating:
6

Confidence:
4

";1
Drrl2gcjzl;"REVIEW 
Summary:
Beyond model training, sequence length is one of the crucial aspect impacting the fluency, accuracy and completeness of LM outputs. For transformer based model's positional embeddings (PE) are used to explicitly represent the location of tokens. Over the past few years several works have attempted to propose a better PE approach, particularly aiming to improve over the original absolute PE of the Transformer. 

This work, proposes a no positional encoding (noPE) model training for decoder only Transformer, to better generalize on generation length. The proposed noPE LM is evaluated on set of maths and reasoning tasks, using a small size (~100M parameter) LM. Findings show that noPE based LM performs similarly with relative PE based model (from the T5 model), and outperforms models with alternative PE variants such as Rotary, AliBi, and absolute PE for length generalization. Along with the evaluation results, detailed empirical analysis and theoretical arguments are provided for why noPE LM can intrinsically support an absolute and relative token position representations.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This work, provided an in-depth analysis of Transformer LM training and inference without using PE. Although, the focus is to address the question of LM length generalization, the findings inform future possibility of an increasingly simplified transformer based LM's. In addition to the surprising results where noPE works in a similar way as the best performing relative PE (T5's PE), the work, delivered a thorough discussion as to why noPE works.
 


Weaknesses:
Despite the finding not using PE (noPE) can generalize for sequence length, the following points remain unanswered or not fully investigated:
- Tasks selected for modeling and evaluation does not cover challenging length generalization scenarios. For instance, the noPE approach could have been testing using a summarization task which is highly impacted by the length of the summary.
- Regardless of the provided results on length generalization, detailed and task specific performance metrics comparing LM's with PE's and noPE LM is missing. The absence of these comparison leaves open question on task specific performance comparison.   

Limitations:
Limitation is provided, specifically explaining why the proposed noPE is not applied for large scale LM training. The limitation, not being able to access publicly available LM can be further expanded, for instance why not use 500M? If there is any identified societal impact its encouraged to include it. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work follows a long line of work analysing the effectiveness of various types of positional encodings and, in particular, a line of work exploring the effectiveness of causal masking without any positional encodings [1 - 8]. To that end, they evaluate the length generalisation performance of Transformers trained with various types of positional encoding schemes on a diverse set of reasoning and algorithmic tasks and find that Transformers without positional encoding schemes generalise better to higher lengths than other encoding schemes.
 
 
They consider 3 three types of tasks -- primitive (copying or reversing strings), algorithmic (addition, parity, etc.), and prior length generalisation benchmarks (e.g. SCAN). They evaluate five different encoding schemes, including Alibi and rotary encodings. They use autoregressive decoder-only Transformers of sizes of the order of 100M. For each task, Transformers are first trained on a set of examples using a language modelling objective and are then evaluated on examples of higher lengths.
 
Across the three types of tasks, they find that Transformers without positional encoding (but with masking) and T5's relative bias outperform other encoding schemes. Additionally, they show via a construction that Transformers without positional encodings can simulate both absolute and relative encoding schemes. They compare the attention patterns of Transformers trained without positional encoding schemes with other schemes and find that the attention patterns are most similar to relative positional encodings. Lastly, they find that when trained with a scratchpad, T5's relative positional encoding scheme and no positional encodings seem more effective than others.
 
 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

(S1) **Interesting results.** Overall, I believe there are several interesting findings in the paper. I believe the contributions in the paper take a step forward in understanding the effectiveness of Transformers with just causal masking. The analysis with the scratchpad setup demonstrating the importance of PE for scratchpad and the effect of scratchpad for length generalisation is interesting as well.
 
 
(S2) **Sound experimental setup and well-motivated.** The question of understanding and identifying the most effective positional encoding scheme is an important problem, and this work takes a step forward in that direction. The choice of tasks seems interesting, and the evaluation with Transformers trained with language modelling objectives is more aligned with practice than prior works that have explored this direction.
 
 
(S3) **Well written.** The paper is well-written, the motivations are clearly outlined, and the arguments are well-presented.
 


Weaknesses:

(W1) **Novelty of results is slightly overclaimed.** Based on my limited knowledge, the use of Transformers without positional embeddings and with only causal masking was first studied in [1]. It was later analysed empirically [2,3] and theoretically [3, 5] in further works. These works mostly suggested that Transformers without positional encodings are competitive with standard encoding schemes on machine translation and language modelling tasks. On synthetic formal language tasks, [4] and more recently [6] have noted that using no positional encoding leads to better length generalisation performance. In more practical settings, recently [7, 8] have shown that large language models trained with only causal masking are competitive with models with positional encoding.
 
The fact that Transformers with only causal masking can encode positional information and generalise well to higher lengths is not entirely new. To be fair to the authors, I do not think that they explicitly claim this. I think the paper does a good job of establishing that just causal masking is better at length generalisation than various other schemes in a more systematic way. However, given the prior work exploring Transformers with only causal masking, the framing of the introduction seems a bit odd, and some of the papers mentioned above [1-8] are mentioned passively in the paper. It could be helpful to readers unfamiliar with the area if the prior work was presented in a more comprehensive manner.
 
 
(W2) **Effect of scale.** Since the tasks in the paper are confined to synthetic tasks and models with ~100M parameters, it is still unclear if training LLMs without positional encodings will be better for length generalisation in practice. Modifications to Transformers at this scale for effectiveness on such tasks have often not scaled to LLMs in practice. As mentioned in the paper, that could require a larger computational budget which is understandable to some extent. But at the same time, comparing Transformers trained for language modelling on text data on higher lengths would have been useful. Based on the results in [7, 8], it seems like positional encoding schemes help improve perplexity in language modelling, but it may be interesting to see if that changes for length generalisation tasks.
 
 
 
 
   
   
[1] DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding. 2017  
[2] Assessing the Ability of Self-Attention Networks to Learn Word Order. 2019  
[3] Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel. 2019  
[4] On the Ability and Limitations of Transformers to Recognise Formal Languages. 2020  
[5] On the Computational Power of Transformers and its Implications in Sequence Modeling. 2020  
[6] Transformers Learn Shortcuts to Automata. 2022  
[7] Transformer Language Models without Positional Encodings Still Learn Positional Information. 2022  
[8] Bloom: A parameter open-access multilingual language model. 2022  
 
 
 
 


Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
Length generalization is one of hot topics for transformers. In this paper authors investigate how positional embedding influences this generalization for **decoder-only** models on **reasoning tasks**. Authors compare for variety of tasks (e.g. copy, summation, sorting, etc.) sinusoidal positional embedding, T5, AliBi, Rotary and no positional embedding (NoPE). 
First, authors proof by construction that NoPE for decoder-only models is able to learn positional information due to causal masking. Then authors show that none of the positional embeddings generalizes to longer sequences, while NoPE in some cases performs even better than others (though performance is still bad compared to seen lengths). Empirically by comparing attention patterns authors show that NoPE learns attention similar to T5 models. Finally authors also play with scratchpad and show that it is not always helping generalization for longer sequences.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
- first time (at least I didn't see in prior works) formal proofs that decoder-only models are capable of positional embedding reconstruction (I went through the proof, no issue found with the math)
- emergency for other positional embeddings development or investigation of transformers failure for the longer sequence generalization for reasoning tasks
- interesting empirical results for reasoning tasks showing that mainly all positional embeddings currently widely used do not work for reasoning task on longer sequences while no positional embedding (NoPE) works similarly or even better sometimes
- interesting insights on the NoPE (learnt attention pattern is more similar to models trained with T5 bias) and results on scratchpad

Weaknesses:
- overstatement in the test about positional embeddings for the decoder-only models: here it should be clarified about the type of the tasks where we apply, as for LMs we know that e.g. AliBi is working.
- small scale experiments and limited length generalization evaluation.
- no in-depth comparison of the root for the different behaviour e.g. for AliBi between the reasoning tasks and LM task.

Limitations:
Limitations in the main text, however main restriction on considering non-LM tasks is not clearly delivered in the paper.

**Update: Originally score was 3 for contribution and overall score 6-weak accept. After rebuttal and discussion they are raised to 4 for contribution and 7-accept.**

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents a comprehensive study on the role of positional encodings in Transformer models, focusing on their ability to generalize over sequence lengths. The authors introduce a novel positional encoding method, which does not use explicit positional information.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper introduces a novel positional encoding method, which is a significant contribution to the field. 
2. The paper is well-written and easy to follow. 

Weaknesses:
 The authors did not study how large-scale pretraining affects different PEs due to the lack of publicly available large language models trained with various PEs under similar conditions. This leaves a significant aspect of the problem unexplored.

Limitations:
NA

Rating:
5

Confidence:
3

";1
HRGd5dcVfw;"REVIEW 
Summary:
The work proposes to address the training of the classifier (last layer of the network) separately in the setup of federated learning with a pretrained weights initialization. In particular, the suggestion is to use a nearest class means (NCM) approach, which saves a lot of computation time. The proposed approach is also supposed to be helpful in heterogeneous setup and thus evaluated on datasets separated into local ones by Dirichlet distribution. The paper provides an empirical evaluation of the linear probing and NCM approaches for training classifier, compared to standard federated learning. The conclusion made from the empirical results is that proposed method saves communication and leads to better performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The proposal to use only the last layer adaptation when the weights are pretrained is a viable and interesting idea for federated setup. It seems to mitigate heterogeneity and save computational time (with NCM) and communication (when no further finetuning is performed).

Weaknesses:
1) The explanation of the approach can be improved---especially the mixed one (is the final tuning happening for all the weights or without head?).

2) Heterogeneity is presented as one of the main reasons to introduce a new approach, but the method is not compared to any specific federated algorithms devised for it.

3) The evaluation is trying to highlight both LP and NCM, while overall they do have rather different approaches; this makes it unclear what exactly is the main message of the paper: to train only the last layer or to train it in the particular way? Moreover, only the combination with further tuning results in best performance.

Minor
- table3 is never referenced
- figure4 is referenced before figure3
- figure5 is called table5

Limitations:
Evaluation for heterogeneity is done only with very particular non-iid with respect to labels, but it is not highlighted.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies strategies to fine-tune models in Federated Learning, starting from a pre-trained model. It has been shown in the literature that pre-training is beneficial in FL, that it improves convergence speed and robustness to heterogeneity.

This paper shows that existent fine-tuning pipeline are not optimal. Further, this paper proposes a two stage head-tuning + fine-tuning technique, where the first head-tuning stage is compute and communication efficient. The authors show that their proposed two state approach are better than Linear Probing and full Fine-Tuning in terms of convergence to accuracy and  robustness to hyperparameters.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. Experiments are done with rigor and described in detail the paper.
2. The authors ran into a discrepancy in terms of experiment results on one dataset reported in (Nguyen, 2023). They provided a concise reason and experiments to explain the source of discrepancy, which is due to the image size in the training data.


Weaknesses:
1. I would like to see how the proposed method compared to other simple two-stage fine-tuning methods, for example, running LP for a few epochs then perform full FT. This simple modification is not compared in the experiments.

2. high-dimensional classification: the output dimension on all problems studied in the experiments are not too big. In some high-dimensional classification problem, e.g. language model, some classes only have a few examples. I would like to understand how they affect the centroid-based initialization and the applicability of FedNCM.

Limitations:
N/A

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes to use nearest class means (NCM) with pre-trained models in federated learning, coined as FedNCM. Experiments show that FedNCM is effective in terms of convergence and communication cost, due to the application of pre-trained models and light-weighted last layers.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Using pre-trained models in federated learning is a new and interesting direction, especially with the popularity of foundation models. This paper explores pre-trained models from a federated learning perspective, which shows the effectiveness of saving communication.
2. It provides code and experimental results on several vision datasets, including CIFAR-10, Flowers102, Stanford Cars, CUB and EuroSAT-Sub.
3. The experimental results show the improvement over Random baseline.

Weaknesses:
1. The algorithm seems to be a mere combination of NCM and FedAvg, which is not novel enough. This is not vital though as long as the performance is great.
2. However, the application of pre-trained models in FL has already been proposed in Nguyen et al. ICLR 2023. 
3. The experimental comparison is not considering Federated Learning baselines, like FedProx, SCAFFOLD, FedYogi, LG-FedAvg [1], FedPer [2], etc.
4. The experiments only include small scale datasets, in total less than 50,000.
5. There is not much theory on the communication cost, or convergence rate analysis.
6. Minor: Line 201 FedAVG -> FedAvg; Line 234: imagenet -> ImageNet;


[1] Liang, Paul Pu, et al. ""Think locally, act globally: Federated learning with local and global representations."" arXiv preprint arXiv:2001.01523 (2020).
[2] Arivazhagan, Manoj Ghuhan, et al. ""Federated learning with personalization layers."" arXiv preprint arXiv:1912.00818 (2019).

Limitations:
The limitation discussion is a bit limited and the authors just spend one sentence on this. Suggestions include the negative societal impact of using pretrained models and the lack of theoretical analysis and large-scale datasets, etc.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the effect of tuning the linear head on federated feature learning. They show that well-tuning the linear head will make the base feature extractor converge closer to the optimal solution while reducing the total communication cost in FL. They evaluate their method on multiple FL datasets and show promising results.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Investigating the effect of a linear head on the final FL model tuning is interesting.


Weaknesses:
One thing is missing. When learning the linear head in the first stage, it is possible to get the closed-form solution of V. Why not do that? This will give a smaller upper bound for the right-hand side of Eq (2).

There are some works to improve the compute/communication efficient FL, such as https://arxiv.org/pdf/2206.08671.pdf (not limited to this). Some comparisons should be conducted.




Limitations:
Please see above.

Rating:
4

Confidence:
3

";1
FdsS51iif3;"REVIEW 
Summary:
This paper proposed to interactively use inpainting (I-step) and outpainting (O-step) to achieve foreground masks without training.
Formally, the I-step refines the false positive masks while the O-step tackles false negative ones. The authors proposed Contrastive Potential to evaluate the differences between the origin and inpainted images, and gradually refine the segmentation.
Various prompts are supported to start the I-step and O-step (point, box, scribble).
Experiment results show the effectiveness of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. Leveraging inpainting and outpainting to address the segmentation is novel and interesting.
2. Quantitative results show the effectiveness of the proposed method. It enjoys good improvements for untrained algorithms.
3. The mathematical formulation of EM is interesting.

Weaknesses:
1. I think this method is not efficient because it used a diffusion-based method to inpaint/outpaint iteratively. But the authors did not provide a comparison of efficiency.
2. Though the authors claim that their method could work with prompts: point, box, and scribble, they did not provide sufficient illustration about the usage of point prompts in Fig.2 and Sec4. In my opinion, both inpainting and outpainting fail to achieve proper results based on only one point prompt. 
3. There are not enough ablations to confirm the effectiveness of Contrastive Potential (paint, color, prompt). It is unclear how to formulate Eq. (2) in the absence of point prompts.
4. Eq.(3) is a bit difficult to understand. It seems like $\mu$ is supposed to represent the center position of a cluster, but I'm not sure what the notation ""$argmax(\mu_k)$"" means. Personally, I believe it would be more helpful if the authors presented the average value of all the samples in a cluster as the value for $\mu,"" instead of just the cluster center.
5. A more equitable experiment would be to classify the competitors in Tab.2 into those with prompts and those without. It would be unjust to compare the proposed method with competitors lacking prompts.

Limitations:
Yes the authors have discussed about them

Rating:
6

Confidence:
4

REVIEW 
Summary:
The author proposes a training-free segmentation technique AMCP, which leverages off-the- shell diffusion model to refine the corase segmentation prompt iteratively. AMCP contains three main components: Inpainting step, outpainting step and adversarial updating. The paper gives detail ablation of AMCP and demonstrate its effectiveness on sevaral benchmarks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper achieves a successful integration of an existing denoising diffusion model, exploiting its potential in segmentation tasks by inpainting and outpainting techniques, resulting in promising performance.
2. This paper conducts comprehensive comparisons with other segmentation approaches and provides a detailed ablation analysis of the techniques employed in AMCP

Weaknesses:
1. The main contribution to the segmentation ability stems from the diffusion model. However, the low speed of the sampling process in the diffusion model poses a significant obstacle for real-time applications. As AMCP lacks classification and object discovery capabilities, it is better suited as a labeling tool or an interactive segmenter, the latter of which demands even higher real-time performance. However, achieving the performance demonstrated in the paper requires AMCP to undergo several diffusion steps (5), further diminishing its effectiveness.

2. I notice that AMCP is sensitive to the cluster center number, which leads to about 20 mIoU variations. In the paper, AMCP is fixed to use the pre-training parameters of one specific diffusion model. When employing different diffusion models, the impact of each inpainting varies. Whether the hyperparameters of AMCP reasoning also need to be adjusted accordingly.

Limitations:
Authors address the limitations of their work and potential negative societal impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The following work proposes an iterative approach to converting any off-the-shelf inpainting/outpainting capable models to perform segmentation. The proposed method alternates between two approaches to separating the segmentable foreground object from the background. In the I(npainting) step, they mask out the foreground object and inpaint the gap, then locally contrast the in-painted image against the original. The inpainting will fill the hole with an extension of the background instead of the masked out foreground object, thus the disparity between the two images gives rise to a segmentation mask for said object. The O(utpainting) step is similar but expands outwards from a sample of the foreground object. The disparity between the outpainted image and the original image will be large on the non-foreground pixels, resulting in an approximate inverted result from the I-step.  Experiments suggest significant improvements over recent methods with limited to no additional training.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
- Approach appears principled and builds upon older methods involving conditional random fields
- Strong results with minimal (or none at all?) additional supervised training necessary; potential for strong generalization

Weaknesses:
To me, the primary concerns of this work lie in the clarity of the writing:
- While I understand why the authors chose to refer to the approach as adversarial masking, I don't think this is correct. The inpainting and outpainting steps come from opposite direction but have a more complimentary relationship than an adversarial one. The formulation in (6) similarly does not exhibit signs of any adversarial interaction between opposing objectives, but would be more aptly described an alternating minimization scheme of two separate constraint terms. 
- Overall, I felt that the writing could have been more clear. In section 2 on contrastive potentials, the authors assign the sum of potential functions to the term contrastive potentials. Critically missing is how the potential values relate to the segmentation task itself, presumably through an argmax obejctive. In addition, while the terms inpainting and outpainting are generally accepted terminology, I don't believe it is common to refer to the resulting image as the ""painted image"", which definitely led to some reading confusion on my end.
- It is also unclear to me where the cluster centers \mu_k come from in the k-means mask update step in section 4.3
- Some additional missing ablations include ablations over the lambda weights for individual potential functions, as well as choice of generative model.

Limitations:
Adequately addressed

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a new unsupervised method for object segmentation, named PaintSeg. Given the initial mask, the proposed PaintSeg adopts adversarial masked contrastive painting (AMCP) to generate the foreground and background masks through the contrastive relations between the original image and the painted image. The AMCP process involves two steps: I-step for the foreground mask and O-step for the background mask. The proposed PaintSeg based on the off-the-shelf diffusion models does not rely on training with labeled samples but generate segmentation results through iterative optimization with the I-step and the O-step. Experiments show the good performance achieved by the proposed PaintSeg.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using inpainting and outpainting for foreground and background masks is interesting and novel to me.
2. This paper addresses unsupervised image segmentation in a training-free manner with pre-trained generative models.
3. This paper presents the optimization steps with contrastive potential and adversarial steps.
4. The proposed approach PaintSeg is effective and achieves good results on several benchmarks.


Weaknesses:
1. Firstly, it's a nice work and there is no significant weakness.
2. Based on LDM, the proposed PaintSeg takes much time to iteratively generate masks.


Limitations:
The authors have clarified the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper designs a novel unsupervised segmentation method (AMCP) without training via image painting. Its main process is to mask the image based on the specified prompt, recover the image using a generative model, and determine the foreground mask and background mask of the image by comparing the original and inpainted images. Specifically, based on the specified prompt, the image is first masked and then recover using a generative model. Finally, by comparing the original and inpainted images, the foreground mask and background mask of the image are determined. Specifically, it designed I-step and O-step respectively. In the I-step, the background of the image is restored from the deleted object region, and the foreground mask can be obtained by comparing it with the original image. Conversely, in the O-step, the background mask can be obtained by drawing the foreground in the image. Finally, a large number of experiments fully demonstrate the effectiveness of the method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The idea of obtaining the segmentation result of an image by comparing the difference between the original and inpainted images is interesting. 
2. The manuscript is well organized and has several experiments.

Weaknesses:
1. Mt+ and Mt− serve as the dilation and erosion masks of Mt, and are important conditions for updating the regions near the foreground-background boundary. The description in the implementation details cannot show the process and loses the discussion of their necessity. 
2. Since AMCP first performs the I-step and the input init mask needs to include the false positives region, it is not possible to directly perform the I-step for point or scribble prompts. In this case, is it feasible to swap the order of the I-step and O-step?
3. DINO and Fully Connected CRF are used in Line 150 and 154 respectively, and they play a very important role in AMCP. What would be the impact on AMCP if they were replaced? DINO's features are highly robust, and replacing DINO with other models can be used to verify if it is DINO that has improved the upper limit of AMCP.
4. The core of this paper is to segment images based on a specific prompt, which is the same setting as SAM and SEEM. Therefore, fair comparisons should be made with them in comparative experiments. Currently, the comparison method introduced in the paper is not fair, as PaintSeg requires a specific prompt to be given, while most of the unsupervised methods being compared do not have this setting.
5. Although AMCP is a training-free method, it involves many hyperparameters and other introduced models. It is recommended to add pseudocode to more clearly demonstrate the detailed process of AMCP.

Limitations:
shown in weaknesses

Rating:
6

Confidence:
4

";1
yAOwkf4FyL;"REVIEW 
Summary:
This paper studies the robustness issue of DARTS from the perspective of overfitting. It uses gradient matching scores to measure the overfitting issues, and proposes an early-stop strategy to address the problem of saturated skip connections in normal DARTS. The proposed approach has been evaluated on a number of search spaces, showing comparable results to the state of the art. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ The idea of using similarity between gradient directions of training vs validation batch makes sense. 
+ Extensive experiments on various search spaces and benchmarks. 
+ Decent results on DARTS C10 space comparing to SOTA.

Weaknesses:
- Although the idea to use early stopping to robustify DARTS is interesting, the metric used in this paper is very similar to that in GM-NAS [1]. Therefore it seems to me that the novelty of this paper is somewhat discounted. 
- It is good to see experiments on a number of search spaces and benchmarks. However, the results on DARTS S1-S4 space is missing. 
- Some discussion on overfitting is not very clear. e.g. from Fig.1, it is difficult to see quantitively how the negative correlation between arch params and val loss: indeed the curvature of the lines are different, but it would be better to have some more rigid analysis. 


[1] Generalizing Few-Shot NAS with Gradient Matching. Shoukang Hu*, Ruochen Wang*, Lanqing Hong, Zhenguo Li, Cho-Jui Hsieh, and Jiashi Feng. ICLR 2022. 

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors are adressing the issue of converging to a degenerated solution (many skip connections) using DARTS. The authors connect this behavior to an overfitting to the train data. To remedy this issue, they suggest to apply early stopping based on the correlation of gradients during the architecture parameters training and the wekths parameters training. 

Soundness:
2

Presentation:
4

Contribution:
1

Strengths:
The paper is trying to tackle an interesting problem in an elegant way. The story is clearly stated and the introduced approach is a nice extension of the classical early stopping. The performance of this method is on par or better than many other more sophisticated (and to some extent complicated) method. 

Weaknesses:
* The authors are claiming that the dominance of skip connections is due to the quick overfitting of weighted operations to the train data. The whole story of the paper relies on this assumption which is unfortunately not sufficiently supported. While curves on figure 1(a) are supporting this claim, figures 3(a) and 3(b) are a bit confusing.  I would have expected the test performance to drop for both cases.

* There is an issue regarding the reported numbers: the authors are claiming that their method benefits operations with parameters and the test error is dropping (as expected), I am wondering why the number of parameters is barely different. 

Limitations:
None

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper demonstrates the fundamental reason for the domination of skip connections in DARTS from the new perspective of overfitting of operations in the supernet, using preliminary experiments,  and proposed the operation-level early stopping method to mitigate this phenomenon by using the GM score metric during the searching.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea of using GM metric to decide whether to update the OPs is broadly used in efficient-training papers.  This paper proposes to use this metric to early stop the updating of specific OPs during the NAS procedure, based on their novel overfitting observations. 

The comparison is intensive, showing the superiority of this proposed method, in terms of time cost, and accuracy. 

Weaknesses:
1. The accuracy metric is one of the metrics to measure a NAS method. We also consider the Kendall rank correlation coefficient. Please compare with previous methods using this metric, because I am not sure whether the proposed early stopping mechanism will hurt the ranking or not. Usually, we say the ranking performance of a NAS method may be more important than the accuracy of the searched model. 

2. About the overfitting threshold, ""we determine the threshold by averaging the cosine similarity over 20 iterations for 30 randomly initiated architectures in each search space""    Is the initial 20 iterations ok or enough for determining the threshold? The initial stages may have dramatic changes in gradient. Also, the threshold is fixed during the NAS procedure, shouldn't it be adaptive or scheduled? 

Limitations:
Yes, the authors adequately addressed the limitation.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper focuses on the robustness issues in differentiable NAS, specifically the domination of skip connections. It first analyzes the issue from a novel perspective, proposing that the domination of skip connections arises due to the overfitting of operations in the supernet during training. Then, the paper proposes the operation-level early stopping method, which monitors each operation in the supernet and stops its training when it tends to overfit. The paper employs a gradient matching approach to detect overfitting, comparing the gradients' directions of operations on training and validation data. A significant deviation in direction is considered an indication of overfitting. The proposed OLES addresses the domination of skip connections with negligible additional overhead. Extensive experiments demonstrate the effectiveness of OLES on different datasets and search spaces. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
S1. The paper demonstrates good originality. It provides a comprehensive analysis of the issue of the domination of skip connections in the differentiable NAS by dopting a new perspective. Although straightforward, the perspective makes sense and is interesting. Specifically, the paper aims to explain the cause of this issue through the overfitting of operations in the supernet. The paper proposes the operation-level early stopping (OLES) method, which introduces gradient matching to address this matter. OLES elegantly and effectively resolves the domination of skip connections, incurring negligible additional overhead. 

S2. The experiments are thorough and well-organized, containing experiments in different search spaces and an in-depth analysis of the proposed algorithm. The empirical results demonstrate that the proposed OLES  achieves state-of-the-art performance on CIFAR. The availability of open-source codes further facilitates reproducibility.

S3. The presentation of ideas and algorithms is clear, while the references and background knowledge are comprehensive. The background knowledge and the issue to be solved are adequately introduced.

S4. The perspective and idea about the overfitting of operations have profound significance in uncovering the underlying causes of the domination of skip connections in DARTS. These novel perspectives, ideas, and algorithms contribute to a deep understanding of differentiable architecture search and may inspire future research in the field.

Weaknesses:
W1. It needs a thorough explanation of the used gradient matching method. The authors should provide a more detailed introduction to gradient matching and clarify the differences from other methods.

W2. While the proposed method demonstrates significant improvements over the original DARTS in the experiments, it does not have a competitive advantage compared to other state-of-the-art methods. 

W3. The paper specifically focuses on addressing the domination of skip connections through operator-level early stopping. The authors are suggested to discuss how the concept of operator-level early stopping can be applied to other scenarios beyond differentiable architecture search.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:

This paper propose a method namely operation-level early stopping to address the skip-connection domination issue in domain of differentiable architecture search (DARTS). Though this problem is heavily explored in the past, the authors believe that the key reason of skip-connection domination is because of they try to overfit the validation set used in DARTS based method, and their proposed operation-level early stop that conceptually stop the training of architecture parameter if the overfitting is observed. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The problem of skip-connection dominance is a long standing problem in DARTS domain, and novel method to address this has a clear motivation. 
The hypothesis of overfitting is the root cause of skip-connection domination is novel and interesting. 
Analysis of architecture with validation loss is quite clear and justify their hypothesis.
I appreciate the extensive and honest experiments on all kinds of settings, even though many of the results does not surpass the state-of-the-art. 

Weaknesses:

I have several questions regarding this paper and hope to hear back from the authors.

1. Utilizing gradient matching as an indicator to perform early stop seems okay, but this paper lacks of sufficient analysis of what is the key difference between GM+DARTS and their approach, especially regarding why OLES is better. In the related work section, it only describes GM-NAS introduces gradient match score into NAS literature, but this is not enough to let readers understand the difference. 
2. In essence, use early stop to avoid overfitting in DARTS is novel, but experiments seem to show that this OLES does not surpass previous DARTS+PT, which is another indicator to select DARTS operation. I do not understand the urgency to accept another indicator work with similar performance. In addition, similar as above, I do not see much comparison of how OLES surpass DARTS+PT. I agree that OLES can surpass original DARTS, but if it cannot surpass other methods that aims to address the skip-connection domination issue, this is inadequate to appear in a top-tier conference in my humble opinion. 
3. In general, the experiments compared to state-of-the-art is inferior. 
To the best of my knowledge, this approach has a close relationship with GM+DARTS and DARTS+PT, where in all experiments, the authors should compare to. However, for example, in Table 2, none of these methods exists. In addition, when solely compared to GM+DARTS, in Table 1, it is 0.05 test error better than GM+DARTS. In table 3, GM+DARTS is basically identical as OLES (24.5 v.s. 24.5). In Table 4, OLES surpass GM+DARTS by a margin of 0.2, which is kind of significant. However, in Table 5, GM + ProxylessNAS surpasses OLES again. Why we should accept a paper using the exact gradient matching score in a different manner, which seems inferior than the original approach?

In addition, I would like to see how this OLES address skip-connection domination with other approaches that aim the same target, not in terms of their final performance, but with respect to their performance to address skip-connection domination issue. After all, this is not the first paper aiming to address it. 


Limitations:
Moderately discussed. 

Rating:
3

Confidence:
4

";1
LnZuxp3Tx7;"REVIEW 
Summary:
This paper considers the various settings of overfitting, including benign overfitting where an interpolating estimator is optimal in spite of perfectly fitting noisy labels, tempered overfitting where an interpolating estimator is inconsistent but error is a function of the label noise, and catastrophic overfitting where an interpolating estimator has no ability to generalize (random guessing in classification or infinite risk in regression). The authors follow-up prior works in benign overfitting by showing that a two layer neural network can benignly overfit so long as the input dimension of the data grows faster than the number of training samples, irrespective of the width of the network itself (assuming convergence to a max-margin solution, and additionally show another benign overfitting result that only requires convergence to a KKT point of a max-margin problem). 

They also provide a theoretical justification for tempered overfitting in two-layer ReLU networks, a phenomena that was empirically studied in prior work. The authors precisely characterize the excess risk of an interpolating network trained to fit uniform data with labels samples as +1 and flipped to -1 with probability p. The authors show that for one-dimensional data, the excess risk of such a network interpolating this problem is tightly bounded by functions of the noise level (e.g. theta(poly(p)) and theta(p) in a more restrictive setting).

The authors then observe that catastrophic overfitting can occur when forcing networks, of width at least 2, without a bias to interpolate the training data (drawn as specified before) when there is at least one sample with -1 label. They also show that you cannot obtain benign overfitting from convergence to a KKT point of the max-margin problem unless the network has a bias term.

Finally, the authors provide an extended version of an experiment from prior work on tempered overfitting in which fully-connected ReLU networks are trained to interpolate data on the unit sphere with +1/-1 labels sampled with probability p, as defined before. They observe that there are regions between benign and tempered that are a function of the ratio of input data dimension to number of training samples, namely: as the input data dimension grows higher than the number of training samples, the excess risk of the model approaches 0. They additionally show experimental results on 3-layer networks to empirically assert that, while their theory holds for 2 layer networks practically the same behavior is seen for 3. Finally, they experimentally show in their setting that the width of the network is not as important as the input dimension when evaluating consistency of the estimator.

EDIT: Thank you for the rebuttal and engaging response, this is indeed a very thoughtful work. I opt to keep my positive score.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper provides a clear theoretical analysis for tempered overfitting for two-layer ReLU networks that has not appeared in prior literature. There is some earlier theoretical work on tempered overfitting in kernel regression and for minimum description length settings, but as far as I know nothing for a standard finite-width neural network. The prior cited work (Mallinar et al. 2022) shows experimental evidence with neural networks that suggests tempered overfitting, and following their formalisms the authors of this work go a step further to give proofs of excess risk of one of the experiments in that paper (more on this in later section). 

I really appreciate the authors providing proof outlines and intuitions in the main text, as well as citing references where they are using techniques and following proof styles introduced in prior works. The style of explanation is accessible and gives me, at a high level, the right amount of information to contextualize the theorem statement while deferring details to the appendix should I want to review them.

There are many intriguing results from this paper. For one, I am initially surprised by the one-dimensional tempered results and Theorem 4.1 on benign overfitting feature bounds that don’t depend on the width of the network. I’ve written more about this in the questions section, but I would have expected a more subtle interplay between width of the network and input dimension of the data. Notably, though, the width of the network has to be sufficiently large to interpolate the data (e.g. overparameterized) and since the authors study 2-layer networks this might imply a large width, though their experimental results show roughly similar behavior for width 3000 and width 50 networks) across varying input dimensionality.

I find the role of bias section illuminating as well. To the best of my knowledge, I haven’t seen a generic negative result like this, that any bias-less network will have a risk lower bounded by a function of the network width or the support of the negative training samples.

Weaknesses:
The authors do acknowledge it and give a tighter bound subsequently under stronger assumptions, but Theorem 3.1 has an upper and lower bound gap that is growing with p. It’s not really a weakness of the paper, but it would be nice if the authors wrote a bit about when this bound can be very tight (as a function of p, m?).

Theorem 4.3 seems to indicate a dependence on p being smaller than some constant divided by the width of the network squared, which seems to indicate for very wide networks that this result holds for p shrinking -> 0. While interesting, I’m not sure if this is the most general result and the authors don’t discuss in too much detail the implications of this, and what settings of network width lead to various results apart from n=O(1). For instance, it could be useful to assert if p is meaningful for n >> m or n ~ m, etc. There are common settings of network width with relation to number of samples or input dimensionality and at first glance I don’t know whether this theorem will give me a useful result in many of those settings. 

Additionally, this theorem trades off one assumption (convergence to a max-margin solution vs. KKT point of max-margin problem) for another (output weights are fixed as +- 1 and only the input weights are trained). They mention this theorem stems from attempting to drop the strong assumptions of the prior theorem (Theorem 4.1), but to me it seems perhaps to still include a new, strong assumption. Granted, I’m unfamiliar with the works cited in the footnote that study this setting in particular. Maybe the authors can provide a little exposition about why this setting is of particular interest and what motivates it in the grander study of neural networks?

Missing concluding thoughts, would like to see some final discussion on everything and where the authors see next steps going.

Other notes / fixes:

- Page 5, 3rd paragraph: double “we we” in the line “we we get that the total length of all…”
- Figures could use grid-lines as well as separate line-styles to differentiate the settings for accommodation to colorblind people and for printing in black and white

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work examines asymptotic overfitting behavior on a simple class of toy classification problem for two-hidden-layer ReLU nets, finding that increasing input dimension tends to push the model from tempered towards benign overfitting.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality: this is, to my knowledge, the first analysis of tempered overfitting for nets outside the kernel regime. The broader problem's admittedly decently well-studied, and the conclusions here are more or less in line with intuitions from other works, but the authors are doing something new here.

Quality: while I haven't checked the proofs, the theoretical results seem conceptually solid. While they rely on certain assumptions (e.g., assuming you've reached a KKT point or local minimum), the authors are upfront about these. I have some Qs about the experiments below.

Clarity: the paper's very well organized and written. Most things were clear on a single linear readthrough.

Significance: to my mind, the nice takeaways here are (a) the identification of a very simple toy problem for the study of these regimes and (b) theoretical results for classification that complement those of Mallinar et al. (which only study (kernel) *regression*). It's nice that the authors recover the $\Theta(p)$ scaling in their Theorem 3.2. I hadn't seen the cited results about margin maximization used in a setting like this; it's nice that they let one avoid consideration of the network dynamics entirely.

Weaknesses:
One omission here seems to be comparison to the experiment of Figure 9 of Mallinar et al, which shows that Laplace kernels basically exhibit the core phenomenon found by the paper: i.e., larger input dim pushing fitting from tempered to benign (with no need to scale dim w.r.t. dataset size).

I suppose the biggest weakness here is that this paper's results seem like exactly what you'd intuitively expect from (the much simpler model of) kernel regression, and so it's sort of unclear what new thing we've gained from this long set of calculations. (For the purposes of review, I wouldn't count this *against* the work -- and I do appreciate that establishing facts for neural nets is hard, and improving techniques there is useful.) It's also sort of unclear to me that the proof techniques used here will scale to other problems -- they seem clever but ad-hoc, and the tempered results rely crucially on the input space being 1D -- and so it's not obvious that, e.g., the specialized + expected results here will enable extension to other more general + powerful conclusions later.

The empirics seem generally good, but a bit odd in certain ways. For example, the input dimensions are very large! The aforementioned Figure 9 of Mallinar et al. suggests that much smaller input dims should be needed to see the correct behavior here. (Could be due to the difference between classifcation + regression; see note A below). A bigger oddity's that the difference in training set size between the two plots of Figure 1 is only 4x, which is quite small. It seems like the authors are trying to say that maybe ""when [input dim] >> [dataset size], fitting is benign, and vice versa,"" but if that's the message, there are much more compelling ways of showing that, including having much greater variation in both parameters or making a 2D heatmap at some fixed and intermediate $p$.

Another concern with the empirics is that converging can presumably take a very long time when you're relying on the exponential tail of the loss fn to push you. Seems worth confirming that these experiments won't change after a much longer training time.

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies the phenomenon of overparameterized neural networks (NNs) generalizing well even when trained on noisy data. Previous research focused on ""benign overfitting,"" where interpolating predictors achieve near-optimal performance. However, recent empirical observations suggest that NN behavior is better described as ""tempered overfitting,"" with non-optimal yet non-trivial performance that degrades with increasing noise levels. In this study, the authors provide theoretical justification and empirical validation, showing that the type of overfitting transitions from tempered to benign as the dimensionality increases in a simple classification setting with 2-layer ReLU NNs. Their findings highlight the intricate connections between input dimension, sample size, architecture, training algorithm, and resulting overfitting.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
This paper establishes the connection between data dimensionality and the occurrence of benign or tempered overfitting. It presents three theorems that address benign, tempered, and catastrophic overfitting, respectively, corresponding to one-dimensional data, high-dimensional data, and intermediate-dimensional data.

The study focuses on a two-layer ReLU neural network that includes a bias term, which is a more comprehensive approach compared to recent works that examined the two-layer ReLU neural network without a bias term. Additionally, a specific data distribution is considered in this paper. Furthermore, the paper explores the relationship between the bias term and the occurrence of benign/tempered overfitting.

Weaknesses:
The findings presented in this paper are built upon the outcomes (specifically, the convergence to the KKT point of the max-margin problem) obtained in previous studies by Lyu and Li (2020) and Ji and Telgarsky (2020). To be more precise, the results in this paper depend on either achieving convergence to a KKT point or making stronger assumptions, such as converging to a local optimum. Nonetheless, these assumptions are reasonable considering that numerous other papers also derive their results based on the convergence to the KKT point, such as the work by Frei et al. (2023).

It is important to note that this paper exclusively focuses on a simplistic data model, where the samples originate from a unit sphere and the labels are fixed constants, specifically +1. The discussion does not extend to other more prevalent data distributions.

Limitations:
Same as weaknesses.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper explores the spectrum of benign to catastrophic overfitting in a specific instance of a 2-layered neural network with ReLU activation when the training data has noisy labels with proportion $p$. The authors when the input dimension is , the test error is lower bounded proportionally to $poly(p)$ under different assumptions. When the input dimension is high, i.e. $d=poly(n)$, a NN overfitting (i.e. achieving perfect training accuracy) is benign and good generalization performance is exhibited. The authors also consider the intermediate  regime for the input dimension and show a gradual behavior for the error rate as a function of the noise factor.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is very well written with a great exposition and tradeoff between results and intuition.
2. Results presented seem comprehensive and I like the fast the authors do not neglect the intermediate regime and provide empirical evidence where theoretical analysis is not attainable.
3. The authors make their theoretical results accessible and explain the importance of the bias for the analysis.

Weaknesses:
1. The paper is missing a discussion/conclusion section to tie the results to real scenarios. Consider shortening one of the sections/proof sketches in favor or the above.
2. The assumptions of the theoretical results are not very realistic, specifically in theorem 4.3, the input dimension is big with respect to both m and n, this does not follow common practice of deep learning where the number of parameters exceeds the number of examples but usually the input dimension is not very big.
3. The considered architecture is rather limited, it would be interesting to explore empirically with much greater depth and not only 2 and 3.

Limitations:
The assumptions made in the theoretical setup are not very realistic, that being said, the reviewer appreciates the difficulty in obtaining such theoretical results. I would recommend that the authors discuss the assumptions and limitations in a dedicated section.

Rating:
8

Confidence:
3

";1
gSyjaunurQ;"REVIEW 
Summary:
- This study proposes a new framework to combine neural coding concepts of information transmission and probability density modeling.
- This framework is based on an even code principle where the output response density strives to be even, given some arbitrary input density.
- The authors show that this coding principle produces sensible bases for low-dimensional inputs, and orientation-tuned filters for natural image patches.
- While conceptually straightforward, it is unclear to me whether this study provides unique insight into sensory coding in neural populations.

UPDATE: Sep 1, 2023. I have read the rebuttal, and maintain my score (see details below).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The study is clearly presented.
- The concepts of max/min entropy on the output and input densities are conceptually simple to follow.
- The numerical experiments are reasonable.

Weaknesses:
- This paper begins with what seems like a false dichotomy of information transmission vs. sensory probability density modeling. Indeed, from a pragmatic point of view, how can one guarantee optimized information transmission without having a good density model of the signal to be transmitted? There exists literature in this area (see questions section), and the motivation/framing of this present study is concerning.
- There is a bit of a conceptual leap from 1 or 2 pixels to full image patches, with additional complexity and machinery introduced. The described rationale seems reasonable enough, but it is unclear whether the two-pixel orthogonal case can provide adequate intuition for the multi-dim case. Would a 2D non-orthogonal example be illuminating at all?
- Unclear to me whether these results, which rely on binary coding provides theoretical insight for real neural coding. Spikes are inherently binary, yes, but typically spike counts/rate are what is considered the informative variable in neural coding.

Limitations:
There was no discussion of limitations. Unclear to me what the drawbacks are of this approach compared to existing literature.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper presents a method for the representation of elementary natural images, based on the observation that classical studies in computational neuroscience focus mainly on methods to improve code efficiency, but that this could be complemented by a study of probability density modeling between neighboring pixels to improve image representation. This work consists in studying a coding principle based on a probabilistic representation and its formalization in a form of variational optimization. The paper presents the elementary method for a single pixel, then extends it to two pixels, and applies it to  small images extracted from natural images. This method is enhanced by a heuristic that allows  to formulate a cost function and thus derive an optimization algorithm. The results allow  to numerically validate this principle by deducing output statistics, as well as the emergence of local contour detectors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
A major strength of this paper is that it derives the image representation algorithm from fundamental principles of machine learning, particularly probabilistic representations. In this way, it rigorously defines the problem of establishing dependencies between the luminance values of neighboring pixels.


Weaknesses:
The first limitation of this paper is that it applies to very elementary signals, i.e. a pixel, a pair of pixels, or small images of dots. As the initial aim of the paper is to understand the computational functioning of the biological networks that underlie the efficiency of vision, this approach is extremely caricatural, and dismisses many fundamental aspects, such as the largely parallel processing of large images, the use of large neural networks, or the ability to process multimodal images, in color or in motion, or more generally hierarchical processing that can be forward, but also modulated by feedback signals. Finally, the results that have been obtained, for example for the detection of local elementary contours, are difficult to interpret quantitatively and seem very preliminary.

Limitations:
Finally, these questions about the paper reveal the main limitations of this work.

In particular, the introduction to the paper presents at length principles that seem very general, such as Shannon entropy, and the rest of the paper does not sufficiently highlight the novelties that are brought forward. This brings to light a main limitation of the paper, which is the fact that the propositions that are put forward are very ambitious, but the results are applied to very limited situations.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper explores the relationship between the information theory approach and the probabilistic generative model approach in the context of understanding neural coding. The author suggests that maximizing the information-carrying capacity of output channels and modeling the input probability distribution can be pursued as independent dual objectives. To investigate this hypothesis, the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. The resulting codes obtained for the images exhibit similarities to edge detectors and orientation-selective neurons in V1, akin to many efficient coding models developed over the past two decades.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The presentation is reasonably clear.  It is rather interesting that the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. 

Weaknesses:
While the idea that both information transmission and probabilistic modeling of the images should be taken into consideration simultaneously might be new,  and is sufficient to learn edge detectors and orientation-selective neurons, the author has not established it is a necessary condition. In fact, literature in the last thirty years (from Law and Cooper's to Olshausen and Field and many others)  that such codes can be learned based on either one of the criteria. 

It is surprising that the V1 neural codes were assumed to be sparse binary codes. What is the evidence?  The distribution of output values as shown in Figure 2a has not been observed biologically.  This brings the Even Code hypothesis into serious question. 


Limitations:
Societal impact not discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors studies simple of neural encoding. The question is whether two
distinct goals, accurate transmission of information and learning the
distribution of environmental stimuli can be achieved simultaneously. The
authors argue that yes, it can, using the key assumption of a uniformly
partitioned input space. The coding principle of the authors is finally applied
to image patches, where it yields edge-like features.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The author studies an important question, namely simple coding schemes that
  reproduce filters that resemble those of deep convolutional networks or parts
  of the visual system.
- The author develops intuitions in simple toy models before moving to
  applications on real images.
- The filters shown in Fig. 3 bear a striking resemblance to the filters of a
  trained VGG model (although I have some questions on the methodology, see
  below)



Weaknesses:
I found the article confusing to read in a few places. For example, early on,
the author states that ""maximising the rate of transmission"" is equivalent to
maximising the entropy of the output distribution $H_Q$. I would think that what
you transmission of information requires maximising the mutual information $I(X;
Q)$ between the distribution over inputs and outputs. (around eqs 1 + 2; note
that the notation is rather confusing here, using lower-case $p$ for the
distribution over input stimuli $x$, and capital $Q$ for the distribution over
output states $y$). Why are you maximising simply the entropy of the
distribution over outputs?

Similarly, in the section on the even code principle, I'm confused by the
question of how the IPU models the input distribution. The way I read Sec. 2,
the IPU is considered a function of the stimuli $y=f(x)$ -- in that sense, it
doesn't model the input distribution, we cannot sample from it. It can give a
more or less faithful representation of $x$, as measured for example by mutual
information if the mapping is probabilistic, 

As you then move on to learn two pixel distributions, I'm confused about your
use of MLPs. MLPs are powerful neural networks, but you seem to use them to
""learn"" to partition the input space into equal partitions - is this not
possible by just writing down a simpler model?

Given my trouble understanding the first few sections, I cannot competently
comment on the experimental results - while the filters obtained by the authors
do bear a striking resemblance to the filters of a VGG network, I don't really
understand how the author obtained them. Some additional clarifications would
therefore be more than welcome.


Limitations:
See above

Rating:
3

Confidence:
3

";0
VCOZaczCHg;"REVIEW 
Summary:
The paper introduces a novel learning approach designed for robot teams to acquire a preferred policy for collaborative task completion using human expert-generated data. Additionally, the method enables the robots to gain an understanding of the theory of mind of each individual agent within the team.






Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper presents a learning framework that enables robots to acquire teaming strategies through human expert demonstrations, while simultaneously learning the theory of mind between agents. The effectiveness of the proposed approach is demonstrated on synthetic and real data collected from humans. The method outperforms other baselines in accomplishing the defined task.

Weaknesses:

1.I agree that incorporating learning communication policies is crucial for both multi-agent learning from demonstrations (MA-LfD) and multi-agent reinforcement learning (MARL). However, the tasks presented in the paper may be considered relatively simple, which might not fully showcase the author's capabilities. It would be beneficial if the author could demonstrate the proposed methods in more complex 3D environments such as ITHOR or Habitat, or even leverage visual-based theory of mind datasets like ""Learning triadic belief dynamics in nonverbal communication from videos"" or ""BOSS: A Benchmark for Human Belief Prediction in Object-context Scenarios.""
2.I'm curious if the author has conducted a performance comparison between a single human expert and multiple experts (N) for each agent. Benchmarking the differences in performance could provide valuable insights into the impact of expert diversity on the proposed approach.
3.Could the author elaborate on the rationale behind selecting GRU (Gated Recurrent Unit) over a transformer for the proposed MixTURE architecture? Understanding the specific reasons behind this choice would enhance the clarity and comprehensiveness of the paper.

Limitations:
Yes, the authors have addressed all the limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper introduces a model, called MixTURE, for learning multi-agent collaborative policies from human demonstrations, addressing the challenges of coordinating heterogeneous agents in complex tasks. MixTURE leverages a mutual information maximization-based differentiable communication module to reason about and adapt to diverse human demonstrations. The authors evaluate MixTURE through synthetic and human-subject experiments in the FireCommander (FC) domain. In the synthetic evaluations, MixTURE outperforms baselines, achieving significant improvements across difficulty levels and setting a new state-of-the-art in learning collaborative policies for complex tasks.
In the human-subject experiments, the proposed approach demonstrates its ability to learn high-quality collaboration policies from diverse human-generated demonstrations, surpassing frameworks that solely rely on demonstrated communication. The model's ability to reason about human demonstrations and adapt to trajectory distributions is highlighted as a key factor in its success. The statistical analysis supports the claims made by the authors. It shows that relaxing the need for demonstrating communication reduces the workload for human experts and improves system usability. Furthermore, the results indicate that MixTURE achieves higher performance scores, better scalability to complex scenarios, and lower demonstration time per step compared to frameworks with demonstrated communication.
Overall, the results support the effectiveness of MixTURE in learning collaborative policies, its ability to leverage human demonstrations, and its potential to enhance human-robot collaboration in complex multi-agent systems.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Originality:
The paper introduces a novel approach by explicitly addressing the challenge of inter-agent communication in dynamic and partially observable domains.
The use of a differentiable communication module based on Mutual Information (MI) is a creative and original contribution.
The paper's focus on learning communication strategies from human demonstrations and addressing heterogeneity and diversity in human data sets it apart from previous methods.
The elimination of the need for an expert to demonstrate a communication strategy and allowing robots to automatically learn communication protocols is a useful idea.

- Quality:
The technical details and algorithms are thoroughly explained, ensuring reproducibility and soundness of the proposed approach.
The paper demonstrates a fair experimental methodology, conducting evaluations in both synthetic and human-subject environments.
The inclusion of ablation studies and statistical analysis enhances the evaluation and supports the provided claims.

- Clarity:
The paper is well-structured and organized, with clear sections that guide the reader.
The authors provide comprehensive explanations of the methodology, algorithms, and experimental setup.
The use of figures, tables, and visualizations aids in understanding the concepts and results.
The paper uses appropriate terminology and notation, making it accessible to readers familiar with the field.

- Significance:
The paper addresses a significant research gap by explicitly considering the challenge of inter-agent communication in collaborative multi-agent problems.
The proposed approach has practical implications for teaching multi-agent coordination, as it eliminates the need for explicit communication demonstrations from experts.
The use of real human-generated data demonstrates the model's ability to cope with variations in demonstration styles and strategies, increasing its applicability in real-world scenarios.
The experimental results support the claims and show the effectiveness of the proposed MixTURE model, contributing to the advancement of Multi-Agent Learning from Demonstrations (MA-LfD).


Weaknesses:
A discussion on how MixTURE could be used for continuous (state/action) problems would be helpful.

It’s also unclear whether the proposed approach would be generalizable to other domains.


Limitations:
Are there specific scenarios or conditions where the MixTURE model might struggle or fail to learn effective communication strategies?

As stated previously, a discussion on whether using MixTURE for continuous (state/action) problems is not trivial would be useful.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a Mixed-Initiative Multi-Agent Apprenticeship Learning (MixTURE) framework to teach a team of agents using demonstrations provided by individual human experts. It learns both a cooperative policy for each agent and the inter-agent communication policy for each agent. The proposed MixTURE, including the framework and the communication learning model, is the main contribution of the paper. Human subject studies on the proposed framework can also be viewed as a contribution.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ Evaluating the framework with statistical analysis/comparison and human subject studies is a strength.
+ Enabling multi-robot learning from demonstrations by an individual expert has the potential of improving multi-robot learning efficiency and human knowledge transfer to robot teams.


Weaknesses:
-  The problem of learning communications is not well justified or formulated. For example, what are assumptions on the protocol and constraints for the multi-robot communication (broadcasting or message passing)? As an example, connected autonomous vehicles use a broadcasting mechanism and follow a standard data format for communication. Why to use a differential communication channel? Are there examples of multi-robot systems using a differential channel for communication?

- If communication is learned based on a mutual information metric in the proposed framework, how does learning communication benefit from human demonstrations? Learning communication seems to be a separate problem from LfD in this work.

- The communication learning module needs the global joint action-observation to compute each agent’s outcome message. How is the joint action-observation information obtained during decentralized execution?

- How the approach can be generalized to realistic multirobot settings and environments? In the experiments, agents have discrete states and actions, and the environment is a grid world. These  experiment settings do not well support the argument of human training of “robot” teams.  

- The proposed framework follows a centralized training decentralized execution paradigm, but does not review related work on this paradigm or discuss its pros and cons. Especially, how important is multi-robot communication in centralized training decentralized execution methods?

- Existing works on learning communications (e.g., when, what, who and how to communicate between multiple agents) are not reviewed. 

- As an opinion, advantages of using an individual human expert to provide demonstrations for a team of agents are not convincing, especially for in decentralized multi-robot systems. The general goal of LfD is to transfer human knowledge to robots/agents. Humans behave differently when working alone or with other teammates. In addition, when a single human expert controls a team of agents to perform a task, the expert still provides a sequence of decisions for different agents. However, decentralized teaming involves concurrent decisions, may have conflicts, and has tightly-coupled teaming activities (e.g., two robots carrying an object). All the above properties of teaming cannot be demonstrated by a single expert.


Limitations:
No negative societal impact of the work is perceived. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes a learning framework for multi-agent coordination using communication from human expert demonstrations: Mixed-Initiative Multi-Agent Apprenticeship Learning (MixTURE). A human expert teaches a team of robots to collaborate on a task by demonstrating actions for every robot, while the robot team learns a emergent communication protocol that encourages sending information about the joint observation. An attentional communication module and a mutual information maximization reverse model  are used. The framework is evaluated on three domains with different levels of difficulty and complexity. They also conduct a human-subject user study to collect real human data and assess the usability of their framework. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper introduces a interesting framework that leverages both human demonstrations and emergent communication to learn effective coordination policies among several robots. 
- A comprehensive evaluation of their method on synthetic and real human data is conducted. 

Weaknesses:
- The proposed framework assumes that the human expert has access to the aggregated / joint observation of all the robots at each time step and the human expert can command every robot's action simultaneously at each time step. However, this assumption is rather unrealistic for real-world robot teams or robot teams in simulation. 
- The synthetic environments that the framework is evaluated on is not large-scale enough compared to common testing environments for multi-agent reinforcement learning like SMACv2 (_SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. Ellis, Benjamin and Moalla, Skander and Samvelyan, Mikayel and Sun, Mingfei and Mahajan, Anuj and Foerster, Jakob N. and Whiteson, Shimon._)
- In the experimental results section, no qualitative analysis or visualization of the learned policies and communication protocols are provided. It would useful to observe how robots coordinate and communicate in different settings. 
- An expert heuristic is used to collect the expert demonstration dataset. However, heuristics can struggle to produce optimal demonstrations for more difficult environments. Then algorithms of multi-agent reinforcement learning still need to be used at the first place to collect the expert demonstration dataset. 

Limitations:
Not adequately.
- need to mention the strong assumptions on the expert's joint observation and action space


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a Multi-Agent Learning from Demonstration approach for learning multi-agent policies for collaborative tasks. The approach learns from human demonstrations of the joint policy, but does not require demonstrations of inter-agent communication, which is learned during training via online interaction. Results are presented on learning policies from both synthetic and real human data. A human study is performed demonstrating the increase in performance and decrease in workload for the proposed method, compared to methods that require demonstration of inter-agent communication.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- This paper relaxes a limiting assumption made by some prior works that a human expert provides demonstrations for both environment actions and communication actions, which can be taxing for a human to actually provide. In doing so, this work strikes a balance between prior RL and IL approaches for learning the action and communication policies.
- The paper trains policies on both synthetic and human data. MixTURE is seemingly quite effective on learning from real human data.
- The paper empirically verifies the motivating claim about the effect of removing communication actions from a human expert's workload, via a human study on 55 users.
- A new ""mixed discriminator"" is proposed, where the discriminator is given global observations but only local actions. Per the discussion in Appendix 5.2, this choice exhibits improvements on the decentralized and centralized discriminators from MA-GAIL.
- The work is well-written and clear. Source code is provided for reproducibility.

Weaknesses:
- The training objective has multiple moving parts (GAIL, PPO, BC, and MIM) which could potentially make training difficult; some additional discussion on this and how sensitive the method is to different weights on the losses would be helpful.
- Contribution 2 is the mutual information maximization-based communication learning model. On the 5x5 PP and PCP environements, the MIM does not have a significant effect. In the FC environment, it moderately increases sample efficiency. Have ablations been run on the larger/more complex environments? This may provide more robust support for the claim that MI maximization is ""helpful for domains with increased task complexity,"" and in general may more strongly justify the inclusion of the MI objective, which otherwise complicates the training procedure.
- Related to the above, in Section 5.1 the authors conjecture that the MIM is a key point in dealing with diversity in the human data, i.e., that it ""provides the model with the ability to reason about the underlying human demonstrations and cope with the trajectory distribution through automatically finding a suitable communication protocol."" It would be valuable to discuss what communication protocol is learned in these settings (if it is interpretable) and why it so strongly outperforms MA-GAIL trained on the dataset with demonstrated communication actions.

Limitations:
- The authors note that the method does not currently account for suboptimal demonstrations. If the method is particularly sensitive to how the various losses are weighed, this may be an additional limitation in terms of the amount of tuning required.

Rating:
6

Confidence:
3

";1
q9WMXjUxxT;"REVIEW 
Summary:
This study focuses on multiple constraints setting in safe reinforcement learning, and an interesting method is proposed by leveraging gradient integration methods. Moreover, the feasibility of multi-constraint problems is addressed, TD distribution method is introduced to decrease the estimation bias.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. Multiple constraints are considered.
2. The code is provided.
3. The writing quality is good.

Weaknesses:
1. Some papers are not investigated, e.g., [1] and [2]	

[1] Gu, S., Yang, L., Du, Y., Chen, G., Walter, F., Wang, J., ... & Knoll, A. (2022). A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330.

[2] Garcıa, J., & Fernández, F. (2015). A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1), 1437-1480.

2. The experimental results do not convince me, as shown in Figure 3, other baselines also present better performance than this study, e.g., the WCSAC method.
3. Could you compare the method with PPO Lagrangian?

Limitations:
1. The cost is assumed as convex. However, in most cases, the constraints may be nonconvex.
2. The computation complexity and sample complexity should be provided to prove the effectiveness of this study.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposed a trust-region method for handling multiple constraints in safe RL for CMDPs. If the TRPO-like gradient calculation with multiple constraints is not feasible, the authors proposed a gradient integration method to calculate feasible gradients. The authors also proposed a TD-$\lambda$ method to calculate the target distribution of distributional critic. Experimental results have shown that the proposed methods seem to outperform baselines.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
## Originality
The gradient integration is novel and looks useful for the infeasible initial stage when solving CMDP. Computing the TD $\lambda$ for the distributional critic is also novel to me. 

## Quality
The paper is well-written and includes many technical details, which is good for understanding and reproducing the work.

## Clarity
The paper is very clear. Especially, I like the two figures which explain the ideas very well and I get the idea immediately. The notation is a little bit messy but clear enough to read.

## Significance
The TD-$\lambda$ method for distributional critic might have a broad effect on other distributional RL algorithms, but I am not sure if similar approaches have been proposed to solve the problem.

Weaknesses:
1. The paper has novelty but the contributions are not clearly demonstrated. From the results in Figures 3 and 4, I did not see too much performance improvement, solid constraint satisfaction, or faster convergence to feasible policies compared to baselines. Given this paper focuses on practical contributions, the performance results are not convincing. 

2. Two contributions of this paper are not related. Gradient integration and TD-$\lambda$ seem to stand alone on their own. The authors should explain why you put these into one paper and how they contribute together to improve the safe RL.

3. The gradient integration problem is fragile and might have some problems in the finite-convergence theorem. I will put more discussion in the questions section.

Limitations:
The authors addressed the limitations well.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper tries to address the problem of safe RL with multiple constraints with a safe distributional actor-critic (SDAC) approach. The approach includes a gradient integration method to manage the infeasibility issues in multi-constrained problems and a TD$(\lambda)$ target distribution to estimate risk-averse constraints with low biases. Experimental results show that the proposed approach outperforms the baselines in both single- and multi-constrained tasks. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Safe Rl with multiple constraints is an important problem. The presented approach uses a gradient integration method to address the infeasibility issues of the trust-region-based algorithms with theoretical guarantees and proposed a TD$(\lambda)$ loss to reduce the estimation bias of the critics. Sufficient experimental results and ablation studies are provided to support the claims of the paper. 


Weaknesses:
1. The writing of the paper can be improved. There are little background information and intuitions introduced, which makes the paper hard to read. For example, In Section 2 when introducing the trust-region method with a mean-std constraint, it would be better if the authors can introduce the intuitions behind the equations instead of just putting the equations there, because [Kim and Oh, 2022a] is not a very well-known paper. Also in Section 3.1, little information is provided about the intuition of equation (9). 

2. See “Questions”. 


Limitations:
The authors discuss the limitations well in Section 6. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents a safe reinforcement learning (RL) algorithm called SDAC for handling multiple constraints in safety-critical robotic tasks. SDAC incorporates risk-averse constraints and makes two key contributions: a gradient integration method for handling infeasibility issues and a TD($\lambda$) target distribution for estimating risk-averse constraints. Experimental results show that SDAC outperforms safe RL baselines, achieving fewer constraint violations and faster constraint satisfaction.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In general, the work is solid and contributes quite some novel ideas for safe RL.

- The formulation of safe RL problem with multiple constraints is very important for real applications.
- The infeasibility issues in constrained optimization are explicitly considered.
- The risk-averse safety measures are practical to safety-critical problems.



Weaknesses:
- While the measures have low bias, it is still important to consider the bias when making decisions.
- The presentation of the technical details, particularly the quantile regression part, is challenging to comprehend. It might be beneficial to refer to Dabney's paper, which provides a clearer explanation of the related content.
- From the empirical analysis, the proposed method cannot ensure safety during the early stage of training, despite the theoretical guarantees.


Limitations:
Yes, the limitations are clearly stated at the end of the paper. 

Rating:
6

Confidence:
4

";1
NbkjMn7X8H;"REVIEW 
Summary:
The paper studies robust nonparametric regression under poisoning attacks. The input is samples from some fixed distribution and the goal is to approximate some unknown function on the input space. It is assumed that the values observed in q of the samples are adversarial. In this setting, classical approaches such as k-NN estimators can fail. This is because a single adversarial sample can affect the prediction on many points. In order to avoid this issue, the paper proposes robust variants of nonparametric regression.

The first result is a bound on the convergence rate of an M-estimator based on Huber loss minimization: the initial estimator has minimal optimal ell_infinity risk. When the number of adversarial samples is not too big, the ell_2 risk is optimal.

It is also shown that the estimator can suffer when many of the adversarial samples are concentrated within a small region. In order to resolve this issue, the paper proposes a correction step that projects the estimator into the space of Lipschitz functions. Upper bounds on the rate of convergence of the corrected estimator are established. Numerical results show that both estimators outperform standard methods in a simple low-dimensional synthetic scenario.



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Nonparametric regression is a fundamental problem. The paper studies a natural adversarial setting where a small number of input samples are corrupted adversarially. This research direction can produce more robust useful data-analytic methods.

The upper and lower bounds match up to polylogarithmic factors when the number of adversarial samples is not too large.


Weaknesses:
The experimental evaluation is very limited. This is unclear whether the proposed methods can lead to significant robustness improvements in real-world scenarios.

No runtime analysis is given for the corrected estimator. This is a continuous optimization problem, so it is not clear how easy it is to solve in practice, especially in high or moderately-high dimensions.


Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
A robust version of the classic non-parametric problem is studied, when the training data is under an adversarial poisoning attack. The work is primarily theoretical, and makes assumptions on Lipschitzness and boundedness of the underlying function, boundedness of the data density, restrictions on sharpness of the data domain boundary, sub exponential noise. With some restrictions on the kernel, and appropriate parameters, upper bounds are shown on the $\ell_2$ and $\ell_\infty$ loss for the robust regression fit using Huber loss minimization. Lower bounds are obtained under the same assumptions, which match asymptotically with the upper bounds for $\ell_\infty$ and a correction is presented to match the upper bound for $\ell_2$ loss as well. Numerical simulations show usefulness of the algorithms on simulated one and two dimensional data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The authors study robust nonparametric regression under poisoning attacks for smooth underlying functions under certain assumptions on the data and domain, under which asymptotically tight rates are obtained for $\ell_\infty$ loss by suitably parameterized Huber loss.
2. A novel correction method is proposed for which tight rates are obtained for $\ell_2$ loss as well. Together with above, the optimal algorithms under the given assumptions are obtained for both losses.
3. Numerical simulations are performed to verify the theoretical results.

Weaknesses:
1. The assumption needed for proving the theoretical result, specifically assumption 1(b) is too strong. Effectively, it reduces the possible test distributions to ""near-uniform"" distributions since there is both an upper and a lower bound on the probability density function. This assumption is too strong to make the results useful in most practical situations.
2. No insights is provided into the proofs of the theoretical results; in particular there are no proof sketches in the main body. This makes it challenging to understand the contributions, given the work is primarily theoretical.
3. Experiments only involve very specific numerical simulations. How does the approach work on real regression datasets? The effect of changing the hyperparameters is not studied and how they are selected is not described.
4. Huber loss is already known as a technique for robust nonparametric regression [1] and is not novel, the paper should mention that only the analysis under the present assumptions is new. The novel correction method is computationally intractable.

[1] Maronna, R. A., Martin, R. D., Yohai, V. J., & Salibián-Barrera, M. (2019). Robust statistics: theory and methods (with R). John Wiley & Sons.

Limitations:
The authors note several limitations of their work in a dedicated section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose two new nonparametric regression methods that and study their resilience under poisoning attacks.  The baseline for comparing the performance of the proposed methods is kernel regression (aka Nadaraya-Watson estimator).  The proposed methods are two, since the initial idea that the authors have, turns out to be vulnerable to poisoning attacks that are concentrated in small regions and a large budget is spent there.  Hence, the authors arrive at a `corrected' estimator which behaves better and is closer to optimal behavior.


After rebuttal:

For the largest part I believe that the authors have provided sufficient clarifications to various issues that were raised and moreover are willing to integrate comments and clarifications that came up during the discussion period in the final version of the manuscript. Therefore, I am increasing my score from reject to borderline accept; I am also increasing the soundness from fair to good as well as the presentation from poor to fair.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ New methods for regression that are resilient to poisoning attacks.
+ Both theoretical and practical results.

Weaknesses:
+ Missing a section with preliminaries or background knowledge, where notions and notation that the authors use in the paper is well-defined. For example, in the section for preliminaries you can give information such as:
   - define loss functions, 
   - discuss kernel regression and separate the presentation from your initial estimator,
   - discuss notions that you use when it is unclear what these things are; e.g., ""bandwidth"".
   - define functions that you use without explanations at the moment; e.g., in (18) we see Clip and med; what are the arguments and what do these functions do?

+ References not in alphabetical order.

+ In line 52-54, you indicate that your approach is similar to a combination of an $\ell_1$ and $\ell_2$ loss functions. At that point in the text, I was expecting a comparison with ""elastic nets""; you may want to consider adding a small comment, or consider rephrasing and avoid such expectations from the readers.

+ The order by which some things are presented should probably change. For example, in lines 115-124, I think you need to give a slightly better explanation so that the reader can get better intuition, and moreover, this discussion should come up before you lay out the equations of the method that you propose.

+ You cannot start sentences with as in lines 167 or 168. You could add a word like ""Part"" or ""Parts"" in the beginning and make it read more naturally even if it takes a bit more space.

+ Larger font size in Figure 1 is expected.

+ It would be nice to see some commentary on the bounds and argue how they compare against each other and potentially compared to other methods that you cite in the literature. 

+ In general, you have no comparison with parametric methods, either in theory, or in the experiments. Alternatively, provide an explanation as to why you don't have such comparisons.

+ It is unfortunate and normally it should not be an issue, but the authors need help on writing a paper that has fewer spelling or expression mistakes. The additional problem here is that the authors also have a big appendix, which I suspect is written similarly along the main text. So, fixing the main text is not enough. The paper needs to be proofread by someone in its entirety, including the appendix of the authors.

I think that you have a very interesting story to tell, but the paper needs restructuring and better presentation of what you have accomplished.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies nonparametric regression, where an adversary can corrupt $q$ samples from the training set. The paper proposes robust estimators for this problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

The paper studied the Huber loss minimization approach under adversarial noise, giving theoretical upper bounds and lower bounds. 



Weaknesses:
The organization could be improved, e.g. the last two paragraphs of section 3 could conceivably be placed elsewhere. 

Some aspects of this paper are not really my area, so I cannot provide many helpful comments. 

Limitations:
The author addressed limitations. 

Rating:
6

Confidence:
2

";0
31zVEkOGYU;"REVIEW 
Summary:
The paper discusses the phenomenon of overestimation, the allocation of higher likelihoods to out-of-distribution data points, in deep generative models. It analyses two factors which may cause the overestimation problem specific to VAEs from a reformulation of the ELBO. These two factors are posterior collapse and a difference in entropies between in-distribution and out-of-distribution datasets. The paper proposes, again specific to VAEs, a method called AVOID for alleviating these two factors.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
* The clear definition of overestimation in Eq. (3) following is useful, also for the wider literature.
* The experimental setup is large: Table 3 demonstrates that the number of dataset combinations considered is numerous, in particular in comparison to previous work. 


Weaknesses:
* In 3.2, the authors pose the questions “When is the design of the prior proper/not proper”, but answer these questions by providing an example for each case. While this is useful for illustrative purposes, it does not answer the stated question. The first few examples are furthermore focussing on linear VAEs which are not relevant for common practical uses, which limits the relevance of the theoretical results in this section. 
* The design of the calibration term in ll. 219 is unclear. In my opinion, it is not properly explained, and important choices like SVD are not well motivated. When would SVD likely fail? Why does SVD intuitively capture the difference in entropy between the datasets?  The words “complexity” and “entropy” seem to be used interchangeably, please explain or use consistently.
* The experimental results are difficult to interpret, it is partly not possible to draw meaningful insights from it. It is worth noting that this is common in similar works on alleviating OOD detection in DGMs, the methods are hard to compare due to different experimental setups. However, in this work, important questions I have are: 1) In Table 1, in the unsupervised column, why is AVOID highlighted in bold, even though WAIC  outperforms it sometimes? 2) Where is the performance of a standard VAE without any adaptations listed?  I find this an important benchmark. 3) What is the decision criterion for OOD vs. in-distribution? Is it a threshold on the amended likelihood? If yes, looking at the density plots of Fig. 6 (b), how is it possible that there is still  a lot of overlap between the two datasets in PHP, even though the accuracy according to Table 1 is 99.2%? This seems inconsistent to me. 4) The experimental results report no standard deviations in key tables, such as Table 1 and 2 . DGM based methods are well known to be unstable, hence standard deviations would be useful. However, Table 3 partly alleviates this problem due to the large number of dataset combinations considered. 5) Table 3: I would argue that comparing CIFAR10 and CIFAR100 (and possibly other combinations) seems meaningless: The datasets are overlapping, hence it is unclear what is OOD and what is in-distribution.
* The language is sometimes unclear, in general slightly hard to understand, and could be greatly improved.

In summary, while this work demonstrates a large effort and a clear analytical approach to alleviating the overestimation problem in VAEs, important questions remain unclear. I am open to reconsidering my score upon a response from the authors.


Limitations:
* The overestimation problem is common to many DGM methods, but this work provides a solution for VAEs only. The scope is bigger, and one could argue that it might be more interesting to find the underlying root cause in all DGM methods which suffer from this problem (if there is one). Yet, considering VAEs is a very interesting start.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper first mathematically examines the unsupervised (without training label) OOD detection performance using VAE, decomposing the expected ELBO into two components: (i) entropy $\mathcal{H}(x)$ of a dataset, (ii) KL divergence $D_{KL}(q(z)||p(z))$ between the estimated $z$ and the prior. It's theoretically shown that the entropy of data distribution is defined by itself, thus may not bring benefit to the OOD detection problem (Eq. 8). Then the paper mathematical and empirically analyzes the second component. The paper shows in some simple cases the prior $p(z)$ and the dataset $p(x)$ can not fit well with the VAE model, which results in for some $x$, $p_\theta(x)$ estimated by the trained VAE model has high value when $p(x)$ has low value, which is the overestimation problem for OOD detection. The paper proposes to use post-hoc prior method (estimate the prior from the trained VAE and the ID dataset) to revise the issue of the improper design of prior and add calibration to alleviate the issue of entropy. Empirical results show that the proposed AVOID method constantly improves the OOD detection performance simply based on ELBO (Table 3), and outperforms existing unsupervised non-ensemble OOD detection methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper mathematically examines the unsupervised (without training label) OOD detection performance using VAE, decomposing the expected ELBO into two components: (i) entropy $\mathcal{H}(x)$ of a dataset, (ii) KL divergence $D_{KL}(q(z)||p(z))$ between the estimated $p(z)$ and the prior, which is quite crucial to understand the underline benefit and drawback to use ELBO as OOD score.
2. The demos including Figure 2,3,4 shows the improper of the traditionally chosen prior leading to the mismatch between prior and post-hoc prior, and the high probability of OOD sample in the prior. The observation well inspires the proposed post-hoc prior method.
3. Experiment includes varied OOD detection methods including supervised, auxiliary, and unsupervised (ensemble/non-ensemble), and shows the proposed method beats baselines within a specific category.


Weaknesses:
1. Notation is not consistent such as $p$ and $p_\theta$ in Figure 3.
2. Eq. 8 uses the entropy difference between ID and OOD distributions. Eq. 8 tells us the more diverse the ID distribution, the harder the OOD detection task. I think the OOD here should consider overall OOD distribution instead of an OOD dataset distribution. If not, I can simply define each OOD data point as a distribution which has $\mathcal{H}_{p_o}=1$ or I consider overall OOD data together (overall OOD distribution) which may have a pretty large diversity and very low entropy. Thus the motivation for the second method is not well held. I believe the idea of the second method is good itself, it leverages some extra information to improve the OOD performance.
3. Sec. 3.1 uses 3-layer NN for $q_\phi$ and $p_\theta$. The dataset is synthetic, thus I wonder whether increasing the number of training samples and NN capability would help better estimate $p_\theta(x)$. In other words, the reason that ELBO suffers from overestimation is the number of training samples, NN capability, or something else. Or perhaps the observation from Figure 3 is even when ID is well estimated, the OOD is still not well estimated.


Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies unsupervised OOD detection (i.e., training data contains no labels) using deep generative models. DGMs model the probability distribution of the inputs, and can be an ideal candidate for unsupervised OOD detection. The authors study one specific class of DGMs, namely VAEs. They show that VAEs suffer from overestimation problem ($P(x_{ood}) > P(x_{id})$) due to two main reasons — dataset’s inherent entropy and improper design of prior distribution. The paper then proceeds to theoretically suggest ways to mitigate this issue, and shows experimental results that do so.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The theory of the paper is simple but inspiring, and matches neatly with the designed algorithm.
2. The experiments are well-designed and executed.
3. The ablation studies are well-done.

Weaknesses:
1. Prior work such as [1] that discusses causes of deep generating models’ (specifically, normalizing flows) reason for failure to perform OOD detection was not cited/discussed in the paper. Similarly, [2] is also an important paper for using DGMs for OOD detection that wasn’t cited.
2. The paper is not self-contained and the organization could be improved — for example, one could put the limitations in the main paper instead of in the appendix.
3. Notation of the paper. For example, $p(x) = N(x | 0, \Sigma_x)$ can be more readable as $x \sim N(0, \Sigma_x)$, following more commonly used convention.

[1] Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson. Why Normalizing Flows Fail to Detect Out-of-Distribution Data, https://arxiv.org/abs/2006.08545, 2020

[2] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality. arXiv preprint arXiv:1906.02994, 2019.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
In the context of VAE, the authors identified two factors that potentially cause VAE to assign higher likelihood to OOD data than ID data. They propose a new scoring mechanism that improves upon VAE's overestimation of the likelihood on OOD samples.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Decomposing the ELBO carefully is interesting. In particular, they give a new prior design targeting the overestimation issue.
- They have a scoring method that improves upon the standard ELBO, which partially validifies their analysis.

Weaknesses:
- the derivation assumes the model distribution can converge exactly to the true one, but this is impractical. 

If it does, there should be no overestimation issue to begin with (for practical datasets that are arguably separable, e.g. SVHN vs CIFAR). Moreover, even if it is possible in theory, the empirical and theoretical observations in [1, 2] will prevent this from happening in practice. 

If it doesn't, the derivation will leave an error gap that is not analyzed. In short, the key reasoning in the above is that real distribution is often supported on low dimensional sets, while model distribution is fully supported.

- the evaluation is a bit outdated on easier benchmarks. To solidifies AVOID's practical impact, evaluation on the harder tasks as in DoSE [3] is necessary.


[1] Dai, Bin, and David Wipf. ""Diagnosing and Enhancing VAE Models."" International Conference on Learning Representations. 2018.

[2] Dai, Bin, Li Kevin Wenliang, and David Wipf. ""On the Value of Infinite Gradients in Variational Autoencoder Models."" Advances in Neural Information Processing Systems. 2021.

[3] Morningstar, Warren, et al. ""Density of states estimation for out of distribution detection."" International Conference on Artificial Intelligence and Statistics. PMLR, 2021.

Limitations:
N/A

Rating:
5

Confidence:
4

";0
JOkgEY9os2;"REVIEW 
Summary:
[Update: After intensive discussion with the authors, I changed my score from 6 to 7. I further increase the score for ""Contribution"" from 2 to 3.]

The paper introduces two new test statistics for permutation-based two-sample tests that are related to the MMD -- called FUSE_N and FUSE_1. These are motivated by selecting/combining good kernels for an MMD-based two sample test *without splitting* the data.
The authors theoretically show that FUSE_1 corresponds to a (regularized) supremum of the MMD over the possible kernel combinations. FUSE_N, which empirically performs much stronger, is derived from FUSE_1 but where the kernels are weighted by their normalized (squared) MMD estimates instead of simply by their (squared) MMD estimates. There is no theoretical justification for using FUSE_N.

The authors provide a theoretical power analysis of the proposed tests. Furthermore they run experiments that show their test has competitive power against a recent SOTA MMDAgg, while having a constant speed-up. The code is provided in a well-structured repository, such that it can be easily reproduced or the community can build on it.

The paper also gives some general insights into what test selection strategies are generally possible when using a permutation test. Whilst this might not be very new in itself, having a clear presentation of the general thoughts in one place, might be very useful for scientist entering the field.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The delivered experiments are done very thoroughly and structured and the code is provided in a high quality.
- The paper is very well written and clarifies important (although not really new) aspects of permutation-based tests in an accessible way.
- The proposed test convinces in terms of power and runtime -- (although I am not 100% convinced by the speed-up over MMDAgg yet).
- Originality and significance: Two-sample testing is an ongoing research area and improvements and new ideas are published at NeurIPS and similar conferences regularly. The community seems nevertheless relatively small. The test statistic seems new, although its motivation is not really clear.

Weaknesses:
- The motivation for the proposed test statistic is not clear enough for me. FUSE_1 has somewhat a motivation (Sec. 4.1.) but:
  - it does not perform well and also the motivation does not arise from test power considerations.
  - It is well-known that selecting the kernel by maximizing the MMD is not a theoretically justified strategy (not scale invariant for example).
- Sutherland et al (2017) give a criterion for optimizing the kernel in terms of asymptotic power, which results in optimizing a Signal-to-noise ratio. The normalized statistic FUSE_N seems to go in this direction, but there is no motivation for the normalization given.
- Generally I am not really understanding the starting point for this paper. What motivated you to do this work and what where you trying to solve? In particular what is the similarity/difference to MMDAgg? 
- The test statistic has a parameter $\lambda$. For the theoretical results, $\lambda$ depends on the sample size. However, in the experiments (as far as I understood) $\lambda=1$ irrespectively of the sample size. How was this value determined and where is it discussed? And what is the recommended way to do this in practice?

Limitations:
nothing specific

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper looks into the problem of permutation based kernel two-sample testing. The paper is motivated by two main challenges: 

1. The first one being the fact that selecting kernel hyper-parameters through standard data splitting often reduces the statistical power due to using less data than we actually have.
2. The second challenge being that if we don't select a data specific hyperparameter for the test, a poor kernel choice will also lead to a decrease in statistical power.

To tackle these problems, the authors proposed the MMD-FUSE method that can select kernels adapted to the data without data splitting. They first restate a result from Hemerik and Goeman Theorem 2 to justify/emphasise that any permutation invariant representation/function of the data can be used a statistic that have controlled type 1 error rate. In addition justifying the adequacy of median heuristics. 

The second contribution is that they proposed to combine multiple MMD via a set of kernels, and as such optimise the test power.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The problem is well motivated as the trade off between data splitting to learn optimal kernel v.s. not splitting the data but selecting some heuristic kernel has been a long standing problem in the community. This authors explained their arguments very clearly. In addition, not only did they justify their method by showcasing the optimal MMD separation rate (Typo in line 290 btw), I appreciate they included a discussion with computationally faster sub-optimal tests to highlight between the trade off of test power and computational efficiency.

Weaknesses:
The authors have discussed the weakness in their conclusion section so I don't think I have much to add. There is some recent work on not using permutation test at all to do two sample test, such as the work of Shekhar et al. 2022. I understand it is not necessary to compare the two types of approach but might be also good to mention non-permutation based approach as a side note to complete your already-detailed background section on testing?


---

Shekhar et al. 2022 ""A Permutation-free Kernel Two-Sample Test"" arXiv:2211.14908 

Limitations:
No.

Rating:
8

Confidence:
2

REVIEW 
Summary:
The paper studies the question of choice of critical values for the MMD test. The authors introduce a new MMD-based test that allows data-driven critical values based on permutation test, without the need to train a new kernel for every permutation. The main advantage of this new method is that it does not require sample splitting and the whole sample can be utilized for the computation of the test value.

pros:
- Experiments show improvements in some cases compared to sample-splitting methods.
- The authors provide plenty of theoretical results in the appendix, including concentration results for the test both under null and alternative.

cons:
- The analysis is very conservative. At no point we can state that the distribution of the permuted test conditional on the observed data approximates that of the original test. Theorem 1 in particular is an upper bound, so there is no guarantees that the type I error of the test will be close to the desired level alpha. It is not clear how crude this upper bound is. I suggest this limitation should be emphasised in the text below.
- I think theoretical analysis can be simplified. For example, proof of Theorem 6 does not have to reproduce Vershynin and Rudelson's proof, it is sufficient to integrate their bound. Also, it is not clear to me why it is necessary to include a proof of Bounded Differences Inequality in the appendix. Instead, one can refer to Lugosi et al Concentration Inequalities, Theorem 6.7.
- There is no dependence of prior on the data in the experiments, please correct me if I'm wrong.

other comments:

line 174 m+ m -> n + m

line 222 inconsistent notation MMD_k or MMD(.; k)

line 829, display below: not sure why t appeared in second line and why it disappeared in the third line

line 747 FUSE depends on lambda, what depends on t then?

line 872 sup_y -> sup_rho?

line 873 what is N?

Serious concerns:

- Section E.3 proof of Theorem 2 (which in my understanding one of the main theoretical contributions of the paper). Could the authors please elaborate on what happens below line 926 in detail? The sup over rho disappears in line 2. The ""object"" S_rho appears to be random, depending on the observed data Z, and it appears in the final bound thanks to the Markov inequality. 
- I can't find proof of Theorem 3 in the appendix with Ctrl + F

To my best understanding if there are mistakes above section E.3 they should be fixable.

### My rating is conditional on whether the authors can fix these two serious concerns. Otherwise, the paper must be rejected.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
-

Weaknesses:
-

Limitations:
-

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a new statistic that can incorporate with the permutation test where we can learn features of data in an unsupervised way. From the perspective of testing, this paper contributes a new idea and paves a new way to perform two-sample testing without data splitting. If we can safely use all data to find a proper kernel, the test power will increase for sure. Theorems and proofs also verify the proposed methods.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
1. This paper focuses on an important problem: two-sample testing, which will be very important when generators are like humans nowadays.

2. The selected research direction, testing without data splitting, is very important. As demonstrated in summary, a proper kernel selection in this direction can directly boost the test power.

3. Experiments include simple data and complex data (Galaxy MNIST and CIFAR), verifying the effectiveness of the proposed statistics.

Weaknesses:
1. The presentation can be improved a lot. The major issue is the lack of motivation for many choices. For example, what is the motivation to consider a mean kernel? what is the motivation to have this kind of design: log( E( exp( ) ) )? Can we consider more possibilities? The finding of this paper is exciting, however, it looks like a rush version. A revised version with a better presentation is required during the rebuttal.

2. Section 3 is a little bit long. The authors can consider introducing the theorem in the preliminary (as it is not proposed in this paper). Or the authors can remove the permutation test from Section 2. You can also consider merging Sections 2 and 3, making demonstrations more compact.

3. After removing the necessary parts in Sections 2 and 3, more experimental results can be put into the main body. For example, CIFAR results look very promising and should be moved into the main body.

4. In line 115, there is a notation typo.

5. Marks are recommended in Figure 1. 

Limitations:
N/A

Rating:
7

Confidence:
5

";1
FTh5Rd3urw;"REVIEW 
Summary:
This paper proposes and analyzes a mechanism for incentivizing data contributions to federated learning systems in a context where contributors consider both monetary incentives for data contributions and the ability to experience the benefits of personalized systems.

The paper provides background on incentivized federated learning, describing the problem setting and proposed mechanisms, and theoretically analyzes a particular subset of this problem category to understand impacts of how underlying data that’s being modeled is distributed. Then, the authors perform experiments with classification datasets, with the assumption that classification datasets that might be under the control of a single entity in a classical setting are distributed between 100 clients and all data points belong to one of two clusters.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper has a number of strengths:

The paper begins with a very helpful overview of work in federated learning. The paper will be of interest beyond the federated learning-focused sub-community. The current draft provides strong motivation by explaining the large gap in incentivized federated learning. It seems with further development this approach could be impactful in practice.

The paper combines theoretical analysis of a new mechanism (which has the key insight of accounting for both monetary incentives and personalization-as-an-incentive) with learning experiments. This combination can help convince readers the proposed approach is useful. While there is some room to provide more details about the intended ""data contributor scenario"" and ""data user scenario"" (see below), overall I think the methods lean more towards being an overall strength vs. a weakness.

Presentation of results was strong overall.


Weaknesses:

While readers will appreciate the combination of theoretical analysis and experiment, it was not entirely clear exactly how the specific research questions being answered in each section were chosen (i.e., the “intuitions” R1-R3 seem useful but were not well prefaced). The experiments could be better motivated (more below in ""Questions"").

The paper is pretty abstracted away from any specific platform or use case where data contributors and data users interface via a particular FL architecture.

There may be an opportunity to strengthen the paper by specifying which ML use cases / contexts map well to the needed assumptions. In particular, while the paper relies on the existence of prior work on FL to motivate the need for FL, it’s not clear how many platforms running actual incentive systems (e.g. data markets, markets for crowdwork, etc.) are also supporting FL.

In general, I worry this paper may undersell its contribution by not providing enough details about what an on-the-ground implementation of the system would look like. Of course, this same criticism can be applied to a large body of similar work (including the RW here) and so it's not existential. Given the specific intended contribution here of proposing a new way to think about incentives in PFL, emphasizing plausibility may be especially valuable.


Limitations:
There are opportunities for the paper to be a bit more upfront about the limitations of focusing on incentive systems without discussing any specific populations of data contributors or specific use cases (beyond computer vision evaluation). While this paper seems to be part of a longer conversation between research papers that is primarily mathematical and leans heavily on assumptions about rational agents with programmatic approaches to computing utility, given that a key goal of having practical advantages mentioned in Related Work, addressing some practical use cases could make the work more convincing. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposed PI-FL, an incentive-aware federated learning method that produces clustered personalization of client models. This work uniquely considers the client contribution assessment in the multi-objective setting of clustered personalization. The clients are also given the freedom to choose the cluster based on their incentives. Therefore, this paper could be valuable in exploring the topic of incentivization in the very specialized area of clustered personalization. Rather extensive empirical experiments were also carried out by the authors to demonstrate the practicability and effectiveness of PI-FL.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. Recognition that personalization and incentivization in FL should be considered as interrelated challenges. In fact, incentivization is easier to be achieved when client models are allowed to be personalized.
2. In addition to the proposed method, the paper provides good theoretical analyses of it through toy examples that are easy to understand. The analysis also offers good and interpretable insights.
3. The types of empirical experiments carried out are rather extensive and demonstrate the effectiveness of the method. However, the comparison to baselines and the dataset choices can be further improved to be more comprehensive (sometimes omitting certain settings for certain methods).

Weaknesses:
1. This paper might need more concise and precise writing in Section 3 Proposed Methodology. The description of the methodology is long-winded and many design choices are not backed by theoretical justifications.
2. The theoretical analysis focuses on the gain from collaboration but neglects the theoretical aspects of the mechanism that incentivizes participation and honest data contribution.

Limitations:
1. It might be necessary to discuss the incentive and fairness more clearly. For example, we can see from Figure 2 that clients have varying PMA, which indicates different levels of benefits were gained by different clients in the collaboration. How should this phenomenon be viewed under the lens of fairness? Also, the opt-outs in a very coarse binary indicator and might not be a good way to see whether clients are really incentivized.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a novel clustering-based pFL combined with a token-based incentive mechanism to address incentive provision in pFL. Specifically, the proposed method clusters clients based on their cluster preference (the distribution similarity), which maximizes their contribution to the clusters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed approach of incentive-driven clustering based pFL is novel.

2. Theoretical analysis of the proposed method is provided.

3. Experimental results on multiple datasets demonstrate promising outcomes.

Weaknesses:

1. Some notations are not explained:


	What's $\zeta^{*}_k$ in Eqn.(4)? Only $\zeta_k$ is explained in line 177 of page 5

	What's $\theta_i$ in line 211 of page 6? Is it the same as the $\theta$ in Eqn.(6)?


2. Some parts of the method is not explained well


	What's the difference between reimbursement and payment? 

	What does the term of ""token"" actually means in pFL?

	What's the bidding strategy of clients?

3. Experiment:
	
	In pFL, the data are often heterogeneous in label shifting or feature shifting. However, the datasets used in the paper seems only related to dataset size imbalance, which are not the representative datasets used in pFL research. 

	Hope to see the experimental results on CIFAR10 data with Dirichlet splitting (referring the settings in [1]).

	[1] Marfoq, Othmane, et al. ""Federated multi-task learning under a mixture of distributions."" Advances in Neural Information Processing Systems 34 (2021): 15434-15447.

Minor:

	Typos: In figure 2, the x-axis is the number of clients which supposed to be a positive int type, but in the figure, it ranges from 0 to 1. 

Limitations:
NA

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper presents PI-FL, a new cluster-based personalized Federation Learning (pFL) approach. The new idea is the first to propose to consider motivation and personalization as interrelated challenges and address them through incentive mechanisms that promote personalized learning. PI-FL let clients provide incentive-driven preferences for joining clusters based on their own data distribution, and this client-centric clustering approach ensures accurate clustering and improved performance. This approach results in improved personalized model appeal (PMA) and reduced opt-outs, which in turn improves the accuracy of the clustering model. Theoretical analysis initially shows the effectiveness of the motivating algorithm. Experiments on several datasets show that the algorithm can obtain higher accuracy than the baseline.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper proposes to consider motivation and personalization as interrelated challenges and to address them through incentives that promote personalized learning. This is a good and reasonably innovative point that is informative for subsequent research on personalized federal learning.
2. The ability of PI-FL, a customer-centric clustering approach, to access the original customer data ensures accurate clustering and improved performance even in the case of dynamic distribution shifts in the customer's local data or incorrect clustering decisions by the customer, which improves the robustness of federation learning.
3. This paper is basically written very clearly. The three main modules of PI-FL: the profiler, the token manager, and the scheduler are very cleverly designed and their functions and working principles are well described.
4. The theoretical analysis is carried out in the paper to prove the effectiveness of the incentive algorithm, and the theoretical part is relatively complete.
5. The experimental comparison of test accuracy in multiple partitions shows that PI-FL can maintain good performance in all partitions. The comparison experiments of the two metrics, PMA and opt-outs, show that PI-FL can not only reduce opt-outs but also improve PMA in all data heterogeneity conditions.

Weaknesses:
1. The paper seems to lack the analysis about the convergence of PI-FL and the comparison with other pFL algorithms about the convergence speed.

~~2. In order to show the effectiveness of PI-FL method, the comparison between PI-FL and other clustering-based pFL algorithms about computation and communication cost should be added, and there should be more comparisons between several clustering-based pFL algorithms instead of only comparing with FedSoft.~~

3. All experiments are conducted in a simple CNN model, and the empirical study would be stronger if the authors could add some linguistic tasks, such as BERT pre-training/ fine-tuning experiments
4. Figure 1, as the overall design of PI-FL, is not drawn in enough detail, e.g., the icons representing customers are not labeled as customers. the fold lines in Figure 4 are dense and affect the perception, e.g., it is difficult to see clearly the fold lines representing 10:90 (NI).
5. The N_p parameter of the scheduler in PI-FL is the number of customers selected based on performance, and N_r is the number of customers selected randomly. N_p and N_r  parameters are very important for the training results of PI-FL, and further analysis should be done on how to select the appropriate N_p and N_r  parameters.

Limitations:
N/A

Rating:
4

Confidence:
4

";0
k1Xy5zCNOJ;"REVIEW 
Summary:
The paper presents Lookaround, a novel optimizer for weight average ensembling (WA). Unlike existing approaches that perform weight averaging post-training, Lookaround adopts a two-step process throughout the training period. In each iteration, the ""around"" step trains multiple networks simultaneously on transformed data using different augmentations, while the ""average"" step combines these networks to obtain an averaged network as the starting point for the next iteration.The approach demonstrates clear superiority over state-of-the-art methods in extensive experiments on CIFAR and ImageNet datasets using both traditional CNNs and Vision Transformers. The paper provides theoretical justification and commits to open science by making the code publicly available.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. `Innovative Approach`: The introduction of Lookaround as a straightforward and effective optimizer for weight average ensembling brings a novel perspective to the field. The two-step process during training enhances network diversity and preserves weight locality, addressing the limitations of post-hoc weight averaging approaches.

2. `Theoretical Justification`: The paper offers strong theoretical support for the superiority of Lookaround through convergence analysis. Proposition 1 provides insights into the convergence variance of Lookaround in comparison to typical SGD and Lookahead optimizers. This rigorous theoretical analysis enhances the credibility and significance of the proposed method, demonstrating a solid foundation for its effectiveness and performance.

3. `Extensive Experimental Validation`: The authors conduct extensive experiments on popular benchmarks, including CIFAR and ImageNet, with both traditional CNNs and Vision Transformers (ViTs). The clear superiority of Lookaround over state-of-the-art methods on these datasets demonstrates its effectiveness and applicability.

4. `Commitment to Open Science`: The authors state their intention to make the code publicly available, fostering reproducibility and enabling further research in the field.

Weaknesses:
1. `Computational Complexity and Comparison`: The paper lacks a comprehensive discussion of the computational complexity introduced by the Lookaround optimizer. As Lookaround incorporates an additional around step during training, it is crucial to assess its computational requirements and compare them to existing optimization methods. A detailed analysis of the computational trade-offs, including runtime and memory usage, would provide a more comprehensive understanding of Lookaround's practical applicability and scalability.

2. `Limited Experiments on Datasets Beyond Image Classification`: The paper primarily focuses on evaluating Lookaround on image classification tasks using datasets like CIFAR and ImageNet. However, to establish the versatility and effectiveness of Lookaround, it is important to explore its performance on datasets beyond image classification. Conducting experiments on diverse tasks such as language modeling or segmentation/detection tasks would demonstrate the generalizability of Lookaround across various domains and provide a more comprehensive evaluation of its performance.

Limitations:
As discussed in the limitation section ""additional cost of network searching using different data augmentation, resulting in a longer training time proportional to the the number of trained networks"", the running time is a important concern. 

Rating:
8

Confidence:
4

REVIEW 
Summary:
This work provides a new optimizer, ""Lookaround optimizer,"" which is built upon a previous proposed ""Lookahead optimizer"" [40]. By incorporating data augmentation, this work shows an improved convergence rate under low condition numbers. It also empirically shows some improvement in classification accuracy under several datasets.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
This work proposes a new optimizer and shows consistent (despite little) improvement in various settings. The supplementary material shows a solid derivation.

Weaknesses:
1. The major concern lies in the limited improvement over the ""Lookahead optimizer."" The improvement in Figure 2 and Table 1-3 all seems to be subtle. Since the proposed methods see more augmented data in each epoch than others, it is questionable whether the improvement actually comes from these additionally augmented data. Also, the idea itself is almost identical to the Lookahead optimizer, so the novelty is rather limited.

2. Minor mistakes: In table 1, row CIFAR 10, column ResNext50 Top5 results, the best results happen in Lookahead (99.96), not the proposed method (99.95).

Limitations:
The author addresses that the data augmentation process consumes extra time, which is likely to be the main limitation.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper introduces a new optimization algorithm named Lookaround, which draws inspiration from the recent achievements of weight averaging (WA) techniques in deep learning. The proposed Lookaround optimizer looks around nearby points by performing multiple gradient computations for a given training input using different data augmentations and averages them to obtain better generalizing solutions.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The theoretical analysis in this paper largely relies on Zhang et al. (2019), including noisy quadratic analysis and deterministic quadratic convergence. While it does not introduce a novel form of analysis, it is significant as it establishes a theoretical foundation within the existing framework.

For experiments, the baseline comprises commonly employed optimization techniques, such as SWA (Izmailov et al., 2018) and SAM (Foret et al., 2021), which are widely accepted as standard approaches in the field. In addition to convolutional neural networks of various scales, the authors also considered experiments on vision transformers.

Weaknesses:
Experimental issues:

* __There are no error bars.__  
While the authors answered ""yes"" for ""Error Bars"", the paper only provide a single value across the tables and figures. It is unclear how many experiments were conducted to derive those values. It would be preferable if the authors also included averages accompanied by standard deviations to provide a more comprehensive representation of the experimental results.

* __The outcomes obtained from the ImageNet experiments appear to be strange.__  
Despite the authors' efforts to demonstrate the scalability of the proposed algorithm, the Top-1 accuracy of 72.27% reported in Table 2 appears to be comparatively low. After reviewing Appendix C.1.2, it seems that the experimental setup follows the PyTorch convention (https://github.com/pytorch/examples/tree/main/imagenet), except for some additional augmentations. It is widely recognized that the ResNet-50 model typically achieves an accuracy of approximately 76% on the ImageNet dataset using this standard setup, which significantly differs from the 72.27% accuracy reported by the authors. It would be beneficial to provide clarification on the reasons behind the considerable performance drop observed in the ImageNet results.

Practical issues:

* __Excessive training costs incurred by the proposed algorithm.__  
The Lookaround algorithm, as proposed, demands $d$ times the number of forward and backward passes for each optimization step. This is considerably higher compared to SAM (Foret et al., 2021), which only requires twice the number of passes. One could argue that the training epoch is effectively enlarged by a factor of $d$ and this is the actual reason for the performance improvements. It would be valuable to present results akin to Table 2 in Foret et al. (2021), that is, exploring the impact of increasing the number of training epochs through an ablation study.

Limitations:
The authors stated the additional training cost incurred by the proposed algorithm.

---
__References:__  
Zhang et al. (2019). _Lookahead optimizer: k steps forward, 1 step back_.  
Izmailov et al. (2018). _Averaging weights leads to wider optima and better generalization_.  
Foret et al. (2021). _Sharpness-aware minimization for efficiently improving generalization_.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes Lookaround, a new optimization method that incorporates weight averaging into the optimization process. The algorithm consists of two steps: 1) the around step launches several parallel runs of gradient descent led by different data augmentations, 2) the average step does weight averaging of the networks obtained by these parallel runs. These two steps are repeated along the whole training process, with the result of the average step being a starting point for the next around step. The proposed method is similar to Stochastic Weight Averaging and Model Soups, though it does the averaging during training rather than at the end of training. Lookaround shows strong results compared to the existing optimization methods (SGD with momentum, AdamW, Lookahead, SAM).

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The idea of using weight averaging during optimization with various augmentations is novel and leads to a better generalization of neural networks.
2. This paper has a theoretical analysis of the quadratic noise setup which demonstrates faster convergence of Lookaround compared to SGD and Lookahead.

Weaknesses:
Despite the fact that the paper proposes a novel and interesting method, in my opinion, it is not quite ready for publishing in its current form. I believe that addressing the following concerns could help to make it a much stronger submission.
1. The text of the paper is hard to follow, contains some vague parts and numerous typos.
    - The theoretical reasoning is heavily inspired by the Lookahead paper, which makes it impossible to understand the theory without reading the original paper (e.g., I could not understand Proposition 1 and the definition of $\alpha$ without reading the Lookahead paper). Moreover, there is no remark that equations 4 and 5 are derived in the Lookahead paper. 
    - Line 5-6: weight averaging and ensembles are mixed up. 
    - Line 313: it is not clear how gradient boosting is related to this setup.
2. I have found two contemporaneous works published on arxiv (https://arxiv.org/abs/2302.14685, https://arxiv.org/abs/2304.03094) that propose approaches very similar to Lookaround. I believe they need to be at least discussed in the related work section.
3. The experimental part of the paper raises some questions and is not convincing enough, in my opinion.
    - The main results (Table 1) lack standard deviations, even though they are claimed in the OpenReview form. 
    - The baselines seem too weak, i.e., the Lookahead paper reports >75% ImageNet test accuracy on ResNet-50 for SGD baseline, while this paper shows 72.27%. 
    - The augmentation policy during baseline training is not clear. If the same set of augmentations as for Lookaround is used, then this may be the reason for the bad quality of the baselines. 
    - Despite the paper proposing a new optimization method, it lacks experiments on a wider choice of architectures. Table 1 illustrates only VGG-19 and four networks from the ResNet family. It would be beneficial to add experiments on more modern convolutional architectures and more extensive experiments on image transformers.
    - The paper lacks a fair comparison to logit ensembles. If I understood correctly, different models of the ensemble are trained with different Lookaround augmentations. This leads to the suboptimal quality of each model and, as a result, to a suboptimal ensemble.
    - The comparison to model soups in Appendix D is not fair as well, i. e., the original paper fine-tunes models with various hyperparameters and does a greedy search for the combination with the best validation accuracy. Near zero accuracy of the average of $\theta_1$ and $\theta_2$ may be due to improper fine-tuning hyperparameters. Moreover, this is an important baseline, and it would be better to move this comparison to the main part of the paper.
    - A proper ablation study of augmentations is required. Is it possible to train Lookaround without augmentations for the around step “branches” (e.g., the only source of randomness is batch ordering)? What if each branch utilizes the same set of augmentations? Why are the mentioned augmentations used (and not some other ones, e. g. vertical flip seems to be a strange augmentation)? Probably, some of these setups are covered in Section 4.4, but it is not clear from the text.
    - Top-5 accuracy is redundant and makes it difficult to read the tables.
4. Minor issues/typos:
    - Line 25: the sentence is about LMC, but the citation [7] leads to the paper about permutations (Entezari et al., 2022). 
    - Line 112-113: we provide
    - Line 147-148: repetition
    - Line 156: we analyze
    - Line 162: it seems that it should be $\mathbb{E} [c_i^2]$
    - Line 242: CIFAR

Limitations:
The main limitation of the proposed method is the increased training budget, which is highlighted by the authors. However, there is no fair comparison to networks trained for a larger number of epochs.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This article introduces the Lookaround Optimizer, a novel optimization algorithm for deep neural networks. The Lookaround Optimizer is based on the idea of lookaround, which involves maintaining two sets of weights. The first set of weights is updated using the standard gradient descent algorithm, while the second set of weights is updated using the averaged gradients of the first weight. The Lookaround Optimizer has been shown to improve the generalization performance of deep neural networks, and it outperforms other state-of-the-art optimization algorithms on a variety of benchmark datasets. The authors also provide theoretical analysis of the Lookaround Optimizer. Overall, the Lookaround Optimizer is a promising approach for improving the training of deep neural networks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Lookaound Optimizer performs better than other state-of-the-art optimization algorithms (Lookahead, SAM) on multiple benchmark datasets.
- The proposed optimizer seems to be model-free and can be applied to various computer vision scenarios.
- The paper is well-written and easy to understand.

Weaknesses:
- While the authors have demonstrated fascinating performance on benchmarks, the technical innovation of this paper is limited. Similar ideas that utilize multiple models have already in proposed in varioius area, especially in meta learning. 
- With that being said, while the authors have mentioned the connections to these related topics, they did not compare with some of them. For example, I think at least the authors should compare the performance of Lookaround Optimizer with Model Soups (i.e., train multiple copies of models using the same initialization, the same order of data, but with different augmentation), and other model merging techniques like Model Ratatouille. 
- The training time will be a bottleneck for applying this method as the authors  acknowledged. 
- Another limitation is that, the augmentation seems to be a must for this method. Therefore, it seems that this method is not applicable (at least not straightforward) when we try to apply it on other modality, such as language and speech.

Limitations:
The authors have addressed the limitation.

Rating:
6

Confidence:
3

REVIEW 
Summary:
Flatness-aware optimizers have gained significant attention in the field of research for training deep neural networks that are robust. Weight Averaging (WA) is a popular approach to finding solutions within the flat regions of the loss surface. However, previous WA methods have two limitations. First, when WA is performed within a single optimization trajectory after training convergence, resulting in limited functional diversity among the averaged members. Second, when WA is performed across diverse modes, the averaged weights may negatively impact performance. To address these challenges, this paper introduces a new method called Lookaround Optimizer, which aims to overcome these limitations.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Originality

- Proposed optimizer seems to be relatively simple and straightforward but still attractive.
- This article presents a theoretical analysis regarding the variance of the steady state.

Clarity

- The paper is written effectively, ensuring high accessibility for readers.
- Methods are simple and easy to follow.

Weaknesses:
Experiments

- Regarding the experiments conducted, the authors acknowledged in the Limitation section that their proposed method incurs additional training costs (I think the proposed optimizer requires nearly $d$ times more computational resources when averaging $d$ models). Therefore, for a fair comparison, it would be necessary for other baseline methods to undergo additional training epochs as well. However, it appears that the authors used the same number of training epochs for all the experiments.

- The overall results of the experiments are rather counterintuitive. Particularly in Table 3, the reported accuracy of the Deep Ensemble method being lower than the accuracy of a single solution trained using the proposed method is quite unexpected. Similarly, in Table 1, the reported performance of flatness-aware optimizers such as SWA [1] and SAM [2] being lower than SGDM contradicts the knowledge and expectations of the research community.

References

[1] Pavel Izmailov, D. A. Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. Conference on Uncertainty in Artificial Intelligence, 2018.

[2] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. International Conference on Learning Representations, 2021.

Limitations:
Limitations are adequately addressed in the Limitation section.

Rating:
5

Confidence:
2

";1
DEqiM9CmmZ;"REVIEW 
Summary:
This paper introduces the ANQ (Approximate Nearest Neighbor Q-Learning) framework, which aims to provide explainability in reinforcement learning models. ANQ combines neural networks for high performance and memory-based structures for explainability, offering a promising solution for domains like autonomous driving, quantitative trading, and healthcare. The paper discusses the challenges of explainability in reinforcement learning and how ANQ addresses them. It also presents the Sim-Encoder contrastive learning used in ANQ's state representation and provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. Overall, the paper presents a novel approach to reinforcement learning that balances performance and transparency, making it suitable for real-world applications. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Some strengths of this paper include:  

1. Novelty: The ANQ framework is a novel approach to reinforcement learning that combines neural networks and memory-based structures to provide explainability in decision-making processes. This approach is different from traditional reinforcement learning methods that focus solely on performance.  

2. Explainability: The paper addresses the challenge of explainability in reinforcement learning models, which is crucial for real-world applications where transparency and trustworthiness are essential. ANQ's ""data is policy"" design principle ensures that the model's decisions are explainable and interpretable.  

3. Sim-Encoder contrastive learning: The paper introduces the Sim-Encoder contrastive learning approach for state representation, which demonstrates its effectiveness in memory retrieval learning tasks. This approach enhances ANQ's performance and explainability.  

4. Evaluations: The paper provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. The results show that ANQ outperforms traditional reinforcement learning methods while maintaining explainability.  

5. Real-world applications: The paper highlights the potential of ANQ in domains like autonomous driving, quantitative trading, and healthcare, making it suitable for real-world applications. 

Weaknesses:
Some potential weaknesses of this paper include:  

1. Despite the advantage of interpretability, the performance of this framework is still far from that of SOTA RL algorithm. I think it would be better if this framework could have similar performance to SOTA RL method. 

2. Lack of real-world case studies: Although the paper highlights the potential of ANQ in various domains, it does not provide specific real-world case studies or examples to demonstrate the practical application and effectiveness of the framework. Including such case studies would strengthen the paper's claims and provide more concrete evidence of ANQ's utility in real-world scenarios. 

3. No comparison was made with other interpretable reinforcement learning algorithms. For example, some Neuro-Symbolic Search methods. 

3. The presentation of this paper could be better, for example, Figure 3 could be larger and clearer. 

Limitations:
Incorporation of latest techniques: The paper mentions that ANQ has not yet incorporated the latest techniques, such as maximum entropy learning from SAC and other contrastive learning methods for representation learning. While the paper acknowledges that these refinements will be addressed in future work, the absence of these techniques in the current implementation may limit the overall performance and effectiveness of ANQ.  

Rating:
5

Confidence:
2

REVIEW 
Summary:
The submission creates a framework called Approximate Nearest Neighbor Q-Learning (ANQ). ANQ uses a sim-encoder contrastive learning and approximate nearest neighbor search to find which states are similar. Utilizing this approach, they can use it to find similar states in aiding for the decision the framework has made. They showcase their performance by performing experiments on MuJoCo with continuous control tasks. The exact environments experimented were Walker2d, Ant, HalfCheetah, and Hopper. There is also an ablation study conducted to show the benefit of the sim-encoder and contrastive learning that aids this framework. They provide a component called Explainable Action to show why it executed a particular action based on the state.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Significance:
Good ablation experiments to showcase the benefit of the sim-encoder. The results make it convincing that for AQL, the sim-encoder is quite beneficial. Plus you did this ablation among 4 environments.

Originality:
The explainable action is an interesting piece that can provide a good impact. By searching for similar states and can provide the explanations as to why the decision was made.

Weaknesses:
Clarity:
Confusion, in Section 4.1, you mention that the algorithms included were SAC, PPO, and TRPO, then in the next paragraph you mention A2C. Consider in the first paragraph to mention the exact algorithms you compare because in the next paragraph, you mention an algorithm that was not discussed exactly in the previous paragraph. In Figure 3, you show TD3-1M, ARS-75M which were not discussed so please in Section 4.1 to denote exactly.

With the figures, please provide more with the caption like a summarization or a sentence to showcase why it is important. It can help the reader if they have not read the parts within the main text.

Significance:
The approach can be on par with one or two deep RL approaches. Consider to improve the performance to have bigger impact. Usually for methods that are explainable there is a drop in performance so others may be hesitant to use it due to the performance drop.



Limitations:
They do address the performance drop compared to the deep RL models. No negative societal impacts.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes a memory-based Q-learning algorithm aimed at better explainability. The method extends the prior work ""Episodic Control"" and enables learning with continuous action space.

This work provides 2 main contributions:
1. a one-step-away contrastive-learning objective to learn embeddings from states.
2. modified policy evaluation and improvement rules to account for the continuous action space.

The authors show empirical results to support their design choices:
1. The proposed algorithm is able to achieve some meaningful learning in 4 continuous control tasks.
2. The question-answering example shows that the method is able to find the nearest-neighbor states and their Q values to explain a chosen action.
3. The ablation study on the embedding module shows that it is necessary to learn dynamics-aware embedding.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The proposed method can learn in environments with continuous state and action spaces, making it ubiquitously applicable to real-world applications.
2. The algorithm and experiment settings are clear.
3. The ablation study on the embedding module justifies the contrastive learning objective.

Weaknesses:
1. The main weakness of this work is in the experiment results. In the Mujoco tasks, the performance hovers around the weakest baseline among all compared methods. For Ant, HalfCheetah, and Hopper, the policies also appear to have converged to suboptimal ones. This could have been caused by insufficient exploration.

2. The algorithm is only demonstrated in state-based environments, whereas the prior work ""Episodic Control"" can work with image observations. Contrastive learning has been shown to be useful for learning good feature extractors for images. The results would have been much more convincing if they were from vision-based tasks.

3. The notations are not fully explained. For example, $k$ and $e_t$ both exist in the dataset but the authors say that they use embeddings as keys. Also, $R$ in Equation 1 is not introduced.

4. Finding out the nearest neighbors and printing out their Q values is not a convincing way to explain the chosen actions because the Q values are computed in expectation. One can arguably explain a neural policy in a similar way, by sampling a few different actions and printing out their Q values.

Limitations:
The authors are upfront about the limitation in task performance and provide viable options for improvement. I would also encourage the authors to try their method on image-based tasks.

Rating:
3

Confidence:
4

REVIEW 
Summary:
Instead of using deep neural networks to approximate Q functions as it's done in deep RL methods, the paper investigates the potential of using nearest neighbor methods to approximate Q functions. They used contrast learning with a Sim encoder. They argue that such a method is more explainable than the ones with deep neural nets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The illustration is the proposed method is clear.

Weaknesses:
* The proposed nearest neighbor might be useful and efficient for low-dimensional domains like Mujoco. I doubt its effectiveness when it goes to high-dimensional domains like video games;

* I don't see why the nearest neighbor method is more explainable;

* The proposed method lacks proper baselines;

* The overall presentation needs improvements.

Limitations:
NA

Rating:
3

Confidence:
4

";0
hJzEoQHfCe;"REVIEW 
Summary:
This paper introduces a novel approach known as Feature Multiplexing, which allows multiple features to share a single representation space in machine learning systems. This is significant for web-scale systems which handle hundreds of features with vocabularies of up to billions of tokens, where the standard embedding approaches introduce a vast number of parameters. The authors propose a new solution called Unified Embedding, which simplifies feature configuration, adapts to dynamic data distributions, and is compatible with modern hardware. The empirical results from multiple web-scale search, ads, and recommender systems show superior performance compared to highly competitive baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1) The paper addresses a crucial problem in large-scale machine learning systems related to efficient and effective learning of feature embeddings. The proposed framework, Feature Multiplexing, is innovative, allowing multiple features to share the same representation space.

2) The authors provide a well-written and clear explanation of the concepts and the proposed solution. The paper is well-structured, with a good balance of theory, experimentation, and discussion.

3) The problem this paper addresses is of substantial significance, considering the scale at which modern machine learning systems operate. The introduction of Unified Embedding could lead to substantial advancements in web-scale ML systems, serving billions of users globally.

Weaknesses:
1) The paper lacks details regarding the computational benefits of the proposed technique, specifically in terms of infrastructure gains, parameter size, hardware usage, and training time. Providing such details would make the comparison to the baseline more comprehensive and persuasive. (particularly the large scale experiments explained at the end)

2) Some specific analysis and explanations are missing. For example, why the Criteo dataset behaves differently from Avazu and Movielens is not explained. A more in-depth exploration would strengthen the understanding of the behavior of the proposed technique across datasets.

3) The authors could have provided more insights into why online deployment results are providing gains. A detailed explanation could better support the claim of real-world applicability and insights into future users. 

Limitations:
The authors have not adequately addressed the limitations and potential trade-offs of their proposed technique. Future work may be constrained or impacted by these unaddressed issues. For instance, the authors have not discussed the ease (or lack thereof) of extending the model with new features or conducting new R&D with the proposed method. They also have not explored the potential maintenance costs and impacts on model health and observability, which can be crucial for deploying such systems in real-world applications. Further information on these aspects could greatly benefit the audience's understanding of the practical viability and potential challenges of implementing the proposed technique.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors present a method for multiplexing embeddings of various features in the recommender and similar applications, i.e., sharing the feature embeddings in order to save space and improve performance, especially at lower memory budgets. They provide a detailed overview of the relevant prior work, and give strong both theoretical and empirical analysis of the proposed method. They show the benefits of the method on three public data sets, and also show how the method helped in large-scale production setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Very important problem being addressed.
- Good theoretical discussion of the method.
- Good results shown in the production-level setup.

Weaknesses:
- In some places the explanations can be improved quite a lot.
- The results are very mixed in some cases.

Limitations:
The authors did not discuss the limitations, and it would be good to add a short paragraph.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes that in web-scale machine learning systems, features from different fileds can share the embedding matrix without significantly affecting the model's performance. The insight lies in that different feature fields are processed by different model parameters. Therefore, compared to inter-feature collisions, in the case of intra-feature collisions, the embeddings of different features can tend to be orthogonal, which is beneficial to the final model performance. This point is confirmed by the empirical study based on logistic regression. The experimental results show that the multiplexed embedding scheme is more effective than the existing schemes that only consider inter-feature embedding sharing.

------
AFTER REBUTTAL:

I've read and appreciated the author’s rebuttal. I understand that collisionless embeddings are considered the upper bound. However, it would be helpful if the advantages of feature multiplexing could be mentioned and demonstrated in the discussion and experiments. Thus I would like to keep the score.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The overall organization and writing of the paper are excellent.
- The proposed multiplexed feature embedding scheme is novel and its feasibility is verified both theoretically and experimentally.
- The paper conducts experiments on multiple public datasets, and the overall results are promising.

Weaknesses:
- In the experiments, although the performance is better compared to the inter-feature embedding sharing scheme, it is worse than the Collisionless scheme on Criteo, very close on Avazu, but no relevant discussion is provided.
- Compared to the Collisionless scheme, the advantages of using feature multiplexing do not seem to be fully discussed, nor are they reflected in the experimental results.

Limitations:
Yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces a novel ""Feature Multiplexing"" framework which uses a shared representation space (embedding table) for multiple sparse features. This approach aims to find a balance between model size and accuracy for industrial level recommender system. Besides, the authors provide a theoretical analysis, highlighting that inter-feature collisions can be alleviated if features are projected using orthogonal weight vectors. Further gradient analysis reveals that these collisions are not uniformly detrimental; the adverse effects can be mitigated when features are processed by distinct parameters in a single-layer neural network.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
**Pros**:

- The paper presents the a novel ""Feature Multiplexing"" framework, offering a straightforward and effective method to optimize the trade-off between model size and accuracy.

- This framework promises considerable practical advantages, especially in the context of large-scale recommendation systems.

- A  theoretical analysis is provided, addressing the advantage of the Feature Multiplexing system from rigorous theoretical analysis . It explains how inter-feature collisions can be reduced when the model uses orthogonal weight vectors to project distinct features. This analysis provide good theoretical ground for a fairly practical design. 

Weaknesses:
**Cons**:

- Certain claims in the paper, such as ""0.02% increase in test AUC is considered significant in Avazu and Criteo"" and ""+0.1% is considered significant in online systems,"" raise eyebrows. These claims appear to be based on subjective opinions rather than objective facts.

- The authors have not provided source code for their work. This factor makes it difficult to reproduce and verify the claims made in the paper, which is a foundational principle of the NeurIPS community. If this work is industry-driven and the code cannot be released easily, perhaps the authors should consider venues more suited to applied data mining.

- The content and focus of the paper lean heavily towards data mining and address real-world, industrial-scale problems. As such, it might be better suited for venues like KDD, WWW, or SIGIR.


Limitations:
The authors did not discuss the limitations at all.

Rating:
5

Confidence:
4

";1
V87gZeSOL4;"REVIEW 
Summary:
The paper discusses the task of identifying causal variables from high-dimensional observations under non-parametric mixing functions and causal mechanisms. This is done under the assumption of single-node, perfect interventions being available for all causal variables, as well as distinct paired perfect interventions in the case of having more more than two causal variables. The paper proves that causal variables are identifiable under this setup, when taking additional assumptions on the interventions being sufficiently different from the observational distribution. Thereby, weaker assumptions are possible for 2 variables than for more variables. Finally, the paper sketches possible implementations of learning algorithms for this setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is overall well written, even if it is aimed at researchers in identifiability and/or causal representation learning (CRL) specifically. A consistent notation is used throughout the paper, and all assumptions are clearly stated before the theorems. It is appreciated that proof sketches have been included in the main paper to support the claimed theorems and make the main paper a bit more standalone. The paper discusses all necessary related work and puts itself into context of the current field of research.

The main contribution of the paper is its theoretical result. It extends the domain of identifiable causal representation by considering yet another setup, where environment pairs with single-node, perfect interventions are given. The benefit of this setup is that it does not require counterfactual observations, while supporting a large function class despite taking needed assumptions on the interventions. The proofs for supporting the claimed theorems are given in the appendix, following common proof strategies in CRL. The proofs appear sound and intuitive, although a very careful check of the proofs was not possible during the review period. Overall, it is a good contribution to the theoretical identifiability in CRL. 

Weaknesses:
While the derived theory in the paper puts weaker constraints on the mechanisms of the causal variables and mixing function, its assumption of having access to single-node, perfect interventions on all causal variables is restrictive. Being able to perform an intervention on a variable is already commonly considered expensive or often not easily feasible, especially if it is a perfect intervention and single-node. However, doing this twice and even different between the two setups is challenging. Further, obtaining such a dataset requires non-trivial prior knowledge of the causal system, since it necessitates the ability to perform such single-node, perfect interventions on causal variables that are yet to be identified. The paper misses to give real-world examples to motivate the setup and its assumptions, which puts it in a more limited spot. 

Besides the theoretical results, it is also important to validate the setup and the practicality of the theory in empirical studies. The paper only sketches some potential ideas, where all unknown parts are learned. However, optimizing the latent encoder, the causal graph, and the intervention targets all at the same time is not trivial as shown in previous works. Further, the appendix shows some limited results on a generative model, where one would need to iterate over all possible causal graphs and intervention targets. Still, this is not practical for systems larger than very few causal variables or high-dimensional observations.

The paper states that the intervention targets are not known. However, under the identifiability up to permutation, the intervention targets in this setup appear to be known. Specifically, assumption (A2') states that there exist $n$ environment pairs, where each pair intervenes on a different causal variable. Thus, the intervention targets for these pairs, as stated in the assumption, are known as $\pi(i)$. Since the variables cannot be identified up to permutation $\pi$ anyway, permuting the causal variables and thus the targets are still considered to be the same targets in the same identifiability class, e.g. as in the works cited for known intervention targets [69, 70]. Thus, the claim of unknown intervention targets appears not valid given the assumptions, or the assumptions should be clarified to e.g. have at least $n$/$n+1$ environments.

### Typos:
- Table 1: 'Causal Representation Learning'

Limitations:
Limitations have been discussed in different parts of the paper. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of inferring causal relationships between $n$ latent variables through observations under a mixing function. Given data $X$ from multiple environments (each of which corresponds to an unknown perfect atomic intervention), where $X$ is the observation of the latents under a fixed mixing function $f$, the goal is to recover $f^{-1}$ and the causal graph $G$ on the latent variables (up to $\sim_{CRL}$ equivalence).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem is well-motivated and interesting. The authors did a good job explaining how this paper differs from prior work while providing a pretty good literature review. Some experiments are also given in Appendix D.

Weaknesses:
While I am not an expert in the area and did not check all the proofs in detail, I do not see any glaring weaknesses. The theorem statements and proof sketches seem believable, especially since there is a lot of assumptions that were made to ""make things go through"". My biggest gripe is that there is a lack of discussion about the assumptions (see Questions section).

Limitations:
Nil.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposed to identify the latent causal representations and their underlying causal structure, which is a very challenging and interesting problem. The \sim_{CRL} is introduced to describe the equivalent class up to elementwise operations and permutation, which is sufficiently meaningful for practical use. The CRL-identifiability theory is given under the data from paired interventional data and other assumptions, such as the pre-given number of nodes and others. In appendix, the authors presented a simple version of learning method and validates it on a synthetic dataset. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
In general, I found this paper to be highly enjoyable and insightful. It successfully addresses a challenging and captivating problem of extracting causal representations and their relationships. Given the increasing prevalence of unstructured data, such an endeavor holds significant importance. The authors have provided a comprehensive overview and engaging discussions that effectively highlight the unique contributions of their work in relation to existing literature. Moreover, the use of paired interventional data, which is more readily obtainable in practical scenarios, adds to the paper's practical relevance. Besides, the organization and writing of this paper are commendable.

Weaknesses:
1. I recommend that the authors provide practical demonstrations of the proposed method in real-world scenarios. While acquiring paired interventional data can be challenging in real-world settings, the authors could consider utilizing datasets generated from virtual environments, such as the causal world (https://sites.google.com/view/causal-world/home), to showcase the utility of their approach.

2. In practical applications, determining the number of latent nodes n, is often difficult. Consequently, verifying whether the number of paired intervention data includes all latent variables becomes challenging. This limitation may restrict the scope of application for the proposed theory and learning methods.

Limitations:
Yes, the authors adequately addressed the limitations. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper gives an identifiability result in a setting that is relevant to causal representation learning, where we wish to infer latent causal variables and their causal graph from high-dimensional observations. They work in a setting that is more general than prior work that relies on, for example, weak supervision, temporal structure, or known intervention targets. Their setting assumes that both the causal model and the mixing function are nonparametric, and the targets of the interventions are unknown.

Their identifiability results are up to trivial indeterminacies (permutations and element-wise diffeomorphisms) and identify both the causal graph and the mixing function. Their first theoretical result shows identifiability for two causal variables given one perfect stochastic intervention per node. Their second theoretical result shows identifiability for an arbitrary number of variables when there are two paired perfect stochastic interventions per node.

The main text of the paper does not have an experiment section.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- This paper frames a problem setting for identifiability that is interesting to causal representation learning, which begins to bridge the gap from existing identifiability results to modern machine learning that occurs on high-dimensional observed data.
- They work in a highly general setting where both the causal model and the mixing function are nonparametric, and the targets of the interventions are unknown. In my opinion, the problem framing and the choice of this general setting are the primary contributions of this work even if the theoretical results have limitations.
- The paper is generally well-written and well-structured.

Weaknesses:
- Their first theoretical result is in a setting with only two causal variables, where you have an observation distribution and one perfect intervention per node. This result would be much stronger in a setting with n>2, as the authors note in the conclusion.
- Their second theoretical result is in a setting with arbitrary number of variables, but requires two distinct perfect paired interventions. Requiring these pairs of interventions is not a terribly realistic assumption, even though they don't require the intervention targets to be known.
- There is no estimation method or experiment results. Other identifiability papers often contribute an estimation method (e.g. a VAE using a regularizer that encourages sparsity of a mixing function), perform disentanglement experiments in settings that match their theoretical assumptions, or perform ablations on synthetic data where they can control which of their theoretical assumptions are met in order to empirically study the necessity / sufficiency of their assumptions.

Limitations:
- The conclusion section includes a thorough treatment of limitations of this work, which helps future work to extend these results. No concerns about negative societal impacts.

Rating:
6

Confidence:
3

";1
rHAX0LRwk8;"REVIEW 
Summary:
The paper proposes a model-learning approach for counterfactual prediction (CP), off-policy evaluation (OPE), and offline reinforcement learning (ORL). The authors introduce the adversarial weighted empirical risk minimization (AWRM) objective to facilitate learning models that accurately evaluate target policies. Additionally, they present the GALILEO algorithm, a generative adversarial training method that approximates the data distribution induced by the optimal adversarial policy.


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ The paper effectively addresses the problem of learning accurate models for CP, OPE, and ORL, which is particularly significant in domains with costly data collection.
+ The authors provide a comprehensive discussion on the impact of selection bias on CP, which serves as motivation for their objective, AWRM, and the GALILEO algorithm. 

Weaknesses:
- However, it is unclear how novel is the weighted version of empirical risk minimization (ERM), compared to prior research. The main contribution lies in the adversarial aspect. Therefore, I recommend revising the introduction to emphasize and motivate the adversarial part.

- The derivations appear reasonable. However, including a brief discussion about Algorithm 1 could enhance the paper's readability. 

- While the experimental results are generally ok, one notable limitation is that the authors only consider three tasks from the D4RL and DOPE benchmarks. In addition, the proposed approach does not outperform other methods in one of these tasks (HalfCheetah). To convincingly demonstrate the efficacy of their algorithm, the paper should include more tasks in their evaluation. Additionally, the constraint of limiting the time horizon to {10, 20, 40} is very strong and lacks proper motivation. 

- Furthermore, the proposed algorithm lacks comparison with state-of-the-art algorithms for the D4RL benchmark, such as the one mentioned in https://openreview.net/pdf?id=VYYf6S67pQc.

Limitations:
yes

Rating:
5

Confidence:
2

REVIEW 
Summary:
An accurate environment dynamics model is crucial for various downstream tasks, such as counterfactual prediction, off-policy evaluation, and offline reinforcement learning. Currently, these models were learned through empirical risk minimization by step-wise fitting of historical transition data. However, we first show that, particularly in the sequential decision-making setting, this approach may catastrophically fail to predict counterfactual action effects due to the selection bias of behavior policies during data collection. To tackle this problem, the authors introduce a novel model-learning objective called adversarial weighted empirical risk minimization (AWRM). AWRM incorporates an adversarial policy that exploits the model to generate a data distribution that weakens the model's prediction accuracy, and subsequently, the model is learned under this adversarial data distribution. They implement a practical algorithm, GALILEO, for AWRM and evaluate it on two synthetic tasks, three continuous-control tasks, and a real-world application. The experiments demonstrate that GALILEO can accurately predict counterfactual actions and improve various downstream tasks, including offline policy evaluation and improvement, as well as online decision-making.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper addresses the problem of accurate environment dynamics model learning, which exhibits wide impacts in many downstream tasks, like counterfactual prediction, off-policy evaluation, and offline reinforcement learning. This is a very important and meaningful research topic.

2. This is the first research on faithful dynamics model learning in sequential decision-making settings like RL, which demonstrates the novelty of this paper.

3. The analysis to the challenges brought by the conventional empirical risk minimization method is deep and insightful. Especially, the authors use a vivid example to illustrate them. Based on this, the transition to the propose of adversarial weighted empirical risk minimization objective is smooth, which strongly supports the following the Generative Adversarial Offline Counterfactual environment model learning (GALILEO) method.

3. The experiments of this work is sufficient and persuasive. The authors conduct experiments in two synthetic tasks, three continuous-control tasks, and a real-world application. They first verify that GALILEO can make accurate predictions on counterfactual data queried by other policies. Then, they demonstrate that the model learned by GALILEO is helpful to several downstream tasks.

Weaknesses:
1. It can be better if the authors can add several baselines to better validate the superiority of the proposed GALILEO method.

2. The description of the method part is deep and comprehensive. But the authors can consider making it easier to understand if possible. 

3. Some typos need to be fixed in the future version, like the subtitle of Section. 5.3.

Limitations:
See the Weaknesses and Questions above.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper presents a novel method for improving the accuracy of environment dynamics models for counterfactual prediction, off-policy evaluation, and offline reinforcement learning. Currently, these models learn via empirical risk minimization (ERM), which the authors show can lead to failures in counterfactual action prediction due to selection bias during data collection. To address this, the authors introduce adversarial weighted empirical risk minimization (AWRM), where an adversarial policy weakens the model's prediction accuracy to encourage improvement. They implement this approach via an algorithm named GALILEO, which is evaluated on synthetic tasks, continuous-control tasks, and a real-world application. Results show that GALILEO can accurately predict counterfactual actions and improve several downstream tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The concept of an adversarial weighted empirical risk minimization (AWRM) is a novel idea that brings together ideas from adversarial training and reinforcement learning. The use of an adversarial policy to manipulate the model's data distribution and improve its learning process is an innovative approach.
2. The paper provides a theoretical foundation for AWRM and shows its implementation through the GALILEO algorithm. The authors also conduct a variety of tests to assess GALILEO's performance, including synthetic tasks, continuous-control tasks, and a real-world delivery platform.
3. The paper is generally well-written and the use of figures to illustrate key concepts also enhances the clarity of the paper.

Weaknesses:
1. The authors didn't cover the preliminaries and related works adequately. I'm not too familiar with counterfactual modelling techniques, and think the authors didn't present enough to situate their work.
2. Justifications for choosing the three baselines are missing. Why not choose the more recent GAIL based methods?
3. Typo: line 320 Downstream
4. Formatting issue: The upper margins in page 2, 7, 8 and 9 seem too small. 

Limitations:
The authors didn't address the limitations nor broader societal impacts in the paper. But I didn't see any ethical concerns. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of environment modeling. An adversarial method is proposed that the adversarial counterparts is trained to exploit the model to generate a data distribution that weakens the model’s prediction accuracy, and then the model is trained under the adversarial data distribution with a weighted empirical risk minimization objective. Experiments are conducted on synthetic, control and real-world tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is clearly written. The illustration examples are clear and well motivate the problem.
Applying IPS and the surrogate optimization step is interesting.
I also agree that conservative or pessimistic offline model-based RL methods often try to limit policy exploration, which might make it hard to obtain an accurate environment model.


Weaknesses:
Complexity and stability of the adversarial method might be concerned, where two discriminators should be learned.


Limitations:
Yes.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces an adversarial training approach to model learning that improves performance for counterfactual data that may differ widely from the data used to train the model. This is particularly relevant when training from offline data and expecting the model to generalize when deployed later. This paper extensively presents an adversarially weighted empirical risk minimization objective drawing inspiration from inverse propensity scoring/weighting. A practical algorithm, GALILEO, is introduced and extensively tested across synthetic, continuous control, and real-world data. The paper clearly lays out the advantages of unbiased and accurate counterfactual models in a wide array of use cases in RL. The authors do this to contrast to major limitations of a majority of model-learning approaches that use supervised learning to perform empirical risk minimization.

This is a complete and well written paper. The development of the proposed method are clearly justified with sufficient grounding in the formal exposition of the equations. The derivation of AWRM was easy to follow with the structure put in place by the authors.

The included experiments clearly lay out the intended contribution of the proposed approach, that GALILEO provides a more accurate and counterfactually correct model. Impressively, this improved model is shown to have clear benefits for downstream performance in tasks beyond the “pre-training” task.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The paper clearly lays out the advantages of unbiased and accurate counterfactual models in a wide array of use cases in RL. The authors do this to contrast to major limitations of a majority of model-learning approaches that use supervised learning to perform empirical risk minimization.

This is a complete and well written paper. The development of the proposed method are clearly justified with sufficient grounding in the formal exposition of the equations. The derivation of AWRM was easy to follow with the structure put in place by the authors.

The included experiments clearly lay out the intended contribution of the proposed approach, that GALILEO provides a more accurate and counterfactually correct model. Impressively, this improved model is shown to have clear benefits for downstream performance in tasks beyond the “pre-training” task.

Weaknesses:
I’m not sure I agree that having real-world offline data being biased is a catastrophic problem. Obviously for model-learning it creates major difficulties but perhaps we can focus on identifying better approaches to using expert data than expecting an RL agent to “figure it out” when there aren’t reliable demonstrations of counterfactuals. There’s likely a good reason why alternative or “exploratory” behaviors are not represented in the data? See Fatemi, et al (2021; NeurIPS) and Killian, et al (2023; TMLR) for a formulation of how we may more adequately think about risk and decision making in such environments.

The spacing between paragraphs and definitions+equations is really tight. This makes the paper difficult to read. I understand that this was likely a response to the NeurIPS template and page limitations but it should be fixed.

While the downstream performance of MBRL methods using GALILEO models is promising. I wish that more analysis was done in the learning dynamics + performance in these downstream tasks regarding sample efficiency and deviations from the behavioral data / optimal policies in these environments. Do the GALILEO agents have predetermined action sequences that they exploit early on or are they flexible to the change in domain/task?

>Fatemi, Mehdi, et al. ""Medical dead-ends and learning to identify high-risk states and treatments."" Advances in Neural Information Processing Systems 34 (2021): 4856-4870

>Killian, Taylor W., Sonali Parbhoo, and Marzyeh Ghassemi. ""Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning."" Transactions on Machine Learning Research (2022).


Limitations:
As stated by the authors, there are several simplications to the modeling process which may be a cause for the deviation in GALILEO performance when applied to downstream tasks. 

Additionally, as mentioned in the “Weaknesses” section, there is an assumption that counterfactual modeling (on action sets outside the support of the dataset) is admissible. This may eliminate the use of GALILEO among safety critical domains, which would be a majority of real-world settings where model-learning could be useful.

Rating:
8

Confidence:
5

";1
mOVEJletyD;"REVIEW 
Summary:
This paper proposed a new method to enable effective contrastive learning (CL) on the lightweight encoder without a mega-size teacher model, which can thus reduce the training cost. More specifically, it expanded the target model to a larger model, which shares its weights with the target model. Then the CL problem is formulated as slimmed training task with asymmetrical encoding. Furthermore, this work incorporated cross-distillation to further minimize the decorrelation between the embeddings of the same view but from different encoders. Experimental results show that the proposed method can outperform the existing baselines with much smaller computational costs on the lightweight models.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-motivated. Figure 1 clearly shows that the efficient CL of the lightweight model is challenging and what improvement the proposed method can make to this problem. 

2. The proposed method is reasonable and Figure 2 clearly shows the key idea.

3. The proposed method can not only be effective in the CL of a lightweight model but also in a mega-size model.  

4. It is good to see that the authors include the experimental results to show that the trained encoder can be well generalized to different types of downstream tasks.

Weaknesses:
1. Although the authors claim Ref [25] cannot work on the mega-size encoder, it should be compared with the proposed method on the lightweight encoder.

2. It would be better if the authors could show the performance of supervised learning in the tables of the experiment section to serve as the upper bound of the proposed method.

3. It would be better if the authors can add a column for the training cost in Table 3, 4 and 5.

4. Some technical details are not clear, which is shown in the question part.


Limitations:
Although the author claims that their method does not need an extra mega-size teacher, their method still needs an expanded version of the task model for effective CL.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a self-supervised contrastive learning method to improve the light-weight network performance and reduce the training cost. It mainly consists of two components: slimmed asymmetrical contrastive learning (SACL) and cross-distillation (XD), which are able to train the efficient network from scratch without the usage of pre-trained strong teacher. Experiments the effectiveness of the proposed method in terms of accuracy and training FLOPs reduction.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. SACL+XD is effective to improve the light-weight network performance with efficient training.
2. well-written and easy to follow this paper.


Weaknesses:
1. The SACL+XD seems to apply the slimmable neural network[ref-1] to Disco[15]. Therefore, the novelty seems to be limited.

2. [Experimental issues.] (1)This paper mainly uses several efficient backbones (e.g. MobileNets) as the base model backbone, It lacks of comparisons on the more dense backbones, such as ResNet-50. (2) SACL+XD trains the models from scratch by removing a unified amount of channels based on the lowest magnitude score. I think the initial weights should have an effect on the final performance, as the channel selection in a slimmed network f_{\theta}^s may be different under the different weight initialization settings. However, the experiments lack such comparison about the initialization, as well as the mean accuracy with std. (3) In this paper, computation reduction is evaluated by training FLOPs. The authors better add the practical training time for a more comprehensive comparison. 


[Ref-1] Yu J, Yang L, Xu N, et al. Slimmable neural networks[J]. arXiv preprint arXiv:1812.08928, 2018.

Limitations:
Yes. The authors well address the impact and limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a self-supervised contrastive learning algorithm designed specifically for training lightweight models, eliminating the need for a large teacher model. The algorithm comprises two main components: slimmed asymmetrical contrastive learning (SACL) and cross-distillation (XD). The authors evaluate their approach using different lightweight models and datasets, demonstrating its superiority in terms of performance and efficiency over existing methods. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1, This paper is well-motivated. This paper tackles a significant and practical challenge of training lightweight models using contrastive learning, which has received limited attention in previous research efforts.  
2, The paper presents a novel and straightforward concept of slimming the host encoder, creating asymmetrical encoding paths for contrastive learning. This innovative approach effectively reduces training costs and enhances the performance of lightweight models.  
3, The writing is clear and easy to follow.  
4, The paper conducts thorough experiments on diverse models, datasets, and downstream tasks, and conducts comprehensive comparisons with state-of-the-art methods. The results strongly validate the efficacy and general applicability of the proposed method.  


Weaknesses:
1, To enhance the clarity and understanding of the proposed methods, it would be beneficial to provide additional intuition. Specifically, more insights can be provided on how cross-distillation aids in overcoming the distortion resulting from asymmetrical encoding. Additionally, exploring the trade-offs associated with different levels of asymmetry and sparsity would further enrich the paper.  



Limitations:
Please refer to the weakness and questions.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose a combination of two method: Slimmed Assymterical Contrastive Learning(SACL) , and Cross-Distilation(XD) with a correlation-maximization loss.  SACL does magnitude pruning of filters at each epoch, and the pruned model is used as a encoder of one of two views used in the contrastive loss.  They find that XD with an efficient network already provides competitive performance, and SACL brings performance to state-of-the-art. Evaluations are done with linear evaluation on ImageNet, CIFAR, and VOC2007. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: Though cross-distillation and training sparse networks are both known techniques, their combination with correlation-based contrastive learning is a novel combination. 

Quality: The quality of the experimental design is sufficient to be convincing, and the idea makes sense, and seems easy to implement, increasing potential impact. 

Clarity: The clarity of the writing is reasonably high, with the exception of the description of XD(see weakneses). 

Significance: Self-supervised pre-training has proven to be quite useful, and efforts to remove key limitations in encoder size can achieve important practical impact. 

Weaknesses:
* Experiments: One key advanatage of SSL methods  is in transfer learning, and learning general features. However, the transfer learning study in this work is relatively small, with only CIFAR and VOC presented. In my mind, this is the biggest weakness of the work. 
*  It's not explicitly stated how XD experiments are setup. Are they cross-distillation across two instances of a network of the same architecture (but independent weights), like Figure 2c but without slimming?
* Slimming idea is general, but implementation seems CNN-specific. 
* Line 85: Minor, but saying ResNet-50 is mega-sized is a bit of an overclaim IMO. 
* Lack of substantial discussion of different axes of efficiency (FLOPs, wall time, activation count). Each of these metrics is useful in its own way, but training FLOPS is primary focus in this work. 

Limitations:
Limitations are adequately addressed? 

Rating:
6

Confidence:
3

";1
jOuPR9IH00;"REVIEW 
Summary:
This paper considers variance-weighted least-squared regression for offline RL with general function approximation. Under a uniform data coverage assumption, they show that the proposed algorithm obtains a sub-optimality bound that scales with the $D^2$-divergence of the offline data set, the positive lower-bounded constant of the uniform data coverage, and the complexity of the function class. Their bound obtains the right order when realized in the linear case. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- clear presentation (though some parts can be improved further -- see Weaknesses)
- the obtained result is new and relevant to the offline RL community 


Weaknesses:
- The main weakness is that the uniform data coverage assumption is very strong. In the linear case, this assumption is equivalent to that the behavior policy is exploratory overall dimensions of the linear feature. A question for the authors is that in such a case, why would we even need pessimism?  Pessimism is used when the data coverage is partial thus we become pessimistic about uncertain actions. But when the coverage is uniform, it can eliminate the need for pessimism and we can simply use greedy algorithms. I understand that without such a uniform data coverage assumption, it seems difficult to get a reliable estimation of the variance of the transition kernel and it would be interesting to get rid of this assumption. But if we could not get rid of it yet, the very least expectation is that we need to explain this assumption further, especially regarding where pessimism is really needed with this assumption. 


- Writing can be improved further. For example, the $D^2$-divergence and the definition of the bonus function (Def 4) can be explained and motivated further. The current presentation of these concepts are not very helpful 

- Some claims might be potentially misleading. It's not comfortable to view the proposed algorithm as computationally efficient even in the oracle sense. Specifically, the construction of the bonus function in Definition 4.1 is far from being computationally efficient since it is essentially a constrained optimization over the version space. That said, it is nowhere more computationally efficient than version-space-based algorithms such as the ""Bellman-consistent pessimism"" of Xie et al. 

- Though the main result is new, it appears expected given the already-developed machinery in Argawal et al. 2022 and Xiong et al. 2022. What are the technical challenges in the current problem that the existing techniques cannot resolve? 

- Some minor: PNLSVI is never introduced before used 

Limitations:
Yes 

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper studies offline RL with non-linear function approximation. The paper is mainly motivated as existing sample complexity guarantees on offline RL algorithms with general function approximation yield suboptimal dependency on the function class complexity, e.g. when the bounds are translated to the linear case. The paper proposes an oracle-efficient algorithm that achieves minimax optimal problem-dependent regret when the bounds are specialized to the linear case. The paper also introduces a new coverage definition.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper appears to be technically sound with some new ideas in the algorithm design and formulation of dataset coverage.
- The approach achieve minimax optimal rate in non-linear function approximation, when bounds are converted to linear.

Weaknesses:
- The main weakness is that the proposed approach either requires uniform coverage or non-linear bonus oracle. The non-linear bonus oracle is a strong requirement and in effect, simply removes the difficulties related to pessimism in offline RL. On the other hand, the uniform coverage assumption is too strong and thus, it is unfair to compare its efficiency to pessimistic offline RL algorithms.
- A clear comparison to prior work is not presented. In particular, there are multiple axes of comparison, such as dependency on $\epsilon$, dependency on function classes, data coverage requirement, type of oracle, computational efficiency/tractability, realizability assumptions, etc. It is difficult to clearly evaluate the results in this paper without such comparisons. For instance, it will be helpful to have a table as well as translating the bounds of the other algorithms into linear case to see in detail. Additionally, there are several pessimistic offline RL algorithms with general function approximation that only require optimization oracles instead of the more difficult bonus oracle, and no comparison with those papers are presented:

Cheng et al. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR

Rashidinejad et al. ""Optimal conservative offline rl with general function approximation via augmented lagrangian."" arXiv preprint arXiv:2211.00716 (2022).

Ozdaglar et al. Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation. arXiv preprint arXiv:2212.13861

Zhu et al. Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a pessimistic nonlinear least-squares value iteration algorithm to tackle the offline reinforcement learning problem. The main motivation of the paper is to propose an algorithm that are both computationally efficient and minimax optimal w.r.t. the complexity of nonlinear function class. The proposed pessimism-based algorithm strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation and is oracle efficient. Also, the proposed algorithm is proven to be optimal w.r.t. the function class complexity, closing the gap originated from the previous work on differentiable function approximation.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
1) The proposed algorithm is proven to be optimal w.r.t. the complexity of nonlinear function class, closing the gap from the previous work on the differentiable function class and generalizes it to the wider nonlinear function class.
 2) The proposed algorithm is computationally efficient if there exist the efficient oracles for both regression minimization and bonus function optimization/searching.

Weaknesses:
1) The paper's presentation needs some work. For example, the terminology definition is not consistent. The D^2 divergence definition in Definition 3.2 is not consistent with the later terminology of D_F in line 239. The language itself needs some work too. For example, lots of places where it needs 'an', but 'a' is used and vice versa. Please define RL before using it in the abstract. There are also some ambiguities in the definitions that needs clarification in the Question section. 
2) The paper's claimed contribution is a bit exaggerated. Although the proposed algorithm does not need the computationally heavy optimization as previous works in planning phase, it transfers the main computation burden to the Oracle to find the satisfied bonus function, which seems to be a very time-consuming task. It also applies to the claim of being the first statistically optimal algorithm for nonlinear offline RL. Being able to get optimal result in the reduced linear function class does not necessarily mean it's optimal in the broader nonlinear class.
3) Although the considered class is the nonlinear one and general than the previously considered linear or differentiable class, the techniques used in the analysis are nothing new in my opinion, except re-defining the metrics in the nonlinear function class and connect the results together along with additional assumptions.
Overall, I think the paper is well motivated, but given the presentation and the insignificant contribution, it's not ready to be published. 

Limitations:
N/A

Rating:
3

Confidence:
3

";0
hSTaTBIUCj;"REVIEW 
Summary:
In this paper, text-to-image synthesis under the abstract-to-intricate setting is studied. Firstly, the input prompt is hallucinated and expanded into feasible specific scene structures by the proposed SGH mechanism. Then, text-to-image synthesis is implemented through a diffusion-based synthesizer by gradually incorporating semantic scene structure induced from the SGH. Extensive experiments on COCO, especially on the abstract-to-intricate text-to-image setting, prove the method could contribute to synthesizing images reasonably and accurately under the simple text.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.	The proposed abstract-to-intricate T2I is a vital research topic in the field of text-to-image synthesis since existing large models require delicate and well-designed prompts for controllable image synthesis.
2.	Abstract-to-intricate T2I is further promoted by the designed SGH mechanism, avoiding vision distraction from text and wrong focus enriched prompts.
3.	Extensive experiments including thorough metrics are conducted to prove the effectiveness. And corresponding analysis and discussion are reasonably stated.


Weaknesses:
1.	Existing experiments compared to the text-based enrichment methods are only conducted on manually selected dataset COCO-A2I with Place Norm and Progressive Verbs, which is not representative and convincing enough.
2.	There exists unfair comparison in T2I results on COCO, since the best FID and CLIP score of Frido are respectively 8.12 and 0.7915. And the best T2I baseline on COCO is not Frido. As far as I know, make-a-scene[1] achieves a 7.55 FID, which is not discussed.
3.	Scene graphs consist of two parts, which are node and edges representing semantics, bounding boxes referring to sizes and locations of objects. As proved by previous works, incorporating visual guidance into T2I training is beneficial. Why bounding boxes information is not included for training?
4.	Overclaims and inaccurate description in contributions:” We propose a diffusion-based model with SG guidances for highly controllable and scalable image generation.” Scene graph hallucinations might include unexpected concepts, which is not controlled by users.
5.	Unclear captions and inconsistent description. The caption of Fig.2 is unclear and lacks descriptions about each subfigure. 
6.	Citations about scene graph-to-image synthesis and scene graph generation are not thoroughly included[2-4].
7.	Typos and inconsistent descriptions: In line 226, “diffusioninspired”. In line 245, Frido-G is inconsistent with the description on Tab.1. 

[1] Gafni O, Polyak A, Ashual O, et al. Make-a-scene: Scene-based text-to-image generation with human priors[C]//Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV. Cham: Springer Nature Switzerland, 2022: 89-106.
[2] Sitong Su, Lianli Gao, Junchen Zhu, Jie Shao, and Jingkuan Song. 2021. Fully Functional Image Manipulation Using Scene Graphs in A Bounding-Box Free Way. Proceedings of the 29th ACM International Conference on Multimedia. Association for Computing Machinery, New York, NY, USA, 1784–1792. https://doi.org/10.1145/3474085.3475326
[3] Lyu X, Gao L, Guo Y, et al. Fine-grained predicates learning for scene graph generation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 19467-19475.
[4] Herzig R, Bar A, Xu H, et al. Learning canonical representations for scene graph to image generation[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVI 16. Springer International Publishing, 2020: 210-227.




Limitations:
yes

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper studies a new setup of generating intricate images from abstract prompts. To overcome the issues of vision distraction and wrong binding of using text as the condition to generate images, the author proposes a two stage pipeline by first generating scene graph from abstract text inputs, and then condition on the sythesized scene graph, another model generates images. The proposed diffusion model with SG guidance showcases its controllability and interoperability and it achieves new SoTA results in the abstract-to-intricate T2I setup. Overall, the paper is well-written and the proposed method is interesting and novel. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The introduction section is easy to follow, they provide examples to show the issues of the current T2I generation and motivate the scene graph representation as a potential solution.

The proposed SGH model is interesting and well adopted from VQ-diffusion etc for image modelling to the problem of scene graph modelling.

The experiments are convincing with strong performance on COCO, outperforming competitive baselines LDM, VQ-diffusion and Frido, they also construct an abstract-to-intricate dataset from COCO and demonstrate SoTA performance in this setup.


Weaknesses:
One claim of the paper is that SG guidance helps image generation with strong semantic controllability, it is not clear to me which experiments can support this claim.

In table 3, it seems like replacing the HSI module with GCN encoding only drops a little bit of the performance, it is questionable if the design of HSI is necessary.


Limitations:
As pointed out by the author, the proposed method depends on the generation quality of the SG, while a large-scale SG dataset is rare. Plus, since the pipeline needs to modify the conditioning of the LDM, how to better leverage pre-trained large T2I model is worth investigating.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel approach to text-to-image (T2I) synthesis, specifically focusing on generating intricate visual content from simple abstract text prompts, aka., abstract-to-intricate (A2I) setting. The proposed mechanism, named scene-graph hallucination (SGH), expands the initial scene graph (SG) of the input prompt with more feasible specific scene structures using discrete diffusion technique. A continuous-state diffusion model then serves as the T2I synthesizer navigated by the semantic scene structure induced from the SGH module. Additionally, this paper further devises a scene sampling mechanism to generate various scene graphs. They also construct a more challenging benchmark data, called COCO-A2I, to effectively evaluate the models under the abstract-to-intricate setting. Experiments on two benchmark datasets shows that leveraging SG imagination helps better image generation, where Salad could hallucinate certain scene clues to facilitate the abstract-to-intricate T2I generation. The study also contributes to a better understanding of the efficacy and rationale behind scene graph features, with potential applications in other tasks such as image editing and text-to-video generation.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Overall, I enjoy the work much where it studies the specific conditioned image synthesis from an interesting and realistic perspective, with robust and novel technical methods. The paper seems very solid to me, with thorough evaluations from many angles, and the details are given quite sufficiently (with long and informative appendix content). While this is a very technical paper, there is immense interest in diffusion models under such novel perspective. I believe this paper will have the potential to unlock interesting future research.

**Interesting and meaningful perspective.**
The paper studies how to generate intricate images from succinct abstract prompts. This can be a very interesting and realistic perspective in many scenarios. The issue essentially lies in the natural modality asymmetry between language and vision, where strict one-to-one correspondences do not exist. This becomes particularly challenging when T2I systems attempt to capture the nuanced content of the prompts and generate corresponding high-quality images, thereby highlighting the need for deep semantic understanding in these systems.

**Innovative methodology.**
The authors propose a novel approach to the task of abstract-to-intricate T2I synthesis, where the image-generating process is controlled and navigated by the underlying semantic scene structure. On the one hand, it satisfies the human intuition for handling this task, which is pretty interesting and makes a lot sense to me. If the system could hallucinate some concrete textual clues which has a more corresponding relationship to the visual scenes, the generation process will ease largely. Technically, the devised method, called scene-graph hallucination (SGH), expands the initial scene graph (SG) of the input prompt by iteratively evolving new scene elements via discrete diffusion model, which are theoretically sound and empirically validated through experimental results. On the other hand, the hierarchical scene integration mechanism is able to ensure the highly effective guidance of the semantic scene features.

**Solid evaluations and convincing analyses.**
The results are presented clearly and concisely, showcasing a significant improvement for the abstract-to-intricate T2I task at hand. Some in-depth analyses are provided to offer substantial evidence and valuable insights from all different perspectives to support the claims made, such as, exploring the effectiveness and rationale behind the employed scene graph features. Also, the comprehensive implementation details will enhance reproducibility and facilitates the smooth adoption of the proposed approach. The codes are provided.

**Well-structured presentation and clear writing.**
This paper is exceptionally clear, well-written (except some notation problems) and good illustration, making it easy to understand. The details are given quite sufficiently, with long and informative appendix content. I’ve gone through the appendix and find almost everything I wanted to see.

Weaknesses:
While I think the paper is solid, it can be significantly improved further if the following issues (major or minor) are considered properly:

(ordered by appearance in the paper)

1. In Figure 1, the issues of vision distraction and wrong binding appear to represent the same issue where the resulted images deviate from the original user intention. Besides, it would be beneficial to maintain consistent terminology throughout. For example, in Figure 1(a), it is referred to as 'vision distraction', whereas in the article, it is mentioned as ""visual distraction."" 

2. Although the authors devise a scene sampling mechanism to generate various scene graphs for diversified image syntheses during inference, the diversity of scene graph remains inadequate. This could be due to the limited number of categories of objects, attributes or relationships in the SGH module, which restricts the model's imagination and exploration capacity. In real scenarios, the objects, attributes can be quite multifarious, for example, the color of the T-shirts.

3. Potentially lack of comparison of the existing scene graph completion work. The performance of scene graph hallucination has a huge impact on the performance of abstract-to-intricate T2I task. It would be interesting to see the performance comparison on existing work about scene graph hallucination, such as [1,2]. Moreover, another intuitive method is the pipeline method, i.e., first scene graph hallucination and then scene graph generation based the imagined SG. Therefore, some evaluating experiments should be provided to make a comparison with the pipeline method.

4. The authors may possibly overlook the imagination ability of the T2I model itself. For example, existing T2I models can synthesis an image based on the prompt, 'a man'. Is the appearance of the man in the generated image, such as hair color, attributed to the imagination of the T2I model?  In other words, the authors fail to provide a clear definition of when the model needs to image. Furthermore, they do not delve into what scene graph needs to be detailed in order to generate intricate images effectively. For instance, in Figure 1(b), the enrich SG not extremely correspond to the image as the screen on the wall are not demonstrated in the enriched SG, but the model is still able to generate it.

5. Some unclear and confusing annotations:

- In Figure 2 right bottom, it is confusing $a_{n, 1}$ whether one value, i.e., the m-th attribute of the object $o_1$, or many values. Same issues for $r_{n,k}$.

- In line 150, the distribution should be $p_{\theta}(s_t|s_{t+1}, y)$.

- Is $\mathcal{B}{s_t}$ a row one-hot vector or column one-hot vector? This should be clarified. Besides, there should be a transition symbol in Eq. 2.

- In Eq. 1, the $\mathcal{L}_{vlb}$ is not explicitly designed for the optimization of conditional image generation. Thus, some clarification should be added, and the correct loss function is demonstrated in Appendix B.1, specifically Equations 19-22.

- No illustration of the $d$ in Eq. 7.

- In the Implementation section, the version of CLIP mentioned is not consistent with the information provided in Appendix C.5. 

- No demonstration of the hyper-parameters? top-A and the temperature $\eta$ in the inference?

- In Table 3, the NTD-CA should be Eq. 5, not Eq. 17

- In Line 314, not ‘object-object’ pairs but ‘subject-predicate-object’ triplets.

- In Line 738, ‘2.005’ should be written as ‘2,005’.

- In Table 5, the second ‘Max’ should be ‘Avg.’


6. The paper lacks in-depth analysis of the circumstances under which the proposed SGH module fails to generate reasonable scene graphs, leaving a gap in understanding its limitations.

7. In the section 4.5, the question Q2 is interesting but strikes me as a rather weird way to answer the question. The Figure 7 precisely answers that the SGH is able to induce intricate SGs. As discussed in Q1, there is a strong semantic alignment between the input prompts and the generated images guided of the SG. Consequently, it becomes relatively straightforward to achieve a high TriRec. Score by comparing the induced SG with the SG of the generated image. Nonetheless, this response lacks directness and comprehensiveness regarding whether SGH is capable of producing reasonable SGs. Besides, the concept of a ""reasonable"" SG remains ambiguous, For example, if the ‘table-in-room’ is a reasonable scene triplet, what about the triplet ‘table-in-ocean’? If a reasonable SG means some SGs that conforms natural conditions, can our proposed model generate some abstract or unconventional images?


[1] Garg S, Dhamo H, Farshad A, et al. Unconditional scene graph generation. 2021.

[2] Agarwal R, Chandra T S, Patil V, et al. GEMS: Scene Expansion using Generative Models of Graphs. 2023

Limitations:
I do not foresee any potential negative impact from this work.

Rating:
10

Confidence:
5

REVIEW 
Summary:
The paper proposes a new setting (or sub-domain) of text-to-image generation (T2I), namely abstract-to-intricate T2I. To tackle the new setting, the authors propose a method (Salad) to enrich the scene graphs parsed from the text prompts based on the discrete diffusion models. The enriched scene graphs are used as guidance to generate complicated scenes that align better with the initial concise prompts. They report quantitative results and analysis experiments to show the effectiveness of the two-stage system in multiple metrics.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The paper addresses a practical and important problem. Users of the T2I system, in reality, have to write unreasonably long text prompts to generate images with plausible scenes/styles/semantics. Abstract-to-intricate T2I could benefit the users with a more faithful and efficient generation process. 

- The scene graph hallucination stage achieved with the discrete diffusion model is interesting. Enriching the text prompt in the scene graph space seems like a more controllable and stable process that can preserve the faithfulness of relations and attributes. 

- The authors conduct an extensive analysis of the components of the system to demonstrate its effectiveness.

Weaknesses:
- Task definition. While I understand the concept of abstract-to-intricate, I think the work lacks a more rigorous definition of the task. It seems unclear why places and progressive verbs are included and other nouns or verbs are discarded in the COCO-A2I. I tried the failure prompt in Fig. 1 on Stable Diffusion, and I got good results most of the time. So what makes these prompts an A2I problem instead of a faithfulness problem of all T2I models?

- My major concern is about the experimental setup. It seems that the Salad system is using the pre-trained Stable Diffusion as the image generator compared to other methods like Frido and LDM-G. The comparison could be unfair as all these baselines are trained with much fewer images and inherently have higher FID scores on MSCOCO. In addition, the authors use Frido for text-based enrichment approaches. 

- The writing needs improvement. There are multiple typos throughout the paper and some inaccurate sentences. For instance, line 133 ""attribute (o)"", line 226 ""baselines: stable diffusioninspired by [6]."", line 235-236 ""For the SIS module, we load the parameters of Stable Diffusion3 (v1.4) as the initialization."", line 238 ""UNet"" (UNet in Stable Diffusion?).

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper aims at the abstractive setting for text-to-image generation (T2I). They propose scene-graph hallucination (SGH) to perform imagination over the scene graph of the input prompt and make up the missing information. Then SGH can perform better T2I with the completed scene graph. Experiments on the COCO dataset demonstrate that T2I can significantly bridge the gap of abstract-to-intricate T2I.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ This paper is well-written and easy to follow.
+ The goal of abstract-to-intricate T2I is important since we can not expect users always input their full prompt. This setting has the potential to improve practical applications.
+ The usage of scene graphs is effective and can lead to better imagination for T2I than the Chain-ot-Though (CoT) language prompt.
+ They provide detailed ablation studies from different aspects (Table 3/4 and Fig. 6/7) as well as rich qualitative examples (Fig. 5 and Fig. 12/13 in Appendix).

Weaknesses:
- Maybe I missed it, but what is the motivation for using scene graphs instead of LLM-completed prompts? Does the imagination over scene graphs work more robustly than LLM?
- Since they rely on imagination to deal with abstractive issues, the hallucination situation also raises. It will be better to evaluate this issue, and how is the trade-off between this and the T2I performance. How to avoid or mitigate this deficiency in the proposed framework?

Limitations:
The hallucination issues over scene graphs should be carefully addressed.


Rating:
8

Confidence:
4

";1
h4r00NGkjR;"REVIEW 
Summary:
The paper presents a method for compositional video synthesis. It introduces motion vectors from compressed videos as a control signal for temporal dynamics. The motion vector can be combined by other conditions such as sketch, and depth map. Both qualitative and quantitative results show that the proposed method can control the spatial-temporal patterns. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The motion controlled generation result (fig 8) using hand-crafted strokes is interesting. 

+ Table A1 shows effectiveness of the proposed method quantitatively compared to previous methods.

+ The paper is well written and easy to follow.

Weaknesses:
- There are a few GAN-based video synthesis approaches that are worth discussing in the related work. For example, MoCoGAN [1] approaches the problem by decomposing motion and content. 
 
- The two-stage training strategy needs more clarification. What is ""compositional training"" particularly in the second stage?  How does it differentiate from the ""text-to-video"" generation in the first stage?

- In line 164-165, the authors ""repeat the spatial conditions of a single image and single sketch along the temporal dimension"". If the input condition is simply repeated, what's the point of applying a temporal Transformer? It will be equivalent to applying the spatial operation only and repeat at the latent space but with higher computation cost, no? (for motion vector, I totally agree that a spatial-temporal modeling would be necessary.)

- Motion vectors can be less meaningful in the background due to lack of high-level semantics. It can also be clearly seen from the top row in Fig 4. I wonder if the authors treat the motion vector field equally for all locations. It seems that the generated results with motion conditions has more blurry background.

- From Figure 2 and Figure 1(d), my impression is that the conditions (say motion and depth) can be combined together. However, in ablation studies (table 2), only one condition is added at a time. Another ablation that studies all combinations of these conditions will be favored. 

[1] Tulyakov, Sergey, et al. ""Mocogan: Decomposing motion and content for video generation."" CVPR 2018.

Limitations:
The authors have addressed the limitations in the supplementary materials.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work aims to allows users to flexibly compose a video with textual conditions, spatial conditions, and temporal conditions.
It introduces a novel framework namely VideoComposer based on the paradigm of compositional generation.
To be specific, it introduces the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics.
Moreover, it develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. 
Extensive experiments demonstrate that VideoComposer control the spatial and temporal patterns simultaneously within a synthesized video in various forms.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. It introduces motion vector as a more flexible user-guided signal.
2. It proposes Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs.
3. Extensive experiments show the effectiveness and superiority of VideoComposer.

Weaknesses:
1. What is the difference between the roles of the ``Style`` of CLIP and ``Single Image`` of STC-encoder? They both seem to provide content to videos.
2. VideoComposer only obtain comparable performance with prior video generative models. Is it more efficient than previous methods? The authors could give their comparisons in training cost and inference time.
3. Lack of extensive visualization comparisons with existing video generative models. The authors are encouraged to provide extensive qualitative comparisons in video generation task.


Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a new method called VideoComposer for conditional video generation, especially for video-to-video translation. VideoComposer is constructed upon the Video Latent Diffusion Model and introduces an STC-encoder to integrate multiple spatial and temporal conditions such as RGB images, sketches, motion vector sequences, etc. The architecture design involves simple 2D convolutions and temporal transformer layers. The conditional features are fed into the U-Net input together with noise. The demonstrated results have good temporal consistency.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This is one of the pioneering works in controllable video synthesis. The temporal consistency of the video results is impressive, considering that its conditioning modeling enables several editing abilities such as image-to-video translation and motion/depth/sketch-driven local/global video editing.

- The jointly training strategy is good for flexible inference within one model, e.g., video inpainting, without second training.

- The paper organization and illustrations are easy to follow.

Weaknesses:
- The authors could have tried other design choices for integrating Condition Fusion as input into the U-Net, such as integration through cross-attention.

- In line 215, it is claimed that “we observe that the inclusion of mask and style guidance can facilitate structure and style control.” However, the corresponding evidence should be presented for the style representation extracted by clip image encoder and concatenated with text embedding.

- It seems that a single STC-encoder is used for all different conditions via random dropout. It would be interesting to see if different STC-encoder weights for different conditions are better.

- The examples in Figure 6 with reference image look like failure cases. Besides, the tiger texture and box shape are changed in Figure 8. It would be helpful to see more discussion and analysis on this part.

- The ablation study of STC-encoder is not presented in a fair way. The main benefit of using STC-encoder comes from the video information condition instead of the network design.

- The important comparisons and discussions with other methods are not sufficient, such as VideoP2P and vid2vid-zero mentioned in the related works.

Limitations:
The societal impact has been discussed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
VideoComposer is a tool designed to enhance video synthesis by incorporating textual, spatial, and temporal conditions. It uses motion vectors from compressed videos to guide temporal dynamics and employs a Spatio-Temporal Condition encoder to effectively integrate spatial and temporal relations of inputs. This improves inter-frame consistency and allows for greater control over the synthesized video's spatial and temporal patterns.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The VideoComposer offers better control over video synthesis, temporal guidance using motion vectors, improved inter-frame consistency with its Spatio-Temporal Condition encoder, versatility in accepting various forms of inputs, and high customizability, resulting in more precise and desired synthesized videos.

Weaknesses:
An ablation study could be conducted on VideoComposer, where each component is removed in turn to evaluate its impact on overall performance. This would help evaluate the value of training under multiple conditions versus a single condition. 

Additionally, comparing VideoComposer to a simpler method like Text2Video-Zero [a] with ControlNet [b] would demonstrate whether the increased complexity of VideoComposer yields significantly better results, hence justifying its sophistication. 

[a] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, L Khachatryan et al.
[b] Adding Conditional Control to Text-to-Image Diffusion Models, L. Zhang et al. 

Limitations:
What are the limitations?

Rating:
5

Confidence:
3

";1
PU3deePP2S;"REVIEW 
Summary:
In classification problems with a finite number of classes under a supervised setting, if the feature vectors are generated from a mixture of distributions conditioned on the class labels and the learning is performed using generalized linear models, a phenomenon called ``Gaussian mixture equivalence (GME)'' simplifying the problem is known to occur. GME is the phenomenon in the high-dimensional limit that when considering a model in which the original conditional distribution of the feature vector is replaced by a Gaussian distribution having the same first- and second-order moments as the original ones, the quantities of interest such as generalization error coincide between the original model with the non-trivial class-conditional distribution and the reduced model with the Gaussian distribution. This paper rigorously proves this GME actually occurs under some (partly mild, partly strong) conditions by extending the method of reference [17] to the case involving multiple sets of parameters, which may be determined from the minimization of different objectives or by sampling from different Gibbs distributions. 


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
Originality & Significance: Although GME has been assumed in previous studies and meaningful results have been shown under the assumption, the results of this paper clarifies the sufficient conditions under which GME can actually occur by the rigorous proof. This provides a mathematical foundation for attempts to understand complicated machine learning/statistical models' behavior through the analysis of simple Gaussian mixture models. The originality and the significance are thus high. 


Clarity & Quality: The main text mainly provides the statements about the theorems, assumptions, and their consequences/applicabilities, and most of the proof details, even including the intuitive arguments, are deferred to the appendix. The statements are also not easy to understand, because many things are explained in the limited pages and the relationships among the theorems, assumptions, and consequences/applicabilities are not evident. The distinction between what has been proven in previous studies and what has been reiterated here is also not clear. Hence the clarity is not high. Nevertheless, the argument can be understood if one reads the manuscript carefully, and considering the importance of what is proven, also with the nice numerical experiment completely supporting the proposition, it is considered that the quality is high.


Here I enumerate some typos found and uneasy expressions for better clarity:

Line 162: The sentence is not easy to understand.

Below Line 200: The quantity $\ell'$ is not explained.

Line 225: be be -> be

Line 246: Theorem 2 -> Corollary 2

Line 262: a result convex -> a result of convex

Line 305,325: gaussian -> Gaussian


Weaknesses:
- Proof technicalities seem to much rely on [17], and there does not appear to have been much technical progress when extending it to the current case. 

- Assumption 4 is the key assumption in the proof but when it does hold is not clear. Maybe this is the difficult point of the problem, and hence some more explanations in words are welcome. 


Limitations:
The authors address their research limitations well. I think there is no concern about the potential societal impact. 


Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors investigated the joint statistical behaviors of a set of genealized linear estimators obtained from either empirical risk minimization or sampling, for data and labels drawn from a mixture model, in the high-dimensional regime where where the data dimension $p$ and the sample size $n$ both go to infinity.

In particular, they establish sufficient conditions under which the asymptotic joint statistics of this family depend only on the means and covariances of the mixture model.

The obtained results extend a few previous efforts: for instance, provide alternative proof to the fact that Gaussian mixture observed through a random feature map is akin to a Gaussian mixture [11, 12, 21, 22, 16]; and that data generated by conditional Generative Adversarial Networks (cGAN) are like Gaussian mixture [23], etc.

Some theoretical results are presented in a not-so-precise manner and some clarifications are needed. The paper is in general well written otherwise.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The topic focuses on the emerging topic of universality of ML methods in the high-dimensional regime.
The obtained theoretical results are strong and improve prior arts in many ways.
I did not check the detailed proofs, but the proof sketch look compelling.

Weaknesses:
There are a lot of assumptions in the paper, and it somewhat makes the paper less easy to grasp and compare to existing works, some more explanations are needed.
Some clarifications are needed to better understand the statements in the paper, as well as the major contribution of this paper.


Limitations:
This work is theoretical and I do not see any potential negative societal impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The submission studies the universality phenomena for general mixture model. The authors show the universality of empirical risk minimization and sampling and several extensions.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The authors give good introduction and literature review.

Weaknesses:
1. The main contribution lies in the technical part. However, it lacks novelty given the results in Hu and Lu 2022, Montanari and Saeed 2022. The assumptions 1-4 and Section 2.1 are almost identical to the main results in Montanari and Saeed 2022 without mentioning or comparisons at the beginning. The proofs for Theorem 1 and Corollary 2 only recap the previous results in the appendix. Section 2.2 gives some simple extensions of those results in Montanari and Saeed 2022 to multiple risk case.

2. The paper has plenty of inconsistencies and lack of explanations in the course of writing. For example, line 109, the notation of \eta is not consistent to equation (2). The three applications in Section 1 are hard to follow given the model setting. 

3. Some of the notations are not proper: For example, line 101, it would be better to write \hat{y}=F(\hat{\Theta}^T x) to denote estimated parameters. I suggest the authors modify the notations in the whole paper to fit their own topic and interests instead of using those in Montanari and Saeed 2022.

4. Also, it would be better to explain the notations immediately after introducing.

5. The statements of the main theorems and proofs in the appendix is lack of polish. It would be better to write remarks after the theorems to improve the readability.


Limitations:
No

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proves that the performance and other quantities of interest output by a number of algorithms in the generalized linear models class are the same as if the data were replaced by Gaussians with the same means and covariance matrices. This universality result suggests that the performance of this entire class of algorithms (generalized linear models) depends only on the first- and second-order moments of the data. This theoretical study justifies the use of a Gaussian mixture model in the study of generalized linear algorithms.
The particularity of the study consists in proving this important phenomenon rigorously even on restrictive assumptions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1-The question of the universality of Gaussian mixtures in the analysis of the performance of generalized linear algorithms is a subject that has long remained empirical and has known very few rigorous and clear answers. This paper by providing a rigorous answer, makes it possible to make an important step in the resolution of this question.

2- The paper is generally well written with very few spelling mistakes and consistency in notations. The motivation and structure of the paper seems to be quite clear.

3- Although not having checked the theoretical calculations, the approach seems to be rigorous and well conducted. The results are consistent.

Weaknesses:
1- Assumption 4 and Assumption 5 need to be better explained in terms of their intuitive meaning and how they are binding. Although there is some discussion that it is very restrictive and experiments tend to suggest that the results should be valid beyond that, it would be important to provide more intuitive explanations of what it means and why it is restrictive.

2- In the experiment shown in Figure 2, there is a systematic bias between the performance of the CGAN and the GMM. It would be interesting to discuss this aspect, providing explanations if necessary. It would have been useful to show the limits of universality in a general way (even in the appendix). Are there cases where this holds less well, and this would be due to what phenomenon, or are there algorithms that are more resistant to this universality in the class of linear algorithms?

3- Although I appreciate the authors' effort to provide experiments on real data, it would have been interesting to do so with different data structures and slightly more challenging data such as time series and language classification. In particular, we could ask the question of universality for highly structured data such as very sparse data (like speech encoding) or time series. It seems somewhat counter-intuitive that for these highly structured data, the mean and variance alone are sufficient to characterize performance. Recently, in random matrix theory, universality has been focused on a class of random vectors that come from the concentration of the measure and it is not obvious that highly sparse data are concentrated and therefore that universality holds for this type of data, even through the prism of a linear model.



Limitations:
The main statement of the paper seems quite strong even though the mathematics seems rigorous. In this case, it would be really important to provide a wide set of experiments over several datasets (with different structure) and several linear algorithms. This would be more reassuring than checking the calculations line by line for the reader.

Rating:
7

Confidence:
3

";1
fKwG6grp8o;"REVIEW 
Summary:
The authors study a finite width correct to the Dynamical Mean Field Theory (DMFT) of finite depth neural networks in the feature learning regime. While I will be the first to admit that I am not an expert on the DMFT calculations, the authors did produce very convincing simulations capturing interesting properties of finite size networks. In particular, the edge of stability behaviour at large learning rates seems to be modelled by the finite width correction. 

I hope the authors can help me clarify some questions regarding the implementation of the solver, after which I would be happy to raise my score to accept. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors develop a strong theory of finite width neural networks capable of making accurate predictions. 

Weaknesses:
N/A

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
Building on past work which set up a DMFT (Dynamical Mean Field Theory) for fully connected networks in the infinite width limit (where the width of each layer tends to infinity), this paper reasons about the *fluctuations* around the infinite width limit. This is important because for finite sized neural networks, these fluctuations are large enough that they are an important part of the network training dynamics. The description of the DMFT is theoretically complicated and cannot be solved exactly, but they enable simulations which confirm that the DMFT captures the behaviour of finite sized real networks quite accurately. As an application, this DMFT theory is used to understand bias, training rates, and variance in realistic tasks and the special case of 2-layer networks (where the theory is quite a bit simpler) is investigated in more detail. This theory has the potential to open the door to many potential future uses that explain how neural networks learn. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
This paper lays out a framework for theoretical understanding of deep neural networks that incorporates the effect of finite width. This is a  problem that has received a lot of attention since the original NTK/infinite width limits came out, and as far as I know, the approach here is novel and powerful. The paper is well written for the most part although familiarity with the infinite width DMFTs is assumed. 

Overall, the fact that the theory and the simulations agree very well is quite impressive, and I think the ideas in the paper are quite ambitious because they can be used for almost any kind of question one might have about the theoretical evolution of the DNN. This paper has the potential to be the basis for future work which uses the theory developed here to investigate questions about how DNNs learn. 

Weaknesses:
The main weakness of the paper is that its a bit spread thin at times: both the theory and a few different applications are covered, but it seems like the authors were trying to make it all fit and I would have liked more detail in a few spots. This is largely due to the page limit of the submission. I personally would have found it to be a stronger paper if a single really clear example was presented in a lot of detail. (Although again, I completely understand that this is largely pressure from the conference format to try and do a lot of stuff)  

The other main ""weakness"" of the paper (which is strictly speaking a limitation of the audience of the paper) is that to understand it, you need to be familiar with the previous DMFT on which this paper is built. The authors include a very short section called ""Review of Dynamical Mean Field Theory"" citing [9],[46] as a review, but this section is extremely sparse for actually understanding what is going on. I essentially had to read [9] in its entirety first to understand what was going on in this paper. (Also the reference [46] could not be found since only authors and title are given...where would one find this reference?) In my view, this weakness could be mitigated by just being more honest with the reader up front about this...for example [9],[46] should be cited at point 1 in the list of contributions to make it more clear the dependence and what is/isn't actually explained in this paper. 

Another (related) ""weakness"" is that the paper relies quite heavily on physics technology and jargon to reach its conclusions. The fact that the results are so heavily entrenched in physics jargon like ""order parameters"" or ""propagator"" makes this paper less likely to have a broad impact on the deep learning community. The authors would add a lot of value to the work by attempting to make a ""translation guide"" to help people who don't have the same physics background understand what is going on in more detail.

Limitations:
A potential criticism is the physics level of rigor used in Appendix C which is used to establish the main results. The manipulations carried out in the proof of Appendix C certainly seem plausible, and I believe the community as a whole is ok with this level of rigour, but the authors could be a bit more clear about what they mean by ""proof"" in the main paper. It is not a mathematically rigour proof (which would involve all sorts of techincal assumptions), but rather a physics-type statement that holds assuming the usual expansions can be carried out without obstructions. To reiterate: I think the actual work is fine, but they could be a bit more honest about how it is ""proven"" and the level of rigour in the main paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of analytical description of the rich (feature learning) dynamics of neural networks. To achieve this, the authors use previously introduced dynamical mean field theory (DMFT), which identifies several key characteristics of the problem - order parameters - defines their probability distribution throughout the training using a path integral representation. The paper considers $\mu P$ parametrization of the network, which is known to display feature learning behavior at infinite width (in contrast to popular NTK and Standard parametrizations) with feature learning strength controlled by parameter $\gamma$.

In the paper, the authors focus on leading width $O(N^{-1})$ corrections to the infinite width limit. Technically, this is achieved by taking into account quadratic (Gaussian) fluctuation around saddle point $\mathbf{q}_\infty$ of DMFT action $S(\mathbf{q})$. After deriving general equations governing $O(N^{-1})$ corrections, the authors consider a number of simplified scenarios where these equations can be solved analytically. Also, the authors validate qualitative conclusions of their theory in the non-synthetic experiments with CNN trained on CIFAR10.  

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
The approach chosen by the authors - identifying saddle point $\mathbf{q}_\infty$ of the system's action $S(\mathbf{q})$ and investigating it together with Gaussian fluctuation around $\mathbf{q}_\infty$ - is a workhorse of various branches of theoretical physics where behavior of a complex system needs to be analyzed. In physics, this approach not only provides a very sizable portion of available analytical results but, for example, also provides SOTA results for numerical modeling of realistic strongly-correlated materials [Kotliar 2004](https://pubs.aip.org/physicstoday/article-abstract/57/3/53/755526/Strongly-Correlated-Materials-Insights-From?redirectedFrom=fulltext), [Vollhardt 2019](https://arxiv.org/abs/1910.12650). Thus, realizing this general strategy is one of the fundamental directions within deep learning theory. Also, as noted by the authors, the DMFT approach is non-perturbative w.r.t. feature learning strength  - a feature that is mostly absent in other approaches to NN dynamics away from kernel regime.

Weaknesses:
* In many cases, solving DMFT equations analytically seems to be intractable. This significantly renders the main purpose of the proposed theory - obtaining analytical insights into the network dynamics.
* The DMFT equations are bulky, which could make working with them quite exhaustive.
* I believe it is hard to understand main DMFT ingredients - order parameters $\mathbf{q}$ and their action $S(\mathbf{q})$ - from the current paper alone. Most probably, a careful reading of the original DMFT paper [9] is required to understand this paper. However, this is not the author's fault but rather a consequence of the chosen approach. 

Limitations:
The authors discuss the limitations of their current approach. The limitations mainly come from 1) the inability to numerically solve DMFT equations for large-scale problems (e.g. due to $O(P^4 T^4)$ propagator size) and  2) the need to consider higher order expansion around the saddle point $\mathbf{q}_\infty$. 

Rating:
7

Confidence:
3

";1
2ccH4zjKVs;"REVIEW 
Summary:
The authors focus on the problem to find approximate second-order stationary points (SOSPs) in the strong contamination model, where they propose an efficient algorithm with an approximate SOSP as an output. The algorithm is proved to have dimension-independent accuracy guarantees. In particular, the proposed algorithm can solve the low rank matrix sensing problem robustly. They first introduce the formulation of generic nonconvex optimization problem and the low rank matrix sensing and the assumptions for the main theorem where the outputs of the algorithm for the corrupted Stochastic optimization problem can achieve approximate SOSP with high probability. For general cases, the algorithm can obtain approximate SOSPs under strong contamination. At last the authors provide the theoretical guarantees for robust low rank matrix sensing problem.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and clear, the approach is well supported by the theoretical analysis. The application on robust low rank matrix sensing problem seems strong and nice. 

Weaknesses:
- It might be good to include some simulations or real data applications on the robust low rank matrix sensing problem to address its computational efficiency. Or maybe some comparisons to first order method such as  projected gradient descent for robust stochastic optimization problem will be better.

- The authors present theorem showing the results for general robust nonconvex optimization, like Theorem 1.5 and Theorem 3.1. As the authors say below Theorem 1.5, the assumptions appear restrictive and are satisfied for the matrix sensing. My question is whether the general results can hold for other nonconvex optimization problems such as matrix completion, phase retrieval, etc. Or what assumptions might be violated in these cases?

Limitations:
I have not found the part explicit addressing the limitations of the algorithm. Maybe authors can elaborate more on these.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of finding a second-order stationary point in corrupted settings. In particular, it considers the adversarial settings, where a fraction of the observations are arbitrarily corrupted after they are observed. In this setting, under certain assumptions on the cost functions, the authors give an algorithm that yields a second-order stationary point with $n=O(D^2/\epsilon)$ samples. This result is applied to problem of robust matrix sensing.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- This work yields the strongest guarantees of finding a second-order stationary point in this setting.
- This is the first work to use both robust mean and robust Hessian approximation to solve the second-order approximation problem.
- The work provides a statistical query lower bounds for rank one matrix sensing that shows that exponentially many queries would be needed to go beyond the $O(d^2)$ bound.

Weaknesses:
- One of the assumptions - that the iterates of the algorithms stay in a bounded region $\mathcal B$, is hard to check or guarantee in general.
- The work is a combination of two past works: the robust mean estimation algorithm of DKP20 and an anonymous work Aut23. It is not clear what new theoretical techniques are employed, or if this work is just a combination of the previous.
- The statistical query result also follows the theorems in past works.

Limitations:
- While an interesting and complete work, I am left unsure of what is new in this work, or if this is just applying already developed techniques to a new setting (finding a second-order stationary point) -- i.e., just combining the results of DKP20 and Aut23.
- No experiments are given showing the practicality of the method, which could help to motivate the usefulness of finding SOSPs. 
- This is a limitation of the setting, but many more samples are needed than the information-theoretic threshold.
- It is not clear why it is useful to find second-order stationary points in general.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this work, the authors proposed a new algorithm to find approximate second-order stationary points for stochastic optimization problems under the strong contamination model. The general algorithm is applied to the robust matrix sensing problem and the convergence results are proved for the robust matrix sensing problem.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The results in this work are novel and should be interesting to audiences in optimization and machine learning fields.

Weaknesses:
It is unclear how the results of this work differ with those in literature; see my comment (10) in the next section. The presentation of the results and the sketch of proofs can be improved. Currently, many important technical details are omitted in the main manuscript. For example, Algorithms A.1-2 and the construction of distributions for the SQ lower bound.

Limitations:
See my comments in the previous section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of finding approximate second-order stationary point when a constant fraction of datapoints are corrupted by outliers. It proposes an algorithm with provable guarantees which matches the statistical query lower bound established in the paper. The general result is applied to study low-rank matrix sensing with outliers.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper first proposes a general result in finding approximate SOSP and then applies it to study the widely applicable problem of low-rank matrix sensing. The newly proposed algorithm is proved to be able to find an approximate SOSP in polynomial time even when a constant proportion of samples are corrupted. It also provides a lower bound on the sample complexity for rank-one matrix sensing which matches the required sample size of the algorithm, confirming the efficacy of the algorithm.

Weaknesses:
1. The results obtained in Theorem 1.7 and 3.3 suggest that increasing the sample size $n$ (while fixing the noise level $\sigma$ and outlier fraction $\epsilon$) does not enhance the algorithm's performance, as the estimation error does not depend on the sample size. This observation seems counterintuitive and calls for additional clarification in order to better understand this phenomenon.

2. In Section 4, the paper presents a lower bound for low-rank matrix sensing specifically for the case where the rank is one (i.e., $r=1$). The statement ""Our main result in this section is a near-optimal SQ lower bound for robust low-rank matrix sensing that applies even for rank $r=1$"" suggests that the rank-one scenario poses additional challenges than the general low-rank case, which doesn't sound reasonable. Further clarification is needed to better understand this.

Limitations:
This paper does not have potential negative social impact.

Rating:
6

Confidence:
3

";1
PXUHrqIL9O;"REVIEW 
Summary:
The authors demonstrate that selective mixup has a similar effect with resampling. This reduces training dataset bias, which may contribute to settings with distribution shifts. Extensive experiments on five datasets were conducted.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- A new perspective of (selective) mixup has been discussed.
- Extensive experiments were conducted on various benchmarks
- The paper was well written and was easy to follow.

Weaknesses:
The authors have done a great job in providing an extensive list of experiments. However, I have two major concerns:
1. The paper's contribution to the body of knowledge is somewhat weak. I believe the gist of the paper is that ""(selective) mixup has a resampling effect"". Although this is a novel and interesting interpretation of the mixup data augmentation method, I feel this is not enough compared to the recent standard level of NeurIPS papers. I believe that a more concrete methodology that reflects the authors' findings is needed to qualify as ""acceptance level"".
2. The experimental results and their discussions are confusing. While the discussion on the waterbirds dataset says that ""selective mixup performance is entirely due to resampling"", the results on the arxiv dataset says ""selective mixup is affected both by vanilla mixup and resampling"", and results on the civilComments dataset implies that ""resampling is better than mixup"". These different experimental results and interpretations do not conclude the effect of resampling and mixup. Rather, the results seem to imply that ""mixup and resampling are independent methods that may or may not have correlating effects, depending on what dataset is used"". If this is the case, the primary assumption of this paper is undermined.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors focus on explaining the working mechanism of selective Mixup in distribution-shift scenarios. They points out an interesting and novel perspective that selective Mixup benefits not (only) from Mixup itself, but rather the resampling effect it induces. They suggest that such a resampling effect can balance the class and/or domain distribution (depends on the pairing criterion) into uniform distribution, which in turn improves the model's performance.

Theoretically, they show that under the different-class criterion of pairing the data, the classes' distribution does become more uniform. Experimentally, they take the worst-group (class or domain or their combination) performance  as the metric, and conduct experiments on several datasets and models. They investigates a variety of algorithms (e.g. ERM, Mixup, conventional resampling, etc.) and pairing criterions (e.g. different class, different domain, etc.). The results confirms their raised point of view on selective Mixup.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper presents an interesting and novel perspective of  undderstanding the mechanism of selective Mixup. That is, instead of focusing on the data interpolation part of Mixup, which is conventionally considered the key of Mixup and any of its variant, this paper indicates that it's the resampling effect, which averages the data distribution, that enables selective Mixup to success.

2. In all the experiments, the paper has fully investigated all the possible sampling criterions. Furthermore, on the basis of these experiments results, this paper combines the algorithms and criterions that give the best performance respectively and obtains a even better result under its own testing metric.

3. All the figures are well displayed and clearly readable.

Weaknesses:
1. This paper is a experiment-driven work. Not many theories are provided to back the main idea up. While Theorem 3.1 shows how certain selective sampling ""uniformizes"" the class distribution, there is no evidence nor explanation of how it affects the covariate distribution. Also, there is no investigation or explanation of what roles (if any) the Mixup operation itself play in selective Mixup.

2. When performing Mixup and selective Mixup training as baselines, the hyperparameter of the Beta distribution $B(\alpha, \alpha)$ is not carefully determined, or at least the choosing process is not fully explained. If the Mixup baselines failed to reach their optimal states, then the comparison of them with other methods like resampling and selective sampling w/o Mixup might be unqualified.

3. The testing metric only considers the worst-group performance, but not including the overall performance. Normally practitioners are concerned not only with the short board of the bucket, but also all the boards as an entirety (a trade-off between fairness and overall generalization). 



Limitations:
1. The paper is fundamentally based on experimental investigation, but with little theoretical support. 

2. The datasets used doesn't include some of the most popular or commonly used ones like CIFAR or Imagenet. 

3. In the experiments, Mixup (and selective Mixup) are taken as baselines, but the value of $\alpha$ (which supervises the Beta distribution) may not have been carefully determined.

4. I personally imagine that the datasets size and\or the batch size will also have an impact on the performance of selective Mixup and selective sampling. There might be some room for further investigations in the future.

5. What roles (either positive or even negative) the Mixup operation itself plays in selective Mixup exactly have yet to be fully understood.

6. The metric is limited at the worst-group performance with no information on how the models perform in general on all the testing data.



Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper performs an analysis of selective mixup techniques, coming to the conclusion that some of the improvements these techniques provide can be obtained by the sampling procedure and not the mixing strategy. Based on some of the insights, variants of selective mixup are sometimes proposed to fit specific settings and shown to work better.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
- The considered datasets and settings are varied and extensive
- The paper proposes plausible explanations of the improved performance of selective mixup variants, and performs extensive ablations and creates experimental settings to test these explanations
- The observations about the roles of re-sampling and regression to the mean are extremely interesting.
- One of the main interesting points of the paper is carefully studying the forms of shift occurring between the training and the test distributions, and studying how specific forms of selective mixup impact them. This methodology can improve the practitioner's understanding of these techniques and how to improve them (e.g. as shown in some of the cases considered).


Weaknesses:
- As the authors acknowledge, their analysis is limited to the original mixing strategy.   
- The formatting of the paper is a bit bizarre, with plenty of extremely short paragraphs, misplaced image captions etc. I would recommend the authors to fix this issue, as it makes their work look unprofessional. 

Limitations:
Adequately Addressed

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presented an insight that selective mixup (different class or different domain) is actually equivalent to resampling. A simple mathematical proof is presented showing that the mixed up distribution is closer to uniform distribution in terms of label distribution. Extensive experimental results are presented to demonstrate the findings. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* novel understanding on selective mixup
* extensive experiments to demonstrate the understanding 


Weaknesses:
The finding is nice but seems value is limited, at least for two-class problems, selective mixup obviously leads to uniform distribution. This paper is more about empirical demonstration.

Limitations:
not new algorithm, but understanding existing algorithm, the value of the finding seems limited. 

Rating:
4

Confidence:
4

";0
O1lYncfVOO;"REVIEW 
Summary:
The paper presents S-CLIP, a novel semi-supervised learning method for training contrastive language-image pre-training (CLIP) models in specialized domains where limited image-text pairs are available. Vision-language models like CLIP have achieved impressive results in natural image domains, but they struggle when applied to specialized domains like remote sensing due to the scarcity of paired data. To address this, S-CLIP leverages additional unpaired images and introduces two pseudo-labeling strategies: caption-level and keyword-level pseudo-labeling.

Caption-level pseudo-labeling assigns pseudo-labels to unlabeled images based on the captions of paired images. This is achieved by solving an optimal transport problem between the unlabeled and labeled images, ensuring that the pseudo-labels represent a probability distribution over the labeled images. On the other hand, keyword-level pseudo-labeling assigns pseudo-labels based on the keywords in the captions of visually similar paired images. It formulates the training as a partial label learning problem, considering a candidate set of target keywords instead of a single exact one.

The experiments conducted in various specialist domains, including remote sensing, fashion, scientific figures, and comics, demonstrate the effectiveness of S-CLIP. It outperforms CLIP fine-tuning and other semi-supervised learning competitors in zero-shot classification and image-text retrieval tasks. For example, in the remote sensing domain, S-CLIP improves zero-shot accuracy by 10.4% and image-to-text retrieval R@5 by 4.4% compared to CLIP fine-tuning. Moreover, S-CLIP remains robust even when the unlabeled images are from a different dataset. Overall, S-CLIP shows promise in enhancing CLIP training using only a few image-text pairs.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- Effective Pseudo-Labeling Strategies: The paper proposes two pseudo-labeling strategies, caption-level and keyword-level pseudo-labeling, which effectively guide the training process. The caption-level pseudo-labeling leverages optimal transport to assign pseudo-labels based on the captions of paired images, ensuring robust training even with distribution shifts. The keyword-level pseudo-labeling assigns pseudo-labels based on keywords in visually similar paired images, capturing local components of unlabeled images. The combination of these strategies results in improved zero-shot classification and image-text retrieval tasks.

- Experimental Demonstrations: The paper provides extensive experimental evaluations in various specialist domains, including remote sensing, fashion, scientific figures, and comics. The results consistently show that S-CLIP outperforms CLIP fine-tuning and other semi-supervised learning competitors, achieving significant improvements in zero-shot classification and image-text retrieval tasks. The robustness of S-CLIP, even when unlabeled images are from a different dataset, further strengthens its effectiveness.

- Relevance and Applicability: The paper addresses an important and practical problem in the field of vision-language models. The proposed S-CLIP method has the potential to enhance the training of CLIP models in specialized domains with limited paired data. The findings are relevant not only for researchers working on vision-language models but also for practitioners in various domains where image-text understanding is crucial.

- Clarity and Presentation: The paper is well-written, organized, and presents the proposed method and experimental results clearly. The figures and illustrations aid in understanding the concepts and methodologies. The strengths and weaknesses of the proposed method are discussed in a concise and informative manner.

Weaknesses:
- Baselines with other semi supervised approaches are severely limited with the added baselines being severely outdated. Several much newer few shot learning approaches are present and should be compared against. A summary could be found in the paper: A Survey on Deep Semi-supervised Learning by Yang et al. for example: SEMI-MAE: MASKED AUTOENCODERS FOR SEMI-SUPERVISED
VISION TRANSFORMERS by Yu et al or Semi-supervised Multimodal Representation Learning through a Global Workspace by Devillers et al
- A lot of the core content like limitations, experimental setup etc has been moved to the appendix which I feel is key to the understanding of the paper and unfairly grants the author extra writing space
- The approach combines keyword and caption level pseudo labelling, both of which have individually been well studied in literature. The combination of both is certainly a novel effort, but the contribution of the same doesnt seem like a strong enough novelty for a venue liek Neurips
- The choice of using OT as the distance measure between 2 images is interesting, it would be nice to see how other image distance measures compare to it

Limitations:
No ethical limitations as such

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies semi-supervised learning for contrastive vision-language pre-training, during which unlabelled/unpaired data is also accessible. The authors formulate the pseudo-labeling as an optimal transport (OT) problem and propose to use the Sinkhore algorithm to produce pseudo labels, which is validated to be superior to vanilla had/soft PL solution. Comparisons with other works in literature utilizing OT are properly discussed. Furthermore, the paper proposes to use the key-level pseudo label to account for partial label learning. Experiments on several data domains are conducted to validate its effectiveness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is technically sound and easy to follow.
- The evaluation is self-sufficient with proper baselines, which helps to corroborate the effectiveness of the proposed loss.
- The ablative study is sufficient and showcases the impact of different composing parts of the method.
- Consistent performance gain over a variety of downstream datasets.

Weaknesses:
- For keyword-level pseudo-label, the compared baseline is somewhat not fair, since one could also leverage the keyword information for labeled/paired data in the vanilla fine-tuning setting, i.e., fine-tuning pre-trained CLIP with CLIP loss and multi-class classification loss. Additional experiments should be conducted to verify the gain derives from the semi-supervised part of it instead of simply leveraging the label information.
- It would also be nice to showcase results in general data (e.g., the ones evaluated in the CLIP paper, Flicker, etc.). Manual data split may be necessary but it helps to corroborate the generalizability of the proposed method.



Limitations:
All limitations are properly addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces an additional training method for vision-language models like contrastive language-image pre-training (CLIP). The proposed method consists of semi-supervised learning with two different pseudo labeling including caption- and keyword-level pserudo-label. These two pseudo labelings are complementarily understand image through different types of text representation. The experimental section describes the effectiveness of the proposed method in terms of zero-shot image recognition and image-text retrieval in several representative datasets. A relatively small image-text paired dataset can improve the zero-shot recognition and image-text retrieval with the proposed pseudo labelings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ The paper serves a method for semi-supervised learning on image-text paired datasets. Usually, it is quite difficult to construct the kind of dataset, however, the paper proposed that a simple yet effective idea to solve a problem in vision language pre-training like CLIP. 

+ This paper also points out that limited focused areas such as remote sensing and fashion cannot be effectively used a vision-language models like CLIP as is. Therefore, the paper insists that an easily adaptive labeling approach and semi-supervised fine-tuning from pseudo labels are important for CLIP to adapt a specific image dataset.

+ In the experimental section, the proposed method S-CLIP is much better than that of used the original CLIP and fine-tuned CLIP (CLIP-FT). It is reasonable to assign soft labels rather than hard labels (Hard-PL) in this problem. 

+ The paper is well-written and effectively communicates the main ideas in the paper, making it accessible to a broad audience. Moreover, there contains various visualizations, e.g., Figures 1 and 2, to confirm that the learnt representations of vision-language pre-training are meaningful. 

Weaknesses:
- CLIP is pre-trained on image-text paired dataset widely extracted from the major space on the Internet. Therefore, it is understandable that the dataset collection does not work well in situations where only a small amount of paired data is collected, or where the paired data is not steadily available on the web. On the other hand, does the proposed dataset collection method only work on remote sensing, fashion, scientific figures, and comics? If you could show the vision-language application based on the proposed dataset collection in more situations, the paper must be more effective to broader readers. Probably, the approach could be shown in more generic situations, however, there are only 4 types of situations. Though the authors do not necessarilly add the experimental results, how about discussing the effectiveness for general purpose dataset collection?

Limitations:
There are no negative limitations and societal impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
Fine-tuning CLIP in specialized domains suffer the problem of limited available image-text pairs. This paper propose to address this challenge via semi-supervised learning methods with the additional unpaired images. Two pseudo-labeling strategies including the caption-level pseudo labeling and keyword-level pseudo labeling are proposed to construct valuable information about the unpaired images.
The problem addressed in the paper is meaningful and the method proposed in the article is demonstrated effective through comprehensive experiments.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
(+) This article is well written, and the description of the method pipeline is clear and easy to understand. 

(+) Experiments are sufficient.

(+) The problem it solves is meaningful.

Weaknesses:
(-) ""Baseline"" in table 5 is a little confusing. See question (1).

(-) The effectiveness of OT-based ""Caption-level pseudo-label"" compared with soft-PL should be further verified.



Limitations:
No

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a method S-CLIP, to enhance CLIP in a semi-supervised way by leveraging captions (via optimal transport) and keywords (via image-keyword similarity) from other unpaired images in the same batch. This is beneficial for specialized domains like remote sensing, where image-text data is usually limited. Results show better performance than the traditional pseudo-labelling methods, especially in OOD domains.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Addressing low-data regimes for contrastive learning. Such a problem is common in specialized domains like remote sensing.

Novel and simple method, but showing promising results compared to the standard pseudo-labelling methods, especially in OOD domains.

Experiments and ablations are comprehensive.

Well written and easy to read.


Weaknesses:
In section 5 experiments, missing experiment details for reproducing, such as model size, epoch, training schedule (lr, weight decay etc.)...

Besides the traditional pseudo-labelling methods, it would be nicer if we could compare with other SOTA pseudo-labelling methods for vision-language training – if there were no such SOTA methods, we could claim that in the paper.


Limitations:
No specific concern.

Rating:
6

Confidence:
3

";1
3S9Oiu6gMf;"REVIEW 
Summary:
The paper describes an approach for learning bounded in-degree polytrees that a family of Bayesian networks. More precisely, given the skeleton of the polytree $P$ from which the samples are from, their algorithm learns a $d$-polytree whose distribution is likely to be close to $P$ (with respect to KL divergence) using mutual information tests. Importantly, the algorithm runs in polynomial time for a fixed $d$, whereas the exact learning problem is known to be NP-hard for $d > 1$.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
To my knowledge, the theoretical results are novel and show that even though the optimal $d$-polytrees are hard to learn exactly, it can be done approximatively.

Weaknesses:
My main concern is in the relevance of the article to AI community, i.e., it has nice theoretical results, but their practicality remains unclear to me (see Questions section of this review). I would be happy to increase my score if the authors can offer convincing arguments for this.

I also recommend carefully proofreading the paper to improve its presentation. To mention some of the minor issues:

- 139: ""We denote $\pi(v)$ to denote""
- 143: The definition of deg-l v-structure should probably include the lack of edges between $u_i$ and $u_j$? Of course, that holds implicitly for forests.
- 143: ""We say that -- is said to be""
- 153: Meek [1995] -> [Meek, 1995]
- 186: has -> have

Limitations:
See Questions.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the number of samples to learn a particular class of distributions: bounded-degree polytrees (Bayesian networks whose skeleton is a forest). Recent work has shown that tree-structured Bayesian networks (1-polytrees) are learnable with finite samples; this work makes progress on the natural generalization to polytrees, showing a positive result when the skeleton is given. The work also provides some conditions under which the skeleton is learnable, and a lower bound for the number of samples required.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Learning a distribution approximately from finite samples is one of the most fundamental tasks in learning theory. This study of the finite-sample learnability of polytrees is a very natural step for building our understanding of this problem, particularly in the context of the recent work showing learnability for tree-structured models.

The main result of Theorem 1 (finite-sample learnability of degree-bounded polytrees given the skeleton) is quite fundamental. The algorithm and proof are generally quite natural, and furthermore they help demonstrate the clean manner in which the mutual information tester machinery of [Bhattacharyya et al., 2021] can be leveraged for such results. While accompanying results in Section 4 (Skeleton assumption) and Section 5 (Lower bounds) are less surprising, their presence adds more completeness to the general picture.

The paper is generally well-written.

Weaknesses:
More motivation for studying polytrees might be appreciated by the general NeurIPS community. Regardless, Bayesian networks are well-motivated and polytrees are a natural continuation of the aforementioned recent work.

The assumption of being given the skeleton is perhaps the most unsatisfying aspect of these results. For context, my understanding is that when learning tree-structured models (as is the focus of the main prior work of [Bhattacharyya et al., 2021]), the entire task is determining the skeleton, as any rooting of the tree is equivalent. In this sense, it is somewhat disappointing that the entire task of the main prior work needs to be given to the polytree learning algorithm. It would be nice to know whether this assumption is inherently required or just an artifact of the current algorithm.

Limitations:
The limitations are addressed fairly in the paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces an efficient learning algorithm for bounded degree polytrees and establishes finite-sample guarantees. Explicit sample complexity and polynomial time complexity are provided. An information-theoretic lower bound is provided, which shows that the sample complexity of the algorithm is nearly tight.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides a novel algorithm for learning d-polytrees with general d, extending a previous algorithm for d=1. The theoretical analysis shows that the algorithm is nearly tight in terms of sample complexity. The results do not require distributional assumptions such as strong faithfulness. The ideas and results are clearly presented in the paper.

Weaknesses:
The recovery of the true skeleton relies on Assumption 11. It would be nice if some comments on this assumption could be given (e.g. whether it is expected to be tight)

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper gives an efficient PAC-learning algorithm for learning graphical models called ""bounded polytrees"". These are distributions where 1) the undirected skeleton of the graph is a forest and 2) the in-degree of every node is bounded by some constant $d$. This extends a recent result [1] for directed trees, which is corresponding to the case $d=1$.

In contrast to [1], the paper gives a learning algorithm assuming that the skeleton is given. To achieve that, the estimator of conditional mutual information from [1] is extensively used. This estimator is used in a sequence of clever greedy-like checks in order to orient as many edges as possible. After orienting the remaining edges, it is shown that the resulting distribution must have small KL divergence to the true distribution.

A sufficient condition is also given, under which the skeleton can be learned for certain distributions by the Chow-Liu algorithm (so it does not have to be given to the algorithm). Finally, a lower bound on sample complexity is proved, roughly matching the upper bound of the algorithm in the case of binary alphabet.

[1] Bhattacharyya, Gayen, Price, Vinodchandran, ""Near-optimal learning of tree-structured distributions by Chow-Liu"", STOC 2021.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The studied problem of efficient learning of graphical models is important and interesting.

* The paper considers distributions with a tree skeleton and arbitrary orientation of edges as opposed to just directed trees. This is a natural and long-studied class of distributions.

* Even given the estimator from [1], the algorithm and proofs are interesting and not trivial.

* Section 3 gives a good outline of the algorithm and its correctness proof and the figures were helpful to me.

Weaknesses:
* The algorithm requires the skeleton as input, which I think is a significant limitation. It is not clear how useful is the sufficient condition proposed by the authors in order to remove this limitation.

* The writing could be clearer. Especially the steps which I assume are more standard/obvious to the authors felt rushed. In my opinion, a few places could be rewritten in order to be clearer and more self-contained.

Limitations:
see above

Rating:
6

Confidence:
2

";0
BbIxB4xnbq;"REVIEW 
Summary:
This paper presents a method for stress-testing visual classifiers. The key idea is to use language to generate counterfactual images. Their approach can be summarized into four steps: 1) take an image input; 2) get its caption from a captioning model (BLIP-2); 3) perturb the caption by changing some words using ChatGPT and fine-tuned LLAMA; 4) generate new images based on perturbed captions using text inversion of Stable Diffusion. On the ImageNet dataset, they showed that various models such as ResNet-50 and ViT-B consistently achieve much lower performance on their generated new ImageNet compared to the original ImageNet, and the generated ImageNet enables surfacing model biases.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper targets a novel direction --- stress-test visual models, which is important in real-world vision applications.

2. The proposed method is technical sound, and the generated images from their pipeline look good and realistic.

3. The paper conducted experiments on the realistic ImageNet dataset instead of simple synthetic datasets, showing the practical values of their method in realistic scenarios.

4. This paper is generally well-written and easy to understand.

Weaknesses:
1. There is no baseline to compare, and the numbers are hard to justify their method's effectiveness. The main quantitive results from this paper showed that various vision models achieve much lower performance on their generated ImageNet dataset. However, it is unclear whether the generated images are 100% correct. If the text perturbation happens to change the class or introduce new classes to the image, the model prediction is expected to change and the performance will drop. Therefore, a human study showing the label validity of generated images is particularly necessary.

2. There are some existing works proposing very similar directions. For example, [1] is almost the same as this work, which uses textual reversion to convert an image into a token and compose this token with natural language and generate new images to test vision models. [2] uses off-the-shelf image captioning and generation models to discover model bugs. While [1] can be viewed as concurrent work, it would be helpful if the authors could discuss the similarity and differences between this work and [1-2].

3. From Figure 6 last row, changing the caption from ""car"" to ""bus"" generates almost the same images, but model predictions change a lot. The change may be because generated images have specific patterns that are not perceptible to humans, which cannot reflect the actual errors using real images. 

4. The stress test will be limited by language itself, as language is hard to represent certain properties of objects, such as orientation and lighting. However, this is also pointed out by the authors and is minor considering the advantage of using language.

[1] Dataset interfaces: Diagnosing model failures using controllable counterfactual generation. https://arxiv.org/pdf/2302.07865.

[2] Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning. https://arxiv.org/pdf/2208.08831.

Limitations:
See weaknesses. I'm happy to increase my rating if these are well addressed, especially 1 and 2.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a way to generate new data to evaluated the robustness of model. It first captions an image, then changes part of it, then generates an image from the modified caption. The model is then evaluated on these new images to see how robust they are to things like background changes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea is interesting and is an easy way to generate new evaluation data. The paper is well written and easy to follow. The approach would be a valuable tool to evaluate models.

Weaknesses:
Since it is an automated benchmark, it is unclear how good the quality is. It relies on a lot of parts working right (captioning model, caption changing model and image generation model).  It would be good to have some human evaluation in Table 2 to see how well humans do on this set.

Limitations:
Yes.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposed a new testing protocol LANCE for existing vision models.
It adopts LLM-based caption perturbation and DDPM based image editor to generate counterfactual samples. 
Specifically, for each image, LANCE first use a pre-trained captioner (BLIP-2) to generate the corresponding caption. 
Then it adopts an LLM to perturb the caption with different factors (subject, object, background, etc.).
The goal of perturbation is to generate a caption corresponding to a counterfactual image with semantics preserved.
With the perturbed caption, the author generates another image with counterfactual aspects and similar looking with the original image.
The authors observed a significant performance drop when testing image classification models on these generated images.
Furthermore, the authors also present a detailed analysis based on the perturbation factors.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The authors establish an interesting pipeline to generate counterfactual hard examples for testing vision models. It involve an LLM-based image caption perturbator and a ddpm-based image editor. Qualitative results show that this pipeline can generate some impressive counterfactual images. 
2. The authors present a detailed analysis on the performance degradation when testing on the generated counterfactual images. 

Weaknesses:
1. The authors should present more quantitative statistics about the counterfactual examples generation pipeline, for example:
    1. What is the success rate of an LLM to generate a plausible perturbed caption (GPT-3.5 & Finetuned LLAMA-7B). Also, I noticed that you are using Nvidia A40 (48GB), thus a comparison between different LLAMA variants (+13B, e.g.) are also welcomed. 
    2. What is the success rate of DDPM-based image editing? I'm specially interested in that since I observe poor quality when using StableDiffusion to generate common images (compared to other advanced AIGC models).
2. Are all newly generated images (781 images, according to the supp) verified manually by human? If so, how many worker-hours are required to generate these samples?

Limitations:
See Weakness.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper proposes to use recent advances in text-to-image generation/editing to design evaluation by generating ""counterfactual"" data. The main idea of the method is to make semantically irrelevant (to the output class) edits to an image and observe its effects on the model predictions. A variety of models including CNNs, and ViT (trained classically as well as via language supervision) are tested, which show that such editing operation can indeed make a ""hard"" test set where most tested methods struggle to do well.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
[S1] Great Presentation Clarity: The paper is very well written and the details about what has gone into the final data-generation scheme are presented in sufficient detail. The experimental setup, the algorithm, and the implementation of the baseline models are clearly explained and should be easily reproducible independently.


[S2] Good evaluation/results: The proposed model, based on the experiments presented, show a huge potential for using counterfactual generation of images to stress-test models. While the paper only makes a handful of edit types, there is a possibility to design hyper-specific probes (does change clothing color change models predictions about a person's profession?) that can be designed for specific applications.

Weaknesses:
[W1] Need more discussion of scope and their limitations: While the main idea is sound and the descriptions are clear, the evaluation/discussion leaves a lot to be desired. Most of the evaluation is done ""end-to-end"" which does not give much insight into the success of the various stages involved in the process. Some example concerns that I have:

- The success of checks and balances (L155 and L185) is not verified/discussed. Do they always work? When do they fail? What are the effects of their failing?
- What is the diagnostic prowess of this method? Besides showing that the edit-based perturbations does indeed make models flip their predictions (which has also been previously shown via adverserial noise injection), what insights can we draw from this type of stress testing? Does the results of this type of probe correlate with other tests (perhaps for bias/fairness and/or other diagnostic datasets? Do the edits represent (in the paper's view), the realm of possible naturally occurring variations in background/unrelated content in the picture? If not, can this still diagnose/predict the model's robustness in the real world? Or, is this just another form of discovering ""adversarial attack"" on the model predictions?

There is only a very short limitation section at the end of the conclusion section. I would have liked to see a more open discussion about exactly what (in view of the paper) can this method tell us about a model and what it cannot. 


[W2] No manual/external/independent validation: The quality of the edits, and the accuracy of the labels post-edit are not validated by any external/manual evaluation. We only see decreased overall accuracies for popular methods. While a simple baseline of random edit is tested, it is not clear whether a noise perturbation/other simpler perturbation techniques could also result in similar loss. Regardless of the loss in accuracy produced, it is also important to know whether the editing process leaves any unwanted artifacts. E.g., In the headline image, there is no actual sled visible, and the prediction of it likely comes from context. As such, it is unsurprising that changing the dog breed to pomeranian (an unlikely breed to be pulling sleds, also both ""husky"" and ""pomeranian"" are actual categories by themselves) would result in flipped predictions. To be clear, I am not claiming that this is ""OKAY"" behavior, just that perhaps the mechanism is ""label noise"" in Imagenet rather than ""context bias"" (Since there is nothing but the context that can help us make predictions). I am claiming, however, that this is a different category of error than other examples in the paper. Both are important, and the proposed method is likely to help uncover both, but at a lack of external validation about the accuracy of the labels before and after the edit operation. Some questions that I'd have liked validated externally:
- Can the accuracy of the ""perturber"" be measured independently?
- What is the accuracy of the ground truth labels post-edit? Are they truly unchanged? Are the methods presented in L185 and L155 (re. checks and balances) enough? Do they work? Have they avoided this issue successfully?
- Since there is a pervasive error in Imagenet (https://arxiv.org/abs/2103.14749), does this method have any correlation with the noisy labels? In other words, is this intervention making ""easy to predict"" examples harder or primarily just the ""confusing"" ones?

Limitations:
Please see the weaknesses above

Rating:
5

Confidence:
4

";1
9HJyRsgU13;"REVIEW 
Summary:
This proposes a novel surrogate for Bayesian optimization (BO), kernelized tensor factorization (BKTF). The authors claim that BKTF is able to model more complex functions (nonstationary, nonseparable) compared to additive and product kernel Gaussian processes. For inference, they leverage Gibbs sampling to do full Bayesian inference. They compare against BO with the regular Gaussian process surrogate and tree-structured Parzen estimators.

Soundness:
1

Presentation:
2

Contribution:
3

Strengths:
* The papers proposes a novel surrogate model, BKTF, for BO. I believe this is new, and as long as the authors can demonstrate the utility of BKTF, this will be a valuable contribution to the BO community.

Weaknesses:
* The proposed strategy uses Gibbs sampling, which is well known scale poorly with the number of parameters and correlations. Thus, I am concerned that this method's performance will fair poorly at even moderately higher dimensions and number of observations than those considered in this paper.
* On a similar note, unless I'm not mistaken, the method requires to infer the latent functions (or bases) $g_d^D$. This means inference needs to be performed at every BO steps. This contrast to GPs, where, even if one decides to do fully Bayesian inference, he does not to run MCMC at every step. Thus, the method comes with a reduction in flexibility. If the authors believe that their method can work with less expensive inference strategies, say, VI or MAP, then this should be demonstrated and evaluated.
* The paper claims that the experiments are ""extensive"" (line 73), but unfortunately, I find that the experiments conducted in this paper cannot be considered to be extensive in today's standards. See for example [1,2], which I would consider extensive. Furthermore, at this small scale / low budget applications, noise can very easily swamp the effects. Therefore, I would expect a lot more runs. Moreover, the hyperparameter tuning experiments in Section 5.2 are not reflective of real-world use cases. So these are gain inadequate to evaluate the real world performance of BKTF.
* Furthermore, the baselines are not enough. The research space for alternatives to BO surrogates has certainly been active, but here only the tree-structured Parzen estimator is considered. In fact, the paper mentions that BKTF here corresponds to a two-layer deep GP. Then, they should compare against deep GPs for an apple-to-apple comparison. The computational costs/scalability of DGPs would probably be comparable so this would be a more appropriate comparison.

Limitations:
Yes, in Section 6. However, I think the limitations I've discussed above could also be included.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Bayesian optimisation most commonly uses Gaussian Processes with the Squared exponential or Matern kernel as the surrogate model. The authors propose a new type of surrogate model, ""Bayesian Kernelized Tensor Factorization"" which introduces some advantages and disadvantages over Gaussian Processes. There seems to be prior work investigating these models for surrogate modding in general, and this paper is a followup applying these models within Bayesian optimisation in particular.

The papers introduces the model which models the data $\{x_i, y_i}$ from a black box function $y=f(x)$ as a sum of functions where each function is a product of 1 dimensional GPs, e.g. in 2D, leteach $g()$ be a 1D GP then 
$$
\hat{f}(x_1, x_2) = \sum_i g^i_1(x_1)g^i_2(x_2)
$$
which is a continuous analogue of how a matrix can be represented by it's SVD or eigen decomposition. This concept generalizes for multiple input dimensions (e.g. $g^i_1(x_1)g^i_2(x_2)g^i_3(x_3)g^i_4(x_4).....$) and the authors discretize the search space into a grid hence the implementation uses tensors.
 and it positive properties,
- to be able to model function with separability (where variables do not interact like in additive kernels) and
- non-stationarity.

In my interpretation, the thesis of the paper is that these properties are significant disdvantes and using a model that has these properties enables a performance improvement.

The disadvantage of the proposed model is that inference is no longer in closed form (a product of Gaussian random variables is not another Gaussian) hence an MCMC method is proposed to sample function values at points across the input space. While one could use a random discretization, (e.g. a latin hypercube, or a cluster around the current best point)  given the product structure of the surrogate model, there appear to be implementation benefits using tensor and matrix Kronecker products if the discretization is a fixed grid, discretize each dimension and build a a Cartesian product of each dimension to have a full grid.

A range of synthetic and hyperparameter tuning benchmarks show the new model performing favourably with standard GP using SE-ARD kernel.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- in theory, I really like the idea of the model, in particular, that any matrix can be decomposed by SVD, hints that any function can be decomposed into a sum and product over functions of each dimension, i.e. the proposed model is, in theory, a universal approximator? Although intuitively, both BKTF and SE-ARD can model any smooth non-stationary surface, discontinuities and kinks are not modellable.

- the inclusion of grid based GP methods is nice to see, and shows how much the grid decays performance compared to using the full unrestricted continuous space

Weaknesses:
# Technical
- the proposed Cartesian discretization, $S_D$, scales exponentially with input dimension, and presumably contains _a lot_ of useless points in empty parts of the search space would a random discretization (LHC or Gaussian around current best $x$) be so much worse? Given a random set of points $X_D$, it is trivial to compute a the joint prior density $P[f(X_D)]$ density and the likelihood is just Gaussian $P[y_i|f(x_i)]$, sampling function values can be done with any off-the-shelf MCMC method.

- I believe at least an additive GP should be a baseline. If non-stationary and non-separability are the main advantages of the BKTF model, presumably an additive GP with 2 kernels per dimension (matching CP rank=2 for BKTF) is an obvious baseline that has separability, such a baseline is exactly equation (7) but with sum-sum instead of sum-product. From this perspective, BKTF is simply an additive GP (that can only model separable variables) with a product over dimensions instead of a sum and this one change introduces a lot of engineering overhead (MCMC inference vs closed form inference) but also introduces more modelling power (separability can be modelled), given a high enough CP rank (CP rank =1 is just a product of 1D funs and is not separable).


- the related work consists of two paragraphs, the first is discusses prior work on  BKTF (and feels a bit repetitive), the second focuses on stochastic process models. I feel the novelty of this paper is in using another surrogate model inside BO methods, and given the large body of BO work, there have been many works acknowledginbg the limitations of SE-ARD and proposing alternative models that are not cited or empirically compared to
  - [Bayesian Neural networks](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+neural+networks&btnG=): 
    - Bayesian optimization with robust Bayesian neural networks, NeurIPS 2016
    - Scalable bayesian optimization using deep neural networks, ICML 2015
    - Multi-fidelity Bayesian optimization via deep neural networks: NeurIPS 2020
  - [Deep GPs](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+deep+gaussian+&btnG=)
    - Bayesian optimization using deep Gaussian processes, Arxiv

# Presentation

(in my personal subjective view) some changes to presentation would have made the paper far more accessible to me
- can ""CANDECOMP/PARAFAC"" be simply described as a tensor generalization of SVD to make it easier for readers?
- L119: ""we construct a D-dimensinoal Cartesian product Space"", can we just say ""grid"" like the authors do for the rest of the paper?
- L81: kronecker product is introduced and never used again in the main paper
- Section 3.1 would be much easier for me to understand if Eq (7) and (8) are introduced first, then next Section 3.3 (model inference) describes the grid and Equation (6) and MCMC details and the justification for the grid.
- Section 3.2 is nice to mention but for me distracts from the main paper hence would be much better suited to the appendix.
- L192: given a mean and uncertainty, this seems to be standard UCB, why is ""Bayesian-UCB"" defined?

Limitations:
- as above mentioned comment, discretizations in higher dimensions are generally considered bad practice, in particular, ungioded/naive grid discretizations that include many dead points.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents a new surrogate model called Bayesian Kernelized Tensor Factorization (BKTF) for Bayesian Optimization (BO).The BKTF model approximates the solid in the D-dimensional space using a fully Bayesian low-rank tensor CP decomposition. It uses Gaussian process (GP) priors on the latent basis functions for each dimension to capture local consistency and smoothness. This formulation allows sharing of information not only among neighboring samples but also across dimensions. The paper proposes using Markov chain Monte Carlo (MCMC) to efficiently approximate the posterior distribution. ). The paper demonstrates the effectiveness of BKTF through numerical experiments on standard BO test functions and machine learning hyperparameter tuning problems. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. One of the significant strengths of the paper is the novel and reasonable solution of incorporating the idea of tensor decomposition into Bayesian Optimization (BO). This approach allows for a more efficient and effective representation of the D-dimensional Cartesian product space, enhancing the performance of BO. The adoption of tensor decomposition represents a significant advancement in the field and demonstrates the authors' innovative thinking.

2. The usage of two-layer GPs is impressive. This approach is clever as it allows for the sharing of information among neighboring samples and across dimensions, enhancing the model's ability to capture local consistency and smoothness.  



Weaknesses:
1. The cost of several cascaded full GPs may be high, especially for cases with large nodes (refer to ""coordinate"" in paper) at some dimension. More discussions are encouraged on the scalability analysis or the possible solutions, such as sparse GP,  to reduce the cost. 
 
2. As the tensor rank R  is always a crucial hyperparameter for tensor decomposition. I'm curious about how the rank setting could influence the BO. It will be great if the authors could give some comments or results on why R=2 is sufficient for the model setting.   

Limitations:
See Weakness

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents a surrogate based on tensor decompositions for approximating complex functions, allowing for Bayesian-style maximization.
Numerical experiments show the (slight) superiority of this model over classical Bayesian approaches. However, a limitation of this approach is the small dimensionality of the target functions and the need to use a discrete grid.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

- The proposed algorithm uses a very small budget to find the maximum of complex functions (gradient-free, multimodal).

- A good potential for expanding and improving the proposed algorithm.

- This article uses a tensor approach for machine learning problems

- The presented new algorithm, in my opinion, has quite a lot of possibilities for improvement, and the article itself is complete.

Weaknesses:
- A small number of numerical examples.

- Final accuracy in Fig. 3b better, but very close to the accuracy of the other methods with which the comparison is made.

- No comparison with non-Bayesian methods of finding the maximum.

Limitations:
Small dimension of functions for which the maximum is searched for. This is due to the fact that AF has to be found by unrolling the tensor from CP to the full format.
Thus, one of the main advantages of the CP tensor format, related to overcoming the curse of dimensionality, is not used.




Rating:
7

Confidence:
4

";0
Cu3T82cegI;"REVIEW 
Summary:
Even though there have been a lot of studies on learning in noisy settings, they are often disparate, and the authors aim to present a unified view of a diverse class of corruptions and their implications on learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper offers an interesting direction of looking at existing learning formulations in a noisy setting. It attempts to unify a more extensive class in a single umbrella and then analyzes the different settings through the unified framework.
- They propose a new taxonomy of corruption in classification sitting and attempt to link the existing works to the taxonomy.
- The authors also derive a loss correction formula, which would help infer the consequences of different types of corruption on learning.

Weaknesses:
I encourage the authors to discuss the practical implications of the paper's theory, which would strengthen the submission.
- It would also be helpful to understand how much insight their work provides compared to the corresponding works for each problem. For instance, what advantages does unifying the noises & studying them offer as opposed to studying them in separation?
- One of the issues is how realistic the assumption is that we know the process, which adds to the corruption. Does the Markov kernel capture all the classes of often encountered corruptions (or what assumptions does it impose on each class of noises)?
- In practice, the models often pick up spurious correlations in the data. Does that also fit into the taxonomy considered? How does the bias problem (regarding the fairness of predictors) fit into the framework (even if that can be considered a noise-generating process)?

Limitations:
The authors do talk about limitations, but it isn't complete. I encourage the authors to refer to the questions & address them in the list of limitations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper presents a theoretical framework for analyzing corruptions in machine learning using Markov kernels. Within this framework, the authors establish a taxonomy of various types of corruptions and provide data processing equalities for each category. Additionally, the paper derives corrected loss functions for different types of corruptions within their theoretical framework.

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
The authors categorize corruptions in a rigorous and systematic manner. This classification and the theoretical view from Markov kernels can potentially help future research future research in understanding and studying different types of corruptions. The derived corrected loss can potentially be used to improve performance of machine learning algorithms in the presence of corruptions.

Weaknesses:
Overall, I find it hard to appreciate the contribution of the paper. It lacks clear intuition and practical implications, making it difficult to understand its usefulness. Theorems are presented without concrete insights, and it is unclear how the results can guide real-world practice.
1. In Section 4, the paper discusses the ""consequences"" of corruption in supervised learning. However, there is a lack of concrete interpretations of these consequences. How do Theorems 3, 4, 5 translate to real-world machine learning scenarios? What can we deduce about the nature of these corruptions and their implications? 
2. The paper introduces a corrected loss function, but it is not well-explained how it operates or why it is effective. What is the underlying intuition behind this correction and how does it improve the performance under label noise?
3. It would be beneficial if the authors could provide concrete examples to illustrate how the corrected loss function can be implemented. For instance, what would be the formula of the loss in the context of cross-entropy loss under label noise? Right now it is even unclear whether these losses are feasible and whether they require any information that is not observable.
4. It would also be valuable to include experiments that demonstrate the effectiveness of the derived corrected loss function.

Limitations:
The authors have adequately discussed some limitations in section 6. For other limitations, see Weaknesses.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This study addresses the prevalent issue of data corruption in machine learning and the lack of a unified understanding of how different corruption models interrelate. The authors analyze corruption models at the distribution level using a comprehensive framework based on Markov kernels, providing a fresh perspective on the issue. They draw attention to joint and dependent corruptions on both labels and attributes, which existing research often overlooks. By examining the effects of these corruptions on supervised learning through Bayes Risk changes, the authors gain insights into the implications of complex corruptions. The proposed framework also has applications in corruption-corrected learning, which the authors explore in this paper. However, the paper does not have any empirical contributions, and has several theoretical discussions that need further clarifications. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper delves into the crucial problem of data corruption in machine learning, offering a comprehensive framework based on Markov kernels to analyze corruption models at the distribution level.

2. It paves a way for future quantitative comparisons by delivering a foundational understanding of more complex corruptions' consequences.

3. The authors' taxonomy of corruption is comprehensive and organized hierarchically through the notion of dependence, helping to relate existing corruption models.



Weaknesses:
1. the paper does not involve any empirical studies, which is very rare for this venue, although there are precedents. Most theoretical papers still choose to validate their finding with some small-scale experiments, the authors does not seem to clarify a particular reasons that they do not need any empirical evidence to support their findings. 

    - This seems a particular issue when the authors explicitly suggest that the theoretical framework can help answer ""what can we do to ensure unbiased learning from biased data"" (line 228), and mentions two previous works [7] and [34], where [34] has a decent amount of empirical works. 

2. on the theoretical end, there are several questions need further clarifications, potentially paint a significant issue of rigor of the results. (see below questions)

Limitations:
no explicit discussions of limitations, but there are discussions about future directions, which might be interpreted as the limitation discussion. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper presents a framework for systematically analyzing and categorizing corruption models in machine learning. The authors propose a new taxonomy of corruption based on its dependence on the feature and label space, rather than relying on invariance assumptions. They utilize the concept of Markov kernels to provide an exhaustive framework that includes all possible pairwise stochastic corruptions. The authors also derive corruption-corrected loss functions within the framework.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- A novel unification of existing data corruption models: The proposed theoretical model based on the Markov kernel is capable of modeling various existing data corruption and distribution shifts. This offers a new perspective to understand how different data dynamics connect to each other.

- Insights on how to correct the errors: The authors also derive loss correction formulas for a few instances of the unified data corruption model. These results give an abstract way to study data corruption-aware learning.

Weaknesses:
- Unaccessible to a wider community: One major concern with this paper is whether it is accessible to the NeurIPS community. This paper seems to be a pure theory paper: it is symbol-heavy, theory-oriented, and without any empirical support. The theory community (e.g., COLT) might appreciate it more, and I am a bit worried about the NeurIPS community. See my questions in the next section for more details.

- Unclear concrete applications of the proposed theoretical models: It is unclear how useful the unification is. Do its insights give statistically or computationally more efficient algorithms for corruption-aware learning? Or does it implies one corruption instance subsume another and thus focusing on it is enough? Or does it reveal new corruption instances not studied before but important in practice? A good paper, in my opinion, should give useful advice to its readers.

Limitations:
Yes

Rating:
6

Confidence:
3

";0
g2ROKOASiv;"REVIEW 
Summary:
The authors investigate the issue of  manipulation (in the form of falsifying data or model updates) among agents who mutually contribute to a shared model. Incentives for such behaviors arise when agents possess differing objectives with respect to the shared model. The authors first demonstrate that without external intervention, these incentives are essentially unavoidable. However, the authors propose two methods for inducing incentive compatibility in such settings; namely payments when utility is transferable, and noisy server messages  when utility is non-transferable. The authors derive these mechanisms and provide additional theoretical results for two settings of collaborative online learning, single shot mean estimation and multi shot shared gradient updates. Lastly the authors provide experimental results on the FeMNIST data set demonstrating that their mechanisms dissuade strategic behavior.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Distributed learning is a rapidly growing area and there is a real danger that strategic agents could disrupt the efficacy of these systems if their incentives are not properly accounted for. As such, the authors’ work is well motivated and helps to fill an important piece which is currently missing from the literature.

2. Manipulations in these types of settings are often framed as being adversarial. The authors model agents as being strategic rather than purely adversarial. As we have seen in areas like supervised learning the differences between adversarial and strategic agents can be highly consequential in terms of designing robust systems; considering both types of behavior is imperative (the former is already covered in prior work).

3. The need for such mechanisms is well motivated both from a narrative perspective and from a theoretical perspective (Corollary 4.2).

- Considering the case of non-transferable utility increases the applicability of the authors results, and the case of transferable utility provides the system more options to incentivize collaboration when payments are feasible.

4. The authors’ results are constructive, rather than simply existential. For example, rather than saying that there exists a $C$ and $\lambda$ such that players participate honestly, the authors provide specific ranges of these variables for which honest behavior is an equilibrium. This makes it easier for others to implement their methods in real-world scenarios. 

5. While some strong assumptions are made for the theoretical results, such as convexity, the inclusion of the experimental results helps demonstrate that the authors’ approach is effective even when such assumptions do not hold.

6. The paper is well written and the authors take the time to outline the intuition and implications of their results.


Weaknesses:
1.  The mechanisms proposed only induce truthfulness as a Nash Eq, implying that other non-truthful equilibria exist. I understand that these types of results are standard throughout the literature, but when deploying these mechanisms in practice it is important to note that we have no guarantee which equilibrium agents will end up in. This is a far weaker result than truthfulness being a dominant strategy.

2. Similar to the last point, the mechanisms do not appear to be collusion proof. In particular one player could pretend to represent multiple clients (i.e., sending multiple updates each round). For example, in the case of single-shot mean estimation, if such an agent monopolizes a sufficiently large position of the data being submitted, they could force the other agents into submitting any desired value for large enough $C$ (since the deviation penalty will outweigh the other parts of their utility). If agents are willing to misreport data, they are probably also willing to collude. Not accounting for this possibility limits the scope of the work. With that said, the authors appear to be the first work to study robustness to strategic behavior in distributed systems, so perhaps asking for additional results on collusion is too much. However, this should be more clearly stated as a limitation of the work.

3. The experiments are somewhat limited. The primary contribution of the paper is theoretical and the point of these experiments is to show that the payment scheme works even when convexity does not hold, but this observation on a single dataset, for a single model, is a bit unconvincing. In particular, I would expect that the average reward received when increasing $\alpha_A$ would decrease more rapidly for larger $C$, however, it is not clear to me that the small amount of fines paid by honest players would hold across different scenarios.




### Comments and minor issues: no impact on score and need not be addressed in the author response. 

1. Line 259: should this say “... at the honest equilibrium [when] ….”? In the supplement this is stated as an [and] rather than a [when].

2. Corollary 4.2 should probably be a Theorem. This is actually quite an interesting result and somewhat non-trivial based on the proofs. Although Theorem 4.1 is doing most of the heavy lifting here the corollary is actually the main results, while the Theorem feels more like a helping lemma. 

3. Links to theorems and references are broken in the main body. Looks like this is the result of compiling the document with the supplement and then using a PDF editor to trim the supplement pages.

4. Figure 6 in the supplement takes up its own page.

5. The naming convention of Theorems is not consistent with the main body and makes it difficult to find a specific theorem within the supplement, unless using the reference provided in the main body.


Limitations:
See above

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents an incentive mechanism to encourage honest data reporting in the presence of spiteful behavior aiming to harm other participants. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

The paper considers an interesting an novel setting.

It shows that incentive schemes can in principle induce cooperative behavior.

The incentive schemes show both budget-balance and individual rationality (ex ante).

Weaknesses:
While the reward scheme in the paper has truthful reporting as an equilibrium, it is well-known that peer-prediction schemes also admit uninformative equilibria; for example in this case all participants could report the same data without any penalty.
*** This was my most important worry and the authors have addressed this weakness in their rebuttal ****

The schemes requires that the participants observe IID data, which is usually not the case in federated/distributed learning.
*** This still remains to be improved ****

Only particular attack and defense strategies are considered.
*** The authors have convinced me that they have gone far enough at least for this paper. ***

There is no consideration of data privacy.
*** This remains future work. ***

Limitations:
I think the fact that utilities have to be known needs to be stated more clearly.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper considers a federated learning setting with strategic data resources. The authors assume that the entities taking part in the learning process are selfish players incentivized to get the best model but benefit if their competitors receive inaccurate models. This selfish behavior pushes players to lie to the central learning mechanism in their reports.

The authors consider two cases: Mean estimation and a multi-round SGD on strongly convex objectives. They model the players' strategies as multiplicative/additive factors that could be added to the players' actual local computations. The authors show that even in the straightforward case of mean estimation, a PNE does not exist. They offer two remedies: Monetary payments (via peer-prediction techniques) and punishments (noisy model updates by the central mechanism). Then, they show that a PNE exists and characterize the form of payments/punishments required.

Finally, they conduct an experimental analysis demonstrating that their remedies positively affect the learning procedure.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1.	The paper deals with a practical issue that is somewhat under-explored.
2.	Despite the abundance of notations, the authors have done an excellent job in making the paper read smoothly. 


Weaknesses:
1.	The paper adopts a game theoretic approach, but many modeling assumptions seem cumbersome and unjustified (see questions below).
2.	It is hard to assess this paper's technical contribution. Particularly, the novelty of the peer prediction-based mechanisms are well-studied ideas. The authors did not explain whether this paper adopts these ideas in a plug-and-play manner or presents new non-trivial derivations. The ""our contribution"" part addresses the paper's content but not its marginal contribution to the line of research, making the technical contribution hard to assess.


Limitations:
None

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper studies a centralized collaborative learning problem. Authors provide theoretical guarantees for an attack method and a defense method. Further, the paper proposes two mechanisms to incentivize honesty: a method that uses an explicit side payment method and requires transferable utility, a centralized punishment mechanism where a central server adds noise to the estimates it sends to players that have sent suspicious updates. Simulation results are provided supporting the claims.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-- The paper is well written and easy to follow.

-- Authors provide a description of related work and background. The problem formulation considered in the paper is well positioned in the relevant literature. 

-- Authors provide several theoretical results making novel technical contributions. Authors provide discussions around the implications of the theoretical results.

-- The paper provides simulation results supporting the theoretical claims.





Weaknesses:
-- Problem formulation is well positioned in the relevant literature. However, authors do not provide a discussion on how their results and methods compare to existing literature.

-- Authors provide numerical simulation results supporting their analysis. However, authors fail to compare their method with existing methods in the simulations.

Limitations:
Authors do not provide a discussion on limitations of their method. Authors include a discussion on societal impacts in the Appendix.

Rating:
6

Confidence:
4

";1
AMIJEupsNq;"REVIEW 
Summary:
This work introduces a framework to Visual Question Answering (VQA) that incorporates understanding of the 3D structure of scenes, a significant leap from traditional 2D-based models. To evaluate the task, they proposed a new dataset, Super-CLEVR-3D, designed specifically for 3D-aware VQA, containing questions that necessitate compositional reasoning regarding the 3D object parts, poses, and occlusions. To tackle these queries, they also put forth a new model, PO3D-VQA, combining probabilistic neural symbolic program execution for reasoning and deep neural networks for robust 3D scene parsing. Experiments show the proposed PO3D-VQA model outperforms existing techniques, especially on more complex questions, underscoring the need for 3D understanding in future VQA research. Despite the improvements, the noticeable performance gap compared to 2D VQA benchmarks indicates that 3D-aware VQA remains a critical area of exploration.






Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I believe the studied direction, 3D-aware 2D-VQA is important for our community, especially when we want to tackle real-world tasks. Both the proposed VQA dataset and the framework, PO3D-VQA, takes a good attempt towards the big goal. 

All the used techniques as shown in method and Figure 2 are sound and composed in a reasonable way. Overall, the paper is easy to follow.

Results in Table 1 demonstrated the strength of the proposed method, which yield significant improvements, and also indicate there are more to do in the future. Some ablation studies are included in Section 5.4. 

Weaknesses:
As the paper claimed a 3D-VQA dataset contribution, I am curious how the model transfer performance from the dataset to real-images. For example, taking a picture with multiple real 3d models  and run the model to check the sim-to-real performance. 

Also, an odd part is that the current motivation for this paper is 3D navigation and manipulation, but few of the defined problems are related to navigation or manipulation. 
- Can the proposed model accurately locate parts of 3D objects that can help manipulate? 
- Are there questions directly related to navigation and manipulation in 3D space?

The current question-answers in Figure1 in my mind is more related to previous VQA while less related to 3D VQA that really matters. 

Besides, there are no failure cases understanding and analysis. For the proposed pipeline, the failed cases should include both the failure of perception module and the failure of reasoning module. Providing failure cases can help people understand the limitation of the proposed system, while not weaken the contribution.

Limitations:
It seems the current framework cannot correct itself if the perception model generated wrong outputs.



Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper introduces the concept of 3D-aware Visual Question Answering (VQA) and presents a new dataset called ""Super-CLEVR-3D"". It also proposes a model called ""PO3D-VQA"" that combines probabilistic neural symbolic program execution with deep neural networks using 3D generative representations of objects. The experimental results show that the proposed model outperforms existing methods, but there is still a significant performance gap compared to 2D VQA benchmarks, highlighting the need for further research in 3D-aware VQA.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ Originality: The authors propose a new dataset, Super-CLEVR-3D, that extends an existing 2D dataset with 3D-aware questions, introducing a novel benchmark for evaluating 3D-aware VQA models. The paper presents the PO3D-VQA model, which combines probabilistic neural symbolic program execution with deep neural networks using 3D generative representations of objects, providing a novel approach for addressing the 3D-aware VQA task.

+ Quality: Thorough experimental results demonstrate the superiority of the proposed model, PO3D-VQA, over existing methods.

+ Clarity: The paper provides clear motivation, defines the task of 3D-aware VQA, and describes the proposed dataset and model in a detailed and comprehensible manner.

+ Significance: The paper addresses the importance of understanding the 3D structure of visual scenes in VQA, introduces a new dataset, and showcases improvements in accuracy, advancing the field of VQA.


Weaknesses:
- The paper addresses the VQA problem in 3D scenes but only takes images as input. Why not use point cloud or multi-view images which are more suitable for 3D scenario?

- Where are the ground truth information of pose and occlusion come from? Are they included in the Super-CLVER dataset?

- The model design of PO3D-VQA is somewhat weak. It looks like the combination of Neural Meshes and P-NSVQA.

- I wonder about the performance of only using language and using oracle object representation ( ground truth class and pose label). In this way, we can show the reasoning ability of the model, or just accurate object detection is needed to solve this task.

- How the proposed model compared with the scene graph-based method, since the method first parses the image as scene representations. What is the advantage of the neural symbolic method against deep graph networks?

- There is no limited discussion on dataset limitations: While the Super-CLEVR-3D dataset is introduced as an extension of an existing dataset, the paper does not extensively discuss the limitations or potential biases of the dataset. Providing insights into the dataset construction process, potential challenges, and potential mitigations would strengthen the validity of the findings. And there is no limited scalability discussion: While the experimental results show improvements over existing methods, the paper does not thoroughly discuss the scalability of the proposed model. Understanding how the model's performance scales with larger and more complex scenes would be valuable for assessing its practical applicability.

Limitations:
Please refer to the weakness part. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
1, The paper extends 2D VQA to the task of 3D-aware VQA, which requires the understanding of the 3D structure of visual scenes and includes knowledge of 3D object pose, parts, and occlusions.
2. Introducing the Super-CLEVR-3D dataset, which contains questions about object parts, their 3D poses, and occlusions.
3. The proposed model, PO3D-VQA, is a 3D-aware VQA model that has two key techniques: probabilistic neural symbolic program execution, and deep neural networks paired with 3D generative object representations for robust visual recognition.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. Proposed an interesting 3D-aware VQA task.
2. Introduced a valuable Super-CLEVR-3D dataset.
3. Developed a novel  PO3D-VQA method, that significantly outperforms prior methods.
4. Detailed mathematical explanation of the components of the methods.
5. Comprehensive analysis and discussions.

Weaknesses:
1. Lack of some statistics about the constructed Super-CLEVR-3D dataset.

Limitations:
1. As described in the paper, the work is currently limited by synthetic scenes.
2. As described in the paper, the method is sensitive to pose prediction errors.

Rating:
9

Confidence:
2

REVIEW 
Summary:
This paper is seeks to improve the VQA models' understanding of 3D structure of images, particularly parts, poses, and occlusions.  There are two main contributions: 
1. Super-CLEVR-3D: a dataset that contains questions about parts, poses, and occlusions.
2. PO3D-VQA: a 3D aware VQA model that combines nuerosymbolic program execution for reasoning and 3D generative representations. 

Experimental settings: the proposed model is tested on the proposed dataset Super-CLEVR-3D.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The dataset is a good contribution for studying 3D vision-language reasoning.
2. The proposed model, PO3D-VQA is well-explained and motivated, along with the addition of a 6D parsing module and 3D NMS for better scene understanding.
3. The proposed model is a good proof-of-concept for the combination of neurosymbolic program execution and features learned using deep learning. 
4. Relevant baselines are used and the experimental setting is soundand well explained.

Weaknesses:
1. The proposed model is only tested on the Super-CLEVR-3D dataset
2. There aren't any questions about ""z"" direction i.e. above/below relationships as the objects in [31] are always on the same surface.  Because of this it is questionable to call the dataset ""3D"" as only 2D relationships are covered. The dataset is missing templates/questions about depth, distance between objects etc.
4. Super-CLEVR-3D dataset only 5 categories, all of which are forms of ‘vehicles’. This could also lead to an overestimation of the object detection module that is used in the model. 
5. The method could be tested on several other 3D aware datasets: for instance GQA [Hudson et al CVPR 2019], or this work https://arxiv.org/abs/2209.12028. Also see Q2, Q3, Q4 for more questions about evaluations.

Limitations:
Limitations are discussed [line 348]. Discussions along the lines of Q2/3/4 above would also be useful when describing the limitations.

Rating:
6

Confidence:
5

";1
2vADOf3K00;"REVIEW 
Summary:
The paper presents one alternative way of finetuning to work on compressed videos. Specifically, it designs a specific data flow within three modalities (RGB, residual, and motion vector). It also presents the way to make the model adapt to new compressed videos and provide a fair comparison. It demonstrates SOTA performance to understand the proposed setting.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The problem this paper studies is pretty interesting and useful. Their method design is also complete and thinks about the alternatives.
The presentation is pretty clear while the results are SOTA under their setting. 


Weaknesses:
The motivation of residual gating motion vector and gating I-Frame information is still weak to me.
It would be better if the author can provide more evidence (exps and visualization).

The presentation of CPR is a little weak to me and needs to be a little more clear.



Limitations:
NA

Rating:
8

Confidence:
4

REVIEW 
Summary:
This work studies the video classification task in the compressed video. With the motion vector and residual as the prompt, this work proposes selective cross-model complementary prompter idea to enhance the cross-model interactions, achieving promising results while maintaining a small number of trainable parameters.  

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Most parts of this paper are well written which clearly demonstrates its motivation and methodology. Especially, the method of this paper is easy to follow.

The idea of using motion vector and residual in the compressed video as prompts is inspiring. These two modalities are computationally free to access, which is a significant advantage compared to traditional motion cues, such as optical flow.

From the experimental results, the proposed CVPT achieves promising results while keeping the tunable parameters low. Visualization is a plus.


Weaknesses:
The evaluation benchmarks (UCF, HMDB, and SSv2) are all small scaled. One concern is the scalability of the proposed method.

The design of some key components is not well justified and the figure 2(right) is confusing. For example: 1. Will the prompt join the multi-head self-attention (MHSA)? If so, why the frozen MHSA is able to leverage prompt feature from other modalities. 2. In the SCCP, are the input prompts directly from the previous SCCP other than MHSA? What’s the motivation and effectiveness evidence of keep using the SCCP-processed prompts in SCCP of different layers? 3. In L_2, there is a closed loop for the I-frames embedding. What is the connection to the previous layer. 4. Not sure what are summed together by using the adding function in L_1 and L_2.

The experiments set-up in the ablation study is not clear enough. In the Table 3, CPR without refinement and CPR are both activated in the last row, which is confusing. Also, the linear probe is used as the baseline, but some implementation details are missing, for example: the number of input frames, how prompts attend the self-attention, and the architecture of classifier. In Table 5, what is the implementation details of fully fine-tune?

Ln 45 suggests the large parameter storage burden from previous methods. However, there is no related comparison in the following analysis. In addition to the trainable parameter, the throughput (samples/second), computational burden (GFLOPs), and memory cost (GB) are important metrics. 

Minor:

Ln 131 The resulting -> the resulting

There is no notation for g_4 to g_6 in Eq 7

Table 4, E_R -> P_R 


Limitations:
The scalability may be one of the limitations.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The authors present a way to adapt pre-trained raw video models to compressed videos. They utilized the existing concept of prompt tuning from NLP and repurposed it for the compressed video domain. Their findings indicate that by fine-tuning just 0.1 percent of parameters for a downstream task such as video classification, the pre-trained model can be modified to cater to compressed videos.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* As per my knowledge, this is the first study to explore the prompt tuning method for the compressed video domain.
* Enough experimental results are provided to back the claims made in paper
* Paper is well written

Weaknesses:
* Only video classification is shown as a downstream task.
* Why are the numbers reported in the submission differ from the numbers reported in the original paper(for eg: CoViAR) is there differences in the experimental setting?
* Add Bold text in the tables to highlight the best-performing methods as it is really hard for the readers to sift through the tables in their current condition.

Limitations:
.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes an efficient fine-tuning method based on the prompting concept in the compressed video domain. The intuition is to freeze the backbone pre-trained on raw videos and use the proposed prompting techniques to query the required information for the compressed videos. To address the multi-modality of the compressed videos (RGB images, motion vectors, and residuals), the authors propose embedding them first and using a SCCP module to fuse and refine them. The SCCP module is designed based on the fact that the video tasks are more important and sensitive to the motion boundaries. Therefore, the residual map acts as a condition to attend the motion vector map. The results are then added back to the RGB image tokens. The outputs of SCCP are then passed to the next pre-trained layer. Superior performance is obtained and the gap between raw video is shortened.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The idea is simple and easy to follow.
- The efficient design is important to the video tasks as the backbone can be frozen. The overall learnable parameter is significantly small compared to the backbone. Therefore, for different downstream tasks, the storing problem can be alleviated.
- The idea of using the residual map to attend to the motion vectors sounds novel and interesting.

Weaknesses:
* Some operations in the proposed method are confusing:
	- In Eq. 6, what is the physical meaning of adding an attended motion vector to image embeddings/tokens? The motion vectors represent the relative movement information for each of the spatial blocks (movement in x and y directions in the form of vectors), while the RGB embedding contains spatial information/structure. The addition operation does not really make sense.
	- The processed \mathcal{M}_l(E)_I^l) is added to E_I^l again, what is the intuition behind this operation? The overall sequence is: the motion vector is attended by the residual information and then added back to the raw image. The output is further processed and added back to the raw image. How to physically explain this?
* The overall architecture is very similar to [51]. What is the fundamental difference?


Limitations:
Limitations are addressed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper explores how to transfer pretrained RGB models to compressed videos with a parameter-efficient paradigm and introduces a prompt-tuning method named Compressed Video Prompt Tuning (CVPT). In CVPT, the learnable prompts are replaced with encoded compressed modalities that are refined in each layer. To improve cross-modal interactions between prompts and RGB input flow, this paper proposes a Selective Cross-modal Complementary Prompter block that refines the motion cues and complements other modalities to the RGB modality. Experimental results show that CVPT outperforms full fine-finetuning and other prompt-tuning methods on SSv2, UCF101 and HMDB51 dataset.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed prompt-tuning method designed for compressed videos can leverage the pretrained RGB models with a few
trainable parameters.
2. On compressed videos, CVPT outperforms full fine-finetuning and other prompt-tuning methods (VPT, AdaptFormer) on SSv2, UCF101 and HMDB51 datasets.

Weaknesses:
1. The computational cost (GLOPs) of the proposed method may be higher than that of some previous works based on ViT. According to (1), the tokens of I-frames, motion vector prompts and residual prompts are all sent to each layer of pretrained ViT, so the number of input tokens may be larger than that of some previous ViT-based models. 
2. This paper says that compressed videos ""provide notable advantages in terms of processing efficiency"", but there is no efficiency comparison with previous works based on raw videos. It would be better to provide the inference time comparison between previous works (especially RGB models) and the proposed method.
3. Some state-of-the-art methods[2,3] on the UCF101, HMDB51, and SSv2 dataset are not compared in this paper.
4. The idea of this paper is mainly from [1], as mentioned in this paper, but the exploration of compressed video understanding is encouraged.

- [1] Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, and Huchuan Lu. Visual prompt multi-modal tracking. CVPR, 2023.
- [2] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. CVPR 2021.
- [3] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. CVPR 2023.

Limitations:
The authors have addressed the limitations of this work.

Rating:
6

Confidence:
4

";1
Ex3oJEKS53;"REVIEW 
Summary:
This paper extends Kronecker-Factored Approximate Curvature (K-FAC) into training generic neural networks, especially for transformers and graph neural networks. This is done by taking advantage of the weight sharing mechanism. The authors propose two flavors of K-FAC (expand and reduce), depending on the dimension of the output tensor. The authors prove that they are exact for deep linear networks. The proposed methods reach a validation metric target of a well-tuned first-order baseline with smaller steps and less wall-clock time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper extends K-FAC into transformers and graph neural networks. Although some previous works try to extend K-FAC into transformers, their approximation is inexact, as pointed by the authors.

2. The proposed methods have mathematical foundations. Because the authors prove the exactness under deep linear settings.

3. The proposed methods reach a validation metric target of a well-tuned first-order baseline with smaller steps and less wall-clock time. So it is possible to replace first order methods.

4. This paper provides sufficient supplementary material, including mathematical derivations, additional experiments, and reproduction codes.


Weaknesses:
1. The performance of transformers is evaluated on ImageNet. Since transformers are mostly used for language modeling, it is better to evaluate your optimizers on some language tasks (a very big dataset is unnecessary).

2. The authors prove the exactness under deep linear settings. If possible, It is better to provide some error bounds for non-linear settings.

3. It is helpful to provide the time complexity (not wall-clock time) and space complexity for both K-FAC-expand and K-FAC-reduce. 

Limitations:
This paper has no negative societal impact.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces two different settings of linear weight-sharing layers to fit for the second-order optimization method Kronecker-Factored Approximate Curvature (K-FAC). Experiments on GNNs and ViTs achieve less number of steps compared with the first-order baseline.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Experiments include GNNs and ViTs, also the results are quite inspiring in the acceleration of steps.

Weaknesses:
Fig. 4 doesn’t show the loss and accuracy variations after convergence.



Limitations:
The authors discussed the runtime of the two proposed schemes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper propose two methods called KFAC-reduce and KFAC-expand to conduct the secon-order based training for neural networks. The key idea is to approximate the Gauss–Newton matrix with low-rank matrix decomposition. The experiments on several tasks show that the propsed method can achieves faster convergence rate compared to NAdamW optimizer.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The proposed method has well-grounded theoretical support.
- The proposed method has shown better results than the NAdamW/SGD optimizer.

Weaknesses:
My biggest concern comes from the evaluation sections.
- The proposed method is only verified on limited number of tasks and networks (4 configurations). The author should show more results with various configurations.
- The method is only compared with two type of optimizers. For example, SGD is the baseline for GNN experiments while NAdamW is the baseline for the ViT experiment.

Limitations:
Please check the weakness sections.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The researchers propose a framework for applying the Kronecker-Factored Approximate Curvature (K-FAC) method to neural networks with linear weight-sharing layers. They identify two different settings of linear weight-sharing layers, leading to two variations of K-FAC: expand and reduce. They demonstrate that both variations are exact for deep linear networks with weight-sharing in their respective settings. They find that K-FAC-reduce is generally faster than K-FAC-expand and leverage this to accelerate automatic hyperparameter selection for a Wide ResNet. They also show that both K-FAC variations can achieve a target validation metric in a significantly reduced number of steps compared to a first-order baseline, resulting in a comparable improvement in training time.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Very interesting perspective by looking at modern NN building blocks as weight shared linear layers. 
Solid results and thorough experiments. It's great that the authors measure actual wall-clock time speedup instead of simply recording the steps. This shows that it's a method with practical merits.

Weaknesses:
Minor issues in presentations, such as the plots are not really hard to read, and organization of the propositions are too clustered to be read comfortably.

Limitations:
providing more intuition in the theory section will be highly appreciated. From the reader points of view, who are not familiar with the notation setup, it looks like a ""wall of math"", everyone complains about.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper formalizes two variants of the K-FAC optimisation method for a family of neural network architectures that can be expressed as linear layers with weight sharing. Experiments are conducted to validate that the proposed method can be applied to different kinds of neural network architectured speeding-up training.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper is a bit out of the scope of my specific field of expertise and it is possible that I failed to fully understand all technical details and mathematical formulation, but the general mathematical formulation of the method seems sound and complete. The family or achitectures where the two variants of the proposed method can be applied is formally defined, and the conditions for which exact approximations can be obtained are also defined.

Weaknesses:
I miss a better contextualitzation, both from a theoretical and experimental point of view,  of the proposed method with other existing approaches, cited in the related work, that also make use of K-FAC and that have also been applied to similar architectures using convolutional networks, transformers or GNNs. From a theoretical point of view, a better justification of the contribution of the proposed method with respect to other applications of K-FAC. From an experimental point of view, a comparison of the proposed method with other works also using K-FAC.

Limitations:
Limitations of the method are partially addressed.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper considers linear layers with weight-sharing, which is used in many neural network architectures, such as transformers, convolutional and graph neural networks. Two variants of K-FAC, K-FAC –expand and K-FAC-reduce are considered for two different settings of linear weight sharing layers. The authors also show that they are exact for deep linear networks with weight-sharing in their respective setting. Numerical results show the efficiency of the proposed K-FAC variants.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written and organized. The use of second-order methods is typically influenced by the specific network architecture. In this paper, we focus on linear weight-sharing layers, which are crucial and commonly used in Transformer, convolutional, and graph networks, but have not received adequate attention in the realm of second-order methods. In my opinion, a more thoughtful analysis of the architecture could enhance the performance of second-order methods like KFAC and others. Undoubtedly, this paper will expand the applicability of second-order methods and their potential benefits for machine learning tasks. 

This paper provides a correct mathematical representation of gradient for the weight-sharing linear layer structure, which is not trivial and can facilitate the development of related optimization methods.

Weaknesses:
One of my concerns is the applicability of second-order methods, particularly for modern architectures such as the LLM model. As noted in the AlgoPerf paper[1], the use of KFAC requires detailed architectural information, which may limit its use in practice. In contrast, methods like Shampoo and NG+ appear to offer more flexibility. Additionally, while the direct approach is effective in practice, there is a lack of theoretical understanding regarding the rationale for using the KFAC-reduce approximation.

The computational complexity of the proposed method appears to be higher than previous approaches, which could pose challenges for implementation. However, there is no direct comparison with the current implementation of the KFAC method in this setting, and it would be beneficial to include such a comparison. Additionally, it is recommended to compare the proposed method with other second-order approaches to gauge its effectiveness.

[1] Dahl G E, Schneider F, Nado Z, et al. Benchmarking Neural Network Training Algorithms[J]. arXiv preprint arXiv:2306.07179, 2023.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
j4QVhftpYM;"REVIEW 
Summary:
This paper proposes a two-layer federated learning (FL) framework (FedSep) by separating the communication and learning parameters. By a bilevel optimization formulation, FedSep enjoys a convergence guarantee. In addition, two settings, communication-efficient FL and model-heterogeneous FL, are solved in FedSep framework.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. FedSep is an interesting and novel setting.
2. Theoretical convergence analysis is provided, showing the sublinear rate.
3. FedSep is applied to communication-efficient FL and model-heterogeneous FL.

Weaknesses:
I have the following concerns and questions:

1. The convergence rate is sublinear, but it does not show any speedup in terms of the clients' number and local steps. So it does not exactly match the convergence of standard FL algorithms. Any possible improvement? Or the statement in the paper should be modified accordingly (e.g., line 75).

2. In the model-heterogeneous FL, the validation data set is also used in the training process both for problem formulation and experiments. I believe this is a concern and unfair for the direct comparison with other algorithms

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors proposed a two-layer federated learning framework called FedSep, with one layer for communication and another layer for learning. The two layers are connected through decode/encode operations. Furthermore, the authors proposed an efficient algorithm to solve FedSep by treating it as a bilevel optimization problem, and showed convergence results which match those of the standard framework. The FedSep can incorporate Communication-Efficient FL and Heterogeneous-Model FL.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
(S1) The proposed framework, FedSep, provided a general framework for resolving the tug-of-war by separating the communication and learning of federated learning into two layers. 

(S2) The algorithm was explained in detail and the theoretical results are solid. The experiments results supports the claims of the authors well.

Weaknesses:
(W) The framework rely on the analyticity and strong convexity of the second level problem, which may not be true in many cases. For example, for problem (equation 4) formulated in Sec 4.1, the analyticity and strong convexity does not seem to hold. Although I still believe some convergence results can be derived.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper asserts that the tasks of communication and learning are at odds in federated learning. As such, a new approach is suggested that separates these tasks. The approach comprises an encode-decode operation, where decoding is cast as an optimization problem. The overall learning task is formulated as a bilevel optimization problem. Standard first-order gradient-based approaches are then employed to solve the bilevel optimization problem. The structure of the proposed algorithm mirrors that of typical FL algorithms. A convergence bound is proven for this algorithm. Some applications pertaining to communication-efficiency and model-heterogeneity in FL are also discussed. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The particular ""separation"" framework suggested in this paper appears to be novel; I have not seen anything exactly like this in earlier FL work.

- One advantage of the proposed bilevel approach is that each agent can train a separate local model $y^{(m)}$ of its own. This allows for personalization in the face of data-heterogeneity.

Weaknesses:
I have several major concerns ranging from the motivation to the utility of the proposed approach. Let me elaborate on them below. 

- The entire paper is based on the premise that communication and learning are conflicting goals in federated learning. However, this statement is never formalized at any point in the paper. The only discussion regarding the tension between communication and learning shows up in lines 25-30, which don't convey anything concrete.  

- Continuing with the above point, I also missed the motivation for the specific bilevel formulation. By now, there are several approaches that guarantee communication-efficiency in RL (see Refs [R1]-[R5] below) using techniques ranging from quantization, sparsification, compressed sensing, etc. It stands to reason that compressing information about models/gradients will naturally come at the cost of performance in learning. So the tension here is evident, and has been well-explored/quantified in the papers I mentioned earlier. In particular, sending less bits of information slackens the rate of convergence, and one can potentially try to investigate the fundamental limits on the rate needed to achieve a desired level of accuracy; see, for instance, [R5]. However, there is no clear discussion of why the FedSep bilevel idea is any significant improvement over any of these MANY existing schemes. 

[R1] Federated learning with compression: Unified analysis and sharp guarantees, Haddadpour et al., AISTATS 21

[R2] Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients, Mitra et al., NeuRIPS 21

[R3] Optimizing the communication-accuracy trade-off in federated learning with rate-distortion theory, Mitchell et al., arXiv 2022

[R4] Communication-Efficient Federated Learning through Importance Sampling, Isik et al., arXiv 2023

[R5] Differentially quantized gradient methods, Yin et al., IEEE Transactions on Information Theory, 2022

- Regarding model-heterogeneity, some of the references I alluded to earlier do account for heterogeneous loss functions across agents. Moreover, if the goal is to allow for personalized models, then why not use any of the existing schemes for personalization in FL (of which, there are many)? See, for instance, ideas based on Moreau Envelopes and MAML in [R6] and [R7], and representation learning in [R8]. If one cares about both personalization and communication-efficiency, I can imagine simply using these ideas in tandem (in meaningful ways). 

[R6] Personalized Federated Learning with Moreau Envelopes, Dinh et al, NeuRIPS 2020

[R7] Personalized Federated Learning: A Meta-Learning Approach, Fallah et al., NeuRIPS 2020

[R8] Exploiting Shared Representations for Personalized Federated Learning, Collins et al, ICML 21

The overarching point I am trying to make here is that each of the key considerations in FL that the authors allude to (communication-efficiency, heterogeneity, personalization, etc), have several existing principled algorithmic solutions. It wasn't apparent to me at all why there is a compelling need to depart from these existing approaches.

- In addition to the motivation, several parts of the paper are somewhat vaguely written. For instance, in Eq. (1), the meaning of the object $g^{(m)}$ is not explained. The encoding operator in Eq.~(2) isn't clear to me either. What is this operation and how does it compare with any of the standard encoding techniques (say for instance, standard scalar and vector quantizers, or sparsifying mechanisms like Top-k)? How many bits are needed to perform this encoding? What is the error caused by this encoding? Is this an unbiased encoding mechanism? No intuition is provided at all about any of these crucial points, making it hard to draw meaningful comparisons with existing schemes.

- The difference with existing federated bilevel optimization algorithms in lines 153-157 is also quite terse. The discussion did not come across as anything fundamental. It wasn't clear to me why the bilevel algorithm proposed can't be analyzed by simply adapting ideas from other existing FL bilevel algorithms. 

- The main convergence result in Theorem 3.7 is hard to parse. I was left wondering about several key questions: (i) How does the compression scheme (encoding-decoding) affect the rate of convergence? (ii) How does this trade-off compare with the existing known bounds for compression in FL? (iii) How does the effect of heterogeneity in the agents' loss functions manifest in the bounds? Does this dependence match with those known for federated bilevel optimization? 

Also, the iteration complexity seems to have a $\kappa^5$ dependence on the condition number $\kappa$. This seems much larger than what one typically obtains. Thus, I am not convinced about the tightness of the bounds either. 




Limitations:
I couldn't find a clear discussion of the limitations. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper uses bilevel optimization in a federated learning setting. A unique decomposition of communication and learning has been identified, which has applications for reducing communication overhead and supporting heterogeneous models. The theoretical convergence guarantee has been presented and experiments also show the advantage of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The use of bilevel optimization as a decomposition into communication and learning in a FL setting is interesting. 
- The framework is general and supports multiple application scenarios of FL.
- Theoretical convergence bound is provided.

Weaknesses:
- The use of encoder/decoder structure may incur additional computation compared to common FL algorithms. It would be nice to quantify the amount of such additional computation.
- The extension of common bilevel optimization algorithms to include multiple update steps (as pointed out at the end of page 4) seems to be somewhat similar to the idea of local updates in FedAvg / local SGD algorithms. It would be nice to identify what are the key technical challenges and novel solution techniques in this extension. 
- There is space for improvement in the experiments (see details below).
- The writing could be improved to highlight the usefulness of encoder/decoder in the context of FL, in early parts of the paper.

Limitations:
N/A

Rating:
6

Confidence:
4

";1
QLllDwizVd;"REVIEW 
Summary:
This paper first proposes a new task, cross-category few-shot learning for articulated object manipulation. And it further proposes a framework that first explictly measures the semantic similarity of local geometries,  interacts with the objects of novel categories according to the similarity, uses the interaction results to update the affordance estimation network, and finally manipulate the target object. Authors perform experiments in simulation and the proposed method outperforms existing methods. They show a successful demo on a real robot.  

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The strengths of the paper are:
1. The paper formulates a new problem, cross-category few-shot affornace learning, which may be useful for robot applications;
2. The paper proposes a framework to address the proposed problem, which leverages the geometric similarity to guide the exploration on objects of novel categories. The paper designs a cross-category similarity learning pipeline to learn the similarity metric.

Weaknesses:
1. Statistics, eg. std, are not provided in the experimental results, especially the variance casued by the choose of few-shot learning instances.
2. While the results of the proposed method are highed than baselines, the successful rate is still not statisfactory, especially the result of `full-data' is still low. More discussion on the results and illustration of failure cases can be added.
3. The assumption of the paper that similar geometry indicate similar affordance for novel categories may be not solid.

Limitations:
The limiation of low success rate is not discurssed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper focuses on the task of predicting affordances given a single-frame observation of an articulated object. The authors utilize few-shot learning to address the novel object adaption problem. However, previous approaches to few-shot learning require per-instance interaction during test time, which can be both costly and inefficient. To address this issue, this paper introduces the Where2Explore framework, which estimates geometric similarity across categories to offer more efficient interaction proposals. Additionally, it enables the transfer of learned affordance to novel cases. Experimental results demonstrate that this method surpasses previous approaches in various aspects.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The motivation to explore more efficient few-shot learning is intriguing. The concept of leveraging affordance similarities across categories is intuitive and seems to be effective based on the experimental results.
* The training strategy to learn the cross-category geometry-aware affordance similarity is straightforward and effective based on the quantitative visualization.
* This paper conducts extensive experiments to compare with prior work and includes sensible ablation studies to demonstrate the effectiveness of the Where2Explore framework.

Weaknesses:
* There are not enough descriptions of the hard-coded intersection sets. Even if it’s proposed in the prior work, it’s better to include more details to be self-contained in this paper. This paper only chooses two actions, “push” and “pull” from the original 6 actions defined in the prior work. For the interaction definition, the action direction should be in the positive hemisphere of the surface normal. It’s not clear if it holds true for the real data. (Because the real data similarity visualization is not based on the surface normal) 
* In the few-shot adaption stage, it’s unclear how many interactions for each point are sampled for the similarity. In addition, the authors mention the similarity is conditioned on the action. It’s unclear if the action is still conditioned in the object/camera/world coordinate or some local coordinate to the interaction points. If the action is still parameterized in the global coordinate, it’s hard to see if the model can really learn the similarity.
* For the few-shot learning, it’s hard to see the usefulness of the models in the supporting set. The current set of experiments cannot show the usefulness of the supporting set. Is this possible to learn from scratch with the few-shot learning strategy?
* For the affordance and similarity training, there seems no reason to train similarity at the start, because the affordance predictions are still quite bad. It’s also not clear if training affordance and similarity in two-stage can achieve better performance.

Limitations:
The authors mention the limitation.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of interacting with novel articulated objects. More specifically, a few-shot learning based approach is proposed to adapt to unseen instances at test time. The methodology explicitly leverages similar local geometries, which may be shared across objects from distinct categories. Experimentation reveals superior performance relative to prior work.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Well-written. The paper is well-written, and the figures aid in the understanding of the methodology.
- Ablations are insightful. The ablations adequately test the utility of the design choices of the proposed methodology.
- Strong results. The proposed approach outperforms baselines from prior work as well as the ablations. The fact that the proposed methodology achieves comparable performance to full-data while using only 0.3% of the data is impressive.

Weaknesses:
- Simplistic problem setting. The problem setting seems to be simplistic. For instance, the robot arm is abstracted away. This completely side-steps the problem of finding a motion plan which actually solves the task in a collision-free manner, limiting real world deployment. Furthermore, the input point cloud is assumed to be cleanly segmented out. It is unclear whether this is a realistic assumption, especially for when the models are deployed to the real world.
- Limited real world evaluation. It is not clear whether the model truly generalizes to the real world. Furthermore, the section on real world experimentation (both in the main paper and the supplementary) seems to be quite limited. For instance, how is the grasping (gripper open / close) done, as this doesn't seem to be an output of the system? Or, how is the motion plan obtained from the end-effector poses? More extensive (quantitative and qualitative) experimentation on real world data would be preferred to be convincing that the sim2real generalizes.

Limitations:
There is a small section on limitations of the work, which could be expanded.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper focuses on few-shot affordance learning and articulated object manipulation. It introduces a framework named Where2Explore, which consists of the affordance module and the similarity module. Both modules share the same architecture, a Where2Act [24]-like network based on PointNet++ [28]. While the affordance module is trained on a single category, the similarity model is trained across categories to understand similarity in local geometry and manipulation semantics. The model is benchmarked on 942 instances from 14 (3 for train, 11 for test) object categories in PartNet-Mobility [37]. It outperforms previous works [24, 34] in simulation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The methodology is well motivated: Learning similar local geometry across object categories could benefit few-shot generalization in manipulation.
2. It is impressive to observe the stark differences between the seen categories and the unseen categories. 
3. Under the same settings, Where2Explore exhibits a clear edge over previous works [24, 34] quantitatively.
4. The demo video is well-prepared.

Weaknesses:
1. The method is limited to pulling and pushing actions, and the action trajectories are short-term and hard-coded. These limitations inherited from Where2Act could result in a significant simulation-to-real gap.
2. There is only a single example of real-world manipulation, which is insufficient to demonstrate the method’s capabilities. Besides, according to the video, it seems that the gripper may not always hold mugs firmly enough. More real-world experiments should be conducted, and their success rates should be reported.
3. Few improvements on the architecture. The current module largely follows the design of [24, 34]. While it is commendable to demonstrate new insights using a minimal network design, the current task may benefit from more fine-grained cross-category transfer learning. There is room for improvement in the methods.
4. Neural Descriptor Fields (Simeonov *et al*.) is also capable of capturing local geometry information using few shots. It would be valuable to discuss and even adapt them for comparison.

Limitations:
The paper includes a section dedicated to limitations; however, it provides minimal discussion on the specific limitations. Please incorporate a comprehensive discussion of failure cases and limitations, including those in the Weaknesses section. To enhance the credibility of this work, it would be beneficial to incorporate quantitative results from real-world experiments and quantitative comparison to other works that follow a different paradigm, such as Neural Descriptor Fields and GAPartNet [8]. This would provide a more convincing evaluation of the proposed methodology.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper explores a few-shot learning strategy to predict the affordance of objects in novel categories. The key idea of the paper is to use a similarity module to transfer the knowledge from known categories to novel categories. Then the affordance prediction on the novel object can be learned with just a few exploration actions. The paper is compared with several existing methods on standard benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper addresses an important and interesting problem of transferring affordance knowledge from known categories to novel categories.

The paper proposes a similarity module for effective affordance knowledge transfer and achieves quantitativel  better results than existing methods.

The paper is overall easy to follow.


Weaknesses:
Method:

I have a couple of questions regarding the learning procedure and intuition.

1. In Figure 2, to train the similarity module, the affordance module predicts the affordance map on the novel object and the similarity module predicts the per-point similarity. In my understanding, is the similarity module exactly the same with the affordance module but just learned with different data and learning objectives? If so, in eq (1), is the Aff the affordance module or similarity module in Figure 2? I am not sure if eq (1) and Figure 2 are aligned, maybe explaining in more detail could be helpful.

2. It is not clear to me how the affordance prediction is updated during exploration/inference. Is it more of a test-time-adaptation? What module parameters are updated?

3. I am not sure if the intuition of generalization using local geometries is well supported by the design. Is the similarity module responsible for this? If so, how do local geometries play a part? Isn’t it still predicted for all N observed points with global information? Since this builds the key idea of cross-category generalization, I think further clarification is necessary.

4. In the paper, 10 instances from each novel category are used for few-shot learning. I wonder, for example, for Where2Act baseline, is it also trained with only 10 instances? There is also no ablation on the number of instances required, could it be fewer?

5. The paper only explores push and pull. Maybe more actions can be studied to validate the method.


Limitations:
Yes.

Rating:
6

Confidence:
2

";1
GEMHw2sd9S;"REVIEW 
Summary:
This paper develops a series of differentially private (DP) Random Projection and Signed Random Projection algorithms. By using randomized responses and the idea of smooth sensitivity, the new proposed algorithms are shown to outperform previous results while still maintaining the same amount of privacy guarantee.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The idea of using the exact tail probabilities of Gaussian to compute the necessary noise level (Theorem 3.2) is pretty interesting. This might imply that we can improve the empirical performance of most Gaussian Mechanism-based algorithms by using this simple fix.

- ""Local perturbation"" is also an interesting idea to investigate since a lot of the time, we only need to maintain the privacy for specific sets of datasets rather than for any datasets in the data universe.

- The appendix has plenty of good and thorough discussions that improve the readability and completeness of the paper.

- The proofs and analyses are fairly well-organized and well-written.

Weaknesses:
- Seems like we do not have a utility guarantee for the smooth flipping probability algorithms? Since this is the main focus of the paper, the result would be stronger if the author can show that the algorithm performs well both in practice and in theory.

- It would be interesting to see if the new DP-random projection algorithms can perform well in deep learning tasks. For example, if we use the DP-random projection to quantize the gradient, how does that affect our results? I feel like this would better demonstrate the power of the projection algorithms.

- The runtime should probably be reported in the experiments section since it's a fairly important aspect of these types of algorithms.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors study the important problem of preserving privacy in matrix data. They study a family of fundamental operations on data which is Random Projection. This is useful for dimensionality reduction, NN search etc. Based on RP one can also obtained signed RP (SRP) which are useful (as in  SimHash) to find near duplicates and estimate similarity of vectors, etc. The authors present DP algorithms for this family of applications (RP, SRP). Their algorithm improve over prior work ([47]) showing methods of reducing the amount of noise using a series of techniques including smooth sensitivity and better upper bounds on sensitivity. Their experiments show improved results.
All in all this is an interesting paper on improving the accuracy of an important building block in data analysis with privacy.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
+ Important problem : computing with privacy random projection based sketches
+ Improved empirical results for a practical algorithm 
+ theoretically sound and non-trivial results

Weaknesses:
- It would be interesting to know how much head room is there if the projection matrix itself is kept secrete. There is no comparison with methods that omit the matrix W.


Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces a series of differential privacy (DP) random projections (RP)  algorithms based on the robustness of the sign flipping probability in RP. This approach is promising as it seeks to enhance the existing methods in both theoretical guarantees and empirical performance.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The analysis of the probability of smooth sign flipping is a novel contribution, offering a fresh perspective for DP RP algorithms.

2. The presentation of the paper is generally clear, providing a concise explanation of the methodology and algorithms developed. The authors effectively explain the motivation behind their research and the rationale for leveraging sign flipping probability.

3. The experimental evaluation of the proposed algorithms is adequate, highlighting the improvements achieved by DP-SignOPORP and iDP-SignRP over existing algorithms in the standard differential privacy setting. The comparison with other methods in the literature provides valuable insights into the performance of the proposed algorithms. 

Weaknesses:
NA

Limitations:
The authors adequately addressed the limitations

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of computing differentially private sketches of high dimensional vectors via random projections. The contributions are:
1. Algorithm DP-RP-G-OPT: An algorithm that uses the Gaussian mechanism to privatize random projections using Gaussian and Rademacher matrices, but adds more carefully calibrated noise by using the CDF of the Gaussian distribution as opposed to the usual tail bounds. It outputs a k-dimensional DP sketch of a p dimensional input vector, it seems to be working in the local DP setting as it operates on a single data vector.
2. Algorithm DP-OPORP: An algorithm that appends the OPORP mechanism of Li and Li ([53] in the paper) with the Gaussian mechanism to produce a DP variant of the former. It produces a k dimensional sketch of the p dimensional input vector but uses only $p$-many multiplication operations instead of $kp$-many.
3. Algorithm DP-SignRP: An algorithm that produces a 1-bit sketch of the input vector in a differentially private way. The algorithm only accepts input vectors with some lower bound $m$ on the norm which must be passed as an argument. They also use this algorithm to produce a DP sign estimator.
4. Algorithms DP-SignRP-RR and DP-SignRP-RR-Smooth: The authors aim to produce an improved version of DP-SignRP by leveraging the stability of the sign of the projected output under small changes in the input. They also use the binning of OPORP to reduce the sensitivity of the algorithm to the input so as to be able to add less privatizing noise.

The authors go on to test their methods empirically on similarity search and classification problems.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The authors want to leverage the stability of the sign operator under random projects to boost the privacy guarantee over what one might get naively, in some sense they want to appeal to the local sensitivity of the mapping when the input vector is far away from the decision boundary. This idea may be fruitful.
2. The authors conduct some experiments to test their methods.

Weaknesses:
1. The privacy model here is not clear - the authors define and talk about differential privacy in the central model (where the stability guarantee holds over adjacent data sets) but their algorithms take as input only one data point. It seems that instead all the guarantees are meant to hold in the non-interactive local model of differential privacy.
2. The methods DP-RP-G-OPT and DP-OPORP are not very novel - the first achieves no asymptotic improvements over prior work and seems to improve the performance by some unspecified constant. The second simply appends the OPORP algorithm from prior work by the Gaussian mechanism.
3. The significance DP-SignRP-RR is not clear - there is a lower bound $m$ on the input vector.
4. The quality of the writing needs to be improved. The most novel contribution seems to be DP-Sign-OPORP-smooth-RR which is relegated to the end.

Limitations:
This is a theoretical work and I do not feel that potential societal impact needs to be discussed here.

Rating:
4

Confidence:
3

";1
hLPJ7xLbNF;"REVIEW 
Summary:
The paper proposed Lagrangian motion magnification using the pre-trained optical flow network. Magnification loss induces that the optical flow of the magnified frame matches with the optical flow of a given frame by $\alpha$ times; color loss regularizes the color consistency between a given and magnified frames. Test-time adaptation improves the quality of magnification on out-of-domain. Experiments show competitive performance compared to the prior arts in terms of SSIM and the proposed evaluation metrics.

Soundness:
3

Presentation:
1

Contribution:
3

Strengths:
1. Simple and effective algorithm to train motion magnification network using the off-the-shelf optical flow network.
2. Given the off-the-shelf optical flow network, this approach enables the training on large-scale unlabeled videos.
3. Targeted magnification and test-time adaptation might provide a better user experience.
4. The proposed method seems to be independent of the architecture of the neural network.

Weaknesses:
1. The term should be used carefully. I am not sure that the proposed method can be named ""self-supervised"" because the off-the-shelf optical flow network, which is used in experiments, is trained by supervised learning. If authors want to use the term ""self-supervised"", the self-supervised optical flow network should be used in the main experiments and the supervised one would be the strong baseline to be compared; It is not sufficient that the self-supervised optical flow network can be used in theory. 
2. Evaluation metric is limited. To justify the underperformance in SSIM, the authors notice this phenomenon in the last sentence in Table 3, ""DeepMag explicitly trains for SSIM"". The proposed algorithm is also optimized by the proposed evaluation metric, Motion Error. For the same reason, I cannot be convinced about the quantitative results. 
3. It is better to include the limitation to use the optical flow network. The optical flow network is inferior to estimate the subtle motion. It is related to the underperformance in the 0.04px subpixel test of Table 3. Thus, the limitation induced by the optical flow network should be investigated as an ablation study because this subtle motion is important in magnification.
4. I think that this proposed method is more general than supervised or self-supervised learning because this is determined by which optical flow network is used. How about using the synthetic data together?
5. Is the network architecture different from DeepMag? As for the control experiments, do the number of parameters affect the performance directly? 
6. I think that DeepMag is sufficient as the strong baseline. However, I wonder why Warp Nearest and Bilinear are used, and the more advanced hand-crafted algorithms [A, B] are not used as baselines. It is because evaluation data might contain large motion?

[A] Phase-Based Video Motion Processing (SIGGRAPH 2013)

[B] Riesz Pyramids for Fast Phase-Based Video Magnification (CVPR 2014)

Limitations:
Limitations and broader impacts are described at the end of the main paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a self-supervised model to solve the Lagrangian motion magnification problem without needing ground-truth labels. The network takes as input the two input frames and a magnification factor that ranges from 1 to 16, and outputs a generated frame that has magnified motion from the first frame. Off-the-shelf optical flow networks are used in loss computation for self-supervision. Test-time adaption has been explored to enhance the generation quality. Experiments show promising results.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Good writing; overall clear. The studied problem has received arguably less attention in the research community, but the related work section is detailed and well-structured, which especially helps the readers to catch up.
2. The method is very simple and easy to understand.
3. Experiments show promising results.
4. The authors promised to release full code upon acceptance.

Weaknesses:
1. The targeting application of this task is not clear. Why is this task important? Which data domains or scenarios do we want it to work? If our goal is just to detect small motions, we can develop optical flow estimation methods that work specifically for small motions. Even for existing state-of-the-art optical flow networks, detecting small motions is generally not a big issue, and it should not be hard to find a way to visualize small optical flow. Why do we need to generate a video? Maybe adding some application examples in the introduction and some results on related datasets will help the reader better understand the background and goal of this task.
2. There are still some confusions on the method. See questions below. 
3. Some minor edits. See additional comments below.

Limitations:
Maybe need to add occlusions as a key limitation

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces an optical-flow-based lagrangian motion magnification method, learned through self-supervised learning. The architecture is very simple -- just a U-Net that inputs two temporally consecutive frames and outputs a motion-magnified image. To train the U-Net, the method uses an off-the-shelf optical flow method, estimates the motion between two frames, and considers the motion as real motion of the scene. Then it penalizes the difference between the estimated magnified motion (constant * estimated motion) and the motion between the reference image and motion-magnified image (i.e. output image). The method demonstrates both good quantitative and qualitative results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Comprehensive related work

  The paper provides a comprehensive literature survey, which helps understand previous related work and where this paper positions among them.

+ Implementation details

  Sec. 4.1, Sec. 4.2, and supplementary material provide sufficient implementation details so that it's easy to understand the choices of hyper-parameters, training configuration, dataset curations, and training details. Parts of source codes are also included in the supplementary material, which all help reproduce the proposed methods. 

+ In-depth evaluation

  Fig. 5, Fig. 6, Table 2, and Table 3 provide in-depth evaluation of the proposed method and related methods on real-world and synthetic videos. Given that there is no public benchmark and the difficulty of evaluation on this topic, the paper tries its best on providing sufficient evaluation.

Weaknesses:
- How to handle occlusion and disocclusion?

  It seems already stated in the limitation section, but I wonder if the method doesn't explicitly handle occlusion and disocclusion. If it doesn't, then can it be a problem? Does U-Net learn to handle them to some extent? When watching the supplementary video, the model doesn't seem to output hallucinated appearance around the disoccluded region, which seems good.

- Worse SSIM in Table 3

  In Table 3, compared to DeepMag, all metrics are better except for SSIM. I wonder why it's the case. What makes the DeepMag's SSIM better than the proposed method?

- Moving background?

  In the supplementary video (1m14s and 2m12s), I am wondering why the background has motion and it's moving? Is it due to that the optical flow method hallucinates motion in the background and it's used during the training? Can this problem be resolved without using the target segmentation mask?
  By the way, this is another question: what if $L_{mag}$ in Eq. (3) is applied to the target segmentation objects only and penalizes the background motion to be zero? Can it produce better results and prevent the background from moving?


There are some unresolved concerns but the strength outweighs the weaknesses for now. I would like to give Borderline Accept for now, but the rating could change after the discussion phase.

---
All concerns are resolved. Thus I am updating my rating to 7. Accept.

Limitations:
- Probably another limitation would be that the method's success depends on the off-the-shelf optical flow and segmentation methods.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper uses the classical method of Lagrangian to self-supervise the task of motion magnification. Thanks to the proposed self-supervision-based technique, the proposed method can also be adapted during the test time. As shown in Figure.1, the proposed method is simple, where the optical flow vectors of videos before and after magnification are compared.  The optical flow of the motion amplified video is compared to the scaled (by the amplification factor) optical flow of the original video, to derive the magnification loss. To make the output video color consistent, color loss is also used. Videos are provided in the supplementary material for qualitative analysis.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1.	The method presented in this paper is simple, straightforward, and meaningful. 
2.	The experimental evaluations validate the proposed method. Supplementary videos are helpful.
3.	The source code is also provided in the supplementary, which further highlights the simplicity.
4.	Limitations of the method are well discussed, and failure cases are shown.
5.	The paper is well-written and easy to follow.


Weaknesses:
1.	The proposed method largely depends on the pre-trained optical flow network. 
2.	Given the nature of the addressed problem, its evaluation is known to be difficult. This is reflected in the experiments. 
3.	The experiments are conducted in a relatively small amount of video frames, and the paper discusses “out of the distribution” and “test time adaptation”. It would be interesting to see how the method generalizes when training on a large number of videos, before proceeding to discuss the rest.  


Limitations:
The authors adequately addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on learning a pair-wise motion magnification model in a self-supervised manner. The authors employ recent optical flow models to estimate the flow fields between the original and the motion magnified image pairs. The UNet concatenates a sinusoidal encoded magnification factor with the original images and generates the magnified image.

The learning process of the UNet is facilitated by a loss function that enforces consistency between the original and magnified flow fields, as well as the consistency between the backward warped images. To demonstrate the effectiveness of the proposed method, the authors curate a large-scale real-world training set. They conduct evaluations both quantitatively on a synthetic dataset and qualitatively on real-world data. The results demonstrate superior performance over previous supervised methods learned on synthetic data.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The paper is well organized and written. Sec.1 introduces the problem effectively and motivates the design of the method. Sec.2 provides a brief but comprehensive review of the previous methods. In addition to the overall organization, sufficient details (including the code) are given for a better understanding of the method, such as the footnote on page 5.

- The method itself is simple and effective:

  - A simple UNet is a compact solution that avoids complicated operations, e.g., explicit optical flow estimation and inpainting.

  - The magnification factor is concatenated with the input image pair after sinusoidal encoding, which enables regionally varying magnification. This is difficult for a single magnification factor as in [33].

  - The self-supervised learning losses enable online adaptation to a specific sequence for better quality.

- The evaluation is comprehensive and achieves significant improvement over previous methods.

- The curated large-scale real-world dataset will encourage further research in this topic.

Weaknesses:
I do not see significant weakness of the paper since it is simple and effective. The only missing piece I come up with is that since the model itself is simple, the author could make some deeper analysis of a learned UNet to understand the underlying mechanism of the model.


Limitations:
The limitations have been addressed adequately in the paper.

Rating:
7

Confidence:
4

";1
IiwTFcGGTq;"REVIEW 
Summary:
This paper studies the adversarial robustness of models on the unseen target domain. Firstly, exhaustive experiments show that existing OOD generalization methods are vulnerable to adversarial attacks. Then, a theoretical analysis for the OOD adversarial robustness is presented. Finally, this paper proposes two methods (AERM and RDANN) based on the theoretical results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The topic of this paper is novel and the paper considers two perspectives of trustworthy machine learning, which is very useful in practice. I agree with the authors that the adversarial robustness of the model on the OOD domain is very important since in the real-world task, the data we face always has a different style from what we use during the training stage.
- The theoretical analysis is strong and interesting. The authors provide theoretical analyses in two different settings (the average setting and the agnostic setting where the distance between the training domains and the test domain is unknown), which cover the practical cases according to how many domains we can sample from. I think the theoretical analysis of this paper is very comprehensive.
- According to the experimental results, the proposed methods significantly improve the OOD adversarial robustness. Furthermore, the observations on RotatedMNIST and ColoredMNIST are quite interesting and are consistent with the theoretical results of the two examples, which motivates researchers to consider the hardness of the tasks in OOD generalization. 
- The paper is well-written. The details of the methods are adequately explained, the proofs of the theorems are presented step-by-step, and the meaning of the theoretical results is discussed in detail.


Weaknesses:
- There are other datasets in the DomainBed benchmark, however, the experiments are conducted only in three datasets (RotatedMNIST, ColoredMNIST, and VLCS). Although AT is time-consuming and DomainBed involve running many groups of random parameters, I think the authors show add at least one more dataset in DomainBed.

Limitations:
No.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper first evaluates the adversarial robustness of current OOD generalization algorithms and shows that existing OOD generalization algorithms are vulnerable to adversarial attacks. Then, the paper presents a theoretical analysis of the adversarial robustness (of a model) on the unseen test domain. After that, the paper designs two algorithms to improve the OOD adversarial robustness inspired by the theoretical implications. Finally, extensive experiments are conducted and the improvement is significant.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
(1) This paper takes the first step to study the adversarial robustness of a model in the out-of-distribution generalization setting. As the extensive experiments show, existing out-of-distribution generalization methods fail under adversarial attacks, which means that it is urgent to study this problem. So, I believe that the studied problem is meaningful for the machine learning reliability community.

(2) The writing is good and the paper is easy to follow. On the one hand, the proof steps are so detailed that it is easy to check the correctness of the theorems. On the other hand, after the theorems, the paper provides some remarks to show the implications of the theorems to help the readers understand the messages that the theorems convey.

(3) The empirical results are strong. The improvement on the out-of-distribution adversarial robustness is significant. The experiments are extensive, and the details of the experimental settings are described in detail.

(4) The proposed algorithms are inspired by the theory, which means that they are guaranteed by the theory. The theoretical analyses are technically solid. This paper not only provides theoretical results in general settings (Theorem 2.1, Corollary 2.2, and Theorem 2.3) where the data distributions and the hypothesis class are not specified but also studies an intuitive example (Example 1, Theorem 2.4 and Example 2, Theorem C.1) that presents many intuitive understandings.


Weaknesses:
(1) The paper [1] seems to be very relevant to this paper, but the discussions are missing.

(2) The clean out-of-distribution generalization accuracy on VLCS decreases for both AERM and RDANN, however, this does not happen for ColoredMNIST and RotatedMNIST. The authors do not provide adequate discussion on this phenomenon.

[1] Ibrahim, Adam, et al. ""Towards Out-of-Distribution Adversarial Robustness."" arXiv preprint arXiv:2210.03150 (2022).


Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the adversarial robustness of OOD generalization algorithms. Authors claimed that existing OOD generalization algorithms fail in adversarial attacks (FGSM and PGD). To this end, authors first theoretically analyze the problem definition and generalization bound of OOD algorithms in adversarial settings. Then, they proposed two algorithms: adversarial ERM and robust DANN to solve the problem. Experiments on several datasets show the effectiveness of the algorithms.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The problem of adversarial OOD generalization is interesting and of great importance in real scenarios.
2. The theoretical parts offer valuable insights in the algorithm design.
3. The experimental results are showing the advantages of the algorithms.

Weaknesses:
I think this paper can be seen as a pioneering work in exploring adversarial robustness of OOD generalization algorithms, but unfortunately has many flaws:
- Lack of related work discussion. Although the discussion of adv. and OOD is rare, there are certainly some existing works on this intersection. For instance, [Yi et al'2021, ICML] theoretically studied the bounds of adversarial and OOD generalization, and they pointed that at mild conditions, adversarial robustness can guarantee OOD generalization (Thm 3.1). And [Alhamound'2022, arXiv] studies the generalization of adv. algorithms, which is of high interest to your work.
- The algorithms are intuitive and not novel. I highly appreciate the efforts in deriving the theories and then lead to the algorithm design. But these two algorithms are very simple and intuitive to design. For instance, the average ERM over several adv. distributions and average domain-adversarial training are easy to propose and can even be seen as baselines when one tries to solve the adv. OOD generalization problems. The only reason is that no one has explored this before. But to me, the two algorithms are very simple and not novel.
- The theoretical part is dense, which I give my appreciation. But this part lacks enough motivation and is hard to read. I strongly encourage authors to revise this to better align with the main topic. In addition, most of the theories are based on Ben-David et al's distribution gap theory and do not present much technical novelty.
- The experiments are lacking sufficient backbones and comparison methods:
1. Lack of various backbones in addition to ResNet50. Have you tried Vision Transformer? ViTs are known to be better cope with adv. robustness than CNNs.
2. Lack of other datasets. The largest dataset in this paper is VLCS, which is not enough for OOD generalization. You should at least utilize the DomainBed datasets.
3. Lack of adversarial comparison methods. Adversarial robustness is an active area and I'm sure most of them can be applied to OOD algorithms. Authors did not compare with any existing efforts.
4. Lack of interpretatbility. Why do the algorithms in this paper work? Or why do existing algorithms not work?



References:

[Yi et al'2021, ICML] Improved OOD Generalization via Adversarial Training and Pre-training.

[Alhamound'2022, arXiv] Generalizability of Adversarial Robustness Under Distribution Shifts.

Limitations:
See weakness

Rating:
3

Confidence:
5

REVIEW 
Summary:
The authors use statistical learning theory tools and the HdH divergence to motivate methods to improve the performance of a domain generalizing model dealing with adversarially perturbed, out-of-distribution domains. The authors evaluate two methodologies in that setting, and show how this can indeed defend models.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. While OoD generalization is mostly concerned with improving performance on out-of-distribution domains, there is not much work on evaluating the performance of OoD-trained models on adversarially attacking an out-of-distribution domain.
2. The two approaches studied are motivated with theory, and are shown to work well in that setting.


Weaknesses:
Fixing the following issues would lead me to accept the paper. They are related to toning down some claims and missing references.
1. Missing references and comparisons. First, the relevant work of “Adversarial Feature Desensitization” by Bashivan et al. 2021. E.g., Bashivan et al. already appear to apply HdH theory to estimate adversarial generalization error bounds, etc. Your setup has some differences in that you’re using pairwise HdH distances on perturbed domains, but it still needs to mention previous HdH approaches to adversarial robustness (and potentially factor that in when making the claims). Moreover, RDANN also echoes “Generalizing to unseen domains via distribution matching” by Albuquerque et al. 2019, e.g., they train on non-perturbed domains while you do, but the logic is quite similar; RDANN would be applying Albuquerque et al.’s approach to adversarially train the model. 
2. Please correct me if I am wrong, but AERM appears to just be adversarial training as done by Madry et al. 2018 (and Goodfellow et al. 2014 before). Adversarial training is done over all samples seen during training; it’s orthogonal to having multiple domains or a single one. I value that the authors provide both theoretical motivation for it and experimental results in the particular setting of adversarial robustness of a model trained for OoD generalization. I would however avoid calling the method itself “your” contribution, or giving it a new name; it’s just adversarial training = training on the adversarially perturbed training dataset (agnostically to whether it corresponds to one or multiple domains).
3. The benchmark/discussion of results/claims re: performance are quite strange, as they effectively compare undefended models with two defended models, so seeing an improvement in robustness is quite trivial. I suggest framing the results a bit better to avoid making it about improving over existing methods (of course it does, you’re using adversarial training), but rather discussing how adversarial training (your AERM) compares with the RDANN approach, and how on their own (instead of compared to undefended models), they provide robustness on OoD datasets that are adversarially perturbed. Interesting things with your results are that on RotatedMNIST and ColoredMNIST you do not suffer much from the usual drop in clean (= unperturbed) accuracy as you defend a model, compared to undefended models. 


**Typos and suggestions:**
1. I believe the phrase “OOD adversarial robustness” is more indicative/suggestive of robustness against diverse perturbations, which is an existing problem, than “adversarial robustness of models trained for OoD generalization” (in fact, a google/bing search appears to confirm this view, with e.g. “Towards Out-of-Distribution Adversarial Robustness” Ibrahim et al. 2022). For minimal effort, I suggest rephrasing to “adversarial robustness of OoD” if you want to keep it short, to clear up any confusion.
2. I suggest adding equation numbers. 
3. The equation below L250, $c_{ij}$ are introduced. I would write in the text what they correspond to (classifiers for pairs of distributions of features of perturbed domains).


Limitations:
Yes.

Rating:
6

Confidence:
4

";1
KtHquQuyA5;"REVIEW 
Summary:
This study focuses on the utilization of deletion diagnostics for modeling pixel relationships. The main contribution of this work can be divided into two parts. Firstly, the simplification of pixel relation modeling as object relation modeling. Secondly, the introduction of deletion diagnostics to enable networks to build object relationships by detecting changes in the outputs of objective functions. Experimental results qualitatively and quantitatively demonstrate the effectiveness of the proposed paradigm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The use of Deletion Diagnostics to update the relation matrix is a novel approach that considers the problem of category context in semantic segmentation from a different perspective than previous multi-scale driven and similarity-driven methods.

* Thorough ablation experiments were conducted to verify the impact of different designs of IDRNet on segmentation performance.

* The writing is clear and easily understandable for readers.

Weaknesses:
* The experiments of Mask2Former+IDRNet only report results on one dataset, ADE20K. As Mask2Former is a general segmentation framework, it is recommended that the authors also report the performance of the proposed model on the other three datasets for a comprehensive comparison.

* Additional experiments based on Mask2Former could be added to Table 2 to verify the effectiveness of IDRNet on a regular backbone.

Limitations:
There seems no discussion about limitations in the paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work presents IDRNet, a context aggregation scheme for semantic segmentation networks which is driven by intervention in the object query stage. The work is a follow up on OCR (object-contextual representations), but adds to the previous work by an intervention-based strategy to update, augment, or delete tokens. The updates to these tokens are based on semantic relations.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
Presentation:
The paper is clear in its presentation of the concept, and the result. Tables and figures are self-explanatory and it is easy to follow the though process of the authors. The reviewer appreciates the effort spent in explaining the concept of this work.

Results: 
As a scheme which is built to add to pixel representations, the results are quite impressive on five different datasets. The authors have also provided ablation studies detailing the effect of each separate block in the proposed algorithm.

Visulalizations:
The reviewer also appreciates the detail the authors have provided on viewing semantic relationships, which was the major motivation of their approach. Hence, it is safe to say that the motivation of the work and results go hand in hand.

Weaknesses:
Originality:
The concept of updating (deleting, augmenting, enhancing, ""diagnosing"") embeddings based on object relations might seem novel when you think of them in a convolution-based setting. However, I am afraid this concept is not original in transformer networks. Works such as Mask2Former do present queries which describe objects and the network learns (based on bipartite matching) to associate queries with the concept of object. There are also several other works [1], [2], [3] which prune, refine or update these tokens based on semantic relationships. 

Complexity:
As the method adds additional compute to the network, the authors mention how much compute is added and also, more importantly, is Baseline+IDRNet better than simply scaling up the baseline to match this compute.

[1] Revisiting Token Pruning for Object Detection and Instance Segmentation
[2] Sparse Tokens for Dense Prediction - The Medical Image Segmentation Case
[3] DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive
Segmentation Transformer

Limitations:
The authors have not addressed limitations of their method.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a context relation modeling method, which can be incorporated with existing semantic segmentation model. Instead of modeling pixel-level relation, it models class-level semantic relation by weighted averaging features at each location and deletion diagnostics. The proposed module is flexible and tested with different network backbone and segmentation architectures, which observes consistent improvements on several widely used datasets. Furthermore, the proposed context relation module can be applied in object detection, that stronger results on MS COCO are shown based on Faster-RCNN.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
+ This paper presents a context relation module, which can be combined with previous semantic segmentation models. It is used to enhance the learned features and perform features interaction with global co-cocurrence. Besides, in order to overcome the heavy computation issue, this work proposes to use semantic relation to replace pixel relation.

+ This paper conduct extensive experimental results, including 4 popular datasets. Besides, this work is combined with many previous segmentation models. For example, even though PSPNet can aggregate context information, the semantic relation module can further improve its performance.

+ Comparison with previous state-of-the-art demonstrates the effectiveness of the proposed module.

+ This work is also compatible with object detection framework. It should be also feasible to apply the proposed module in semantic instance segmentation.

Weaknesses:
- Too many symbols in section 3. For some details, I cannot follow.
In Eq 3, what is the output dimension for R_{sl} ?
In Eq 6, why is R_{esl} a Ne xZ matrix, as M_r is KxK ?
In Eq 8, what are the values in R_a if Y_c != k ?
In Eq 9, what does the \bigoplus operation represent ?

- Figure 5 is hard to see the graph connection, for example, the rider class. For sky, it is connected to truck, building, road, which does not seem to make sense.

- In Table 5, why not show the performance for Mask2Former+IDRNet on LIP/COCO-Stuff/PASCAL-Context datasets?

- Suggest to list all the comparing methods.

Limitations:
See weakness section.

Rating:
6

Confidence:
5

REVIEW 
Summary:
To further enhance the segmentation performance, the authors utilize deletion diagnostics to model pixel relationships. Specifically, to address the computational cost of pixel relation modelling, they simplify it as object relation modelling. Then, they introduce deletion diagnostics to facilitate the network in building object relationships. The experimental results qualitatively and quantitatively demonstrate the effectiveness of their proposed paradigm.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
To improve the contextual information aggregation，the authors propose a novel Intervention-Driven Relation Network (IDRNet), which leverages a deletion diagnostics procedure to guide the modelling of contextual relations among different pixels. This method brings consistent performance improvements to state-of-the-art segmentation frameworks and achieves competitive results on popular benchmark datasets.

Weaknesses:
1. The paper lacks the presentation of additional time-consuming and memory-consuming aspects associated with the Intervention-Driven context module.
2. The paper lacks interpretability regarding how the Intervention-Driven context module improves contextual information aggregation.
3. Some minor issues: confusion in the writing, some symbols used in equations or throughout the paper are not adequately explained. 

Limitations:
There is no limitation claimed in the paper. For example, the cost of the Intervention-Driven context module, particularly in terms of training time and storage, remains unknown.

Rating:
4

Confidence:
3

";1
pH4Fv7C3yC;"REVIEW 
Summary:
This paper considers two problems: 1. Finding the most probable graph that makes a desired causal query identifiable. 2. Finding the graph with the highest aggregate probability over its edge-induced subgraphs makes a desired causal query identifiable. This paper shows that both problems reduce to a special combinatorial optimization problem called the edge ID problem. They prove that the edge ID problem is NP-hard, implying that both of the problems are also NP-hard. The paper presents several exact and heuristic algorithms for these problems and evaluates their performances through different experiments. The experiments note that the heuristic algorithms performed remarkably well across all metrics. The paper also discusses the application of these algorithms to four real-world datasets.







Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper addresses a complex causal inference problem, specifically identifying the most probable graph that makes a desired causal query identifiable. This is a significant contribution to the field.

- The authors provide a detailed analysis of the problem's complexity, demonstrating that it reduces to a special combinatorial optimization problem, the edge ID problem, which they prove to be NP-hard. This rigorous theoretical analysis is a strength of the paper.

- The authors propose several exact and heuristic algorithms to solve the problem and evaluate their performance through different experiments. The heuristic algorithms, in particular, performed remarkably well across all metrics, demonstrating the practical applicability of the proposed methods.

- The paper also applies the proposed algorithms to real-world datasets, further demonstrating their practical utility. The authors provide a detailed comparison of runtimes, solution costs, and failure rates, offering valuable insights into the performance of the algorithms in real-world scenarios.

Weaknesses:
- The authors made the assumption that the edges in the graph are mutually independent. This assumption may not be held in all cases.

- The external validity of the derived subgraph is not guaranteed. This means that the subgraph may not be correctly specified with respect to the corresponding real-world process. This could limit the applicability of the results in practical scenarios.

- The EDGEID algorithm, one of the exact algorithms proposed in the paper, had large runtime variance, which depended heavily on the specifics of the graph under evaluation, particularly for graphs with fewer vertices. This could limit its utility in certain scenarios.

- The EDGEID algorithm also timed out on all but one of the real-world structures tested, indicating that it may not be as consistent as other algorithms in terms of runtime.

- The runtimes for the MCIP variants exceeded the HEID variants due to the required transformation. This could be a potential drawback in scenarios where computational efficiency is a priority.

Limitations:
- The authors made the assumption that the edges in the graph are mutually independent. This assumption may not hold in all cases, and the authors themselves acknowledge that future work should explore scenarios where this assumption does not hold.

- The external validity of the derived subgraph is not guaranteed. This means that the subgraph may not be correctly specified with respect to the corresponding real-world process. This could limit the applicability of the results in practical scenarios.

- The EDGEID algorithm, one of the exact algorithms proposed in the paper, had large runtime variance which depended heavily on the specifics of the graph under evaluation, particularly for graphs with fewer vertices. This could limit its utility in certain scenarios.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper is about causal identification in a setup where the assumption of reliable knowledge of the causal graph is relaxed. The authors assign a probabilistic weight to the possible confounders.  The combinatorial task of finding the most probable causal graph such that a given causal query is identifiable is considered. The same probable with an aggregate probability over the induced subgraph is also considered. Not surprisingly, both these problems are hard. The authors first deliver an exact solution. Two approximate procedures are obtained: (i) a recursion calling min-cut; (ii) a reduction to MCIP. Experiments on real and synthetic data are finally discussed.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The idea of an uncertain causal network with distributive semantics looks very reasonable.

The hardness results are not surprising, but they appear important for properly characterising the problem.

The heuristics presented are non-trivial and effective.



Weaknesses:
- Both heuristic algorithms lack a worst-case complexity characterisation.
- In the case of MCIP, it needs to be clarified whether the polynomial reduction considered by Proposition 4.2 can be implemented in practice.
- Problems 1 and 2 are not necessarily the only/best ways to cope with uncertain causal networks. No robust discussions to advocate such a choice are reported.


Limitations:
I don't see relevant issues on this point.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers a setting in which we have a probabililty distribution over causal graphs. This paper considers the problem of finding the most probable subgraph in which a given query is identified, and then that subgraph with the highest sum of probabilities of its own subgraphs in which the query is identified. (The latter problem is significant since the result of the identification must agree among all these subgraphs.) A reduction is given to a known NP-hard problem for which various algorithms are evaluated empirically.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written and the results are clearly set out, extensive, and correct as far as I can tell.

Weaknesses:
I had a little bit of difficulty understanding the problem initially. I am currently a little dubious about the paper's ultimate significance, but if the authors can make the links to structure learning more explicit then it will represent an interesting synthesis with the other approach in which ADMG encapsulates ""a priori"" facts about the causal structure that are known for certain.

Limitations:
Yes, they have (for instance, the assumption of independence between edges).

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper studies the problem of causal identification in a setting which the structure of a causal graph (of interest) is probabilistically uncertain i.e., only known per a certain degree of belief or with a degree of confidence of a particular statistical set, asking the most likely subgraph in which causal effect identifiable. In doing so, paper reduces it the combinatorial optimisation problem i.e., edge ID (as they call it) whose computational complexity is shown to be NP-hard through a reduction from minimum-vertex cover problem. Authors also introduce exact and approximate algorithm and show empirically that the algorithms work good as well. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Deep theoretical paper.

- The theoretical results  are complemented with good empirical evaluation. 

 - Good exposition. 

Weaknesses:
I did not spot any major weakness. Only very few minor issues:

- causal query better be formally mentioned in *the introduction* (i.e., how does it look like formally)
-problem To be surmountable (missing dot)
-In Figure 1 caption, also mention the longer version of Q[y] to help following it. 
-Definition 2.2: explain what you mean when you say functional (again formally). 
- (potential typo): In Example 1, I sense the direction of the subset to be other way around  Gˆ⊆G1
-Assumption 2.1: form (1) -> Equation 1
-In the next [s]ection

Limitations:
I did not see any particular limitation. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors consider the problem of causal effect identification assuming the given causal graph G is probabilistic where each edge (directed or bi-directed) is associated with a measure of confidence. Under this setting, the objective is to identify a subgraph of G with the highest plausibility such that the target causal effect Q[Y] is identifiable. The authors propose an exact algorithm that is asymptotically exponential, two heuristic approaches, and a reduction to an NP-hard problem that has approximation algorithms. Finally, they evaluate the proposed methods on synthetic and real-world graphs.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The formulated problem of identifying the most plausible subgraph where a target effect is identifiable is algorithmically interesting and the author highlight its complexity by showing it to be NP-hard. Moreover, the paper is well-written, for the most, with examples and discussions to illustrate the problem (i.e., Section 2.1). The experimental evaluation of the heuristic algorithms shows favourable performance.

Weaknesses:
1- Motivation for the problem: Whenever the target causal effect is not identifiable in the full graph G, it is not justifiable why we would drop edges from G just to make the effect identifiable; an alternative to exact identification could be bounding the target effect. The authors can provide more motivation in the introduction to justify their approach.

2- Lack of performance guarantees for the heuristic approaches: There is no theoretical analysis for the heuristic algorithms discussed in Section 4.2 in terms of upper bound on the cost.

Limitations:
Yes

Rating:
6

Confidence:
4

";1
QezJbfW01r;"REVIEW 
Summary:
This paper presents extensive background on ex-post-privacy as well as some new analysis providing a new method for leveraging this framework for the design of differentially private interactive protocols. Finally, this framework is instantiated in a method for releasing private counts while targeting an accuracy constraint.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
* The theoretical framework presented is quite general and seems to be very strong.
* Though relaxed notions of privacy are used internally in mechanism design, the main results here are stated for more tradition $(\epsilon, \delta)$ DP.
* The power of the theoretical framework _could_ conceviably lead to quite substantial concrete implementations and applications. 

Weaknesses:
* The generality of the analysis yields an associated weakness: for the vast majority of the main body, the precise methods of application are somewhat unclear. E.g. it is not obvious until the end of the paper that we will not literally be guaranteeing that all of our released values are withing percent, which makes the design of interactive protocols which use this style of mechanism + composition an important and more or less unstated problem space.

* The point above can be restated in more direct terms: if the authors wish to see this analysis and framework have a big impact, it is probably still to much of a lift to ask the community to fill in the gaps between the abstract presentation which dominates here and concrete algorithmic implementations.


Limitations:
The major unaddressed limitation is the potential difficulty of designing effective algorithms which leverage this perspective and analysis. That is, it is not entirely clear how fruitful the analysis presented here will be. This paper _could_ be _extremely_ strong with some reasonable extensions, perhaps as a theory paper with associated 'mechanism design paper' that provides many instantiations of this theoretical framework to perform various privacy sensitive tasks. For this reason, the paper reads as somewhat borderline to me--potentially pointing the way to serious applications, but not quite closing the gap entirely yet.

Negative social impacts not immediately applicable.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper tries to tackle the following basic question in differential privacy: suppose I desire to answer as many as possible counting queries with low **relative error** with a predetermined privacy budget (\eps,\delta). What is the best mechanism I can use?

Building on prior works on ""ex-post"" private mechanisms (Whitehouse et al '22), this paper makes the following contributions:
(1) An improved analysis for ex-post privacy mechanisms (in particular the Brownian Noise Reduction Mechanism);
(2) A ""Privacy filter"" which allows the user to compose concentrated-DP mechanisms and the Brownian mechanism adaptively.
(3) An experimental section, demonstrating the improvement of the proposed algorithm, compared with the baseline approach of ""doubling"". It appears that, compared with the baseline approach, the new algorithm can answer twice as many queries with the same privacy budget and accuracy requirement. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The new analysis for the Brownian mechanism looks nice. It can potentially make the technique more useful.
* The experiment setup is thoroughly described, and the improvement looks significant.

Weaknesses:
* It looks like the composition for ex-post mechanisms is omitted in the main body. However, you claimed this as one of your main contributions.
* Why only analyze the composition of concentrated-DP mechanisms with Brownian mechanisms? This seems to make the theory less general.
* From the practice side, this paper only runs experiments on synthetic data sets, which might not be convincing enough to a broader community.


Limitations:
NA

Rating:
5

Confidence:
3

REVIEW 
Summary:
‘Accurary-first’ mechanisms try to achieve the strongest possible privacy guarantee that satisfies a target level of accuracy. One such example is a noise reduction mechanism, which scales up the privacy cost / decreases the noise level until the target utility is achieved. Noise reduction mechanisms only pay the privacy cost of the final noise addition, but provide ex-post privacy guarantees whose composition bounds are unknown and which are not compatible with DP. This paper provides composition bounds for ex-post DP, and develops a unified privacy filter that can switch between DP and ex-post DP mechanisms subject to an overall privacy guarantee.

Soundness:
3

Presentation:
1

Contribution:
4

Strengths:
1. Very interesting and strong potential for impact. Seems like it could be a big step towards making DP more practical!
2. Novel privacy filter and analysis of the ex-post privacy guarantee for a Brownian filter.

Weaknesses:
The paper was very dense and difficult to read, which I feel limits its appeal. (Brownian motion / the Brownian mechanism, for example, are never really explained.) I think the paper would benefit greatly from being gentler to its readers.

Limitations:
No issues.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper focuses on the study of adaptive compositions of ex-post differential privacy, where privacy guarantees depend on the outcome of the algorithm. The utilization of ex-post DP enables analysts to achieve a more favorable privacy-utility trade-off. The paper introduces a tool for composing ex-post DP and traditional DP, providing a unified privacy filter that combines the Brownian Mechanism with traditional DP mechanisms. The authors apply the proposed framework to the task of releasing counts with bounded relative error and demonstrate empirical improvements over existing methods.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The propose framework could be useful for DP practitioners to compose traditional DP and ex-post DP mechanisms. The proposed methods provide DP guarantees subject to outcomes satisfying strict accuracy requirements.


Weaknesses:
The presentation of this paper is not very clear to me. The first listed contribution seems to be the general composition theorem for ex-post differential privacy; however, it is only mentioned in the appendix. In the main text, I noticed a list of theorems, but there seems to be a lack of intuition provided, such as explanations on why these bounds are tight.

Limitations:
This paper is theoretical and does not have potential negative societal impact.

Rating:
7

Confidence:
2

REVIEW 
Summary:
One of the challenges any attempt to implement DP preserving techniques, is the inherent tradeoff between the level of guaranteed privacy and accuracy levels. While most of the analysis done by privacy experts focused on the perspective of maximizing accuracy subject to an upper bound on the privacy loss, many data scientists might prefer the inverse perspective, e.i., minimizing privacy loss subject to a lower bound on accuracy.
This latter perspective was first formalized by [1], who referred to it as ""accuracy first"". They proposed a mechanism that releases a sequence of responses with increasing accuracy and decreasing privacy levels, which - somewhat surprisingly - pays only for the privacy loss of the last released response, which they analyze using ""ex-post privacy"". Later [2] extended these results to Gaussian noise as well using the Brownian mechanism, but since they did not rely on the classical DP notion, they lacked a composition guarantee.

The authors of the current work resolved this issue, by leveraging the privacy filter toolkit, in which privacy parameters can be adaptively chosen, under the constraint that they will not exceed some pre-defined threshold. Specifically, they use a zCDP filter for the Brownian mechanism which can be used for composing several calls to the mechanism as well as other zCDP mechanisms. They use this technique to analyze a setting where the accuracy goal is defined in terms of relative error rather than additive error, and use empirical evaluation to asses the accuracy improvement achieved by applying their analysis technique.

[1] K. Ligett, S. Neel, A. Roth, B. Waggoner, and S. Z. Wu. Accuracy first: Selecting a differential privacy level for accuracy constrained erm.

[2] J. Whitehouse, A. Ramdas, S. Wu, and R. Rogers. Brownian noise reduction: Maximizing privacy subject to accuracy constraints.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
Providing composition guarantees for ex-post privacy is an important step towered utilizing the accuracy first perspective in the real world, which can help to further extend the usage of differential privacy techniques in the real world. The background and intuition are clearly presented, the results in the body of the paper are formally stated, and the proofs are sound.

Weaknesses:
While the results presented in this paper are not just a simple application of the privacy filter technique to the ex-post privacy framework, as discussed in section 5.1, the novel results are somewhat limited in their scope, as they apply only to the zCDP privacy measure and the Brownian mechanism. The empirical evaluation is limited to a specific use case, and a single $(\epsilon, \delta)$ tuple (with $\epsilon = 10$ which is higher than what is usually considered private).

In the introduction, the authors also claim to provide a basic composition theorem for arbitrary ex-post privacy mechanisms, which appears only in the supplementary material. Putting aside the fact that the result does not appear in the body of the paper, and the fact the supplementary material is not comprised of appendix but is an extended version of the paper which makes it hard to find the additions, the actual claim (Theorem 3) is somewhat hard to parse, and could probably benefit from removing the zCDP part.

Small minor comment: It seems like the two references to Whitehouse et al. papers in the abstract were mixed up. The first one should've been the 22 paper and the second the 23.

=======

**Edit after rebuttal discussion:**

The authors have addressed my concerns.

Limitations:
N/A

Rating:
6

Confidence:
5

";1
f0Jj3C3Pnp;"REVIEW 
Summary:
This paper proposes a two-phase learning framework called HubRouter for global routing in chip design. Different from previous works that directly generate routes from chip images, which potentially cause inconnectivity, this paper proposes to generate hubs representing tiles in the first phase and then construct RMST with hubs by an actor-critic model. The experimental results show the effectiveness in terms of higher correctness, and shorter WL compared with SOTA model. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:

1. the paper firstly proposes hubs for the global routing task in chip design, transferring the pin-pin problem into hub-pin problems, which can passively avoid the in-connectivity problem when regarding the routing design process as image generation.

2. When dealing with large chips, HubRouter is the best performer among SOTAs regarding both correctness and wire length.

3. The proposed model can have good scalability with GAN or VAE as their generative model for the hub generation phase while maintaining the property of making wire length short and overflow less.

4. In the hub-generation phase, besides hubs, it also generates routes and masks to avoid noises brought by the generative model itself since the noises can greatly impact the results. Especially, the stripe mask can be greatly helpful for complicated cases.


Weaknesses:
1.	The overflow still exists for those generated global routing even though the proposed model reduces overflow better than PRNet.
2.	Even though HubRouter can get a good performance on replicating the known facts, it does not discuss the quality of the generated routes regarding congestion, and it possibly cannot generate novel routes since it has limited knowledge of routing design.

Limitations:
The authors discuss the limitations of the lack of training data about hub generation, and the model is not an end-to-end model. Still, it also should discuss how the ground truth comes and the potential impact of adopting such methods when generating it. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper focuses on the generative global routing tasks and mainly ensures the connectivity of generated routes via a two-stage framework. In the first phase, the approach involves a typical generative task, which exploits multi-task learning to promote the generation quality and utilizes a trick called stripe mask to decrease some redundant noise points. In the second phase, the work is formulated as an RSMT construction problem and addresses this problem by REST. The authors show that with correctly generated hubs, the RSMT construction can be solved with less time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ The structure that generates hubs first and then connects them with pins is novel and interesting, and is reasonable to guarantee the connectivity of routes in the global routing. 
+ The motivation is clear, and the authors show in Table 2 that the so-called `unconnectivity’ caused by existing generative global routing algorithms (PRNet) is severe, but I would have preferred the authors to also refer to it in the introduction part to strengthen the motivation.
+ The proposed approach performs better than other generative global routing algorithms in several metrics, especially the connectness rate is 100% and the running time of HubRouter (GAN) is much less than PRNet (GAN).
+ The authors also give some applications other than global routing to show the generality of the proposed approach.

Weaknesses:
- The approach is clearly divided into two different phases, but the running time shown in the experiment seems to be combined. The authors could show both generation time and connection time to display the time overhead in either phase.
- Some possible typos: The $r_{(n+1)j}$ should be $r_{(m+1)j}$ in Definition 1.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates the issue of global routing in VLSI systems and introduces HubRouter, a method that initially generates hubs and subsequently connects them to pins. In the first phase, the authors explored different generative models. In the second phase, the authors employs an actor-critic model to generate a final routing. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow.
2. The authors conducted many experiments under different baselines.


Weaknesses:
1. The two phases are independent, meaning that the feedback from the second phase does not influence the hub generation, potentially leading to suboptimal results.

2. The authors tried three generative models (VAE, DPM and GAN) in the paper. But the paper lacks clarity on which model should be used in specific situations.

3. The performance improvement achieved is marginal.

Limitations:
Overall, I believe that the integration of the generative model and reinforcement learning algorithm could be more elegant, potentially leading to further performance improvements.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper presents a new two-phase learning approach, called HubRouter, to address the issue of unconnectivity in the generated routes of global routing (GR) tasks in VLSI systems. It has two steps. Firstly, a deep generative model generates a 'hub,' which acts as a key point in the route; then secondly, HubRouter involves an actor-critic model-based RSMT construction module to connect the hubs. This shift from a pin-pin connection to a hub-pin connection method solves the unconnectivity problem in generative approaches. The HubRouter system ensures all generated routes are connected, eliminating the need for time-consuming post-processing. Experimental results show that HubRouter outperforms other state-of-the-art generative global routing models in wirelength, overflow, and time efficiency. It also finds application in RSMT construction and interactive path replanning, demonstrating its versatility.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper introduces a novel approach, HubRouter, to global routing, proposing a hub generation and hub-pin connection scheme that effectively addresses the challenge of route unconnectivity.

The experimental results show HubRouter outperforming existing generative global routing models in terms of wirelength, overflow, and time efficiency.

The approach is very general. The authors show that the approach can also be applied to RSMT construction and interactive path replanning, demonstrating the versatility of their method.


Weaknesses:
The challenge of connectivity problem is unclear. Specifically, it introduces a novel approach to tackling the unconnectivity problem in global routing, fails to establish the significance and relevance of this problem adequately.  It is helpful to elaborate more about the difficulty and significance of connectivity problem.

The effectiveness of the second phase is dependent on the quality of the hubs generated in the first phase. If the generative models do not create effective hubs, the entire approach could be compromised.

Limitations:
None

Rating:
5

Confidence:
2

";1
zR6V9fPRBn;"REVIEW 
Summary:
This paper presents a Shapley value based cohort discovery, by constructing ""Negative Sample Shapley Field"" that possesses isotropy property. By doing so, negative samples can be effectively clustered and separated with respect to the Shapley values. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
I think this paper points out many important problems in healthcare research. Specifically, 
1: how to deal with pos/neg imbalance and how to make better use of vast negative samples?
2: how to identify negative samples that are more useful for the current research problem? 

The use of latent variable models to deal with misssingness in EHR dataset is a promising approach too. 


Weaknesses:
1: Correct me if I'm wrong, but I think Eq.2 is ill-defined. For any metric M, let's say accuracy, then $s_i = s_j$ as long as both negative samples have the same predicted labels by a predictor F, right?
2: Many parts need justifications. For example, a) why is the defined metric in Eq.2 means high contribution to prediction task? b) why does Eq.6 help with isotropy? c) How is k-th DAE different from a normal encoder with k layers? 
3: I don't see why DBSCAN + AE is proposed as a contribution when you can simply use VAE. 
4: I doublt the logic between line 248 and line 249. The defined Shapley value in Eq.2 is 0 does not mean these patients are healthy. Note that you are defining M=AUROC, therefore M has a very stable value when you have sufficient samples to draw a smooth ROC curve when calculating $M(D^+ \cup A)$. As a result, $s_i = E[ M(D^+ \cup A \cup d_i) - M(D^+ \cup A)]$ is usually zero.


Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
The paper addresses the cohort discovery problem for supervised learning in the machine learning for healthcare domain. Positive examples of the cohort are easy to identify while it is not as straightforward to determine which negative examples should be admitted into a cohort.  To deal with this problem, the paper calculates the data Shapley value of the negative samples in the dataset. Then, the paper carried out representation learning using a stacked denoising autoencoder to mitigate the nonuniform changes of Shapley value in the original feature space. Finally, the paper carried out clustering in the learned representation space to identify important negative examples to create the cohort. The paper evaluated the proposed method on a clinical dataset to demonstrate the utility of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The paper deals with the cohort discovery problem, which is an important problem in the machine learning for healthcare domain.

* The proposed method is straightforward and makes sense to me for the most part.

* In terms of empirical evaluation, the paper provides a detailed explanation of the cohorts discovered from a clinical perspective, although I won't be able to judge whether such findings make precise, clinical sense given that I do not have a medical background. I also appreciate the authors carried out additional experiments to study the effectiveness of each component of the proposed method.

Weaknesses:

* Because the proposed method is straightforward and directly takes advantage of existing methods, I am not quite sure whether the paper has enough technical novelty from a machine-learning perspective.

* Regarding experiments, while I think the authors dive deep into providing an analysis of the outcome of the proposed method from a clinical perspective, there are no alternative methods compared to the proposed method to understand the performance of the proposed method. It would also be interesting to see the proposed method applied to more than just one dataset as discussed in the paper. Finally, it should also be noticed that identifying relevant negative examples is not a problem that is exclusive to the healthcare domain. Many application domains will be interested in the proposed method to identify relevant negative examples for binary classification problems. As such, the authors may also consider applying their methods beyond the medical domain down the road.

* Clarity of the paper can be improved. Some key concepts are not well explained. For example, what is the role of data Shapley value? It appears to be the contribution of a data point to the learned classifier. The authors do not seem to elaborate on this concept enough in the paper. What's the intuition behind it? Why it makes sense to use Shapley value to measure contribution?  I also don't think the authors explain well the phenomenon of ""the non-uniform distribution of negative samples with similar data Shapley values"". Further intuition on this point will help to better motivate the need for representation learning.



Limitations:
The paper mentions some limitations related perspectives in the conclusion section.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper describes a method to understand the set of unlabelled / negative samples in a healthcare data-set. In this setting, one typically has a set of patients with a particular label, such as indicidence of a particular disease, and a large set of unlabelled samples. Training a classifier involves selecting some subset of the unlabelled samples as the set of negative samples for training. This set is often quite heterogeneous, so methods that enable better understanding of the structure in the data and selection of negative samples can be informative. 

The key contributions are as follows:
1. Definition of the Negative Shapley Value Field, which associates the Shapley Value for the the prediction task of interest with each negative sample
2. Illustration of a representation learning method which discovers a low-dimensional representation for the negative samples in which samples with similar Shapley Values are close to one another
3. A method for cohort discovery based on clustering in the low-dimensional space and interpretive analysis to demonstrate the clinical coherence and relevance of the discovered cohorts
4. Demonstration of improved predictive performance when selecting samples based on the negative Shapley value
5. Demonstration that predictive performance is maintained when the low-dimensional representations are used

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The problem of understanding and selective negative samples for cohorts in healthcare data is an important one and methods that can be used by practitioners in this domain will be valuable. 

The authors provide a demonstration of the utility in improved classifier performance when limiting to the set of negative samples with Shapley value > 0. This is suggested to be due to those samples which have negative Shapley value corresponding to patients who are likely to present with AKI in the future, but do not yet have this information in their medical record. This is a nice result and tackles a common problem in biomedical data science. 

Mapping the unlabelled samples into a low-dimensional space where samples with similar representations are expected to have similar Shapley values is shown to enable the discovery of distinct and interpretable cohorts of samples with similar features and similar Shapley values. This result could provide a useful tool for practitioners to select or filter the set of negative samples when building classifiers on EHR data

Weaknesses:
*** These weaknesses have been addressed in the author response ***

The ""Effectiveness of the Negative Sample Shapley Field"", described in lines 316-322, is illustrated by showing that filtering the set of unlabelled samples to exclude those which had a negative Shapley value improves the performance of the trained classifier on held-out data reminded me of co-training [1] or positive-unlabelled learning [2]. It would have been interesting to see this approach benchmarked against other methods for developing classifiers based on positive and unlabelled data, where we expect a number of the unlabelled samples to be positive rather than negative samples

The ""Effectiveness of Cohort Discovery"", described on lines 332-345 is a nice result but it is not clear from the experiments to what degree the isotropy constraint enabled this. This could be demonstrated by an experiment in which the same SDAE model is applied to the data  without the isotropy constraint.

[1] Blum, A., Mitchell, T. Combining labeled and unlabeled data with co-training. COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann, 1998, p. 92-100.
[2] Bekker, J., Davis, J. Learning from positive and unlabeled data: a survey. Mach Learn 109, 719–760 (2020). https://doi.org/10.1007/s10994-020-05877-5

Limitations:
*** These concerns have been addressed in the author response ***

This is an interesting paper with some results which could be useful in biomedical data science, but which would be made more convincing by more thorough benchmarking and comparison with other approaches to achieve each of their key results.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In healthcare analytics, cohort constructions is one of the key steps that drives the analysis. For most problems, where the outcome of interest is a disease, the problem has asymmetrical formalism - while patients with disease are defined using string criterion and are homogenous w.r.t problem the negative set can be diverse and can have important information that is under-analyzed. The authors present a Shapley value driven approach to analyze the negative set in terms of their contribution to the predictive power of the models. Furthermore, these mappings are transformed and clustered to identify potentially clinical important patients. They have presented results and commentary from clinicians on identified clusters.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The authors raise an interesting hypothesis about under-analysis of the negative samples that can drive the community to develop standard methods to handle such problems 
- It is commendable that the authors validated their findings and presented commentaries from clinicians on the identified patterns. Such efforts are increasingly important to ascertain the clinical validity of proposed AI methods
- Overall, the intuition behind the method is novel and somewhat defensible. The authors have also made an effort to formalize many aspects of their approach 
- The authors have also made an effort to validate the components of the method individually (see more on this below)

Weaknesses:
- The primary weakness of the paper is a lack of comparison against baseline methods that necessitates the complexity of the proposed methods. There is also a lack of studying the correctness of the proposed cohort discovery method. It may be beneficial for the authors to support their claim on a synthetic datasets and/or provide comparisons of discovered cohorts using other standard methods such as contrastive PCA. 
- Continuing from the above, the computational complexity of the proposed approach hasn't been acknowledged in a satisfactory manner. While Monte Carlo methods have been proposed to calculate the values, the true complexity in evaluating over the entire negative set and the subsequent calculations imposed the isotropy constraints hasn't been analyzed clearly. 

Edit: The authors have responded by providing additional baseline comparisons that alleviates some of the concerns. I have updated my review to reflect the same

Limitations:
N/A

Rating:
5

Confidence:
5

";0
GfZGdJHj27;"REVIEW 
Summary:
This paper introduced Martingale Property (MP) in diffusion model and the training strategy to apply this property to modify the score network outputs. In short, the martingale property states the condition that the SDE trajectory conditioned in a sample at time t is equal to the denoising function. After showing that MP transfers the score functions of small t to that of large t, this paper adds additional loss term that considers MP that the denoising function at time t' to be equal to that at time t, where t' < t. With this loss term added with the EDM model, the image generation quality in terms of FID is improved.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
* This paper introduces a unified technique to improve the diffusion model with simple fine-tuning with short SDE trajectory starting from noisy data. Training with this new loss improves the sampling performance with some margin for CIFAR and AFHQ datasets.
* Compared to the concurrent work [1] that focused on the small-NFE performances, this directly learns ""better"" score function by reducing the learning variance, where the variance between x_t' and x_t is much smaller than that of x_0 and x_t.

Weaknesses:
* The FID performance gain is small, considering the additional training budget consumed. 
* Theoretically, training solely with MDM loss term should show fair generation quality, since when the MP is satisfied, the model should be learned properly. However, the performance became worse, as Table 2 have shown.
* In the date of submission, there is a concurrent work [1] that also modifies the score network output by distilling knowledge from less noisy data.
* The second term in the equation after line 198 is not yet considered. Adding (even after some approximation) this term may further improve the method.

[1] Y. Song et al., ""Consistency models"", https://arxiv.org/abs/2303.01469

Limitations:
The authors adequately addressed the limitations of this paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Using approximate score functions and discretization can lead to compounding distribution shift as samples drift toward less likely regions of the training distribution. The authors propose to address this problem by enforcing that the learned denoiser satisfies an invariant they call the Martingale Property: $E_{gen} [x_0 | x_t = x] = h(x, t)$, where $h$ is the optimal (true) denoiser. The authors prove that if the learned denoiser satisfies MP and the score function is conservative, then there exists some underlying corresponding diffusion process; moreover, uniqueness holds. This implies that if in addition the learned denoiser matches the optimal denoiser at any time in a small region of the spatial domain, then the learned and optimal denoisers must match everywhere. Empirically, an extra loss term encouraging MP is added to the standard score matching loss, and results suggest this improves generated samples.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The presentation of the theoretical results is clear.
- Experimental results are promising. Although MP is expensive to enforce exactly, the experiments demonstrate that the proposed MP loss is effective in practice.

Weaknesses:
- The main theorem (Thm. 3.2) does not seem to be very actionable. The results assume that MP holds exactly, which is an unreasonable assumption when considering diffusion model training in practice.
- Some of the theoretical results seem to have limited novelty. For example, uniqueness (Lemma A.9), which underpins the second claim of the main theorem, appears to be a standard argument.
- The experiments are promising but somewhat limited. Results are reported for only two datasets (including CIFAR10 cond vs. uncond) and the benefits of the MP loss is not very clear.

Limitations:
It could be helpful to clarify the relationship between MP and the consistency property from [1]. In practice, does enforcing the martingale property in expectation along generated trajectories behave similarly to the consistency loss, which is enforced pointwise along ODE trajectories?

Rating:
5

Confidence:
2

REVIEW 
Summary:
Diffusion models are trained to denoise images that have been generated using a particular schedule of additive Gaussian noise; however, when sampling from a trained diffusion model, one applies a chain of denoising steps, each one operating on the output of the previous step. This may lead to sampling drift, where the distribution of the images at intermediate denoising steps differs from the distribution encountered during training. This sampling drift is hypothesized to be detrimental to the sampling process.

This paper proposes an approach to mitigate this drift, by enforcing a Martingale property (MP), which states that the denoising function $h_{\theta}(x, t)$ for all $t \in (0, 1]$ and $x \in \mathbb{R}^d$ outputs $h_{\theta}(x, t) = \mathbb{E}[x_0 \mid x_t = x]$, where this expectation is over the reverse diffusion process that starts from noisy example $x_t$ and runs the diffusion SDE, which applies the learned denoiser $h_{\theta}(x, t)$. They show that to enforce the MP, $h_{\theta}$ must satisfy a consistency property $h_{\theta}(x, t) = \mathbb{E}\_{h} \left[ h_{\theta}(x_{t'}, t') \mid x_t = x \right]$ for all $t > t'$, where the expectation is over all $x_{t'}$ sampled using the reverse diffusion process with denoiser $h_{\theta}$. In addition to the consistency property, they require the boundary condition $h(x, 0) = x$, such that at time $t=0$, the denoiser simply outputs the original input.

In order to encourage a denoiser to satisfy the MP, the authors propose a loss function that takes two successive points along the reverse diffusion trajectory, $x_t$ and $x_{t'}$, where $t'$ is slightly smaller than $t$, and minimizes the squared error between the denoiser output on those two points, $\mathcal{L}\_{t, t', x}(\theta) = \frac{1}{2} \left( \mathbb{E}\_{\theta}\left[ h_{\theta}(x_{t'}, t') \mid x_t = x \right] - h_{\theta}(x_t, t) \right)^2$. This term acts as a regularizer (with a weighting coefficient $\lambda$) on top of the standard denoising objective.

The paper presents results on several real-world datasets, including CIFAR-10, AFHQ, and FFHQ. Overall, it is not completely convincing that enforcing the Martingale property is worth the effort (of computing the datapoint pairs $(x_t, x_{t'})$ and potentially tuning the contribution of the new loss term). The results do not seem to improve dramatically when using the MP term.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The paper addresses an important problem with good motivation, to mitigate drift when sampling from a trained diffusion model. The idea to enforce the Martingale property is very nice, and the authors develop this idea into a practical method that adds an additional loss term to the standard denoising objective.

* The proposed approach is theoretically justified, and fairly intuitive. The authors give a thorough discussion of the necessary assumptions and theoretical properties of their approach.

* The authors apply the Martingale property regularizer to train diffusion models from scratch as well as to fine-tune pre-trained diffusion models. They evaluate the approach on three datasets---CIFAR-10, AFHQ, and FFHQ---and they show slight improvements over two baselines in terms of FID scores.

* The authors also verify that their approach decreases the Martingale loss, which measures consistency in the denoised outputs, comparing this loss between a baseline model and one trained with the Martingale regularizer.

Weaknesses:
* The proposed framework has some similarities to Consistency Models (Song et al., 2023) and it would be helpful if the authors could add more discussion of this in the paper. I don't think the two approaches are identical. The core similarities are in the reverse-Martingale property, which looks basically the same as the consistency property for CM, that states that the output of the function should be equivalent whether it is computed on $x_t$ or $x_{t'}$ for any $t' < t$. Also, both CM and this work have a similar boundary condition for $t=0$. The motivations for the approaches are different, as CM aims to produce a 1-step sampler that maps directly from noise to a clean image, while the Martingale property is used to mitigate sampling drift to improve sample quality when using standard sequential sampling chains. Both approaches sample two points along the denoising trajectory, $x_t$ and $x_{t'}$, which are used to compute a consistency-like loss.

* One apparent difference between MP and CM is that the MP uses an expectation over samples from the reverse diffusion process for a single input $x$. Also, MP seems to apply the same parameters $\theta$ to both parts of the consistency function, while CM uses an exponential moving average of the parameters for one part? In MP, what is the number of samples used to approximate the expectation in the first part, $\mathbb{E}\_{\theta}[h_{\theta}(x_{t'}, t') \mid x_t = x]$?

* The paper should specify clearly what the difference is between $\bar{B}_t$ and $B_t$ in equations 3 and 4.

* In L33, why has the notation switched to using superscripts $p^*_0$? Does this denote anything different from the non-superscripted version $p_0$ used just before this sentence?

* In L38, I think it would be clearer if the learned score function were denoted by writing its parameters $s_{\theta}(x, t)$ rather than just $s(x, t)$, to clarify how it differs from the true score function $s^*(x, t)$. Otherwise, this distinction is subtle and unclear.

* Equation numbers need to be added to the two equations in the introduction (page 2). Also, it is confusing that the order of the left- and right-hand sides are flipped between the first and second equation on page 2. I think it would be clearer for the second equation to be $h(x, t) = \mathbb{E}[x_0 \mid x_t = x], \forall t \in [0, 1] \forall x$.

* The superscripts used to denote the loss functions $L^1_{t, x_t, x_0}$ and $L_{t, t', x}^2$ are confusing because they look like they would represent $L_1$ and $L_2$ losses, respectively, while they are actually just indexes for the first and second loss terms. It would be better to rename them $L^{\text{SM}}$ for the score matching loss and $L^{\text{MP}}$ for the Martingale property loss.

* The title, caption, and labels are too small in Figures 1(a,b). Why are the ""baseline"" and ""Ours"" curves following the same pattern (e.g., the same fluctuations)? Why are the curves non-monotonic? Are the plots shown for one example or are they an average over many examples? Could the authors report min/max values or the standard deviation over different examples for these curves? What do the values of $\sigma_t$ from 0 to 80 represent?

* Why is the FFHQ dataset not included in Table 1?

**Minor**
* L9 typo: ""describes conservative"" --> ""describes a conservative""

* L14 typo: ""in CIFAR-10"" --> ""on CIFAR-10""

* In Eq. 1, explain the notation $\oplus$ as it is fairly non-standard.

* The conventional notation for normal distributions is $\mathcal{N}$, not $N$.

* L24: ""$\sigma_t$ is an increasing function"" --> $\sigma_t$ is a value, not a function, so instead one can say that $\sigma_t$ is given by an increasing function?

* L44: It is strange to say ""Question 1"" when there are no other questions.

* L84: ""The formal statement is summarized as follows below: Theorem 1.1 (informal)"" --> The ""formal"" statement immediately says ""informal.""

* In Eq. 2, why use this strange notation, rather than following the standard convention of using subscripts of $\mathbb{E}$, that is $\mathbb{E}_{x_0 \sim p_0, x_t \sim \mathcal{N}(x_0, \sigma_t^2 I_d)}$?

* L41 typo: ""the larger is also the error"" --> ""the larger the error""

* In L52, ""relates multiple inputs to $s(\cdot, \cdot)$"" --> Before seeing the final consistency objective, it is unclear what is meant by this.

* L58 typo: ""this phenomena"" --> ""this phenomenon""

* L59 typo: ""called optimal denoiser"" --> ""called the optimal denoiser""

* L64 typo: ""technique via score-matching"" --> ""technique of score-matching""

* L167 typo: ""can be relieved"" --> ""can be relaxed""

* L182: I do not think that the term VE-SDE or VP-SDE have been defined yet, before they are used.

* L188 typo: ""trajectories:"" --> ""trajectories.""

* L195 typo: ""$x_{t'}$ and $x_{t'}$"" --> these are the same.

* In the last equation on page 5, it is hard for the reader to see which is the first and second term of the summand. This equation should be numbered, and the summand terms could be labeled explicitly, so that it's clear which one is used in practice.

* Typo in equation at the bottom of page 5: there is an extra bracket in $\mathbb{E}_{\theta}[]$.

* L207 typo: ""choises"" --> ""choices""

* L228 typo: ""droped"" --> ""dropped""

* L262 typo: ""$64x64$"" --> ""$64 \times 64$""

* Table 1 has a significant amount of empty space, as the last 4 rows for each dataset block are empty in 6 out of 7 columns. Also, most of the numbers in the final column come from other sources.


Limitations:
* As the authors acknowledge, they do not verify the assumption that the learned vector field is conservative, which is necessary for the theoretical results.

* The authors do not provide an algorithm box to state the approach formally (not in the main paper nor in the appendix).

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposed a new method to address the issue of imperfect score-matching in diffusion models by enforcing a Martingale Property (MP) during training. This MP property ensures that the model's predictions on its self-generated data are consistent with the generated outputs. The key contributions include the identification of an invariant property - the denoiser satisfies MP property - that any perfectly trained model should satisfy, proof of this principle is provided. Further, the paper introduces a novel training objective that enforces MP, optimizing the network to consistently predict data points from the learned distribution. Empirically, the paper showcases that using the proposed objective, paired with the original DSM loss, improves the generation quality in both conditional and unconditional generation contexts, validated on CIFAR-10, AFHQ, and FFHQ datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well organised and presented, easy to follow.

The work is novel, enforcing the MP property cast the drift issue into a relatively straightforward question. The methodology is appropriately described and presented and should be of interests to researchers in diffusion model field.

In my view, general clarity and quality of this paper is good, mathematical formulas are clear and the claims are generally supported by the results.

Weaknesses:
It seems too much space was occupied by background introduction in the first 3.5 pages, might be good to further compress them.

The objective function presented in Section 4 makes sense if one follows the paper from the beginning, but might be a bit difficult for quick interpretation and implementation, would be nice if a further simplified version (or even a special case) under some certain conditions can be presented

Can Figure 1 be combined as a single figure? - it doesn't add too much information to show them in two separate figures.

Limitations:
Limitations were clearly stated by the authors and the discussion of future work is sensible.

Rating:
7

Confidence:
3

";1
hsZTLwE6N1;"REVIEW 
Summary:
In this paper, the authors propose an enhanced version of DensEMANN, which efficiently grows and trains small DenseNet architectures. They employ a macro-algorithm to expand new layers and utilize a micro-algorithm to construct new convolution operations. Through iterative layer growth, this method generates novel architectures within a few GPU hours.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Detailed experimental settings are provided in this paper.

Weaknesses:
- The motivation behind this research requires additional clarification.
- As shown in Table 2, GO methods consumes the leaset GPU days and achieves the best performance. Therefore, it raises the question of why not directly utilize the GO methods.
- The experimental comparison with the original DensEMANN is missing.
- Experiments are only conducted on small datasets. Can DensEMANN be applied on large datasets, for example ImageNet-1k?
- Too many hyper-parameters are introduced in this method, which brings difficulties for applying this method on other tasks.

Limitations:
Please see the Weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors study an algorithm for neural architecture search (NAS) called DensEMANN, which uses a progressive adaptation of a DenseNet architecture during training to find an efficient neural network for the target task.

Soundness:
3

Presentation:
4

Contribution:
1

Strengths:
The paper is very well written and the authors do a good job of explaining how the DensEMANN algorithm works.

Weaknesses:
I’m not sure that the paper currently has enough substance for publication. The authors’ spend the first 5 pages on introduction and description of the previously published DensEMANN algorithm. The changes made to this algorithm are described in section 3.2 in only 20 lines of text and appear to be primarily changes to the various hyperparameters of the existing algorithm.

The primary contribution of the paper is comparison of DensEMANN with other NAS methods on CIFAR-10 in section 4.3. The results are certainly interesting, but I think the authors should focus on quality per unit time rather than quality and parameter counts plotted in Figure 2. Plotting quality against the execution times in table 2 would make it much easier to compare DenseMANN to existing methods in efficiency, which is the primary property of interest, I think.

That being said, I’m not sure empirical comparison of an existing method with state-of-the-art methods is enough novelty to merit publication at NeurIPS. I’d encourage the authors to continue to develop their exploration. For example, clearly establishing a new state-of-the-art in efficiency for NAS. Or, taking the models from DensEMANN and studying their efficiency for deployment.

Limitations:
I did not identify potential negative societal impact of this work.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper presents  a new version of DensEMANN, an algorithm for generating small and competitive DenseNet architectures with optimal weight values. The authors aim to approach state-of-the-art performance for well-known benchmarks, or at least the state-of-the-art Pareto front between performance and model size. They achieve this by introducing a new version of the algorithm that uses a combination of layer pruning and weight optimization techniques. The authors evaluate DensEMANN on three popular image classification benchmarks (CIFAR-10, Fashion-MNIST, and SVHN) and show that it outperforms or matches the state-of-the-art methods in terms of accuracy and model size. The contributions of the paper are a new algorithm for generating small and competitive DenseNet architectures, a combination of layer pruning and weight optimization techniques, and state-of-the-art results on popular image classification benchmarks.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The pruning and recovery stages which are the contributions of this paper are very well motivated and clearly explained. The way the pruning and recovery stages are designed is novel and using it in this context is rather unique.
- The authors have very aptly identified how DensEMANN fits in and very well introduced incremental approaches and NAS architectures.

Weaknesses:
- The difference in this paper with the original DenseEMANN is clearly communicated by the authors however all the the points mentioned except 2 (d):

> The pruning and recovery stages have been heavily modified to avoid long recovery
stages and their effects. We indeed observed that the kCS of settled filters is not
constant but actually decreases very slowly over time. If the recovery stage is too long,
this causes a very harsh pruning after which the accuracy cannot be recovered.

are just based on observation or not introduced in the paper or are very straightforward changes, I would suggest to consider only 2 (d) as a contribution of this paper.
-There are other aspects of this model which are very well framed and novel however it is important to note that these parts of the DensEMANN architecture arer not introduced in this paper but the original DensEMANN paper which reduces the novelty of this work by a huge margin.
- The paper does not provide a clear explanation of how the parameter limit of 500k was chosen for the experiments. Does going above these number of parameters make DensEMANN very computationally intensive or is unable to grow the network sensibly especially considering that 500k parameters in modern comparison are very few parameters especially for vision tasks?


Limitations:
- With the current set of evaluation the authors do it is very hard to determine if DensEMANN like techniques can be applied for larger and more modern models

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a new version of the existing DensEMANN, which grows small DenseNet architectures and trains them on target data. It claims that this version can quickly and efficiently search for small and competitive DenseNet architectures. The proposed approach has been evaluated on a number of benchmarks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The idea of automatically generate efficient architectures from a reference makes sense, and can be of great interests in many application scenarios. 
- The proposed approach grows the architecture at both macro and micro levels, which seems to be a valid strategy. 
- The proposed approach has been evaluated on various benchmarks, showing comparable performance with the state of the art. 

Weaknesses:
- The delta compared to the original algorithm seems to be not very significant. 
- The claim on being able to generate efficient densenet architectures is only supported by the number of parameters. For densenet like models the number of parameters might not be a good indicator for efficiency, due to many skip connections. Thus it seems not a very fair comparison.
- It is not clear how the proposed approach performs on larger datasets such as ImageNet. 

Limitations:
N/A

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper proposed a new and improved algorithm to grow small DenseNet architecture from scratch while simultaneously training them from target data.


Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
1. The paper is very clear and readable.
2. The evaluation is comprehensive and detailed, demonstrating the effectiveness.

Weaknesses:
1. The novelty is limited. The algorithm is backboned on a well-known algorithm, and the change to it is limited.
2. The scope of this algorithm is limited too. 

Limitations:
Can this algorithm is adapted to other application fields?

Rating:
4

Confidence:
2

";0
pw5hEuEroL;"REVIEW 
Summary:
This paper introduces a framework for analyzing mixture distributions via $f$-DP and its tradeoff function. Additionally, the paper leverages this framework for improving bounds for shuffling mechanisms in DP, and the same framework to prove a statement about privacy of a single-step of gradient descent from random initialization.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper addresses a fundamental problem in DP: computing effective estimates on additive mixtures of random variables. Methods of analysis for these mixtures are, to date, reasonably ad-hoc and certainly not known to be tight. This paper presents a clean unified approach, making connections to more abstract and clean mathematically-posed problems (IMO, this is the manner in which DP should imagine evolving itself, away from its CS + statistics formulations, which are often focused on data structures and properties of particular random variables). Not only does this paper formulate a nice presentation, but  to my knowledge it improves existing statements through this framework. Clear accept.

Weaknesses:

Writing some nits no presentation that came up from a stream of consciousness read below. Not necessarily all weaknesses, just comments.

* I would prefer a slight reordering of the content. AFAICT, the main logical flow is: Lemma 4.1 -> Prop 4.3 -> Thm 3.1, with the rest of the stuff more or less as add-ons (though FWIW i have not yet read the section on advanced concavity). I would really prefer this logical flow to be more clear. Ideally, for me, we would present the proofs straight through, beginning with the lemmas. However, I get that this is CS and we don't really do that here. I think a reasonable option is to sketch the proof in the main body, or discuss that the major results seem to follow primarily from the fairly simple Lemma 4.1, plus some calculations, given the reduction introduced by [20]. This logical flow is simple and quite nice, and IMO it should be highlighted on its own terms.

* Editorially, I would suggest pushing the DP-GD stuff into an appendix. I think it confuses the message, and it's not clear either how extensible it is (presumably it breaks after one step due to the injected dependence? Though I could imagine conditioning on stuff and invoking nested mixtures, this may be prohibitively difficult since IIUC Gaussian-ness will go away). 

* Confusion on 'the outputs of two neighboring datasets are post-processing of r.v.s $X$ and $Y$' (beginning of section 4). Going back to the reference, it seems like the technical meat there is effectively a good reduction to scalar-valued random variables (if  I read correctly, the content of their Thm 3.1)--presumably that is what is invoked here? But it reads as if the development were restricted to scalar-valued mechanisms (which I assume it is not).

* Setup to 4.1: the way that $X$, $Y$, and $I$ are coupled is unstated. Meaning: in the setup here, it is possible that $I$ is independent of $X$ and $Y$ (in which case $X|I$ and $Y|I$ would just be $X$ and $Y$). I guess the coupling is something like, e.g. $X$ is coupled with $I$ such that $X|\{I=i\} \sim p_i$? But if so, I don't see this stated anywhere before the proof of Lemma 4.1.

* I know this is not how CS papers are generally written, but (being an old mathematician myself) I would really personally prefer to read this paper straight through and without applications, building up to an improved analysis (which I assume is Thm 3.1), then closing with discussion of relation to prior results and tightness.

* Speaking of relation to prior results, and putting my CS hat back on: this could really use some deeper dives / commentary. I would recommend cutting the DP-GD stuff (or pushing to an appendix) and extending the discussion of relationship to prior bounds. For example: there is a table showing $\delta$s for varying $\epsilon$s at a fixed $\epsilon_0$. What happens when we vary this $\epsilon_0$ over a wide range--are the results here _always_ better (and qualitatively similar)? Or are there some regimes in which previous analyses were tighter?

* Following the citations of [20] leads me to https://arxiv.org/pdf/2304.05007.pdf. Glancing through this paper, it seems to me that one major difference with the present work is that this work claims improved amplification relative to [20] _independent_ of mechanism structure, whereas that work improves their bounds in the case of some particular mechanisms (and their worst-case bounds, for general mechanisms, are identical). Is this correct? Can you speak to this relationship?

* One confusing thing on a read-through: it is difficult to find the proof of proposition 4.3. This proof can be found in the 'technical details for section 3.1', which is not where a reader like me would look right away (given, in particular, that there is an appendix section on proofs for section 4).

Limitations:
Tightness or lack thereof of particular results was discussed. Negative social impacts not immediately applicable.

Rating:
7

Confidence:
4

REVIEW 
Summary:
One of the main challenges the differential privacy frame work is facing these days, is the gap between the variety of randomization techniques applied in the machine learning community for various reasons, and our limited capability to prove the privacy amplifications they entail. Among primary examples we can mention random initialization and shuffling techniques.

The authors of this paper leverage the known connection between DP and hypothesis testing, specifically relaying on the newly presented notion of f-DP, to transition the analysis of the effect of these mechanism from the domain of privacy loss distribution to the domain of hypothesis testing, where they prove several key results. Combining these results with the known implications between f-DP and DP, they provide tighter bounds for privacy amplification by shuffling and new results for privacy amplification by random initialization (for the limited setting of one gradient step).

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper partially fills a long time gap in our understanding of the way privacy is improved by introducing various random steps to the learning algorithm. Unlike most previous works, the authors use the newly presented f-DP definition and the known implications between it and the DP definition, to analyze the effect of these randomization techniques on the trade-off function.

The contribution of this paper extends beyond its results, as the proof techniques presented in it might be used for the analysis of other potential amplification techniques as well.

Weaknesses:
At first glance, I got the impression that the results of this paper aim to provide guarantees using a separate notion of privacy, which cannot be compared to DP. This is indeed not the case, thanks to the known implications between DP and f-DP, but I found the current presentation somewhat confusing regarding this point, and I recommend the authors clarify it in the final version.

I found some of the choices made in the numerical evaluations presented in Figure 1 to be not so clear. First of all, the sample size was chosen to 1,000 while the reference paper used 10,000, and $\delta$ was chosen as $1/n$ while it is often expected to be of the order $o(1/n)$.

A more substantial issue with the comparison presented in Figure 1 has to do with the presented baseline. If I understand correctly, the baseline results were chosen to be those of the closed-form presented in [20], but that work contains tighter results presented in Theorem III2, which can be numerically evaluated, and should be presented as the more relevant comparison.

Minor comments:

* In line 104, the inequality if a functional inequality, but this notion was not presented before, so the notation is somewhat confusing.

=======

**Edit after rebuttal discussion:**

The authors response satisfied my remaining concerns.

Limitations:
N/A

Rating:
8

Confidence:
5

REVIEW 
Summary:
Randomization is an essential too in deriving differentially private algorithms. However, sophisticated randomization techniques such as shuffling induce complicated distributions on outputs, making analysis of privacy loss difficult. The paper uses the framework of f-differential privacy to provide tighter analysis on the privacy guarantees of shuffled DP-SGD than existing methods do. Additionally, the authors show that randomized initialization can beneficially improve privacy guarantees for one step problems.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-	The closed-form bounds for shuffled DP-SGD seem particularly important, even if the bounds are complicated. Table 1 does a good job of convincing the reader that the obtained bounds are significantly better than existing ones.
-	Moreover, the authors present some useful results for studying tradeoff functions in the f-DP framework. These will likely be useful for future investigations.


Weaknesses:
-	Results for random initialization for one-step SGD do not seem particularly useful? In particular, this contribution seems like a bit of a toy problem.
-	Some demonstration of the improved bounds for Shuffle-SGD would be useful to the reader. In particular, it would be convincing if there was some simple learning task on which the accuracy of the model produced by shuffle-SGD (under the new analysis) significantly outperformed the model produced under the old analysis. This should happen for any experiment, as the privacy budget savings seems to be significant per table 1.


Limitations:
-	The authors fairly discuss the limitations of their work, in particular their intentions to study multi-step SGD with random initialization.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies an important problem in DP: the privacy quantification based on a mixture of randomness. Based on f-DP, the authors point out the joint concavity of the tradeoff function. Two potential examples are proposed, including the shuffling model and the privacy amplification from random initialization. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The problem studied is important to the privacy analyses of many applications. In particular, I really like the idea to consider the privacy amplification from the random initialization.

+ The introduction and the potential limitation are nicely written. Motivation is clear. 

+ The improvement upon the privacy analysis on shuffling model seem interesting.

Weaknesses:
- Though I really like the idea to take the randomness of initialization into account and study its privacy implication, the results presented in Section 3.2 are not convincing. The claimed simulatable f-DP guarantees seem to be instance-based, or is with respect to a given adjacent datasets $D_0$ and $D_1$. Moreover, even for the particular instance-based guarantee, there is no analysis provided, such as high confidence bound, to allow us really apply this bound to produce rigorous privacy guarantee. Moreover, (please correct me if I get it wrong), based on my understanding, applying the joint concavity, Theorem 3.5 is essentially determined by the average case of the ""local"" sensitivity given the random initialization. So, I guess current studies cannot be generalized to the standard worst-case DP guarantee. 

- The notations are confusing. For example, I did not find the formal definition of $(\theta(D)|I,I)$ in Section 3.2. My best guessing is because there are two random sources? But why we must assume the noise and random initialization are in the same distribution? 

-  Section 3.1 is lack of theoretical analysis on how tighter the f-DP bound given in Theorem 3.1 is, as compared to previous works. Though the empirical improvement on the $\delta$ numbers seem significant, given that the dependence of $\epsilon$ on $\delta$ is in logarithm, it is not very clear whether such improvement is general. Another missing issue is the explanation on the computational accuracy $10^{-6}$ in Table 1. Where is this restriction coming from? Moreover, I think one major motivation to introduce f-DP in [14] is for tighter composition. The authors may also want to take it into consideration. 

Limitations:
## Update after reading the authors responses: 
I summarize my concerns and suggestions for the authors to improve this paper:
1. As I said, I always like the idea of privacy amplification from random initialization and this is an important open question widely-recognized in DP community. But the authors's results on this part, from my opinion, is trivial given that it can only study closed-form iterate distribution for the first round and thus negligible amplification in DP-SGD, and the authors agreed to put it as a minor contribution in the revision.

2. I think some claims in the paper are overblown and the limitations of the proposed methods are not properly discussed. At least to me, the joint convexity thing in the tradeoff function is not that surprising given that Poisson subsampling has already been studied in Gaussian DP. The authors also agreed that their improvement over the subsampling case is limited either for i.i.d. sampling or with a fixed batch size. 

3. For the main contribution to the shuffling model, I never say that there is no novelty in this paper. I agree that the paper presents tighter bounds on some special cases, for example with additional assumptions on the ordering tradeoff function or special mixture weights. But for the shuffling model, the advantage is only measured empirically without a clear asymptotic analysis. As I mentioned, a small constant improvement on the $\epsilon$ can lead to exponential improvement on $\delta$. The authors' response on this part is somewhat double talk and constantly say they simulate it and it is better than prior works. But my question is always how better it is and the authors did not  directly give an answer to show analytical bound of improvement. Anyway, I suggest the authors clearly discussing about both the advantages and limitations. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies improving the analysis of privacy bounds for two randomization processes (privacy amplifications): shuffling models (where each user record is privatized by some local randomizer like randomized response mechanism) and differentially-private gradient descent (DP-GD, where a Gaussian noise is added to the gradient update at each iteration). Previous bounds were given for the standard $(\epsilon, \delta)$-DP bound, while this paper analyzes through $f$-DP, a differential-privacy bound considering the trade-off between type I and type II errors (like the $f$-score), using some joint convexity arguments.

When translating the $f$-DP bounds back to bounds on $\epsilon$ and $\delta$, this paper gets much tighter bounds that works for more general ranges of $\epsilon$ and $\delta$.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The improvement in DP bounds are non-negligible and more general, and the presentation is convincing.

Weaknesses:
Certain critical aspects of the proofs might be better explained: the proof depends on some close-form expressions of piecewise linear functions (Theorem 3.1, Corollary 3.2, Proposition 4.3), which might have meaning beyond just those arithmetic manipulations or expressions as is currently shown in the paper. A one- or two-sentence description, or some simple examples, might help here.

Limitations:
The theoretical results in this paper does not have broader societal impacts.

Rating:
5

Confidence:
2

";1
Bkrmr9LjeI;"REVIEW 
Summary:
This paper proposes an algorithm called DISCO-DANCE for unsupervised skill discovery in RL. The algorithm augments the mutual information reward of DIAYN with a gudiance reward. The guidance reward encourages indistinguishable or unconverged skills to follow skills that can potentially visit under-explored states, such that the over all skill collection can have broader state coverage. The authors conduct experiments on 2d navigation, Ant maze, and DMControl suites by evaluating the state coverage and the fine-tuned downstream task performance, showing that DISCO-DANCE outperforms baselines in most of these benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Illustration of the main idea (i.e., find and follow a guide policy) is clear (in particular fig.1) and well-motivated.

+ The authors provide open-source code and extensive experiment details such as hyperparameters and resource requirements, which make the results presented in this paper reproducible.

+ The appendix provides extensive discussions about the algorithm's limitation and comparison with other baselines, which can help reader understand the proposed algorithm deeper.

+ Experiment results are promising.

Weaknesses:
+ The authors tend to address the problem unsupervised discovery in complex environments where existing methods are no longer effective, but experiments are mainly conducted on common benchmarks. I acknowledge that several maps in the navigation task are challenging, but the locomotion tasks (AntMaze and DMC) are not. Actually, baseline algorithms can outperform the proposed algorithm in terms of downstream task performance (see Appendix G).


Limitations:
+ The authors have addressed several main limitations in appendix I.
+ The selection of the guide skill depends on the final state visited existing skills. It does not seem to be a general solution to select guide skills even for state-based environments.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new unsupervised skill discovery method based on the guidance of exploration. A policy is conditioned on a latent skill variable like in prior work. The method first starts by identifying a ""guide"" skill variable that is likely near unexplored states, the novelty of unexplored states is measured by the density of random walk arrival states started from the terminal states of each skill. It then trains other skills that explore the vicinity of terminal states region from guide skill.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper analyzes the limitations of state coverage in previous skill discovery methods and provides nice motivation for the proposed method. 

2. The empirical result in mazes and control tasks are promising.


Weaknesses:
1. Measuring the density of the state distribution via generating random walk arrival states from terminal states is not sample efficiency. 

2. The method needs to select a guide skill that is most adjacent to the unexplored states. In the bottleneck maze tasks in Figure 3, what if all skills including the guide skill cannot pass the first room? Will this method also encourages effective exploration? 


Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this submission, the authors propose an unsupervised skill discovery method called DISCO-DANCE. It samples *guide skills* with random walk processes that start from the terminal states of a set of skills and use them to guide less discriminable or new skills toward those guide skills so that they can reach unexplored areas more easily. They test their method in two navigation environments (2d maze and Ant maze) and Deepmind Control Suite and compare the state space coverages and performances with the baselines.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- In terms of orignality, the main idea of this work to find a reachable state that is close to the unexplored region and to expand the skill set based on it is novel to some degree.
- The manuscript is mostly clear and easy to follow. Also, the concept figure effectively provides the intuition behind the method.
- The state space coverage problem is an important aspect of unsupervised skill discovery.

Weaknesses:
- The exploration issue with the mutual information (MI) objective could be more than what is described in this work. In theory, the MI objective is not supposed to contribute to the exploration meaningfully, especially in continuous control environments (Park et al. [21]), which can make this method mostly rely on the random walk processes for its exploration.
- I believe one important weakness of this submission is the random walk process. The manuscript mentions that the rise of the environmental complexity makes existing skill discovery methods less effective and motivates this work, but ironically, in complex environments (e.g., with high-dimensional state spaces), random walk would be one of the main bottlenecks in encouraging exploration. In such environments, this algorithm could require a large number of iterations.
- In terms of writing, I think it is not very fair to call the state spaces of the environments used for the benchmark *high-dimensional*. They are higher-dimensional compared to the 2D maze environment, but labeling them high-dimensional in general may not be a good standard for the field.

Limitations:
The authors state some limitations of the proposed method (difficulty in high-dimensional state spaces and stochastic environments), but I encourage the authors to consider taking the points I listed in the Weaknesses section into account.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper addresses the challenge of learning diverse skills in unsupervised reinforcement learning by introducing a method to selectively guide candidate skills to areas of the state space with low coverage from the current set of skills.  Rather than relying directly on mutual information maximization like many skill-learning approaches do, this paper proposes an algorithm for selecting guidance skills and optimizing apprentice policies through a combination of mutual information rewards and a guidance reward.  The guidance reward is high when the apprentice skill reaches similar areas of the state space as the guidance skill, which can help direct the apprentice skills to previously unexplored areas.  The paper shows that the proposed method, DISCO-DANCE, can cover more of the state space than alternate approaches by evaluating in maze navigation environments.  The paper also provides ablation studies to help understand what each part of the method contributes to the overall performance.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is clearly written, with each aspect of the DISCO-DANCE algorithm explained and compared with other methods from the literature.  The contribution, both in terms of empirical results and novelty of the proposed method, is well defined.  Diagrams were effectively used to make the approach intuitive.  

In addition to describing the method well, the paper evaluates the experimental performance thoroughly, comparing it to several similar methods on several domains.  The experiments use both state coverage and fine-tuning performance, and show that DISCO-DANCE performs well.

Weaknesses:
The main weakness of the paper mostly involves my uncertainty around the questions in the next section.  I hope that the rebuttal can help clear up confusion.

Limitations:
Limitations are explicitly described and claims are not overly grand.  

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduce a reward shaping approach called DISCO-DANCE to enhance exploration in unexplored states in the context of latent variable skill learning in self-supervised RL context. The approach consist in defining a guide skill with the highest potential for reaching unexplored states for a given environment, then select unconverged skills and incentivize them to follow the guide skill expressed at terminal state, helping them bypass state regions with low rewards. Finally, the approach disperse the skills to maximize their distinctiveness, resulting in a set of skills that cover a wide range of states. DISCO-DANCE fills the pathway to unexplored regions with positive rewards. The approach is compared in navigation and locomotion scenarios surpassing some previous methods in terms of exploring the state space and performing well in navigation tasks in 2D mazes and Ant mazes.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
The paper is well written and explain rather well his approach.
The problem of skill learning with self-supervised learning is important and actual.
The justification is reasonably clear, maybe it would have been interesting to discuss more the difference between environment were most degrees of freedom are part of the action space, like in locomotion and the cases of manipulation that poses the most issues.
The experiment use classic but simple 2D navigation and simulated locomotion scenarios to illustrates the benefit.

Weaknesses:
The comparison is rather limited, we woud have like to see DADS, MUSIC and LSD for example.
We would also have like to see experiments in more known challenging environment like manipulation where MI is the most in trouble.

Limitations:
The experiments are rather limited to justify the benefit of the approach.
The random walk as last step of the definition of the guide skill definition isn't shown to be scalable to larger state environments.

Rating:
3

Confidence:
5

";1
VQ1heZKSLQ;"REVIEW 
Summary:
This paper shows that for batch normalized deep image recognition architectures, intermediate latents that are produced after a batch normalization step by themselves suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, without relying on any label. The success of the proposed method implies that leakage of intermediate representations may create a security breach for deployed models, which persists even when the model is transferred to downstream usage. The proposed attack also empirically works even for transformer architectures (ViT) that employ Layernorm over batchnorm.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.	The proposed method is well motivated and technically sound.

The proposed loss is motivated by the geometry of batch normed representations and their concentration of norm on a hypersphere and distributional proximity to Gaussians. Through theoretical analysis, the authors show the contribution of batch normalization to an unsupervised radial attack and the optimal layers for the latent to lie near the end of the network. These theoretical findings are also supported by their experimental results. Therefore, the proposed method is well motivated and technically sound.

2.	The experimental results are promising.

Comparing to single-stage attacks, the proposed method can obtain higher attack success rates, which shows the effectiveness of the proposed attack method and loss function. 


Weaknesses:
1.	The significance of the proposed method is not clear.

While the proposed method can get rid of the reliance on label information, as shown in their experiments, the obtained attack success rates are inferior to well-recognized attacks, i.e., PGD. Therefore, it is unclear what is the significance of the proposed method. Since the authors consider white-box settings, attackers can use white-box attacks, like PGD, with predicted labels of the target images, which can also avoid the usage of ground-truth label information. A potential application of the proposed method seems to be the transfer learning setting. However, from the experimental results, i.e., Table 10, the proposed method is also inferior to PGD. Therefore, it is unclear what the proposed method can be used for.



2.	Some claims are not validated or clearly explained.

1)	The authors failed to clearly explain the meaning of Figure 1, which is closely related to their motivation. The notation of Figure 1 is different from that in the text, which can be confusing.
2)	The authors claimed that “the 2-step process is key.” However, they failed to explain why the proposed 2-step process is better than 1-step methods.
3)	The meaning of lines 171-173 is confusing, can the authors further clarify their claims?
4)	When we have a different latent Zj from a point Xj of a different label from Xi, the authors proposed to use this latent in their loss function to generate adversarial samples. However, this scheme is inferior to the proposed method. Can the authors explain why?

3.	Presentations can be improved.

1)	The authors split the attack results of their method and baseline methods into different tables, i.e., Tables 1-4 and Table 8, which makes it inconvenient to compare different attacks. The authors can merge these tables for better readability. 
2)	The authors claimed one of their contributions as “To improve the supervised attack case when labels are available by using our loss in conjunction with the loss from the true label.” However, the corresponding experimental results are put into the appendix. This reviewer would suggest the authors put these experimental results into the main paper to support their claim.


Limitations:

The authors have acknowledged the limitations of their work that it cannot be easily extended to networks utilizing alternative forms of normalization such as LayerNorm or networks that employ no normalization whatsoever. It is acceptable that this study focuses on a specific but widely used module in modern neural networks. Please see the weaknesses section for the other limitations of this work.

The authors have discussed the potential negative societal impact of their work. The authors also proposed a mitigation strategy to alleviate potential harm of this study.



Rating:
4

Confidence:
3

REVIEW 
Summary:
In this paper, the authors proposed a label-free attack utilizing a portion of all layers, which does not require to have gradient access to the full model, and the generated adversarial methods generalize to the case where the model was fine-tuned afterwards. These results have relevance at the intersection of the theory and practice of adversarial robustness and the growing study of batch normalization and its weaknesses and drawbacks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The proposed attach method is a label-free method, and it is a quite strong one. Moreover, the authors use the diagram to demonstrate the idea on their proposed method, which is very clear. The paper is well-written, and easy to follow. Moreover, the authors extended their work to the layer normalization case, showing the proposed method is a quite generic one.



Weaknesses:
The proposed method only tested in the white-box setting, which means the adversary has the access to all parameters of the model, while in practice, this might be not practical, as usually the service provider only provides the api for the service. I was wondering how the model performs under the black box setting.

The paper mainly presents the experimental results, but lacks of some theoretical insights why intuitively it would work.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors present and evaluate an algorithm to construct adversarial examples without labels by minimizing the cosine similarity between intermediate layers. The authors show that the attack only works with BatchNorm.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper presents strong evidence that the attack is successful on multiple datasets and with multiple architectures

- Despite not achieving state-of-the-art attack success rate when compared to attacks with labels, the paper demonstrates that attacks on BatchNorm architectures are possible without knowledge of the labels, which is required by previous methods

- Paper is generally well written and thorough

Weaknesses:
- There are some clarity issues regarding the explanation for the attack, see Questions

- The authors make the assumption that norm is concentrated, and make an argument that it should be concentrated based on statistical assumptions about the data, but it seems that this should be verified empirically by plotting the distribution of the norms, rather than relying on an imprecise argument.

- The assumptions aren't verified to be necessary. It would be helpful to include an ablation where the norm concentration is broken.

Limitations:
Authors address limitations adequately.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes an adversarial attack (angular attack) method that does not require any label information and works by only accessing the network's first part (up to a specific layer). The attack is based on the assumption that the BN layer converges and forms a hyperspherical latent space, where an angular loss is applied to guide the direction of the adversarial updates. 

Soundness:
3

Presentation:
1

Contribution:
3

Strengths:
1. The proposed angular attack requires no labels, and only partial access to the network.
2. The hyperspherical assumption about the geometry of BN makes sense. Both positive and negative results (e.g., angular attack on Fixup ResNet) support this assumption. 


Weaknesses:
1. Although Figure 2 is straightforward and informative, the other table results, including those in the appendix, are unclear. For example, it is unclear whether the numbers in Table 9 are absolute mean correlation or the fall in absolute correlation. Table 4 and Table 47 in the appendix are identical tables that only report the “max” over seven baseline methods. I think it makes more sense to include the full results of each baseline.
2. There are some missing ablation studies that I think should be included in the experiment section. a. How does the number of iterations affect the method (currently, the experiments only use a default iteration of 40). b. The current angular loss is computed by averaging the loss over the last two layers; I'd like to see more results of computing losses over more layers or just the last layer. 


Limitations:
See weaknesses. 

Rating:
5

Confidence:
3

";1
CLjBBd8u2j;"REVIEW 
Summary:
This paper proposes a decompositional approach to evaluate and improve a pre-trained text-to-image diffusion model given complex input prompts describing multiple objects and novel combinations. To that end, a LLM decomposes a difficult caption into disjoint assertions that can be individually evaluated using a VQA model. The assertion scores can be used to a) evaluate fine-grained image-text alignment and b) provide feedback for iterative refinement of generated images. The proposed metric correlates stronger with human ratings and the iterative refinement procedure increases the fidelity of generated images on complex input prompts.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The paper is very well written and the method tackles an important problem
- The proposed DA-score has clear motivation and correlates much better with human ratings
- Using a LLM to produce a diverse pool of input prompts across varying complexity in terms of participating objects and realism is novel
- Quantitative and visual results are strong. The resulting model produces images that align a lot better on unreal/creative input captions.

Weaknesses:
I don't have any substantial weaknesses but including early approaches to combining text-to-image with VQA models would enhance the comprehensiveness of the related work section.
- https://dl.acm.org/doi/abs/10.1145/3372278.3390684
- https://aclanthology.org/2020.lantern-1.2/

Limitations:
- Yes, authors have extensively discussed limitations of the proposed method in the supplementary material.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper makes a twofold contribution: namely introducing a new metric for evaluating text-to-image alignment, as well as a method that builds upon this insight to improve the process of text-to-image generation. 
For the first contribution of evaluating text-to-image alignment, the paper employs the strategy of using a Large Language Model (LLM) to decompose  the textual prompt into multiple components, and then using a Visual Question Answering (VQA) model to answer the individual components/assertions to obtain a final score for the alignment between the generated image and the textual prompt. 
To improve the image generation process, the paper builds upon the existing approach Attend-and-Excite, that uses the cross-attention maps to focus on different tokens in the textual prompt. Here, the weights applied to the sub-prompts, as well as the strength of the cross-attention are used as parameters to be optimized in an iterative refinement procedure (while optimizing the decompositional metric) to obtain more faithful generated images.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1) The paper tackles an important problem (of both evaluating text-image alignment, and improving text-to-image generation), and proposed interesting ideas for the same.
2) The Prompt Decompositional Model intuitively makes a lot of sense, and quantitative results indicate that it seems to be significantly better than prior methods (i.e CLIPScore)
3) Using the improved evaluation metric to improve the generation is a good idea. While in terms of novelty, it is almost entirely based off Attend-and-Excite and prior work that uses prompt weighting, optimizing it during test-time is new, and seems to bring good improvements. 
4) The experiments seems to be reasonably thorough. There are user studies that validate the key claims (of improved text-image alignment evaluation, and improved generation), as well as a new dataset of prompts.

Weaknesses:
One major concern I have with respect to the evaluation metric proposed in the paper is its similarity to [a]. While [a] is only an Arxiv preprint, and can definitely be considered as contemporary work, I would think that it is a good idea to atleast acknowledge it, since the idea behind breaking down a prompt into multiple Question-Answer pairs and then using a VQA model to evaluate it has been first introduced there. 

In terms of the quantitative results, my worries are that the paper mostly relies on user studies/human evaluations to compare the proposed method against existing methods. While this is understandable (and quantitative results on the DAScore metric would be unfair, since the model is explicitly optimizing it), it might make sense to have quantitative results on other metrics (such as T2TScore) to demonstrate that the improvements are visible on automatic quantitative metrics. This would also allow benchmarking on other datasets beyond the Decomposable Prompts. 

[a] Hu et al. TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering

Limitations:
Yes, the limitations section in the supplementary material provides a fairly comprehensive overview of the possible limitations of the proposed method in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a method called Decompositional-Alignment-Score (DA-Score) to assess and enhance the alignment between text and generated images during training. The authors accomplish this by breaking down each prompt or text into a series of assertions using a large language model (LLM). They then calculate correlation scores between each assertion and the generated image, employing a pre-trained VQA model. The final score, obtained by combining all the assertion scores, serves as an evaluation metric for text-image alignment. It further guides the generative models to produce more aligned images iteratively.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper seeks to address a significant challenge in the text-to-image generation task, i.e., text-image alignment. The proposed framework adopts LLM and pre-trained multi-modal models, which is reasonable.

Weaknesses:
1. There is a concern regarding the performance of the VQA model, as it plays a crucial role in determining the accuracy of the proposed score. The effectiveness and reliability of the evaluation process heavily rely on the capabilities and performance of the VQA model.
2. The experiments are not convincing enough and there and there is a lack of some specific details regarding the design of the dataset.

Please refer to more details in the following.


Limitations:
The authors discuss three limitations of the current version and provide the corresponding proposal for a solution.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a new approach to evaluate text-image alignment of generated images and how to use that feedback to improve the image quality. For this, a LLM is first used to parse the text description into individual assertions and the alignment of those individual assertions with the generated image is then measured via a VQA model. The resulting metric is shown to better correlate with human judgement than other metrics such as CLIP/BLIP score. Furthermore, the feedback of the VQA model can be used to increase the impact of specific parts of the promp during the denoising process which improves the text-image alignment of the generated image without requiring additional model training.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and presented. The novel evaluation process is intuitive and automated, and shows better alignment with human judgement than other metrics. The use of the metric to improve tet-image alignment at test time is also shown and leads to better results than other approaches. The evaluation includes several human user studies highlighting the merits of the new approach.

Weaknesses:
The approach introduces the need for two additional networks (LLM and VQA model) into the pipeline which will incur additional cost and compute requirements. The overall quality and the resulting scores are likely also heavily dependent on the quality of the LLM and VQA model.

Limitations:
Overall this is a good paper and the evaluation metric reminds me of an updated and improved version of the SOA score (Semantic object accuracy for generative text-to-image synthesis, TPAMI, 2020).

Rating:
7

Confidence:
4

";1
R4xpvDTWkV;"REVIEW 
Summary:
Paper proposes a linear attention mechanism in the transformers for graph data. The attention becomes linear by eliminating the softmax in the dot product attention and multiplying K (transposed) and V matrices first followed by multiplication with Q matrix. The Q, K matrices are normalized by their respective frobenius norms to prevent explosion in the dot products. Authors further propose using a single attention layer instead of multiple layers. This they claim is sufficient over multiple layers from a signal denoising perspective. Experimental results demonstrate comparable results with baselines. The linear attention mechanism helps in reducing training compared to baselines.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
1) The model is shown to be efficient compared to the standard scaled dot product attention which enables scaling to graphs with millions of nodes.
2) The results are comparable with baselines

Weaknesses:
1) Considering that the essence of the method lies in the linear attention which is obtained by eliminating nonlinearity and using the associative property of matrix multiplication, the novelty is limited. Further analysis needs to be done on what is the benefit/deterioration due to the elimination of softmax in isolation.
2) The gains for the small datasets are marginal, in some cases within deviation. In the large datasets too it appears that the optimal baseline results have been omitted. For example in the ogbn-papers dataset there are efficient transformers (https://arxiv.org/abs/2009.03509) that achieve better results than reported in this paper. Also on the same dataset SIGN method reports best result of 69.84 but lesser value is reported. Since the proposed method is not a pure transformer but a mix of GNN and transformer I would also suggest trying GraphGPS on the same GNN and another efficient transformer on these large datasets as a fair baseline.
3) While the method claims to not be using PEs, the authors induce representations learnt from a GNN which indirectly acts as a PE. This is similar to the framework of GraphGPS and the claim doesn’t seem to hold.
4) In continuation to the above point, the results without GNN are very poor and so it seems that it is not the transformer alone that gives the gain but the combination with the GNN. Thus the results may not just be from the single layer linear attention as claimed and the claim should be modified in accordance with the experimental evidence.
5) The model with linear attention also seems to be having problems scaling to large graphs and the authors resort to sampling techniques from Nodeformer for the same. Since efficiency is the main proposed contribution of the paper it is expected in principle to be integrated in the method rather than induced externally.

Missing Citations:
1) https://proceedings.mlr.press/v162/choromanski22a.html

Limitations:
The limitations of the method have been addressed in the paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a novel approach called Simplified Graph Transformers (SGFormer) for large-graph representations using Transformers. SGFormer achieves competitive performance with just one-layer attention, eliminating the need for positional encodings, pre-processing, and augmented loss. The paper provides theoretical justification for the methodology and suggests potential future directions for building powerful and scalable Transformers on large graphs.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- SGFormer simplifies the design philosophy for Transformers on large graphs, making it highly efficient and scalable.
- The paper provides theoretical justification for the methodology, which can help in understanding and improving the approach.
- The authors experimented with ample datasets. The empirical results look promising.
- The writing is clan and easy to follow.

Weaknesses:
- The paper mainly focuses on node property prediction tasks, and it is unclear how well SGFormer performs on other graph-based tasks such as link prediction, node clustering, etc.

Limitations:
The authors discussed some limitations. I don't see any potential negative social impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Simplified Graph Transformers (SGFormer) that uses a one-layer attention to achieve competitive performance in node property prediction benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea and the design of the simple global attention is novel.
2. The proposed SGFormer is efficient and scalable.

Weaknesses:
The analysis in section 6 ignores possible sub-nets (e.g. FFN with non-linearity in the original transformer architecture) between two global attention layers.

Limitations:
Please refer to the weakness section

Rating:
6

Confidence:
4

REVIEW 
Summary:
SGFormer adapts the transformer to large graphs by replacing the usual quadratic self-attention mechanism with a convex combination of linear attention and an arbitrary Graph Neural Network (GNN). The benchmark using node prediction tasks from Open Graph Benchmark (OGB) and achieve SOTA results. Moreover, the method is fast and scales to large graphs. They provide some theoretical justification why their one layer method can be as good as multiple layers.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-written. Although dense, it provides a good summary of related works and experiments and methodology are clear.

Results are very strong in terms of model quality and performance.

The simplicity of the method is appreciated.

Weaknesses:
There seems to be a lot of hyperparameters in the attention mechanism like \alpha and \beta that were not ablated? Also the structure of the GNN may matter a lot?

Limitations:
The paper only focuses on node prediction tasks.

Rating:
7

Confidence:
4

";1
f2U4HCY8bg;"REVIEW 
Summary:
This paper presents  Reachability Estimation for Safe Policy Optimization (RESPO), for safety-constrained RL in general stochastic settings. The authors extend the previous RCRL approach into stochastic settings and push the agent to (re)enter the feasible region. They formulate a safe RL problem with REF and further develop an adapted AC algorithm to solve it, with convergence analysis. They compare their approach against CMDP-based approaches with a soft constraint and RCRL with a hard constraint, showing the advantages.  

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper is well-motivated and well-structured to follow. It studies a critical problem. 
Based on RCRL, this paper does have some novelties in problem formulation and proposed approach. Technically, this paper is sound to me, although I only checked part of the math proof in the appendix.  
The experiments are promising as they show mixed performance and safety violation improvements. 



Weaknesses:
1. The writing could be further improved, especially the comparison with RCRL. The reviewer acknowledges that there is some explanation of the difference between the proposed approach and RCRL, still, it would be much better to add more and clarify it. For example, the reviewer is confused why RCRL cannot guarantee or optimize (re)entrance to the feasible set. Couldn't use the same proof of Proposition 2 to obtain the same (re)entrance proposition?  

2. For the deterministic environments, my understanding is RCRL considers a harder constraint as it is per state constraint than the discounted additive constraints in your paper, why the harder constraint cannot optimize/guarantee (re)entrance to the feasible set?  

3. How does the reentrance proved from the deterministic environment applied to stochastic systems? 

3. There are some recent safe RL papers considering hard constraints. For example, 

 a. Wang, Y., Zhan, S. S., Jiao, R., Wang, Z., Jin, W., Yang, Z., ... & Zhu, Q. (2022). Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments. ICML 2023.

 b.  Xiong, N. (2023). Provably Safe Reinforcement Learning with Step-wise Violation Constraints. arXiv preprint arXiv:2302.06064.
The authors may consider talking about these recent references in the paper revision. 

4. what do you mean by ""almost surely"" in the convergence analysis? 

Limitations:
What are the limitations of RESPO? 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposed an iterative reachability estimation method for safe RL. The reachability is estimated by the probability of future trajectories entering unsafe state sets. Compare to previous reachability-based methods, the proposed method could handle stochastic dynamics and also improved the performance with deterministic dynamics. The proposed algorithm also leverages more information from data which could explain the performance improvement compared to existing methods. Theoretical convergence results are provided. Experimental results well supported the claimed performance improvement and safety guarantees. 

This paper has removed one significant limitation, the deterministic dynamics assumption in previous studies. Removing this limitation should be very careful, and I think the authors did it very well. The author also connected HJ reachability with CMDP, which is two important definitions in the safe RL. Therefore, I strongly suggested the paper be accepted.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Originality:
Very good. As far as I now, no paper has considered stochastic reachability with the constrained RL setup. The authors also did a good job of connecting HJ reachability and CMDP, which are two important definitions in constrained RL that used to be separately considered. This combination also removed significant limitations, the deterministic dynamics of the previous study. 

Quality:
Excellent. The paper very clearly explained the relation and improvement with respect to the previous paper in both deterministic and stochastic settings and did a comprehensive comparison in the experimental section. The authors also summarized the novelty and advantages of intuitions which is very easy to understand, i.e., previous methods only consider the maximum violation, which might lose information in the whole episode.

Clarity:
The presentation of this paper is good. 

Significance:
The safety of stochastic systems is very important and challenging. There have been many theories and studies to formulate the problem, and reachability is undoubtedly one of the most powerful methods. Handling stochastic reachability should be very careful.

Weaknesses:
I feel good about most of the paper, I only have comments on some minor problems:
1. The problem formulation, equation (4) should be emphasized better so that the reader will know this is the proposed problem formulation. 
2. The notation system is a bit messy. The readers might get lost easily, especially those not familiar with the previous paper.
3. Algorithm 1 actually did not provide too much useful information. You should improve it to highlight the differences between your algorithm and the previous ones, like the REF update.

Limitations:
The authors have adequately addressed the limitations.

Rating:
8

Confidence:
5

REVIEW 
Summary:
Previous approaches to safe reinforcement learning used the constrained MDP formulation where there is a constraint imposed on the cumulative sum of costs to minimise violations. This framework is not applicable very easily where there is a need for hard constraint satisfaction. The previous approach (RCRL) which leverages reachability analysis to strictly satisfy hard constraints is limited to deterministic MDP and is not suited to bringing the state back to the feasible set when already outside the feasible set. In this work, the authors minimize the expected chance and frequency of violations under stochastic transition dynamics thus resolving the two problems with the previous work. The order in which to update the Q-networks, the policy networks, the Lagrangian dual factors and the reachability estimation function is studied using an empirical approach and a theoretical convergence guarantee to a local optimum is provided for this alternating optimization. Empirical comparisons are made extensively to a wide spectrum of existing approaches.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Advantages of paper in relation to deterministic dynamics assumption of RCRL is clear and has merit. Empirical comparisons to previous work are quite extensive with additional explanations in the appendix. Convergence to local optimum is presented to establish rigor and soundness of the method.


Weaknesses:
1) In section 6, it was not very clear what to see in the figures and it looked like RESPO was achieving a different point in the trade-off curve compared to the other methods. More information was available in the appendix. More discussion on how to compare the different methods and why one method performs better in a specific metric can be written out in the main section. The motivation of the paper provides the twin advantages of getting back into the feasible set and accounting for stochasticity. The first advantage is seen in the double integrator example in the appendix. Do any of the previous methods suffer due to deterministic assumptions and is the actual MDP stochastic?

2) Since there are many networks and parameters updated at the same time, the robustness and reproducibility of the training process for this method and similar previous methods seem suspect. The authors have discussed this aspect in the ablation studies and the best possible convergence is obtained in the way the authors are doing the training. This insight, though a mild weakness, could benefit the community as we are inferring new insights about alternating optimization between multiple networks tied to each other.


Limitations:
Authors have adequately addressed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a new algorithm that may handle hard and soft constraints, in which the policy optimization and Hamilton-Jacobi reachability are leveraged to ensure safety. Moreover,  experiment results on safety gym, safety PyBullet, and safety MuJoCo also show the good performance of their algorithm.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The convergency analysis sounds good.
2. Comprehensive experiments are provided.
3. Hard constraints and soft constraints are investigated.

Weaknesses:
1. Paper writing quality needs to be improved a lot, I am confused about the paper notation, e.g., V_h and V_c.
2. The experimental results are not correct regarding some baselines, especially for CRPO, in CRPO paper, the algorithm presents better performance than PPO-Lagrangian and CPO.

Limitations:
1. The balance between reward and cost is not addressed well, in the experiments, as shown in Figure 3, although RESPO can ensure safety, the trajectory is longer than other baselines, and also not smooth.
2. Some related papers are not mentioned in the study, e.g., [Kochdumper, N., et al., 2023], [Gu, S., et al., 2022] and [Selim, M., et al., 2022].

[Kochdumper, N., et al., 2023] Kochdumper, N., Krasowski, H., Wang, X., Bak, S., & Althoff, M. (2023). Provably safe reinforcement learning via action projection using reachability analysis and polynomial zonotopes. IEEE Open Journal of Control Systems, 2, 79-92.

[Gu, S., et al., 2022]  Gu, S., Chen, G., Zhang, L., Hou, J., Hu, Y., & Knoll, A. (2022). Constrained reinforcement learning for vehicle motion planning with topological reachability analysis. Robotics, 11(4), 81.

[Selim, M., et al., 2022] Selim, M., Alanwar, A., Kousik, S., Gao, G., Pavone, M., & Johansson, K. H. (2022). Safe reinforcement learning using black-box reachability analysis. IEEE Robotics and Automation Letters, 7(4), 10665-10672.

Rating:
5

Confidence:
3

";1
c5Inzw6giM;"REVIEW 
Summary:
The paper employs a few heuristic methods to accelerate logistic regression training over encrypted data. The heuristics considered include: a new loss function called squared likelihood error (SLE) along with a polynomial approximation of sigmoid function, a faster gradient-descent method based on quadratic gradient, and a matrix encoding method called volley revolver.  

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The only strength of the paper is that it attempts to solve a really challenging problem of learning over encrypted data and it provides the code apriori through an anonymous GitHub link.

Weaknesses:
1) First and foremost, the title of the paper is misleading. The paper never deals with CNN training even in the limited context of transfer learning. It is true that most practical ML applications start with a pre-trained model and finetunes the parameters of this model. However, transfer learning implies that the whole model is finetuned apart from learning the application-specific last fully connected (FC) layer. What this paper attempts to do is just learn the last FC layer, which is nothing but multiclass logistic regression (MLR) training. Therefore, the title of the paper should not claim anything about CNN training.

2) Numerous attempts have been made over the last five years attempting to achieve MLR training on encrypted data, which have not been acknowledged in this paper and compared against. For example, see the works starting from:

[A] Crawford et al., ""Doing Real Work with FHE: The Case of Logistic Regression"", 2018
[B] Han et al., ""Logistic regression on homomorphic encrypted data at scale"", AAAI 2019
[C] Bergamaschi et al., ""Homomorphic Training of 30,000 Logistic Regression Models"", 2019

3) This current paper appears to be very similar to the rejected NeurIPS 2022 submission entitled ""Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant"". While the NeurIPS 2022 submission focused on only the quadratic gradient component, the current paper also introduces the SLE loss. However, it is not clear how this SLE loss function is better. Moreover, what is the expression for the gradient of $ln L_2$ and where is it used in Algorithm 1?

4) The so-called volley revolver does not constitute any novel ""matrix-encoding"" method. Such packing tricks are regularly used in the context of efficient SIMD operations in FHE.

5) Overall, none of the three claimed contributions (namely, quadratic gradient, SLE loss, and volley revolver) appear to be original or significant enough to make an overall impact.

6) Finally, though the paper claims that the goal is to make logistic regression training practical, not a single experimental result has been shown to prove this point. Running 2 iterations with 128 MNIST images takes approximately 21 minutes and the last line claims that real experiments would take ""weeks, if not months"". There are other reported works in the literature, which showed more realistic results.

[D] Nandakumar et al., ""Towards Deep Neural Network Training on Encrypted Data"", CVPR-W 2019
[E] Lou et al., ""Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data"", NeurIPS 2020

Limitations:
All the limitations have not been presented and addressed. There appears to be no potential negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper presents a method for CNN transfer learning implemented in homomorphic encryption to protect privacy.


Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
I'm not aware of the method being implemented in HE before.


Weaknesses:
I cannot judge the machine learning aspects, but I don't see a strong novelty on the cryptographic side. The paper claims that some prior work is overly complex without going into details.

I find it concerning that the work relies relatively heavily on non-peer reviewed references by a single author (5 out of 19).

Line 150 says ""well-studied by several works"" without giving any reference.


Minor issues:
- l6: ::
- l15: .;
- l50: pervacy-persevering (privacy-preserving?)
- l51: diffuclt
- l71: seveal
- l154: After many attempts (unscholarly language)


Limitations:
n/a

Rating:
3

Confidence:
2

REVIEW 
Summary:
In this paper, the authors proposed a CNN training technique on the homomorphic encryption domain based on transfer learning. A gradient variant called Quadratic Gradient on homomorphic encryption was proposed. And a sigmoid function-based Softmax approximation was proposed. In addition, a new loss function for squared likelihood error was proposed, and a matrix-encoding method called Volly Revolver was also proposed. Finally, they released the code they implemented.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Properly implementing functions for training on a homomorphic encryption domain is challenging. It is worth evaluating for implementing this and also disclosing their source code.

Weaknesses:
This paper performed a simulation on the MNIST dataset for performance evaluation. This seems too simple a dataset, even considering the homomorphic encryption environment. Although they claim that it is to be the first implementation of transfer learning-based CNN training on the homomorphic encryption domain, a similar study was recently published first. Of course, this paper takes a different approach.

[*] https://openreview.net/forum?id=jJXuL3hQvt

This paper is considered incomplete in several respects. The main reason is that the proposed scheme's threat model needs to be clarified. At first, ""the proposed architecture"" is not clear. There needs to be a description of the proposed architecture. They should explain the exact part where homomorphic encryption was actually carried out in the transfer learning process and what benefits can be gained from doing so.


Limitations:
Not exactly.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper combines several existing techniques to achieve privacy-preserving CNN training. These techniques include transfer learning,  Quadratic Gradient,  mathematical transformation, and matrix-encoding method Volley Revolver. 

This writing is more of a technical document rather than a research paper with insights.

Soundness:
4

Presentation:
1

Contribution:
3

Strengths:
1)For the first time, they apply homomorphic encryption to neural network training.
2)They demonstrate the feasibility of homomorphic CNN training.
3)They propose pervacy-perserving friendly Squared Likelihood Error (SLE) for CNN training.
4)Experimentally, their algorithm has a state-of-the-art performance in convergence speed.

Weaknesses:
1)The introduction of related works is pretty simple, which makes it difficult to evaluate the contributes of the paper.
2)The quality of writing/presentation is very weak and unreadable.

Limitations:
See Questions and weakness.

Rating:
5

Confidence:
3

";0
MfiK69Ga6p;"REVIEW 
Summary:
This paper proposes a discrete diffusion model guided by protein properties. They add a property predictor to the diffusion model, sharing the hidden states. The method is largely inspired by previous work, while experimental results are positive.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written.
- The experimental results are convincing and good.
- The function-guided protein design task is important.

Weaknesses:
- Many baselines addressing a similar problem are overlooked [1-4]. 
- The experimental evaluation is limited to a single attribute. The authors could extend the method to multi-objective optimization.



[1] Kirjner, Andrew, et al. ""Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing."" arXiv preprint arXiv:2307.00494 (2023).

[2] Sinai, Sam, et al. ""AdaLead: A simple and robust adaptive greedy search algorithm for sequence design."" arXiv preprint arXiv:2010.02141 (2020).

[3] Ren, Zhizhou, et al. ""Proximal exploration for model-guided protein sequence design."" International Conference on Machine Learning. PMLR, 2022.

[4] Brookes, David, Hahnbeom Park, and Jennifer Listgarten. ""Conditioning by adaptive sampling for robust design."" International conference on machine learning. PMLR, 2019.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes new methods for discrete diffusion guidance, making it possible to optimize protein sequences for local and global properties while retaining high sequence likelihood.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper addresses a problem closely related to the NeurIPS community.
2. The methods sound reasonable.
3. I think the introduction is well-written, especially the second paragraph, which effectively highlights the significance of protein sequence design.
4. The experimental results are also significant.

Weaknesses:
1. Overall, I find the presentation of the paper quite comfortable, but I feel that the reading experience is slightly compromised due to some details not being included in the main text.
2. Lack of the comparison with the guidance for discrete diffusion in [1].

Limitations:
The paper lacks a discussion of the limitations of the work. I hope the authors will address the limitations and include this discussion in the revised version of the paper during the rebuttal period.

**Reference:**
1. Vignac, Clement, et al. ""DiGress: Discrete Denoising diffusion for graph generation."" The Eleventh International Conference on Learning Representations. 2022.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper is focused on the task of multi-objective protein sequence design using guidance in diffusion models. Since protein sequences are discrete objects, classifier guidance tools from continuous diffusion are not directly applicable. To bridge this gap, the paper proposes a new method NOS, where the classifier guidance is applied to the latent representation of the sequence rather than the discrete sequence. Empirical evaluations on multiple real world sequence design tasks showcase the improved performance of their proposed methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well written and easy to follow. The technical choices made across the paper are well motivated.

2. The biggest strength of the paper are the empirical evaluations on real world sequence design tasks. In particular, the antibody lead optimization experiment has both an in-silico component and an in-vitro component, with experimental validation of predicted model designs.

Weaknesses:
1. The technical contributions of the paper are largely incremental. Upon first reading the paper abstract, it appeared as though the paper proposes a new guidance method to work directly with discrete protein sequences. However, the guidance was applied to the continuous latent representation of the sequence. This is a natural and elegant direction to investigate, but diminishes the technical novelty.

2. Some questions pertaining to the experimental evaluation:
    * In the antibody lead optimization task, could the authors add other baselines such as DiffAb / RFDiffusion?

Limitations:
The authors have addresses limitations and proposed directions for future work.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The submission introduces a new diffusion for protein design. It focuses on overcoming the challenge of discrete nature of protein sequences. The model succeeded in local and global properties optimization of antibody sequences making them more applicable for therapeutic purposes. 

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
Originality: Protein diffusion models aren't new (there are many related contribution into this field), however, the discrete nature of protein sequences challenges the usage of these models directly in sequence space, Therefore, the submission is important, novel, and useful. The claims are well supported.
Quality: Overall quality of the paper is high, but it's better to add and extend the comparison to and discussion of other protein diffusion models.
Clarity: The paper is clearly written and contains all the necessary citations. Methods, experiments, and equations are clearly described. 
Significance: Diffusion is a promising approach in protein design. The submission provides a valuable impact to optimize protein sequences for local and global properties.

Weaknesses:
The submission lack of comparison to currently available deep learning protein design approaches such as RFdiffusion. 

Limitations:
The paper doesn't describe limitations and future work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents a novel approach to applying diffusion models to optimize protein sequences, specifically for antibody design. Diffusion models have been successful in handling continuous data like images, but their direct application to protein structures faces challenges due to limited high-quality structural data. The authors propose a new method for discrete diffusion guidance, enabling the optimization of protein sequences for desired properties while maintaining high sequence likelihood. They further apply this method to a real-world protein design task, focusing on optimizing antibodies for increased expression yield and binding affinity to a therapeutic target while adhering to locality and liability constraints.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
Novel Ideas and Comprehensive Solution: The method proposed in the paper introduces several novel ideas that address a critical challenge in protein design.

A notable strength of the method is its ability to optimize protein sequences without relying on structural information.Protein design approaches often face limitations due to the scarcity of high-quality structural data and the need to convert structures into sequences for synthesis. By bypassing the requirement for structural information, the proposed method reduces complexity and provides a more straightforward and practical solution for designing proteins.

The absence of a structural component in the optimization process simplifies the method's implementation and reduces computational complexity. 

The authors demonstrate the efficacy of their approach through real-world evaluations. By applying the method to optimize antibodies for increased expression yield and binding affinity to a therapeutic target, the paper offers tangible and meaningful results that resonate with practical protein design needs.

One of the noteworthy strengths of the method is its ability to optimize protein sequences while maintaining high sequence likelihood and by extension expression efficiency.

Weaknesses:
Unclear Introduction and Insufficient Motivation: The paper's introduction lacks clarity and fails to provide a meaningful digest of key topics in the field of protein design. As a result, the motivation for the research is poorly established, which hampers the reader's understanding and engagement throughout the rest of the paper, making it challenging to appreciate the significance of the presented work.

Lack of Reproducibility: The absence of all test data and failure to provide the code used in the experiments significantly impairs the reproducibility of the results. Reproducibility is a fundamental aspect of scientific research, and the unavailability of essential resources restricts other researchers' ability to validate and build upon the findings.

Missing Computational Cost and Time Information: The paper lacks essential details concerning the computational cost and time required for both training and inference of the proposed method. Such information is crucial for assessing the feasibility and practicality of the approach, and its absence leaves readers with an incomplete understanding of the method's resource requirements.

Inadequate Description of Model Architecture and Training Procedure: Section 4.2 requires more comprehensive information on the model architecture, feature extraction layers, and the encoder used in the proposed method. Similarly, the training procedure and hyperparameter tuning should be thoroughly explained to ensure clarity and reproducibility.

Insufficient Information on Test Data: The paper does not provide sufficient details about the test data used, such as the number of samples and the criteria for their selection. Without this information, it remains unclear if the reported results generalize well to diverse protein sequences.

Lack of Relevant Information in Figure Captions: Figure captions throughout the paper lack relevant information that could aid in the readers' interpretation of the presented results. Clear and informative captions are essential for facilitating a comprehensive understanding of the visual data.

Missing Quality Assessment of IgFold Structures: In section 5.1, the authors fail to report on the quality of the IgFold structures used in structure-based methods. Since the performance of these methods can be highly dependent on the starting structure's quality, this information is crucial for the paper's credibility.

Inadequate Explanation of Comparison Choice: In section 5.3, the rationale behind comparing salient selection with uniform random selection from the entire sequence remains unclear. The authors should provide a more insightful justification for this choice, considering the low probability of randomly selecting critical regions like CDRs.

Lack of Ablation Analysis for Objective Functions: The authors do not thoroughly evaluate the importance of the chosen objective functions through ablations. Specifically, in section 5.2, the effect of 40% beta on ProtG3T Log Likelihood should be explained and analyzed in more detail.

Unexplained Statement on Sensitive Data: The statement regarding non-disclosure of specific drug targets in the experiments lacks a clear explanation. The authors should clarify the reasons behind this decision and provide reassurance regarding the privacy and ethical handling of sensitive data.

Avoidance of Emotional Statements: The use of emotional statements, such as ""eager to"" or ""encouraging to see,"" should be avoided in scientific writing. Such statements can introduce bias and subjectivity, undermining the paper's objectivity and professionalism.

Minor issues
* Line 46: “there is no analogous method for proteins, which do not encode the same general-purpose semantics. As a result many […]” It is not clear what the authors are saying here. Explain how the encoded semantics in protein space differs from NLP and why other sampling methods have been used to address this issue.
* Figure 1 caption: “[...] a powerful method for protein design.” Please stick to informative text about the figure in the caption.
* Line 84: “A protein is only useful if it can be expressed in living cells [...]” This is incorrect. A protein could be expressed for instance in a cell-free translation system and still be useful.
* Figure 7 caption: Last sentence doesn’t belong in the caption.
* Equation (1), remove Z and replace equation with proportionality or explain what Z is.
* The OAS dataset needs to be briefly described somewhere in the text.


Limitations:
The presentation lacks clarity and fails on several points (see weaknesses).

The unavailability of all test data and the absence of provided code significantly impair reproducibility, limiting the validation and extension of the research by other researchers.

The paper does not present detailed information on several topics (see weaknesses).

Ablation studies are lacking (see weaknesses).


Rating:
6

Confidence:
3

";1
M1dTz6QmuM;"REVIEW 
Summary:
This paper focuses on developing theories for the learning dynamics of policy gradient reinforcement learning (RL) algorithms with a particular focus on high-dimensional latent feature space. As an early work along this direction, the authors study a binary-action environment setup for simplicity. The authors develop ODE-based learning dynamic equations that generalizes across diverse protocols, including different policy horizons, the existance of failure penalties, and the choices of dense / sparse rewards. The authors further develop theories for optimal learning rate, optimal horizon scheduling, and learnability with respect to these hyperparameters. The authors finally conduct an experiment on vision-based Procgen Bossfight environment and demonstrate that under more general settings, similar phenomena arise as their theoretical model developed under simpler setups.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall the paper is well-structured and well-written. Under the binary-action environment setup, the ODE dynamics model developed by the authors is shown to accurately describe diverse common learning setups. There are also extensive experiments and plots that illustrate the difference of learning dynamics under different environment and optimization parameters, which provide much insights for the readers.

Weaknesses:
In the main paper, the latent feature space dimension $D$ is fixed to 900 except Procgen. It would be helpful if authors provide more analysis on the influence of latent dimension $D$ on the learning dynamics. Empirically, for which $D$ is author's proposed ODE-based learning dynamics equation still accurate?

For Fig. 6b, Plotting environments of different episode lengths by comparing their ""Number of episodes"" seems misleading. Authors claim that agents learn slower for environments with shorter episode lengths, but this is not accurate. If one compares the total time step of learning (num episodes * episode length), agents acturally learn faster on these shorter horizon environments.


Limitations:
Limitations need to be explicitly addressed in the conclusion section, including (1) the simplicity of the problem setup studied by the paper (binary action scenarios); and (2) the paper's focus on shallow, one-layer neural network that takes high dimensional feature as input. For more general applications, neural networks typically consist of many layers, activations, and normalizations stacked on top of each other, so empirical analysis in these scenarios will be particularly helpful.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a model for solving high-dimensional problems in reinforcement learning (RL) referred to as the RL perceptron. The model is used as a framework for analyzing generalization dynamics of simple neural networks for RL tasks. More precisely, the model employs a student teacher design in which the student takes a sequence of choices and the correct choices are given via the teacher. However, the student does not have access to the correct choice at every step but rather only receives a signal at the end of each episode. As such, the model is studied as a sequential version of the perceptron algorithm. The work first derives a set of differential equations that capture the learning dynamics of the model. The dynamics are analyzed via the overlap of the weight vectors of the student and teacher respectively. The manuscript studies multiple reward settings: a vanilla setting that constitutes sparse rewards, a setting with penalty at every step and a setting where the agent is given small sub-rewards after a certain amount of time. Each of the proposed settings provides insights into how different reward functions lead to different solutions for optimal parameters when solving the system. The derivation of optimal hyperparameters demonstrates that annealing the learning rate and building a curriculum of episodes are crucial for optimal convergence. Then, it is shown that there exist phase transitions under different learning rates that can lead to convergence to sub-optimal minima and that there is a speed-accuracy tradeoff when varying reward functions. Lastly an experimental section provides insides in the practical properties of the algorithm that closely follow the analytical results on the speed-accuracy tradeoff.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.) First, I would like to say that I enjoyed reading this paper. I think it is well written and well structured with a clear line of reasoning throughout the manuscript. The figures are illustrative of the analysis and the captions are sufficiently descriptive to understand the plots quickly.

2.) The idea of employing a perceptron-like algorithm in order to understand the policy gradient system dynamics is novel to the best of my knowledge and the generated insights are interesting. I think this is a nice contribution as it offers a way to analyze high-dimensional RL systems in a different way than the commonly employed linear MDPs.

3.) The manuscript is technically sound and the analysis provides a good understanding of the proposed algorithm and its inner workings. I did check the math in the appendix for crude errors and was unable to find any but I did not try to understand all the math in detail.

4.) The manuscript provides a way to think about RL systems that is not common and I think it is likely going to be useful in understanding some parts of the systems that may have been hard to understand previously. As such, I think it is a decent contribution that can likely be built upon by others in the sub-field of policy gradient methods.


Weaknesses:
a.) The connection to policy gradients was not immediately clear to me. It might make sense to move equation 9 in place of equation 1 and highlight the connection between the perceptron update rule and the REINFORCE algorithm in a brief sentence. I think it would be good to highlight that the update rule uses an approximation that is only accurate early in training.

b.) The limitations of the model are addressed rather sparsely. I think the work would benefit from having a clearer picture of the weaknesses of the approach which would enable researchers to use it and improve upon it in the future.

c.)  The model seems to be unable to solve the benchmark problem fully. However, it is hard to tell whether that is just a limitation of the model or whether the task is hard the way it is designed with the changes in the manuscript. Having a baseline performance line in the plot or giving a brief sentence of what the expected performance of a commonly used RL algorithm on the benchmark is would be very useful to determine the capabilities of the method.

d.) I think one key thing that is missing from the paper and would make it a very strong contribution is to show the relationship between the proposed model and common deep RL methods. It would have been nice to have a direct comparison from the proposed method to a neural network approach using standard REINFORCE-like updates to see if there is a correlation produced from insights of the RL-perceptron with the behavior of the regular deep neural network. I do understand that space is limited though.

----

Overall, I think changes that could improve the manuscript would establish the model's connections to other research that people have done in the area. This could, for instance, include baseline performances, transfer of insights to other methods or any theoretical results that put the work into reference with commonly knows results.

Minor clarity suggestions:
* Line 250, there is a broken off sentence in there that should be removed.
* Line 562 equation reference is missing.


Limitations:
I do not believe there is any negative societal impact that needs to be addressed regarding this work. The limitations are addressed rather sparsely. As stated before, I believe that the manuscript would benefit from more structured limitations sections. One limitation that I see is that the model currently requires actions to be discrete and (possibly ?) binary. While for a first version of the model this is absolutely fine, this might be something that researchers can work on in the future. Another limitation of the model is that it seems to not be able to fully solve the suggested benchmark problems. Again, I don't believe this to be an issue for the manuscript as the goal is not to provide a state-of-the-art model but rather to make progress towards understanding the learning dynamics of deep RL systems. Yet, in the future a goal should be to have models that can be described analytically that also achieve comparably high performance on realistic tasks.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors develop a set of differential equations that describe the learning dynamics in high-dimensional settings, allowing for a quantitative analysis of learning behaviors. This framework enables the computation of optimal hyper-parameter schedules and the visualization of phase diagrams for learnability. It also serves as a starting point for exploring RL scenarios closer to real-world situations, including those with conditional next states. The RL perceptron can be used to investigate various training practices, such as curricula, and advanced algorithms like actor-critic methods. The authors aim to gain analytical insights from the differential equations, particularly regarding how initialization and learning rate affect an agent's learning process. Overall, their research highlights the complex interplay between task, reward, architecture, and algorithm in modern RL systems.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The paper tackles an important problem of understanding high dimensional RL policies. 
* The method section seems theoritically sound.

Weaknesses:
Experiments are not convincing. For a paper investigating such an important problem, the paper should have shown results on multiple environments.

Limitations:
A considerable limitation of the paper is the lack of a comprehensive evaluation of the method. The paper provides only results on one experiment. I would suggest the authors conduct more thourough experiments in order to make the applicability of method more clear.

Rating:
6

Confidence:
1

REVIEW 
Summary:
The work proposes a theoretical framework to study the average case learning behavior of deep RL policy gradient methods. The framework is based on ODEs that can describe the typical learning dynamics of PG RL agents. The framework is used in various settings to describe learning behaviors in these and a final experiment on training a policy-gradient agent in a ProcGen game is added to bridge the theory-practice gap. This final experiment verifies that a speed-accuracy trade-off exists in practice, similar as predicted by the theoretical framework.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
* The work addresses an important aspect in deep RL research as most theoretical guarantees for RL are not well connected to the practical side of deep RL.
* The work sets out to provide a theoretical framework from which to study deep RL methods.
* It shows how such the theoretical framework can be used to understand the influence of
  * delayed rewards and reward penalties
  * learning rate schedules and episode lengths
  * reward stringency

Weaknesses:
* The work was very difficult to follow for me. Due to the structure of the paper, many aspects seem to ""fall out of thin air"".
* Without Appendix A it seems impossible to begin to understand Section 2 since more assumptions about the student teacher environment are given.
* It often feels like the work requires extensive prior knowledge to be understandable.
* Wording is often confusing. For example, in the beginning when talking about the reward for the RL-Perceptron case Fig 1 has a description about rewards that seems permissible for very dense rewards, whereas lines 48-50 talk only about extremely sparse rewards.

Overall the paper seems very interesting and full of great ideas but due to a somewhat convoluted presentation and missing details that seem to be pushed to the appendix it falls short of clearly communicating these ideas.

I might have missed something obvious, but to me it seems that the paper would first need fairly substantial rewriting to be easier to parse before it can be accepted.

Limitations:
The authors do not explicitly list limitations of their framework 

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper discusses the application and theoretical understanding of Reinforcement Learning (RL) algorithms in high-dimensional settings. The authors propose a high-dimensional model of RL that can capture a variety of learning protocols and derive its dynamics as a set of closed-form ordinary differential equations.

The authors introduce the RL perceptron, a model for high-dimensional, sequential policy learning. In this model, a student network learns from a teacher network in a sequential decision-making task. The student does not observe the correct choice for each input; instead, it receives a reward that depends on whether earlier decisions are correct.

The authors derive an asymptotically exact set of ODEs that describe the typical learning dynamics of policy gradient RL agents. They use these ODEs to characterize learning behavior in a diverse range of scenarios, including exploring several sparse delayed reward schemes, deriving optimal learning rate schedules and episode length curricula, identifying ranges of learning rates for which learning is 'easy,' and 'hybrid-hard,' and identifying a speed-accuracy trade-off driven by reward stringency.

They also demonstrate that a similar speed-accuracy trade-off exists in simulations of high-dimensional policy learning from pixels using the procgen environment ""Bossfight"" and Atari ""Pong"". The authors aim to close the gap between theory and practice in high-dimensional RL.
The paper also discusses the sample complexity in RL, statistical learning theory for RL, and dynamics of learning, providing a comprehensive overview of the current state of RL theory and practice.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This is a robust theoretical paper that enhances our comprehension of high-dimensional RL policy learning. The paper's claims are supported by high-level experimental evidence. A significant advantage of this paper is its use of the challenging procgen ""Bossfight"" environment, which is much closer to the real use cases compared to the preceding works. Additionally, the ""Pong"" game was analyzed in the supplementary materials to demonstrate the speed-accuracy tradeoff. Another strong aspect of the paper, and a significant advantage, is its focus on analyzing the average-case scenarios rather than the worst-case ones. The authors have also released the code to reproduce the results, further strengthening the paper's credibility.

Weaknesses:
To the best of my knowledge, the paper does not exhibit any significant weaknesses.

Limitations:
Limitations were addressed reasonably well. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces a new problem definition and class of solution methods for decision making. 


The motivation put forward by the authors is that the current pool of solution methods do not have theoretical results that capture the neural functional classes used by practical applications in RL, leaving a gap between theory and practice. 

The authors claim that the model of learning by reinforcement that they propose has sufficient flexibility to capture the same class of problems as the classic RL model, and that their problem definition, despite its generality, is solvable in closed form for higher dimensionalities (and even infinite), as opposed to the classic RL model, presumably formalised as POMDPs. In addition, they claim their model (unclear whether the authors mean the solution method or the problem, see detailed comments) behaves similarly w.r.t. hyperparameters, for which they provide optimal schedules and hypersensitivity plots. 

Moreover, the authors claim their proposed class of problems and solution methods exhibits in practice similar behaviour as predicted by theory, and closes a gap between theory and practice, providing as evidence empirical illustrations in a particular environment, specially designed, called “Bossfight”, which they engineer using a platform for a procedurally generated environments. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
$
\textbf{Originality}$

The authors propose a new problem definition and class of solution methods for RL, particularly policy optimisation. They describe their model as a perceptron, which they call “the RL perceptron”, and analyse the learning dynamics of a particular solution method for action space of 2 discrete actions. They then define different problem instances via heuristics for various feedback signals that such a solution method can receive. The authors propose a new perspective on RL inspired by statistical mechanics and dynamical system theory.

$\textbf{Quality}$

The particular math and statistical expressions the authors derive was not checked in detail beyond  the first few simple update equations, which seem correct. The appendix was not verified for correctness. 
Plots also appear to be consistent with the experimental claims provided in text.


$\textbf{Clarity}$

The paper is at times clear, with some mysterious redefinitions, naming, and confusions (see in the next sections). Although the flow is cursive, the authors do not use the general breakdown of the paper and miss important sections like a “Background” section, which is placed in the introduction. It does not bother much, but clearly marking such section can help the reader understand better what is novel and what is known. 


$\textbf{Significance}$

The main motivation of the paper is very important. Theoretical guarantees for solution methods with feasible at-scale practical implementations is highly desirable, since this informs us that they are generalizable to all problem instances, not just the particular settings in which they were tested in and shown good empirical results. Generalisation and theoretical guarantees in RL, particularly in policy-based methods with neural policy classes, which are the most successful algorithms used in practice, is a very important area of research. 



Weaknesses:
$\textbf{Motivation}$

While the motivation of the paper is very important, i.e. insufficient theoretical guarantees for policy-gradient methods with neural function classes, PG have been shown to have global convergence beyond tabular fn classes which the authors claim to be the issue.  For log-linear policy classes see: Yuan  et al - “Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies” - and references within. For neural policy classes see “Neural Policy Gradient Methods: Global Optimality and Rates of Convergence” -- Wang et al and references within. Both of these show global convergence, the latter for a two-layer NN, including finite-sample guarantees/convergence rates and properties of actor-critic methods, with estimated critics, required for convergence. 

$\textbf{Relation to RL and placement in context}$

The second main weakness of this paper is the lack of placement of their work in relation to the standard RL model used by the community through the formalism of MDPs or POMDPs, with accompanying class of solution methods. The paper would be significantly strengthened if they authors can clearly state what the limitations are with the previous class of problems described in RL via a reward signal and transition dynamics and in addition for POMDPs, how current definitions of high dimensional observation spaces proposed are better captured with their problem definition. It is unclear how the models proposed by this paper generalize beyond the log-linear policy class and action spaces of dimension 2, described within. The authors claim lack of theoretical work, yet previous work in RL has also analyzed distribution shift and generalization error, but with a different definition than the authors propose. It is unclear to me what the prior limitation in definition were and how this is a better way of capturing such quantities. Additional details in this respect would significantly strengthen the paper..

$\textbf{Purpose/Goal}$

Furthermore, it is rather confusing the purpose of the paper, i.e. it seems the authors compare problems against each other, for the same solution method, instead of comparing solution methods that are general enough to work on every problem instance. It is also confusing the setting and problem definition. It appears that the problem is not learning by trial-and-error, and that the problem instances proposed need heuristic descriptions of reward signals, horizon sizes and task termination. It is unclear the setting in which we are in, whether that is undiscounted finite-horizon, or continuing learning (infinite horizon, average reward). The authors reference terms from RL related to this but never actually formalize the problem.

$\textbf{Empirical study/experimental illustration}$

Lastly, the experimental section is performed on a certain game designed in particular way, which is an illustration rather than a practical algorithmic implementation, akin to the solution methods employed in empirical RL. 

More details/questions in the next sections about these points.

I am happy to adjust my score if I have not correctly understood the paper, and the authors provide more details on how their work can be placed in the context of RL, which would help me understand the significance and impact of the result and analysis provided.

Limitations:
$\textbf{Lack of clear description of the limitations}$

The authors do not provide a clear description of the limitation of their model. It is unclear what exactly is “solvable” in closed form, is it any problem of any dimensionality with any kind of reward signal, including non Markovian? How general is the problem that is “solvable”? Does it capture all problems a POMDP would capture, and these problems are solvable? Any kind of additional information in this respect would be useful for the reader. It is also unclear what is “the average-case”. A short definition would be very useful.


Rating:
5

Confidence:
3

";0
3Da0eESvN1;"REVIEW 
Summary:
The paper introduces an efficient algorithm for learning halfspaces in the testable learning model in which the tester-learner first applies a test on the training data and if the test succeeds the algorithm produces a hypothesis which is guaranteed to be near-optimal. It is required that if the data comes from a target distribution, then the test must succeed with high probability.

The paper considers learning halfspaces in the case where the target distribution is Gaussian (or strongly log-concave) and where the labels are subjected to Massart noise or adversarial noise (i.e., agnostic setting). The paper builds on several ideas from previous papers by Diakonikolas et al.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
Learning halfspaces is a fundamental problem in machine learning. Even though it is one of the simplest tasks, the problem becomes non-trivial in the presence of label noise. Recently, there has been a lot of interest in developing algorithms for learning halfspaces in several settings (e.g., Massart noise, ...). The submitted paper is the first work that presents an efficient algorithm for learning halfspaces in the testable learning model of Rubinfeld and Vasilyan.

I find the paper to be clear and generally well-written, and I find the results novel and interesting.

Weaknesses:
Minor comments/typos:
- Page 2 line 71: ""an important one being that the probability mass of any region close to the origin is proportional to its geometric measure"" -> ""an important one being that the probability mass of any region close to the origin is roughly proportional to its geometric measure"".
- Page 5, line 198: Shouldn't the title of the section be ""Testing properties of isotropic strongly log-concave distributions""?
- Page 5, line 214: ""and runs and in time poly(...)"" -> ""and runs in time poly(...)""
- Page 5, line 235: There should be a comma between \tau and \delta.
- Page 7, line 307: ""Each of the failure events will have probability at least $\delta'$ "" -> ""Each of the failure events will have probability at most $\delta'$ "".
- Page 8, line 322: ""under theempirical distribution"" -> ""under the empirical distribution"".

The following relevant references seem to be missing:
- [1] Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau, ""Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability"", NeurIPS 2020.
- [2] Rajai Nasser, Stefan Tiegel, ""Optimal SQ Lower Bounds for Learning Halfspaces with Massart Noise"", COLT 2022.

Limitations:
No concerns regarding potential societal impact of this work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This papers gives an efficient algorithm for learning halfspaces under the testable learning framework of Rubinfeld and Vasilyan (STOC'23) and facing either Massart or agnostic noise. In this setting, the algorithm is given some reference marginal distribution $D^*$ (which is assumed to be isotropic and strongly log-concave), and it may choose to ""reject"" (asserting that the actual marginal is different from $D^*$) instead of outputting a hypothesis. Naturally, the learner needs to satisfy the following two conditions:
- Completeness: When the marginal is indeed $D^*$, the learner does not reject w.h.p.
- Soundness: The probability that the learner outputs an insufficiently accurate hypothesis is low. Here, sufficient accuracy means achieving either $\mathsf{opt} + \epsilon$ (under Massart noise) or $\tilde O(\mathsf{opt}) + \epsilon$ error (in the agnostic setting), where $\mathsf{opt}$ is the loss of the optimal halfspace and $\epsilon > 0$ is a parameter.

The solution is built upon the nonconvex optimization approach to learning halfspaces under noise in the literature. The key property for this approach to succeed is that when some appropriate loss function is minimized, all the stationary points are reasonably close to the true parameter. The crux of the current work is then to identify certain testable properties of the marginal under which the above argument goes through, so that we either get a good learning guarantee, or obtain a witness for that the marginal is not $D^*$.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper studies a fundamental learning theory problem (i.e., learning halfspaces) in the newly introduced testable learning setup. The results are strong and comprehensive, and the solution is nontrivial and requires several novel ideas. The paper is very nicely written and well-strutured, and the main paper contains sufficient details (including the ""technical overview"" section) for the reader to appreciate the high-level ideas behind the work.

Despite the few weaknesses discussed below, I found the paper a strong submission that should be accepted.

Weaknesses:
- The hypothesis class is restricted to homogeneous halfspaces (without a bias).
- In the agnostic case, the error bound can be higher than the optimal error by a constant factor.

Limitations:
This is a theory paper and its limitations are in the assumptions made by the problem setting as well as the main results, e.g., the restriction to halfspaces, the noise model, and that a single reference marginal distribution is provided. These are clearly stated in the paper as well as the separate ""Limitations and Future Work"" section.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Learning halfspace is a very important problem in machine learning which has been studied extensively. However, generally some distributional assumptions like gaussianity are assumed which in general is difficult to verify. To address this issue, recently Rubinfeld and Vasilyan (STOC 23) have introduced Testable learning framework. Here the primary objective is that if the tester accepts, then the output of the learner is close to OPT + \epsilon (OPT being the optimal error), and when it satisfies the distributional assumptions, the algorithm accepts with high probability. However, when the Gaussian distributional assumption is taken (let's denote this as D^*), it takes $d^{1/\epsilon^2}$ samples, which is also tight. Thus often researchers are interested to design algorithms that have better complexity with respect to $1/\epsilon$, but the error becomes $f(OPT) + \epsilon$ for some function f.

In this work, the authors first design a tester when $D^*$ is isotropic log-concave distribution and the labels are corrupted according to Massart noise (the labels are changed by an adversary with a small probability $\eta$).  Their algorithm runs in polynomial time and has error $OPT + \epsilon$ (Theorem 4.1). Later they design testers for adversarial noise with respect to Gaussian distribution with error $O(OPT) + \epsilon$ (Theorem 1.2).

In Section 4, the authors study the case with Massart noise. The primary idea here is to minimize a non-convex smooth surrogate loss (4.1) such that its stationary points correspond to halfspaces with small error. The authors first run PSGD on this surrogate loss function to get a set of vectors L such that one vector in L is close to the optimal weight vector. Then they apply localization ideas based upon the region that is an axis-aligned rectangle T, and check if the low degree moments of input distribution D conditioned on T match with D^* conditioned on T. This will ensure that the angular distance of a stationary point w is close to the optimal w^* (Lemma 4.3). To convert closeness in angular distance to closeness in 0-1 loss, they use the fact that the distribution is isotropic strongly log-concave (Proposition 4.4).

Later in Section 5, the authors study the agnostic setting where they will call the algorithm from Section 4 several times, each time with different parameters. The idea is that in the agnostic setting, running the algorithm for only once, the algorithm might only consider points that lie within a region with small probability. This finally gives a tester with error $O(OPT) + \epsilon$  when $D^*$ is isotropic log-concave distribution as well as Gaussian (Theorem 5.1 and Theorem 5.3).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper gives the first algorithm for testable learning of halfspaces that runs in poly(d, epsilon). The algorithm is very nice. With the complexity pulled down drastically, a proper implementation and experimental results for this algorithm would be possible and it would be nice to see the relevance of the concept of testable learning in various applications.

Also the algorithm can handle both adversarial and massard noise.

Weaknesses:
The usefulness of the testable learning model in real life applications is yet to be understood.

Limitations:
It is a pure theoretical work in the paradigm of testable learning - a relatively new concept whose importance is not yet fully confirmed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This work provides an efficient algorithm for testably learning halfspaces, extending the frontier of the recently introduced testable learning which does not assume anything about the given data distribution. Specificially, the setting is as follows: the target distribution is standard Gaussian (or any fixed strongly log-concave distribution) and the label noise is Massart or adversarial (agnostic).

The main result is two-fold. 
1. For Massart noise, if the target distribution is strongly log-concave, the paper proves an algorithmic guarantee that testably learns halfspaces up to $opt + \epsilon$ error and runs in $poly(d,1/\epsilon,1/(1-2\eta), \log(1/\delta)$ time. 
2. For agnostic learning,  if the target distribution is strongly log-concave, the guarantee is that the algorithm testably learns halfspaces up to $O(k^{1/2} opt^{k/(k+1)} + \epsilon$ error and runs in ""roughly"" $poly(d^k,1/\epsilon^k, \log^k(1/\delta)$ time (ignoring some logarithmic factors). One can strengthen this result if the target distribution is standard Gaussian. Then the algorithm testably learns halfspaces up to $O(opt) + \epsilon$ error and runs in $poly(d,1/\epsilon, \log(1/\delta)$ time, a result that matches previous non-testable learning results for halfspaces.

The methodology borrows two algorithmic ideas and strengthens them. One is the algorithmic idea that runs (nonconvex) SGD on a convex surrogate (ramp function) for the 0-1 loss. Originally, given some distributional assumption, this approach would yield a hypothesis found from an approximate stationary point. In this paper, the authors check if such property is satisfied for the unknown distribution of the testable learning setting, leading to develop a three-stage testing procedure for strongly log-concave distributions. Here, the second algorithmic idea of moment matching kicks in. For the band $T$ s.t. $|\langle w,x \rangle| \le \sigma$, tests are ran on the empirical distribution conditioned on $T$ to check moments matching those of the target distribution on $T$.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- This work extends upon the recently introduced testable learning in one of the fundamental problems of learning, i.e., learning halfspaces. I believe the topic is of good importance as testable learning yields more practicality to learning algorithms. In that sense, the work studies and provides strong algorithmic result to an important problem.
- The techniques used in the paper utilize two previous algorithmic ideas (convex surrogate SGD + moment-matching to fool) and neatly tie these two ideas together to a testable algorithm. The algorithmic techniques are also practical.
- The tests are ran on the distribution conditioned on the band $T$. With this ""trick"", the paper manages to change weak additive guarantees to strong multiplicative ones.
- The results are technically strong and presentation is clear.
- The agnostic learning algorithm only modifies the Massrt one slightly, which is neat, though this may be more of contribution of previous work than that of this work.

Weaknesses:
No notable weaknesses, but refer to Questions for a potential undecided one.

Limitations:
No limtation addressed.

Rating:
7

Confidence:
4

";0
nSgMh5v5Ne;"REVIEW 
Summary:
The paper proposes a zero-shot method for non-rigid shape matching based on 
unsupervised functional map regularisation and non-rigid shape deformation regularisation.
The proposed method combines both intrinsic information from the functional map regularisation and
the extrinsic information from shape deformation regularisation (i.e. PriMo energy) leading to a better matching performance compared to methods solely based on either intrinsic or extrinsic information from the 3D shapes.
In the standard shape matching benchmarks, the proposed method demonstrates better matching performance 
compared to axiomatic approaches and competitive results in comparison to both supervised and unsupervised learning-based methods. 


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow. Both the motivation and the proposed method are well explained in the main paper. The technical details are also well illustrated in the supplementary material.
2. Unlike most existing shape matching methods solely based on functional map regularisation, the proposed method also considers spatial alignment and adds the shape deformation regularisation to incorporate also the extrinsic/spatial information leading to a better matching performance.
3. Instead of using common deformation regularisation such as ARAP, Laplacian, the paper utilises the PriMo energy leading to a more smooth and natural shape deformation, as demonstrated in the supplementary material.

Weaknesses:
1. The paper claims that the method is a zero-shot method for non-rigid shape matching. But the main difference between 
the proposed method and the previous unsupervised methods is not well discussed. In theory, all previous unsupervised
methods can also be used as a zero-shot method to optimise only for a single shape pair. More explanation and experiments should be provided.
2. In the supplementary material, it shows the average runtime on the FAUST dataset is about 70s, which is much slower compared to existing learning-based methods. Prior unsupervised works (e.g. Cao et al. Unsupervised Deep Multi-Shape Matching, ECCV 2022) typically train on the training data and utilise test-time-adaptation for a few iterations to reduce runtime. More discussion and experiments about the runtime is expected to be provided.
3. Unlike purely intrinsic/spectral methods, the proposed method also aims to spatially align two shapes. 
Therefore, more insights about the rotation robustness of the proposed method should be given. 

Limitations:
1. The proposed method is specifically tailored for complete shape matching, so more adaptations should be taken for partial shape matching.
2. The runtime of the proposed method takes 70s for shapes with ~5000 vertices, which is much slower compared to existing learning-based methods.
3. The proposed method is tailored to shapes represented as triangle mesh, so the extension to other data representation like point cloud could be the future work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
A zero shot method for computing correspondences between meshes is proposed. The core idea is to use DiffusionNet to produce features which are then used to produce a functional map from which a p2p mapping is produced. Using it, a deformation module using Primo is employed to deform the meshes to one another. This deformed shape is then used to reevaluate the functional maps and the process continues, training an autoencoder to predict the features that drive the correspondence and deformation.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
Zero-shot correspondence computation is an important problem to solve in cases where no object categories are known. The combination of an extrinsic deformation module with an intrinsic functional maps approach is appealing and novel as far as I can tell. The paper is written in a clear manner for the most part (see below). 

Weaknesses:
I have several concerns:
* In this context, ""zero shot"" essentially means ""no use of semantic information"" (as opposed to, e.g., zero shot stemming from using some existing module that has semantic knowledge and applying it to the correspondence problem without further training). Thus, the term used in the paper for ""axiomatic"" methods simply means ones using geometric priors rather than semantic ones. As such, I am uncertain why use the word ""zero shot"" at all -- this is simply an optimization algorithm for shape correspondence. As such it lies within a different research area than learning which encompasses many other works. Two recent ones are ENIGMA and Neural Surface Maps. The advantage of this approach of theirs is not immediately apparent and they should be discussed at least, if not compared to.
* It is unclear to me what is the exact prior being enforced here. At first I thought it's a an isometric-distortion minimization prior but the authors show results on non-isometric datasets, so it is unclear to me what is the prior here and what is the justification for it. Without a justified prior nor semantics, it seems concerning the method solely relies on empirical success. It is further unclear what specific class of deformations is Primo regularizing for. It does seem to be for reducing isometric distortion here, thus I do not fully understand the claim to work on a non-isometric case. I suspect that the method is still working on rather-isometric cases - SMAL's 4-legged animals are still rather isometric. Again in this context, ENIGMA shows success and seems like a good comparison.
* Additionally, the method is currently limited to matching meshes that are homeomorphic and are made up of one connected component - this is not mentioned in the paper and is somewhat limiting. 
* in terms of novelty, the method is simply a mix of several techniques - DiffusionNet, Functional Maps, Primo, which indeed work well together, but there does not seem to be a great novel insight beyond combining them to have a method to predict correspondences and then update a deformation prediction, which is a very classic idea (in essence, a non-rigid ICP approach where the closest point is replaced with a functional map prediction). 

Limitations:
yes

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper proposes a novel zero-shot method for non-rigid shape matching that eliminates the need for extensive training or ground truth data. The proposed method, Shape Non-rigid Kinematics (SNK), operates on a single pair of shapes and employs a reconstruction-based strategy using an encoder-decoder architecture, which deforms the source shape to match the target shape closely. SNK demonstrates competitive results on traditional benchmarks, simplifying the shape-matching process without compromising accuracy.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper introduces a novel method for non-rigid shape matching that does not require extensive training data, making it more practical for many applications. The proposed method also combines the benefits of axiomatic and learning-based approaches and addresses their limitations. Moreover, the paper introduces a new decoder architecture that facilitates the generation of smooth and natural-looking deformations. The proposed method is thoroughly evaluated and achieves state-of-the-art results on the SHREC dataset.


Weaknesses:
While the paper highlights the strengths of the proposed method, it lacks a thorough comparison with closely related work. For instance, although several learning-based methods are presented, there is no detailed comparison, such as a visual comparison and evaluation of other datasets (recommend moving some results to the main paper). Also, the limitations of the proposed method are mentioned but need to be adequately discussed.

For performance, the proposed method can not achieve the best average, especially for some unsupervised methods. For the SOTA - Deep Shells, why do the proposed methods have the similar performance on Faust and Scape datasets instead of Shrec? Could there be some analysis and discussion of the results?

The description of the Prism decoder is not clear, and I cannot find the corresponding components in Figure 3. I suggest the author re-illustrate the figure and add more details for easier understanding.

Some mathematical notations are repeatable:
> F_i and F_j in eq3, F_1 and F_2 at line 1130


Limitations:
The paper adequately addresses the limitations of the proposed method but could benefit from a detailed discussion of the potential negative societal impact. Besides, I think the running time should be evaluated. I think the optimization of Eq. 5 is too slow and takes other steps. The paper also mentions it in the last section.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper works on learning a point-to-point mapping between two sets of deformable mesh vertices in a self-supervised manner. To this end, the authors extract features of two meshes from a DiffusionNet, solve for functional maps by an optimization problem, and finally convert the functional maps to a point-to-point map iteratively. 

In the meantime, a per-face rigid transformation is generated from a shape decoder and is used to transform the source shape to the target shape. To demonstrate the effectiveness of the proposed method, the authors conduct experiments on both near-isometric and non-isometric datasets, and achieve reasonable performance as a self-supervised method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well organized and written. The problem is well defined in Sec.1, while the previous works and preliminary knowledge are also well introduced in Sec.2 and Sec.3.

- The two-stream (implicit shape transformation decoder and explicit functional map) pipeline is a reasonable design:
  - The low-rank functional map estimation is not only efficient but also facilitates the learning of feature extraction.
  - The MLP shape decoder and the PriMo energy regularize the predicted transformation by both network structure and training losses.

- As a self-supervised method, the performance is strong on both near-isometric and non-isometric data.

Weaknesses:
I do not see major weaknesses except for some details:
- The feature extractor should have more discussion, since it is key for the functional map prediction. Specifically, the author could either visualize or quantitatively measure the consistency of the extractor features. In addition, it would be better to explicitly indicate that the DiffusionNet is not pre-trained (in spite of lines 36~37) to avoid confusion with existing models.

- The pipeline figures could be better illustrated, such as the module blocks can be colored based on whether they are learned or have explicit formulation.

Limitations:
The limitations have been adequately addressed.

Rating:
7

Confidence:
3

";1
yVWBv0N1xC;"REVIEW 
Summary:
The paper introduces LayerNAS, which is a method for neural architecture search (NAS). The idea is to reduce the computational cost of NAS, which is exponential in the number of layers. As such, the paper introduces a layerwise search option with the idea being that the current layer can be directly determined based on the results of the previous layers. This enables polynomial computational complexity. The paper goes one step further and establishes the inclusion of the cost-constraints, e.g. in eq. 4. LayerNAS is then empirically validated in ImageNet and in a standard benchmark of NAS, which includes cifar10, cifar100 and Imagenet16 datasets. 



**Post rebuttal**: I appreciate the effort by the authors and their numerical evidence. I would urge the authors to include the references and the discussion in the camera-ready version. I would strongly encourage the authors to also include the numerical result on the cost per layer (see answer 2 in the original responses) and the additional results in transfer learning. One thing that I believe should be explained better in the main paper is the reasoning for the separation from larger architectures, e.g. the somewhat arbitrary 600 MAdds.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The reduction of the computational cost of NAS is an important aspect that makes the paper relevant for the NeurIPS community. Besides, a number of papers on NAS are published in NeurIPS and related conferences (**relevance**).  The solution proposed for the layerwise search has appeared before, but the paper makes a complete framework to support the idea and empirically validates the framework. In addition, the paper makes a clear statement of the limitations and the assumptions that led to those (**clarity**). This enables the research community to extend this paper further.

Weaknesses:
I am not sure what the novelty of the proposed method is, this is not clearly stated at the moment. 

I find that the train-free methods of NAS are more important for the story and as alternative methods than the two lines devoted to the end of the related work. Having said that, I do understand that they do not cover the contribution of this work.

Limitations:
The limitations are explicitly identified in the conclusion.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper tries to overcome a drawback of Neural Architecture Search (NAS), an enormous search space that hard to traverse whole space to design a well-optimized network.
From an assumption that a previous layer in a network doesn’t affect the subsequent layers, the paper converts multi-objective NAS to a combinatorial optimization problem.
With the proposed method, the paper designs optimized networks layer by layer, unlike other works that design the whole network simultaneously. It leads to reducing the search complexity of NAS to polynomial complexity.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- Writing is easy to follow.

- The paper compares the proposed method with other methods fairly with NATS-Bench.

Weaknesses:
- The proposed method that removes networks from search space by their costs is not novel.

- The search cost of the proposed method is still worse than one-shot NAS.

- The paper naively analyzes the cost of searched networks with MAdds, omitting other metrics such as energy consumption or latency, etc.

Limitations:
The paper refers to several limitations of the proposed method in Conclusion and Future Work. However, a thorough analysis is needed in terms of cost, too.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose LayerNAS with polynomial complexity. Namely, this work transforms the multi-objective NAS problem into a Combinatorial Optimization problem with proper assumptions. 
LayerNAS is benchmarked against recent NAS arts on ImageNet classification task, as well as on dedicated NATS-Bench in terms of quality, stability, and efficiency. 
Algorithm details and searched architectures are provided, which might benefit the community. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper is clearly written and easy to follow. The detailed algorithm and searched architectures are provided, which makes the results replicable. 

2. Extensive results are reported on NASBench-101, and the results are promising. 

3. The limitation of the assumptions are properly discussed. 

4. Performing NAS in polynomial complexity is of good research impact and real-world applications. 

Weaknesses:
1. There are some format flaws, e.g., the abstract should be a single paragraph. 

2. Table 2 is not informative enough. It would be better to include more details such as training epochs, augmentations, whether distillation is utilized or not, etc. for comparison. Plus, sometimes it is hard to tell whether LayerNAS is better than previous arts when MAdds or Params are not aligned (e.g., LayerNAS has 50% more params than MobileNetV3-Small). I wonder if it is possible to strictly align both params and MAdds and compare accuracy, as anyway, LayerNAS is a multi-objective search. 

3. I wonder if there are latency-driven search results. 

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper propose a novel approach of breaking down NAS problem into a Combinatorial Optimization problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and easy to follow. The authors provide clear explanations and examples throughout the paper. 

Breaking down the search problem into a Combinatorial Optimization problem seems novel and interesting, and reducing the search cost to polynomial time, which is clearly a breakthrough to the research community. 

LayerNAS can be applied to operation, topology and multi-objective NAS search

Results on ImageNet seems to surpass state-of-the-art methods by a clear margin, evidencing their effectiveness of LayerNAS. 



Weaknesses:
I do not particular have a question, this paper seems to be easy enough to follow. 

Limitations:
Yes

Rating:
7

Confidence:
4

";0
9buR1UFCDh;"REVIEW 
Summary:
This work proposes an extension to DQN aimed at improving projection steps in Q value updates. There are two main contributions of the paper:

- The paper intuitively explains the Q-value learning characteristics of DQN variants caused by a mismatch between the optimal Bellman operator and the set of representable Q functions.
- The authors propose the iterated DQN (iDQN) method, which keeps track of a collection of K online Q-functions. When updating these Q networks, the previous Q function is used as the target network.

Experiment results show that iDQN outperforms a collection of DQN variants on the standard Atari benchmark. Further ablation studies explore the effect of K and sampling strategies for iDQN and conclude that bigger K and uniform sampling of Q networks are in general preferable.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The figures explaining the projection characteristics of existing DQN variants are very intuitive and provide excellent motivation for the proposed method.
- The iDQN method, the newly introduced hyperparameters, and the experiment settings are communicated clearly and transparently.
- The results on Atari are convincing.
- The ablation studies on the choice of K and sampling method are insightful.

Weaknesses:
- The figures used to explain the projection behaviors of DQN variants are created for illustration, but not from actual experiments.
- As discussed by the authors, iDQN doesn't outperform more recent DQN variants which employ other tricks to improve performance.

Limitations:
The authors discussed how iDQN is not able to outperform more recent DQN variants using other tricks. It would also be nice if the authors can discuss the training stability of iDQN.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, a new variant of DQN algorithm, iDQN, is proposed by replacing the classical Bellman iteration with several consecutive Bellman iterations and using multiple Q networks.
Intuitively, this new Bellman operator propogates reward sigals more efficiently thus speeds up learning, with the cost of more computation and memory.
As the number of consecutive Bellman iterations increases, it is shown that the learning performance of iDQN in Atari games is also increased.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
As far as I know, the proposed method is novel. The new algorithm, together with several baselines, are tested in 54 Atari games.
Many illustrative pictures are included to help understand the new Bellman operator.
The paper is generally well-written.

Weaknesses:
1. It will make this work much better if a theoretical analysis of the proposed Bellman operator is provided, such as convergence speed, the affect of the number of Q networks, etc.
2. The performance of iDQN is only slightly better than baselines, such as DQN(Adam). A summarized result (e.g. the first figure in [DQN Zoo](https://github.com/deepmind/dqn_zoo)) can make the comparison in Atari games much clearer.
3. Missing related works about ensemble methods + DQN, e.g. Averaged DQN and Maxmin DQN.
4. Misssing baselines: DQN + n-step return. Both iDQN and this baseline try to accelerate the propogations of reward signals. Furthermore, although it is mentioned that ""We do not consider other variants of DQN to be relevant baselines to compare with."", more explanations are necessary.
5. It is claimed that ""Our approach introduces two new hyperparameters, rolling step frequency and target parameters update frequency, that need to be tuned. However, we provide a thorough understanding of their effects to mitigate this drawback. "". However, I don't find the understanding thorough enough.

Limitations:
The limitations of iDQN are that it takes more memory and computation than DQN. Jax is used to parallelize the computation, making the training time increase acceptable.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on learning the projection of the empirical Bellman operator's iteration on the space of function approximator (neural model). This being done through increasing the number of gradient steps using multiple heads with a certain form of update. While retaining the same total number of gradient steps and samples compared to common approaches, the proposed method seems to provide better results (at the cost of retaining multiple heads and more computation).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea is interesting, novel and practical. The paper also **experimentally** shows noticeable improvement over various baselines.

Weaknesses:
- The presented method is quite simple and could have been presented much more efficiently with simple math and direct explanation rather than lengthy discussions over multiple figures. 

- The choice of $K$ seems to have a significant impact on the behaviour, which also varies depending on the domain. Suggestion: some formal analysis (e.g., the algorithm's variance) could be useful to provide better insight about what to expect from larger $K$ in terms of other characteristics such as the transition kernel.

Limitations:
While the authors mentioned at the end of Introduction that ""We conclude the paper by discussing the limits of iDQN ..."", they apparently forgot to do so! No discussion of limitations is provided.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents Iterated Deep Q-Network (iDQN), a new DQN-based algorithm that incorporates multiple Bellman iterations into the training loss. The paper highlights the limitations of traditional RL methods that only consider a single Bellman iteration and proposes iDQN as a solution to improve learning. The algorithm leverages the online network of DQN to build targets for successive online networks, taking into account future Bellman iterations. The paper evaluates iDQN against relevant baselines on 54 Atari 2600 games and demonstrates its benefits in terms of approximation error and performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed iDQN algorithm introduces a novel approach to incorporate multiple Bellman iterations into the training loss, addressing the limitations of traditional RL methods.
2. The paper provides a well-structured review of relevant literature, discussing the behavior of various Q-learning algorithms in the space of Q-functions. This analysis helps in understanding the motivation behind iDQN and its relationship with other methods.
3) The empirical evaluation on selected Atari games demonstrates the superiority of iDQN over its closest baselines, DQN and Random Ensemble Mixture. This provides empirical evidence of the effectiveness of the proposed approach.

Weaknesses:
1. It would be helpful if the paper included more comparisons with widely-known baselines in the field. While the paper compares iDQN to DQN and Random Ensemble Mixture, it would be valuable to see how iDQN performs against other popular RL algorithms like R2D2.
2. Some parts of the paper could be further clarified to improve the reader's understanding. For example, the explanation of the loss function and the graphical representations of DQN variants could be made more concise and intuitive.

Limitations:
It would be helpful if the paper included more comparisons with widely-known baselines in the field. This paper has no negative social impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers the problem of how to get accurate approximations of optimal Q-functions. The paper introduces a new algorithm called iterated DQN (iDQN). iDQN incorporates multiple consecutive Bellman iterations into the training process, which aims to allow for better approximation of optimal action-value functions. It uses the online network of DQN to build a target for a second online network, and so on, for considering future Bellman iterations. The authors conducted several experiments based on Atari games by comaping iDQN with baseline methods, including DQN and Random Ensemble Mixture. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Significance and Originality: The topic that the paper studies - how to learn the Bellman iterations efficiently, is an important topic in reinforcement learning. The authors propose a simple yet effective method, called iterated DQN, to improve the learning efficiency. Specifically, iterated DQN uses a second online Q-network for learning the second Bellman iteration simultaneously, where the target for the second online Q-network is created from a second target network according to the first online network. In this way, the loss can include K-1 more terms compared to the original loss used in DQN. The way for using such kind of ensemble seems novel, which allows for improved efficiency.

- Clarity: The paper is well-written and easy to follow, with very clear illustrations for the update for DQN, REM, and iterated DQN as in Figures 1-4.

Weaknesses:
- Quality: The paper presents a simple yet effective idea, but it could be further strengthened particularly in theoretical and empirical analysis. First, the authors could provide a theoretical guarantee for iterated DQN by examining its convergence speed, in addition to the intuitive explanation given in Section 4.1. This would lend credibility to their claims. Second, the empirical validation raises concerns, as iterated DQN's performance is only marginally better than that of previous baseline methods such as DQN (Adam), C51, and REM. This modest improvement does not strongly support the paper's assertions. Lastly, it would be beneficial for the authors to include a memory comparison, as employing more Q-networks may lead to increased memory costs, which is an important consideration for practical applications.

Limitations:
The authors have discussed some of the limitations by considering other value-based methods.

Rating:
4

Confidence:
3

";0
XbInLmYLDr;"REVIEW 
Summary:
This paper proposes a two-stage framework for neural 3D reconstruction from disparate views via neural templates regularization. In the first stage, a network is trained for predicting shape templates. After that the volumetric surface reconstruction network with depth and SDF constraints is trained with the templates prior. Compared to other 3D reconstruction methods, DiViNet specifically targets disparate input images and achieves SOTA in sparse image views.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.The proposed DiViNet gets rid of the require of explicit cues, unlike former methods. Moreover, only a small set of images with small overlaps are used.

2.This proposed designs are technically sound.

Weaknesses:
1. Although DiViNet performs best with sparse view inputs, with dense view inputs, it's worse than the latest methods. According to Table 2, DiViNet is the second-best method. 
The DiViNet achieves SOTA in too many constraints to other methods. MonoSDF[1] using multi-resolution feature grids gets 0.73 on DTU dataset.

2. As mentioned in the paper, the neural templates lacks generalization in different data distribution. It matters how big the influence of the data distribution is. The authors claim the drawback of other methods is the requirement of dense views with overlap. The generalization to new data distribution is also important in consideration of real-world setting.

[1]MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction


Limitations:
Please refer to Weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes DiViNet for sparse multi-view reconstruction, which specifically targets on sparse input as few as three disparate RGB images. The key is to regularize the reconstruction process by learning a set of neural templates as surface priors, which is basically a set of 3D gaussian functions with optimizable features.  Extensive experiments are conducted to demonstrate the quality of DiViNet in sparse and disparate view settings.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
•	This paper focuses on an important and practice problem. Priors works fail to reconstruct accurate geometries when input views are sparse, while this paper proposes novel neural temporal regularization method to achieve good quality even with only three disparate images as input.
•	The idea of learning surface priors with neural templates is novel, and it’s effectiveness is also validated through the experiment section. 
•	Some interesting observations are draw from the experiment part, for example, under sparse view input cases, VolSDF and NeuS even fail to bear COLMAP, while under dense input cases, all the neural methods outperform COLMAP.


Weaknesses:
•	The ability of learned neural template to generalize to new scenes is not very clear. On one hand they are learned across different scenes, on the other hand the author claimed the need to be learned again when deployed to datasets from a different data distribution. A clarification and explanation on under what scenario can the learned template functions be reused will be very helpful.
•	Since each learned template is represented as a scaled, anistropic 3D gaussian, a visualization showing the learned gaussian from a scene and the corresponding sparse reconstructed pointcloud from Colmap will be helpful. I am especially curious about the positional distribution and the scale of Nt gaussian, whether the query points are affected by only a small number of 3D gaussians or not.


Limitations:
The authors have discussed potential negative social impacts of this paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper addresses the problem of surface reconstruction from spase input views. The authors adopt a SDF representation parameterized by an MLP. They propose learning a set of neural templates (in the form of 3D Gaussian functions) to serve as anchors in the reconstruction process to help stitch the surfaces over sparse regions. Reconstruction is carried out by optimizing the SDF (MLP) through minimizing the rendering loss and the Eikonal loss. The authors introduce a depth loss and SDF loss, both computed based on the estimated neural templates, to regularize the optimzation. Experimental results on the DTU and BlendMVS datasets show the proposed approach can reconstruct surface details to a reasonable extent from few disparate input views.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The template prediction network can be trained end-to-end using RGB reconstruction loss without any 3D supervision.
+ The predicted templates, which approxmiate points on object surface, help to regularize the SDF optimization, allowing the proposed approach to produce more complete reconstructions with reasonable surface details.
+ The depth loss, which computes the difference between the rendered depth and depth cues obtained from the predicted templates, and the SDF loss, which evaluates the signed distance at template centers, both sound logical and correct in regularizing the SDF optimization. Their effectiveness has been validated through ablation study.

Weaknesses:
- It is not clear about the generalization ability of the template prediction network. No training details have been provided in the paper. In the evaluations, are the same datasets being used in both training and testing the template prediction network? What would be the performance if the template prediction network trained on one dataset is applied to a completely different dataset? How will this (negatively) affect the reconstruction?
- Quite often the reconstructions also include incorrect background surface. Is that caused by incorrect template predictions? No discussions or analysis have been included in the paper.
- The description of the template prediction network architecture is very confusing (both in the main paper and the supplementary material). For instance, it is not clear how per-template feature are being sampled using bilinear interpolation from the extracted feature maps. There is also no explanations for the design of the dimension (C x M x sqrt(N_t) . M x sqrt(N_t) . M) for the volumetric feature grid.
- The formulation in (2) may be problematic. Note a ray will in general intersect an object surface in at least 2 points. This implies more than one template will produce a large value for w_k and therefore colors at multiple surface points will be mixed (as depth is not being considered).

Limitations:
Yes

Rating:
3

Confidence:
4

REVIEW 
Summary:
This work presents a volume rendering-based sparse view neural surface reconstruction method. For the hard sparse view reconstruction ,the authors propose to learn neural templates as surface priors to guide the learning of neural fields. The results on DTU and Blended MVS are better than NeuS and MonoSDF .

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The idea of learning templates for enhancing spase view reconstrucion is novel and make sense. 


Weaknesses:
This paper did not compare with the the most related works on sparse view reconstruction like SparseNeuS and VolRecon. Comparing only with NeuS / MonoSDF are not convincing, since this methods do not have special designes for sparse view reconstruction. 

The results on dense view reconstruction are quite poor, shown in Fig. 4. My concern lies in the poor results in dense view reconstruction, i.e. the artifacts in Fig.4, where all the baselines perform better. The quantitative comparisons shown in Tab.2 further deepen my concerns, where the SOTAs this paper not compared already reduce CD to less than 0.55 (e.g. Geo-NeuS), where this paper can only achieves comparable results with NeuS (0.84 vs. 0.79), which was published two years ago.

I do not think that choosing MonoSDF as the main baseline is convincing, since MonoSDF is mainly designed for scene-level reconstruction, where the monolar priors of MonoSDF are not suitable for object level reconstruction (e.g. DTU).
 

Limitations:
See the weaknesses above.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose a framework for sparse view 3D reconstruction from disparate views. A two stage approach is presented for reconstruction of a scene from posed sparse images. In the first stage a template is predicted from the sparse images, represented by a number of parametric 3D gaussians. The second stage uses the predicted template to reconstruct the scene from sparse views. An SDF representation is used to represent the geometry. State of the art performance is shown on shape reconstruction and novel view synthesis. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. **Clarity** : The paper is very well written with great attention to detail. Each component is adequately motivated. The approach section is built up in a very methodical and thorough manner. 
2. **Reproducibility**: The exact implementation details and training specifics are made clear, aiding in the reproducibility of the proposed approach. 
3. **Results**: The qualitative results especially for shape recovery is very compelling, particularly for scenes with wide baselines. 
5. **Novelty**: The use of gaussian templates to guide the reconstruction of the scene under sparse setting is a simple and elegant idea that is also easy to incorporate into existing frameworks as a form of regularization. This approach would serve as an important and strong baseline for sparse view 3D reconstruction method.
6. **Quantitative analysis**: The approach has been validated against a variety of contemporary approaches and state of the art performance is shown on chamfer distance for recovered surface. 

Weaknesses:
1. **Assumption on 3D information for stage 1**: The losses to train the template prediction network pretrains this network against dataset that have point clouds from COLMAP. Does this limits the applicability mainly to kind of scenes where COLMAP provides enough reconstruction information? Providing some more details about this prior is helpful. 
2. **Evaluation**: Although quantitative metrics are provided for geometry, also include image level metrics like PSNR/ SSIM/ LPIPS for the novel view synthesis task is potentially helpful to strengthen the narrative of the evaluation section.
3. **Effect of number of template gaussians**: An ablation study showing how the number of gaussians are chosen and the effect that this number has on the reconstruction quality is instructive.
4. **Additional ablations**: Quantitative ablations are provided for CD as a function of number of input views. However, the manuscript would benefit from the ablations below:
> - *Quantitative ablation* showing the effect of different regularization terms (particularly the SDF constraint and the depth constraints).  
> - *Quantitative/ Qualitative ablation* showing the direct optimization of posed sparse views without needing stage 1.  
> - *Qualitative ablation* showing the importance of $L_{cov}$ , $L_{radius}$ and $L_{var}$ in Stage 1. 
5. **Video Results**: Although not strictly necessary, including turntable video results of the recovered geometry in the supplm will help demonstrate the efficacy of the approach better.


Limitations:
Adequate treatment of the limitations of the approach has been provided. 

Rating:
7

Confidence:
5

";1
ekMLUoC2sq;"REVIEW 
Summary:
A new method is proposed to allow the simultaneous embedding of multiple attractors in an RNN through minimization of the energy function corresponding to the dynamics.
Two different methods to achieve this are considered, the first one based on the linearized energy function and the second on  constrained optimization. The second method optimizes the weights for a flat energy landscape with the constraint that there is no change in the center of mass of the bumps.


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The aim to flatten the energy landscape of an attractor network with multiple attractors embedded in it is novel.
The main aim and the methods are clearly described.
The discussed problem of detrimental inference is definitely very important for theoretical neuroscience.

Weaknesses:

There is a lack of assurance that the modified network actually has the equalized energy function maintains the bumps as minima of the energy landscape.


The procedure also doesn't necessarily contribute to a flat energy landscape, it just ensures that all the evaluated states have the same energy value.


A more thorough way to enforce flattening energy functions for RNNs is described in: Noorman, Marcella, et al. ""Accurate angular integration with only a handful of neurons."" bioRxiv (2022): 2022-05.


The time to implement the algorithm is also really long 72 hours for $L=60$ and does not seems to be practical.

The reason to use the bump score is not fully justified. 

Limitations:

They are adequately discussed.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper investigates the challenge of embedding multiple continuous attractor manifolds within a single RNN, with a focus on hippocampal place cells. The issues arise due to the presence of discrete steady states, visualized as minima on an abstract energy landscape, which disrupt the continuity of network activity patterns. This disruption prompts systematic drift of population activity patterns towards these discrete states, resulting in degraded memory over time. Past studies have considered the stabilizing influence of external stimuli; however, solutions in their absence remain unclear. The authors address this issue by modifying the synaptic weight to flatten the energy landscape, showcasing through simulations how this significantly stabilizes the activity pattern.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
o	The methodology and problem formulation are clearly articulated.

o	Simulations indicate that weight modification significantly improves the stability of activity patterns.

o	The authors provide the code in the appendix. 


Weaknesses:
o	While the manuscript is generally well-written, some areas could benefit from improvement. A diagrammatic illustration could help elucidate the issue of discretized states in the context of the energy landscape. Further details of the simulations (e.g., duration and specific inputs to the network) need to be provided. The appendix, which helps clarify the methodology, should be referred to more often in the Results section.

o	Strengthening the paper could involve additional simulations and/or discussions (see questions below).

o	There is a minor typo: ""emfbedded map"" should be ""embedded map."" 


Limitations:
The authors have acknowledged certain limitations, such as the unclear biological plausibility of their approach. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper studies the storage of multiple continuous attractors in a recurrent neural network. Specifically, the authors tackle the interference between attractors and its effect on activity bump drift. By using a perturbative approach, they compute a correction to the connectivity that reduces the drift dramatically.
Continuous attractors (e.g., ring model) are important models in neuroscience, and understanding them is an important task. Multiple attractors are relevant, for example, in the CA3 region of the hippocampus, where remapping of place cells in different environments is common. Nevertheless, a naïve connectivity that is a superposition of several ring-connectivities results in only approximate continuous attractors. The result is a few stable points in each attractor, to which dynamics converge.
The authors use the perspective of an energy function (Lyapunov), and examine how the interference renders this function non-flat. They then calculate the perturbation to leading order, and solve for a change in connectivity that will flatten the energy. Furthermore, using gradient descent, they are able to achieve even greater precision.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
Continuous attractors are a fundamental building block in the study of recurrent neural networks in neuroscience contexts. There are relatively few studies tackling multiple such attractors. The method to reduce interference is novel.

Weaknesses:
First, as the authors note, the resulting connectivity is extremely fine-tuned. This is a known problem with continuous attractors that is not addressed here.
Second, the problem and solution are highly related to similar problems in discrete attractors. Interference in Hopfield networks was tackled using pseudo inverse rules, either approximated online or as a global formula. There is no discussion of the relation to these works. The SVM approach of Battista and Monasson (Ref 5) is perhaps a similar example in continuous attractors.


Limitations:
yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work tackles the problem of interference between continuous attractors when they are held in a single RNN.  The authors adopted a Lyapunov function as a depiction of energy of the network and tried to flatten the energy landscape of attractors by adding a modulation term to the original connection matrix.  The modulation term was derived by two methods respectively: a first-order approximation of the energy function and a constrained iterative gradient-based method.  They showed that by constraining bumps at their initial position, the gradient descent achieved a better result.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The work achieved the goal of encoding multiple continuous attractors into a single recurrent network. 

Weaknesses:
1. The recent experimental data actually showed that in the remapping of cognitive maps in hippocampus, place cells encoding different maps actually have little overlap, i.e., the hippocampus recruits different groups of neurons to form different continuous attractors. In other words, the interference between multiple continuous attractors is only a mathematical problem, not a biological problem. This limits the contribution of this study to neuroscience. 
2. There are several issues in this study whose biological plausibility are not justified, the energy function (note the real neuronal connections are not symmetry), the modification of synapse strengths based on the attractors the network has stored, the gradient-based learning method. Overall, the insight of this study to neuroscience is rather limited. 
3. Some important references are missed, such as the work of Misha Tsodyks et al. on stroring multiple continuous attractors (PLoS Computational Biology?)


Limitations:
The limitations are discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors present a new technique for embedding multiple attractor manifolds into RNNs. To do so, they first randomly choose a number of attractor manifolds, embed these into an RNN, and then make weight adjustments to smooth out the interference created by multiple manifolds. The authors propose two strategies for weight adjustment, which consider first- and then second-order interference effects. These adjustments are shown to iteratively improve several intuitive metrics.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper addresses and interesting and well-defined problem. This is not my area of expertise, but the results seem quite general to understanding RNN function, and thus significant. 

The paper is well written and the results clearly presented. The stability metrics are intuitive.

The presented solutions are strikingly effective!

Weaknesses:
While the results are very strong, the paper only explores a single task (embedding 1D ring attractors). Do the results hold when moving to, say, two dimensions? How does the number of neurons in the network interact with task complexity?

Limitations:
One limitation that is already pointed out by the authors is that their approach is not a biologically plausible learning algorithm; however, I agree that this proof-of-principle is a solid first step and that investigating other mechanisms for weight updates is an interesting direction for future work.

Rating:
8

Confidence:
3

";1
Oj7Mrb4009;"REVIEW 
Summary:
The authors observe that the loss of some adversarial examples generated by the inner maximization process during training decreases, which they call abnormal adversarial examples (AAEs). Based on this, they discover a relationship between AAEs and catastrophic overfitting. Therefore, they propose an abnormal adversarial examples regularization to hinder the classifier from becoming distorted.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The finding and definition of abnormal adversarial examples are interesting. The corresponding analysis is comprehensive.
The proposed method can effectively alleviate catastrophic overfitting when the perturbation radius increases.


Weaknesses:
The experiments were not sufficient to comprehensively evaluate the proposed method. Experiments on a larger dataset (ImageNet-100, ImageNet) should be conducted. In addition, GAT (NeurIPS 2020) and FGSM-PGI (ECCV 2022) could be added as baselines.
30 epochs are insufficient. Experiments with more epochs should be involved to show that the proposed method can eliminate catastrophic overfitting.


Limitations:
See above.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work finds that abnormal adversarial examples(AAE) are generated during single-step adversarial training. And AAE has a deep relationship with catastrophic overfitting. According to the observation, authors propose a new regularization method AAER to regularize the numbers and output variations of AAER. Results show that their method can improve the performances of existed SSAT methods.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. Figures in this work express their ideas and observations clearly, such as Figure 2.
2. The method work for existed SOTA single-step adversarial training methods.
3. Results of diverse networks and datasets are provided. 
4. Confidence results are provided to show the stability of methods.


Weaknesses:
1. Compared to N-FGSM, N-AAER does not have an obvious improvement and takes more time.
2. AAER can improve the clean performance sometimes. For example, RS-AAER improves the clean performance a lot, 8%, compared to RS-FGSM when the $\epsilon$ = 16/255. However, RS-AAER decreases the clean performance when the $\epsilon$ = 32/255. And for N-FGSM, AAER always decrease the clean performance. What's the role of AAER for clean performances?
3. In general, the high performance of AAER is build on previous successful training methods. N-AAER is the best one because N-FGSM is the best one. It's difficult to say it's the AAER make the success as the improvement brought by AAER is not that obvious.

Limitations:
This work is about the safety in deep learning.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The paper deals with the mitigation of catastrophic overfitting in FGSM adversarial training. The paper first presents the properties of adversarial samples before and after catastrophic overfitting. Finally, the paper proposes a regularizer (Abnormal Adversarial Examples Regularization AAER) to mitigate catastrophic overfitting during FGSM AT. The proposed regularizer prevents the generation of abnormal adversarial samples and stabilizes the training process. The effectiveness of the single-step AT with the proposed regularizer is demonstrated on CIFAR 10/100, SVHN, and TinyImageNet datasets. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper presents important observations on adversarial samples before and after catastrophic overfitting (CO) (i.e., presence of pseudo-adversarial samples, population and properties of pseudo-adversarial samples before and after CO). Though not novel, these observations are presented from a CO perspective. The proposed regularizer harnesses these observations to mitigate CO. 

Weaknesses:
1. The paper presents important observations on adversarial samples before and after catastrophic overfitting. However, it fails to explain the cause for these observations (why do models generate abnormal adversarial samples during single-step AT?)  

2. The observations presented in this paper are not novel [1,2]. The paper presents the extreme case of gradient masking, i.e., the label leaking [1], where the accuracy of the model on adversarial samples (with large perturbation size) is greater than clean accuracy. Furthermore, not all pseudo-robust models generate abnormal adversarial samples.  

3. Comparison with existing robust single step adversarial training is missing [3-6]

4. The paper fails to demonstrate the effectiveness of the proposed regularizer. The proposed regularizer is plugged into existing robust single-step adversarial training methods (these methods mitigate CO) to show its effectiveness. Most of the results are shown in this setting, and a marginal improvement in robustness is observed. The robustness of the model trained using the vanilla variant is sub-par compared to models trained using existing robust single adversarial training methods (compare table-1 and 4 (supplementary)). 

[1] Kurakin et al. ""Adversarial machine learning at scale."" arXiv  2016.

[2] Tramer et al. “Ensemble adversarial training: attacks and defense” arXiv  2017

[3] Sriramanan et al. ""Guided adversarial attack for evaluating and enhancing adversarial defenses"" NeuRIPS  2020

[4] Sriramanan et al. ""Towards efficient and effective adversarial training."" NeuRIPS  2021

[5] Kim et al. ""Understanding catastrophic overfitting in single-step adversarial training"" AAAI 2021.

[6] Jia et al. ""Boosting fast adversarial training with learnable adversarial initialization"" IEEE Transactions on Image Processing,  2022.

Limitations:
The limitation of the proposed approach is not discussed. Authors are suggested to present failure mode/cases for the proposed approach.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduces the concept of abnormal adversarial examples (AAEs) and shows their strong correlation with the issue of catastrophic overfitting in single-step adversarial training. Based on this observation, the authors propose a new regularization method, AAER, which constrains the generation of abnormal adversarial examples to address this problem. The paper's insights, such as the sudden onset of catastrophic overfitting, are interesting, and the regularization-based approach is flexible enough to be combined with other methods. In addition, the experimental results confirm the effectiveness of the proposed method.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This paper provides a new perspective to explain the issue of catastrophic overfitting in single-step adversarial training.

2. Some observations in this paper on anomalous adversarial examples are interesting and may provide inspiration for subsequent works.

3. The proposed method is concise and intuitive, and it aligns well with the motivation of the paper.

4. This paper is well writen and easy to follow.

Weaknesses:
1. Some claims in this paper are debatable.

2. Insufficient ablation studies.

3. The experimental comparison is insufficient.

More detailed comments please refer to the questions.

Limitations:
N.A.

Rating:
4

Confidence:
3

";1
jRL6ErxMVB;"REVIEW 
Summary:
This paper conducts a detailed study on what attributes of data augmention in Visual Reinforcement Learning are playing essetianal roles. With extensive experiments on DMC tasks, four main findings are given regarding the strength and diversity. Based on these findings, authors introduce two new data augmentation techniques: `Random PadResize` and `Cycling Augmentation` and achieves reasonble improvement over DrQ-v2.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The attributes of data augmentation are well categorized and systematially studied, supported by extensive experiments.
2. The newly proposed data augmentation mechanism is well motivated and simple enough for the Visual RL community to widely adopt.

Weaknesses:
1. **Lack of novelty.** The given properties of data augmentation are not too surprising and so is the proposed mechanism.
2. **Limited performance.** As shown in Figure 8 and Figure 9, the sample efficiency is very close to DrQ-v2 (almost the same). However, the main claim made by authors is the improved sample efficiency. Moreover, from the curve trend in Figure 8 and Figure 9, a bit more training would possibly lead to the same performance of all these compared methods.
3. **Lack of enough training.** The training steps across all environments are not enough to make the curves converge, thus it is unclear whether the newly proposed mechanism could lead to better convergence or merely give a mild improvement in limited steps.

Limitations:
The limitations have been mentioned by authors in the conclusion section.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper explores the crucial attributes of domain adaptation (DA) in achieving sample-efficient visual reinforcement learning (RL) and emphasizes the specific requirements of DA for visual RL. Extensive experiments are conducted to investigate these attributes.

The paper introduces two practical guidelines that aim to maximize the potential of DA. The first guideline focuses on individual DA operations, while the second guideline explores multi-type DA fusion schemes. Based on these guidelines, the authors propose two improvement strategies, namely Rand PR and CycAug.

CycAug, which incorporates Rand PR as a key component, is shown to outperform existing methods in terms of sample efficiency. Through comprehensive benchmark tasks on DM Control and CARLA, CycAug demonstrates state-of-the-art performance in enhancing sample efficiency in visual RL.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The hardness of DA in RL has been well analyzed.

2. Rand Padresize: This paper introduces Rand Padresize as a novel augmentation method in reinforcement learning (RL). Unlike cropping, Rand Padresize retains all the information, which is highly advantageous. This unique approach addresses the challenge of hardness from augmentation in RL, making it a notable strength of the paper.

3. CycAug: The paper proposes CycAug, which employs a cycling method to overcome the potential disruption caused by excessively frequent variations. This demonstrates the authors' thoughtful consideration of the impact of augmentation strategies on the learning process. The inclusion of CycAug as a solution to mitigate disturbances is another strength of the paper.

Weaknesses:
1. Lack of novelty: The paper acknowledges the problem of hardness in RL as a well-known issue. 
2. Only use two augmentation in CycAug. 

Limitations:
- The problem of hardness from DA in RL has already been addressed, so there is no novelty in tackling it again.
- It is necessary to also discuss the case of using more augmentation methods in CycAug.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper explores the fundamental aspects of data augmentation (DA) in the context of visual reinforcement learning (RL) and introduces two methods, Random PadResize (Rand PR) and Cycling Augmentation (CycAug), to enhance its efficacy. Extensive experiments on the DeepMind Control suite and CARLA are conducted to showcase the superior performance achieved by the proposed methods.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1, This paper is well-motivated. The paper tackles a pressing and significant issue of sample efficiency in visual reinforcement learning (RL), which poses a crucial obstacle for the real-world implementation of RL agents across diverse domains.  
2, The paper presents a comprehensive and meticulous analysis of how the attributes of data augmentation (DA), including hardness, diversity, and fusion schemes, affect the sample efficiency of visual reinforcement learning (RL). Moreover, the paper highlights the unique requirements of DA for visual RL, distinguishing it from other domains such as supervised learning or adversarial training.  
3, The writing is clear and easy to follow.   
4, The paper showcases the effectiveness of the proposed methods on two demanding benchmarks for visual reinforcement learning (RL): the DeepMind Control suite and CARLA. These benchmarks provide challenging environments that allow for a thorough evaluation of the proposed methods.  

Weaknesses:
1, This paper overlooks the comparison of the proposed methods with other existing data augmentation (DA) techniques explicitly designed for visual reinforcement learning (RL), such as Spectrum Random Masking [11] or PlayVirtual [12]. It would be intriguing to observe and evaluate the performance of the proposed methods in relation to these techniques in terms of sample efficiency, generalization, or diversity. Incorporating such a comparison would provide a more comprehensive understanding of the strengths and limitations of the proposed methods within the context of existing approaches for visual RL.  
2, The paper lacks ablation studies or qualitative analysis that elucidate the individual contributions of each component or attribute of the proposed methods to their effectiveness. For instance, it would be valuable to compare Rand PR with PadCrop or Translate, examining factors such as hardness or spatial diversity. Additionally, evaluating CycAug against other fusion schemes in terms of type diversity or data stability would provide further insights. Including such ablation studies and qualitative analysis would enhance the understanding of the proposed methods and their specific strengths relative to alternative components or attributes.


Limitations:
Please refer to the weakness.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper explores the conditions for achieving sample-efficient visual RL with data augmentation. Based on the findings, two guidelines are proposed: one emphasizes sufficient spatial diversity with minimal hardness, leading to the introduction of Rand PadResize. Additionally, the data-sensitive nature of RL training is considered when designing multi-type data augmentation fusion schemes in visual RL. Drawing inspiration from this guideline, a RL-tailored multi-type data augmentation fusion scheme CycAug is proposed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper exhibits strong writing quality and clear communication of ideas. The content is well-structured and effectively presents the research findings. The authors' explanations and descriptions are concise, enabling easy comprehension of the key concepts and methodologies discussed. Overall, the paper demonstrates a high level of writing proficiency.


The paper presents a comprehensive analysis of effective augmentation techniques for visual reinforcement learning (RL). The authors delve into the topic with depth, examining the hardness and diversity of various augmentation methods and their impact on the performance of visual RL algorithms. The analysis is thorough, providing valuable insights into the benefits and limitations of different augmentation strategies in enhancing visual RL.


Weaknesses:
1、	To obtain specific values for ""Strength D"" and ""Spatial D,"" it is necessary to train a strategy on raw data before estimating diversity, as mentioned in line 137. Does this imply that training the strategy using the original, unmodified data precedes the selection of suitable augmentation techniques?  

2、	The author asserts that CycAug promotes stability throughout the training process, which raises curiosity about the variance of Q values during training. It would be beneficial for the authors to provide further analysis of the stability results. Additionally, considering that SVEA is another method to enhance stability in the context of augmentation, it would be valuable to compare the advantages of both methods when utilizing the same augmentations. 


Limitations:
What are the limitations of the proposed method? I know there is something about this in the paper. But it is not solid and broad.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents a thorough empirical analysis of visual data augmentations and their effects on RL training. They benchmark various spatial augmentation on two axes of variation, spatial diversity, and hardness, measured by the amount of distortion created in the image. The authors perform a series of experiments to benchmark the effect of augmentations along these two axes and propose best practices for training visual RL policies. Additionally, the authors offer a new data augmentation named Random Pad Resize and empirically demonstrate its benefits. They also propose a multi-DA fusion scheme, named CycAug which boosts sample efficiency even further and prevents training instability. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Works like these perform a systematic empirical analysis of a known technique to help bring the community on a common page about its usage are very useful. 
2. Proposal of the CycAug method for multi-DA fusion, is a simple, but clever idea, and in combination with RandPR shows state-of-the-art sample efficiency on two benchmarks.
3. The paper is well-written and easy to follow.   
4. The authors have provided code and an extensive discussion of their experimental setup in the supplementary material. 



Weaknesses:
1. The combination of RandPR and CycAug shows improved performance over previous methods. I failed to find any analysis that isolates each part and analyzes its impact on existing methods. 
2. Given that the major contribution of this work is a thorough empirical analysis of existing data augmentation strategies, adding breadth to the experiments and including Embodied environments like Habitat or AI2THOR, or manipulation benchmarks like MetaWorld and studying the effect of augmentations would make this paper even better. Note that the absence doesn't make the work any less useful.

Limitations:
I would encourage the authors to include a section in the main paper or supplementary about the potential societal impacts of their work. The section is currently which may or may not be against the Neurips policy.  

Rating:
6

Confidence:
3

";1
xNUmTRYtV1;"REVIEW 
Summary:
This paper considers approximate message passing algorithms for reconstructing a rank-1 signal when corrupted by a symmetric matrix of noise with a block-variance structure; it is assumed the signal _x^*_ has iid coordinates generated from a prior distribution.  One then forms the matrix $Y = x^* (x^*)^T/\sqrt{N} + A \odot \sqrt{\Delta}$, where $\Delta$ is a blockwise constant, positive matrix.  The matrix $A$ is a symmetric gaussian matrix with iid off-diagonal entries of size $N \times N$.   

The number of blocks $q$ appears to bounded independent of matrix size, and it is assumed one has access to the scale matrix Delta.

The first algorithm is a proper generalization of an AMP recursion for the case of a rank-1 signal with Wigner noise.  The state evolution is shown to converge (Theorem 1.2 -- using a modification of existing techniques) and correspond to the solution of the Bayes optimal estimator (Theorem 1.4).  

The spectral method designs a linear recurrence which is (locally? and conditionally?) optimal, in that it also recovers the same estimator by computing a principal eigenvector of an associated matrix.  

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1) The mathematical content is sound.  The main content (theorem 1.2) is proven in the supplemental material.  This adds an algorithmic aspect to a model which has attracted recent information theoretic attention.
2) The paper presents an algorithm which is optimal for the problem posed.  The algorithm is part of a larger well-studied class.
3) The performance of the algorithm is illustrated numerically in a simple case.
4) A linear method is presented, which may reproduce the more complicated general AMP performance for the hidden spike.

Weaknesses:
0) The article does not provide any broader context for the technical results it develops; all but the first few pages are technicalities related to AMP theory.  It is targeted at experts in approximate message passing, and it does not develop much of the information-theoretic aspects of the model (which I gather are proven in Guionnet et al).  There is no conclusion.  There is nothing in the way of practical considerations or relations to application (although I would say this alone is forgivable, if the paper were otherwise immaculate).  Much of the main text is occupied by technicalities related to the formulation of the main theorem and in summoning relevant AMP theory from Javanmard and Montanari as well as Deshpande et al.  
1) The main theorem (1.2) is an adaptation of an existing result.  Moreover, approximate message passing algorithms are well studied and many theorems exist for them.  The presence of non-iid noise makes it somewhat unique, but I think it is fair to say that this is not a big extension of existing theorems.  (In particular, there is a change of variables to connect the homogeneous and inhomogeneous cases).
2) The spectral method, which is introduced, is largely left half-baked.  There is a conjecture (1.6) related to it, and there is a 1 page description of how the method is developed.  There is an equation (42) showing that the overlap evolution is unstable when a certain Perron-Frobenius eigenvalue is larger than 1.  But finally, the main points here are left as conjectures.

Limitations:
The assumptions are clear.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the (symmetric) rank-1 matrix estimation problem with inhomogeneous noise. Here inhomogeneous noise refers to a symmetric noise matrix that is block-wise constant where the number of blocks is a constant relative to the dimension. 
This paper proposes an approximate message passing (AMP) algorithm and shows the corresponding state evolution result. 
Another piece of contribution is the design of a spectral algorithm that outputs the principal eigenvector of a rescaled and recentered matrix. 
Numerics suggest that this outperforms the naive estimator of the principal eigenvector of the data matrix per se. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The most interesting (at least to me) part of the paper is Section 3 where a nonstandard spectral algorithm is introduced and analyzed to some extent. 
As the authors commented, this estimator outperforms (at least numerically) the naive one corresponding to the original matrix Y. 

Another satisfactory aspect of the result is the coincidence between the fixed point of AMP and that of the Bayes-optimal estimator (i.e., E[x^* | Y]), though this is not surprising. 

Weaknesses:
1. The majority of the paper is devoted to AMP and its state evolution whose proof is a rather standard reduction to the matrix-valued AMP by Javanmard--Montanari. I didn't check the details carefully since everything goes as expected. But it's still good to see things written down formally. 

2. Section 3 is interesting at a heuristic level. However, I have a doubt regarding the authors' claim. 

In line 131, it is claimed that ""we rigorously show that with SNR<1 our proposed spectral method fails to recover the signal"". I don't think the analysis in Section 3 constitutes a proof of this claim. 
It was shown that the trivial fixed point of an AMP with linear denoiser is attractive when SNR<1. I agree with this, but this does not imply that the asymptotic overlap of the spectral estimator is 0 when SNR<1. 
The iterate of the linearized AMP converges (in constant number of steps) to the principal eigenvector (i.e., the spectral estimator) only when a spectral gap is present. 
When SNR<1, there is no spectral gap and it is unclear how the iterate of linearized AMP is related to the principal eigenvector. 
It may converge to some other vector, depending on the initialization. 
In fact, rigorously speaking, I think it's fundamentally unlikely to prove subcritical behaviour by exploiting linearized AMP. 
The analysis only proves the attraction of 0 when SNR<1, which is an *evidence* that the phase transition threshold is 1.
However, this implies neither ""spectral fails when SNR<1"" nor ""spectral works when SNR>1"". 

3. If I understand correctly, the Delta matrix is assumed to be *known*. A very important aspect that was not discussed at all (correct me if I'm wrong) is what happens when Delta is unknown which appears (to me) to be a slightly more realistic assumption. 
In that case the Bayes-AMP is no longer a practical algorithm (even with warm start) and the spectral algorithm is also not computable. 
In fact, is it fair to say that the proposed spectral algorithm outperforms the naive one *because* it uses Delta information?
Can the price of lacking the knowledge of Delta be quantified?
I know this may go beyond the scope of the present paper. 
But it seems to be an interesting nontrivial point that's worth mentioning/discussing. 

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper provides an analysis of an AMP algorithm for the spiked Wigner model with inhomogeneous noise. The paper builds on the matrix AMP framework to derive the state evolution equations for the considered AMP recursion for the studied model. The paper further shows that if the denoising functions are the Bayes one, then the fixed point equation of the state evolution of the AMP algorithm is the same as the one satisfied by the Bayes optimal estimator.

The paper also leverages the developed machinery to study the properties of a spectral algorithm which is motivated by considering the the identity denoising functions. It is conjectured that this spectral algorithm exhibits optimal phase transition.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I find the paper to be generally well-written and it is not hard to follow. The problem that is considered is interesting and the presented results generalize previously known results for the spiked Wigner model with homogenous noise to the non-homogenous case.

Weaknesses:
Minor comments:
- Page 1: Please state whether $\tilde{\Delta}$ and/or $g$ are assumed to be known. From Eq.(3) it seems that we do assume that $\Delta$ is known.
- Page 2, line 82: The notation for $f_t:\mathbb{R}^N\times \mathbb{N} \to\mathbb{R}^N$ is confusing/informal because from the displayed equation, it seems that $f_t$ takes input from $\mathbb{R}^N$. Is $t$ supposed to be the input integer in $\mathbb{N}$?
- Page 2, line 83: If $f_t^a$ are general Lipschitz functions, it is not clear to me why $f_t$ is linear.
- Page 3, line 112: It seems to me that if we replace the second moment assumption by finite $k$-th moment, we are strengthening the assumption, not weakening it.

Limitations:
No concerns regarding potential societal impact of this work.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper considers the spiked Wigner problem with inhomogeneous noise, i.e. the inverse problem of estimating a rank-matrix through an inhomogeneous noise channel. This problem naturally arises in many applications and a universality result makes the problem considered quite general with regards to the noise distribution.

The authors have made several contributions in this paper:
1. They have derived the AMP recursions to solve the spiked Wigner problem.
2. The most interesting property of AMP-like methods is that their behavior can be characterized exactly through a set of low-dimensional state evolution equations. This paper obtains the state evolution for the AMP recursion that solves the problem considered.
3. The authors analyze the AMP algorithm with identity denoisers and show that it corresponds to an spectral method for a specific matrix. More interestingly, the authors conjecture that this linear version of AMP detects a spike in the same region as the general AMP.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper considers an interesting problem with many applications. 
- The authors derive an AMP method to solve this problem.
- The authors obtain the state evolution of this AMP recursion, thus fully characterizing the macroscopic behavior of the AMP recursion at **each** iteration of the algorithm and **not** just the fixed points. This result gives us a **theoretical** way of obtaining the estimation error of the AMP method using very general metrics
- The authors show that when the Bayes optimal denoisers are used (i.e. the mean of the posterior) the fixed point equations exactly match the Bayes optimal fixed point equation of another recent work.
- Finally, the authors analyze the linear version of the AMP algorithm and show it is equivalent to spectral method for a specific matrix.

To summarize, the authors look at the spiked Wigner problem with inhomogeneous noise, derive an AMP method to solve it, and fully characterize the theoretical behavior of the AMP method in a certain high-dimensional asymptotics. This is in contrast to many other methods that are used in practice, but have no theoretical guarantees or guarantees of the form of high-probability upper bounds on the error in certain metrics. AMP methods, allow us to obtain the **exact** error in many **different error metrics** in a certain **high-dimensional limit**.

Weaknesses:
The weaknesses that come to my mind are mostly the usual weaknesses of the AMP algorithms:
- The AMP algorithms are often described as not very useful to solve problems in practice due to their instability often requiring a lot of tweaks such as damping to make them converge. I do not see much comments in this work regarding the stability of the AMP method described. This is in part due to generality of the algorithm (described using general denoisers) and in fact for the linear case the authors show a condition for convergence, however no mention of convergence or potential issues in the general case is mentioned. I should admit however that for example Bayes optimality of the AMP method with Bayes optimal denoisers makes this method interesting in practice for this problem.
- That being said, AMP having theoretical guarantees would still be very valuable as a theoretical tool. However, obtaining the errors through the state evolution equations are often very nontrivial due to the need to calculate expectations in a recursive formula that often need MCMC methods and are sometimes computationally not much better than running AMP for several instances of the problem and estimating the final error.
- As mentioned above, the AMP results only hold in a certain asymptotic regime. However in practice, very good match is observed even for moderately sized problems as the authors also mention.

## Minor comments
I believe the work is hard to follow for someone who is even familiar with the problem considered but not familiar with the AMP literature. It assumes the readers are knowledgeable in this area and very familiar with previous works which makes it hard to follow.

Limitations:
- The practical limitations of the AMP algorithms and how easy/hard it is to use the state evolution in practice are not adequately discussed.

Rating:
6

Confidence:
4

";1
mA7nTGXjD3;"REVIEW 
Summary:
This work proposes an analysis of the independent natural policy gradient algorithm for Markov Potential games. Under technical assumptions on a sub-optimality gap problem dependent quantity and supposing access to exact (averaged) advantage functions, this paper provides a novel $O(1/\epsilon)$ iteration complexity to guarantee that the average Nash gap along iterations is smaller than the accuracy \epsilon, improving over the previously known $O(1/\epsilon^2)$ iteration complexity in the same independent learning setting. After discussing the potential game setting as a warm-up and generalizing to the MPG setting, the paper provides simulations in a synthetic potential game and a congestion game. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The convergence analysis improves over the $O(1/\epsilon^2)$ iteration complexity in prior work under some technical assumptions.

- To the best of my knowledge, the analysis provided in this paper is new and the proofs seem to be correct (I went through the appendix, please see some additional comments below). However, please see some comments below regarding related works and quantities/notations to be precised for clarifications. In my opinion, the main technical novelty is Lemma 3.2 (and Lemma B.3 relying on the technical lemma C.1) which is key to obtain the result in Theorem 3.3. This result is interesting and connects the sum of the ‘gaps’ (over agents) for a fixed temperature parameter $\eta$ and for $\eta$ going to infinity. 

- The paper is well-organized. 


Weaknesses:
**1. About the suboptimality gap \delta_K and the $O(1/K)$ convergence rate**:  
Theorem 3.3 provides an upper-bound on the average NE-gap which depends on the K-dependent quantity $\delta_K$ without further assumption, this provides a $O(1/K\delta_K)$ convergence rate as mentioned in the paper. 
I think it should be clearly stated that the $O(1/K)$ convergence rate which is claimed in the abstract and in the contributions under ‘mild assumptions’ (as it is stated) is actually “asymptotic”.  Indeed, under assumption 3.2, the bound in Corollary 3.6.1 features an unknown constant (number of iterations) $K$’ that is not explicit in the problem parameters (such a constant is only guaranteed to exist under assumption 3.2). This is due to the fact that the NE gap is only controlled (independently of $\delta_k$) for a large enough number of iterations as it appears in l. 500 in the appendix (p. 17) under the chosen assumption.  The results that the paper compares to in Table 1 have stronger guarantees in the sense that they are not asymptotic. This is not clear in the presentation and the comparison to prior work. 

**2. About Assumption 3.2**: this technical assumption guarantees that $\delta_K$ is uniformly lower bounded away from zero but it seems hard to interpret or give a meaning to this assumption although the ‘sub-optimality gap’ $\delta_k$ is a standard quantity in the bandit literature for instance. I am also not sure if this assumption is ‘mild’ as it is formulated. See the ‘Questions’ section for clarification questions regarding this assumption. 

**3. Discussion of related works**: Some relevant related works are missing in the discussion. 

**(a)** While Song et al. 2021 [27] is cited, it is not mentioned in the discussion that a $O(1/\epsilon)$ iteration complexity has been achieved in that work for MPGs with a Nash-coordinate ascent algorithm (see Section 5 Algorithm 7, note that sample complexity is given and the $O(1/\epsilon)$ iteration complexity appears in the proof when discarding the $O(1/\epsilon^2)$ sample complexity needed for policy evaluation). However, this algorithm is ‘turn-based’ and requires coordination and hence is not independent as the present work. 

**(b)** Fox et al. 2022 derived an asymptotic convergence result for the independent natural policy gradient algorithm considered in this work that seemed to be later used by Zhang et al. 2022 [33] but this reference does not appear in related works. This same result seems also to be used in Proposition 3.1 of the paper. 

**(c)** The results shown in this paper seem to have some similarities with the asymptotic convergence analysis provided by Khodadadian et al. 2022 in the single agent setting. For instance, the analysis in that paper introduces the optimal advantage function gap (see $\Delta$ as defined in Definition 2), a quantity similar to the sub-optimality gap $\delta_k$ in the present paper (up to the multi-agent setting). However, I would like to point out that this is just for the purpose of comparison and the present work has to overcome many difficulties related to the game theoretic setting and the multi-agent nature of the problem that make this work very different from Khodadadian et al. 2022. The abstract precises that the result improves over “ $O(1/\epsilon)$, that is achievable for the single-agent case”. Actually, Khodadadian et al. 2022 provide an asymptotic geometric convergence rate. Other recent related works even prove a global linear rate with increasing step sizes for the natural policy gradient algorithm (see for e.g. Xiao 2022, Section 4.2).  

Fox et al. Independent natural policy gradient always converges in Markov potential games, AISTATS 2022.

Khodadadian et al. On linear and super-linear convergence of Natural Policy Gradient algorithm,  Systems and Control letters 2022.  

Xiao. On the convergence rates of policy gradient methods. JMLR 2022

**(d)** minor remark: you might want to give reference to [Monderer and Shapley 1996, Potential Games, Games and Economic Behaviour 14, 124-143] which introduced this class of games in section 3.1 when you mention [12] (l. 128) which is much more recent. 

**4. Regarding the definition of Markov potential games** (Definition 2.1), for a fair comparison in Table 1, it might be worth mentioning that this definition differs from the one considered in [8,16] although it matches the definition used in [33, 34]. Indeed, the potential function in Definition 2.1 is supposed to have a discounted cumulative structure whereas such a structure is not available in the more general definition considered in [8,16]. As a matter of fact, the analysis becomes more challenging in [8] for instance since showing the potential function improvement is then more involved in that case, another decomposition different from the decomposition in Lemma B.2 l. 440 is then used to guarantee policy improvement (see Lemma in [8]). Also the dependence with respect to some parameters such as $(1-\gamma)$ and the state action space sizes are usually improved with this additional structure. 

**5. Originality of the analysis:** While Lemma 3.2 and its use is indeed new, the potential function improvement lemmas (Lemma 3.1, Lemma 3.5) and their proofs in appendix follow prior techniques used in [4, 33]. I suggest that authors mention this somewhere in the main part or in the appendix and emphasize the novelty of the analysis (Lemma 3.2). For instance Lemma A.1 was proved in [4], the proof of Lemma A.2 is almost identical to the proof in [4] while the proof of Lemma B.2 is very similar to the proofs in [33].   

**6. Clarity**: Overall, writing can be substantially improved in my opinion. Few minor details below: 

— l. 108: what do you mean by ‘multiple stationary points for the problem’? Stationary points if the potential function?

— l. 134-135: not very clear, see the ‘Questions’ section below. 

— l. 136-137: ‘for any two sets of policies’. I guess you mean any two joint policies (in the product of the individual simplices) when you say sets.

— l. 158: ‘They are related by the following lemma’, the constants you just defined in l. 157 or the quantities defined few lines above in l. 153?

— notation $f(\infty)$ is used in Lemma 3.2 and does not seem to be defined before although it is quite obvious this corresponds to $\lim_{\alpha \to \infty} f(\alpha)$ as used in l. 153. 

— Lemma A.4 in the appendix: the statement and the proof are not very precise. What is $\mathcal{R}_i$ (it seems only defined in the end of the proof of this lemma, or we can guess it from the title of the lemma)? I guess this is the set of reward functions which is known to have a particular structure for MPGs as you write it. $\mathcal{R}_i$ is a linear space in which ambient space? Please precise the proof even if we can guess the idea. Also, where is this result used? 

—  Lemma C.1: if rewards are vectors, please precise somewhere in notations that inequalities hold for all the entries of the vectors.  

**Typos (main part and Appendix)**: 
 l. 91: $V_i(s)$; do you mean $V_i^{\pi}(s)$? 

l. 476-477 (proof of Lemma B.2 in appendix): $\bar{A}_{f_i}^{\pi^k}$, what is $f_i$? I guess you mean $h_i$.
 
l. 479 to l. 480: $\pi_i^{k+1}(\cdot|s)$, $s$ is missing in the first term of the last equation and in the KL divergences.

l. 503 (proof of corollary 3.6.1): is there a missing $\phi_{\max}$ in front of $K’$ in the first inequality of the page (given l. 501)?  

Limitations:
The paper mentions the MPG setting as a limitation in the conclusion. 

**Extension to the stochastic setting:** While the analysis in the deterministic setting is an important step towards understanding the more practical stochastic setting where exact advantage functions are not available and can only be estimated via sampled trajectories, this analysis does not seem to be easily extendable to the aforementioned stochastic setting. This seems to be related to the fact that showing that the constant c is positive in the stochastic setting seems to be much harder if not hopeless even in a single agent setting (see for e.g. Mei et al. 2021). However, this is a limitation that also applies to prior work in MPGs analyzing independent natural policy gradient such as Zhang et al. 2022 [33]. A comment on this or an additional remark in the paper would be welcome. For instance, [8] analyzes the sample complexity in the stochastic setting but their algorithm does not cover the case where the regularization in the policy mirror descent-like update rule is not euclidean, KL regularization (which leads to natural policy gradient) is not covered. 

   Mei et al. ‘Understanding the effect of stochasticity in policy optimization’ (Neurips 2021)

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper provides a new analysis for the (Markov) potential games with a $O(1/\epsilon)$ convergence rate. The new results are problem-dependent and may be a tighter guarantee for certain classes of potential games. Asymptotic guarantees are also provided to elaborate on the problem-dependent nature of the results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The results it presents showcase an improvement over previous results on (markov) potential games. The new rates are now unaffected by the number of actions and are at the order of $O(1/\epsilon)$. The problem-dependent nature of the results may provide a tighter guarantee on a certain class of potential games and the new analysis methods may lead to future works. The results are extensively discussed and empirical results are provided to corroborates the theoretical results. 

Weaknesses:
The new results do not seem to be directly comparable to the previous ones due to the use of a suboptimality gap. I encourage the authors to explicitly state this early in the paper to avoid confusion (e.g. in Table 1). I also encourage the authors to complement the work with more empirical results. For example, it would be interesting to see how the algorithm performs against previous ones when the suboptimal gap is very small. It may also be helpful to state when the suboptimality gap is very small, the results can degenerate into previous results. 

Limitations:
Yes, it is discussed in the conclusion part.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The article considers what the authors describe to be “natural policy gradient” learning in normal form and Markov potential games. This is the discrete time algorithm in which individual players' mixed strategies are multiplied by a softmax response to the opponent mixed strategies then normalised. Convergence rates are derived in both normal form and the Markovian potential games.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
The article is nicely written, and the claimed results are supported by the theory.

The convergence rate results are nice, given the general difficulty to provide convergence rates in anything multiagent, and especially in Markovian games - of course restricting to situations in which there is a potential function for the full Markovian game makes things much less impossible, but it is still a hard problem.

The paper is nicely self-contained, and a real pleasure to read.

Weaknesses:
I have some doubts over the claimed results which I would like to see clarified.

The authors claim that Thm3. Implies a 1/eps convergence rate. I don’t buy it I’m afraid. c and delta_K are not controlled. They could be arbitrarily small, at least without further work. I suspect c is okay, although a sudden switch in best response could easily result in a very small pi_i^k(br_i(pi_{-i}^k)) coming into c late in the process. And since delta is the optimality gap when playing a mixed strategy, I find it very difficult to see how to constrain it effectively. (I think line 158 tells us that c is the smallest ever pi_i^k(br(pi_{-i}^k)), and delta_K is the smallest optimality gap that occurs up to time K – if I have misinterpreted this then my objections may dissipate!)

The synchronous form of ""learning"", and the tight construction of Markovian potential games with average reward, means the step up to Markovian settings is much less then in a less restrictive setting.

A more philosophical point is that the article assumes players can receive oracle information about the long term payoff of any action. While it makes for a nice compact paper, I think for NeurIPS the authors need to at least posit some suggestions for where learners might be able to access the advantage functions that are required to implement the method.

Furthermore, the dynamic is well known as the multiplicative weights algorithm, and I would expect the authors to compare their results with those presented under the multiplicative weights description.

Limitations:
No discussion of limitations is presented. I don't feel such a discussion would add much to the paper, but the review form asks the question!

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the convergence of Natural Policy Gradient method (NPG) in Markov Potential Game. Under stronger assumptions, the convergence result improves upon previous ones.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Under the considered setting, NPG is shown to achieve a $1/K$ convergence rate for multi-agent Markov Potential Game, matching the convergence rate in single-agent setting. Discussion on the parameters that appear in the convergence rate is presented in detail.

Weaknesses:
The convergence result only makes sense under the assumption that $c>0, \delta>0$. However, such assumption could be too strong and unnecessary (as the table 1 indicates).

Limitations:
N/A

Rating:
6

Confidence:
4

";1
4ULTSBBY4U;"REVIEW 
Summary:
This paper proposes both an analysis and a contribution to fix a problem found during the analysis.

They start with the premise that diffusion models should be able to generate arbitrary size images, and training specialized models for each image size is too expensive, which is correct. Using diffusion models trained for square images would be much cheaper and easier.
They identify two key problems when using square models to generate arbitrary aspect ratio images: incomplete/inadequate objects and repetitive/disorganized patterns.
In order to improve performance all around (quality, prompt following, etc.) they study entropy in the generated image.
Specifically, they note that as entropy rises tokens attend to wider regions, and relate this phenomenon to the problems delineated above.
Finally, they find that simply proposing a scaling factor mitigates many of these issues.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The idea to add a scaling factor that counteracts entropy fluctuation in a training-free manner is really nice and the formulation is a principled approximation that is easily computable.
2. The implementation is incredibly simple, a large plus.
3. The work presents strong quantitative evaluations as well as a strong user evaluation that is commendable.
4. The work presents a wide amount of qualitative samples that show improvement in very common problems for diffusion models such as double heads, double hands and other issues. Samples look good.

Weaknesses:
1. Hard to find many weaknesses, it's a strong paper that is well written, with a clear argument, clear analysis, clear solution that simply seems to work after both quantitative and user study evaluation (and with substantial qualitative samples showing edge cases).

Limitations:
1. Good limitation section. Metrics are a bit of an issue but there is a strong user eval in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper analyzes the issues of using a fixed-resolution diffusion model to generate varied-size images and proposes a scaling factor to stable the attention entropy which remedies the issue. The method is evaluated on text-to-image models when the inference resolution is moderately different than the model resolution and the results show the effectiveness of the proposed method compairing to directly using fixed-resolution diffusion model for varied-size synthesis.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow. The motivation is clearly explained and it is with some theoretical analysis.
2. The proposed method is simple to implement, which does not require costly large scale training and could help varied-size synthesis with a fixed-resolution diffusion model.
3. The effectiveness of changing the scale factor in attention for varied-size synthesis is an interesting observation.

Weaknesses:
1. Synthesizing images at different resolutions is an important research problem, but it is not very clear that why a fixed-resoltion pre-trained model should be used for this task (which is the main baseline in this work). To achieve this goal, there are many other candidate methods: (1) using diffusion-based (e.g. cascaded diffusion models) or GAN-based super-resolution method to upsample the output to some resolution of powers of 2, then downsample it to the query resolution. (2) using a any-scale super-resolution network (e.g. LIIF) to upsample the network result. It is not clear that why modifying fixed-resolution model is an important research topic if it's not shown to outperform other candidate methods.
2. The test resolutions are still in a small changing range compared to the model resolution (about x0.5\~x1.5), and the FID on x0.5 scale suggests that the quality could be significantly decreased when changing the scale to be different than training scale. Does this suggest that the proposed method does not work for a wider range of scales (for example, any-scale SR works typically evaluates x1\~x4, or even extrapolates to x20\~30)?

Limitations:
Yes

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work adapts a pre-trained Stable Diffusion model for variable-resolution image generation. Since Stable Diffusion is trained on a fixed image resolution, naively varying the output size results in abnormal patterns in the images. This paper tracks the problem down to self-attention weights in the denoiser network, and identifies the so-called attention entropy as the root cause by proving that attention entropy is fundamentally correlated with image resolution. Drawing on this insight, the paper introduces resolution-dependent scaling to self-attentions to calibrate attention entropy throughout the generative process. The effectiveness of the proposed method is validated through extensive experiments.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The method is training-free. It works by modifying the generative process of a pre-trained diffusion model at inference time.

- The method reveals a strong connection between self-attention entropy and image quality. It builds on the key finding that attention entropy is correlated with image resolution, and artifacts in low / high-resolution images can be attributed to mis-calibrated attention entropy. It thus attempts to calibrate attention entropy across various image resolutions using a resolution-dependent scaling factor. Turning theoretical insights into a simple, actionable solution is a key strength of the paper.

- The effectiveness of the method is validated by qualitative, quantitative and user study results. The improvement over the Stable Diffusion baseline seems quite consistent.

Weaknesses:
- A few simple baselines are missing. For example, one can simply down-sample a 512x512 image to reach a lower resolution. Similarly, one can generate a 512x512 image and subsequently up-sample it using an off-the-shelf super-resolution model. In fact, Stable Diffusion supports super-resolution and uneven aspect ratios with community effort (check out AUTOMATIC1111). To justify the main contribution of the paper (i.e., improved generation quality of variable-sized images), it is important to show that the proposed method performs equally well, or even better, compared to these simple baselines.

- Arguably, generating high-resolution images is of greater interest to the community. To this end, it would be interesting to probe the limit of the proposed method. That is, what is the highest resolution it can handle without introducing noticeable artifacts? Most high-resolution images in the paper are 768x768. There is one image in the supplement at the resolution of 1024x1024. The proposed method will generate more impact if it can further grow the image resolution.

- According to Figure 5 (left), attention entropy is up by 1 bit as image resolution grows from 512 to 768. However, Figure 6 (right) seems to suggest that applying the scaling factor only reduces entropy by a very small amount. My early impression is that the scaling factor aims to restore entropy to the level of 512x512 images. I am curious about what causes this discrepancy.

Limitations:
The paper discussed the limitations and societal impact of the method.

Rating:
5

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a new scaling factor for attention based text-to-image generative models in order to handle variable sized generations. The authors establish the relationship between attention entropy and token size and use this newly found relationship to design a scaling factor that takes into account the image resolution. The authors also empirically show the effectiveness of this scaling factor by comparing FIDs, CLIP scores and qualitative examples.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed scaling factor is very simple and easy to implement. It does not require model architecture modifications or any training of the pretrained model. The qualitative results also look very promising. Therefore I think this can be easily applied in many existing models. Overall the paper is well organized and well written.

Weaknesses:
1. The main formula (Equation 6 and 7) is not very well explained. There are a lot of conjectures and approximations without justifications.
2. The authors did not compare with synthesis in fixed resolution and then performing super-resolution/downsampling. Since the resolution experimented in this paper is not very far away from the pretrained resolution, it is highly likely that super-resolution/downsampling may work just as well.
3. The authors should also compare their method with other machine learning based methods such as MultiDiffusion (https://multidiffusion.github.io/)
4. The compensation for human annotator is not mentioned
5. Typo in Equation 1,3: j is used in both the numerator and the denominator of Aij

Limitations:
Related to Weakness (4), the compensation for human annotators is not specified in the paper.

Rating:
5

Confidence:
5

";1
u6Xv3FuF8N;"REVIEW 
Summary:
The paper studies the problem of differentially private prompting, i.e. the scenario where a prompt-augmented LLM is exposed to users, which should be able to interact with the model, but should not be capable of extracting the private prompt prepended to their queries. The authors first show, that a membership inference attack against example data used in a prompt is feasible and easy to carry out (given access to model logprobs). They then show how to adapt the DPSGD algorithm to enable differentially private soft prompt learning (PromptDPSGD) and devise a new method for differentially private prompt learning in the discrete setting (PromptPATE) based on teacher-student knowledge transfer. Their evaluation shows that both methods are effective in protecting prompt privacy without sacrificing too much model performance under reasonable \epsilon values. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The paper is well-written and easy to follow, even though it covers a lot of ground. It motives the problem well by first demonstrating an effective MIA.

* The paper considers both the soft and discrete prompt learning setting. I think this is a good and pragmatic choice, as the most popular LLMs are API-guarded and typically do not provide access to gradients or hidden states, which are required for soft prompt learning. 

* The paper explicitly considers efficiency and scalability of the proposed methods. This is important as state-of-the-art LLMs are very large and training them is expensive or even infeasible when accessible only via an API. Both the discrete and soft prompt learning methods are efficient and scalable, which is a major advantage over more data-hungry methods such as full model fine-tuning.


Weaknesses:
* The shown membership-inference-attack relies on model logits (log probabilities), which are not always available to users (or developers). This means the attack will not work with e.g. the latest OpenAI models like ChatGPT or GPT-4 [1]. Further, even with access to logits via API, model vendors often add noise to the logits to prevent distillation attacks.

* While stated later on, the paper initially does not clarify that PromptPATE is specifically designed for the relatively simple few-shot LLM classification setting. It is not clear how well the methods would work in other settings, e.g. when the model is used for free-form text generation. For PromptDPSGD this is less of an issue, as it seems to be more of a general method for prompt learning.

* I rated the contribution only as ""fair"", because after reading the paper it seems like PromptDPSGD is a rather simple adaptation of DPSGD and PromptPATE also is a relatively simple idea. It may be helpful if the authors could highlight the novelty of PromptDPSGD and PromptPATE compared with existing work.

* The paper does not cover the case where attackers want to extract instructional information from a prompt, e.g. when a prompt not only contains example data, but also a description of the task and the desired LLM output. From what I understand, PromptPATE would not work very well in this case, since even with teacher-student transfer, the resulting student prompt would still have to contain the same (likely human-readable) instructional information as the teacher prompt. I recognize this is as a different problem, but it may be worth mentioning in the paper, as common LLM use has shifted heavily from example-driven prompting to instruction-driven prompting, with the advent of instruction-tuned and RLHF models like ChatGPT.

[1] Chat API documentation, OpenAI, URL: https://platform.openai.com/docs/api-reference/chat/create

Limitations:
I think the paper should clarify early on, that it focuses on the restricted classification setting. This would help readers to better understand the specific problem the paper is trying to solve, where the more general prompt extraction problem should be considered as a separate problem.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper investigated the privacy leakage in prompted large language models (LLMs), and have proposed methods to protect the privacy of potentially sensitive data used for prompt engineering. The authors first demonstrated high membership inference leakage in existing prompted LLMs and then proposed PromptDPSGD and PromptPATE for privately learning soft prompts and discrete prompts with DP guarantees. PromptPATE specifically works with existing commercial API, making it ideal for deployment in the real world.  The authors have validated their approaches on LLMs with extensive experiment setup and have shown a reasonably good privacy-utility balance.   


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper is well-motivated as LLMs and prompt engineering are increasingly popular, the privacy concerns of sensitive data used in prompted LLMs are less studied. The privacy leakage analysis in this paper also validated the concern. 
* The paper is organized and easy to follow. The literature review is comprehensive and the methods are clearly described.
* The experiments are conducted on LLMs with real world deployment scale and on black-box commercial API, showing the applicability of this research on existing real world LLMs systems. The setup of the experiment is extensive, including different scenarios such as private and public data from different domains.


Weaknesses:
Compared to the parameter-efficient fine-tuning approach (LoRA), prompt learning with DP has inferior performance.  Although the authors argued that the storage is cheaper with prompt learning, one might still prefer parameter-efficient LLMs based on their needs on the utility (also storage is typically considered cheap compared to other resources).


Limitations:
This paper has raised a privacy concern about real world deployment of LLMs. The authors have acknowledged limitations in the appendix of the paper.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies privacy preservation in the context of prompt learning for large language models (LLMs). It highlights the vulnerability of prompting data to membership inference attacks (MIAs) and proposes differential privacy (DP)-based defense methods for both soft and hard prompt learning. For soft prompts, the DP-SGD algorithm is utilized, while the PATE algorithm is adapted for hard prompts. Experiments on several common datasets and LM architectures demonstrate that the proposed algorithms can achieve good utility with a relatively small privacy budget.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
S1. This paper presents a timely study on privacy preservation for prompt learning.

S2. The paper addresses both soft and hard prompt settings and proposes suitable differential privacy (DP) algorithms for each. The PromptPATE algorithm includes in-depth adaptations to the data-efficient characteristics of prompt learning.

S3. It presents MIA to motivate the DP-based defense algorithms.

S4. This paper provides systematic experiments on several common datasets and LM architectures to demonstrate the effectiveness of the proposed methods.

S5. Very well-written and easy to follow.

Weaknesses:
W1. PromptDPSGD is somewhat of a direct application of the original DPSGD to the soft prompt learning setting.



Limitations:
The authors have adequately discussed the limitations and potential negative societal impact of their work in the appendix.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper discusses the potential privacy risks associated with prompting data in large language models, which can be exposed through a membership inference attack. To address this issue, the authors propose two methods, PromptDPSGD and PromptPATE, for achieving private prompt learning. PromptDPSGD involves obtaining input gradients from the LLM and using FedAvg to update the soft prompt embedding while keeping the LLM frozen. On the other hand, PromptPATE creates a noisy ensemble of private discrete prompts and transfers knowledge by selecting a student prompt that can be publicly released. The experiments demonstrate that LLMs prompted with these private algorithms closely match the non-private baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The authors explore the potential privacy risks that may arise from prompting data in large language models, which can be exploited through a membership inference attack.
- The proposed approach for privately learning to prompt is novel and effective in preserving privacy while maintaining high accuracy.
- The experimental results demonstrate the effectiveness of the proposed approach in preserving privacy, which is important for real-world applications of large language models.


Weaknesses:
- The authors only verify the effectiveness of the proposed approach on NLU tasks. It is necessary to conduct experiments to verify the performance of the proposed methods on NLG tasks.
- I am not entirely convinced that prompting data poses significant privacy risks. In my view, the user input itself may present a more significant potential privacy risk.

Limitations:
n/a

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents differentially private techniques for optimizing continuous and discrete prompts for solving text classification tasks using Large Language Models (LLMs). The need for differentially private prompt learning is motivated by showing that examples used in few shot prompts for classification tasks are highly susceptible to simple membership inference attacks

- For optimizing continuous prompts, the paper uses DP-SGD in combination with existing parameter efficient prompt tuning techniques. When used in combination with either soft prompt or prefix tuning, the method is generically called **PromptDPSGD** in the paper. This method requires access to gradients for tuning and the ability to adapt the model at inference time with tuned parameters, neither of which is typically supported in consumer APIs.

- For optimizing discrete prompts, the paper adapts the Private Aggregation of Teacher Ensembles (PATE) approach to label public examples using an ensemble of LLMs prompted with private few shot examples in order to learn a few shot prompt. This method is called **PromptPATE** in the paper. This method can be implemented on top of existing LLM completion APIs, even when they do not expose token prediction probabilities.

The paper evaluates the utility of using **PromptDPSGD** to fine-tune RoBERTa-Base on the SST-2, QNLI, QQP, and MNLI tasks from the GLUE benchmark with $\varepsilon \in$ {3,8}. It compares it to parameter efficient LoRA-tuning and full fine-tuning using DP-SGD. The results show that the accuracy of **PromptDPSGD** tuned models is competitive despite having orders of magnitude fewer tunable parameters than full fine-tuning and LoRA-tuning.

The paper evaluates **PromptPATE** using GPT-3 (Babbage and Curie) on SST-2, TREC, AG News, DBPedia. The results show that for $\varepsilon < 0.3$, it achieves similar accuracy to few-shot (1- and 4-shot) classification and to a non-private majority vote using the same teacher ensemble.

The supplemental material contains an Appendix describing hyperparameter choices, additional experimental results, and a discussion of broader implications and limitations. It also includes implementations of **PromptDPSGD** and **PromptPATE** built on existing libraries.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Demonstrates for the first time that examples used in LLM prompts for text classification are susceptible to membership inference attacks.

- Evaluates the use of DP-SGD together with 3 different parameter efficient fine-tuning techniques for text classification tasks and compares it to full fine-tuning.

- Proposes a novel adaptation of PATE to optimize discrete prompts for solving text classification tasks using LLMs that is implementable using available commercial LLM APIs using a much lower privacy budget than fine-tuning techniques.

Weaknesses:
- Oversells the novelty of **PromptDPSGD**, which is just standard DP-SGD applied to existing parameter efficient fine-tuning techniques.

- The susceptibility of few shot learning to membership inference attacks has been studied before at least for image classification (https://openreview.net/forum?id=39kvovk9ju7). This diminishes the novelty of the observation that the same phenomenon holds for text classification.

- **PromptPATE** is applicable to classification tasks, but it does not appear generalizable to generative tasks where LLMs excel.

- **PromptPATE** performs worse than a non-private few-shot baseline in all scenarios evaluated. Table 2 shows that even a 4-shot **PromptPATE** has worse accuracy than the 1-shot non-private baseline. This casts doubt on the necessity of learning differentially private discrete prompts. It would be enough to declassify a single private prompt to get better utility with perfect privacy for the rest of the prompts.

- The choice to evaluate **PromptDPSGD** and **PromptPATE** on model classes with hugely different capabilities (RoBERTa, and GPT-3 and Claude, respectively), and on mostly disjoint tasks (with the exception of SST-2), makes it hard to compare the two approaches. Despite the difference in model capabilities likely making **PromptPATE** look better than **PromptDPSGD**, the results still show a large gap to a non-private baseline: e.g., for AG News, Table 2 reports 71.7 $\pm$ 0.8% with $\varepsilon = 0.248$ vs. 81% using a 4-shot prompt (which I think is a more appropriate baseline than a 1-shot prompt). The utility of **PromptPATE** cannot be significantly improved because it saturates at a lower privacy budget (cf. Figure 3b) compared to parameter efficient fine-tuning.

**Comments**

- In line 120, ""To the best of our knowledge, no prior work attempted to provide DP guarantees for prompt data in LLMs."" This peer-reviewed paper published shortly before NeurIPS 2023 deadline explores the use of differentially private prompt tuning in federated learning: https://doi.org/10.1109/ICASSP49357.2023.10095356.

- In line 322, The reference should be to Table 2 instead of Table 5.

- In line 539 in the Appendix: ""trend the our private prompts"" should read ""trend that our private prompts"".

- In Algorithm 1 in the Appendix: $D$ is specified first as a set of labeled examples $(x_i,y_i)$ but later only the features $x_i$ are used. In line 4, $L_P$ should read $L_{P_t}$ and $p_t$ should read $P_t$. In line 9, $p_T$ should read $P_T$.

- In Algorithm 2 in the Appendix: $\mathbf{x}$ should read $x$ and the parameter $E$ is not described.

- In the caption of Figure 5, ""each prompt has only one member"" should read ""each prompt has only four members"".

Limitations:
Yes, both broader societal impact and limitations.

Appendix A discusses societal impacts. It highlights the risk of overreliance on DP guarantees and the need to correctly select the privacy hyperparameters $\varepsilon$ and $\delta$.

Appendix B discusses limitations. It highlights that privacy concerns are limited to private data in few-shot examples (as opposed to the training data of the LLM itself), the limited scale of experiments driven by cost constraints, and that private data used to construct prompts is exposed to the provider of the LLM API.


Rating:
5

Confidence:
5

";1
NfpYgGZC3B;"REVIEW 
Summary:
This paper proposes B4B as a defense against model stealing attacks for pretrained encoders. The main assumption of the defense is that attack queries generally have a much broader coverage of the embedding space than queries from normal users. Based on this assumption, B4B utilizes Local Sensitive Hash and separates the embedding space into a number of 2$^12$ buckets. B4B then calculates the fraction of buckets covered by the user queries, then adds Gaussian noise to the subsequent queries when the coverage is large, where this cost function exponentially increases. To defend against sybil attacks, different transformations are applied to the returned embedding vectors of different users, so that legitimate users can obtain high-quality models with the transformed embedding vectors, while attackers who aim to combine training data from different accounts cannot get performance boost.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Designing an active defense for model stealing attacks is a good topic. The proposed approach is interesting, and seems to be generally applicable to different models and tasks.

2. The results show that B4B does not degrade the response quality for legitimate users, while significantly decreases the model performance trained by attackers. 

Weaknesses:
1. In general, I wonder what is the valid condition to consider a user as an attacker. In the current evaluation setup, all legitimate users are querying with images drawn from different data distributions. However, even if the users directly query the model using ImageNet samples, if the user queries are only limited to a small subset of labels instead of the full label set, then I also would not consider it an attack. Have you evaluated this setting? Would the input coverage in this case still be high?

2. Another question is whether the attack queries should be images included in the pretrained data, or does the defense also work for queries drawn from a similar distribution? For example, if you use STL-10 unlabeled data for pretraining and STL-10 labeled data to query, or if you use CIFAR-100 data for pretraining and CIFAR-10 to attack, does the defense work?

3. The full defense includes a couple of hyperparameters that seem critical, i.e., the number of buckets, the cost function, and the transformations. Is the same set of hyperparameters used for all tasks and models in the experiments? How much work is required to tune the implementation for new tasks and model architectures?

4. I do not fully understand the process of per-user representation transformation. How is it done for a new user? Does the defense randomly select/construct a transformation from all possible choices?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies active defenses against model stealing attacks, with a specific focus on SSL encoders. The authors propose B4B, a framework including three modules: 1) a coverage estimation to distinguish legitimate users and adversaries; 2) a cost function that maps the coverage to a penalty to prevent stealing; 3) a per-user transformation function to prevent sybil attacks. The results show that B4B can prevent stealing and also preserve the utility of representations for legitimate users.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Trendy topic
- Extensive ablation studies

Weaknesses:
- Absence of comparison with previous work.
- Impractical assumption
- Design intuition lacks persuasiveness.
- Lack of experiments

Limitations:
This paper focuses on a trendy topic: defenses against model stealing attacks. To tackle this problem, the authors propose an active defense framework, namely B4B. They evaluate its performance on two SSL vision encoders, i.e., SimSiam and DINO, and consider four downstream datasets, i.e., CIFAR10, STL10, SVHN, and FashionMNIST. 

I appreciate the authors' efforts in conducting extensive ablation studies to understand the impact of hyperparameters. These experiments provide valuable insights and guidelines for readers interested in implementing B4B in real-world applications.

However, there are still some concerns:

- Absence of comparison with previous work. The authors claim B4B is the first active defense that prevents stealing without degrading representation quality for legitimate API users. However, it is important to note that [13] has demonstrated active defenses, such as adding random noise, can already achieve this goal. Additionally, other research works have proposed multiple active defenses against encoder stealing attacks, such as [26] (top-k, feature rounding, feature poisoning) and [32] (noise, top-k, rounding). I would suggest the authors reassess their claims and compare B4B with existing defense methods to elaborate on the advantages and unique contributions of B4B.
- Impractical assumption. The authors declare that ""B4B is independent of the protected encoder’s architecture and does not rely on any assumption about the adversary’s data and query strategy"" (line 148-149). However, the design of B4B's cost function suggests that it primarily works for users who query data from a small distribution, while users with queries from a larger distribution are considered adversaries. This assumption is strong and impractical for API providers. I recommend that the authors discuss this limitation of B4B. Furthermore, it would be valuable to analyze the impact of queried distribution and the corresponding utility to demonstrate the trade-offs of B4B in real-world scenarios.
- Design intuition lacks persuasiveness. The design of B4B is based on an observation: the representations returned to adversaries cover a significantly larger fraction of the embedding space than representations of legitimate users. However, a previous study [32] has shown that a small surrogate dataset is sufficient to steal the encoder. This raises concerns about the effectiveness of B4B, as attackers are able to employ small datasets to bypass it. Furthermore, in Figures 2 and Figure 9 in the Appendix, the authors visualize downstream datasets in the encoder’s embedding space. It is confusing to observe that STL10 and CIFAR10, two datasets that shared 90% classes, appear to be separated in both figures. Ideally, these datasets should be located in a similar space. This further weakens the design intuition’s persuasiveness. I recommend that the authors review their statements and provide explanations for the two points.
- Lack of experiments. As a defense method, it is important to show its effectiveness against different encoder stealing attacks [13, 26, 32]. However, the authors only consider [14] in this paper, which limits the utility of their work. I recommend the authors expand their experimental evaluation to include different types of stealing attacks and report the performance of B4B. This will provide a more comprehensive assessment of B4B. Furthermore, the authors propose five transformations, namely Affine, Pad, Add, Shuffle, and Binary. However, there is a lack of experimental analysis regarding the impact of these transformations on B4B’s effectiveness. Additionally, in Table 1, the authors should explicitly mention the specific transformations applied to the legitimate users, attackers, and sybil attacks. This will enhance the clarity and reproducibility of their experiments.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors proposed B4B, which aims at actively preventing model stealing while preserving high-utility representations for
133 legitimate users. The paper is overall well-organized and easy to follow. The proposed three blocks are clear and effective, namely the embedding space estimation, cost increasing, and per-user representation transformations. This topic also makes sense in practical to protect the commercial APIs. Experiments are well designed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is easy to follow.
- The motivation is clear enough, and the study is of value for protecting commercial APIs especially for those foundation models.
- The proposed three blocks are simple yet effective.

Weaknesses:
- Is there any defense baselines in Table 1? 
- Have you tried an ablation study on ""SYBIL"" in Table1? For example, maybe the user has 5+ sybil accounts with 20k queries per account. I think it could be interesting to explore if there is a convergency.
- I would like to suggest more experiments on existing public APIs. As emphasized in paper, it should be a valuable contribution to a safer sharing and democratization of high-utility encoders over public APIs. It could be better to conduct experiments to directly support this claim.

Limitations:
refer to weakness

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper propose a defense over model stealing on-the-fly by assessing dataset distributions, and directly train on it. The paper is well-written and easy to read, while proposing a novel type of active defense over model stealing, without harming the performance of benign users.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The authors propose an active defense over model stealing, which is based on the assumption that malicious queries will cover larger fraction of representation space, while benign queries are limited to a smaller fraction of space. The authors assign adversaries with noisy representations and benign users with clean representations. They also provide an additional transformation to avoid adversaries aggregate the stolen representation via creating multiple accounts.


Weaknesses:
I have particular concerns on the assumptions and experimental setup of this paper.

* The paper is based on the assumption that individual queries in a much narrower distribution, while adversary queries in a broader one. However, I wonder if this is the case in practice. For large tech firms, they might have requests from diverse users and query a specific API for many times, which also results in diverse usage. For a model trained for specific downstream task (eg, to identify cancer), there can be only one domain and the queries and adversaries can be similar (both include images containing/not containing cancer). Can B4B defend over this? Otherwise, authors should quality the potential usage of their work. Indeed, this assumption is difficult to prove to be true or to be false, so at least discussions should be included.

* I question the experiment setting of Table. 1, which follows the assumption that adversary query is inherently different from benign ones. However, this also means adding a simple classifier will also have high defend accuracy, since the distribution can be drastically different for ImageNet and CIFAR 10. While training on image classifiers is nowhere near to real world applications, I believe authors should test on a wider range or threats possible.

* In Figure 3, it turns out ImageNet naturally occurs for more precentage of buckets (covers larger proportion of representation space), while other dataset covers smaller fraction. Thus, I am again skeptical of the result given in Table. 1. It turns out that, ImageNet naturally occurs for larger fraction of representation space, thus naturally occupies more buckets. In this way, what if the legit query is ImageNet, while attack query is CIFAR-10? Will the proposed defense crash, since the legit query occupies larger fraction of representation space?


Limitations:
The authors do not discuss potential negative social applications. However, I do not find any potentially negative impact in this paper.


Rating:
5

Confidence:
3

";1
YOZaej0ZC7;"REVIEW 
Summary:
The paper considers the problem of measuring fairness in generative models. In particular, the paper has two main contributions: (1) they have produced a dataset of hand labeled (sensitive attributes, SA) dataset for various SOTA generative models; and (2) they have proposed a method for estimating the expected sensitive attribute distribution which utilizes the error rates of the SA classifier. These two contributions are used to show that SA classifiers with low error can still cause high errors in previous methods of approximating the sensitive attribute distribution.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper presents a strong empirical study of measuring fairness in generative models, and outlines flaws in prior studies.
- CLEAM seems to be an intuitive correction to the naive baseline method.

Weaknesses:
1. One weakness of the paper is a hole in the narrative: how is $C_{u}$ obtained? In particular, it is unclear in the main-text on how one would generate a SA classifier. Furthermore, the assumption of knowing the underlying error rates should be discussed as a potential limitation. I assume that the SA classifier is trained on data which are not samples from the generative model one is trying to measure the fairness of (as otherwise we already have labels). In this case, the validity of error rates transferring might be questionable.
2. The notation in the paper is somewhat strange. In particular, as far as I can see, $ Pr(u \mid x )$ is not a probability … despite the notation. And it is further “aliased” as $ C_u(x)$ which is additionally confusing. I don’t see why one could not just define the “argmax classification” as $C_u(x)$ directly.
3. The notation of $\hat{p}$ and $p^{\*}$ seems consistent to me. In Section 2, it seems that $p^{\*}$ is the population statistic. However, in Section 3 $p^{\*}$ becomes the estimate generated from GenData. Yes, one could argue that GenData somewhat becomes the “new” population, but this change in perspective is not clear. I think it makes more sense to think of the $p^\*$ in this Section as a high quality estimate of the true statistic. It may be worth changing notation to reflect the 3 possible $p$’s: that generated by the SA-classifier, that generated by GenData, and the unknown true statistic which is being approximated by the former two.

Typos:
- Appendix Eq (3) LHS


Limitations:
I think the assumption of knowing the underlying error rate / accuracy needs to further discussed (see Questions)

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper considers the fairness measurement for generative models. The contributions of this paper are three-fold. First, the authors reveal that the existing frameworks have significant measurement errors, even using sensitive attribute classifiers. Second, the authors propose a new framework namely CLEAM that uses a statistical model to evaluate inaccuracies of SA classifiers, thus reducing the measurement errors. Finally, the authors use the proposed CLEAM to measure fairness in important text-to-image generators and GANs, which show the effectiveness of the proposed framework. Experimental results with a manually labeled dataset show that the proposed CLEAM can achieve lower error as compared with some baseline schemes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1)Propose a novel framework to measure the fairness of generative models from both theoretical and experimental perspectives. 
2)The fundamental statistic model is easy to follow, and the proposed method CLEAM is able to reduce the fairness measurement error.
3)The dataset created in this paper will benefit the research community.

Weaknesses:
After reading this manuscript, this reviewer finds that there are some issues that need to be addressed, as
1)On page 6, the authors say that “the probability of the counts for each output cT in Eqn. 2 (denoted by Nc) can be modeled by a multinomial distribution.” Does this assumption hold in practical systems? 
2)The presentation of the statistical model should be improved. Why do the authors assume a multivariate Gaussian distribution instead of other statistical distributions? 
3)On page 6, what does ""M"" represent in equation (3)?
4)The description of equation (8) is not clear enough. The authors are suggested to explain it in detail.
5)The authors are suggested to provide more experiments with other datasets and generative models, in order to demonstrate the effectiveness of the proposed framework.

Limitations:
Please refer to my comments for details.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors conduct a study on the fairness of generative models.  They propose a CLassifier Error-Aware Measurement (CLEAM) framework which accounts for inaccuracies in classifiers involving sensitive attributes.  The authors also create a new dataset of generated images from a text-to-image generator which are then used to evaluate the accuracy of existing fairness frameworks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Better measurement and quantification of bias is a very critical topic both in classification as well as generation tasks.  

The proposed CLEAM framework appears to produce results which are more balanced than the baseline methods.  

Comparisons are made to other approaches.

Weaknesses:
The text/language/grammar could be improved in many places, especially the abstract.  Even the opening of the intro is confusing- ""fairness is defined as equal generative quality and equal representation  w.r.t  some sensitive attributes (SA). In this work, we focus on the more widely utilized definition – equal representation.""  Isn't this the same, or actually less demanding definition?  Does generating samples from both classes, but one at a much lower quality level, actually constitute fairness?  Doesn't there need to be some demand on the quality of generation?

The jumping around between the language of SA classifiers and generative models is confusing throughout.

The description/introduction of CLEAM (lines 59-68) is wordy and a bit confusing.  Other than the fact that it is ""a statistical model"", it's unclear what it is after reading this section. Since this is the focus of the paper, it should be crystal clear what the core of this method is after reading this section.  

The authors state ""we observe an intriguing property in Stable Diffusion Model""- however what they are observing is instability due to noise.  Again, I believe this is more related to instability/adversarial attacks than ""bias"" as it is traditionally defined.  I think disentangling these (related) concepts is important.

The paper is very dense with many results as well as mathematics.  It's clear the authors have much to tell with limited space.  However, they need to walk the reader through it a bit more as it is hard to read through the tables and understand what are the key take-aways from each.

Limitations:
This work is explicitly designed to overcome limitations of other frameworks which may have generative bias

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies measuring fairness in generative models, which is defined as equal number of samples generated from different groups. The measurement needs a sensitive attribute (SA) classifier to predict group attribute to compute the fairness. The paper emprically finds out the error in SA classifier would largely impact the measurement performance of the fairness. They test it by manually labeling samples and comparing fairness measure. The paper then proposes a calibration trick to reduce the fairness measurement error from the SA classifer's error,

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Fairness in generative models is an important problem and open problem

Weaknesses:
1. The paper has no insights on why the error in SA classifier would propagate to fairness measurement. The finding is entirely empirical, and seems independent of what kind of fairness definitions used. Would the SA classifier error impact all fairness definitions equally? How much would it impact? What determines the fairness error? The paper is mostly emprical in this finding without providing good insights.
2. The mitigation method seems to be simply computing the metric on different data subsets and then taking average and computing confidence interval. The technical contribution is a little too elementary.
3. The paper largely ignore the vast literature of noisy label, which I think is directly relevant to the problem, i.e. studying the label noise (i.e. imperfect SA prediction in this case) on the impact of fairness and models. For example:

[1] Natarajan, Nagarajan, et al. ""Learning with noisy labels."" Advances in neural information processing systems 26 (2013).
[2] Lukasik, Michal, et al. ""Does label smoothing mitigate label noise?."" International Conference on Machine Learning. PMLR, 2020.

There also seems to be some connection between label smoothing and the mitigation. Can you point out any if it exists?

Limitations:
See Weakness.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes a framework for fairness measurement. It first shows that existing framework has considerable measurement errors even when highly accurate sensitive attribute classifiers are used, then propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. Shows the significant measurement errors of existing frameworks by experiments.
2. Propose new datasets based on generated samples with manual labeling w.r.t. SA.
3. Proposes a simple statistical approximation method is proposed to obtain a stable and accurate estimation of the GT probabilities.

Weaknesses:
1. The organization of the paper is kind of hard to follow. The first contribution takes too long and may confuse the readers.
2. The introduction to the proposed method is too short and not very solid. Some theoretical support may be better. Maybe you can talk about cases when using distributions other than Gaussian to approximate the distributions.
3. Only a public dataset CelebA-HQ is used. It's better to test methods on various datasets.

Limitations:
The structure of paper is hard to follow and somehow boring for readers.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The objective of this paper is on measuring the fairness in generative models. There are three contributions. (i) Consideration of measurement errors of sensitive attribute (SA) classifiers in fairness measurement of generative models. (ii) A classification error aware measurement framework, called CLEAM, which based on statistical model accounts for the inaccuracies of SA classifier to reduce measurement error in generative models. (iii) As an application, the authors demonstrate that CLEAM can be applied to measure fairness in text-to-image generator and GANs. 



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well written, with clear intuitions, illustrations, and experimental results. 

Weaknesses:
- Putting the application of gender bias in the introduction seems to be out of place, possibly undermining the main framework CLEAM. Similarly, table 1 is also out of place, throwing numbers to the readers without explaining the setup. 

- The authors make an assumption of ground-truth labels of sensitive attributes, which may not be available in practice.

- Some of the human faces in Figure 2 and figure 3(a) are different, revealing the uncertainty of the generative models. I understand the intuition of the authors to provide a demonstration of the paper at an early part of the paper. Please clarify if I misunderstood anything.



Limitations:
NA.

Rating:
6

Confidence:
1

";1
Xxllzjt6T5;"REVIEW 
Summary:
The paper concerns synchronization of observed rotations with incomplete and corrupted observations. The authors construct the method ReSync that is a gradient-based algorithm for solving the problem. The paper describes the context, prior results on the synchronization problem, and the algorithm, and presents a thorough convergence analysis together with experimental evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- well-written and clearly presented paper
- the presented method deals with an important problem
- the presentation of the algorithm is followed by a thorough convergence analysis
- the method performs well in the experimental validation

Weaknesses:
- constructing a gradient descent algorithm is not a big contribution in itself. However, I believe the geometric setting and the connection to the theoretical analysis, which is not trivial, makes the contribution important

Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents a theoretical study for robust rotation synchronization with a least-unsquared minimization formulation over the rotation group.

In particular, this paper proposes a two-step algorithm called ReSync, where the first step uses spectral initialization to generate an initial guess and the second step performs Riemannian Subgradient descent from the initial guess.

The paper proves that, under suitable conditions of the random corruption model, this algorithm converges linearly to the groundtruth rotations.

The paper presents numerical experiments that verify the correctness of the theorem and compares the performance of ReSync with other state-of-the-art algorithms.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The theoretical contribution of this paper advances the previous state of the art.
- Paper is well written and easy to follow, despite being a theory paper.

Weaknesses:
- I am curious if similar guarantees could be made in the case where the inlier measurements are corrupted by small (and bounded) noise? Could you guarantee the algorithm converges to a solution that has bounded error from the groundtruth rotations?

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
This work proposes to solve the rotation synchronization problem using Riemannian subgradient method with spectral initialization. The proposed formuation is sum of absolute deviations, which is robust to outliers. Exact recovery guarantees are provided under uniform corruption model (the graph is Erdos Renyi and probability of corruption 1-p). Numerical results show competitive performance of the proposed method compared to other state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The theoretical result in the noiseless case (corruption only) is quite strong. That is, it shows linearly convergence to the ground truth rotations whenever n>1/(p^7q^2) up to a log factor, where p is the probability of being a clean edge (conditioned on being an edge), and q is the probability of being an edge.
2. The numerical experiments show advantages over previous state-of-the-art methods in the presense of both corruption and noise.
3. The proofs look correct.

Overall, I enjoyed reading the paper.

Weaknesses:
1. This is not necessarily a weakness, but it would be even nicer if the authors could comment on the stability of your algorithm to noise (would it be possible to show an approximate recovery in this case)?



Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a Riemannian subgradient based algorithm for the robust rotation synchronization (RRS) problem. RRS involves recovering the absolute rotations of objects from the possibly corrupted/noisy relative rotations between pairs of objects. The problem setting involves two ratios: q \in [0,1] denotes the observation ratio and p \in [0,1] denotes the true observation ratio. The paper pose the problem as a (non-convex and non-smooth) least-unsquared minimization formulation over the rotation group. The proposed method ReSync has a spectral relaxation based initialization procedure, which is followed by Riemannian subgradient iterations. The main contribution of the paper is to show that under random corruption model (RCM) setting: (a) the proposed initialization (X^0) can be relatively close to true solution (X^*), depending on p and q, and (b) given the initialization guarantee, the Riemannian subgradient descent show local linear rate of convergence. Overall, the paper show that ReSync converges linearly to the ground-truth rotations when p^7 q^2 = \Omega(log n / n). Towards the end of the draft, the paper has few experimental results that compare the proposed algorithm against state-of-the-art.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The paper presents an interesting approach for recovering ground truth (X^*) in RRS problem. The key theoretical guarantees for ReSync comes from a) an initialization procedure SpectrIn, which ensures that the initialization X^0 is close to X^*, b) weak sharpness property of the least unsquared formulation, which is being solved via ReSync, and c) local linear convergence analysis for ReSync based on initialization and weak sharpness property. 

I have, however, not verified the correctness of the theoretical results.

Weaknesses:
Concerns regarding theory:
1. The paper assumes missing observations as zero matrix, which does not lie on the SO(d) manifold. Hence, only if (i,j) belongs to available observation, Y_{ij} \in SO(d). The paper does not provide any justification for this choice. A more suitable choice seems to be Identity matrix as it lies on SO(d). 
2. In line 178-180, it is stated that E[Y_ij] = pq X_i^*(X_j^*)^\top for all (i,j). This does not seem correct as it is not clear how outlier points O_ij \in SO(d) are handled while computing this expectation. 
3. While the paper cites and discusses its differences with [27] in lines 171-175, it seems that [27] should be discussed in more detail. While  [27] focuses on orthogonal group with additive Gaussian noise and permutation group with outliers, it should be noted that permutation group is a special subset of orthogonal group. Interestingly, [27] states that ""though it is not analyzed in our manuscript, the proof technique for the permutation group synchronization under uniform corruption could be directly modified to tackle this O(d) synchronization under uniform multiplicative corruption"" (in the paragraph before Section 3.2). The proof of leave-one-out technique seems to be adopted from [27]. Hence, while [27] has been cited in Section 3.1 of the paper, it does not seem to be the main contribution and could been discussed in the supplementary material. Overall, [27] deserves more discussion, especially w.r.t. the above quoted statement, and in this regard contribution of the paper should be clearly highlighted. 
4. A discussion on computational cost of the proposed algorithm is missing. 

Concerns regarding experiments: 
1. The paper shows only a few empirical results on synthetic datasets. While this gives some insights on how the algorithm works in lab environment, performance on real-world setting gives an idea on how the algorithm will perform when used in real applications. If space was a factor, the paper could have moved some of the proofs/proof-outlines to the supplementary section. 
2. While the paper discusses [40] and states that it ""introduces a least-unsquared formulation and applies the SDR method to tackle it"", the paper should have mentioned more directly that the main formulation (2), which papers tries to solve, was originally proposed in [40]. Hence, while the theoretical results of [40] are in q=1 setting, the paper should have empirically compared with [40] as well. Similarly, paper [30] should also be compared empirically. 
3. Experiments are done in two settings: with and without additive noise. Without additive noise setting has been theoretically analyzed in the paper. One question is that in this setting, DESC method seems to be better or similar to ReSync. Any insights as to why it does better (where it does)?

Limitations:
not applicable

Rating:
6

Confidence:
4

";1
MtekhXRP4h;"REVIEW 
Summary:
The authors propose a UNet-like architecture for learning solution operators of PDEs. The architecture is assembled from building blocks analogous to the classical UNet architecture, but individual components respect a continuous-discrete equivalence. This makes the proposed architecture a representation equivalent neural operator in the sense of a recent paper (Bartolucci et al., 2023). The authors show that under some continuity assumptions the proposed method can learn the solution operators to a variety of PDEs. They also compare the in- and out-of-distribution performance of their method to a variety of baselines for a suite of different types of PDEs.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- There is a large body of work on methods and architectures for operator learning which this paper builds up on. Having architectures that respect the continuous-discrete equivalence seems very significant to me.
- The presentation is clear and the paper overall well written.
- The proposed CNO architecture improves the overall performance for a comprehensive suite of problems and appropriate baselines.
- Many evaluations and ablation studies are included in the appendix.


Weaknesses:
- As discussed in the related work section, the building blocks that make up the CNO architecture have been introduced in previous papers (Karras et al. 2021, Alias-free generative adversial networks). This slightly weakens the novelty of the paper.
- Many definitions are adopted from a relatively recent paper (Bartolucci et al. 2023), which to my knowledge is only available as a preprint at the moment.

- The modifications to the UNet architecture introduce additional up-sampling and down-sampling layers, which increase the training time significantly compared to other methods. This needs to be kept in mind. Also, the implementation of CNOs is much more complicated than of e.g. FNOs due to the windowed-sinc filters.

- I found the evaluation across resolutions somewhat unclear and partially misleading. The appendix goes into more detail here, but the results in the main paper show a single NS case that resulted from a downsampled solution (hence no high-frequency details exist in the targets). In this case the CNO seems to show a constant error, which is dubious for meaningful real world cases where higher resolutions naturally would exhibit structures that aren't resolved with lower resolutions. I think it will be important for future versions to clarify this, and replace figure 2 with one of the other two cases from the appendix.


Limitations:
Limitations are only discussed as a future work discussion. I don't think this is sufficient. E.g., a clear discussion of the performance impact is completely missing as far as I can tell. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a new operator learning framework called Convolution Neural Operator which although works on discrete space, satisfies the property of Continuous-Discrete Equivalence (CDE) property. They define CNO over a UNet architecture and provide some universal approximation proofs. Further, they come up with a Benchmark of representative PDEs and compare all the existing methods and present superior performance of CNO over other methods.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
+ Novel idea of CNOs
+ Theory provided for soundness
+ Representative Benchmark Dataset and making it public in zenodo is also good.
+ Very neat and clean code. 

Weaknesses:
- Some notations are messed up or confusing to follow. e.g. r in line 82.
- While the performance of these models is compared... the computation requirements like memory or training time are not compared. I think if we are talking about benchmarking, it is appropriate to give the complete story including the computational requirements.
- Some ablation studies on whether using CNOs reduces the architecture/compute requirements compared to just using a CNN or FNO is missing.

Limitations:
Yes. The authors have accurately identified the limitations. There are no negative societal impact for their work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper under review proposes a CNN architecture as a neural operator for solving PDEs via neural networks.  The goal is to preserve the underlying continuous structure while being implemented with discrete convolutional architecture, based on U-Net.  The idea is to consider the space bandlimited functions in Sobolev spaces.  Defining a convolution layer involves a discrete convolution, upsampling/downsampling that requires sinc interpolation to avoid aliasing, and an activation layer that upsamples, activates and then downsamples to maintain the original band-limitation.  The authors prove in supplementary material that their approach can approximate the solution to a PDE with arbitrary accuracy with the CNN architecture based on their neural operator.  Experiments are done on solving several PDEs, with experiments favorable to the authors' method against state of the art for most PDEs considered.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The architecture based on CNNs potentially simplifies e.g., FNO operators where operations are done in the Fourier domain without the use of down-sampling - which can potentially be more expensive than the authors' proposed method.
- Rigorous mathematical treatment of the interplay between continuous and discrete operators.
- Experiments show higher accuracy compared to SOA in PDE solving with NN.

Weaknesses:
- No evaluation of efficiency; is there a speed advantage of the proposed method (e.g., compared to FNO)?
- Evaluation of parameter sizes is in supplementary and there isn't a clear trend; I would have thought this method would require fewer parameters than e.g., FNO; but in many cases this isn't so.
- I would think efficiency in the sense above is a key differentiator compared to SOA, but this isn't the case at least looking at supplementary.
- The authors make use of sinc interpolation, which is the ideal interpolator for sequences of infinite length.  In practice, you are operating on finite length data, for which the sinc interpolater is not the right one.  How applicable is the theory presented to the case of finite length data?
- Prop 2.1 talks about a representation equivalent operator and the definition is cited in a reference.  Please specify the definition in the paper.
- Theory is poorly presented in the paper; everything is in supplementary.  At least the key ideas should be in the main paper.
- In general many of the key parts that should be in the paper are in supplementary.
- I find the authors' proposed idea to set a standard for benchmarking misguided.   There is a laundry list of PDEs to approximate in the benchmark, however, I doubt any one architecture would be good for all PDEs.  Each PDE has particular properties that one would want to aim to preserve (e.g., conservation laws in some PDEs) that may not be relevant to others.  So the implication that one architecture should do well on all PDEs seems mis-guided.  I do not think this should be an accepted standard benchmarking.

Limitations:
Yes, discussed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper re-introduces convolution neural networks to the operator learning setting. It uses interpolation including up-sampling and downsampling layer to make sure the convolution layer is well-defined in the function space. The paper also discuss the space of band-limited functions and the trade off between continuous-discrete equivalence and representation power of the infinite dimension function space. The paper proves an approximation theorem for the CNO model and compared it across many partial differential equations. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The convolution neural operator (CNO) takes the advantages of the efficient of the conventional CNNs and UNets methods, meanwhile it also satisfies the resolution-invariant and representation-equivalent properties. The work shows an approximation theorem that the CNO models can approximate any continuous solution operator, and the numerical experiments show it has a comparative performance compared to other machine learning methods such as  UNet, DeepONet, and FNO.  

Weaknesses:
I have a few questions and concerns, mainly in the trade-off between the band-limited functions and Lp/Hp space.
1. The paper claim the CNO models can only learn band-limited functions, however, the target PDE system are intrinsically infinitely dimensional system (in Lp/Hp). It means, given a fixed number of parameters (size of the model), as the number of training sampling and the qualify(resolution) of training dataset increase, the truncation error will dominate and the bandlimited class of operator will underperform the band-unlimited models.
2. The band-limited operator is, by definition, linear-reconstruction of the chosen basis, meaning it can only learn the coefficient but unable to learn the basis. The linear-reconstruction are less efficient compared to non-linear reconstruction model, as discussed in [1].
3. The experiments (Table 1) are designed in a manner that CNO dominates all other methods. It shows the potential of CNO, but the results might be biased. It's certainly reasonable that the band-limited CNO outperform band-unlimited model on super-resolution tasks. However, for in-distribution problems, if the resolution is fixed and the dataset is sufficient, then Unet should be equivalent to CNO, right? And for these non-smooth problems, the band-unlimited could be more expressive than CNO. These cases might not be sufficiently considered in the experiments design. It is great to introduce these new experiments, but it would be better to include as least one of the previous standard benchmark, for example, the Burgers or Darcy equation dataset from [2].
4. While the perspective is new and interesting, it's hard to say the proposed model is very novel. The architecture is highly similar to the previous UNet model + StyleGAN3 de-aliasing trick.

[1] Lanthaler, Samuel, et al. ""Nonlinear reconstruction for operator learning of pdes with discontinuities."" arXiv preprint arXiv:2210.01074 (2022).

[2] Li, Zongyi, et al. ""Fourier neural operator for parametric partial differential equations."" arXiv preprint arXiv:2010.08895 (2020).

Limitations:
The author discussed some limitations, but it's also good to discuss the trade-off between using band-limited functions and Lp/Hp space. I think both side have their advantages and disadvantages.

Rating:
6

Confidence:
4

";1
65aDEXIhih;"REVIEW 
Summary:
The paper presents results on the nonexistence of efficient learning algorithms for $3$-depth ReLU networks with smoothed parameters and Gaussian input. It also proves that in general, there is no efficient learning algorithm for $2$-depth ReLU networks with smoothed parameters and smoothed inputs (the smoothness is applied to a specially constructed input, which is Bernoulli). The neural networks they analyze, unlike previous works, have a ReLU unit at the single output.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is theoretical in nature and so it is good that it is rigorous in its analysis and seeks to provide enough detail to understand its proofs. The paper presents its results in a logical order.

Weaknesses:
I will first introduce my main concerns, and then other important observations to address. (I must say that I haven’t followed Section 4 with enough detail, though I do have some observations based on what I read.)

1) I am trying to understand the relevance of the learning problem studied by the paper. When talking about learning, from what I can recall, I have basically seen two approaches in the literature: 

Case a) When training a neural network, e.g. using SGD, one of the things we are concerned about is the testing performance of the network after being trained for some number of iterations or number of epochs.

Case b) I have seen other works that are concerned on generalization issues in terms of how close is the empirical loss (with respect to samples of the input data) from the population loss (defined by the expected value with respect to the input data distribution), which such closeness depending on different parameters such as the number of samples being used, the width of the network, the depth of the network, etc. 

Both of these cases tackle the issue of how well a neural network learns and generalizes. However, the paper seems to tackle the problem of actually learning a given neural network (see Definitions 2.1 and 2.2), where the true parameter of the neural network is chosen adversarially (and sometimes smoothed afterwards). In the paper, essentially, the learning is done by some algorithm that outputs some hypothesis that seeks to approximate the adversarial neural network. However, how is this relevant in the context of training neural networks and their testing/generalization performance? The type of works in Case a) and  Case b), though theoretical, are trying to understand some practical issues in neural network learning that is relevant to the general ML community, but I don’t see such immediate connection in this paper. In which situations do we care about “learning a neural network” instead of “training a neural network so it learns”? This needs more motivation and clarification. 
Also, the hypothesis $h$ that is supposed to learn the neural network in Definitions 2.1 and 2.2 seems to belong to any arbitrary class of functions. Is this right? If so, why isn’t this intractable to compute? In my mind, it would make sense for $h$ to belong to the same class of neural networks as defined by $N_\theta$ --- maybe it is the case in Section 4, but I just don’t know; all I know is that in Section 2 and 3, this is not clear to me.
Curiously, thinking about Case a) above, the paragraph that starts at line 102 mentions stochastic gradient descent (SGD), which makes me think that previous works have addressed the question of learning in a more practically motivated way, since SGD is used in practice. Can the authors also comment on this? How does it relate to your paper?

2) The paper addresses computational complexity, and from reading the paper it seems that such computation is linked to the algorithm $\mathcal{L}$. The computation by $\mathcal{L}$ seems to be related to how many times the algorithm needs to access the oracle in order to compute the final hypothesis $h$. This number of times is, I believe, the sample complexity of the algorithm. So, are we talking about computational complexity or sample complexity in the paper? Is the paper concerned about sample complexity at all? This must be clear on the paper, probably since the beginning of it. As far as I can see, the concept of sample complexity was only referenced in line 251 of Section 4 without much more detail about its relevance. Please, address this, since sample complexity is very important in ML and learning in general.

3) The first part of Section 4 explains how the proof used by the authors is related to [15] and what is the additional challenge that the authors present, i.e., the handling of smoothing. However, besides the first paragraph of Section 4, there is no more indication of whether some of the constructions being used throughout the proof correspond to ones in [15] or not. It would be nice to know which parts of the proof were taken from previous papers that also study learning of neural networks, and which parts weren’t.

4) This comment applies to networks of $k$-depth where $k>2$. When considering the neural network in definition 2.1 and 2.2, as well as in the statement of Theorems 3.1, there is no mention on how the neurons are distributed in the feedforward network. Do all hidden layers have the same width? Are the number of neurons per width irrelevant in terms of learning? Since the paper is concerned with presenting negative results, I guess they consider specific constructions of the networks in order to prove their results. Is that correct? If so, could this be mentioned? In general, could there be some mention about the specific topology of the networks in terms of the widths per layer?

5) Naturally, the paper recognizes that it might be possible to obtain results of efficient learning for different assumptions and topologies, such as not including an activation function at the output. I believe the authors should investigate this case for $3$-depth networks because this will strengthen their paper’s contribution. For example, if efficient learning is demonstrated for $3$-depth networks without activation function at the output, this will be very interesting because it elucidates more the role of the extra activation in efficient learning! I know that theoretical works are hard to do because proofs can be very non-trivial; however, since there is at least one previous work showing the efficiency of learning $2$-depth networks [6], how difficult would it be to adapt their proof to the case of $3$-depth networks?

Other observations:

1) The paper mentions that the neural network architecture they focus on have a ReLU at the single output layer. Previous works seem to focus on linear output, i.e. regression. Is having a ReLU at the output layer closer to real world applications? What is the motivation for it?

2) Definition 2.1 and 2.2 says that $(x,y)$ is drawn iid; however, it seems that only $x$ must be drawn iid, since $y$ is a deterministic function of $x$. Please check.

3) I think a proof outline is needed in Section 4, probably right before subsection 4.1 to better understand how the rest of the proof is built. Though the subsection titles in section 4 indicate what is being done, how they are pieced together in order to better understand the overall proof is not clear to me.

Minor:

1) In the abstract, line 9 mentions the word “hard”. It would be better to instead use expressions in terms of efficiency of the computation, etc.

2) Line 35, it seems that it should say “bounded from below”.

3) In line 86 it says that considering standard Gaussian distributions is “perhaps the most natural choice of an input distribution”, why is that? In real world applications, practitioners don’t care about this type of input. It may be good to insert a better motivation for it.

Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the computational complexity of learning 3-layer neural networks under the standard Gaussian distribution. Specifically, under a standard cryptographic assumption on the existence of local pseudorandom generators, the authors show that there is no poly-time algorithm that can learn 3-layer networks under the smoothed analysis framework. As a corollary, they show learning 3-layer networks is hard even with assuming a lower bound on the smallest singular values of the weight matrices.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- It is an interesting question to understand the worst-case hardness of learning neural networks, and under what assumptions learning is possible. Prior work has shown 2-layer networks can be learned under the smoothed analysis framework. The current paper, however, shows that learning 3-layer networks in the smoothed setting is hard, which is quite an interesting result.
- At a high level the proof appears to be sound, however I admit that I do not work on complexity theory and am unable to verify the proofs fully.

Weaknesses:
The novelty of the paper seems limited, particularly in light of the related work [11]. [11] shows that learning three-layer networks is hard, under the Learning With Rounding (LWR) assumption. [11] considers three-layer networks with no activation in the last layer, while the current paper considers networks with an activation in the last layer, but this last-layer activation seems like a very minor difference. Can the authors please comment further on the novelty of the current paper in comparison to this prior work?

[11]  S. Chen, A. Gollakota, A. R. Klivans, and R. Meka. Hardness of noise-free learning for two-hidden-layer neural networks. arXiv preprint arXiv:2202.05258, 2022.

Limitations:
Limitations are adequately addressed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The submission studies the classical problem of constructing ReLU-activated neural networks, specifically from the learning point of view. Previous work has established the existence of an efficient learning learning algorithm for learning depth-2 ReLU networks under the Gaussian distribution assuming a non-degenerate weight matrix. The authors provide a lower bound that rules out an extension of this result to depth-3 ReLU networks even under the ""smoothed analysis"" framework, where one assumes the presence of random noise to avoid the use of ""degenerate"" instances in the hardness reduction. As a second result, the authors provide a lower bound that rules out the extension of the aforementioned algorithm on the Gaussian distribution to general smoothed distributions.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The construction of ReLU-activated neural networks is a central topic at NeurIPS, and other lower-bound results on the problem have also appeared at the proceedings. The article is well-written and the results are non-trivial. 

Weaknesses:
My main issue with the submission is that the newly obtained lower bounds seem to only shift the boundaries of intractability by a very small step compared to what was known previously. In particular, previous results of Daniely and Vardi [15] already ruled out (under the same complexity-theoretic assumptions) an efficient learning algorithm for the setting studied in this submission, with the distinction being that their lower bound does not operate in the ""smoothed analysis"" framework. The same work of Daniely and Vardi [15] also established a lower bound that matches the second result in this submission, but once again without the use of the ""smoothed analysis"" framework. In this sense, it seems to me that the main contribution of the submission is showing that previously established lower bounds do not require the use of purely degenerate cases. In combination with the fact that the lower-bound proof itself builds on the approach introduced by Daniely and Vardi [15], I cannot help but feel that the present submission is somewhat incremental in nature. 

Also, a clearer and more to-the-point comparison of the submission's results to those obtained in the previous work(s) of Daniely and Vardi would have been appreciated... but that is only a minor (and perhaps subjective) point.

Limitations:
The limitations have been adequately addressed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
  This paper addresses the complexity of learning neural networks, a
very fundamental problem in learning theory. Previously, some 
complexity and efficient solvability results were known for networks 
of depth 2.  The paper shows that when the depth is increased to
3, the learning problem becomes computationally intractable (under
a hypothesis on the existence of local pseudorandom generators) 
even when one uses the assumptions (e.g., Gaussian input distribution,
non-degeneracy of the weight matrix) that lead to efficient algorithms
for the depth-2 case.  The paper also shows that the learning problem
for depth-2 networks is hard under a smoothed analysis framework
(where both the input distribution and the network parameters are
perturbed).

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
(a) Understanding the boundary between hard and easy cases of learning
    neural networks is an important problem in learning theory. This is a 
    very nice contribution to that area.
  
   (b) The paper provides a nice summary of previous work which makes
   it easier to understand the context for the contributions.

   (c) The technical results are presented very well.

Weaknesses:
 This reviewer can't see any weaknesses.

Limitations:
None

Rating:
7

Confidence:
1

";1
OwpaO4w6K7;"REVIEW 
Summary:
The authors address the problem of reassembling fractured 3D objects from its pieces, where each piece is represented as a point cloud. The main challenge lies in the fact that one has to simultaneously solve a segmentation problem (which part of an object is fracture surface and which part is the original surface) and a multi-matching problem. The authors propose a learning-based approach that nicely take into account these different aspects. Their approach is supervised, which sounds reasonable given that it is relatively straightforward to simulate object fractures from complete objects.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
+ The authors address a difficult, relevant and relatively novel problem and set the new state of the art
+ The proposed solution is a neat combination of learning and traditional optimisation techniques
+ All pieces are matched simultaneously which really is key to address this difficult problem
+ The idea of using a primal-dual feature descriptor pair to account for the complementary geometric properties of features is novel and useful 

Weaknesses:
- Although the presented results look appealing, they only show a small subset of all results. Given the relatively large angular errors (up to 52.4 degrees), it is unclear whether results are cherry-picked or representative. Given the overall improvement over previous methods I do not see this as critical for acceptance, yet, it is important to understand this.
- There a few things that leave some room for clarification (see below).

Limitations:
Limitations are adequately addressed.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel framework for the task of 3D fracture assembly. The proposed approach leverages hierarchical features of global and local geometry to match and align the fracture surfaces. Experiments are conducted on the Breaking Bad dataset to verify the effectiveness of the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper is well-written and easy to follow;

Weaknesses:
1. I am not the guy from the same area and therefore I cannot judge the novelty of the proposed mehtod well. However, from my experiences in my research field, this task is quite close to pairwise and multi-frame point cloud registration, and the major difference is the utilization of surface segmentation and the significantly lower overlap ratio between parts. Based on that, some techniques, especially the geometric learning and the matching, are well-explored and widely-adopted in the aforementioned registration tasks, and therefore I consider the proposed solution trivial.

2. For the experiments, I think there are some problems:
  
   2.1 The experiments are conducted on a single dataset, and I am not sure whether it is a synthetic or real one. To this end, I think it is necessary to also evaluate the proposed method on a second dataset. If the leveraged dataset is synthetic, it would be better to include experiments on real data;
 
    2.2 In Tab. 1, the latest baselines are from 2020 which is 3 years ago. I have no idea why some latest ones are not included. Does it mean that there have been no work in this topic for 3 years? I would like to see the comparisons to some more recent works during rebuttal;

    2.3 In Tab.1, the only point cloud registration baseline is Predator which is a work in 2021. Some more recent works, for example, CoFiNet (NeurIPS 2021), GeoTransformer (CVPR 2022), Lepard (CVPR 2022), RegTr (CVPR 2022) should be considered as the baselines from the field of point cloud registration (GeoTransformer is the state-of-the-art).  

    2.4 As the task is similar to multi-frame point cloud registration, I also think it reasonable and necessary to include baselines from that area, like (1) and (2).

3. Fig. 1 carries overly much information and is hard to focus on each specific part. I would suggest the authors modifying it to make it clear. Also, in Related Work (line 90), [30] is not based on the overlap prediction.


---------------------------------------------------------------------------------------------------------

(1) Gojcic et al. Learning Multiview 3D Point Cloud Registration, CVPR 2020

(2) Yew et al. Learning Iterative Robust Transformation Synchronization, 3DV 2021

Limitations:
Limitations have been discussed in the main paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors present a learning framework for assembling physically broken 3D objects with point cloud observations. There are three components in the framework, surface segmentation to identify the fracture points, multi-parts matching to find correspondences among the fracture points, and the global alignment to recover the global poses of the pieces. The main contributions are the joint learning of segmentation and multi-parts matching and the design of the multi-parts matching including the primal-dual descriptor and the losses. The contributions are validated by superior performance on the Breaking Bad dataset compared with other methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The task is new with just few methods already developed. This work may serve as a baseline for future work in this line.

The overall framework is clearly clarified and organised. The adding of self-attention and cross-attention is well justified and illustrated for the front-end feature extractor. The primal-dual descriptor is introduced in the multi-part matching part, which is new for this task to my knowledge. A further analysis of the primal-dual descriptor is also provided in the appendix to provide better understanding of the effect. 

The experiments and results are also solid.

Weaknesses:
The absence of the statistics of the number of pieces in the dataset may lead to unfair comparison against the baseline methods as points are sampled at different level as shown in Table 3 in the appendix.

A further analysis of the multi-parts matching w.r.t. the number of pieces could better show the effect of multi-part matching against pairwise matching.

The notations are sometimes a bit confused, especially for the tilde and hat. For example, the hat_P_i at line 258 seems to be not defined.

The experimental settings are not clear for all the involved methods. A table to list and compare the conditions and data/label requirement for each method would be helpful. 

The introduction of the rigidity loss is not well justified and analyzed, especially it is only applied to the model training after 200 epochs.

Texts in the figures are too small to recognize.

Limitations:
The authors deals with the accuracy drop of pose estimation when the number of fractured pieces increases.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes Jigsaw, a novel framework to assemble 3D fragments of a shape to complete the whole 3D shape. 
Jigsaw first segments the fracture surface, since only the points on the fracture surface are involved in the physical assemble between fragments. Jigsaw then performs multi-part matching to find correspondences among the fracture surface points, which is finally used to recover the global pose of each fragment via global alignment. 
Jigsaw exhibits state-of-the-art performance on the Breaking Bad dataset.
The authors propose Jigsaw to be the first learning-based method to assemble 3D fractures over **multiple** pieces.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
* First learning based method to tackle the task of multi-part assembly of 3D fragments.

* Well-motivated, novel learning pipeline to facilitate multi-part assembly of 3D fragments. 

* Strong performances on the Breaking Bad dataset. Considering that a two-fragment assembly problem can also be seen as a zero-overlap (or surface-overlap) point cloud registration, the baseline methods in the table are also appropriate.



Weaknesses:
* Lacks self-containedness. While it is understandable that Shonan-averaging was used as the off-the-shelf global alignment method, but the paper does not go on to explain how this global alignment functions at the minimum, nor how the global pose values from Shonan-averaging is used for evaluation against the given ground-truth global pose values.

* Jigsaw samples a fixed number of points across all the fragments of a single shape. In that case, Jigsaw is expected to fail given a particularly small fragment, from which a very small number of fragments would be sampled. This may result in not a single point on the fracture surface being sampled, and this lacks emphasis in the manuscript.

* The manuscript lacks explanation on how the primal-dual descriptor works. The authors state that their 'system learns surface features that capture the characteristics of a local surface from both directions' - which lacks the evidence the substantiate this claim. Is it the **aim** of primal-dual descriptors to function as mentioned, or is there a theoretical evidence to prove that the primal-dual descriptors actually captures such characteristics?

* While the idea of surface segmentation to narrow down the points to consider is viable, it is expected that the fracture surface is rather easy to segment due to its irregularity in comparison to other parts of the shape. How would the proposed method perform if the fracture is not irregular, but of regular cuts (e.g., straight lines, smooth curves) such as in Neural Shape Mating? [1]

[1] YC Chen et al., Neural Shape Mating: Self-Supervised Object Assembly with Adversarial Shape Priors, CVPR 2022

Limitations:
The limitations of Jigsaw have been discussed by the authors. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work provides a Joint Learning of Segmentation and Alignment Framework(Jigsaw) framework for 3D fracture assembly task. Experiments are evaluted on Breaking Bad dataset and show better performance than other existing methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed approach applies a joint learning framework specific for multi-part fracture assembly, which appends an attention-based 
network module to capture local geometric information. To further capture viewpoint-dependent features for surface matching, the proposed method applies the primal-dual descriptor to achieve better matching.

2. The authors incorporate fracture point segmentation to capture intrinsic features and introduce a multi-part matching approach
for internal automatic piece positioning, with global pose alignment capability. 

3. The proposed method shows better results than baseline methods evaluated on the multi-part assembly Breaking Bad dataset. The ablation analysis also validates the efficacy of the proposed network structure.

Weaknesses:
1.  The motivation of using primal-dual descriptor seems not sufficient, it would be better to append a more description of it. 
2.  The ablation study part is considered a bit insufficient. e.g. The authors already mention matching loss and rigidity loss. The related ablation analysis about the impact of segmentation loss, matching loss and rigidity loss is missing.

Limitations:
In the experiments, evaluation on extra dataset rather than the Breaking Bad dataset is much desired.

Rating:
6

Confidence:
4

";1
zQTi3pziFp;"REVIEW 
Summary:
This paper deals with the problem of predicting sound fields around human bodies and proposes a method that exploits binaural audio signals and human body motions. The proposed method first encodes binaural audio signals and human poses and then decodes them into audio signals that are supposed to be captured by surrounding microphones. Finally, the proposed method renders the predicted signals into sound fields. The proposed method also introduces a loss function that is robust against small time shifts of time-domain audio signals. Experimental evaluations with originally collected datasets demonstrate that the proposed method captured the ground-truth sound fields reasonably well and it requires full body poses instead of only head poses for obtaining good predictions.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The problem dealt with in this paper is novel as far as I know. Although sound field prediction is one of the standard tasks in audio signal processing, sound field prediction with blindly moving microphones would be novel and challenging. This paper solves this problem by (1) assuming that sound is emitted from a human and the human wears microphones and (2) integrating pose estimation of the target human.

2. The proposed method is technically sound. Since the current problem setting is sensitive to time shifts, the proposed method handles audio signals in the time domain, which is not so popular in multi-modal and cross-modal analysis with deep learning. The proposed method achieves time-domain processing by incorporating WaveNet-like decoders.

3. The newly introduced loss function is also technically sound. Time-domain signals are more sensitive to time shifts than frequency-domain ones. The proposed loss function named the shift-\ell_2 loss effectively mitigates this problem while maintaining its discriminability.

4. The experimental evaluations demonstrate that
(1) the proposed method reasonably worked well,
(2) full-body poses contributed to the performance improvement of sound source localization compared with head poses, and
(3) the proposed loss function was effective for non-speech sounds.

Weaknesses:
1. The current experimental evaluations do not contain comparisons with other existing methods. I understand that the current problem setting is novel and thus there are no existing methods that can be directly applied to this problem. However, several previous methods for sound field prediction have already been known, and experimental comparisons with one of those methods will be informative for readers.

2. The problem setting seems to be too extreme. Sound field prediction and reconstruction are one of significant problems in audio signal processing. However, the current problem setting requires (1) humans as sound sources, (2) microphones worn by humans, and (3) external video cameras. In particular, the first constraint seems to be too strict. Some justifications and potential applications will be required.

Limitations:
I could not find any negative societal impacts of this work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces a novel model for a novel task to render a 3D spatial sound from human body motion and audio collected by headset microphones. The authors also present a novel dataset containing human body motion and audio for this task. The model takes encoded audio, pose features, and target microphone position to render the corresponding spatial sound. A novel shifted-l2 loss is also proposed to address the issue of big spikes in l2 error due to the small shifts of impulsive signals. Experiments and results demonstrate the effectiveness of the proposed method.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The motivation and objectives are clear. To my best knowledge, it is indeed the first paper that aims to render 3D spatial sound from a human body pose.

2. The dataset itself is valuable. It could unlock various interesting research projects in the future.

3. The system is simple but effective. The proposed shifted-l2 loss is interesting and could be useful in other audio applications as well.

4. Analyses of the roles of body pose and head-mounted microphones are well-done in the ablation studies.

5. The phase is important and I am glad that the authors consider it in the optimization.

Weaknesses:
1. While extensive quantitative evaluations and analyses are done, it is often challenging to understand the gap in the perceptual level. It would be great if there is a subjective evaluation to demonstrate both the importance of the proposed task/approach and the contribution of the technical components.

2. It would also be nice to have more samples in the supplementary material for a subjective evaluation of the proposed approach.

3. I wonder whether any failure cases are appearing in the current exploration stage.

4. audio and visual data time synchronization is often challenging but the details to achieve that are missing.

5. As the authors mentioned in the limitation, the current approach could not handle spatial sound for human-object interaction, mainly for far-field modeling, and require intensive computation resources.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work introduces an approach for generating spatial audio from a 3D human pose and microphones placed close to the subject’s head, e.g. on a VR/AR headset, as is the case here. The authors first collect a multimodal dataset that contains 3D bodies and audio, recorded using multiple Kinects for body tracking and 345 microphones for audio acquisition. The dataset contains multiple participants in different outfits and poses, generating different sounds following a pre-defined script. To generate spatial audio, the authors propose a model that receives as input the audio recorded from the headset microphones and the 3D body pose and predicts the audio signal recorded at a microphone of the spherical capture array. Using the audio at each microphone and the locations of each microphone, the authors compute harmonic sound field coefficients that best represent the sound field. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- Important problem with large impact: The authors tackle an important problem for AR/VR applications that has not yet received a lot of attention. Generating accurate sound fields promises to increase immersion and the realism of virtual experiences. The proposed dataset will be an important benchmark for the community and open up new and exciting avenues of research.
- Novelty: The proposed dataset is an important contribution with unique data that are not publicly available. 
- A loss that is well-motivated by prior-work and adapted to the problem structure that leads to clear benefits.
- A simple model that merges different types of information, using well-proven components.
- Writing: The paper is well-written and easy to follow.
- Experiments: The experimental analysis is clear, highlights the effects of each design choice and provides insights into what makes the method work. 


Weaknesses:
- The related work section lacks a discussion of recent methods that model sound fields using neural network fields, such as:
  - [Learning Neural Acoustic Fields](https://www.andrew.cmu.edu/user/afluo/Neural_Acoustic_Fields/).
  - [INRAS: Implicit Neural Representation for Audio Scenes](https://openreview.net/forum?id=7KBzV5IL7W)

These could also be useful baselines for the model, both in terms of accuracy and in terms of computation compared to the proposed method. Furthermore, the following datasets could be a useful discussion point for the related work section:
  - SoundSpaces: Audio-Visual Navigation in 3D Environments: A discussion of whether a similar dataset as the one proposed in this work could be constructed by simulating audio propagation from artificial bodies.
  - Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis: I believe the related work section could benefit from a comparison with this paper in terms of granularity of pose representation and sound field, number of persons and type of conversations.


Limitations:
The authors present the limitations of the proposed dataset and method. They acknowledge that the current work is limited to modeling sound effects for a single person in an artificial scene and that it cannot model sound close to the body.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper tackles the task spatializing a mixture of speech and body sounds at different points in a sphere around a human body without explicitly recording or knowing the sound source location. Towards that goal, the paper captures a new dataset of humans speaking and making different body sounds in a very controlled and extensive setup. The paper proposes a model that takes as input the time-warped audio recorded by a VR headset and the body pose sequence, and renders the audio at different points on the sphere conditioned on the azimuth and elevation of the points. Furthermore, the paper proposes a novel and intuitloss function to accurately model both speech and short-lived body sounds. The paper compares its model with a heurisitcal baseline, evaluates different versions of the loss function, and also reports results for different microphone channel count and an ablation of the model. 


Post author-reviewer discussion: I have read the rebuttal. The responses are detailed and answer my questions. I have increased my score.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The proposed task is interesting.

2. The proposed loss is intuitive and seems to be well-designed.

3. The dataset could also be a valuable contribution for the community.

Weaknesses:
1. a) The dataset capture seems to be done in just 1 environment. There is no discussion or experiments on the model's generalization to other environments.
   b) The capture setup seems to be pretty complicated and could limit the applicability of the model to novel environments if training needs to be redone for new environments.

2. a) The capture scene seem to be simple in the sense that it doesn't contain other objects, is single-room in nature, etc. How would the model fare for more complex realworld scenes?

    b) Would the model be able to accurately render sounds at locations that don't have a path of direct sounds from the source? 

3. How is the source location in L150-1 determined?

Minor:
1. The shift-l2 loss doesn't seem to perform by itself in most cases (as also pointed out by the authors) even though the design is pretty intuitive.

Limitations:
The paper discusses its limitations, and I can't think of any serious negative societal implications of the work that need to be discussed in the paper.

Rating:
5

Confidence:
3

";1
OmTMaTbjac;"REVIEW 
Summary:
This paper proposes a self-supervised audiovisual representation learning framework. It combines MAE with contrastive learning in both intra and inter modality settings. The authors also suggest a self-training schema where the student model, which only sees masked input, needs to align with the latent representations of the unmasked input produced by the teacher. This shows to improve the quality of learned representations in just a few iterations without relying on external pre-trained teacher models.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The work is well-motivated and for the most part easy to follow.
2. Experiments and ablation studies are comprehensive with some reservations (ref to Questions section)
3. Components of the proposed approach (Fig 1) are not novel, however putting them together and studying how they interact with one another is valuable.
4. While there are other works which have combined MAE with contrastive learning, this paper has done a good job distinguishing itself from those concurrent/earlier efforts.
5. Self-training results are promising (Table 4.b) and the ablation for reconstruction target (Table 4.a) provides valuable insight.


Weaknesses:
1. The effectiveness of the fusion layer is not clear: Table 1 does not show any meaningful gain added by the fusion layer over unimodal MAE. It is worth noting that “None” has fewer parameters compared to other columns in Table 1 due to not having a fusion layer. Similarly in Table 4.a when comparing M-Fusion against M-Uni. 
2. Table 3: Intra is lower than Inter, and their combination provides marginal gain over the inter (0.6 on audio and 1.2 on video). It is worth remembering that the addition of intra doubles the computation cost since we need to process two views from each input-modality. Intra loss is one of the main points that this work is using to distinguish itself from CAV-MAE. 
3. Table 5.e summarizes the effectiveness of the contributions claimed by MAViL. We can see that the addition of joint AV-MAE adds nothing meaningful to the baseline. Similarly is the marginal gain due to the addition of intra loss over the inter one. The student teacher learning is the only novel contribution that shows to be effective. 
4. Minor note (not a weakness): arxiv paper https://arxiv.org/abs/2303.12001  also combines MAE and contrastive losses though the setup and modalities are different from the current paper.

Limitations:
Yes, they have.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The authors present Masked Audio-Video Learners (MAViL), pretrained on 3 pseudo tasks and consisting of a 2-stage approach. i) masked reconstruction (adoption of MAE, AudioMAE, VideoMAE) ii) intra-modal and inter-modal contrastive learning (similar contrastive loss like SIMCLR, MOCO), and iii)  student-teacher learning (performed in stage 2, by minimizing the error between earlier pretrained encoders referred to as teachers and the current encoders referred to as students). The proposed framework is pretrained and evaluated with AudioSet and VGGSound. Additionally, audio-to-video retrieval experiments are done on MSR-VTT and YouCook.




Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is a nice extension of masked reconstruction in a multimodal setup. 
- Fair amount of experiments are conducted as part of ablation to support the design choices.
- Good presentation and easy to follow.
- Thanks to the authors for making the code to be public and sharing the details of the implementations which are helpful for reproducibility.

Weaknesses:
- This work does not provide results on linear evaluation. The whole point of SSL is to learn rich abstract representations that can be used through minimal training with labelled data, i.e., linear SVM or FC-tuning.
- The comparison setup with prior works is insufficient. For example, the SOTA evaluation on visual tasks is extremely weak and most of the entries in Tab. 6 for the video are blank (-), while there are plenty of methods that can be compared. This work does not compare the results of ESC50 with several AV-SSL prior works.
- The proposed method lacks technical novelty as all the pieces (masked reconstruction, contrastive learning, student-teacher) of the proposed method are already been studied quite heavily in the literature, be it in a uni-modal or multimodal/cross-modal setups.

Limitations:
In summary, this work lacks novelty, a comprehensive evaluation on different downstream benchmarks, and a thorough comparison with several prior works. These weaknesses outweigh the reasons to accept.

The proposed method has three objectives and all of them are heavily explored in the literature, and I do not see solid technical novelty. First, the masked reconstruction technique has already been implemented in several published works such as MAE, MAE_ST, VideoMAE, AudioMAE, CAV-MAE, and many more. Secondly, contrastive learning in multimodal/cross-modal setups has been widely studied. Lastly, the use of a frozen pretrained network to supervise an online encoder is not novel, whether in uni-modal or multi-modal setups.




Rating:
4

Confidence:
5

REVIEW 
Summary:
The authors propose to learn audio-visual representations with two stage process with three self-supervision losses including 1. masked reconstruction 2. intra and inter-modal contrastive learning with masking 3. self-training via teacher-student setup to contrast on contextualized representations in 2nd stage. The authors show that proposed approach achieve state-of-the-art audio-visual classification performance on both AudioSet and VGGSound, audio-only tasks, and audio-to-video retrieval.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The authors provide thorough ablation studies for each loss function introduced, masked ratio, choices of dataset size, backbone init, model size. This helps to understand the proposed system with a more complete view.
- The proposed approach to combine two popular loss function masked reconstruction and contrastive learning, and further introduce the self-training on contextualized representation is original.
- This paper is well written with clear narrative on the motivation and it provides sequences of experiments which help the reader to understand their thoughts step by step.

Weaknesses:
- This work lacks evaluation on the visual modality, especially video, as this work claims to be audio-video learner. Both AudioSet and VGGSound are mainly used in audio research, it would be beneficial to provide performance on some video benchmarks. For example, MBT also include EpicKitchens-100.
- Minor suggestion: consider add AS-20K in the caption of table 1, as it is less clear which dataset/task is used. Relevant to this, the use of AS-20K for ablation study is mentioned in the last paragraph of section 4.3, consider move it to the first paragraph of section 4.4 to better highlight this choice.

Limitations:
- This work provides thorough study on proposed approach applied to audio related tasks including classification and crossmodal retrieval. It lacks some aspects on the video modality. It would be beneficial to include more evaluation with video downstream tasks.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents a new self-supervised learning framework for audio-visual modalities. It combines three types of SSL techniques, namely MAE, contrastive learning, and student-teacher distillation. The proposed method is a two-stage process by first training a teacher network to reconstruct the raw signal and then using this network to guide a student network to learn in the second stage. Authors show that this method improves the SOTA on AudioSet, VGG-Sounds as well as several other classification and retrieval benchmarks.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Although the three SSL losses in this paper are not new, this paper did a good job of putting them together and making them work. The performance improvement over the SOTA method is also pretty decent, validating the effectiveness of the approach.

The authors conducted a very sufficient ablation study of the model architecture, showing that multi-modal fusion, contrastive learning, reconstruction, and distillation all contribute to the model's performance.

The writing is very clear-written and easy to follow.

Weaknesses:
In the implementation details, authors mentioned many data augmentations are used in training. How much do they matter? I do not see an ablation on this component.

The limitation of this work is not discussed in this paper.

Limitations:
Limitations and societal impact are not discussed in this work.

Rating:
8

Confidence:
4

";1
sXD4idbnBw;"REVIEW 
Summary:
This paper propose a differentially-private local stochastic gradient descent both for centralized and distributed settings. The authors argue that the proposed method has less number of clipping and in turn produce less clipping bias compared to its counterpart DP-SGD which do not involve local steps. They also show that DP-LSGD converges sublinearly to a ball of the optimum, which is claimed to be faster than that of DP-SGD, and exhibit a better utility-privacy tradeoff.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The authors characterize the convergence performance by regarding clipping noises as biased noises and assuming that the incremental norm of local update be bounded, which is otherwise not easy to deal with.
- The authors prove that the proposed DP-LSGD converges faster than that DP-SGD and empirically show that it also has a better utility-privacy trade-off.
- The paper is technically sound and well organized.


Weaknesses:
- Assumption 4.1 and 4.2 seem restrictive to the reviewer. In particular, Assumption 4.1 seems not practical in the sense that one can not ensure the boundedess of the incremental norm of $\nabla w$ without knowing in advance the basic convergence of the algorithm (note that the algorithm may diverge, making $\nabla w$ unbounded); Assumption 4.2 is assumed to be hold for any value of w instead of the optimum w*, which is more common to be adopted.

- Theorem 4.1 and 4.2: the authors claim that DP-LSGD enjoys faster convergence to a neighborhood of the global optimum/ saddle point than DP-SGD; however, local sample-level differential privacy is guaranteed for DP-SGD, but not for DP-LSGD. For a fair comparison, each client for DP-LSGD should clip the calculated gradient and add the DP-noise at each SGD step in local update to satisfy the local sample-level differential privacy, which will inevitably degrade the convergence performance of the algorithm. In that case, what are the advantages of DP-LSGD compared to DP-SGD and does DP-LSGD still produce less clipping bias than DP-SGD?

- The selection of many parameters such as B, \eta and c lack of intuition; also, the experiment does not corroborate the theoretical result very well; for instance, the clipping bias captured by $\mathcal{B}$ is independent of $K$ (c.f., Theorem 4.1) and thus can not reduced with increasing value of $K$, which is inconsistent with the claim that DP-LSGD Produces Less Bias.


Limitations:
please refer to weaknesses and questions.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper studies DP-LSGD and compares its performance with DP-SGD. The paper first provide the convergence result of FedAvg under the bounded variance assumption (Theorem 3.1 and 3.2) and provide the convergence analysis of DP-LSGD-GC under the bounded gradient assumption 4.1 and similarity assumption 4.2. for both convex and non-convex cases. The results imply that using multiple local updates, DP-LSGD converges faster to a neighborhood of the stationary point. Through numerical experiments, the paper demonstrates that DP-LSGD converges faster than DP-SGD with the same privacy budget and ""communication"" iterations.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: this paper provides a novel analysis of DP-LSGD, which considers both clipping and DP-noise. The resulting convergence rate improves upon the existing FedGD algorithm. 

Significance: This paper provides a DP algorithm (DP-LSGD) that outperforms DP-SGD with faster theoretical convergence to a neighborhood of stationary points.

Clarity: This paper provides a clear statement of the theorems, assumptions, and adequate numerical justification for the assumption used in the proof.


Weaknesses:
Assumption 4.1: This assumption assumes that the clipping error is bounded, which might be too strong. In Fig 1 (a), it seems like $\Phi$ is increasing as $t$ increases. Such an assumption simplifies the analysis of gradient clipping; Thus, it weakens the significance of the paper a bit.

Comparison with FedAvg: In general FedAvg considers local SGD updates, while the analyzed DP-LSGD algorithm considers local GD updates. Therefore, such a comparison is unfair. [R1] also provides the convergence rate of FedAvg, which matches the rate in this paper. Therefore, the convergence part without clipping is hard to be said to be an improvement.

It is unclear whether the numerical comparison is fair or not. It is hard to decide if the reported result for DP-SGD matches the SOTA results (e.g. in [R2]). The authors should report how the hyper-parameters are chosen and whether they are optimal for the algorithm.

The theoretical result suggested that $c = \Theta(\eta)$ and at the same time $B = O(c)$. However, it is unclear if these two results can be satisfied at the same time. The authors should also conduct numerical justification on different choices of $\eta$ and $c$ and report the corresponding $\Phi$.


[R1] Glasgow, M. R., Yuan, H., & Ma, T. (2022, May). Sharp bounds for federated averaging (local SGD) and continuous perspective. In International Conference on Artificial Intelligence and Statistics (pp. 9050-9090). PMLR.
[R2] De, S., Berrada, L., Hayes, J., Smith, S. L., & Balle, B. (2022). Unlocking high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650.

Limitations:
The authors have addressed the empirical efficiency limitation of the proposed algorithm, which I believe is the largest limitation.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposed a unified analysis of the convergence of (DP)-Local SGD, which covers (DP) parallel SGD as a special case with K=1, for both convex and non-convex optimization. Under this unified analysis, one can identify error effects due to non-iid objectives, clipping, and DP noises and the convergence rate to the error neighborhood.

  

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1) The introduction section provides a brief yet insightful summary of related subjects including ""relation between local SGD and parallel SGD"",  ""sensitivity and privacy analysis methodology of DP"", ""effect and convergence limitation of clipping"" etc.

(2) The unified framework that covers both local SGD and parallel SGD is attractive and makes sense at a high level.

Weaknesses:
(1) The problem setting of this paper assumes each sampled worker is running gradient descent instead of stochastic gradient descent as the full gradient is available in equation (1) though later an additive stochastic noise is added after the aggregation of local updates in equation (2) and (3).   This problem setting is much simpler than the standard setting of local SGD as less divergence across involved local workers is introduced.  The comparison between the analysis from this paper, e.g., Thm 3.2, and the state-of-the-art is no longer fair.  (All the state-of-the-art compared in this paper considered stochastic optimization.  And it is well known that GD for deterministic non-convex has O(1/T) convergence while SGD for stochastic non-convex only has O(1/\sqrt{T}) convergence.) 


(2) The assessment in line 313 that DP-LSGD converges faster with O(1/T) than DP-SGD corresponding to K=1 with O(1/\sqrt{T}) is unfair as the convergence rate from Thm 4.2 requires K=\Theta(T) such that the overall computation/iteration is TK = O(T^2) to attain an error decay like 1/T.  This is effectively the same O(1/\sqrt{S}) convergence where S is the number of computation steps/iterations. 


(3) The new assumption in Assumption 4.1 that involves \Phi in Definition 4.1 does not seem much different from a bounded 2nd-order moment assumption as \Phi_i measures how much the norm of update is larger than the clipping threshold.  (Given that a bounded 2nd-order movement further implies a first-order moment by the inequality E[||X||] \leq \sqrt{E[||X||^2]}, I don't see how this new assumption can relax the widely used bounded gradient assumption in the literature.) Could the author discuss whether the new assumption strengthens or relaxes the standard assumptions? 




Limitations:
I do not see any potential negative societal impact of this paper.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper focuses on Differentially-Private Local SGD (DP-LSGD), and studies its advantages over the foundational technique of DP-SGD. In particular, the authors show why DP-LSGD provides higher clipping efficiency and less clipping bias compared to DP-SGD. The authors start by showing a convergence analysis on the released iterates of LSGD under perturbations and a bounded variance assumption on the stochastic gradients. Next, they generalize the results to DP-LSGD, and show that DP-LSGD has a faster convergence rate near an optimum point compared to DP-SGD. Lastly, they show that DP-LSGD behaves as an efficient variance reduction of local update, and enables more efficient clipping compared to DP-SGD.


Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The authors focus on the important problem of improving the privacy-utility trade-offs for DP Learning.


Weaknesses:
1. The paper contains many theoretical results (Sections 3-5), and I have not been able to verify the correctness of any of the proofs in the Appendix, but after reading the paper it is not even clear to me whether the proofs use any techniques/ideas that are novel (and might be of independent interest), or use methods from prior works to obtain novel results for (DP-)LSGD.
2. The empirical evaluation is very limited, focusing only on image-classification settings (CIFAR10 and SVHN datasets). Given that the focus of the paper is on (DP-)LSGD which is a building block of (DP) Federated Learning, it might be useful to have experiments on FL benchmark datasets, e.g., StackOverflow, EMNIST, etc.?


Limitations:
Yes.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This submission studies the Differentially-Private Local Stochastic Gradient Descent (DP-LSGD), and shows that  DP-LSGD with multiple local iterations can produce more concentrated local updates and   less clipping bias compared to DP-SGD, assuming that the stochastic gradient is of bounded variance.  The main contribution of this submission is to show that DP-LSGD has a faster convergence rate  compared to DP-SGDThe authors also add the experiments to  show that
DP-LSGD produces a better  utility-privacy tradeoff  than DP-SGD.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This submission develops the connections between the clipping bias and the second moment of local updates, which is something new in the research of differentially private optimization.
2. This submission shows that DP-LSGD can converge faster compared to regular DP-SGD.
3. The experimental results ( the comparison between DP-SGD and DP-LSGD) in this submission look convincing, and this paper is well-written.


Weaknesses:
1.The implementation inefficiency (local update in parallel at a cost of large memory) is a minor issue here.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors provide a unified analysis of the clipping bias and the utility loss in privacy-preserving gradient methods for centralized and distributed setups. The conclusion shows that LSGD behaves as an efficient variance reduction of local update, where multiple local GDs with a small learning rate cancel out substantial sampling noise and enable more efficient clipping compared to DP-SGD.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The authors build the connections between the clipping bias and the second moment of local updates. This initializes a new direction to systematically instruct private learning by connecting the research of variance reduction in distributed optimization.

2. The authors conduct analysis on both convex and non-convex ERM problems with a fairly mild assumption of the bounded stochastic gradient variance.

Weaknesses:
1. I understand this is a theoretical paper, but the authors should claim the experimental setup more clearly. It is unclear whether the setting is IID or non-IID. The authors should illustrate the consistency of the empirical support under both IID and non-IID settings, which are the most concern to the FL community.

Limitations:
The authors discuss the limitations of their work in the last section.

Rating:
5

Confidence:
1

";0
58HwnnEdtF;"REVIEW 
Summary:
The paper addresses conditional generation with reward-conditioned diffusion models. They propose to learn a reward function from a small subset of labeled data. The paper aims to answer an intriguing research question: ""How can we reliably estimate the reward-conditioned distribution through diffusions and balance the trade-off between the reward signal and generating quality?"" Additionally, the paper claims that the reward-conditioned diffusion model implicitly learns the latent subspace representation of x.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Strength:
1)  The paper is well written and the problem they try to tackle is interesting.
2) The paper provides an insightful theory for conditional distribution learned through a reward-based diffusion model


Weaknesses:
1) The paper lacks a comparison to other similar models, such as classifier-guided diffusion models.

2) The assumptions made for the theoretical work appear to be overly simplified.

Limitations:
The paper did not discuss any limitations. 
The paper has no potential negative societal impact. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
In this work, authors explore the problem of reward-directed generation using conditional diffusion models in a semi-supervised learning setup. More specifically, they consider a dataset which has a small subset of it labeled with rewards and the majority of it unlabeled. Using the small labeled subset, they first train a reward approximator with regression, then use the learned reward approximator to label the unlabeled portion of the dataset with pseudo labels. Finally, they train the conditional diffusion model using the samples and their pseudo labels. Authors present theoretical explanations for conditioned diffusion models and reward improvement guarantees for reward-conditioned generations. Finally, experimental results are provided that demonstrated the findings of the theory.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Overall Strengths:
1. This paper is very well-organized and well-written and I was able to follow along fairly easily. So, I highly commend the authors for the great job they have done. In particular, all the sections leading up to the main theory are organized well and give a nice background and trajectory to where the theory lies. 
2. The high-level idea of theoretically studying how reward-conditioned distribution of diffusion models change and how to balance reward to trade-off sample quality and reward maximization is well-worth pursuing.
3. Theory is well-driven and thoroughly discussed.


Weaknesses:
Overall Weaknesses:
1. One of the main questions that this paper aims to answer is “How to balance the reward signal and distribution-shift effect to have high-reward and high-quality samples?” which is what I was most excited to learn about. However, I'm not confident that I truly got an answer to this question.
2. The experimental setup is limited and the computed metrics do not thoroughly cover the theoretical findings of the papers.
3. Although figure 2 looks very nice, I’m not sure if it has any added value in the main body of the paper. 


Limitations:
In addition to the points provided in the weaknesses section, I believe a deeper effort on the experiments section would be of tremendous value for this work. For me, the experimental results fall short in supporting the theory of the paper and I’m not sure if I’m convinced the full message is conveyed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents an approach to generation using diffusion models augmented with a reward function. It does so by setting up a semi-supervised learning setup, where the reward function is learned from a small set of data. The reward is then used to learn a reward conditioned score function, which is subsequently used to generate data conditioned on requested reward.
Assuming a linear subspace, the analysis approaches this setting through the lens of linear bandits, and characterizes the error or suboptimality in terms of regret to the target or requested reward, off-distribution error, and on-distribution error.

Experiments evaluate the above interplay, and how requesting higher rewards leads to distribution shift in the generated samples. Further experiments show how pre-trained models can be adapted to generate reward-conditioned samples.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Presents a practical way to add subjective rewards to generate data beyond concrete prompts like text
* Analyzes this reward conditioned generation setting and identifies the interplay between rewards and distribution shift.
* Experimentally verifies the claims made in the analysis.

Weaknesses:
* The paper does not discuss a practical manner in to identify when the generative model starts deviating from the training distribution.
* Comparing the diffusion model's error to a linear bandit setting is interesting, but the exact connection between the two seems a bit murky. The training process does not seem to take advantage of any bandit learning algorithms.
* While it shows that the model can be adjusted to arbitrary rewards, it does not showcase a practical use case.
* The technical novelty is unclear.

=============================
### Post-Rebuttal
* The author's rebuttal and other reviews have made the technical contribution of the paper abundantly clear.
* Additionally, the connection to _offline_ bandits is more evident and does add value.
* The additional RL experiment grounds the claims made in the paper more concretely than the previous experiments.

As such, the majority of my concerns have been allayed with the author response.

Limitations:
while the proposed approach opens up avenues to communicate and generate data using feedback other than text prompts, it does not sufficiently address problems that might arise from such freeform feedback.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper addresses the problem of conditional generation with diffusion models in a self-supervised setting, where the conditional generation is guided by a learned regressor on the small labeled subset. This is referred to as reward-directed conditional diffusion. Assuming the inputs have a latent linear representation, it is shown that reward-conditioned diffusion models implicitly learn this latent representation. Further, assuming a linear reward model is used, it is shown that reward-conditional generation can be viewed as off-policy bandit learning in latent feature space. The theory is also extended to non-nonparametric reward and score functions. Experiments on synthetic and text-to-image data support the theory.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Steering generate models (especially diffusion models) towards generating samples with desirable properties is a topic of wide interest. The semi-supervised setting is a particularly important special case of that, which comes up in many real-world applications. As the authors note, there is not much theoretical work in this space yet, making this a valuable contribution.

I found the theoretical analysis to be quite insightful. I particularly like the analysis regarding the trade-off between distribution shift and reward maximization. The simulation experiment seems to align well with the theory. I also appreciate that the theoretical analysis extends to more general reward and score functions.


Weaknesses:
I found some parts of the paper a bit difficult to read. First, the motivation and derivation of the score network architecture is unclear without reading the reference ([8]). For example, it is not obvious that this functional form follows from the linearity assumption of x = Az. Second, a lot of notation is not introduced, e.g. $k$ in Eq. 2.3, $x_\parallel$ and $x_\perp$ in Assumption 3.2. This makes some equations difficult to understand. Third, it does not become clear until halfway through the paper why the pseudo-labeller is called a reward function. Perhaps a forward reference would help to clarify this.

I am also not sure what the practical implications are. I could have (qualitatively) predicted the results for text-to-image generation without the theory, simply because you are asking the model to extrapolate beyond the training data. The quantitative results also only make qualitative statements.

Apart from that, I found the experimental setup of using a random reward model a bit strange.

Minor comments:
- Given the similarity of Fig. 2c with Fig 2 in [8], citing [8] is probably warranted (""Figure adapted from [8]"") 

Limitations:
The authors have clearly laid out the assumptions for the theoretical analysis. I do not see any major limitations within this scope.

Rating:
6

Confidence:
3

";1
DNHGKeOhLl;"REVIEW 
Summary:
The paper presents a comprehensive theoretical framework for continuous meta-learning in both static and shifting task environments. It also conducts a theoretical analysis of the bi-level trade-off in shifting environments. Furthermore, the paper proposes a method to adapt meta-parameters and the corresponding learning rate.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The main contribution of the paper is the theoretical analysis of continual meta-learning, particularly in the shifting task environment section, which illustrates the concept of meta-level forgetting.
2. The paper provides theoretical evidence of the task-level trade-off and meta-level trade-off in continual meta-learning, offering valuable insights for optimizing the sequence of meta-parameters.
3. The paper introduces a novel AER objective and presents the DCML algorithm, which effectively mitigates forgetting and accelerates adaptation to new environments.


Weaknesses:
1. In my opinion, both continual meta-learning and continual learning share the common objective of addressing the stability-plasticity dilemma, albeit in different settings.  What is the key difference between them? And are there any real applications where the settings of continual meta-learning are more suitable?
2. It is unclear about the training efficiency compared to other methods. 


Limitations:
adequately addressed

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper aims to address the balance between stability (preserving past knowledge) and plasticity (rapid learning from new experiences) in Continual Meta-Learning (CML). The authors propose a novel theoretical framework for CML in both static and shifting task environments. They discuss the bi-level (task- and meta-) trade-off in shifting environments and theoretically identify factors that affect stability and plasticity. They then propose a novel algorithm, Dynamic Continual Meta-Learning, that addresses the bi-level trade-off in shifting environments. DCML minimizes the expected regret by dynamically adjusting the meta-parameter and its learning rate when an environment change is detected.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is theoretically robust, proposing a unified framework for understanding CML in both static and shifting task environments.
2. The paper is the first theoretical work studying the stability-plasticity dilemma in CML with bi-level shifts.
3. Based on the theoretical findings, the authors introduce a novel and practical algorithm which show improved estimated bounds and superior performance compared to baselines.

Weaknesses:
1. From a non-expert perspective, the practicality and necessity of the problem the paper addresses are unclear to me. While meta continual learning is understandable (non-stationarity comes from the non-i.i.d data points within a task), it's not clear why continual meta-learning (non-stationarity comes from the task changes for each time step) is a practical scenario. More real-world examples or explanations of why continual meta-learning is required would be beneficial.
2. The paper lacks a clear, detailed explanation of the robustness of the proposed algorithm. For instance, it's unclear how to set parameters such as a, b, and kappa for DCML. Additionally, the impact of the initial meta parameters (beta) on the model's behavior is not well-explained.

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper considers a continual meta-learning setting with both static and dynamically changing environments and seeks to study the trade-off between learning and forgetting. It derives an upper bound on the excess risk, i.e. the expected gap between the true risk of the predictor and the optimal true risk and uses the average excess risk across tasks as a meta-objective to develop an algorithm that can meta-learn online. The resulting algorithm is empirically evaluated on a synthetic task and a subset of the OSAKA continual meta-learning benchmark. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper derives novel upper bounds on the average excess risk across a number of methods.

Weaknesses:
- Despite its framing, the general approach of using a bound to derive a meta-objective is not completely novel and related work should be more prominently referenced (e.g. [1]).

- The method introduces many hyperparameters and it is unclear where improvements result from. An empirical verification of the theory in the considered settings is missing and ablations on different components of the algorithm that are not necessarily prescribed by the theory (e.g. annealing $\gamma_t$) are missing.

- The presentation of the paper needs to be improved by better motivating and contextualizing theorems and ensuring that all symbols are defined in the main text.

[1] Amit, Ron, and Ron Meir. ""Meta-learning by adjusting priors based on extended PAC-Bayes theory."" International Conference on Machine Learning. PMLR, 2018.

Limitations:
Limitations have been addressed appropriately.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper theoretically studies the stability and plasticity dilemma in Continual Meta-Learning (CML). 

This paper formulates the CML objective as selecting u_t at each time t to ensure a small Average Excess Risk (AER) upper bound for the task sequence. Based on the AER objective, it proposes a continual meta-learning framework in both static and shifting environments.

The proposed Dynamic Continual Meta-Learning (DCML) algorithm can quickly reconstruct meta-knowledge to alleviate forgetting and fast adapt to new environments when change occurs. 

The corresponding theory provides tighter bounds and more flexible base learner selections. 

Experiments demonstrate the superiority of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper first introduces a novel and unified theoretical framework for CML in both static and shifting task environments.
Adaptively learning the meta-parameter can facilitate the training process in deep learning. 

The paper is written and organized well.

The source code is provided.

The paper has good and relevant citations.

The results are quite extensive and convincing.

Weaknesses:
The paper lacks a clear figure that contextualizes the base model, learner, and how continual meta-learning fits into the setup. This visual representation would enhance understanding for readers.



Limitations:
The proposed theory works for non-convex loss when using Gibbs algorithm. However, this paper does not offer an analysis for SGD with non-convex loss.

This paper does not provide theoretical guarantees for memory-based approaches, which can better address the meta-level forgetting with additional memory cost. 



Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors present a framework for continual meta-learning, that is the training of meta-learning models through exposure to a sequence of tasks. The paper presents a formalism for the setting which allows for non-stationary task distributions.
Theoretical analysis is provided both for the framework and for the proposed method, and experimental validation is reported for a reasonable set of continual learning tasks.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

- The problem setting is interesting, and the work is thorough.
- The experimental verification is convincing.


Weaknesses:

- I am not sure if the comparison with non-continual meta-learning method is fair.


Limitations:

N/A

Rating:
7

Confidence:
3

";1
CyuhEwvSof;"REVIEW 
Summary:
This is a theoretical paper that proposes novel ideas for neural architecture search based on neuroscientific research. The author argues for the fundamental role of metabolic contains in the development of biological neural networks, which should inform the optimization strategies used for artificial neural networks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper points out interesting and neglected constraints that should be investigated for NAS

Weaknesses:
- The authors do not present any experiments that could strengthen their hypothesis
- The paper could give a more comprehensive overview of the current state of NAS, also including early work from the evolutionary community that already did incorporate energy constraints in the form of connection costs, e.g. ""Clune, Jeff, Jean-Baptiste Mouret, and Hod Lipson. ""The evolutionary origins of modularity."" Proceedings of the Royal Society b: Biological sciences 280.1755 (2013): 20122863.""
- Some of the concept, such as glia-neural networks, could be explained in more detail


Limitations:
Yes.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to neural architecture search that optimizes for energetic efficiency and draws inspiration from the role of glial cells in biological brains.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The idea of energetic efficiency objective functions for neural architecture search may have merit.

Weaknesses:
The paper does not really make a technical contribution -- there are no experiments or theoretical results.  The paper is essentially a proposal for a high-level idea about neuroscience-inspired neural architecture search, which is not adequate for a conference like NeurIPS.  Morevoer, the survey of neuroscience literature, while potentially interesting, is not presented in a way that would be comprehensible or actionable for machine learning researchers hoping to leverage these ideas.  I encourage the author(s) to make more concrete their proposed approach and evaluate it empirically if their goal is to improve upon current NAS methods.

Limitations:
I found the discussion of limitations hard to follow and so cannot evaluate it properly.

Rating:
2

Confidence:
3

REVIEW 
Summary:
This article suggests that the concept of energy-driven network architecture selection in the brain can be applied to artificial neural architecture search (NAS). It proposes redefining the relationship between artificial neurons to include the metabolic cost of computation. The article also discusses the relationship between brain behavioral states and energy management, as well as the theory of dynamic coordination and metabolic optimization in biological systems. It suggests that the optimization of energy-driven systems can be seen as the optimal solution to the complexity of biological systems. The article concludes by proposing an updated NAS strategy that incorporates glial-neuronal ensembles and energy-constrained architecture search.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
I believe that this kind of interdisciplinary perspective should be encouraged. AI has benefited greatly from biological inspiration and insight, and I commend the authors for their effort. The article proposes that two aspects of biological brains receive higher focus in modern deep learning: the contribution of glial cells and metabolism. I do believe that it is useful to clarify the differences between the biological brain and deep learning on these aspects.

Weaknesses:
The article does not sufficiently cover contemporary deep learning literature, which does make links to biological neural networks or their properties. While this is less evident for the inclusion of glial cells, there is substantial literature on network efficiency, including in NAS. It is common practice in NAS to compare architectures not only on error on a test dataset, but also measures of efficiency like parameter count or number of floating point operations. NSGA-Net, for example, explicitly searches using two objectives: one of accuracy and one of efficiency. 

Lu, Zhichao, et al. ""Nsga-net: neural architecture search using multi-objective genetic algorithm."" Proceedings of the genetic and evolutionary computation conference. 2019.

Hardware-aware NAS methods take into account the energy limitations of different hardware, aiming to find a balance between energy efficiency and performance:

Chitty-Venkata, Krishna Teja, and Arun K. Somani. ""Neural architecture search survey: A hardware perspective."" ACM Computing Surveys 55.4 (2022): 1-36.

A common motivation to compress networks, through pruning or reducing precision, is to reduce their energy use:

Yang, Tien-Ju, Yu-Hsin Chen, and Vivienne Sze. ""Designing energy-efficient convolutional neural networks using energy-aware pruning."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.

The proposed NAS method which takes ""metabolic"" constraints into account is in the same line, however the article does not acknowledge the existence of this common consideration in the current NAS literature. The claim that ""Current AI is based on optimization strategies for information (Imax ) without metabolic constraints (Emin )"" is false if we consider measures of floating point operations or computational time on specialized hardware as equivalents to metabolic constraints. The article does not detail how metabolic constraints would be calculated, asking instead ""Is Emin for AI just the cost of electricity or also an intrinsic system’s property as exemplified by the theory of dynamic coordination?"". Given that measuring energy cost in terms of computational complexity (number of floating point operations) is standard in NAS, the proposed definition of Emin should be clarified in the article.

The article is very short for a NeurIPS submission at less than 5 full pages and could have gone much further in detail both on the proposed ideas and their context in contemporary deep learning.

Limitations:
As previously mentioned, the main limitations of this article are the scope of work considered and the lack of detail of the proposed ideas. The proposed ideas should be more fully realized and more firmly grounded in the current literature. The Limitations section on page 5 does not adequately address these limitations.

Rating:
2

Confidence:
3

";0
B4TAPfHa7g;"REVIEW 
Summary:
The paper introduces Constrained Proximal Policy Optimization (CPPO) for Constrained Reinforcement Learning (CRL). The CPPO method is designed to overcome the limitations of existing methods by offering a first-order feasible region method that doesn't require dual variables or second-order optimization, and it is an incremental extension of the  CVPO algorithm (Constrained Variational Policy Optimization for Safe Reinforcement Learning). The improvement seems incremental, improving the computational efficiency of CVPO. The method is evaluated in different environment,  comparable or even superior performance to other baseline methods.  . However, the paper does not provide  a direct comparison between CPPO and CVPO.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The authors propose a new first-order method for constrained RL. The method seems to be designed to be simple, and overcome  limitations of existing methods such as CPO and CVPO (e.g., the authors do not require the usage of a dual variable).

- The proposed method demonstrates comparable or even superior performance compared to other baseline methods. 

Weaknesses:
- Clarity: the paper could benefit from improvements in clarity and readability. The presentation of their ideas is somewhat dense and could be difficult for readers to follow. A concise description of the algorithm is missing.

- Novelty and results: The method builds directly upon CVPO. The authors need to clearly highlight this fact when introducing their method, and clearly explain what are the differences. As of now, the changes seem incremental, and therefore the paper seems to lack in novelty. Furthermore, the paper does not provide  a direct comparison between CPPO and CVPO, especially in terms of computational complexity

- No theoretical results are provided, therefore I'd have expected more extensive numerical results.

- Typos and notation: there are several typos and errors in notation throughout the paper.

Limitations:
Authors briefly discuss limitations and broader impact.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper introduces the constrained version of PPO that is devised for solving constrained MDP problems in the discounted reward setting with discounted cost constraints.
 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper presents the constrained PPO. It also gives some analytical results and presents a heuristic algorithm that is seen to perform well numerically. 

Weaknesses:
1. Inadequate literature survey. The authors should broaden the scope of their survey - in fact constrained actor critic was proposed in the paper: ""V.S.Borkar, An actor-critic algorithm for constrained Markov decision processes, Systems and Control Letters, 54(3):207-213, 2005'', for the full state case and ""S.Bhatnagar, An actor–critic algorithm with function approximation for discounted cost constrained Markov decision processes, Systems and Control Letters, 59(12): 760-766, 2010, in the function approximation setting.

2. The authors point to the fact that they have not provided a convergence analysis for their algorithm even though other works in the literature such as the ones listed above do possess an asymptotic analysis of convergence.

3. The results given in the text carry imprecise statements, for instance, Prop. 3.1 says ""if there are a sufficient number of sampled v, then E[v]=1 and E[vlog v] <= var(v-1). This has to be made precise. Does it  mean that in the limit that the number of samples goes to infinity, the statement is valid? Then, if so, the question will be what will be the form of the statement in terms of the number of samples N.

4. The recovery update problem in Sec 3.2 is not clear - how it has been arrived at? 

5. Subsequently a heuristic procedure has been proposed and the authors say that an optimal solution to (5) is provably obtained, But if it is provably obtained, how is the procedure a heuristic procedure? 

6. After (4), it is said that (4) can be directly solved through existing convex optimization techniques. This is not clear since one needs A(s,a), A_c(s,a) etc. to be known in order to solve it directly. But that is not known and needs to be estimated. So not clear what the authors mean?
****
7. On further reading of the paper and supplementary material post the response of the authors, I feel the technical results are imprecise and flawed. For instance, there is no way to verify Assumption 3.5. Moreover, the statement of Proposition 3.1 seems to suggest that it depends on a ""sufficient number of sampled v"" but gives a bound on the true expectation. Why should the true expectation depend on the number of samples of v? Also, when you say ""sufficient number of sampled v"", what does it mean? How many v is a sufficient number?


Limitations:
The major limitation is in terms of a lack of credible analysis. Even the theoretical results presented are not precise, so nothing much can be said about the algorithm. In addition there are several typos and grammatical errors throughout the paper.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel first-order feasible method, CPPO, for efficient constrained reinforcement learning. The proposed approach integrates the Expectation-Maximization (EM) framework to solve the policy optimization problem by treating the CRL as the probabilistic inference. In the E-step, CPPO calculates the optimal policy distribution within the feasible region. In the M-step, CCPO conducts a first-order update for policy optimization. The authors also propose an iterative heuristic algorithm from a geometric perspective to efficiently solve the E-step and a recovery update strategy to improve constraint satisfaction performance. They evaluate their algorithm in several benchmark environments. The reported results show comparable performance over other baselines in complex environments.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The proposed algorithm converts the CRL problem into a convex optimization problem with a clear geometric interpretation which mitigates the impact of approximation errors and strengthens the capability of the proposed method to satisfy constraints.

(2) Since the proposed method does not require second-order optimization techniques or the use of the primal-dual framework, the policy optimization process is largely simplified.


Weaknesses:
(1) I suggest further polishing and improving the mathematical formulation and notations to make them more rigorous. For example, the objectives and constraints in (5) (6) are represented by the dot product of two vectors. In my understanding, it should be easier to understand with a transpose on the top of the first vector. 

(2) The policy update strategy looks overly conservative. Figure 1 shows that only in case 3, the policy can be updated toward seeking a higher reward. Does this strategy result in over-conservativeness? I am also wondering in cases 1 and 3 why the solution is not at the feasible boundary (that is to find the feasible distribution to maximize A).

(3) It is very impressive that CPPO can work well in the AntCircle and Push tasks. However, the experiment lacks sufficient baselines and environments for comparison. For example, the recovery update in Section 3.2.2 is similar to the idea in Yang et. al [1, 2], where an additional projection step is introduced to recover the safe policy, so I am wondering how does the author compare CPPO against these methods, both in theory and empirically. In addition, the results only present a subset of tasks in SafetyGym, so I wonder how the algorithm performs in other tasks.

[1] Yang, Tsung-Yen, et al. ""Projection-based constrained policy optimization."" arXiv preprint arXiv:2010.03152 (2020).

[2] Yang, Tsung-Yen, et al. ""Accelerating safe reinforcement learning with constraint-mismatched baseline policies."" International Conference on Machine Learning. PMLR, 2021.

Limitations:
The authors adequately addressed the limitations.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel CPPO method to solve the contained RL (CRL) problem. Specifically, CPPO leverages the probabilistic inference and converts the CRL problem formulation based on the probabilistic ratio, resulting in the first-order optimization solution. CPPO also develops the recovery update method to safely optimize the policy when there are inaccurate cost evaluations and infeasible solutions. 
The resulting EM-based framework of CPPO shows its effectiveness across various safety gym scenarios compared to baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. CPPO solves the constrained RL based on first-order and does not use dual variables and second-order optimization, resulting in a simpler, intuitive (i.e., geometric perspective), and computationally efficient method. 
2. The recovery update method is developed thanks to CPPO's first-order optimization/geometric perspectives and shows its effectiveness (as shown in Figure 4).


Weaknesses:
1. As stated in Section 3.2.1, CPPO builds on CVPO and has two main differences (using advantage instead of q and using the probability ratio instead of directly calculating q). While  Some readers may understand these as a limited novelty. Possibly adding comparisons against CVPO in the evaluation section can convey the importance of these differences better. 
2. While I agree that CPPO reduces computational complexity, there are no empirical results in the evaluation section. Possibly, adding computation time results in the evaluation section can help. 
3. I understand the benefits of converting the CRL problem into first-order optimization, but it is unclear what potential limitations/disadvantages the first-order optimization may have. Could there be an approximation error compared to second-order optimization? Related to this concern, could the authors clarify further why Assumption 3.5 is a fair assumption?


Limitations:
The authors have adequately addressed the limitations.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies constrained reinforcement learning problems. It proposes a new EM-type algorithm and designs a heuristic version for practical use. The authors also conduct some numerical experiments to validate the performance of the algorithm.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is first-order and thus computationally efficient in practice.
2. The numerical results look convincing. 

Weaknesses:
1. The algorithm looks very similar to CVPO and only has some small modifications.
2. This paper does not have convincing theoretical analysis. For example, the authors claim that the heuristic algorithm in Section 3.3 will stop in just a few iterations (remark 3.6) but there are not any theoretical proofs. It would be better if the paper can provide some theoretical study, even only for simple cases.  

Limitations:
None.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper focuses on constrained reinforcement learning and proposes a method called Constrained Proximal Policy Optimization (CPPO). Experimental results demonstrate the improved performance in terms of episodic return and episodic cost.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ The proposed CPPO achieves improved balance between return and cost empirically.

Weaknesses:
- The novelty and contribution of this work is not clear. The difference between the proposed CPPO and CVPO does not seem to be very significant. Adopting the advantage value instead of Q-value is a natural extension. Besides, it is unclear how much sample complexity can be reduced by replacing $q$ by $v$. After all, $v$ should still satisfy the expectation constraint.
- The paper does not have any theoretical characterization of the proposed CPPO algorithm.
 

Limitations:
 The paper mentions that CPPO method is an on-policy constrained RL, which suffers from lower sampling efficiency compared to other off-policy algorithm.

Rating:
4

Confidence:
3

";0
PbMBfRpVgU;"REVIEW 
Summary:
The paper tackles interpretable RL with a neurosymbolic method (NUDGE) separating perception and action prediction. Given a pre-trained actor-critic model and a pre-trained/given perception module, NUDGE checks randomly generated rules and assess their quality by comparing them to the pre-trained actor. It then performs rule weight learning using an A2C-like algorithm using the pre-trained critic.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
I like the ideas, and the paper offers an excellent combination of methods with clear benefits in terms of explainability and possibly generalisation (although I think the verdict is out on this). The paper is well-written, easy to understand and complete. 
The explicit parameter $k$ allows limiting the complexity of policies, generating interpretable policies as highlighted in Figure 4.

Weaknesses:
The experimental setup varies significantly throughout the different settings, leaving me with questions about the experimental methodology.

Limitations:
The method requires a (pretrained / hand-designed) perception module that can be hard to come by. The paper does not discuss this limitation thoroughly. Otherwise, limitations are addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents NUDGE, an RL technique that combines neural network and logic program learning to learn policies that are interpretable and explainable by humans. NUDGE produces agents that match or exceed the performance of agents trained using standard RL techniques like PPO.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written, clear, and addresses the significant challenge of uninterpretable reinforcement learning policies. The paper proposes to solve this by learning policies that are interpretable and explainable by design. 

I am unqualified to assess the originality of this paper because I am unfamiliar with the logic program learning literature.

Weaknesses:
This work can be improved with stronger definitions of interpretability and explainability. Currently, it’s unclear what criteria make a model interpretable or uninterpretable (line 43). What exactly makes a policy readable by humans? Is this person an expert or a layperson? I suspect that interpretability here means that an expert can assign meanings to the policy. Similarly, what constitutes an explanation (line 44)? From their definition, it seems that the explanation need not be causal? Do all methods from explainable AI count?

There are also existing definitions of both terms from the community. I would like to see the authors engage with the existing literature on definitions of interpretability and explainability. See Lipton 2016 for a discussion on interpretability. I think that having set definitions is important for this work in order to avoid moving goalposts.

Line 141: Another weakness is that the relational perception module maps all entities to a particular index in the valuation vector. This method does not seem like it would scale to new objects. Similarly, the identity of entities seem to be fixed given a particular validation vector. It seems like the parallel version of this weakness (parallel in the sense of scaling issues) for the forward reasoning module is solved by neurally guided symbolic abstraction (line 193).

I also found some parts on what parameters are and aren't learned to be confusing. See questions.

Limitations:
The authors have adequately addressed limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper aims to learn more interpretable and explainable logical policies. It modifies NLRL, Neural Logical RL, with (1) an extra neural-guided heuristic, i.e., alignments with a pretrained neural policy, for filtering logical rules during search; (2) changing policy learning algorithm to actor-critic from policy gradients; (3) pre-defining / hardcoding the meanings of symbols for more interpretability; and (4) explanation of policies according to the gradients. It conducted experiments in five simple games. 

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
This paper uses pre-trained policy networks as neural guidance for faster search of logical rules (filtering according to their alignments). This could be an interesting direction to explore. However, experiments that evaluate/support its effectiveness are not adequate in the paper. 

Weaknesses:
This paper claims that their methods improve the interpretability and explainability of logical policy learning. However, as far as I understand, this is not true because 
* The proposed methods (compared with existing ones) are mainly about optimization and logical rule mining, instead of about interpretability and explainability.
* Policies become more interpretable only because they change the problem setting to an easier and more interpretable one. They assume predefined/hardcoded meanings of all the predicates/symbols (e.g., `close_by(obj1, obj2)`), as well as a pretrained neural module to detect the objects and those meaningful attributes. 
  - Previous methods do not assume those and instead, jointly learn the symbols and their meanings with logical rules. 
  - Any model with access to those predefined meanings and those pretrained detection modules is more interpretable and explainable. For example, when given access to those symbolic inputs (i.e., memory states), even DQN can adapt to environmental changes (as defined in Q2, Sec. 4) and would be explainable by performing the gradient analysis. 

From the optimization perspective, the new problem setting is less interesting and challenging as well. With access to symbolic inputs (e.g., the memory states of Atari games), classic ILP methods may even work, especially when considering the relatively small search space in the five simple games. The authors should compare with those baselines as well. 

There are other minor concerns such as 
* If the neural policies are much worse than the symbolic ones as reported, why and how would it be helpful to distill/use them as neural guidance?
* The games are relatively simple and easy. 
* There should be experiments analyzing the effects of actor critics v.s. policy gradients. 


Limitations:
N/A

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper introduces NUDGE, a framework for interpretable and explainable policy reasoning and learning in reinforcement learning. NUDGE employs differentiable forward reasoning during inference to derive a set of weighted rules that form an interpretable policy. During training, NUDGE leverages neurally-guided symbolic abstraction to efficiently distill rule-based symbolic representations from neural-based policies. It then employs gradient-based policy optimization using actor-critic methods to learn the weights of these rules. Empirical results demonstrate that NUDGE achieves comparable performance to neural-based policies while providing interpretable and explainable logical representations. Furthermore, the rule-based symbolic policies in NUDGE exhibit adaptability and flexibility in addressing environmental changes. Overall, NUDGE offers a promising approach to achieving interpretable and adaptive policies in reinforcement learning.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ NUDGE introduces end-to-end reasoning architectures based on differentiable forward reasoning, allowing for comprehensive policy generation directly from raw input. 
+ Its neurally guided symbolic abstraction leverages a pre-trained neural policy to induce a set of candidate action rules, which are subsequently optimized using the critic component of an actor-critic agent. 
+ Notably, NUDGE policies are differentiable, enabling the utilization of action gradients with respect to input atoms. This unique feature facilitates the explanation of state features that influence the decision-making process, shedding light on how specific actions are chosen in a given state. 

These aspects collectively contribute to NUDGE's interpretability and explainability, providing insights into the underlying mechanisms of the learned policies.

Weaknesses:
It appears that the individual components of NUDGE are not inherently novel on their own. The paper's main contribution lies in the integration of existing components to form the NUDGE framework. By bringing together existing components such as differentiable forward reasoning, neurally guided symbolic abstraction, and policy optimization, NUDGE creates a framework for interpretable and explainable policy learning. Although the individual components are not novel on their own, the paper would have been acceptable if it can still demonstrate the novel insights that arise from their integration. However, the novelty of NUDGE in comparison to prior work such as PIRL [Verma et al., 2018] appears to be very limited. Both NUDGE and PIRL employ a pretrained neural policy as a guide for learning policies. Additionally, both NUDGE and PIRL utilize random search to update policy structures and employ optimization techniques to learn policy parameters. The primary distinction lies in the fact that NUDGE learns programmatic policies within logical programs, whereas PIRL learns policies within functional programs. While NUDGE can learn from raw image inputs, this aspect is not considered a core contribution, as it relies on off-the-shelf tools for object extraction from visual inputs. 

Limitations:
Although the paper is well executed and integrates existing components effectively, it does not present advancements that significantly differentiate it from prior work PIRL [Verma et al., 2018].

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this work, the authors propose neurally guided differentiable logic policies (NUDGE), which learns differentiable logical policies that are represented by interpretable rules. It is capable of producing explanations for their decisions in complex environments. NUDGE utilizes neurally guided symbolic abstraction to boost the model's performance. Specifically, the candidate rules for the logic-based agents are obtained efficiently by being guided by neural-based agents. Experimental results show that NUDGE can achieve competitive performance compared with the neural baselines. Besides, the NUDGE agent can adapt to environmental changes and its learned policies are interpretable and explainable.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The authors propose using a novel mechanism called neurally guided symbolic abstraction, to boost the neural logic model's performance.
2. The writing of the paper is well-organized and clear, and the evaluations are quite comprehensive.

Weaknesses:
It is unclear how are the abstract head atoms being interpreted, for example, **right_go_to_door** used in the GetOut task and **right_to_eat** in the Fishes tasks. Are they automatically learned or interpreted by developers manually?

Limitations:
The authors have adequately addressed the limitations.

Rating:
5

Confidence:
2

";1
7LtzqnfuOs;"REVIEW 
Summary:
The paper focuses on value decomposition without IGM constraints by proposing Dual self-Awareness Value dEcomposition (DAVE) framework. DAVE is inspired by dual self-awareness studied in psychology and uses an ego model, i.e., a policy for each agent for actual action selection and an alter ego model, i.e., a value function to address credit assignment without the IGM constraint. To avoid premature convergence to poor local optima, an anti-ego exploration method is proposed. DAVE is evaluated in several domains and compared with many state-of-the-art MARL approaches.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper proposes an interesting approach inspired by knowledge from psychology.

The evaluation is structured well by starting with small and tractable problems first and scaling up. Many state-of-the-art MARL algorithms are used as baseline for sufficient comparison.

Weaknesses:
The paper strongly focuses on the omission of IGM, which is motivated as a limitation of MARL. However, DAVE introduces additional complexity that at least doubles the computational effort compared to the baselines, e.g., two networks are used per agents and the exploration mechanism adds more parameters on top of that (note that despite doubled parameters, DAVE does neither perform twice as well nor learn twice as fast to fully compensate for the additional effort). Therefore, the fairness of comparison is questionable. Furthermore, I am not sure about the purpose of the individual Q-functions, since they are not used to maximize the joint Q function as usual. The ego model/policy is used instead thus individual Q-functions could be completely omitted as far as I understood the text.

The initial motivation of dual self-awareness in the introduction implies that ego and alter ego models have a symmetric relationship. However, the ego model represents a policy while the alter ego model represents a value function, which is sometimes called ""alter ego model"" and sometimes ""alter ego value function model"", which is confusing to read.

The maximization of the joint Q function through sampling, reminds me of FACMAC, where agents select their actions in a similar way (albeit for continuous action spaces though).

Despite the nice structure of the experimental presentation, the SMAC results are not particularly overwhelming because SMAC is a widely solved benchmark, where most baselines already achieve high win rates. Furthermore, the results are not in line with previous work, e.g.

- MAVEN reaches an average win rate of 40% at 5 million steps in 6h_vs_8z in the original paper, while it remains with 0% win rate in this paper with the same about of steps.
- QMIX reaches 100% win rate in 10m_vs_11m and 80% in 3s_vs_5z in the SMAC paper, while it is notably lower in this paper.

The paper heavily depends on the appendix indicating its lack of self-containment. The paper's content or the approach need more simplification to give a sense of completeness of the presentation.

Figure labels and numbers are only readable on a sufficient zoom scale which is problematic when printed.

Limitations:
Some limitations like the additional compute through sampling actions are mentioned in the text.

Potential negative societal impact is not discussed at all.

Rating:
4

Confidence:
4

REVIEW 
Summary:

This paper proposes a different approach for multi-agent RL algorithms that does not depend on individual global max (IGM), but instead builds a new framework, DAVE,  based on dual self-awareness. The algorithm is shown to perform favorably in several testing cases.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well organized and the sections’ structure serve their purpose in illustrating this new framework. The experimental results also corroborate the authors’ claims and show that the proposed framework/algorithm, DAVE, performs better than other MARL algorithms. 


Weaknesses:
The main weakness in this paper is the lack of justification of differences between DAVE and the actor-critic models. In addition, since this paper proposes a new algorithm, it would be nice to have the pseudocode be in the main text. Overall, a better analysis of the sampling procedure is needed to justify the new DAVE framework.
 


Limitations:

The proposed model “dual self-awareness” requires a specific sampling procedure to get the actor policy which renders this framework as a special case of actor-critic algorithms.  


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel MARL algorithm, Dual self-Awareness Value dEcomposition (DAVE), which avoids the IGM constrain obeyed by most of the previous researches. The algorithm introduces three different policies including Ego Policy, Alter Ego Policy and Anti-ego Policy, where the Ego Policy tries to fit the optimal actions of the Alter Ego Policy and the Anti-ego Policy serves as an exploration strategy. Empirical results show that DAVE outperforms the baseline methods on several cooperative tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed DAVE algorithm avoids the IGM principle, which typically requires consistency between local and global optimal actions. This deviation enables DAVE to leverage the expressive capabilities of neural networks, particularly the mixing network component.
2. The Anti-Ego policy, a key component as the DAVE's exploration strategy, adopts the idea of count-based exploration techniques such as RND [1] and ""Ensemble"" in model-based RL. It quantifies the familiarity of $(s,a)$ pairs by evaluating the reconstruction loss of an auto-encoder.
3. Experimental results on Matrix Games show that DAVE does avoid local optima. Additionally, experiments conducted on SMAC, MA-MuJoCo indicate that DAVE-QMIX performs comparably to QPLEX and other baseline algorithms.


[1] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.

Weaknesses:
1. **Methodology**: 

    *a*) The Ego Policy only approximates the global optimal actions $u^{*}$ by NLL loss, which means the equivalence between the Ego Policy and the global optimum is not strictly guaranteed while it is hold strictly by IGM principle.

    *b*) The basic idea of the exploration strategy is not quite novel. Simultaneously, while sampling from the Anti-Ego Policy guarantees the selection of low-Q actions, it does not guarantee that suboptimal-Q actions have been explored enough in the environment when the $M$ is not large enough. This is because reaching state $s$ is not guaranteed. As a result, the Q-values associated with these actions are prone to overestimation or underestimation, and the exploration policy tends to overlook this aspect of the action space.

2. **Experiments**: The results on SMAC and MA-MuJoCo are a bit confusing to me and i have listed my concerns in the *Qustions*.

Limitations:
Limitations: This work is mostly aimed to solve the cooperative Multi-agent tasks.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents DAVE, an IGM-free value decomposition method, to enhance coordination ability in MARL. Drawing inspiration from the concept of dual self-awareness in psychology, DAVE consists of two components: an ego policy responsible for executing actions, and an alter ego value function involved in credit assignment and value estimation. By incorporating an explicit search procedure, DAVE eliminates the need for the IGM assumption and updates the actor using supervised signals instead of policy gradients. Additionally, the authors propose a novel anti-ego exploration mechanism to prevent the algorithm from being trapped in local optima. The method demonstrates impressive performance across a range of cooperative tasks, highlighting its effectiveness.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. By employing NLL instead of policy gradients to update actors, DAVE effectively addresses the challenges associated with using unconstrained (without IGM assumption/monotonicity) factored critic.
2. The utilization of AE loss in Anti-Ego Exploration to assess the novelty of joint-action is intriguing, albeit bearing some similarities to RND [1].
3. The paper is well written and easy to follow. The integration of psychological concepts with algorithm design is a unique aspect that captured my interest. Furthermore, each component of the overall framework is thoroughly elucidated.
4. DAVE demonstrates impressive performance in both matrix games and SMAC, with clear discussions regarding its limitations.

Weaknesses:
1. While the authors claim that DAVE is the first method to completely eliminate the IGM assumption in value decomposition approaches, it's worth noting that FACMAC-nonmonotonic [2] also achieves full IGM-free. Although DAVE may outperform FACMAC-nonmonotonic, the contribution of DAVE might have been overestimated. 
2. The discussion on related works is insufficient, and it would be beneficial to include references to value decomposition actor critic methods [2][5].
3. Minor 1: The idea of applying NLL loss bears similarities to previous works that combine Cross-Entropy Methods with Policy Improvement [3][4] (discussion can be included).
4. Minor 2: It appears that exploration has a significant impact on the performance of DAVE, making it a crucial component that cannot be disregarded, unlike simple epsilon-greedy exploration. It would be advisable to include comparisons between DAVE and other MARL exploration methods, such as EITI/EDTI[6] and CMAE[7], in the appendix.

Limitations:
As mentioned in Weakness.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper proposes a novel value decomposition framework for MARL based on the notion of dual self-awareness in psychology. The framework, called DAVE, consists of two neural network models for each agent: the alter ego value function model and the ego policy model. The former participates in the evaluation of the group to solve the global credit assignment problem, and the latter helps the former to find the optimal individual action through an explicit search procedure. The paper claims that DAVE is the first value decomposition method that completely abandons the Individual Global Max (IGM) assumption, which requires consistency between local and global optimal actions. The paper also introduces an anti-ego exploration method to prevent the ego policy from getting stuck in a bad local optimum. The paper evaluates the performance of DAVE in both simple and complex environments, such as StarCraft II and Multi-Agent MuJoCo, and compares it to other popular baselines. The paper shows that DAVE can solve non-monotonic problems that challenge existing value decomposition methods, and can achieve competitive performance despite the absence of IGM. The paper argues that DAVE provides a new perspective on addressing the problems posed by the IGM assumption in value decomposition methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Improving the efficacy of CTDE and value decomposition-based approaches to cooperative MARL is an important topic. The paper's investigation of the role of the IGM principle in CTDE approaches is novel and should be of interest to the broader MARL and RL communities. The method is supported by empirical results in both small-scale, well-principled environments and larger-scale, complex environments; and is comprehensive in its comparison to baselines. The paper is well-written and clear.

Weaknesses:
- The related work in the main paper is a bit sparse given the depth of work in this area. Moving more of the discussion from Appendix E into the Related Work section would improve the contextualization of this work.
- Given the importance of sample size M, it would have been nice to see a greater exploration of the performance/computational cost trade-off. For example, how does M scale in more realistic environments? Are there environments where an unreasonably high value of M is needed? In the MMM2 example in Fig. 7 it looks like the learning signal is just beginning.
- There is limited discussion of limitations, future work, and societal impact.


Limitations:
The main limitation here is the computational cost of large values of M. This does not prevent strong performance in the environments studied in this work, but may be expensive in more complex and/or real-world environments. There are also other small limitations, e.g. tuning exploration coefficients.

Rating:
7

Confidence:
4

";1
TRuqrVsmZK;"REVIEW 
Summary:
The authors introduce BayesTune, a method for choosing which are the parameters to fine-tune in a pre-trained model. Their formulation is based on Bayesian inference where they use a Laplace prior over the parameters/network weights. The prior has two variables: mean, which is the value of the pre-trained parameters, and scale which specifies how important is to fine-tune it. These hyperparameters are also controlled by a hyperprior that is fixed among all the experiments. For inferring the posterior distribution of the weights and scales. they adopt Langevin dynamic method. After obtaining the scale value, they compute a cut-off value that determines the parameters to be updated. Experiments performed in Computer Vision and NLP tasks demonstrate that the method is competitive and outperform common techniques.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
* The problem of efficient fine-tuning is very important nowadays given the availability of large pre-trained models.
* The method is intuitive and interpretable, as it is based on the Bayesian interpretation of the network weights.
* There is a large set of experiments demonstrating the utility of the method.

Weaknesses:
* This model is close to related work such as SP-regularization [1] which uses a regularization term to encourage the updated weights to stay close to the original pre-trained weights. Using an L1-SP regularizer might have a similar effect as the one introduced in this paper. I think it is important the authors mention this and compare BayesTune against it.
* The efficiency of the method is not clear to me. For instance, in an attention layer, updating only some parameters (sparse updates) still demands computation of the matrix multiplications for query, key, and value matrices. The same logic applies when backpropagating. Thus, choosing only some parameters to update might only save memory demands, but not time. If my reasoning is correct, then the authors should specify this in the paper. Also, some measures of time and memory consumption might be useful. 
* There is a discrepancy between Algorithm 1 and the method described in the paragraph starting at line 206. Thus, I think Algorithm 1 is incomplete.
* Their method seems competitive for NLP but not so for Computer Vision, based on Table 2.

[1] Xuhong, L. I., Yves Grandvalet, and Franck Davoine. ""Explicit inductive bias for transfer learning with convolutional networks."" International Conference on Machine Learning. PMLR, 2018.

Limitations:
No limitation was mentioned by the authors.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes an approach for selecting a subset of weights in a foundation model to fine-tune on a downstream task. The method consists of a two-stage pipeline, where in the first stage a Laplace prior is placed on each weight with a Gamma hyper-prior on the scale. Samples are obtained via SGLD and only the weights with a mean posterior scale above some threshold are then trained via SGD in the second stage. The method is evaluated on GLUE and SuperGLUE tasks with RoBERTa and on VTAB-1k image prediction tasks with a vision transformer and compares overall favorably to a range of baselines from the literature.

There are quite a few design choices constituting the proposed method and, unfortunately, none of their added complexity is justified via ablation studies. Further, the paper in my view overstates how principled it is quite significantly, so that at this point I would lean towards rejection.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
* The approach is new as far as I am aware.
* The technical description of the method is clear.
* Performance seems to be good and approaches for better fine-tuning are of high interest to the community.

Weaknesses:
* The core problem in my view is that the method consists of quite a few moving parts, but these aren’t justified via ablations. It is not clear at all where the performance improvements come from and whether all parts of the method are needed. E.g. it might be the case that the two-stage procedure with magnitude-based pruning would be enough (at least my understanding of MagPruning based on the description in the paper is that the smallest pre-trained values are pruned). Similarly I wonder if sampling in Stage 1 is necessary or if MAP estimates would be good enough for the scale parameters.
* I don’t really see what makes the proposed method particularly principled as claimed at various points in the paper. I don’t think there is a probabilistic justification for the two-stage procedure and fudging the dataset size and noise scale for SGLD is just a hack.

Limitations:
n/a

Rating:
5

Confidence:
3

REVIEW 
Summary:
A principled approach for selecting a subset of parameters to fine-tune in large foundational models is proposed. The authors rely on Bayesian inference to identify this subset. They begin by placing a Laplace prior over the model weights and a gamma hyperprior over the weight scale. Next, they employ an MCMC method to obtain a posterior distribution. Finally, they rank the weights based on the posterior scale values and proceed to fine-tune the parameters with the highest inferred scale values. Across standard NLP and vision adaption tasks, they demonstrate a strong empirical performance compared to previous state-of-the-art approaches. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- I find the proposed methodology technically sound, novel, and appealing. Though it should be kept in mind that I am not particularly familiar with the related work on the finetuning of large models. Similarly, I like methods that are Bayesian, so this might add to my potentially biased evaluation here. 
- I like the simplicity of the proposed method. It does not happen often that I understand the method upon the first pass through the paper, but I think that was the case when reading this manuscript.
- The empirical results are strong and I want to commend the authors on extensive evaluation (i.e., they do not stop at the language modality, but additionally consider a vision domain as well).

Weaknesses:
A presentation could be somewhat improved to further strengthen the manuscript. Some concrete suggestions:
- I find it a bit strange to start introducing the notation already in the Introduction section. Hence, I would change your current section 1.1 into a separate section 2.
- Similarly, I find it a bit weird to list out the related work as bullet-points they way you do in Section 3. I find it more natural to use separate paragraphs for that (see Section 8 in [1] for an example of that). This way you can also directly talk about how each related approach connects to your proposed model, instead of doing that in a separate paragraph as is currently done (lines 192-198).
- Line 227: I assume you will include a GitHub link here, not put the code in the Supplementary material (whatever that means).
- Grammar and style could be improved at some points to improve readbility. This is easily done these days via tools like Grammarly, ChatGPT...

Limitations:
Limitations are currently not discussed. Perhaps authors could use the gained space from restructuring the related work section (see above) to add a paragraph or two on the limitations of their approach.



[1] Daxberger, E., Nalisnick, E., Allingham, J.U., Antorán, J. and Hernández-Lobato, J.M., 2021, July. Bayesian deep learning via subnetwork inference. In International Conference on Machine Learning (pp. 2510-2521). PMLR.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper proposes a new Bayes-based framework for sparse fine-tuning. The proposed method applies a Laplace prior (centered at the pre-trained weights) and a hyper Gamma prior over the scale parameter \lambda of the Laplace prior. The method then performs posterior inference over \lambda to determine whether or not a parameter is ""useful"": A large value of  \lambda indicates a flat prior, i.e. an informative parameter, and vice versa. The paper then adopts SGLD to perform inference over \lambda and then uses human assistance to determine the cut point for \lambda given a budget. The method is evaluated on language and vision tasks and compared against a wide range of parameter efficient fine-tuning methods. The proposed method overall demonstrates performance superior and acquires a sparser model than baseline methods.

## Post rebuttal update: I raised my score from 4 to 5 seeing the new results provided by the author.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is technically sound and well presented.

- The proposed method is evaluated on a wide range of tasks.

- The authors consider a thorough amount of baseline methods.

Weaknesses:
The biggest issue of the proposed method, in my opinion, is the novelty. The use hierarchical prior, which induces sparsity, is a widely known idea since the last century, from Radford Neal and David MacKay's early works to recent works such as [1, 2, 3, 4]. Although these works do not consider the fine-tuning setting, they can technically be applied to the fine-tuning of large pre-trained models by simply letting the prior to be centered at the pre-trained weigh rather than zero. 

In addition, the configuration of SGLD is not presented very clearly. I would suggest the author use formula to describe the modifications to SGLD described from line 212 to line 223, e.g. a modified version of Eq.5 where a few additional hyper-parameters are added to the likelihood and the injected noise term.

*Minor: Some figures are not in vector format and the fonts are too small.

*Minor: It would be interesting to see the performance of the proposed model on *Large* language model.


[1] Masked Bayesian Neural Networks : Theoretical Guarantee and its Posterior Inference
[2] Dropout as a structured shrinkage prior.
[3] Posterior concentration for sparse deep learning.
[4] Bayesian compression for deep learning

Limitations:
The proposed method introduces extra hyper-parameter for SGLD (besides step-size scheduling): Effective data size and noise discount factor.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose an automated sparse fine-tuning method for foundations model, that bypasses the need for human intuition-based heuristics. The neurons to update are revealed during the posterior inference of the sparse scale parameters of a Laplace prior, by thresholding the scale parameters.
The method is experimentally validated in both vision  and NLP tasks.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The proposed method is principled, and relies on a hierarchical Bayesian model.

Posterior approximation is done with Langevin MCMC, which does not introduce a significant computational overhead. 

The experimental results show that the proposed method convincingly improves upon already existing heuristics.

Weaknesses:
I wonder what's the rationale behind using the elbow rule (figure 1) to select the proportion p of parameters to update.

Limitations:
N/A

Rating:
8

Confidence:
4

";1
cGeLeh995N;"REVIEW 
Summary:
This paper studies quantum learning theory, in particular the relationship between the quantum version of PAC learning and the quantum version of the statistical query (QSQ) model, as well as their connection to other considerations in quantum computing, such as entangled measurements and separable measurements. Specifically, the authors proved that

1. For learning Boolean concept classes, the entangled and separable sample complexity are polynomially related (at most quadratic power).

2. There exists a concept class with an exponential separation between quantum PAC learning with classification noise and QSQ learning.

Along this, the authors also proposed novel technical tools of quantum statistical query dimension, and these technical results are applied to problems ranging from states learning, distribution learning, and specific problems in quantum computing such as shadow tomography, error mitigation, etc.

Soundness:
4

Presentation:
2

Contribution:
2

Strengths:
This paper is technically very solid and is able to prove a series of new results in quantum learning theory. PAC learning and SQ learning models are both important concepts in classical learning theory, and it’s nice to see that the authors are able to generalize them to the quantum domain, connect them to natural definitions in quantum, and prove separation results between these concepts. It’s also nice to see that the authors are able to find wide applications in quantum learning theory.

Weaknesses:
From my perspective, the most notable weakness of this works is its interest to the general machine learning community. There are theory papers in NeurIPS each year, many of which are very interesting and well received by NeurIPS audiences, but I’m afraid that this one falls into too much on the theory side and might be of limited interest to the NeurIPS community. In general, the topics of PAC learning and SQ model are more of theoretical interest and target more on theoretical computer science. The quantum version further delves into those directions and bring into definitions that do not exist in classical machine learning, such as entangled measurements/separable measurements, shadow tomography, etc. As another evidence, the references in the main body contain literally 0 paper coming from top-tier machine learning conferences targeting at general audiences, including, NeurIPS, ICML, ICLR, AAAI, etc. Instead, there are many top-tier theory papers at STOC/FOCS, Journal of the ACM, and top-tier physics journals. In general, I believe that this paper can be quite competitive in top-tier theoretical computer science venues or quantum physics venues, but is out of scope for NeurIPS due to lack of insights for up-to-date trends in current machine learning research. 

As a minor point of weaknesses, I think the references can be better presented. First, I find it a bit hard to locate references: there are quite a few big brackets citing >5 papers at the same time, and it’s very difficult to determine which paper talks about which topic. For instance, in Page 1, “There have already been many theoretical proposals for quantum algorithms providing speedups for practically relevant ML tasks such as clustering, recommendation systems, linear algebra, convex optimization, SVMs, kernel-based methods, topological data analysis [34, 14, 9, 42, 32, 26, 35, 23, 49, 46]” can be better written as … such as clustering [xx], recommendation systems [xx], linear algebra [xx], convex optimization [xx], SVMs [xx], kernel-based methods [xx], “and” topological data analysis [xx].

Actually, I think here the authors lost some quantum computing papers accepted by past NeurIPS/ICML conferences that provide quantum speedup for solving machine learning problems, such as Kapoor et al. (https://proceedings.neurips.cc/paper/2016/hash/d47268e9db2e9aa3827bba3afb7ff94a-Abstract.html) and Li et al. (http://proceedings.mlr.press/v97/li19b.html) for classification, Arunachalam and Maity (https://proceedings.mlr.press/v119/arunachalam20a.html) for boosting, Childs et al. (https://proceedings.neurips.cc/paper_files/paper/2022/hash/933e953353c25ec70477ef28e45a2dcc-Abstract-Conference.html) for logconcave sampling, etc.

Limitations:
N/A – this work is purely theoretical.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the power of different quantum machine learning models for learning Boolean functions, namely quantum PAC-learning (QPAC) with entangled measurements, QPAC with separable measurements, and quantum statistical query (QSQ). It has two main results. First, it shows that QPAC with entanglement measurements is not more powerful than separable measurements in this task. More specifically, to *exactly* learn an $n$-bit Boolean function class, every learning algorithm using entangled measurements with $T$ copies of the quantum sample $|\psi_c\rangle=2^{-n/2}\sum_x |x,c(x)\rangle$ can be transformed into a learning algorithm using just separable measurements with O(nT^2) samples. Second, it provides an exponential separation between QPAC with noise and QSQ. In the QSQ model, the learner can query the oracle with any observable $M$ (implementable with $poly(n)$ gates) and obtain an estimate of $tr[M\rho]$ within $1/poly(n)$-additive error. It constructs a concept class of $n$-bit Boolean functions that is QPAC learnable with $\eta$-classification noise (i.e., $|\psi_c\rangle=2^{-n/2}\sum_x \sqrt{1-\eta}|x,c(x)\rangle+\sqrt{\eta}|x,1-c(x)\rangle$) in time $poly(n, 1/(1 − 2\eta))$, whereas every QSQ learner requires $2^{\Omega(n)}$ queries. Furthermore, it also provides several applications in shadow tomography, testing purity of quantum state, error mitigation, and learning output distributions of quantum circuits. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Understanding the power of different quantum learning models is an important research question in quantum learning theory. The results of this paper are quite solid. More specifically, for the first main result about the relation between entangled measurements and separable measurements, prior to this work, we only knew that they are exponentially separated when learning quantum states. This paper focuses on a smaller concept class and shows that they are polynomially related, which is quite surprising. For the second result about measurement statistics, compared to the classical result that separates PAC from SQ due to Blum et al., the construction in this paper is based on degree-2 polynomials, which are more natural. And the proof of this result provides a novel approach to lower bound the complexity of QSQ via the quantum statistical dimension, which is the main technical contribution of this paper. The applications also improve over prior works in several aspects. Furthermore, this paper is well-structured, and most proofs are mathematically sound to me.

Weaknesses:
This paper may seem quite difficult to follow for people not working on quantum computing. Also, the proof overview section is too technical, and more intuition should be given. 

The relation between entangled measurements and separable measurements is $O(nT^2)$, which seems to be not tight. (It has already been pointed out in the paper)

Limitations:
The limitations are stated. Potential negative societal impact does not apply here.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This submission investigates the relationship between learning models with access to entangled measurements, separable measurements, and statistical measurements in the quantum statistical query (QSQ) model. The authors make several notable contributions: they establish the polynomial relationship between the sample complexity of entangled and separable measurements for learning Boolean concept classes, demonstrate an exponential separation between quantum PAC learning with classification noise and QSQ learning, introduce the concept of quantum statistical query dimension (QSD) to provide lower bounds on QSQ learning, prove exponential QSQ lower bounds for various tasks, show an unconditional separation between weak and strong error mitigation, and derive lower bounds for learning distributions in the QSQ model. These contributions advance our understanding of quantum learning theory and have implications for the development of quantum learning algorithms.






Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The paper successfully addresses multiple open questions regarding the capabilities of diverse quantum learning models. The insights gained from the obtained results offer a valuable understanding of the potentials and limitations of near-term quantum machines in comparison to fault-tolerant quantum computers. Additionally, the authors leverage the findings from the QSQ model to enrich our comprehension of crucial aspects in the field of near-term quantum machines, such as error mitigation and distribution learning. The theoretical contributions made in this work play a vital role in advancing quantum learning theory and provide practical guidance for developing and optimizing quantum algorithms on both current and future quantum hardware.






Weaknesses:
According to the whole main text, I did not find the main weakness of the submission While I did not review the entire proof, the portion I examined did not reveal any apparent issues. However, I did notice some minor concerns such as typos and incorrect notations. I suggest that the authors thoroughly examine their work to address these issues, ensuring a self-consistent and easily understandable manuscript. For instance:
1. Line 346, 'a important'
2. Line 375, '?, (ii)'
3. Line 41, Supplementary: '$\sum_{x\in D_0}$'
4. Line 44, Supplementary: '$I(D_0(x)\leq D(x))$'
5. Line 123, Supplementary: 'a n-bit'
6. Line 602, Supplementary: 'Theorem ??'

Limitations:
The authors have acknowledged the limitation by discussing open questions in the Discussion section.






Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper considers the task of learning an unknown concept class with quantum accesses. In particular, the authors make a comprehensive comparison of the settings with entangled measurements, separable measurements, and statistical measurements in the quantum statistical query (QSQ) model, respectively, and showed that entangled measurements are at most polynomially more powerful than separated measurements, which are exponentially more powerful than QSQ learning. Notably, this second separation is a quantum analog of the classical result separating between classical SQ learning from classical PAC learning. The authors also discuss possible extensions and applications of their result.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
This paper provides a clear answer on the possible role of entanglement the quantum PAC learning setting, and establishes a distinct separation between quantum PAC learning and QSQ learning, which provides an interesting conceptual message. The technical contribution of this paper is solid, and the presentation is clear.

Weaknesses:
The two main results of this paper are not closely related from my perspective. 

Limitations:
N/A

Rating:
6

Confidence:
3

";1
y50AnAbKp1;"REVIEW 
Summary:
The paper studies the problem of noisy label learning. The paper adopts an optimal transport approach to generate pseudo labels for noisy samples. Particularly, the paper builds on the existing method and adds additional regularization terms to enforce the consistency between sample classes and learned representations. The paper also extends the sinkhorn algorithm to solve the proposed OT objective efficiently. Empirically, the proposed method has improved performance over baselines on widely used datasets with various noisy ratios.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper extends the existing optimal transport approach to include the consistency between the sample representations and predictions/labels. It is a novel objective and a solid idea intuitively.

2. The paper extends the sinkhorn algorithm to solve the proposed new OT objective efficiently.

3. The proposed method has strong empirical performance, especially for high noise ratios.

4. As a pseudo-labeling step, the proposed method can potentially work with other noisy label learning objectives.

Weaknesses:
1. The proposed method is a regularization of the existing optimal transport pseudo-labeling method. The novelty is thus limited.

2. The introduced regularization uses the same weight kappa for the two terms, while the two terms could have quite different behaviors/values.

3. It is not clear to me how does the method perform or should be modified in the case where the class distribution is imbalanced.

4. The OT objective is not directly related to the training objective but serves as a sieving step. It would be great if a more comprehensive training objective can be formalized to include the OT based selection.

Limitations:
Yes.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper introduces a novel formulation of Optimal Transport (OT), named Curriculum and Structure-Aware Optimal Transport, for generating pseudo labels by considering both inter- and intra-distribution structures of samples. Moreover, to efficiently estimate the distribution's structure, the authors adopt a curriculum paradigm to progressively train the proposed denoising and relabeling allocator. Additionally, they present a computation method for the proposed CSOT that ensures faster processing speeds, reducing computational overhead. Experimentally, this paper achieved SOTA performance on various benchmarks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.Estimating the intra- and inter-structure coherence of samples is a convincing and reliable method for improving relabeling accuracy. The proposed prediction-level and label-level consistency constraints seem also interesting and plausible.
2.The combination of the proposed OT and curriculum learning for solving INL is also smooth and makes sense.

Weaknesses:
1.The effectiveness for alignment of global and local structures between samples and classes is not fully convincing. The ablation results for OT in Table 3 seem weak. The comparison between row (a) with 78.07 and row (b) with 78.65 suggests that the performance improvement brought by the proposed prediction-level and label-level constraints is limited. Moreover, the introduction of two additional constraints adds complexity to the optimization. Similarly, row (e), CSOT w/o Ω^{L}, achieves the best performance, indicating that the benefits brought by the prediction-level and label-level constraints are unstable.


2.As the part that readers are most concerned about, the section 4.3, the loss function needs to be able to reflect the integrity of the method and the specific combination with its own innovation points. I can’t see the innovation of this article in the loss function here, and each loss term is an existing work. The work in this paper seems to be only used to build a dataset $\mathcal{D}_{\text{clean}}$ and  $\mathcal{D}_{\text{corrupted}}$ for training? This structure and method of writing can greatly weaken the contribution of this paper. Besides, I would like to suggest the authors provide an overall alghrothim to show the whole training process, where the proposed methods would have been used during each training epoch and the training objectives are not the key point in this paper.

3.Considering the complexity of the proposed algorithm, and its marginal improvement over previous methods on two real datasets in Table 2, especially compared to NCE. The effectiveness of this work is questionable. Additional experiments are suggested, especially on Clothing1M. Besides, some related works[1] should be discussed which are published recently.  

 [1] OT-Filter: An Optimal Transport Filter for Learning with Noisy Labels (CVPR 2023)

4.The results of row (g) in Table 3 are not sufficient to show that the performance improvement is brought about by the method in this paper. For example, we need a more detailed ablation study to explain the role of NCE loss and the role of CSOT.

5.The structure of the article is confusing, which weakens the contribution of this article. Secondly, the introduction of some tool concepts is quite abrupt. For example, the proposal of Eq. (3) and (6) gives people a new form of OT formulation and will give specific solutions later. This expectation is affected by the entropy regularization in the following text, whose introduction is abrupt. This results in the final Eq. (16) which does not look significantly different from the original sinkhorn algorithm. It is recommended to introduce sinkhorn from the beginning and to emphasize that there are new constraints based on it.

6.The characters in the figures in the experimental part are too small, which affects reading. At the same time, why are there two figures, and their labels are figure 2? Moreover, Fig. 4 is quoted in the description of the text, but there is no figure 4 in fact.

Limitations:
The authors describe its limitations.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces CSOT, an approach to address the challenge of noisy labels in machine learning models. CSOT incorporates optimal transport formulation to assign reliable labels during training, considering the structure of the sample distribution. The authors also propose an efficient computational method for solving CSOT. Experimental results demonstrate the superior performance of CSOT compared to existing methods for learning with noisy labels.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The paper is strongly motivated by theoretical analysis, particularly optimal transport analysis.
- The writing style is clear and easy to follow.
- CSOT exhibits superior performance when compared to previous algorithms.

Weaknesses:
- The paper lacks a comparison with a baseline algorithm called UNICON [1], which has shown good performance in highly noisy scenarios (e.g., 0.9 noisy ratio). It would be valuable for the authors to include a performance comparison with UNICON.
- The authors do not analyze the case of instance-wise noisy labels, which is a prevalent type of noisy label model. Including an analysis of this case would be beneficial.
- The paper does not investigate the sensitivity of hyperparameters, which are required to run the algorithm. It would be valuable for the authors to perform a hyperparameter sensitivity analysis.
- To enable a comprehensive comparison, the authors should report both the best and last performances of the model, as models trained on noisy labels tend to memorize the noisy labels.

[1] UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning

Minor)
The legend size in Figure 2 is too small to read.

Limitations:
The limitations of this paper are summarized in the ""Question"" and ""Weakness"" sections
------
(Raise score from 5 to 6 after rebuttal)

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel optimal transport formulation, called Curriculum and Structure-aware Optimal Transport (CSOT) for learning with noisy labels. CSOT considers both the inter- and intra-distribution structure of the samples to construct a robust denoising and relabeling allocator. It’s worthing noting that Notably, CSOT is a new OT formulation with a nonconvex objective function and curriculum constraints. The authors developed a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework to solve CSOT efficiently. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. Novel Approach: The paper proposes a novel optimal transport (OT) formulation called Curriculum and Structure-aware Optimal Transport (CSOT) to address the challenge of learning with noisy labels. I believe this paper introduces a new perspective and potentially brings fresh insights to the field.

2. Consideration of Global and Local Structure: Unlike current approaches that evaluate each sample independently, CSOT concurrently considers the inter- and intra-distribution structure of the samples. This consideration of both global and local structure helps construct a more robust denoising and relabeling allocator, potentially leading to improved performance.

3. Incremental Assignment of Reliable Labels: CSOT incrementally assigns reliable labels to a fraction of the samples with the highest confidence during the training process. This approach ensures that the assigned labels have both global discriminability and local coherence, which could contribute to better generalization and reduced overfitting.

4. This paper provides a very detailed derivation for the lightspeed computational method. 

Weaknesses:
Researchers or practitioners interested in using CSOT may need to invest additional effort in adapting or developing specialized solvers. 

Limitations:
CSOT is described as having a nonconvex objective function. Nonconvex optimization problems can be challenging to solve, and they may have multiple local optima, making it difficult to guarantee finding the global optimum. This could potentially impact the reliability and efficiency of the proposed method. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a new noisy label learning approach based on Optimal Transport (OT) and Pseudo-Labeling (PL). Specifically, the authors extent OT-based PL with the consideration of the intrinsic coherence structure of sample distribution. Consequently, this paper proposes a novel optimal transport formulation, namely Curriculum and Structure-aware Optimal Transport (CSOT), which constructs a robust denoising and relabeling allocator that mitigates error accumulation. Experiments on both controlled and real noisy label datasets show the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper proposes a method named Curriculum and Structure-aware Optimal Transport (CSOT) to address the problem of noisy label learning, and the application of OT-based pseudo-labeling in tackling noisy label learning problem has not been thoroughly investigated.

2. The experimental results on different datasets in this paper validate the effectiveness of the proposed method. Additionally, several ablation experiments are conducted to demonstrate the effectiveness of each module in the method.

Weaknesses:
1. In terms of methodological novelty, OT-based PL has been previously applied to other problems, and this paper only applies it to the specific problem namely noisy label learning rather than introducing it for the first time. Additionally, employing curriculum learning to address the issue of pseudo-labeling is a common strategy in the field of weakly supervised learning.

2. The utilization of SOT is a key contribution of this paper. However, the current motivation behind this aspect, as presented in Figure 1, is not sufficiently clear. The authors are encouraged to provide additional descriptions in this section to enhance the clarity and understanding of the motivation.

3. Since the differences between the proposed method and the comparison methods in several cases is too small, it is hard to provide a clear comparison without the present of standard deviations. Additionally, the backbone and other parameter settings of the SOTA methods are not clearly listed, thus, further evidence is needed to establish the fairness of the comparisons.


Limitations:
N/A.

Rating:
5

Confidence:
3

";1
yjWVd8Fhqt;"REVIEW 
Summary:
The paper studies the problem of object-centric image editing. The authors first curate a dataset based on Objaverse by selecting high-quality textured samples, and then simulate+render them on a plane. The objects can be manipulated in 3D and rendered correspondingly, which generates the groundtruth for training learning-based object editing models. The paper further presents a diffusion-based object editing model (3DIT) based on zero-123, where the major difference is the addition of editing prompt conditioning. Results in quantitative and qualitative experiments show 3DIT outperforms baselines based on foundation models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The dataset curation using 3D simulation for the object-centric image editing task makes sense. This task intrinsically requires understanding of the 3D world and 3D-aware image formation process. The proposed dataset is guaranteed to be 3D-correct, and would be useful for research along this direction. The proposed method also achieved great performance compared to the baselines. Besides, the paper is well-written and easy to follow.

Weaknesses:
1. The realism of the generated dataset is still limited: 
a) The single directional light (why not multiple lights or Image-based lighting?) which makes the shadow and shading's distribution not diverse/realistic enough.
b) The size range of the object is quite limited (0.8 ratio threshold), in practice, there are a lot of cases where different sized objects are placed nearby (also the true size seems not kept, so chairs and lamps appears to be of similar scale).
The authors indeed show some qualitative results of direct transfer to real data, but no 1) quantitative evaluation of sim2real or 2) comparison to baselines' generalization have been provided.

2. Although the data is generated with 3D, the method follows a 2D design. Such a model does not exhibit a great understanding of 3D objectness -- for example, the identity of the object is sometimes not well-kept during rotation or translation (e.g. the change of coke/headphone texture in Fig 1), or the cast shadow or the shading do not make full sense w.r.t. the whole scene if we look close enough (e.g. first 3 rows of clevr in Fig 1). Having some systematic analysis of the failure cases here would have been very helpful for future research.

Minor questions/comments:
1. Why is the background always black? This seems to be creating domain gap, as usually we don't have pure black background in real life.
2. Wrong number highlight in Table 1, 3DIT(Multitask) - insertion - LPIP, 0.585 is worse than the other two.
3. The multi-task model is much worse in terms of FID (Table 1), but not other metrics. Are there any specific reason for this?


Limitations:
More analysis on failure cases and limitations will be beneficial.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper constructs a dataset containing 400K examples, which is used for the task of language-guided 3D-aware image editing. This paper also proposes a model, named 3DIT, to solve this task. The model is based on 2D diffusion model, which first goes through the pre-training of text-to-image generation and Zero-1-to-3, and then is fine-tuned on the dataset for the 3D-aware image editing task. The model is evaluated on the proposed dataset and achieves state-of-the-art performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper constructs a large-scale dataset, which is a good contribution. And language-guided 3D-aware image editing is an important task.
2. The paper is well written and easy to understand.
3. The experiments show that the proposed model trained on the proposed dataset have a reasonable ability on the task of language-guided 3D-aware image editing.

Weaknesses:
My biggest concern is that the performance of the proposed model is not very good.

1. The GIF results in the supplementary material exhibit incorrect results on the shadow.
2. The lighting and shadow of experimental results on real-data is not realistic. The quality of edited images degrade a lot.

Due to the poor performance, I am not sure if the quality and diversity of the proposed dataset is good enough.

The illumination and background of presented real data is too simple. It would be great to add more challenging test data to see the upper bound of the proposed model.

Limitations:
The authors have discussed the limitations.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper formulates a task of 3D aware editing using the language guidance. The task aims to insert, remove, translate or rotate objects in a scene (2D images) by maintaining the details like shadows, 3D consistency of the object, changes in the object sizes due to perspective projections etc. The model is based on Stable diffusion, Zero 1-to-3 method and fine tuning on the given dataset having editing information and text to describe the edit. The paper promises to release the dataset OBJECT derived from Objaverse, which the authors use to train their model on.  The results in the teaser figure and others show that the model is able to perform the given edits, while preserving the semantics of the image. For examples, the objects are translated and it respects the perspective projection, shadows and placement on the surface. The authors also claim that the method is generalizable to the real images.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The paper is able to show that the manipulations possible with the method can preserve the 3D properties of the scene including localization of the objects, scaling, shadows and consistency of the inpainted regions. 

2) The paper compares with the image based baselines, for examples uses SAM to segment the images and translates the objects in the scene. It also compares with the a 3D baseline for rotation using Zero 1-to-3.

3) The paper shows quantitative and qualitative results of their method comparing scores such as PSNR, SSIM, LIPIPS and FID between the original and edited images. The authors also conduct a user study to access the quality of lighting and geometry.

Weaknesses:
1) The method to train on the given dataset is not clear to me. There is no pipeline figure to explain the stages of the training. The first two steps are previous works. The contribution which is in the third step is not explained properly in the paper. Is this fine-tuning stage similar to Zero-1-to-3? How was the editing sequence fed to the network? How is it 3D aware besides the Zero-1-to-3 training? Is the method's 3D consistency (for example in rotation) upper-bounded by Zero-1-to-3?

2) While the images shown in the paper show that some properties are preserved as the editing operations are done, the results in the gifs show some obvious flickering artifacts which do not respect the properties like shadows. This does not go well with the objective of the paper. Besides the problems with Stable Diffusion , where do the problems arise?

3) How is the quality drop if number of sequential operations are done? For example once can perform insertion-> rotation ->translation etc for the same/ different objects in the scene. How does the quality drop compare with the Stable diffusion image editing methods? This is more interesting to me. A 3D aware editing  framework should be able to handle multiple sequential edits with consistent results.

4) Another baseline would to use a monocular depth estimation model (eg Zoedepth, Midas) to extract the surface and perform the edits using Zero-1-to-3. This can handle the perspective projection of the objects, and/or even lighting and shadows. Did the authors try similar more strong baselines? How do they compare with the current method?

5) The paper claims to generalize to the real scenes. This is a significant claim and needs to be evaluated. The issue of synthetic and real image domain gap is an active research area. How does the current method solve that in this particular task? Were real images considered in Table 2?

Limitations:
The authors do not discuss the limitations of their work in detail. Please add a detailed section on where the method fails.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a large dataset of  3D aware image edits along with editing instructions built on the objaverse dataset. They also introduce a model finetuned on Zero-1-to-3 for 3D aware editing tasks which include object insertion, removal, translation and rotation. Comparisons are provided against state of the art models for each task and performance improvement is demonstrated. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. **Clarity**: The paper is well written with attention to detail. All the necessary details particularly with regards to the dataset creation have been adequately explained.
2. **Interesting dataset**: The 400k dataset of images along with edit instructions would serve as an interesting training and benchmarking dataset for the task of 3D aware editing. 
3. **Quantitative metrics**: A number of qualitative comparisons and user studies are provided to demonstrate the geometric consistency of the edits and lighting consistency.


Weaknesses:
1. **Novelty**: Although the proposed dataset represents an important contribution, the proposed approach relies on zero-1-to-3 and finetuning on a new dataset.
2. **Need for zero-1-to-3**: The approach finetunes a model on top of zero-1-to-3 to incorporate edit instructions. Can the finetuning be done on top of base SD?. Adding an ablation to this effect will be helpful to demonstrate the need for the 3 stage curriculum. 
3. **Related work**: Several related work that may provide important context are missing. The authors might find some of the following works relevant and interesting [1,2,3,4]. Although some of these works are pre-prints and do not warrant strict comparisons, incorporating them into the related work section would place the proposed work appropriately w.r.t the landscape of current literature 
4. **Changes in the edited image** : There are certain global changes in the edited image, that dilutes some of the claims w.r.t editing. Particularly, for the CLEVR dataset, the provided supplm examples show changes in the color of certain objects upon insertion/ removal


[1] ControlNet
[2] InstructPix2Pix
[3] InstantBooth
[4] GLIGEN

Limitations:
Adequate treatment of limitations have been provided.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The 3DIT model is a language-guided 3D-aware image editing tool that allows for effective object editing while considering scale, viewpoint, lighting, and object occlusions. The model builds upon previous work in scene rearrangement and image generation, and incorporates a diffusion process to render object transformations. The authors conducted human preference evaluations to measure geometric and lighting consistency, and found that 3DIT outperformed relevant baselines in both categories.

One of the key strengths of 3DIT is its ability to add, remove, or edit shadows to maintain consistency with scene lighting. This is achieved through a shadow generation module that takes into account the position and orientation of the light source, as well as the geometry of the objects in the scene. Additionally, 3DIT accounts for object occlusions by using a novel occlusion-aware rendering module that predicts the visibility of each object in the scene.

The authors also introduced a new benchmark dataset called OBJECT, which consists of 3D scenes with multiple objects and associated natural language descriptions. They trained 3DIT on this dataset and found that it generalized well to images in the CLEVR dataset as well as the real world. This demonstrates the robustness and versatility of the model, and suggests that it could be applied to a wide range of real-world scenarios. Overall, by enabling users to edit objects in a natural and intuitive way, 3DIT opens up new possibilities for creative expression and visual communication.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The 3DIT model is a novel approach to language-guided 3D-aware image editing that builds upon previous work in scene rearrangement and image generation. The model incorporates a diffusion process to render object transformations and uses a novel shadow generation module and occlusion-aware rendering module to maintain consistency with scene lighting and object occlusions.

- The authors conducted human preference evaluations to measure geometric and lighting consistency, and found that 3DIT outperformed relevant baselines in both categories. This demonstrates the effectiveness of the model in producing high-quality, visually consistent image edits.

- The authors trained 3DIT on a new benchmark dataset called OBJECT and found that it generalized well to images in the CLEVR dataset as well as the real world. This suggests that the model is robust and versatile, and could be applied to a wide range of real-world scenarios.

- The potential applications for this technology are vast, including virtual and augmented reality, gaming, and e-commerce. The model could be used to create personalized avatars for virtual reality environments, or to generate realistic product images for e-commerce websites. Additionally, the model could be extended to support more complex scenes and interactions, such as object physics and collision detection.


Weaknesses:
- The ablation study is relatively weak. It's unknown which component contributes most to the final performance and which is effective.

- The shadow of the box did not follow the rotation action in the shown GIF. And some artifacts are obvious.

- How did the method choose which box to be moved? Or did it need a handcrafted mask as the selection?

- What is the number of samples used for calculating FID? Normally FID is not reliable of the number of samples is small.

- The comparing table did not include the previous method. I believe several important baselines [a] methods are missing.

References:
[a] Editable free-viewpoint video using a layered neural representation

Limitations:
See above

Rating:
5

Confidence:
4

";1
uRewSnLJAa;"REVIEW 
Summary:
The authors propose a self-supervised method to learn approximate multi-step Q-estimates by leveraging random features as bases for a reward function in the target task. This approach addresses some limitations of model-based and model-free RL. In particular, model-based RL typically suffers from compounding error of predicting future states from predicted states. On the other hand, model-free RL tends to suffer from instability when transferring knowledge. The proposed method mitigates these problems by implicitly learning transition dynamics (without explicitly modeling future trajectories) from offline data in a self-supervised manner based on the prediction of random features.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The methodology is interesting. Using random features in a self-supervised manner to form a Q-basis such that one can find a linear combination of these random features for the reward function of a downstream task using approximate multi-step returns makes sense and seems novel.

Weaknesses:
The main weakness of this approach lies in the experiments and poor framing of how this work fits in the context of meta-RL. meta-RL is not mentioned, but it entails this same problem of having multiple tasks with shared dynamics such that the reward function changes from one task to another. As such, the authors should be comparing against meta-RL benchmarks, not standard RL. It is interesting because the authors are running their experiments on a popular meta-RL benchmark called Meta-World and yet they do not compare their approach against meta-RL approaches. One of the reasons why this is currently an unfair comparison is the need for the competing approaches to learn their value functions, etc. from scratch, whereas the proposed approach had the benefit of learning from an offline dataset befrorehand and adapting to the target task.

Limitations:
The limitations have been addressed.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces RaMP, an approach for fast adaptation to unseen reward functions given offline data collected with arbitrary behavior policies under the same dynamics. RaMP learns a set of basis multi-step Q-functions, each corresponding to a random reward defined as the accumulation of random state action features. For online adaption to new rewards, the new reward/Q function is then identified using linear regression given the basis functions and used for control via MPC. 

**Edit After Rebuttal**:

Raised score from 3 to 5, see comment below for details

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
RaMPs' key idea, learning ""random feature rewards"" which are later combined, is novel and interesting as it allows offline pertaining without ""reward labels"" and provides an easy and efficient adaptation mechanism. The paper addresses a significant problem, the generalization to novel rewards under little to no assumptions on the knowledge of the current scenario and previously seen rewards, and is mostly clearly written and easy to follow. 

Weaknesses:
The paper's main weakness is its experimental evaluation, which I think is in its current form insufficient to validate the paper's claims or allow assessment of the method's potential for several reasons: 

- The algorithm described in the main paper seems to be improved by several ""implementation details"" (c.f. Appendix B). While the paper states that ""RaMP’s performance already surpasses the baselines even without them"", I believe a rigorous analysis of their impact would be necessary to assess the method's potential and workings.
-  Baselines: none of the considered model-based RL methods (PeTS, MBPO, Dreamer) was designed to work with dynamics models trained on **offline** data. For a fairer comparison, methods designed to work with such dynamics models (e.g.  MOPO[1]) should be considered. 
- While RaMP adapts very fast in the considered environments, its final performance is often lower than that of MBPO and/or PeTS, in particular for the Hopper and D'Claw experiments. Further discussion and analysis in the paper would allow a better understanding of RaMPs limitations.
- Compared with most recent work in RL and/or offline RL the paper only uses a small set (8) of relatively simple environments - the maximal state dimension is 39 (meta world, where only a subset of these is relevant for the considered tasks) and maximal action dimension is 9. 
- Higher dimensional observations (pixels) are used for some experiments in the supplement but the results are again inconclusive or seem to favor the baseline (Dreamer-V1, which is to the best of my knowledge not SOTA on pixel metaworld [2]) 
- See questions below. 

While the majority of the paper is clearly written and understandable, this is, in my opinion, not the case for section 3.4. Here the assumptions, their realisticness, and practical consequences of theorem 3.1 could be stated much more clearly 

[1] MOPO: Model-based Offline Policy Optimization, Yu et al 2020

[2] Masked World Models for Visual Control, Seo et al 2022


Limitations:
I believe all limitations have been addressed, although some only in the supplement (infinite horizon problematic with multi-step q function), and I believe the paper would benefit from moving this also to the main part.
 

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes an approach for unsupervised pre-training of RL agents, ie pre-training on offline agent experience without rewards. The approach generates a set of random reward functions and then learns a successor representation of the state-action space for predicting cumulatives of these rewards on fixed-horizon trajectories from the pre-training data. At test time, it uses a small reward-annotated dataset to learn a linear mapping from the pre-trained successor representation to the target reward. Then it extracts a policy by greedily maximizing the resulting estimates of future cumulative rewards. The paper demonstrates that this allows for faster finetuning than model-based RL since only a linear mapping needs to be learned on the target task data instead of the full Q-function. In contrast to prior work on successor features it does not assume access to given features or a reward-annotated training set to learn features, but instead uses random reward projections.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- the problem formulation is impactful: pre-training with reward-free data has great potential for generalist RL agents

- the paper is well-written and easy to follow, it clearly outlines the problem formulation, explains the method and provides theoretical justifications

- the experimental evaluation seems comprehensive, with only minor experiments missing (see below): the paper compares to a representative set of baselines on multiple environments, including image-based control (though there with limited success) and also performs ablations of the key elements of the method



Weaknesses:
(A) **Relation to prior work not sufficiently explained**: the proposed approach heavily builds on top of prior work on success features for RL, yet the current submission only mentions this relation in a half-paragraph in the main paper and adds a slightly more detailed discussion at the very end of the appendix. It would be helpful to more clearly introduce this most relevant prior work and explain the deltas, so it is easier for readers to understand the main novel components. Concretely, one could add a preliminaries section that summarizes the idea of successor features and then point out the two main novelties in the proposed approach: random reward projections to be independent of training tasks and action chunking to be independent of training policies.

(B) **Random reward features don't scale well to images**: the main difference to prior successor feature works is that the proposed approach avoids the assumption of pre-defined features or given pre-training task distributions by using random reward projections. The downside of this is that the algorithm cannot ""discard"" any information and struggles on high-dimensional inputs like images, where predicting random rewards over pixel inputs is complex and lacks semantic meaning. I agree with the authors that this can potentially be mitigated with pre-trained representations, but since removing the assumption of pre-defined features is one of the main selling points of this work, this tradeoff should be discussed more prominently in the main paper (currently only in appendix C.3) and the corresponding image-based results should be included in the main text. It would also be helpful to show the successor feature baseline results on the image-based domain.

(C) **Missing Ablation**: The other introduced novelty is the use of action chunking, ie conditioning the Q-function on a sequence of H actions instead of on a single action. However, this choice is not ablated. It would be good to include versions of the proposed approach without action chunking (ie H = 1) and with different horizon lengths to see the benefit of chunking.

(D) **Missing baseline**: The paper compares to goal-conditioned offline RL (CQL) but with a pre-specified selection of goals. Prior work has instead proposed to use goal-conditioned offline RL on randomly sampled goal states from an offline dataset for pre-training [1]. This has two benefits: (1) it does not require pre-defined goals / tasks, fully matching the assumptions of the proposed approach, (2) because of that it may be more robust to test time adaptation to new tasks. Thus, it would be good to add comparison to Actionable Models on all environments.

**Minor Point**:

(E) **MPC is misnomer**: The paper refers to the downstream policy extraction as ""model predictive control"" and ""planning"" (see eg Fig 2). However it does not perform prediction or use a model. Instead it performs greedy policy extraction by maximizing the Q-function, but uses action chunking (ie N-step action inputs). Thus I would call this step ""greedy policy extraction with action chunking"" instead.

[1] Actionable Models, Chebotar et al. 2021


## Summary of Review

Overall I like the paper and am happy to accept it if the authors can more clearly mention the connections to prior work on successor features, discuss tradeoffs for image-based domains and add the suggested ablation + baselines. The idea of using random reward projections for successor feature learning is novel to my knowledge, but I am not 100% certain and will thus assign lower confidence to my review.




Limitations:
Limitations on image-based environments are demonstrated in the appendix but should be discussed more prominently in the main paper (see weaknesses (B)).

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper tackles the problem of 

In the training phase, they create $H$ randomly initialized neural networks that serve as reward functions during the purpose of pretraining. The Q-function learned online is a linear combination of these random features: $w^T\sum_{h\in [H]}\phi(s_h,a_h)$.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
This is a beautifully written paper. The motivation is crisp and the proposed method is very well grounded theoretically. Results across a broad range of tasks are presented. On originality, clarity, and method quality, I rate this paper highly.

Weaknesses:
The paper would benefit from more baselines: for example doing something like  IQL with a reward of 1 at the end of each trajectory during offline training. Less importantly, I also think IQL would be a more natural oracle than CQL.  It performs better than D4RL and directly evaluates online fine-tuning. In fact, the IQL paper shows that CQL can sometimes collapse during online fine-tuning.

I also don't quite understand the choice of offline dataset: from the appendix, it sounds like the policies that generate the offline dataset are quite ""expert."" It would be interesting to see how well the model performs if the offline dataset was instead modified from the D4RL benchmark, which includes different levels of expert policies. I expect high quality data to benefit any self-supervised method, but the claims of the paper are not limited to offline learning on expert demonstrations.

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper proposes a Random Features for Model-Free Planning (RaMP) algorithm to solve the problem of learning generalist agents that are able to transfer across platform where the environment dynamics are shared, but reward function is changing. The proposed algorithm leverages diverse unlabeled offline data to learn models of long horizon dynamics behavior, while being able to naturally transfer across tasks with different reward function. The authors evaluated RaMP on number of simulation based robotic manipulation and locomotions tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The problem addressed in the paper is very important problem for RL in robotics/robot learning real-life tasks.
- The proposed method is very interesting and seems relevant to the research community.

Weaknesses:
- The paper is not well-written and very hard to follow. For example, Line 13-15 and Line 59-61 is very hard to follow.
- The paper attempts to solve an interesting problem and has strong results in simulation based robotics task. Since, the motivation of the paper draws from real-robotic tasks, it would be good to see how this method performs on real-robotic tasks. I understand authors have mentioned in future work but to understand complete effectiveness of the proposed method, it seems critical. 
-

Limitations:
mentioned in the paper.

Rating:
4

Confidence:
4

";1
6lgugutkin;"REVIEW 
Summary:
This paper proposes a new approach for fine-tuning language models (LMs) with RL in order to achieve a good trade-off between maximizing reward and minimizing drift from the initial model, which is typically undesirable since it results in losing some capabilities while acquiring others. The approach consists in resetting the model to an exponentially moving average (EMA) of itself, while also resetting the EMA model to the initial one. The method is simple and demonstrates good performance on three different settings with varying degrees of complexity, including RLHF on top of LLaMA-7B for a QA task.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed approach is simple and effective. 

2. The evaluation is quite thorough considering multiple settings and thus demonstrating the generality of the approach. 

3. The paper is well written and the problem setting is clearly stated. 

4. The problem setting is of high importance to the community. The authors point out a major limitation with existing RLHF methods and propose a method for improving upon it. 


Weaknesses:
1. The main limitation of this study is the evaluation protocol used to assess the results. The model's performance is evaluated by the same reward model used to train it. However, it is well known that LMs fine-tuned with RL(HF) are prone to overoptimizing their reward (models), overfitting and actually performing badly when evaluated by humans (which is a more robust metric and ultimately what we care about for many applications). While I understand that performing human evaluations can be expensive, it is very difficult to assess the validity of these results otherwise. It could be the case that Elastic Reset is a more powerful optimization approach that overoptimizes the reward better than PPO i.e. can obtain high reward during training but this performance doesn't actually transfer well to unseen prompts when evaluated by humans. At the very least, I suggest using a different reward model for evaluation such as a different base model (of similar size) trained on the same data or the same model trained on a different dataset such as the summarization data from [1] or the HHH dataset from [2]. You could also hold out part of your data and train a separate reward model with a different base on it in order to bring it more in-domain.

2. Can you include experiments with varying reset intervals for the all tasks? It's important to know how sensitive the model is to this parameter and better understand if similar / same values work across different tasks. Do you have any suggestions or insights for selecting this hyperparameter for new tasks?

3. It would also be interesting to see how the results change if only some of the parameters are reset e.g. the last few layers as it typically done when fine-tuning LLMs. 

References:
[1]. Stiennon et al. 2020, Learning to summarize from human feedback. 

[2]. Bai et al. 2022, Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.

Limitations:
Yes, the authors clearly state the limitations of their study at he end of the paper. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper aims to address the problem of language drift issue during RLHF (reinforcement learning with human feedback), which is also known as alignment tax and reward hacking. The problem is that during RLHF process, the model can ""overfit"" to the given suboptimal rewards while forgetting some important skills such as linguistic capabilities. The authors proposed a method named Elastic Reset that periodically reset the online model with an exponentially moving average (EMA) of its previous checkpoints. The authors use a few tasks to showcase the effectiveness of the proposed Elastic Reset method on top of GPT-2 and LLaMA-7B, and shows that it is better than vanilla PPO with a reduced KL penalty.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method is rather simple and easy to implement. It just needs to save the parameters of previous checkpoints during training, so that every $n$ steps, one can compute an EMA and merge it to the current model. 

The empirical results suggest that the proposed method is effective on the selected tasks and datasets, outperforming the simple PPO and NLPO baseline methods. 

Weaknesses:
- The method itself is not particularly novel. Using Reset mechanism and EMA of model parameters to mitigate overfitting is rather common, although it might be a novel application in RLHF. 

- The ""drift"" problem is not very clearly defined and formulated. The motivation is not well justified. Many descriptions are references while there is no concrete experiments and case studies to show the problem of drifting clearly. 

- I believe the key issue that this paper wants to address is essentially the same to many continual learning problems -- learning new knowledge while not forgetting the acquired skills. Therefore, many CL methods such as experience replay, regularization, EWC (Elastic weight consolidation), should also be applicable. But none of them is mentioned in the paper. The authors focused too much on the RLHF literature, using newly invented terms, however, ignored that the key challenge can be formulated with existing problem setup and can be addressed by existing techniques.  

- The selected tasks and datasets are quite narrowed. The experiments on GPT-2 and even smaller models are also not that convincing in that RLHF is rarely used on such LMs. I suggest authors can replace those experiments with more commonly used datasets to support the claims. 


Limitations:
The authors use Section 8 to describe the limitations. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes Elastic Reset, a simple technique for countering language drift and reward model overfitting when optimizing a language model policy against some communicative reward via reinforcement learning, as is done in RLHF. The idea of elastic reset is to periodically reset the trained model to an exponentialy-weighted moving average (EMA) between the initial pretrained model and the online model trained since the last reset. 

The authors demonstrate on a variety of RLHF tasks from relatively simple (pivot translation) to relatively substnatial (StackExchange QA) that Elastic Reset seems to be a simple way to mitigate language drift which outperforms other approaches, including the commonly-used KL penalty in RLHF. They propose a nice way of interpreting the tradeoff by using a ""pareto frontier graph"", which plots methods' downstream task reward and measure of language drift on x/y axes, similarly to ROC curves, and demonstrate that for some given compute budget, Elastic Reset dominates existing baselines like REINFORCE and PPO with KL penalties.

Overall there's a lot I like about this paper, but there are some issues I have with the experimental setup that prevent me from unconditionally recommending the paper for acceptance. If my concerns are clarified or resolved, I am willing to update my score and look forward to the author response.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- A simple technique that appears to give gains across a variety of RLHF benchmarks at varying scales (from translation to stackexchange Q/A). I am impressed by the breadth of experiments in this paper. It seems this could be one of those simple tricks that practitioners find useful and widely deploy in future RLHF pipelines (but time will tell whether the results are robust enough).
- Good comparison to other sensible baselines, with some sensible ablations on the IMDB mock sentiment task, but some areas for improvement here (see Weaknesses)
- I like the Pareto figures, reminiscent of ROC curves, which demonstrate the tradeoff between task performance and language drift and how to identify an optimal method under a given practitioner's constraints. I agree with authors that this is the right way to think about language drift, though I have some points of confusion (see firsrt Weakness)

Weaknesses:
## Unclear how robust elastic reset under different compute constraints

- The IMDB and StackLLaMA experiments demonstrate something subtly worrying in my view: they show that there are portions during training where elastic reset *doesn't* help over comparable baselines. For example, in Figure 4 a/b, training prior to the first two resets demonstrate a ""spiking"" behavior where the model overfits to the reward and has higher language drift than the other two methods. It is only after the 2nd reset, untnil the end of training, that the reset causes task reward to rise higher than existing methods *but*.
	- This feels ""lucky"" to me, and it is hard to measure how robust elastic reset is under different compute constraints. For example, lets say we only have enough compute to train the model up to (but not after) the 2nd reset, i.e. epoch ~33 in Figure 4. It seems like in this case we would *not* prefer to use the elastic reset model, since it seems to still be in the regime of overfitting, and it is only after the 2nd reset that things start to look better.
- A similar concern exists for StackLLaMA where there is not a significant difference between Elastic reset and PPO, until around ~500-600 epochs, when there suddenly is (and perhaps there's even a regime in 400-500 where Elastic Reset is overfitting).
- To address this concern, it seems like the pareto graphs, and in general the performance of elastic reset, need some notion of compute budget to more accurately assess when it is appropriate to use elastic reset. For example, what does the pareto frontier graph look like if we only consider training up to about epoch 33 in Figure 4? (i.e. before the 2nd reset)? Is elastic reset still the preferred choice? Would changing the number of resets change anything? (Perhaps I'm misinterpreting the pareto graphs here).
- A more detailed analysis of how many resets are needed and how robust elastic reset is to the timing of rests would partially alleviate these concerns (see next point) 

## Could use more carefully controlled baselines

- The baselines could be clearer. If my understanding is correct, the *only* way in which elastic reset diverges from traditional RLHF is by periodically resetting the model to be the EMA of the past n model steps. As authors say, this is a strength of the method, but the comparisons tend not to directly compare to a model with/without elastic reset. For example, considering Elastic reset with 3 resets during training, the sensible baseline is to compare to the exact same method (e.g. REINFORCE), with the same hyperparameters, just with 0 resets during training. But the baselines in the paper seem to always have a slight confounding factor, e.g. for the pivot translation task L152, a KL penalty for Elastic Reset is added on top of the REINFORCE baseline that does not seem to be present in vanilla REINFORCE; same for L219 on top of PPO (the beta parameter is reduced, and the decision to let the PPO model ""drift more"" is not well justified); only the StackLLAMA experiment in Section 7 appears to stay consistent (L280), but the improvement of elastic reset here is not as clear.
- More generally it would be better if, instead of a ""1 vs many"" comparison where a single implementation of elastic reset is compared to a single implementation of PPO and REINFORCE baselines, a ""paired"" comparison strategy was adopted, where for each method and specification of hyperparameters for each baseline, authors measure the effect of elastic reset on top (as authors say, it seems simple to just add this onto any arbitrary alignment technique, even not necessarily RLHF). Does Elastic Reset improve consistently over other RLHF methods, keeping hyperparameters consistent? Of course, it doesn't have to improve all the time, but knowing when it helps and when it doesn't (because maybe the baseline method by itself keeps language drift under control) would be nice.
	- Again, Figure 5c is lacking some context. The Elastic Reset figure is built on PPO with beta = 0.01. I appreciate the KL ablation but does Elastic Reset dominate PPO across all KL penalty values kept constant? Otherwise what is the reason for setting the KL penalty lower for elastic reset?
- Again, to address this concern, I would love an investigation of how many resets affect performance, and any guidelines for choosing a set number of resets, in the same style as the nice ablations in Figure 5. Authors state that it is difficult to find a heuristic for how often to reset (L307), but graphs showing the effect of performance for different reset timescales and/or compute would be enormously informative, and help address the first main weakness I outlined above.

## Mathematical connection/intuition as to why elastic reset differs from distillation and/or KL

- I get the feeling there are some mathematical connections to both KL penalty and alternating RL/distillation algorithms that are not fully explored in the paper. I haven't thought too deeply, but, for alternating RL/pretraining methods (S2P; Lowe et al., 2021) and iterated distillation, one can think of the distillation/SL phase as precisely doing a sort of model reset to the initial pretrained model via gradient descent to minimize KL between the online model and the initial model. For RLHF with KL penalties, we can also view this as a sort of bayesian inference, computing a model average of the prior pretrained model and the posterior (online) model trained via RL ([Korbak et al., 2022](https://arxiv.org/abs/2205.11275)). The number of steps taken is a hyperparameter that, if chosen carefully, results in a model after distillation that is likely some average of the online model and the initial pretrained model, as in elastic reset. If elastic reset is a simpler or more efficient way of doing the same thing as such a distillation phase, this is a great thing, but it's not quite clear to me now. Could authors clarify any differences between distillation and elastic reset? In particular, why might we expect elastic reset to be **more** performant than than alternating RL/distillation or a KL penalty, given that they seem to have the same motivation (which might even be mathematically formalized)?

## Minor
- L99 ""Elastic Reset takes inspiration from both of these""—it's not quite clear what ""both"" refers to.
- Some qualitative examples of model outputs in the appendix could potentially be nice to have in the paper, perhaps even identifying what model outputs look like at different points on the pareto frontier curve for different training methods.

Limitations:
yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
Finetuning language models with reinforcement learning (RL) using human feedback, i.e. RLHF, has emerged as a promising paradigm for aligning large language models (LLM) to human preferences. Though RLHF has shown promising results when training models such as ChatGPT, RL has some inheritance drawbacks. Merely optimizing a reward model trained on human preferences can degrade performance, known as reward hacking, alignment tax, or language drift in the literature. This paper argues that the standard way for addressing this is insufficient and proposes a new idea. In particular, the authors propose, Elastic Reset, a technique to reset the model weights to address reward hacking.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Addressing reward hacking is an age-old problem, and many solutions have been proposed. The strength of this paper is that the solution proposed has several key benefits:
- Easy to implement: The authors implemented their idea on top of several existing RLHF frameworks and showcased the benefit of Elastic Reset to existing frameworks RL algorithms.
- Written Presentation: The author's presentation of the proposed algorithm, explanations, and ablations studies were justified and thoroughly explained.
- Experiments: The authors presented several experiments across three very different domains and showcased the benefit of their proposed method.
- Clear Definition of Language Drift: Often language drift is not clearly defined, but found very clear examples to showcase language drift issues.

Weaknesses:
Although the proposed algorithm is straightforward to implement and has shown good results across three challenging datasets, the approach has several weaknesses.
- Resetting on-policy vs. off-policy: Resetting weights are typically done with off-policy algorithms because the policy has a problem exploring when increasing the replay ratio. Whereas for on-policy to deal with exploration, we typically add temperature parameter, entropy loss coefficient, or some other exploration bonus. Instead, in the on-policy case, the authors use reset to keep the policy close to the original policy so it does not experience language drift, which seems like the opposite use-case of the original intent.

- Second reset: The first reset in which EMA is reset to the initial model means that you are still searching around an epsilon ball around the initial model. I am unsure why this would be much better than KL divergence, which explicitly does this.

Limitations:
Yes

Rating:
7

Confidence:
4

";1
v5Aaxk4sSy;"REVIEW 
Summary:
This paper draws inspiration from prior studies that suggest robust models can offer strong prior information, thereby enhancing both the robustness and uncertainty of the model. Accordingly, we propose a new Information Bottleneck (IB) objective, which is designed to distil robustness in the context of a Variational Information Bottleneck (VIB).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Pros:

1. The proposed Information Bottleneck Distillation (IBD) method can significantly improve the robustness of Deep Neural Networks (DNNs), protecting them against most attacks such as the PGD-attack and AutoAttack.

2.  The IBD method optimizes the information bottleneck efficiently and effectively by maximizing mutual information between intermediate features and output prediction via soft label distillation, and restricting the mutual information between the input and intermediate features via adaptive feature distillation.

3. The adaptive feature distillation transfers appropriate knowledge from the teacher model to the student model, resulting in a more accurate estimation of the student feature distribution.

4. The method was extensively tested on various benchmark datasets, including CIFAR and ImageNet, demonstrating its effectiveness and robustness compared to state-of-the-art methods.



Weaknesses:
N/A



Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes Information Bottleneck Distillation (IBD) to improve adversarial robustness from the perspective of information bottleneck principle. Specifically, two distillation strategies are proposed to boost information bottleneck. Different from the existing works, this paper utilizes the predictions of robust models to maximize the mutual information. Also, the authors design an adaptive feature distillation based on the attention mechanism to facilitate the student model to inherit knowledge from the teacher model. The experimental results demonstrate the proposed strategies are effective in improving adversarial robustness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1: This paper is technically sound, and the motivation and formulation of the proposed method are elegant.

S2: Formulating the information bottleneck objective from the perspective of adversarial robustness distillation is novel and well-motivated.

S3: The difference with the existing works is clearly demonstrated and discussed.

Weaknesses:
W1: Some derivation details in this paper are overly simplified and jumpy, making it difficult to understand. For example, how is the last step of the derivation of Eq. (11) obtained?

W2: The proposed methods rely heavily on a robust teacher model, hence the experimental evaluation of what effect of varying teachers on the proposed methods should be conducted. 


Limitations:
The derivation details are not very clear.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper takes a closer look at the information bottleneck principle and show that specially designed robust distillation can boost information bottleneck, benefiting from the prior knowledge of a robust pre-trained model and presents the Information Bottleneck Distillation (IBD) approach. What’s more, this paper also propose two distillation strategies to match the two optimization processes of the IB, respectively. The experimental results demonstrate the effectiveness of IBD.


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This article is the first to design a distillation objective function for information bottlenecks called IBD objective, and then propose two distillation strategies tor perform the two optimization processes of the information bottleneck for the further use in adversarial training.This article is the first to design a distillation objective function for information bottlenecks



Weaknesses:
See Questions part.


Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes the Information Bottleneck Distillation (IBD) method to enhance adversarial robustness, derived from revisiting variational information bottleneck from the perspective of robustness distillation. IBD leverages two distillation strategies to perform the optimization processes of the information bottleneck, namely soft label distillation and adaptive feature distillation. The final experimental results show that the proposed method enhances the adversarial robustness against both the white- and black-box attacks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper revisits variational information bottleneck from the perspective of robustness distillation, which utilizes the intermediate features extracted by a pre-trained teacher model to approximate $q(z)$. 
2. The experimental results demonstrate that the proposed method improves the adversarial robustness against both the white- and black-box attacks.


Weaknesses:
1. This paper contains two hyperparameters, namely $\alpha$ and $\beta$. The authors should also analyze the impact of $\alpha$ on the proposed method.
2. In the experiment part of this paper, the classical adversarial training methods are SAT and TRADES. These benchmarks are somewhat old, and it is recommended to use newer training methods as baselines to boost the universality of the methods.
3. There are some minor typos in this paper, such as inconsistent presentation tenses, errors in the use of singular and plural (line 99), and the representation of the cross-entropy function in Eq.(2). 


Limitations:
The authors point out the potential limitations of the proposed method.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper aims to improve the adversarial robustness of deep neural networks. From the perspective of the Information Bottleneck, a knowledge distillation method is proposed. It makes use of intermediate features and logits from a robust teacher to get priors for guidance in training of the student model.

Experiments on CIFAR-10, CIFAR-100, and ImgaeNet are conducted. Obvious improvements are obtained when compared with previous methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper gives some insights into knowledge distillation from the perspective of Information Bottleneck.
2. The distillation method shows impressive results on model robustness.

Weaknesses:
1. From the perspective of knowledge distillation, feature-based methods have already been explored by previous methods, like [9], [ref1].
As the paper claims, the major difference between the proposed method and previous distillation methods is that an adversarially-trained robust model is used as the teacher.

 [ref1] Fitnets: Hints for thin deep nets. ICLR 2015.

2. It is interesting that the students can have stronger robustness than their teachers.     
How would the performance of teachers affect that of student models?     
Will the robustness of student models be enhanced when a more robust teacher model is deployed?      




Limitations:
limitations are discussed in the supplementary file.

Rating:
5

Confidence:
4

";1
yHdTscY6Ci;"REVIEW 
Summary:
The paper studies an interesting and important question, i.e., how to automate LLMs to call existing models for solving specific tasks. The authors propose a novel framework that contains the following steps: (1) task planning, (2) model selection, (3) task execution, and (4) response generation. The experimental results well support the claim. By leveraging ChatGPT and abundant AI models Hugging Face, the proposed method is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in many tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ The paper is well-written and easy to follow.

+ A novel idea proposed and well-supported experiments conducted.

Weaknesses:
- The method heavily relies on the existence of SOTA LLMs (e.g., ChatGPT), which may involve some practical issues (e.g., unaffordable API costs). It remains under-explored whether open-source LLMs (e.g., vicuna) can be leveraged for the framework (or how to adapt vicuna for better task planning).

Limitations:
NA

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper presents a pipeline to manipulate many autonomous agents (mainly open-sourced models in  Hugging Face) . Tother with these models could solve NLP, CV, audio and Video tasks,  the resulted HuugingGPT could could complicated multi-modal tasks that might be decomposed a sequence  of atomic  tasks or a graph. 


Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- The phylosophy is interesting and insightful. I like the idea very much.
- This seems be a promising direction to solve multi-modal tasks using HuggingGPT.


Weaknesses:
- The whole pipeline seems to provide a series of prompts to solve some combined tasks. The method is not scientific from a traditional point of view.
- The evalaution protocol seems not mature.  For example, there is not evidence to check whether the evalution makes sense or not.


Limitations:
- Evalution is weak

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose HuggingGPT , a collaborative system for solving AI tasks, which is composed of a large language model (LLM) and numerous expert models from ML communities. They provide methods for each of the four stages involved in HuggingGPT's workflow: task planning, model selection, task execution, and response generation.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Impactful and well written paper
- simple strategy for handing resource dependencies for executing tasks
- compelling idea of chaining of expert models to provide a tool to decompose a task into sub-tasks and identify the appropriate expert models to solve these sub-tasks
- reasonable format for inputting task request along with examples from the user
- robust evaluation  - human evaluation along with automated evaluation

Weaknesses:
I don't see any weaknesses in the experiments, evaluation or novelty of this paper.

Limitations:
Yes, the authors list the limitations of their work as a separate section.

Rating:
9

Confidence:
4

REVIEW 
Summary:
This paper considers large language models (LLMs) like ChatGPT as a controller and presents a new framework called HuggingGPT, which connects various AI models in the existing ML community (i.e., HuggingFace). Specifically, HuggingGPT consists of four steps including task planning, model selection, task execution, and response generation. By leveraging the strong capability of LLMs and numerous AI models in different modalities, HuggingGPT can solve sophisticated AI tasks and achieve promising results.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1) The idea to connect LLMs with rapidly developing ML communities like HuggingFace is novel. It largely extends the applicability of LLMs to solve multi-modality AI tasks by fully utilizing the existing powerful models in HuggingFace.
2) Each step of HuggingGPT is well designed from Figure 2. The whole paradigm of HuggingGPT is neat and effective.
3) This paper is well written and easy to follow.


Weaknesses:
1. In Section 3.1, the demonstration examples may have an important impact on the parsing performance. The authors should provide more details such as the number of demonstration examples and the method to select these examples. The demonstration case provided in Table 1 is somewhat confusing for me because it only involves the tasks about images and texts. Can these demonstration examples benefit the parsing of tasks in other modalities like audio? I also wonder whether the demonstration examples for each user request are the same.

2. In Section 3.2, the authors propose a model selection strategy based on in-context alignments and the number of downloads. But in my view, the contribution of this module is questionable. Since the performance gap between different models for each task may be significantly large due to model scales (e.g., GPT-2 and LLaMA for text generation), it’s nearly impossible to select weaker models to dealing with the corresponding task. Thus, I’m curious about the performance if we directly use the best model for each task. There are also no empirical results to show the necessity of model selection.

3. The experimental result is somewhat weak in terms of the following points:

(1) The authors only conduct the empirical analysis on task planning. Other modules should be also tested individually. 

(2) The authors only use automatic evaluation metrics to measure the model performance. However, even GPT-4 score may have potential biases in the evaluation of generated texts. It’s better to involve human evaluation to make experimental results stronger.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents a framework that uses LLM as controller over modularized and specialized task models to plan and execute a complex task. The approach is to prompt LLM to decompose a given task command into a execution DAG, and for each step, parse model specifications (as metadata expressed in HuggingFace model cards) and select according modularized models for execution, and finally summarize a response to give it to the user. The idea is on the line with recent works on LLM-based planning and tool using.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
I think the idea is rather novel. It aligns with recent works on using LLM as a central component to query more specialized models to complete a complex task. This paper proposes to exploit the vastly available models hosted on Hugging Face in a combinatorial way. If things work out well, it could have a substantial application impact.

Weaknesses:
The biggest selling point of this paper, as is repetitively mentioned in the paper itself, is the planning. But compared to recent works, the planning strategy in this paper is actually rather simple. Existing works on planning often involves an iterative process where LLM plans, executes, and observes, and improves (for instance, Reflexion, AdaPlanner, Self-Refine, etc). In this paper, it's just plan and execute. So, on the planning part, I do not see any contribution. Maybe, compared to the planning, a bigger contribution of this paper is the task decomposition.

Experiments are very limited.
- Data scale is quite small (46 trace annotations).
- No planning baseline, no ablation study, and no insight about model interplay.
- Comparisons are all on different LLMs; this has little to do with the claimed contribution on planning.

A simple baseline can be directly using specialized models. I think even if in some cases the proposed HuggingGPT does not outperform, it still gives reader a good picture about the pros and cons. An immediate ablation study I can think of is why not merge the model selection and task planning into one step, or what is the impact of model selection, especially given the large number of models on Hugging Face. I see no clue in this paper.

It is not clear how the proposed approach rely on the few-shot demonstration. Prior works on planning mostly rely on few-shot in-context prompting. It seems this paper is also on this technical line. But such dependency is also a limitation in the general use case. It's also no clear about the variance of the few-shot prompt used, e.g., whether the few-shot examples are fixed, and what about their diversity.

Limitations:
Please see my general comments.

--------------
**This section summarizes my concern based on rebuttal.**

There is a very simple baseline this paper needs to compare with. That is a simple one-pass reasoning by LLM without any implicit or explicit planning or model selection. The author so far refuses to do such ablation, therefore I am not convinced that the approach is effective. The proposed framework is indeed interesting, but an interesting thought remains a thought without proven effectiveness.

Rating:
3

Confidence:
4

";1
lds9D17HRd;"REVIEW 
Summary:
This paper exploits Stable Diffusion (SD) features for semantic and dense correspondence tasks. The authors first conduct evaluations of SD features and found that SD features provide high-quality spatial information but sometimes inaccurate semantic matches. This paper further shows that such SD features are complementary to DINOv2 features which provide sparse but accurate matches. By fusing SD and DINOv2 features with a simple weighted concatenation strategy, this paper achieves significant performance gains over state-of-the-art methods on benchmark datasets, e.g., SPair-71k, PF-Pascal, and TSS.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- **Extensive evaluation and analysis.** This paper conducts lots of experiments and visualizations to analyze the behavior of Stable Diffusion and DINOv2 features. The experiments are also well-organized, with a smooth logic flow, which makes it easy for readers to follow.
- **Strong zero-shot performance.** By simply combining the Stable Diffusion and DINOv2 features, this paper shows strong zero-shot performance of the semantic correspondence task, outperforming previous task-specific methods by a large margin.
- **The message shown in this paper could be interesting to the community.** Unlike previous methods that mostly focus on studying single image diffusion models, this paper studies the properties of diffusion model features across different images in the context of semantic correspondence. This paper shows that the publicly available Stable Diffusion and DINOv2 models contain rich information to solve the semantic correspondence task, even outperforming task-specific models in the zero-shot setting. This would be a strong message and might stimulate future work along this direction.



Weaknesses:
It's unclear how the proposed method handles outliers in the correspondences. Since the correspondences are obtained with nearest neighbor search, which might be less robust for occlusion or out-of-frame points. 



Limitations:
Yes, the limitations are discussed in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper, for the first time, proposes a novel method to fuse Stable Diffusion features and DINOv2 features to obtain robust feature representation that readily surpasses the SOTA semantic correspondence work without further training, rather simply adopting Winner Takes All yields SOTA correspondence performance. Interestingly, with such correspondences, high-quality object swapping is also enabled without task-specific fine-tuning.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novel idea, approach and thorough analysis.

2. Motivation is really good and the paper was easy to read and understand. 

3. Impressive performance. 

4. Good application (swapping)

Weaknesses:
1. This is more like a question rather than a weakness. In figure 1, the visualization shows that a cat given as a source image, the proposed approach finds dense ""correspondence"" to dog, horse and motorcycle. Is this really a correspondence? To me, it feels like this is more like  foreground segmentation.  From the colors of the features, iI could tell that the facial part of cat is matched to the frontal part of motorcycle and facial part of other animals. This is very interesting. Why do you think this visualization is obtained even though strictly speaking, there should not be correspondences?  

2. Why is it that on SPair-71k, the proposed method yields leading results, but not for  PF-PASCAL and TSS? As the authors state, PF-PASCAL and TSS are less challenging datasets, but results seem to show the opposite.

3. Missing baselines : for supervised ones, CATs++ (extension of CATs) and IFCAT from the same author achieve better performance. Also, PWarpC is a work that adopts unsupervised learning for semantic correspondence. I would suggest including this as well. It is better to compare with them as well. I don't think it would be a weakness even if the proposed method does not attain SOTA, since the contribution lies more on the fusion of features and their analysis. 

4. In implementation detail, it says the input resolution is 960 x 960. What was the evaluation keypoint annotation resolution? The input resolution, evaluation resolution and many other factors related to resolutions have high influence on performance in this task. This is addressed by PwarpC (CVPR'22) and CATs++ (TPAMI). So if the evaluation is performed at higher resolution than other works, the comparison may not be fair at all. This needs to be clarified. 

Limitations:
Limitations are adequately discussed. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper explores the use of Stable Diffusion (SD) features for dense correspondence. The authors investigate the potential of SD and DINOv2 features and show some complementarity. SD features provide high-quality spatial information but sometimes inaccurate semantic matches while DINOv2 features offer sparse but accurate matches. The authors show that averaging the features from the two models might achieve strong performances for dense correspondence. The fused features are evaluated using a zero-shot approach, where no specialized training is performed for the correspondence task, and nearest neighbors are used for evaluation. Surprisingly, the fused features outperform state-of-the-art methods on benchmark datasets like SPair-71k, PF-Pascal, and TSS. The authors also demonstrate that these correspondences enable interesting applications such as instance swapping between images while preserving object identities.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
**In-depth Qualitative Analysis** The paper conducts a detailed qualitative analysis of the Stable Diffusion (SD) features and DINOv2 features, shedding light on their respective strengths and weaknesses. This analysis provides valuable insights into the semantic and texture relevance of these features, highlighting their distinct properties.

**Extensive Experiments** The paper presents extensive experimental results on benchmark datasets, including SPair-71k, PF-Pascal, and TSS. The authors report significant gains compared to SOTA methods.

**Application of Instance Swapping** The paper well illustrate the complementarity of the feature on this task.

Weaknesses:
**Lack of Strong Quantitative Assessments** While the paper provides an in-depth qualitative analysis of the complementarity between SD and DINOv2 features, it lacks strong quantitative assessments to support these claims. The true added value of the paper should have been a robust quantitative evaluation showcasing in particular the non-redundancy of the SD and DINOv2 features. This absence limits the overall impact and credibility of the proposed approach.

**Incomplete Figure 2 ?** There appears to be an issue with Figure 2, as the bottom illustration is missing despite being mentioned in the legend.

**Lack of Clarity in PCA Aggregation** The authors mention the use of PCA for aggregating SD features, but the explanation provided is not clear enough. The authors mention computing PCA across the pair of images for each layer of the feature map, followed by upsampling features from different layers to the same resolution to form the ensembled feature. I however have trouble understanding clearly what is the workflow here.

Limitations:
The authors underline the technical limitation of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes to study the effectiveness of features extracted from Stable Diffusion for dense correspondences. The extracted features are compared to that of DINOv2 and shown to be complementary. A very simple fusion scheme is then proposed and evaluation on datasets for sparse and dense correspondences as well as on instance swapping with convincing results.

---

The rebuttal is convincing, I maintain my initial rating leaning towards acceptance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
With all the effort that has been poured recently in diffusion models, this empirical study regarding their use outside of generative nice images is refreshing, especially since it tackle a very low level problem (keypoint correspondences) that is somewhat very remote from image generation and yet surprisingly close since the model has to generate coherent local structures. The study shows that indeed, diffusion model features are very useful for such problems and thus sheds light on what one can expect from the latent space structure of SD.

Weaknesses:
- All the results are performed in zero-shot, which is a bit limiting. I am not familiar with the recent literature on correspondence problems, but I assume that a fine-tuning of the features (at the very least of the projection) is possible to see how much we can get from these models.
- All the experiments were made with stable diffusion 1.5. It would be interesting to test another model to see if the same results hold across architecture and training change and thus if it is a generic property of diffusion models.
- The fact that the best results are obtained using 2 really big models is a bit annoying. It means it is difficult to know if the results come from the nature of the methods employed (SSL and diffusion) or just from the sheer capacity of the models employed.

Limitations:
The paper discusses 2 limiations: namely the low resolution of the features and the computational budget to get features from a big diffusion model. The first one is probably the bigger concern for correspondences.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper proposes using features extracted from a stable diffusion model for dense image correspondence tasks. The paper further proposes using DINO features along with stable diffusion features for the task and empirically shows that the combination has a complementary effect--specifically SD features have good spatial localization and are smooth, whereas DINO features are sparse but accurate. Experiments are done on several datasets and the proposed feature extraction technique improves drastically over supervised, unsupervised, and weakly supervised baselines.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper is well-written, adequately inspired, and well-executed.
2. Experiments depict the importance of these large pre-trained models for the task of correspondence learning in images. Specifically, the large gains over supervised methods are quite surprising.
3. Instance swapping application clearly demonstrates the ability to do accurate correspondences between different instances of the same category of objects.

Weaknesses:
1. The use of pre-trained features from large models is a well-explored area of research [1,2,3]. These pre-trained models are known to perform well on downstream tasks such as semantic segmentation, detection, and classification. It is unsurprising that these features are also useful for correspondence learning.
2. Though the authors show limitations of the features. It should have been focussed more on the paper. Specifically categorizing the mistakes made by the approach and possible ways to alleviate the problems. This will be a good contribution to the community as it informs when to avoid using the proposed approach.


[1] _Unleashing text-to-image diffusion models for visual perception._ Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. 2023

[2] _Emerging properties in self-supervised vision transformers._ Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021

[3] Open-vocabulary panoptic segmentation with text-to-image diffusion models. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. 2023

Limitations:
Yes.

Rating:
7

Confidence:
5

";1
HtMXRGbUMt;"REVIEW 
Summary:
This paper aligns with a range of recent works which investigate the tendency of diffusion models to memorize  training data and emit this during sampling, when prompted to, and presents strategies to mitigate this for data-processing, training and inference. Other than previous works, which mainly focused their analyses around duplicate images in the training data and model overfitting (i.e. dataset size, portion of train examples seen during training, …) , this work separately analyzes the influence of both these points and further adds to this the influence of captions (and their duplication) in the training data and complexity of the images in the training dataset. The main findings for these are that highly specific captions for every image promotes duplication which is furthermore also more likely for images which are simple (as measure by pixel histogram-entropies and jpeg-compressability). 

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
* The paper separates the main factors of influence during training from each other and bases the findings made for each of these separate categories on rich sets of empirical experiments, making the paper conclusive and interesting
* The main take-aways are novel and of high importance for the community, especially since the authors center their experiments around fune-tuning a foundational DM, which is what many researchers do nowadays.
* The key take-aways for every analyzed influencing factor are summarized below the respective sections which makes the paper well structured and raises comprehensibility.

Weaknesses:
* It is totally reasonable that the authors cannot train a foundation model like SD from scratch and therefore focus on finetuning based on these models. However, the exact influence of the pretraining on all the results presented on conclusions taken remains unclear to me after the paper. It would therefore be interesting to also include results of models trained from scratch for all of the presented experiments, since it would provide insights about the generality of the statements made.
* Related to the above point, I think the title may promise a bit too much, since it would need experiments from scratch (for models such as SD) to choose that title. Maybe sth. Like “Why finetuned diffusion models memorize and …” would be better. What du you think?
* To me, it is counterintuitive that the model trained with no duplication has a higher dataset similarit score than the one with partial dup. in Fig. 5. If I didn’t miss sth, this is never discussed and is therefore confusing. It might also raise concerns about the significance of the results presented in this part which I think is crucial for the entire paper. Could the authors share their take on this?
* For a reader not knowing SSCD it may be hard to estimate, how reasonable the chosen newly introduced metrics related to SSCD are, since many of these are based on thresholds. Therefore, it would be great of the authors could add similarity scores to all visual examples showing found duplicates (especially in Fig. 1). 
* In line 91 it is stated that FID would measure image quality and diversity. I don’t agree for diversity since there are many models with low FID scores which actually have very low diversity (many GANs, see e.g. [1,2]). I’d therefore recommend to either additionally include Precision and Recall Metrics [2] in the evaluation or remove diversity here.
		
[1] Sauer, A., Chitta, K., Müller, J., & Geiger, A. Projected GANs Converge Faster.
[2] T Kynkäänniemi, T Karras, S Laine, J Lehtinen, T Aila ,  Improved precision and recall metric for assessing generative models

Limitations:
There is not dedicated limitations section, however, the limited scope of the experiments, which focus only on finetuning is mentioned in the paper. I’d recommend to add  a dedicated limitations section and once more state this. Also it would be interesting to see this analysis for other types of models such as AR models or GANS (although I know that no comparably powerful models to SD are publicly available as of now). The fact that this misses could also go into such a section

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper discusses the issue of data replication in text-to-image diffusion models, specifically focusing on the memorization problem. It mentions that these models, such as Stable Diffusion, are known for unintentionally replicating their training data, which has led to recent lawsuits. The commonly held belief is that duplicated images in the training set contribute to content replication during inference. However, the paper's analysis reveals that the text conditioning of the model also plays a significant role in data replication. The experiments show that unconditional models don't replicate data as frequently as text-conditional models. Building on these findings, the paper proposes several techniques to mitigate data replication during inference by randomizing and augmenting image captions in the training set.






Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-organized and well-written
- The experiment setup is clear and sound.
- The originality of resolving the memorization issue of the diffusion model is interesting.
- The experiment results are convincing.

Weaknesses:
- Even though this paper is focusing on the empirical results and evidence of memorization of the diffusion model, I am still expecting a high level of theoretical understanding. Specifically, does it holds for the general case regardless of the type of SDE, data distribution etc.?
I think these questions are also important.

Limitations:
The observation is based on the heuristic observation, however, it is fine since it is impossible to have a white-box explanation of a neural network. It would be better to have some guidance in theoretical reasoning.

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper tries to explain the memorization and copying of training data in diffusion models. The paper establishes a connection between caption duplications and memorization on top of data duplications analyzed in previous works. In their main experiments, the authors fine-tune various SD models on ImageNette/LAION subsets and show that controlling data duplication does not completely prevent memorization and that highly specific captions amplify copying. They also relate memorization to training parameters and data complexity. Finally, the paper introduces several mitigation strategies based on randomizing conditional information. 


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
 - Overall, I find the experiment design in the paper to be good and got a better understanding of the memorization phenomenon from the paper. I also enjoyed the writing style of the paper. 
 - I find the connection between image complexity and copying to be reasonable and believe this is the first time this has been analyzed this way. 
 - While previous papers were mostly about analyzing the impact of data duplication on memorization, it makes sense to also control the captions and the paper shows that there exists a connection between the two. 
 - The problem setting of the paper is very relevant. 


Weaknesses:
 - Pretty much all experiments are based on the dataset similarity score which is defined as the 95-percentile of image-level similarities. As this is very important for the validity of the experiments, I believe that it would make a lot of sense to motivate this design choice further. From previous works, the authors quote that an image similarity larger than 0.5 shows strong similarities. In the worst case, the distribution of image similarities could contain slightly less than 5% near duplicates and slightly more than 95% unrelated images which would result in an arbitrarily small dataset similarity but still a decent memorization quote. Maybe the authors could simply demonstrate that the distribution of image similarities is well-behaved and also quantify the number of outliers, eg at a dataset similarity of 0.2, how many of the remaining images above the 95%-quantile actually achieve an image similarity larger than 0.5. Ideally, it would also be great to shortly demonstrate that results look similar with larger percentiles and that the 95-percentile is not critical for the outcome of the experiments. 
- The range of similarity scores in most of the experiments is actually somewhat limited. I do not have a good feeling about the scale of this metric, but for example, the data duplication factor seems to only influence the score in the range 0.64 to 0.70. As this is the 95% quantile and 0.5 can be assumed to be a duplicate, this means that in all cases we have strong duplicates. I do believe the authors that there is a connection between duplicates/captions and copying, however, either the dataset score is not the right metric for this kind of experiment or duplicates/caption can actually only explain a relatively small part of the problem. Also even with the best-performing train time mitigation strategies, the similarity score is 0.42/0.47 so from my understanding this could mean that we still have about 5% of duplicates. 
- The mitigation strategies are somewhat simplistic. I also believe that it is crucial to give a more detailed analysis of the ""minimal effect"" that the mitigations have on model performance. The images on the left half of Figure 8 work well in the sense that they seem to produce the same object that was also on the original image. However, most of the images on the right side of Figure 8 are not convincing to me. For example, I do not see any interaction between the hippo mother and her child (prompt: ""Mothers influence on her young hippo""), and from a short Google search I would say that the person in the image is not Ann Graham Lotz (prompt: ""Living in the Light with Ann Graham Lotz"") and I could not find any connection between the last image with mitigation and ""hopped up gaming east"". I think mitigating copying is clearly important, but if this prevents the SD model from following the user's prompt it becomes useless. I think Figure 8 should show the results of the best train and test time mitigation strategies for the same images and I would also like to see the results of the best train time defenses on the images on the right side as apparently they seem to be harder. For some images in Figure 16, I also do not find any of the test time defenses to produce reasonable results. 
 

Limitations:
In my opinion, one of the largest limitations is that even the best-performing training guidelines/mitigation strategies might not always prevent copying. The paper gives us a dataset similarity result of 0.42/0.47 and claims that in terms of image similarity, 0.50 would be a duplicate. So it is not clear to me that this actually solves copying. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper is a deep dive into the phenomenon of memorization in Diffusion models. I find that the paper is very well position given the current state of generative ML and the paper does a great job at asking the right questions about various factors concerning the memorization of images. Some of the key takeaways in the paper are really strong, such as randomizing captions, or partial duplication of images, but some others are not very well substantiated. Finally, the authors also conclude with some guidelines and mitigation strategies for practitioners, making this a refreshing read.

This paper is hard to review in a strengths/weakness pattern because each section has its strengths and weaknesses. First, this paper tries to do a lot. I think that ends up being a weakness of the paper because the high-level message from the abstract and intro gets swayed away through the body making this a very confusing read. Especially in the sections from 3 to 5.3. It is only after that the main narrative of the work catches on. I understand that the authors want to show a lot of experiments and analyses they did, but many of them are not at all central to the key message of the paper.


Soundness:
2

Presentation:
2

Contribution:
4

Strengths:
1. This paper tries to break down various factors that are responsible for the memorization of training data by diffusion models.
2. The discussions in sections 6 and 7 are really valuable to the community.
3. The paper asks the right questions and also performs experiments to support their hypothesis (more about this in the weaknesses). But overall, the paper serves as a great starting point for understanding the state of memorization in generative models, giving a peak at some thought-provoking questions, and using some of these hypotheses to actually suggest mitigation strategies.
4. I am curious about the image complexity argument. This is a refreshing take, but also very different from image classification literature where typically hard images are memorized because the model can easily learn the simple ones to classify. It seems that the take for generative models is the opposite. but again there is a distinction between anecdotal arguments of features versus the argument of compressibility and entropy studies here. A good metric potentially would be the embedding similarity as assessed by a CLIP model.

Weaknesses:
My biggest qualm with this paper is about the metrics. It seems that this paper is based in a delusional world where dataset similarity and FID are the sole indicators of model memorization and model performance. I know that evaluating both generation and memorization are hard problems, but this paper takes these yardsticks way too far, to an extent that following up with some results becomes uncomfortable. And in many sections, I get the sense that hypotheses and conjectures are drawn just to satisfy these so-called oracle notions of memorization and quality.

1. On generation quality: Section 7 severely needs an analsis on the impact of these changes on the quality of generation. Similary, in Figure 4, FID scores are all over the place. are these even good models?
2. On memorization: In Section 5.1 the authors note that they did not observe any duplication in the clusters they studies. Again telling how the metrics of DS are not informative or reflective of memorization in the first place. I do not understand why checking similarity scores with the top 5% of generations. Further, in Figure 5: lower SS when the image is duplicated. and this happens even in the right figure. But does this actually related to ""memorization"" or just stay till the idea of dataset similarity. For instance, in the de-duplication done for SD2, not all duplicates were ""full duplicates"" if I am correct. So i think the explanation saying random captions helps reduce SS is an over statement.
3. On section 3 and Figure 1 proper attribution should be given to the original authors who devised the technique
4. I have not seen people use clip text features to assess how close texts are. I would suggest using some text model such as RoBERTa for these use cases. 
5. ""possibly because FID is lowest when the dataset is perfectly memorized"" I do not see how this logically flows. And in general I did not like that this paper makes too many conjectures to explain every possible phenomenon they see in results. It just feels like the authors are trying to fit a jigsaw no matter what.






Limitations:
-

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper systematically investigates the memorizing phenomenon of stable diffusion. As they empirically observed, most of the memorizing phenomenon originates from the duplication of images and captions in the training set. Owing to this, the authors propose to deduplicate the captions during the training stage to obviate the memorization of the diffusion model.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The experiments in this paper are quite complete. In my opinion, most of the components that may be related to the mesmerization phenomenon are considered. 

Weaknesses:
To measure the memorization magnitude, the authors use a similarity score and set the threshold as 0.5, i.e., the score is larger than 0.5 means there is a memorization for a certain generated image. My concern is whether this score and threshold are valid criteria. 

For the stable diffusion trained on a large-scale dataset without duplication i.e., stable diffusion v2.1, the memorization phenomenon seems almost mitigated. Thus, my question is whether increasing the unduplicated data can fix the memorization phenomenon. 
 

Limitations:
no

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors conduct a large-scale empirical study about the memorization problem in diffusion models. Their experimental results indicate that text conditioning plays an important role in this problem. Based on these observations, they propose to randomize the text prompts during training to mitigate memorization and copying.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The memorization problem is valuable for the AI safety community.

2. The authors design a large number of experiments to study this problem.

3. The authors observe that text conditioning plays an important role in this problem, which is a new discovery compared with the existing works.


Weaknesses:
1. Some experiment designs cannot verify the arguments. And some results are not consistent with the arguments of the authors. Please refer to Question1-3.

2. The proposed solutions are not novel. Please refer to Question4.

3. Some discussion and references about mode collapse are missing. Please refer to Question5.

4. Some figures in the paper are messy and confusing. Please refer to Question6.



Limitations:
The authors do not discuss the limitations.

Rating:
6

Confidence:
3

";1
E58gaxJN1d;"REVIEW 
Summary:
This paper proposes a novel approach to deal with the ""learning from expert visual observation"" setting, lacking action labels. The proposed method first pretrains State-to-Go Transformer (STG), which takes the sequence of encoded visual observations as inputs and predicts next-step embeddings under causal masking. It is trained with adversarial training inspired by WGAN, and with temporally-aligned regressor objectives. The agents are learned, similar to IRL, to follow the action-unlabelled demonstrations. The experiments show the better performance of STG approach on 4 Atari tasks and 4 Minecraft tasks.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
### Originality
- Incorporating sequence modeling with causal transformer and temporally-aligned regressor into learning from expert visual observation imitation learning seems novel and interesting.

### Quality
- Overall, the paper is well-written and easy to follow.

### Clarity
- Figure 1 helps us understand the brief workflow of the proposed method.

### Significance
- The performance of STG in Atari and Minecraft is better compared to the baselines (ELE, GAIfO, PPO, Expert).
- It would be an insightful observation to the community that combining temporally-aligned regressor into sequence modeling works really well (Figure 5).


Weaknesses:
### Originality
- Seeing Figure 3,4,5, the results of STG without temporally-aligned regressor is almost the same as ELE. The most important contribution may come from the off-the-shelf module. Maybe ELE + temporally-aligned regressor would be an interesting ablation if applicable.

### Clarity
- ""State-to-Go"" might be misleading naming, if it is named as an analogy of Return-to-Go in Decision Transformer. The role of State-to-Go Transformer is quite different from Return-to-Go in Decision Transformer. The agent do not leverage any multi-step-extended values or state representations.
- Minecraft experiments use 50000 demonstrations, while ones on Atari only use 50 demonstrations. There is no description on such a significantly different data size. Also, including the ablation with dataset size is interesting.
- The authors said, ""Dopamine is not the expert demonstration"" in Section 4.1, but I guess the generated dataset is also not the expert one, seeing Table 4. It is good to include the ablation on dataset quality.


### Significance
- In Minecraft, MineDojo or CLIP4MC builds open-source large-scale pre-trained models that can predict the surrogate reward to measure the similarity between rollouts and textual descriptions. Those are also useful in LfVO settings considered in this paper. In this current situation, the significance of building a small-scale per-task module for surrogate reward is unclear.

Limitations:
I think this paper appropriately mentioned the limitations and broader impacts.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of learning from visual observation. While most adversarial imitation learning methods learn intrinsic reward online, in order to improve the sample-efficiency of online learning, this work proposes to offline pre-train a state-to-go prediction transformer and a discriminator, both conditioned on current states and operating on next states. Representation learning is also enhanced with a shared encoder and an auxiliary temporal distance regression task. Empirical evaluations are conducted in both Atari and Minecraft domains.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. To my knowledge, the proposed offline pretraining framework for adversarial imitation learning is novel and intuitive.
2. Empirical experiments have been conducted in the challenging Minecraft domain.
3. This paper is well-written and easy to follow.

Weaknesses:
In my opinion, the most significant weakness is that empirical evaluations are insufficient to support the superiority of the proposed method since some highly relevant but strong baselines are not compared. (see Question 1 below).

Limitations:
This work has discussed its limitations and future work in the conclusion. There does not seem to be any negative social impact of this paper that should be discussed. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a new method, called state-to-go (STG) transformer, for learning from visual observation (LfVO), which aims to learn strong policies given the access to (i) offline datasets without actions and rewards and (ii) following online interaction. The main idea of STG transformer is to learn representations by predicting the temporal diference and learn adversarial discriminator that discriminates the embeddings predicted from the model and expert states within the dataset. Then the model is trained to achieve the target tasks by learning to optimize the intrinsic rewards given by the discriminator, which is basically trained to follow the expert behaviors within the offline dataset.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Strengths
- 1. Pre-training the discriminator from offline datasets instead of learning the discriminator using the online samples is a novel and interesting idea, to my best knowledge.
- 2. Consistent improvement over the baselines on Atari and Minecraft domains.
- 3. Main description of the method is clear.




Weaknesses:
Weaknesses
- 1. This paper is missing crucial analysis that investigates the importance of learning the discriminator using the offline samples. Experiments on Figure 5 (STG, and STG- that does not use temporal representation learning) actually shows that removing representation learning makes performance be very similar to baselines that do not depend on pre-training in several tasks. This makes it questionable whether the proposed offline pre-training component of learning the discriminator is indeed important for the performance and make be think that pre-training good representation learning from offline datasets might be the most important factor. Including more investigations, e.g., experiments with pre-trained encoders for all the methods, could might support that the proposed discriminator training scheme and the intrinsic reward from this are indeed important.
- 2. If the paper wants to claim the newly proposed visual representation learning is one of main contributions, I think proper comparison to prior generic representation learning methods (contrastive learning, masked autoencoding, time contrastive networks, ..) is crucial to claim such a contribution. It makes sense that the proposed method works well but its effectiveness and novelty are not well verified in current status so to be deserved as one of main contributions of the paper, especially considering that using the time index for representation learning is also investigated in prior work ([Cai et al., 2023], [Yun et al., 2022])
- 3. The paper is misusing the term 'State-to-Go' which is not clear what exactly this means. Decision Transformer uses return-to-go as their inputs because their main idea to learn a return-conditioned policy that enables it to stitch the offline samples. But there is no definition of state-to-go in the paper and it is actually not used as inputs or conditioning the policy in any part of the paper. This makes it very difficult to understand the main message from the paper.
- 4. The paper often positions itself as a paper that addresses the difficulty of learning from visual observations on video game domains, while prior work only considers the robotics domains (e.g., line 97). But what exactly are the main factors that make video game domains is more challenging and which experimental evidence supports this? Isn't this claim actually narrowing down the scope of the paper? Also, this should be clearly stated in the abstract or the title that the paper mainly wants to address the main challenges for training offline agents in video game domains.
- 5. This is a minor point, but the paper assumes some background knowledge of prior methods by passing too fastly over the baselines. For instance, the authors might not be familar with ELE and the exact meaning of progression rewards, but the paper assumes such knowledge and does not give a formal definition of progression reward, thus making it difficult to understand the main message from Figure 7 and corresponding subsection.

[Cai et al., 2023] ""Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction."" CVPR 2023

[Yun et al., 2022] Yun, Sukmin, et al. ""Time is matter: Temporal self-supervision for video transformers."" ICML 2022

Limitations:
Yes.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Paper presents a novel method for learning from visual observations. Specifically, a state encoder is first trained in an offline fashion with two tasks: 1) predicting the normalized relative distance between two states; 2) predicting the relative difference between two consecutive states. Then the whole agent is trained using a standard WGAN-based IRL from observation loss. Experiments on Atari and Minecraft shows some promises against baselines and ablations.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
+The paper is overall clear and well-written. The authors did a good job illustrating their main idea with stylish graphics and visualization, which is friendly to non-expert readers.

+The method itself is technically sound with good motivations. Although some design choices are not properly backed by ablations, I think the authors explain the rationales under their losses and model architecture well, mostly relevant to the challenges, ex. distinguishing states.

+On the selected arenas, the method indeed show superior performances against the counterparts and the gap is quite significant, suggesting the effectiveness. The TSNE viz is also quite helpful with better understanding on how the method work.



Weaknesses:
Having said those above, I do have some major concerns regarding some claims and mostly experiments. I hope the authors can help clarify them in a rebuttal:

-In sec. 3.3, the text reads “That is, the WGAN discriminator can clearly distinguish between the state sequences collected under the learning policy and the expert state sequences, without fine-tuning”. This could be a very strong argument on the proposed representation learning method and the authors seem to derive this from the comparison to GAIfO (w/o the proposed representation learning). But this can be indirect evidence as there might be other factors contributing to the differences, ex. hyper-parameters, additional models, etc. The authors are suggested to include a trajectory visualization that shows if the collected trajectory indeed becomes more distinguishable to expert demonstrations with the proposed representation learning method. 

-Baselines; Although I can understand the authors are focusing on IRL-based LfO approach, but indeed I think it is still necessary to include some baselines based off behavior cloning, ex. BCO ([19] in the original paper) and some follow-ups. It would be exciting to see if the proposed representation learning pretraining could help with these BC-based approach as well. Moreover, some additional IRLfO baseline, ex. IDDM ([2] below) and some more recent work are also worth comparing.

-Ablations; The authors only include an ablation that removes the TDR loss from the complete pretraining objective. However, there are many more components and design choices that should be justified by ablations. To name a few:
- transition discrimination losses, L_{mse} and L_{adv}
- coefficient of these losses

-Is It possible to use multiple trajectories for the TSNE visualization in Fig. 6? 

-Also Fig. 6, I am a bit confused why the state can be clustered with TDR loss. I will expect a task of predicting the distance between the current state and the goal state (ex. as in [1]) to encourage the emergence of clustering based off the distance-to-go. However, the TDR task is about pair-wise distance between two arbitrary states. Can the authors explain why such loss could facilitate the clusters we observed in Fig. 6?

-citations; some relevant papers on learning Minecraft controllers and LfO in general should be cited: [1-2]

[1] https://arxiv.org/abs/2301.10034 

[2] https://arxiv.org/abs/1910.04417  


Limitations:
I am wondering whether the proposed representations losses have to be tied with the transformer architecture, i.e. will they still work with a RNN-style recurrence?

Rating:
6

Confidence:
3

";1
9ych3krqP0;"REVIEW 
Summary:
This paper presents MultiFusion, a multilingual multimodal image generation model, which can be effectively trained by fusing existing pre-trained visual models, language models, and stable diffusion models. Using a multilingual autoregressive language model as a bridge, MultiFusion follows MAGMA to enable multimodality by learning adapters. Before connecting the language model with the stable diffusion module, it learns semantic embeddings with a contrastive learning objective in a parameter-efficient setup. Finally, it connects the language model with the diffusion model with monomodal data. i.e., an image or caption.

Experimental results demonstrate that the trained MultiFusion model can generate high-quality images with multimodal interleaved prompts. Besides, with modular design and the fusion of pre-trained models, the training can be quite effective compared to training from scratch. Several analyses such as attention manipulation also provide insights into the multimodal language models.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
- The paper makes a clever combination of pre-trained models and adapter learning techniques, including 1) MAGMA for cross-modal adaptation/fusing, 2) contrastive learning before fusing LM with the diffusion module, 3) cross-attention learning of SD to align the conditioning with the new embedding space. These operations delicately combine the pre-trained modules into an end-to-end multimodal-text-to-image model.

- Experimental results show that MultiFusion can produce high-quality images conditioned on multimodal and multilingual inputs, with a wide range of applications and use cases. 

- The analysis on attention manipulation is quite interesting.

Weaknesses:
- It would be great to improve the presentation of the paper, especially methods and implementation details. Although Figure 1 presents an overview of the architecture, I have to guess some of the implementation details and carefully find clues from a large amount of text. Some suggestions: (1) you could provide some figures to show the details of how the adapters connect the pre-trained models and how you learn them; (2) you could also clarify the training tasks, and data in tables.

- Existing works have explored how to learn adapters to connect pre-trained modules. For example, Flamingo learns gated adapter modules to connect language models with visual models, and generate text conditioned on multimodal inputs. The paper provides a full solution to the problem with careful design but it is kind of an integration of existing adapter methods.

Limitations:
As mentioned in the paper, the model always produces variations of input images, which limits its applications in image editing. I think it is worth mentioning this limitation, which provides further understanding of MultiFusion.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors present a novel approach to expressing complex concepts with arbitrarily interleaved multimodal and multilingual input. Their approach leverages pre-trained models and allows an efficient fusion of different component without training a model from scratch.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-written and easy to follow
2. The experiments are well-designed and allow one to use existing pre-trained models while reducing the demand to train a system from scratch.
3. The result on various benchmarks are promising and would invite more discussion in this line of work.

Weaknesses:
The motivation to attempt such a problem is rather weak. Under what circumstances, would one want to have interleaved multimodal input to generate images? Is it because we want to control the input? If so, why not compare the proposed approach with similar models such as ControlNet and DreamBooth? 

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors introduce MultiFusion, a novel approach that enables the expression of complex and nuanced concepts in text-to-image diffusion models (DM) through arbitrarily interleaved inputs of multiple modalities and languages. The “fusion” concept is at the core of the whole work: to fuse modalities together, pre-trained models (a LLM and a stable diffusion backbone) are fused together. Experimental results highlight the efficient transfer of capabilities from individual modules to the downstream image generation module. Notably, MultiFusion empowers the image generation module to effectively utilize multilingual, interleaved multimodal inputs, even when trained solely on monomodal data in a single language. The contributions of this work include the fusion of modalities for image generation, experimental evaluations, and the introduction of a benchmark dataset for further analysis and comparison regarding the multimodal compositionality of the models.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
1. **Innovative model fusion approach**: The paper introduces an innovative approach by combining a partially frozen multilingual Language Model (LLM) with a stable diffusion backbone. This fusion results in an interesting multilingual and multimodal encoder capable of seamlessly interleaving between input items, treating them as a modality-agnostic sequence.
2. **Multilingual alignment investigation**: The authors conduct an investigation into the model's multilingual capabilities by translating the prompts from the DrawBench dataset. This exploration demonstrates an understanding of the importance of multilingual alignment. While there is a question regarding the accuracy of the translations, the authors acknowledge the potential benefit of utilizing literal translations in training the multilingual encoder, even though nuances in meaning may not be fully captured. This highlights the authors' attention to addressing the challenges and complexities of multilingual representation.
3. **Contribution of benchmark dataset**: The authors contribute to advancing research in multimodal compositionality by producing and sharing the MCC-250 dataset. This benchmark dataset, described in detail in the supplementary material, serves as a valuable resource for assessing the compositionality of multimodal inputs, specifically comprising English text and images. The production and release of this dataset demonstrate the authors' dedication to promoting reproducibility, comparison, and further progress in the field of multimodal compositionality.

Weaknesses:
1. **Lack of clear architectural design and novelty**: The paper suffers from a lack of clarity in explaining and justifying its design choices. While references are provided, the underlying motivations and problem-solving aspects of these choices are not adequately explained. While Figure 1 attempts to illustrate the model structure, it is not accompanied by a clear rationale and explanation for the chosen modules and their interactions in the text. Enhancing the clarity of the architectural design would elevate the novelty and originality of the proposed approach. It is suggested to provide a high-level description that guides the reader in understanding the motivations behind specific choices. By focusing on the ""why"" rather than the low-level details, readers can, for example, grasp the purpose of unlocking only the biases in the LLM. Currently (line 130-131), it is unclear if this is a crucial step to obtain good results while keeping a parameter-efficient regime, or if it is marginal in that regard. The supplementary material can be utilized to provide additional low-level details for interested readers.
2. **Lack of clarity in Figure 4 and semantic search paragraph**: While Figure 4a demonstrates higher similarities of translated prompts in the authors' method compared to competitors, it does not provide insights into the similarities between the reference and other negative samples. This additional information is crucial to establish the range of similarities that can be considered as genuinely low. Furthermore, Figure 4b indicates that the AltDiff competitor generates potentially more consistent images in each language, suggesting that the embedding similarity between references and translations may not be entirely representative. Clarifying these aspects would enhance the understanding of the results and provide a more comprehensive evaluation of the proposed method's performance.
3. **Missing standard deviation in tables**: Including standard deviation in the results would provide important information about the variability and statistical robustness of the findings. By incorporating this measure, the paper would strengthen the reliability and credibility of the reported results.
4. **Performance comparison and insights**: In Figure 4b, the AltDiff method demonstrates better performance, raising questions about the potential benefits of adding more languages to the MultiFusion method. While the authors suggest that alignment remains similar despite MultiFusion being fine-tuned using only English data, additional experiments are needed to provide substantial evidence that adding more languages to MultiFusion indeed yields improved results. Further investigation and insights in this area would enhance the value and understanding of the proposed method.

In offering these critical observations, I would like to emphasize that my intention is not to be harsh, but rather to provide constructive feedback. I acknowledge that explaining such a complex pipeline can be challenging and that significant effort has been invested in this work. However, I strongly believe that there is room for improvement in describing the architectural choices and highlighting the strengths of the paper, and I’ve tried my best to give possible suggestions in this regard. I’m convinced that addressing these aspects would significantly improve the quality and impact of the paper, but I don’t think this is something that could be fixed within the rebuttal period. In any case, I remain open to reconsidering my recommendation if any relevant insights emerge during the discussion.

Limitations:
The authors adequately addressed the limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposed a novel method to build multilingual multimodal generation models that supports prompts composed of interleaved text and image. It combines a strong pre-trained multilingual language model with the image generation model from Stable Diffusion (SD) and achieves alleviated capabilities such as prompting with text and image combined. It also shows that the new model does better in composition generations, as one can provide reference images as part of the prompt. 

In this work, a multilingual language model is trained in the first place (13B encoder-decoder structure model trained on 400B tokens), which itself is a strong multilingual model. It then adds an adapter module to the LLM model to support input in image format, following methods proposed in MAGMA. Finally, it aligns the trained encoder with the diffusion model taken from Stable Diffusion, with 15M text-image pairs. As a result, it can support multilingual, multimodal prompt for image generation, without training on massive text-image pairs dataset.

In experimentation, it showed that using both text and image as prompts can be beneficial, especially in composition generations. It also shows superior performance to existing multilingual text-image generation model AltDiffusion, possibly due to better alignment in multilingual embeddings. Further, the support of taking image as prompts can enable varies applications such as negative prompting with image, image composition, image variation and style modification.  

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
* Novel and efficient method: fusing different pre-trained models works very well which can bootstrapping existing models such as stable diffusion to achieve different input format, avoiding the heavy cost of training model from scratch.
* Enables prompting using both image and text and generates better images both in terms of metrics such as FID and human evaluation,  comparing to baselines that only takes text prompt.
* Better results on composition generations from considering reference images in prompts.
* Well written overall and addressed limitations of the work very well.

Weaknesses:
* Some of the details such as model parameter size, training data source and size are not presented in the main paper (included in appendix), which can be less clear when interpreting results presented in experimental section. It would be better to point those factors out when comparing with baselines in the main paper.



Limitations:
The authors addressed the limitations relatively well in the paper: 
1. The generated image cannot do copy of exact prompt images;
2. Sometimes the image prompts need to be carefully chosen and do not always work;
3. It suffers the same shortcomings (such as inappropriate content) as other generation models trained on very large-scale crawled dataset (LAION).


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents an approach for creating a model that can take interleaved sequences of images and multi-lingual text as input, and generate novel images as output, by fusing together pre-trained models: (1) a ResNet image encoder from CLIP (2) an encoder-decoder text Transformer LM and (3) a Stable Diffusion (SD) image decoder. Most of the weights of the models are frozen, with some fine-tuning of adapter layers, the biases of the LM, and the cross-attention layers of the SD U-Net. Multimodal training is done on a combination of large scale image captioning, and VQA datasets, using the standard text-conditioned diffusion objective. The encoder-decoder text model was pre-trained on multi-lingual data, making the resulting image generation model also multi-lingual. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
S1) The aim of this work, enabling an image generation model to take both images and text (in multiple languages) as input, is exciting and well-motivated by some of the qualitative examples in the paper: multi-modal inputs give complementary info, and multi-lingual text capabilities should broaden model accessibility. 

S2) I found the experiments on compositional robustness (MCC-250), with the improved results from the combination of this text encoder and the image inputs, interesting and think it has the potential to be a timely addition to the ongoing conversation about the role of the pre-trained text-encoder in compositional robustness of image generation (but see suggestions on baselines below). Doing a human evaluation user study was also a real strength of these experiments.

S3) The qualitative results were compelling, particularly Figure 5 in the main text and Figure 4 in the appendix. 

Weaknesses:
W1) The experimentation was a bit thin. 
- Although there is definitely a shortage of current benchmarks for the new capabilities presented by this model, the contribution of the paper would be stronger if it were able to reappropriate existing benchmarks or create new ones to evaluate some of these capabilities (e.g. negative prompting with images, multimodal image composition). 
- The method has a few steps (e.g. contrastive fine-tuning on a natural language inference dataset; training on a large number of multimodal datasets, both VQA and captions; and using attention manipulation), but I couldn't find any ablations on these components. This, in combination with the lack of details on the [apologies, the rest of this sentence was missing earlier] datasets, makes me worried about whether the overall approach will benefit future work.
- The quantitative results that are presented here would be more convincing with a few (hopefully) easy-to-run variants of the current settings (another classifier-free-guidance weight; ablating image inputs in the MCC-250 experiment); see questions below. The compositional robustness results are interesting, but giving an image as input is a pretty strong (and potentially unrealistic) source of supervision.

[update after response] : I still feel that point a) above, about capability evaluation, is a weakness, but the author response definitely helped address the other points. Thank you!

W2) The method relies on proprietary datasets and models for the language model (and possibly also for the image datasets, see questions below). I don't think this would be a crucial weakness except that almost no information is given about these datasets and models, even in the appendix. Given that the LM is frozen when doing the multimodal training, and that the capabilities of the fused system (with respect to multi-linguality, and the compositional robustness experiments) seem very likely to me to depend on the properties of this LM, more openness (ideally, using a publicly-released multimodal encoder-decoder transformer, like mT5-XXL, which also has 13B parameters) would really enhance the scientific value of this paper.
- The encoder is described as a ""13B transformer encoder-decoder similar to GPT-3"", but GPT-3 is a decoder-only model, trained with a language modeling objective.
- The LM dataset is described only as ""400B tokens of English, German, French, Italian, and Spanish"", and it's unclear whether the multimodal training data includes datasets other than the ones listed in lines 17-18 of the appendix. 
- The German-English versions of SNLI and MNLI used for the semantic embedding objective also seem to be proprietary. 

W3) The writing was somewhat unclear. In particular, a lot of details about the model (the pre-trained models used, the training data for the full approach) were unspecified in the main text, although outlined in the appendix (Section A). Some details about the experiments were also unclear, see questions.

[update after response]. The response effectively addressed both W2 and W3 -- thanks!

Limitations:
I felt that the limitations section was pretty solid in qualitatively outlining weaknesses of the approach, although I'd appreciate experiments to quantitatively support the claim that attention manipulation can help prevent image context overriding text context.

Rating:
6

Confidence:
4

";1
maSAKOKXTi;"REVIEW 
Summary:
The paper introduces a Generative Evolution Optimization (GEO) algorithm to black-box optimization, introducing. The GEO algorithm is claimed to combine the strengths of Evolution Strategy (ES) and Generative Surrogate Network (GSN) to address the limitations of Bayesian optimization and other existing methods. Some benchmark functions are tested to verify the performance of GEO.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Originality: The paper introduces GEO which combines the strengths of L-GSO and Evolutionary Generative Adversarial Networks (EGAN). I did not see such method before.
Quality: The authors provide explanations of the GEO method, including its foundational concepts, operation steps, and algorithmic structure. Some benchmark functions have been tested.
Significance: The paper addresses a significant challenge in the field of black-box optimization, e.g., the optimization of non-convex, high-dimensional, multi-objective, and stochastic target functions. 


Weaknesses:
Some claims and concepts are not adequate, like the O(N) complexity. Without the target of finding global optimal, we can design various methods that achieve O(N) complexity easily.
Some related works are not cited adequately, like Xavier and He initialization.
The experiments seem to be limited to specific test functions. Performance on such few benchmark functions are not convincing
The paper discusses the potential application of GEO in other areas of machine learning, such as reinforcement learning. However, it does not provide any empirical evidence or case studies to support these claims. Including real-world applications or case studies could strengthen the paper's significance and practical relevance.
The paper does not clearly outline the limitations of the GEO method, which could be beneficial for future research and application of the method.
The paper mentions the tendency of GEO to collapse towards one side while optimizing certain functions, but it does not delve into why this happens or how it could be mitigated. A more in-depth analysis of this issue could improve the paper's quality.


Limitations:
Societal impact of the work is not discussed in this paper. Furthermore, limitations of the proposed technique are not discussed clearly.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper investigates a new integrated optimization method targeting at black-box optimization within high-dimensional spaces scenario, called Generative Evolutionary Optimization (GEO).
Different from the popular black-box optimization method - Bayesian optimization, GEO exhibits a linear time complexity.
Intrinsically, Geo a black-box optimization method that combines an evolutionary strategy with a generative surrogate neural network (GSN) model,
and the two basic components could function in a synergetic manner.
Specifically, evolutionary strategy helps to deal with the stability of the surrogate network training for GSN,
while GSN improves the mutation efficiency (sample efficiency) of the former.
Since the fitness results are combined and ranked using non-dominated sorting in GEO, it can be applied to multi-objective scenario.
Besides, the age evolution strategy is leveraged to dominated sorting step when the target function is stochastic.
Finally, the experimental findings reveal that GEO can accomplish the mentioned objectives: optimizing convex, high-dimensional, multi-objective, and stochastic target functions while maintaining O(N) complexity.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. This paper is well-written and easy-to-follow, and the following parts are highlights: technique explanation, limitation analysis, high-level summary.
2. The technical design (GSN, ES, training stability) is reasonable, and the experimental evaluation is clear.
3. The key design in the cooperative framework is novel, which integrates the strengths of both GSN and ES while mitigating their weaknesses.


Weaknesses:
1. The specific parameter settings are not clear.
2. The reviewer suggests that in the discussion chapter, the related multi-objective high-dimensional solutions from Bayesian optimization community could be analyzed in terms of time complexity or efficiency if possible.
3. The test function in the experimental evaluation is limited, and this hamper the evaluation confidence. As mentioned by the authors, more test functions from different domains (maybe a fair benchmark) should be included to evaluate the performance of GEO.
In addition to Ackley, Rosenbrock and Styblinski-Tang, there are many objective function family, including CONSTR, SRN and so on.


Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, a black-box optimization approach is proposed that combines an evolutionary strategy (ES) with a generative surrogate neural network (GSN) model. This integrated model is designed to function in a complementary manner, where ES addresses the instability inherent in surrogate neural network learning associated with GSN models, and GSN improves the mutation efficiency of ES. From the overall view of this paper, the authors basically expressed clearly the point of innovation and the proposed algorithm.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The organization of this paper and the technical details of the proposed method are clear and easy to follow.


Weaknesses:
Theoretical derivations and proofs are lacking, and the validity of the method is difficult to be supported.

Limitations:
The relevant limitations are described, but not in depth and specific enough.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper introduces a new method called Generative Evolutionary Optimization (GEO) that aims to address the challenges of black-box optimization in high-dimensional problems. The authors highlight that existing algorithms, such as Evolution Strategies (ES) and Bayesian optimization, have limitations when it comes to optimizing high-dimensional, non-convex problems while maintaining linear time complexity. They propose GEO as a combination of ES and Generative Surrogate Neural networks (GSNs) to achieve better performance in terms of stability, mutation efficiency, and optimization in high dimensions. The paper outlines the goals of GEO, discusses related works (L-GSO and EGAN), presents the methodology, and provides experimental results showing GEO's superiority over traditional ES and GSN in higher dimensions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper addresses an important problem in black-box optimization: optimizing high-dimensional, non-convex problems while maintaining linear time complexity
- The introduction provides a clear overview of the challenges faced by existing algorithms and the potential benefits of using GSN-based approaches like GEO
- The goals of GEO are well-defined, and the paper sets the stage for discussing the methodology and experimental results
- Combining EA with GSN is novel and interesting


Weaknesses:
- Some simple ES algorithms, such as OpenAI-ES [1], can optimize about 100k parameters in their paper; it is used to optimize the weight of the policy network. Although the idea of this paper seems novel and interesting, I am not sure that the 10k params can be called high-dimensional. 
- The main results are shown in Figure 3, but it is unclear which function is used for 3-a), and the figure is not explained in the manuscript. 

[1] Salimans, Tim, et al. ""Evolution strategies as a scalable alternative to reinforcement learning."" arXiv preprint arXiv:1703.03864 (2017).


Limitations:
- In the single-objective function, I think more algorithms should be compared to the proposed algorithm. 

Rating:
4

Confidence:
3

";0
3o4jU8fWVj;"REVIEW 
Summary:
The authors propose EquiformerV2, which is an improvement over the original Equiformer architecture. The main improvement is using a more efficient parameterization of the tensor products used in Equiformer which is computationally expensive for higher order representations. The more efficient parameterization involves using SO(2) linear layers instead of SO(3) tensor products. Moreover, they have three architectural improvements – adding an extra layer norm during attention, S2 activations instead of gates and ""separable"" layer normalization which differs from the previous layer normalization in terms of the denominator used. They perform experiments on OC20 to show improvements compared to other models in the literature. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Overall, the idea of using computing tensor products more efficiently using SO(2) linear layers makes good sense in its application to Equiformer in order to scale it up via higher order SO(3) representations. Empirical work around improving the architecture by appropriate use of different type of layer norms and activation functions is also valuable. The components feeding into this model aren't always original, but that shouldn't detract from the significance of putting together the full model and showing that it improves performance on a benchmark. The clarity of the writing is usually good but there are places where it can be improved (see below).

Weaknesses:
The main weakness is insufficient comparison against the original Equiformer architecture. There are four differences versus the V1 architecture as far as I can tell: (1) the more efficient parameterization of tensor products via the ideas of eSCN, (2) an extra layer norm applied to scalar features in the attention module, (3) separable S2 activation instead of gates and (4) layer normalization with a different way of computing the denominator (called ""separable"" layer normalization in the paper). 

- There is an ablation in Table 1(a) on one of the datasets looking at the effects of changes (2), (3), and (4). It is unclear how much of the difference in performance (specially, on energies) is statistically significant. I understand training each model is expensive, but since the experiments in this table are on the smaller S2EF-2M dataset, isn't it possible to have error bars here?
- Why isn't there a comparison in Table 1(a) of the effect of incorporating change (1) in EquiformerV2? The SO(2) linear layers replacing the tensor product are in principle as expressive as the tensor products but the optimization dynamics can be different since the parameterization of the architecture is different. It would be good to see a side-by-side comparison of Equiformer's V1 and V2 when keeping other architectural hyperparameters fixed (e.g. number of channels, maximum representation order etc.)
- It would be good to include EquiformerV1 performance in the other sub-tables of Table 1 too, in order to see the cummulative effect of the changes to the original architecture.
- Similarly, it would be useful to see EquiformerV1 performance in Table 2 and 3 as well (possibly with a lower $L_\text{max}$ but with other hyperparameters tuned).
- How does the setup for IS2RE in Table 2 differ from EquiformerV1's Table 3? Is one with relaxations after training on S2EF and the other is through direct energy prediction? A side-by-side comparison under the same setup would help a lot here. 
- How necessary are the higher orders compared to increasing the number of layers or channels? SO(2) linear layers instead of tensor products will improve the efficiency for lower orders too, so can we make up for the performance of higher orders by instead increasing the number of layers or channels?
- Have you benchmarked EquiformerV2 on QM9 where EquiformerV1 has already been benchmarked? It's a dataset that is different from OC20 in various ways, so it would be useful to see a comparison.

For other suggestions for improvements, see the questions below. 

Equiformer(V1): https://arxiv.org/abs/2206.11990

Limitations:
Limitations and broader impact are adequately addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper provides a new equivariant graph neural networks named EquiformerV2 to enhance the original Equiformer performance. It uses four new modules. The first module is to use the convolution in the eSCN (https://arxiv.org/abs/2302.03655) to replace the depth-wise tensor production accelerating the speed. The second module is the separable $S^{2}$ activation using the spherical grids in SCN (https://arxiv.org/abs/2206.14331) to encourage the non-linearity. The third module is the separable layer normalization which uses the variance of all equivariant feature $\ell \geq 1$ to do normalization. The fourth module is the attention re-normalization using a layer normalization before the non-linear function of the attention score branch.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well organized and written. The figure clearly shows the modification on the model architecture.
2. The experiments in Table 2 supports the proposed model architecture can achieve SOTA performance on the OC20 All training sets as well as OC20 All+MD. For example, on OC20 All, test energy MAE is improved 13meV in S2ET test and 25meV in IS2RE test. Such improvement is great. These two datasets usually take extensive training time to perform experiments on them. From the Throughput metric, the EquiformerV2 has better training efficiency compared to current baselines.


Weaknesses:
1. As a suggestion, it will be better if efficiency study includes both training time and inference time. Computational complexity metric such as FLOPs or MACs can help measure the inference complexity.
2. The description of spherical grid is not very clear in the paper. Although it is introduced in the SCN paper, I think a brief introduction can help people understand why such operation can enhance the non-linearity.


Limitations:
1. As an suggestion, to comprehensively study the improvement on original Equiformer, it will be better if there is a comprehensive experiments on the original datasets such as QM9.


Rating:
5

Confidence:
5

REVIEW 
Summary:
In this paper, the authors proposed EquiformerV2, which is an equivariant network for 3D molecular modeling. The EquiformerV2 is built on the Equiformer with several architectural modifications: 1) replace SO(3) convolutions (tensor product operations) with efficient SO(2) counterparts from eSCN; 2) Attention Re-normalization; 3) Separable S2 activation; 4) separate Layer Normalization. These changes enable EquiformerV2 to achieve good performance on the large-scale OC20 benchmark and also the new AdsorbML dataset.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The targeted problem is of great interest to the community. The EquiformerV2 provides another attempt to enlarge the maximum degree of irreducible representations and obtain performance gains on large-scale DFT benchmarks.

2. Good empirical performance. On the OC20 benchmark, the EquiformerV2 achieves state-of-the-art performance on the Structure-to-Energy-Force task. The model trained on this task further serves as a force-field evaluator to achieve strong performance on IS2RS and IS2RE tasks. The EquiformerV2 outperforms the compared baselines on all these tasks, especially on force prediction. Additionally, it also improves the success rate a lot on the AdsorbML dataset.


Weaknesses:
1. The novelty of integrating the eSCN convolution and S^2 activation into the Equiformer is limited. Among the proposed architectural changes, the eSCN convolution is the key component to enable Equiformer to use irreducible representations of higher degrees, and the S^2 activation also replaces all non-linear activations. However, these design strategies should be mainly credited to the eSCN work.

2. The motivation for the other architectural modifications should be thoroughly clarified. First, lines 174-175 in Section 4.2 suggest the ""less well-normalized"" issue, which motivated the authors to propose re-normalization and Separable Layer Normalization. It is better to provide further quantitive evidence to reveal how such an issue affects the model's performance, and why these modifications could remove or mitigate such effect. Second, the authors proposed Separable S^2 Activation because the original S^2 activation would make the training process diverge. However, it is hard to understand why such separable modifications could make the training process stable. Is there any further essential reason behind such a phenomenon? It is suggested to provide further analysis on such modifications.

3. More analyses are necessary if the computational resources are acceptable. First, the authors did not provide a performance comparison between whether using the eSCN convolution. Although the eSCN convolution is equivalent to the SO(3) counterpart, the computation processes of these two parameterizations are different. Second, it is suggested to further report the number of model parameters and memory costs for the EquiformerV2 and the compared baselines in Table 2. Third, how does EquiformerV2 perform on the IS2RS and IS2RE using the direct setting (use or not the denoising setting) that is the same as EquiformerV1?

Overall, the major weakness of this work lies in the novelty, unclear motivations, and incomplete analyses. If the authors could well address the above concerns, I would like to increase my scores.



Limitations:
The authors carefully discuss the broader impact and limitations of this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper propose EquiformerV2, a advance verison based on Equiformer and eSCN structure extend to higher degree representations, which achieve better performance in force and energy tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written and organized, presenting a clear and coherent structure throughout. The introduced EquiformerV2, an upgraded version of the original Equiformer, including three architectural improvements: attention re-normalization, separable S^2 activation, and separable layer normalization. These enhancements contribute to the SOTA performance in  OC20 dataset. 
The proposed model achieves a high degree representation with efficiency, as outlined in the paper. The authors also provide a comprehensive ablation study to support the necessity of these modifications, effectively highlighting their respective contributions to the overall performance improvement.

Weaknesses:
* A few spelling errors. For instance, in Section 6, the word ""acknolwdge"" etc.  Along with any other mistakes found throughout the manuscript.

* The experiments conducted in this study primarily utilize the OC20 dataset. While this dataset is relevant, it is essential to note that there are various other DFT-based datasets available that could provide a more comprehensive evaluation of the proposed architecture. Such as OC22, OQMD[1,2], SPICE[3], and PCQM4Mv2 etc.

[1]  Saal, J. E., Kirklin, S., Aykol, M., Meredig, B., and Wolverton, C. ""Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD)"", JOM 65, 1501-1509 (2013). 
[2]. Kirklin, S., Saal, J.E., Meredig, B., Thompson, A., Doak, J.W., Aykol, M., Rühl, S. and Wolverton, C. ""The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies"", npj Computational Materials 1, 15010 (2015).
[3] SPICE, A Dataset of Drug-like Molecules and Peptides for Training Machine Learning Potentials


Limitations:
same above.

Rating:
5

Confidence:
3

";0
uOEeui0rL7;"REVIEW 
Summary:
The paper introduces Human Guided Exploration (HUGE), a system designed to integrate human feedback within the Goal-Conditioned Reinforcement Learning (GCRL) environment, offering a cost-effective and straightforward method for learning diverse robotic tasks through actual human interaction. The authors' primary objective is to strike a balance between overexploration and underexploration in GCRL tasks by incorporating human-in-the-loop assistance. Consequently, human feedback is utilized to ascertain the states requiring further exploration, building on the traditional GCRL algorithms and the GO Explore exploration framework. In particular, a target distance estimation function is trained using binary feedback, facilitating the generation of a more precise subgoal. Subsequently, the Goal-Conditioned Supervised Learning (GCSL) training paradigm is implemented to establish an interaction strategy with the environment. Experimental results in Mujoco and Pybullet demonstrate that HUGE outperforms the preceding GCRL algorithm and Human-in-the-loop algorithm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The integration of human feedback into the GCRL setting is well-conceived. Hence, the motivation is clear, and the proposed method is, in general, logical and sensible.
- This technique facilitates a simple interface between the human labeler and the algorithm, in which the human supervisor provides binary evaluations to establish which state-goal pairings are closer in comparison to others.

Weaknesses:
- The paper presents a degree of innovation that is somewhat restrained, as it essentially combines modified versions of existing methodologies (GCRL, Go Explore, and Human Preference). The foundational GCRL algorithm used is the previously established GCSL, while the exploration component is derived from the Go-Explore paradigm. It is important to note that GCSL paired with Go-Explore could have operated independently without the need for human feedback. 
- As per the previous discussion, it's not unexpected to see improved results when human feedback is integrated into the Go-Explore process, as it introduces more a priori knowledge. The implementation of binary feedback can help to decrease the exploration space by learning to rank and establishing a subgoal model.
- This strategy essentially transforms the behavior cloning challenge into a binary human feedback problem. While this appears logical, it makes the motivation somewhat ambiguous and makes specific presumptions about the task at hand. Consequently, the work lacks a degree of originality and novelty. The three fundamental components of the proposed method: the ""goal-conditioned policy"", ""hindsight relabeling"", and ""learning reward models"" from human preference, have all been previously suggested in earlier papers.
-  I think the tasks for evaluation are borderline novel and perhaps not hard enough when we consider a setting where human feedback can be utilized (these tasks are still challenging for RL-based methods learning with dense/sparse rewards). The maze tasks, robotic manipulation tasks are commonly seen in literature (in fact I am surprised PPO is completely failing on some of these tasks, I have personally trained PickNPlace tasks with a RL-based method with a properly shaped reward - Otherwise the authors can also explore other SOTs such as TD3 or SAC) . These tasks are not seen as a fundamental challenge for present learning methods. The main result of the paper is comparison of the proposed method against RL algorithms like PPO and LEXA in terms of sample efficiency. I prefer to see longer horizon tasks where an RL agent with a dense reward cannot easily solve.  I also did not see comparison results for the Sim2Real tasks nor for PickNPlace.

Limitations:
-  The work lacks novelty.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper focuses on the exploration problem in decision-making tasks. Previous works try to leverage human guidance with constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this paper, the authors propose Human Guided Exploration (HUGE), which is able to leverage low-quality feedback from non-expert users. Specifically, the key idea is to separate the challenges of directed exploration and policy learning. Human feedback is only used to direct exploration, while self-supervised policy learning is used to independently learn unbiased behaviors from the collected data. The task for human annotators is just to select which of two states is closer to a particular goal. Then this ranking will be used to train a distance function for goal selection. The experimental results on robotic navigation and manipulation tasks in both simulation and real-world robots demonstrate the advantages of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The idea of the proposed method is simple yet effective. Separating the exploration and policy learning dramatically reduces the annotation effort and influence of noisy annotation on policy learning. Using human preference to learn the distance function is also better than directly learning the reward function.
2.	Thorough evaluation of multiple environments, including both simulation and real-world tasks in robotic navigation and manipulation domains. The experimental results show that HUGE outperforms other methods by a large margin. 


Weaknesses:
1.	Figure 1 provides limited information. It is hard to find that human feedback is noisy and asynchronous. The tasks of pick-and-place and drawing do not contain detailed explanations. This figure can be improved by adding a comparison between previous works that require high-quality data and their method.
2.	The method may need to be evaluated on more complex real-world tasks. The authors argue that Novelty-based exploration performs task-agnostic exploration of the entire state-space, thereby over-exploring the environment. But it seems that the environments used in this paper do not contain very large exploration space. 


Limitations:
No limitation is discussed in the paper. One potential limitation is that the human annotation of the preference of states may be difficult for tasks that require logic reasoning. The experiment environments used in this paper mainly focus on L2 distance, which is easy for humans to quickly select the state close to the goal.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces HUGE, a Reinforcement Learning algorithm that makes use of human preferences to guide the selection of partial goals.

HUGE expands upon Goal-Conditional Supervised Learning (GCSL) by improving the goal selection method using noisy labels from humans to form a model, $f_\theta$, of distances to a goal state. This model $f_\theta$ is then used set goals for the GCSL algorithm to learn in a self-supervised fashion, thus bootstrapping learning.

The paper features an extensive evaluation of HUGE, with 6 synthetic tasks, 2 real robot tasks and one task trained from crowd-sourced feedback. Results show that HUGE outperforms a large number of baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* HUGE combines intuitions from GCSL and GoExplore, and can take advantage of asynchronous human preferences.
* The paper is really well written and is easy to follow.
* The paper presents a thorough evaluation. I find it particularly that HUGE Was able to train a real robot policy in under 30 hours with just 130 annotations.
* HUGE can take advantage, but does not require, expert demonstrations through Behaviour Cloning.

Weaknesses:
* It would have been more representative to choose PEBBLE [1] rather than Christiano et al. (2017) as a more representative example of the performance of Preference-based Reinforcement Learning.
* Some of the results are buried in the appendix. For instance section 5.2 does not mention what was the task analysed are. From Figure 6. left, I assume it's the Kitchen task from D4RL. Still, neither the final performance nor the task are mentioned in the main text. 
    * Similarly, the main results of the analysis comparing the final performance and the number of annotations should be included in the main text, since it provides readers with an estimation of how many labels they may need for their task.

**Post-rebuttal update**

Authors added a comparison against PEBBLE, where HUGE outperforms it.

Limitations:
The limitations have been adequately addressed.


Rating:
7

Confidence:
3

REVIEW 
Summary:
Broadly, the paper addresses the challenge of learning multi-stage robotic navigation and manipulation tasks in simulation. 

The paper frames several benchmark tasks as goal-conditioned RL (GCRL), and they present a novel method (""HUGE"") for leveraging human preferences collected during learning. In particular, the preferences are used to train a goal-selector model that guides (biases) exploration. For policy-learning, they use goal-conditioned supervised learning on the collected replay buffer, without requiring a handcrafted task-specific reward.

Compared to [1], HUGE also uses hindsight relabeling to learn from a replay buffer despite a sparse reward, but it differs in that it biases sampling from the replay buffer based on human preferences. Compared to [11], HUGE also collects human preferences iteratively (intertwined with learning). However, HUGE doesn't use its collected human feedback to learn a reward or otherwise directly bias the policy. Instead, the feedback is only used to bias exploration. Compared to [17], HUGE also has an exploration phase that builds an archive of trajectories. However, HUGE chooses promising goal states and generates trajectories using its goal-condition policy, while [17] chooses promising start states based on novelty and can generate trajectories with various exploration policies (random, epsilon-greedy, novelty-based).

The authors evaluate on four simulated manipulation tasks, two simulated navigation tasks, and two real-robot tasks. They compare to several baselines including the related work mentioned above as well as PPO (with dense and sparse reward variants).

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
Their method doesn't require a handcrafted reward and it's able to leverage noisy, infrequent human feedback. These strengths are relevant and valuable to this domain.

Evaluation on a variety of simulated and real tasks.

Impressive crowdsourcing infrastructure and diversity of annotators.

Interesting, well-explained adaption of real-world tasks to fit their use of goal-conditioned RL.

Weaknesses:
Lack of clear, fair numeric results on task performance across HUGE and the baselines:

= In Figure 5, I can only compare task performance after a given number of steps. A table with a performance number after training each approach to convergence would be more clear and would better support the claim from your text ""HUGE beats prior work on the proposed long horizon tasks in simulation"".

= It's not clear why some tasks were trained for millions of steps and others not.

= The paper text conveys that Figure 5's various approaches are using a poorly-tuned reward function (""we did not do any reward finetuning""). In particular, this makes the PPO results questionable.

= This section also uses synthetic human preferences derived from your reward function, instead of real human preferences. This confuses me and contradicts the earlier premise of HUGE. HUGE with this synthetic source of preferences would appear to be another RL method that utilizes a handcrafted reward function and not human feedback. This muddies your comparison with other approaches, especially [11] which is intended to be guided by real (not synthetic) human feedback.

= It seems like other baselines might benefit from the addition of behavior-cloning pretraining, as you did for ""BC + Ours"".

Limitations:
No concerns here

Rating:
6

Confidence:
4

";1
qY7UqLoora;"REVIEW 
Summary:
The work derives a conservation for the dynamics of the GAT gradient flow during training. The conservation law is used to explain why it is challenging to train deep GAT models and in particular why a large portion of parameters do not change much throughout training. A new initialization scheme is introduced to mitigate these issues.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Training GATs is notoriously problematic and this is especially the case with many layers. The work takes an interesting angle of trying to further explain why this is the case by studying conservation laws induced by the gradient flow during training. The observation that weight gradients must be small in order to satisfy such a conservation law is very interesting and practically useful. The balancing algorithm proposed is also practically useful and seems to perform well especially with deep GATs.

Weaknesses:
While the work focuses on GAT, it would be interesting to have a statement on more general classes of MPNNs. There are some faint connections in the paper to GCN for instance, but it would be interesting to have a more general result as well. Experimentally this might also be interesting to see if such an initialisation can be adapted to different types of MPNNs. 

Limitations:
The authors address the fact that the derived theory defines a conservation law that does not explain detailed dynamics but still remains relatively coarse. As such it is not able to explain some phenomena as why attention parameters change most on the first layer. I still believe that the work is a good step in helping explain phenomena in this direction. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on the parameter struggle training problem of the well-known GNN structure GAT. The authors propose to alleviate the issue via parameter norm balancedness. A conservation law is derived for GATs with positive homogeneous activation function as the theoretical support for the parameter norm balancedness based initialization method. The authors also conduct extensive experiments to prove its effectiveness and fast convergence property.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The problem is well identified.
- The proposed theories are solid, and well serves the problem.
- The proposed initialization method is simple yet effective in improving accuracy and speeding convergence.

Weaknesses:
- Some explanation skips details and may raise confusions. For example, the equations in line 140-142.
- Some formats could be improved for better illustration, such as larger figure size and fitted table style.


Limitations:
The author may address the limitation from the generalization of the proposed method beyond the GAT structure. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose a theoretical analysis of the initialization of GATs and their impact on the performance of such networks, focusing on the performance vs depth aspect.

The authors propose an initialization algorithm, that starts from a random initialization and modifies the initial random weights to adhere to the findings of the theoretical analysis.

The authors then show the impact of their proposed initialization on several node classification datasets. Specifically, they show that by initializing GATs with the proposed algorithm, deep GATs can be trained to achieve better than standard (Xavier) initialized GATs.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper addresses a real issue with GATs - the degradation of performance when more layers are added.

The theoretical analysis seems correct to me and is supported with experimental results as well as the inspection of actual training artifacts and details (e.g., the gradients of the GAT layers).



Weaknesses:
The paper can be slightly better written, in terms of organization. I think that adding more paragraphs/subsections to better separate between the parts of the paper can help to ease the reader.

The paper lacks a few relevant citations that also consider graph neural networks as gradient flow (see [1][2][3]). However, their focus is not on the initialization of GATs, and therefore the paper here is novel on its own.

While the experimental results are compelling and show the benefit of the proposed method, I think that the authors should also include comparisons with other methods.

[1] GRAND: Graph Neural Diffusion

[2] Understanding convolution on graphs via energies

[3] Improving Graph Neural Networks with Learnable Propagation Operators

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work proves a conservation law for GAT architectures, which is similar to conservation laws shown for fully connected networks. The conservation law shows a simple connection between the norms of weights of two consecutive layers. Using this law, an intuitive explanation is given for the trainability issues of deep GAT architectures. To mitigate this issue, a balanced initialization is suggested which shows improvements in generalization and training speed of deep GATs.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novel conservation law for GAT architectures.
2. The paper and technical details are well written.


Weaknesses:
There are no major weaknesses.
1. The motivation for using the balanced initialization and why it should improve only for deep networks is not so clear.
2. The experimental results are not complete.

See Questions for more details.


Limitations:
Limitations are not discussed. Maybe it would be helpful to add a section on this.

Rating:
7

Confidence:
3

";1
ooXpTZYwXa;"REVIEW 
Summary:
This paper proposes an in-text learning method for multi-task 3D shape analysis. It handles several tasks such as denoising, part segmentation, reconstruction and registration with a single pretrained masked point model (MPM).  The authors also claim previous MPM methods introduce information leakage during pretraining. To solve this, this paper proposes a JS module, which facilitates the model to learn the inherent association between input and target and streamlines the learning process.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.	In-context learning for point clouds is a new and interesting topic.
2.	The unified modeling of different 3D shape analysis tasks makes sense.
3.	The performance of the proposed method is satisfactory.

Weaknesses:
1.	The writing of this paper is not clear. (3.1) The authors should add more description on the task definition, such as which kind of model should be pretrained, is it fixed during inference, etc.  (3.3) It is claimed that previous methods will bring information leakage. However, the explanation is confusing. The authors should clearly show this in Figure 2.  Moreover, as a core technical contribution of this paper, JS module should be clearly demonstrated. It is hard for me to understand why using the same FPS Idx.
2.	The JS Module seems to be straight-forward (and confusing). As a simple technique, more insight should be explained.

I will consider to improve my rating if my concern can be properly solved.

Limitations:
see weakness

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel framework, named Point-In-Context, designed explicitly for in-context learning 3D point clouds. The authors conduct extensive experiments to validate the versatility and adaptability of the proposed methods in handling a wide range of tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper explores an interesting topic -- how to do in-context learning for 3D understanding; and it demonstrates a reasonable way to achieve it, addresses some issues like the ""position information leakage"", and shows some positive results.

Weaknesses:
The reviewer is concerned about how well this framework can generalize, its general usefulness, and its scope, since to some extent the performance may depend on the ""prompt"". In reality, it might be hard to choose a proper prompt for it and the performance may not be stable, and it kind of involves some extra tuning effort compared to using the model which is directly trained for that task.

Limitations:
refer to the weakness part

Rating:
6

Confidence:
5

REVIEW 
Summary:
This work is conducted toward in-context learning for 3D point cloud data. Similar to 2D in-context learning, the authors first define and construct the in-context learning 3D dataset covering reconstruction, denoising, registration, and part segmentation tasks. To avoid information leaking during masked point modeling, a joint sampling module that samples correspondingly from inputs and outputs is proposed. Extensive experiments have been conducted, and interesting results have been obtained.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- This works targets in-context learning for 3D point clouds, which is a very relevant but underexplored problem.
- The setup of in-context learning in 3D and curation of the in-context learning dataset is helpful to the community.
- Extensive experiments have been conducted, where results demonstrate the superiority of the methodology design, which also indicates that in-context learning in 3D is non-trivial.
- Code and dataset are promised to be released. Good.

Weaknesses:
- For 2D or NLP, in-context learning is generally free-form, targeted at various tasks. However, for 3D, due to a seriously lacking of data, it only shows very limited applications. It seems not practical in real-world deployments.
- The novelty and technical contribution of the method part is somewhat limited. The joint sampling module is relatively straightforward. The model and loss function is the same as in previous works. However, the contribution of setting up the 3D in-context learning baseline is good.
- Missing citations or comparison: For 3D MIM methods, important recent cross-modal representation/prompt learning methods should be discussed or compared [Dong et al., 2023; Qi et al., 2023]; For the loss function, Chamfer Distance should be cited [Fan et al., 2017]; For in-context learning, some works are missing [Sun et al., 2023; Balažević et al., 2023].
- The compared methods Point-BERT and Point-MAE are a bit old and are not SOTA. I wonder what about the comparison to other cross-modal 3D MIM methods like ACT [Dong et al., 2023], I2P-MAE [Zhang et al., 2023], and ReCon [Qi et al., 2023]? I think it is important to conduct solid comparisons to more advanced methods.
- Minor suggestion: There are not many formulations but the current formulation is not neat. Please improve the presentation quality including the formulations. For example, pure texts should not be italic in equations. Besides, the writing is overall okay but could be improved.

[Dong et al., 2023] Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning? In ICLR.

[Qi et al., 2023] Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining. In ICML.

[Fan et al., 2017] A point set generation network for 3d object reconstruction from a single image. In CVPR.

[Sun et al., 2023] Exploring Effective Factors for Improving Visual In-Context Learning. arXiv preprint.

[Balažević et al., 2023] Towards In-context Scene Understanding. arXiv preprint.

[Zhang et al., 2023] Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. In CVPR.

Limitations:
The authors have discussed the limitations, and I think it is somewhat okay for this work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
Inspired by in-context learning in NLP and 2D vision tasks, this paper aims to explore the in-context learning in the 3D point cloud. The authors present Point-In-Context, which is a 3D mask point modeling framework. Meanwhile, to handle the data leakage issues, the authors also present a simple solution, named joint sampling. Then, extensive experiments are carried out in modified ModelNet40 dataset. The performance of two different baselines is good to other specific baselines.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The proposed point in-context is the first work that explores the in-context ability in the point cloud understanding, which is novel and interesting to me. The authors show the effectiveness on four different tasks, including rotations, registration, denoising and part segmentation. 

2. The overall writing and motivation is good, clear, and easy to follow. 

3. The proposed joint sampling can effectively solve the data leakage problems. 

4. The experiments results are good. The ablation studies are extensive. The authors re-benchmark several representative works, including SOTA single model and multi-task models. The analysis on visual point example is good and convincing. 

5. This work may bridge the connection between 3D point cloud and 2D image in-context learning. 


Weaknesses:
1. Are there any other solutions to replace joint sampling to handle the data leakage problems?

2. The proposed approach is verified effective on simple scene on ModalNet40. The ability to extend to large scale scene including in-door point segmentation is unknown. 

3. What are the results of two combined prompts: denoising and part segmentation jointly?

Limitations:
Yes.

Rating:
7

Confidence:
5

";1
cCYvakU5Ek;"REVIEW 
Summary:
This work investigates the geometry of hidden representations of transformer models trained via a self-supervised task on either amino acid prediction in proteins or pixel prediction in images. The work uses two tools to understand this geometry: intrinsic dimension (ID) (estimated via the TwoNN algorithm) and neighborhood overlap. The paper shows that as data passes through the layers of a transformer, both the ID and neighborhood overlap change in characteristic ways that the work connects with the extent to which the representation is organized by the semantic content in the data. The paper validates this claim by looking at the extent to which each data point shares neighbors that belong to the same semantic class, showing that this peaks at layers when ID is low and changes in neighborhood overlap is also low. The work then speculates that these observations could provide a way of identifying the best representations to use for downstream tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- While there is a rich literature exploring hidden representations of deep learning models, most works continue to focus on CNNs or MLPs with supervised training on medium sized datasets. Given the growing importance of transformers to NLP and vision and the increasing use of self-supervision for large-scale training, there is a need for works that explore how these approaches impact a model’s hidden representations. Thus, this is a welcome work that will doubtless be of value to researchers training large transformers via self-supervision. 
- The paper contains careful analysis of the experiments (as opposed to other works which all too frequently just list summary statistics). The conclusions which are reached are all fairly-well supported within the scope of the experiments that were performed. Crucially, the two metrics that are used, intrinsic dimension and neighborhood overlap, reinforce each other’s conclusions. This increases the believability of the results significantly.
- The paper is able to suggest some practical value (identifying layers of the transformer that capture the most semantic content) in their scientific observations (patterns in representation geometry in large-transformers), helping to connect practice with theory.


Weaknesses:
- The experiments in this work all use transformer models trained via self-supervised tasks based around data reconstruction (as opposed, for example, to a contrastive learning task). While most of the phenomena observed in this paper is attributed to the “large transformer” architecture, this reviewer wonders if some of the conclusions are also contingent on the self-supervision task. For example, does one see similar behavior in a large vision transformer that has been trained in a supervised manner. Disentangling which of the conclusions are due to architecture and which are due to training method would make the work significantly stronger.
- Many of the claims in the work rely on comparison between the shapes of curves which plot intrinsic dimension of data between layers. While the claims seem mostly reasonable given the figures, it would make the work stronger if more quantitative measures were used to, for example, compare ID curves. This sort of automation might also make it easier to compare against a broader range of models and datasets. While this reviewer thought that the use of both protein and image datasets was a strength of this work, comparing against other types of transformer architectures and datasets would reinforce the conclusions.
- The main content of the paper consists of discussion of several figures. This reviewer felt that the way this discussion was written/organized, it was easy for the reader to get disoriented and forget the main points already established. Possibly making the discussion more concise or better highlighting the main takeaways from each section would help the reader to mentally organize the primary findings of the work. Being more succinct might also allow further experiments to be included.

Nitpicks

- The abstract describes a transformer as being composed of a “sequence of functionally identical transformations”. To this reviewer, functionally identical transformations would be transformations that behave the same way on the level of functions (for the same inputs, they give the same outputs), but may be parametrized in different ways. The layers of a transformer are instead transformations that all belong to the same functional family, though each layer is generally a different function.
- Some sentences in this work have an over-abundance of commas. For example, “In this work, we systematically investigate, in some self-supervised models, fundamental geometric properties of representations, such as their intrinsic dimension (ID) and neighbor composition.” To improve the flow of the paper, it would be good to find such sentences and remove some of the commas.
- Line 66: “…where the annotation is very scarce” -> “…where the annotations are very scarce.”
- Reading this work would have been easier if the figures were closer to their corresponding analysis. As it is, the reader must flip between pages frequently to validate claims.
- Line 110: “More in detail,” -> “In more detail,”.


Limitations:
This reviewer believes that some limitations could have been discussed, including:
- The use of intrinsic dimension estimators, which can provide misleading feedback in certain situations.
- A limited number of different model types and datasets.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents an analysis of the internal representations of transformers trained with self-supervised learning, such as masked language modeling, from two perspectives: intrinsic dimension and adjacency structure. Experiments on two datasets - protein sequences and image data - revealed that the intrinsic dimension within the transformer has two peaks, one in the early layers and another in the latter layers. This result is qualitatively different from previous convolutional networks trained with supervised learning. Notably, the intermediate layer, where the intrinsic dimension is minimized, is the most suitable for categorical discrimination in downstream tasks.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
This paper presents a notable analysis of the intrinsic dimension of transformer models trained with self-supervised learning. While many approaches have been used to analyze the internal dimension of deep neural networks, most have focused on convolution-based networks. The discovery of two peaks in the internal dimension is intriguing, offering insightful contributions to the community. The results are clearly presented, and the paper is well-structured, making it easy for readers to follow the logical development. Of particular interest is the finding that such representations can spontaneously appear in models trained with masked modeling, even without an explicit bottleneck structure in the model.

However, this interesting result also raises many considerations and discussions. There are points in the current paper where sufficient evidence to support the authors' claims is lacking and further room for discussion remains. These points will be discussed in detail below.

Weaknesses:
The paper's experimental comprehensiveness raises two issues. The first issue revolves around the focus on internal representations of transformer models trained with self-supervised learning, with the assertion that the evolution of the internal dimension displays two peaks. However, it appears that sufficient experiments have not been performed to identify the causes of this occurrence definitively. An ablation study would be incredibly beneficial in distinguishing the elements that stem from the model architecture (transformer) and those arising from the training protocol (masked/autoregressive modeling).

Secondly, the paper state in the introduction that the adjacency structure is “rearranged” at the peak of the representation’s internal dimension, regardless of the dataset domain. Yet, upon the qualitative comparison of Figures 1 and 2, it is challenging to confirm a consistent change in the internal dimension and the overlap of the adjacency structure between layers in the image domain experiments. Therefore, the current portrayal of the contributions gives the impression of over-claiming. As asserting a clear correlation in qualitative behavior is difficult, a comparison with some control conditions should be made. Additionally, it would be desirable to have a discussion concerning the influence of differences in the data domain.

Limitations:
Given that this paper is not proposing a new method but focusing on analyzing the internal representation of existing methods, it may not strictly apply to a discussion on limitations. The estimation of the internal representation is based on a method proposed in previous research, and the applicability of this method has been sufficiently demonstrated. However, as mentioned in the Weakness and Question sections, there are concerns about the comprehensiveness of the experiments in this paper. Improvements in these points would lend greater significance to the authors' claims.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work focuses on characterizing the geometrical and statistical properties of data representations across the hidden layers of large transformer models. Specifically, it demonstrates the similarity in the evolution of geometric properties, such as the intrinsic dimension, between image-based and protein-based transformers. Additionally, the work proposed an intuitive, unsupervised strategy to identify the most semantically rich representations of transformers. The effectiveness of this strategy is demonstrated by the SOTA performance achieved by leveraging previous work and replacing the last layer with the layer identified in this study. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- This work presents a novel strategy to select the layers that produce the most semantic representation to be used in downstream tasks, opening many possible applications. Moreover, the method seems to be general and could be exploited in other architectures where the layer choice is currently arbitrary, such as classifiers.
  
- This work demonstrates state-of-the-art results in identifying protein relations by leveraging existing methods. It simply involves swapping the previously employed last layer with the layers that maximizes the semantic content identified through the strategy proposed in this work (Figure 5, right).

- The writing is extremely clear and easy to follow, presenting intuitive ideas, a solid experimental setup and convincing results.

- This work does not involve training models but instead analyzes publicly available models, completely avoiding any possible bias that could have been introduced in the training process.

- Among the others, the insight that large transformers behave essentially like sophisticated autoencoders (lines 337-338) is insightful and may serve as a source of ideas for future research.

Weaknesses:
This work presents a novel idea with exceptional clarity in its writing, accompanied by a robust experimental setup that yields convincing results. While no major weaknesses were identified, a more comprehensive discussion on the current limitations of the study would enhance its overall quality even further.

Minor:

- In line 36 (and others), the paper mentions the term ""semantically rich representation."" However, it is important to clarify how this term is defined. It can be argued that the richness of semantic content in a representation is dependent on the specific downstream task being addressed. In other words, a representation can be considered more or less semantically rich based on its effectiveness in a particular downstream task.

- To enhance readability, it would be beneficial to include the ID (or at least the minimum ID) directly in the plot in Figure 4. This additional information (even though repeated from Figure 3), would facilitate the reader to compare the peak categorical information with the ID minimum.

Limitations:
Although not extensively, Section 4 explores the limitations and potential areas for future research.

Rating:
9

Confidence:
4

REVIEW 
Summary:
The paper studies the hidden representation of pretrained transformers via the ID (intrinsic dimension) on protein language tasks and image reconstruction tasks . The papers show that on protein LM, from input to output layer, the ID first increase to a peak, then decrease to an elbow, and finally increase to near its ID value at input layer; this is robust from small to largest models. For image reconstruct, the picture is quite different; the input & output layers have the smallest ID and there two peaks near the input and output layers, respectively. The results on iGPT is less robust compare to protein LM; (e.g., small iGPT does not seem to have two peaks). 


Overall, I think the paper is well written, presentations and experimental setup are clean, the results are interesting. However, I am not very familiar with the field and could not judge the novelty of the paper. 





Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A well written paper; structure is clean; presentation is good; authors spent decent amount of efforts to introduce / motivate / explain key concepts using in the paper, e.g. ID etc. 

The results seem interesting even to folks who are not working this area.  








Weaknesses:
The results on iGPT is less robust (compare to protein LM). In particular, for the small model (I don’t see two peaks). Naively, I also expect the shape of ID of iGPT to be similar to pLM, encode (decreasing ID, smaller than input) and decode (increasing ID), just like the elbow in the pLM. Is there an explanation? 

Why not also including a third task: NLP  (casual language model) in the main text?  


Limitations:
Have several discussion about further extension of the current approach. 

Rating:
5

Confidence:
1

";1
7ym1abRTF3;"REVIEW 
Summary:
This works addresses two problems: first, the problem of solving a specific class of information-gathering decision processes, and second, applying the resulting algorithm in the real world by creating an intelligent tutoring system. The algorithm is a straightforward greedy optimizer that gathers information based on maximizing one-step value of information.  The intelligent tutor blends the algorithm with a UI that gives humans feedback on their decisions by comparing them to the algorithm's; the paper shows that the humans implicitly learn from this feedback and improve their decision making abilities.  The paper concludes with a few small comparisons of the algorithm to other algorithms.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ The paper is well-written
+ The problem seems reasonably well formulated
+ The algorithm is natural and reasonable
+ The algorithm seems to perform well compared to alternatives
+ The ""intelligent tutor"" is a bit of a misnomer, but does seem to improve human decision making ability

Weaknesses:
- This paper blends two distinct ideas, and does not take either very deep. The MGPS algorithm is straightforward, and the authors spend very little time (for example) analyzing its properties or connecting it seriously with the vast literature on decision processes.  (For example, I'm surprised not to see any connection to bandits).  I think you could have written a whole paper about MGPS -- establishing properties, contrasting with other algorithms, etc.  For example, a regret bound or similar form of theoretical analysis would have been nice.

On the other hand, the ""intelligent tutor"" seems like a very simple UI that gives very basic feedback.  While the authors do demonstrate some effectiveness, it too is not investigated deeply.  There is no comparison, for example, to other forms of UI, to other forms of feedback, etc.

So, it's hard to say: is this paper about MGPS, or intelligent tutor UI/UX design?  And while I appreciate that the authors probably have a vision of ""solving a real-world problem"" with this combination of ideas, I have to say that the problem formulation seems pretty far away from something that could actually be used in a business decision support system.


Limitations:
The authors do not explicitly discuss the limitations of their algorithm/UI in a dedicated section.

I do not see any potential negative societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The problem this paper tries to tackle is how to improve human decision making in the specific problem of selecting a project between a candidate set of existing projects. The strategy to improve the human's decision making is to build an agent that can solve the project selection itself by framing as a POMDP and then having that agent serve as a tutor during a teaching phase. The agent is learned by first framing the problems as a POMDP with Guassian states and the paper proposes a myopic algorithm to select actions (actions allow you to acquire information about each project on a specific dimension of the Guassian). The main evaluation of the paper is with a user study where human participants are trying to select between multiple projects in terms of estimated performance. The authors show that participants who received training with the tutor performed better than participants without a tutor or with a dummy tutor.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
- very well-designed and rigorous user study that shows that the agent and the tutoring was effective. 


Weaknesses:
- (main reason for low rating) out of scope of NeurIPS: this paper seems like a bad fit for NeurIPS, the paper does not contribute new algorithms that are broadly applicable and does not evaluate methods on broadly recognized datasets or benchmarks.  This papers models the problem of human project selection as a PODMP and proposes a relatively straightforward procedure and evaluates on a synthetic task of project selection. I don't see any insights that the community might benefit from. I think this would be a strong paper for a conference that suits it more. 
A bad (but still useful) heuristic is to look at the references where I count a single NeurIPS paper (from 2010) and one UAI paper (among a lot of management and behavioral science citations that don't include other human-ai conferences like CHI). 

- why not display the recommendation of the AI agent during test time as a baseline? since the AI agent performs well at the task, participants should be able to see its recommendations at test time. 

- the proposed algorithm while adequate for the problem is not a generalizable solution for the broad family of problems that are of interest in the human-AI space. In particular, the myopic approximation is very limiting. While section 6 does evaluate it against (a relatively old) baseline in PO-UCT, it limits the baseline to 5000 steps because of runtime constraints, however, it might be possible to optimize the performance of PO-UCT to be faster. 

- novelty: the method is a modification over MGPO [14] which introduces the myopic approximation, the proposed method MGPS modifies it to make it suitable for project selection. The authors do a good job of comparing to MGPO, but novelty overall is limited.

- value of tutoring for project selection: why not just follow the recommendations of MGPS? the tutoring is specific to the problem domain, thus I don't understand anything that the human can take away from the tutoring to future tasks except the one they will encounter. Moreover, for the real world problem presented, it is a one shot decision problem, so tutoring is not as well motivated.

Limitations:
limitations are well discussed.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper focuses on the problem of project selection (how does a human choose which, among a set of possible projects, is the best one to pursue). To address this problem, they develop an algorithm called MGPS that discovers a rational greedy strategy for solving this problem, and then they attempt to teach this strategy to a human via an intelligent tutoring system. The approach is evaluated in a real world project selection scenario.

- page 2, line 64: ""selection.To"" -> ""selection. To""
- page 3, line 127: ""we introduce explain our general"" -> ""we introduce our general""


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
It is interesting to see a paper that goes all the way from finding an algorithm to solve a problem, to teaching humans based on that strategy and then evaluating human performance after being taught.


Weaknesses:
The same strength of the paper seems to also be its main weakness: because the authors try to tackle a whole large problem end to end, I did not find each if the individual pieces that ground breaking. Perhaps if the paper focused on just one of the problems (finding optimal strategies, or just better techniques for teaching the learned strategies), maybe a stronger contribution would arise, by more systematically addressing one problem. But as it stands, the paper seems two-headed, with limited contributions, as each of the two problems is only dealt with shallowly.


Limitations:
See weaknesses.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors pose the problem of teaching decision-makers how to take a single action (picking a project from among a set of projects) based upon costly advice from experts across different, weighted criteria. The authors develop a reinforcement learning approach to creating a tutor that approximately solves the MDP (using a myopic approximation). The proposed tutor outperforms a baseline on a banking dataset. The tutor is also shown to improve decision-making of subjects in an online study, who get to learn from the tutor by watching the tutor make decisions.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
+Section 3 is very clearly written. Well done (though Line 162 could have used O-notation).
+The paper clearly presents the algorithm.
+The paper is addressing an important problem of developing a tutoring system for solving MDPs
+The paper has strong empirical results vs. PO-UCT.
+The paper shows statistically significant results in a user-study. It is nice that a user-study was done.

Weaknesses:
-The terms h, e, \lambda, N, and R_{total} are not sufficiently defined in Lines 41-50. Perhaps it would be better to give a more complete description later in the paper and abstract the presentation here to make it more intuitive just with words.
-The comment on brainstorming in Line 86 ignores relevant literature on the wisdom of the crowd, the science behind brainstorming and focus groups, etc.
-The statistical analysis does not report testing for the meeting of a Gaussian assumption for the confidence intervals. Details of the Box approximation should be provided. Further, it would have been better to also report p-values in that table. 
-For the user study, Table 1 should report how optimal (the RR) the MGPS tutor would be if run automatically (no human intervention) and how poorly a random, automatic system would be for the RR-score.
-I am uncertain that the authors are reporting the degrees of the freedom of the F-test. The F-test has two degrees of freedom, but only 1 seems to be indicated in Lines 295-299.
-I can appreciate that the authors might have thought that the results in lines 329-345 indicate that MGPS > PO-UCT and therefore not included PO-UCT as a baseline in the user study; however, that is a debatable decision. It could be that the behavior of PO-UCT is more intelligible by users, and users with PO-UCT could have outperformed those trained by MGPS. As such, I recommend the user study be re-ran (to account for cohort effects) with the PO-UCT baseline and randomizing the allocation of participants to the conditions.
-The paper isn't exactly ""tutoring"" participants. Rather, the system is providing recommendations (or making decisions) and the users have to reverse engineering the actual strategy. Considering that the strategy itself is not constrained to be a set of if-then rules, it is unclear what exactly is being learned or how. It would make the paper better to have actually analyzed (by collecting the data from users) what users are learning and thinking. I would recommend looking at methods in explainable Artificial Intelligence.

Note:
-I recommend the ""Dummy"" tutor be called a random tutor -- a ""random tutor"" is a more clear description of that condition.

Limitations:
Limitations are reasonably discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper addresses the problems of (1) sequential decision-making of information gathering through asking experts for information about the rewards for different projects (as meta-reasoning towards project selection), and (2) teaching people how to make near optimal decisions in the same problem through training in an intelligent tutoring system.  Specifically, novel problem aspects are considered compared to recent work on similar problems, including the availability of different experts of different reliabilities (instead of one information source) and multiple criteria for evaluating the quality/reward of a project.  An algorithm is developed for generating solutions guiding information gathering.  That solution is used to guide the training of humans through an Intelligent Tutoring System (ITS).  Experimental results highlight the benefits of the approach through both better training of humans in an ITS (measured through better rewards earned) and better performance in simulation of agent reasoning than the problem modeled as a belief MDP solved by PO-UCT.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The primary strengths of this paper include:

S1) The problem addressed is one that is relevant in the human-AI applications and it considers novelties not previously addressed that are important real-world complexities.  The problem of sequential decision-making will be of interest to the planning community at NeurIPs.

S2) The paper is well written and easy to follow.

S3) Modeling the problem as a POMDP is appropriate, and the MCDM component seems to be appropriately modeled.

S4) I appreciated the use of two very different sets of experiments -- both training humans within an ITS and simulating the approach directly.  The ITS experiment was well designed and the evaluation was very carefully conducted and convincing.

Weaknesses:
The primary weaknesses of this paper include:

W1) The main contributions identified in the abstract are incremental, seemingly adding some environment complexity and solution adaption to [14], rather than being entirely novel.

W2) While the choice of a POMDP was appropriate, I wasn't entirely sure why the authors relied more on the belief MDP formulation rather than a true POMDP with observations separate from belief state transitions.

The problem being solved is gathering information to ultimately choose the best task to accomplish.  This is very closely related to prior POMDP usage for problems like preference elicitation where the agent gathers information about which is the user's main preference or task they want the agent to perform.

Boutilier,C. 2002.A POMDP Formulation of Preference Elicitation Problems.In Proceedings of AAAI'02, pp. 239–246.

Doshi, F., & Roy, N. 2008. The Permutable POMDP: Fast Solutions to POMDPs for Preference Elicitation. In Proceedings of AAMAS'08, pp. 493–500.

In those models, the state space is the set of possible tasks/preferences, and actions either (1) query an information source (e.g., the user) for observations used to update the agent's Bayesian beliefs about the top preference/task, or (2) end information gathering to perform the perference/task the agent thinks is the top.  That is fundamentally the same as the problem being addressed here, but the details are slightly different.  Instead, your state space is belief states over the details of the tasks, from which a top one is selected.  It's not clear to me why the former wouldn't work in this situation and what the advantage is in your formulation, which would help strengthen the novelty of the work. (Note: what makes your paper different from [14] also makes it different from those works).

Information gathering in POMDPs in general also have special formulations, such as the \rho-POMDP and equivalent POMDP-IR, and I would think your problem model would also fit nicely in the POMDP-IR, but neither is considered in your related work.

\rho-POMDP: Araya-Lopez, M., Buffet, O., Thomas, V., & Charpillet, F. 2010. A POMDP Extension with Belief-Dependent Rewards. In Proceedings of NIPS'10.

POMDP-IR: Spaan, M.T.J., Veiga, T.S., & Lima, P.U. Decision-theoretic planning under uncertainty with information rewards for active cooperative perception. Journal of Autonomous Agents and Multiagent Systems, 29(6):1157-1185.

W3) I also wasn't sure why you chose PO-UCT as your baseline?  Its the simpler version of POMCP (whereas POMCP would have been more appropriate if you had explicit observations in your model).  Belief MDPs can also be considered continuous state MDPs, and there have been many advancements in Monte Carlo Tree Search planning in that area since PO-UCT, such as:

Sunberg, Z. & Kochenderfer, M. 2017. Online algorithms for POMDPs with continuous state, action, and observation spaces. arXiv, 2017. doi: 10.48550/ ARXIV.1709.06196. URL https://arxiv.org/ abs/1709.06196.

Finally, the \rho-POMDP has a MCTS solution that would also be relevant that is more recent:

Thomas, V., Hutin, G., & Buffet, O. 2020.. Monte information-oriented planning. In Proceedings ECAI’20.

W4) I also didn't quite understand the novelty of the ITS experiment compared with [14].  Was the only difference the change in the meta-reasoning algorithm, or were there other differences?

Limitations:
These were appropriate addressed.

Rating:
5

Confidence:
5

";0
l6pYRbuHpO;"REVIEW 
Summary:
This paper studies online learning in a contextual setting when a feedback graph determines the feedback received by the learner. Namely, the learner observes all the losses experienced by the actions in the graph-neighborhood of the action it played. 

Feedback graphs are a well-known feedback model for online learning, and their study in the contextual setting is natural. The paper's main result is a general framework (Theorem 3.1) that reduces the problem to the solution at each time step of a convex program. Then the authors instantiate this general theorem for the strongly observable and weakly observable case, obtaining the same regret rates that characterize the non-contextual version of the problem. 

The primary technical tool of the paper is the parameter defined in equation (3). Following Foster et al. 2021, the authors use this new formula instead of the bandit one (equation 1) to prove their results. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem studied is natural and well-motivated.
- The regret bounds match the lower bounds in Alon et al., 2015, so they are tight. 
- The experimental results and the theoretical guarantees support the claim that a richer feedback structure improves the regret bounds w.r.t. the bandit case. 
- It is nice that the authors devoted some time to presenting the detailed results for some famous classes of feedback graphs. 


Weaknesses:
- Once equation 3 is designed, the rest of the paper seems incremental to Foster et al.  
- Equation 3 needs knowledge of the feedback graph. Therefore the paper works only in the informed setting. 
- The introductory model entails time-varying feedback graphs, while the results for strong and weakly observable feedback graphs need the graph to be deterministic and known up-front. 

Minor comment:
- uniform the \citet and \citep notation



Limitations:
See above

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors consider the adversarial contextual bandit problem with feedback graphs, in the finite function class setting with a realizability assumption, with an access to an online regression oracle. They extend previous approaches for the vanilla contextual MAB setting in order to obtain regret bounds of $\tilde{O}(\sqrt{\alpha T})$ with fully observable graphs and $\tilde{O}(d^{\frac13} T^{\frac23})$ for weakly observable graphs, where $\alpha$ and $d$ denote the graph's independence number and weak domination number respectively. The authors demonstrate their results empirically and show that their algorithm performs better than existing approaches when side observations are available.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
* The authors exhibit the first efficient algorithm for contextual bandits with feedback graphs, which obtains near-optimal regret guarantees both in the strongly observable and weakly observable settings.
* The techniques utilized by the authors seem to neatly generalize the approach of Foster et al. ('21) to the feedback graph setting, by generalizing the inverse gap technique to solving a convex problem together with utilizing a combinatorial property of the feedback graphs.
* The authors demonstrate in several empirical experiments that their algorithm out-performs SquareCB when feedback graphs are present, thus strengthening the idea that better performance can be obtained when side observations are available.

Weaknesses:
* This is not a very major issue, but the algorithm suggested by the authors requires knowledge of the feedback graphs' independence number (or a good bound on it), which is a hard quantity to compute in general. This is only a minor point because many previous works in the feedback graphs literature also require knowing such a parameter, but I will remark that some results can be obtained without this knowledge.


Limitations:
Yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work is concerned with the problem of contextual bandits with graph feedback. The authors consider a setting where the contexts and the graphs are generated in an arbitrary manner and revealed to the learner at the beginning of each round. For a given context, the mean loss of each arm is assumed to be fixed. Furthermore, it is assumed that the learner is provided with a class of functions mapping context-action pairs to their mean loss, and that this class contains the true function. Following prior works in contextual bandits, the authors use an online regression oracle to estimate the true mean losses. These estimates are then used within the estimation-to-decision framework of  Foster et al. (2021).  Within this framework, the proposed algorithm requires solving a convex program at each round, for which closed form solutions are provided for special cases of interest. The authors prove regret bounds that depend on the structure of the feedback graphs and the regret of the online regression oracle. Additionally, empirical evaluations are carried out to showcase the algorithm's ability to take advantage of the side observations provided via the feedback graphs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Although the adopted approach relies on existing techniques, the application of the estimation-to-decision framework of Foster et al. (2021) for bandits with graph feedback is still an interesting contribution. Most notably, in Theorems 3.2 and 3.4, the authors bound the decision estimation coefficient for their algorithm in terms of the independence number for strongly observable graphs and the weak domination number for weakly observable graphs, which respectively are the graph theoretic quantities that characterize the minimax regret for these settings. Overall, the paper is well written, the setting is adequately motivated, and the clarity of the presentation is decent.

Weaknesses:
- The regret bounds in Corollaries 3.3 and 3.5 are stated in terms of a uniform upper bound on the independence number or the weak domination number of the observed graphs. This is unsatisfactory since a single sparse graph could render the bound vacuous.
- In the formulated setting, the graphs are allowed to be stochastic, where each edge is realized with a certain probability which is revealed to the learner at the beginning of the round. However, all the provided theoretical results are for deterministic graphs.
- One point in need of clarification concerning the experiments is that it is mentioned in the beginning of Section 5 that all the graphs used in the experiments are deterministic, while the experiment described in Section 5.2.1 seems to involve stochastic graphs.
- For the experiment of Section 5.1, it might be more informative to include more intermediate cases between bandits and full information.
- A minor correction: In the discussion section, the authors address the limitation of their approach in the uniformed setting, where the feedback graph is only revealed to the learner after making a decision. The authors then cite results for the non-contextual case from (Cohen et al, 2016). However, in that work, the graph is never revealed to the learner, only the actions in the neighbourhood of the played action and their losses are observed.

Limitations:
The authors did address some of the limitations of their work. Notably the fact that their approach requires the knowledge of the feedback graph before choosing the action.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work studies the contextual bandits problem in the presence of a feedback graph G_t. An edge (i -> j) in G_t means that taking action a_i allows us to observe the loss for action a_j. The work extends the SquareCB algorithm to this setting, the primary difference being the way the action sampling probability p_t is learned (Eq. (1) vs Eq (4)). The authors also present regret bounds for strongly observable and weakly observable graphs where the bounds improve over the standard contextual bandits setting when the independence number and cardinality of the dominating set is small.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
 - Overall I found the paper to be well-written where the setup was explained well and the notation was easy to follow.
 - This paper builds on the SquareCB algorithm. I appreciated the fact that the work makes an effort to try to separate their own contributions from the SquareCB paper.
- The paper covers both strongly and weakly observable graphs. In general, the statements in Theorems 3.2 and 3.4 are fairly intuitive.

Weaknesses:
 - By and large, the paper relies heavily on the SquareCB paper's analysis. Although the setting of feedback graphs is new, the analysis seems derivative.
- I would suggest that the authors provide more intuition behind the proofs of the key theorems. Even though the theorem statements are easy to understand, it would be nice to get some insight into the proofs, and specifically what are steps different from the standard CB regret analysis.

Limitations:
See weaknesses part. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This work studied contextual bandits with feedback graphs. The authors provided an algorithm based on the recent Decision-Estimation Coefficient (DEC) framework that finds the next-step action distribution by solving a minimax optimization problem. To address the issue that the minimax problem is hard to solve, the authors also showed that there exists an efficient implementation of the solver, and the closed-form solution also exists for some special cases. For the experiment, the authors compared the proposed algorithm with a vanilla baseline algorithm that does not utilize the feedback graph information. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The presentation is clear. The overall approach to solving the contextual bandits with feedback graph is convincing. The theoretical results are sound. 



Weaknesses:
The importance of this work remains unclear. The proposed algorithm is very similar to the original E2D algorithm proposed by Foster et al. 2021 with an additional expectation over the feedback graph. The analysis technique is also very similar. The key difficulty in proving the regret of SquareCBG is to build an effective estimate the DEC constant. However, from the proof of Theorem 3.2, it seems that the proof is pretty standard by following the decomposition technique developed in Foster et al. 2021 (like their Proposition 5.1), while utilizing the graph node estimation lemma proposed by Alon et al. 2015. Therefore, I would recommend the authors highlight the main challenge for them to derive the theoretical results. 


A typo. Line 62, for some action $j$-> $a_t$. 




Limitations:
The authors addressed the limitations. 

Rating:
5

Confidence:
3

";1
Cs74qIBfiq;"REVIEW 
Summary:
This paper introduces RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot manner. The key idea is to leverage insights obtained from language models based on task descriptions. These insights are used to modify the embeddings, removing harmful components and enhancing useful ones, without the need for supervision. Technically, the method ensures invariance to spurious features by projecting pretrained model embeddings onto the subspace orthogonal to the subspace spanned by spurious feature descriptions. Experiments demonstrate that RoboShot improves multi-modal and language model zero-shot performance. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. **Novel and useful setting:** The setting of improving the robustness of pretrained model embeddings with task description is novel. RoboShot offers a unique approach that preserves the out-of-the-box usability of pretrained models, which is a key advantage.

2. **Extensive experiments and analyses:** The authors demonstrated the efficacy of the proposed method and setting with extensive experiments and analyses, in terms of both datasets and settings. 

3. The paper is well-written. 

Weaknesses:
1. **Limitation of method:** The robustification relies on the insights provided by language models. However, if the language model does not identify the potential failure cases of the model, the method cannot remedy it. 

2. **Gender bias:** Some experiments are targeted at gender bias. It's better to discuss the scope of evaluation here, e.g., what genders are considered and what biases remain unresolved. 



Limitations:
Limitation needs to be elaborated on. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents an innovative approach to enhance zero-shot classification inference without the need for fine-tuning pre-trained models. The authors introduce a method that partitions input embeddings into three components: harmful, helpful, and benign. By leveraging task descriptions and querying language models with harmful and helpful prompts, they successfully extract harmful and helpful components. This leads to the removal of harmful components and a boost in the helpful ones, ultimately improving robustness in zero-shot classification. The proposed method demonstrates promise and contributes to the field of zero-shot classification without extensive model finetuning.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-structured and easily understandable, with a strong and compelling motivation. As model sizes continue to increase, fine-tuning LLMs becomes increasingly expensive. This paper presents a compelling alternative by enhancing zero-shot performance while retaining the power of LLMs without the need for fine-tuning.  The innovative method of partitioning embeddings into three concepts and leveraging task descriptions and LLMs to strengthen or weaken them is intriguing and holds promise for embedding-based zero-shot text classification.

Weaknesses:
1. The proposed method is primarily applicable to embedding-based zero-shot classification approaches, while prompt-based methods like ChatGPT3.5/4 have gained popularity recently. Prompt-based methods allow humans to directly query language models based on downstream task knowledge, resulting in impressive performance. Although the proposed method is interesting for embedding-based zero-shot classification, its impact may be limited due to the current research trend.

2. Considering the above point, it would be beneficial if the authors compare their method to a more reasonable baseline, such as asking ChatGPT about predictions. This approach could be employed, for instance, by asking ChatGPT to identify if a given text contains gender bias. While ChatGPT's performance might not extend to image-based tasks, applying this baseline to text datasets would provide valuable insights. If ChatGPT achieves accurate predictions, it questions the necessity of an embedding-based zero-shot text classification method.

3. Although the method exhibits significant improvement in worst group performance (WG), it would strengthen the findings if the overall average performance also demonstrated improvement. It is worth noting that in some datasets, the average performance did not improve. This implies that the proposed method sacrifices performance for some classes to achieve better results in the WG. Ensuring a balance between overall average performance and WG improvement would bolster the method's effectiveness.

Overall, I am inclined to accept the paper if all weaknesses are properly addressed but I am also not strongly against rejecting the paper.

Limitations:
Please refer to weaknesses.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach called ROBOSHOT that aims to enhance the robustness of pre-trained models for zero-shot classification tasks. The key idea is to leverage task-specific queries to prompt the large language model (LLM) to generate textual insights about the task, which can be classified as helpful or harmful concepts. These textual insights are then used to obtain ""insight representations"" based on the corresponding encoded embeddings, which are utilized to calibrate the vector of input representation to predict the class. The proposed method is evaluated through experiments on zero-shot image and text classification tasks. The paper also includes a theoretical analysis of the bound of the coefficient of targeted harmful concept post-removal. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation of this paper is clear. The problem of ""robustifying zero-shot models without labels, training, or manual specification"" is challenging and interesting. 
2. The paper proposes a simple method to calibrate the input embedding to make predictions.
3. The paper is generally easy to read and easy to follow. The settings of experiments in this paper are clear.
4. The theoretical proof of the coefficient bound of the targeted harmful concept is a plus.




Weaknesses:
1. The paper aims to robustify zero-shot models without labels, training, or manual specification. However, the proposed method still requires the use of manually-designed helpful/harmful queries (shown in Table 6&7) to query the LLM. 
2. The paper did not explore the impact of using different prompts to query textual insights. Given that the experiments were conducted on small pre-trained models such as BERT, which are less robust than LLMs, it is unclear whether the resulting different text insights would significantly affect the model's performance.
3. The paper employs LLMs (e.g., chatgpt/LLAMA) to generate textual insights for calibrating the input embedding of a small pre-trained model for prediction. It raises the question of why not directly prompt the LLM for zero-shot text classification if it already possesses sufficient knowledge about each class. The paper would benefit from including such a baseline comparison.
4. The paper did not include and compare some relevant works, such as [1] [2], which calibrate the logits of predictions for zero-shot/few-shot text classification. 
5. Table 1 indicates that the proposed method does not improve the ALIGN model on the Waterbirds dataset, but it does improve ALIGN's performance on CelebA, VLCS, and CXR14. The paper attributes this discrepancy to the harmful and helpful insight embeddings of Waterbirds being indistinguishable in the text embedding space of ALIGN. However, it is unclear how to determine when the ROBOSHOT method is applicable to different combinations of models and datasets. The paper could benefit from providing some metrics to predict the conditions under which ROBOSHOT is effective.

[1]Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right

[2]Calibrate Before Use: Improving Few-Shot Performance of Language Models

Limitations:
please refer to weakness point 5

Rating:
4

Confidence:
4

REVIEW 
Summary:
The main objective of this research is to enhance the robustness of image/text classification by taking into account the relationships between labels. The authors utilize Large Language Model (LLM) to incorporate prior knowledge about these labels. They also propose methods to amplify useful features and eliminate harmful features induced from these labels.

In contrast, prior work require manual identification of biased features for debiasing, whereas this work leverages LLM to automatically suggest such features, thereby reducing the need for manual effort.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and can be easily understood.
- Using LLM to propose useful and spurious features is somewhat novel.

Weaknesses:
- This work aims to incorporate the knowledge of LLM into image classification. However, the method used, which is based on CLIP, does not seem to fully utilize the knowledge from LLM. The author's approach involves generating insight descriptions to establish a connection between LLM and CLIP's text encoder.  A more intuitive alternative would be to link CLIP's image encoder directly with LLM through an adapter, similar to the Flamingo (https://arxiv.org/abs/2204.14198) and LLaVA (https://arxiv.org/abs/2304.08485) methods. By doing so, the knowledge from LLM can be more comprehensively utilized, rather than solely the generated insight descriptions.
- The subspace-based debiasing methods have already been used under various contexts as also mentioned in the paper, which cannot be viewed as novel to some extent.

Limitations:
The limitations part is missing in the paper.

Rating:
4

Confidence:
3

";0
szFqlNRxeS;"REVIEW 
Summary:
The authors present online algorithms for geodesically convex losses on Riemannian manifolds. Their algorithms do not call the expensive operation of projection onto a feasible set. Instead of projection, they rely on two oracles to provide a direction of descent: a separation oracle and a linear oracle. Both require an extension of the concept of hyperplanes from Euclidean space to the manifold, for which they use an inverse exponential map. This map transforms points on the manifold into tangent space vectors, enabling operations on the manifold without the need for explicit projection operations. In addition, they also consider the projection onto a geodesic ball, which is computable with high accuracy and thus effectively projection free. Their algorithms give adaptive regret guarantees which are sublinear in the horizon. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The authors' contribution is significant as they address the challenge of projecting onto a Riemannian manifold, which is computationally expensive and can lead to geometric distortions. This area of research seems relatively less explored. 

- The writing style is clear and they provide a comprehensive background on Reimannian geometry, making it easy for a read to grasp the problem and motivation. 


Weaknesses:
As I am not familiar with this area, I am unable to identify any weaknesses or limitations. 

Limitations:
The authors have acknowledged the limitations of their work, specifically the absence of a membership oracle, and have discussed potential enhancements to their algorithms. These improvements include utilizing the separation oracle for strongly convex losses, reducing the reliance on the number of calls regarding the set's diameter and a faster method to optimise the objective in the linear oracle. 

Rating:
8

Confidence:
1

REVIEW 
Summary:
The paper considers Riemannian online optimization problems over sets of constraints and tries to tackle them by avoiding projections onto the constraint set. This is an already established line of research in Euclidean optimization and the results follow the structure of Garber and Kretzu (2022). The results of the latter are adapted to the Riemannian setting using well-known geometric bounds.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The paper is concerned with problems of profound importance for the neurips community. It is well-written and can be followed easily by people with reasonable background in Riemannian optimization. The use of geometric bounds (law of cosines in negative curvature, spread of Jacobi fields) is clearly explained. The authors pay special attention in computability issues behind the separating and linear oracles used, building upon the contributions of Weber and Sra (2022b). The convergence guarantees much the ones of the Euclidean setting in Garber and Kretzu (2022) up to constants.

Weaknesses:
I am not thrilled by the level of originality since the paper follows closely the one by Garber and Kretzu (2022) and used geometric bounds that are well-known in the Riemannian optimization community for quite some time. Still, it is a clearly-written concrete contribution to a problem of interest, thus I think that publication in the conference is totally justifiable.

There are some points in the paper that I find peculiar as I specify in my questions' section. This is the main reason that I give a score only slightly above the acceptance threshold (which I will revisit in case the authors answer me concerns convincingly).

I would advise the authors to revisit the title of the paper. When I see ""On ..."" I automatically form the impression that the paper features some vague discussion about a research topic without really useful contributions, which is not the case here.

Limitations:
The authors adequately discuss some limitations of their work in the conclusion of the paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper focuses on constrained Riemannian online optimization on the Hadamard manifold. Existing Riemannian online optimization methods often require projections, which present computational complexity challenges in high-dimensional settings. To address this issue, the authors have developed a projection-free Riemannian online optimization structure, implemented under two scenarios - the separation oracle and the linear optimization oracle.

Initially, they explore the Infeasible R-OGD and Infeasible Riemannian bandit algorithm under a separation oracle, for which the regret bounds are proved to be $\mathcal O(\sqrt T)$ and $\mathcal O(T^{\frac{3}{4}})$ respectively for geodesically convex functions. Furthermore, they consider the Block R-OGD under a linear optimization oracle, and provide a proof for regret bounds of $\mathcal O(T^{\frac{3}{4}})$ and $\mathcal O(T^\frac{2}{3})$ for geodesically convex and geodesically strongly convex functions respectively. It's noteworthy that all these regret bounds match their respective Euclidean counterparts.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
-  Originality & Significance: Althogh the paper has retricted novelty (discuss in the following), it provides, to my best of knowledge.  the first no-regret guarantee for projection-free Riemannian OCO. Also, it is nice to see that all regret bounds match their Euclidean counterparts.

- Clarity & Significance: The paper is well-organized, techinally sound and easy to follow. The assumption is standard in the literature of Riemannian optimization and Riemannian OCO.

Weaknesses:
- Retricted novelty: The majority of the analysis leans heavily on the Euclidean Analogue and Jacobian/Hessian comparison. Although it is the standard structure in Riemannian optimization literature, it would have been enriching to see some novel conceptual ideas that engage more specifically with the geometry of the problem.

- Positive curvature: The study focuses on the Hadamard manifold, known for its non-positive sectional curvature. Conversely, in practical scenarios, there's considerable work undertaken within spaces possessing positive sectional curvature like SO(3). The methodology and algorithms presented in this paper appear, at first glance, to be easily extendable to accommodate $cat(K)$ spaces. However, the authors have not provided any explanation or context for why such positive curvature spaces were not considered in their study.  A consideration of such spaces could have added more depth and wider applicability to the research.

- Possible typo: formula about $\bar r $ after l227: there should be a additional $r$ at RHS.  
 Algorithm 5: $y$ seems to be a point rather than a tangent vector.


Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors study online learning on homogeneous Hadamard manifolds with geodesically-convex, Lipschitz losses. They focus on projection-free methods and in particular they develop algorithms that use either a separaton oracle or a linear optimization oracle. They study adaptive regret algorithms for the full information and the bandit feedback settings. 


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Besides from the strengths of having the theoretical results authors have, I want to emphasize the following.

This work usually states clearly the assumptions that is making, even though this could be improved, as I suggested. It was good to see the authors taking into account the non convexity of (1-\delta)\mathcal{K}. Some works ignore this fact, I wonder if they learn about this in the review of a paper. But the authors here are rigorous and work with the possible non-convex set.

The results with linear minimization oracle look fine to me

Weaknesses:
# Main points of my review

Several results in this paper follow closely Garber and Kretzu (2022). This is not a criticism, of course there are several challenges in the Hadamard case that need to be surmounted. 

I have several points of criticism though:

+ The paper contains some mistakes, and I am not sure if one could fix at least one of them while keeping the stated results (see below). 
+ Moreover algorithm 4 has a regret guarantee in Theorem 2 which is non standard (algorithm plays a point and feedback is given for another point but the regret is still measured with respect to the played point). This seems an artifact of the proof, as in it was the notion that made the proof work, but without any other justification for this made-up definition of regret, usefulness of it or an application of it, the result is very weak.
+ Lemmas 3 and 4 yield that the number of iterations of algorithm 2 is exponential on R. This is not desirable. I do not want to criticize this point harshly, since Hadamard is hard and sometimes the geometry makes these exponential dependences appear, and this could be a result deserving its own paper. This dependence also appears in previous Riemannian online learning works. But rather, I just want to encourage the authors, possibly for future work, to try to improve this to a polynomial dependence (see below for a few comments on this).

While I think this works has value, I also think this is not ready for publication. At the very least the errors should be corrected, and the bandit results should not be presented as results (e.g. in the table) unless the authors can justify that the modify notion or regret makes sense. And in any case this should be clearly stated for the readers that only want to firstly check the introduction out.

# Proof Errors

Correct me if I am wrong but I think I found the following errors in your proofs. I can increase my score if the technical errors can indeed be fixed.

+ The proof of Lemma 1 is wrong, although fixable. The points y_{t+1} are not guaranteed to be in B_p(R) and so you cannot use lemma 29 as you stated it. However, if you look closely at Lemma 5 of Zhang and Sra, you will realize that the lemma actually says that \zeta depends on \kappa and c, which a stronger inequality than what you stated (dependence on \kappa and D). Using that stronger statement then you can show in your Lemma 1 that the \zeta in the proof depends on d(\tilde{y}_t, x) \leq D, and so the proof follows.

+ However, your lemma 2 has the same problem, and when you use it in the proof of lemma 4, you use it with the point y_i, which as you point out in L435, it is y_i \not \in \mathcal{K}. Therefore, the geometric deformation is not \zeta (which was defined as depending on D), and this does not seem to be so easily fixable.


# Other comments / suggestions

The results with linear minimization oracle look fine to me.

You should specify in lemma 5, that you always use y\in B_p(R), since you use f(x) = d(x y)^2/2 and you claim that it is \zeta smooth, which you can have because when you use lemma 5 in alg 6 it is always y\in B_p(R). But this is not stated and it was confusing for me.

This work usually states clearly the assumptions that is making, even though this could be improved, as I suggested. It was good to see the authors taking into account the non convexity of (1-\delta)\mathcal{K}. Some works ignore this fact, I wonder if they learn about this in the review of a paper. But the authors here are rigorous and work with the possible non-convex set.

""(pinpointing a compact set that contains the true optimum can improve time complexity) This makes metric projection onto a subset of Riemannian manifold seemingly indispensable, yet this operation is not only computationally taxing but can also lead to unwanted geometric distortion, which can undermine convergence. Certain works have depended on simplifying assumptions to prove results..."" I disagree with the message that is given here. For several reasons:
    + Firstly, in many situations, one can ""pinpoint"" a compact set where the optimum is by just estimating the initial distance to minimizer and imposing ball constraints (and using a doubling trick or other tricks if necessary to avoid assuming knowledge of this distance ). In that case, the operation is ""projection-free"" according to your own definition in line 178, due to the simplicity of the operation, so not necessarily computationally taxing.
    + Secondly, while it is true that using projections can lead to greater geometric distortion that can undermine convergence (e.g. see https://arxiv.org/pdf/2305.16186.pdf where the authors make projected gradient descent work for smooth functions while quantifying the distortion), several works make use of projections to obtain algorithms that previously had to make the assumption of iterates staying in some set, and at the same time the geometric dependence on the convergence rates does not worsen. See for instance https://arxiv.org/pdf/2211.14645.pdf and theorem 7 in https://arxiv.org/pdf/2305.16186.pdf, and https://arxiv.org/pdf/2111.13263v2.pdf section 6 
    + Thirdly, the works that mandate the iterates to be in a feasible set by assumption do so because they do not know how to enforce or guarantee their algorithms are in some feasible set, even if they have access to projections. It is not like the assumption is made to avoid projections that are ""computationally taxing and with extra geometric distortions, which can undermine convergence"" by using projections. Their techniques do not allow them to use projections and as explained above and more clever algorithms do without extra distortions.


You probably cannot compute \sigma_i in Algorithm 5 in close form. You probably would need a binary search and then have the algorithm be able to account for an error. Is that right?

# Exponential dependence on the diameter

Regarding the exponential nature of the constants in lemma 3 and running time in lemma 4, this phenomenon is very similar to the exponential constants that appear in the deformations of lemma 2 in https://arxiv.org/pdf/2012.03618.pdf However, in that paper they are interested on optimization, and they show that one can get away with this exponential complexity. The trick is a reduction from global optimization to approximately implementing a ball optimization oracle, so that the algorithms would only need to run in balls where R\sqrt{-\kappa} = O(1). The reduction goes as follows: optimize in one such constant-radius ball with linear rates (regularize as it is done in reductions if necessary, although you'd need smoothness of the losses) and then optimize in another ball with center equal to the previously computed point. In the online learning setting, it is harder but you could in principle try to run Riemannian OGC by minimizing the sort of FTRL objective you have by using this reduction (you'd have to optimize with linear rates in the intersection of constant curvature balls and your sets, but every time the infeasible projection would be easy). Or just implement the separation oracle by looking at making progress in constant-curvature balls. Anyway, maybe it doesn't work for you, but I hope it is useful to you to at least know that a problem of this kind has been sort of solved in Riemannian optimization.

# Minor suggestions / typos

L62 ""While a horosphere gsc-convex"" ->""""While a horosphere is gsc-convex""
L200 ""theLipschitz"" -> ""the Lipschitz""
L259 Alg 5. I would not say target vector y, since y is a point.
L269 Alg 6. ""initial vector y"" -> ""initial vector y_i"" (although again, I would not say initial *vector* )

# Edit after rebuttal
The authors provided fixes for the proof errors and a new algorithm two work with two point bandit feedback. I increased my score from reject to accept (from 3 to 7)


Limitations:
see above

Rating:
7

Confidence:
5

REVIEW 
Summary:
The authors propose a generalization for online learning on the Riemannian manifolds via separation and linear optimization oracles. The core idea here is to use the oracles to construct an infeasible projection, which may not be the nearest point in the constrain/decision set $\mathcal{K}$, but are in $\mathcal{K}$ and is closer to a shrunk feasible set $(1-\delta)\mathcal{K}$ than the original point. The interesting point finding here is that because there is a buffer between $\mathcal{K}$ and $(1-\delta)\mathcal{K}$, we have a constant distance decrease of $O(\delta^2)$ (Lemma 4 eq (5) in the appendix) w.r.t. by the separating oracle direction to the shrunk set $(1-\delta)\mathcal{K}$, and hence the projection to $\mathcal{K}$ can be done in constant time to $\delta$. This means we can construct an infeasible/inaccurate projection by the separation oracle in $O(1/\delta^2)$ time, and the algorithm follows from replacing the projection part with the infeasible projection and bound the errors. The authors also consider the linear optimization oracles, and it's done by constructing the separation oracles using the linear optimization oracles in Frank-Wolfe. Actually, this draft is a combination between Garber & Kretzu (2022) and Wang et al. (2021), where the authors generalize the online learning by SO/LOO framework by Garber & Kretzu (2022) with the tools and assumptions in Wang et al. (2021). 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The idea of constructing an infeasible projection (the solution is feasible to $\mathcal{K}$) and providing guarantees to a shrunk set $(1-\delta)\mathcal{K}$ is very interesting, and it's good to see it works on the Riemannian manifold. The introduction is pretty well-written, and there is no major error in the proof, and generalizing the result from Euclidean space to the Riemannian space may be challenging in some corner cases.

Weaknesses:
The major weakness of the paper is its complexity, the lack of experiments, and the limiting assumptions.

First, the proof in the paper is pretty complex and is not self-contained. To verify any proof in the paper, the readers usually need to go to Garber & Kretzu (2022) and Wang et al. (2021), which have another set of symbols and may again point to other papers. And this may lead to confusion for both the authors and the readers. For example, the Lemma 19 in the draft points to Wang et al. 2023 Lemma 45, which requires the curvature to be also upper-bounded, but such assumption and coefficient disappears in the draft. Garber & Kretzu (2022) assume that the feasible set contains the origin. The paper generalized it to arbitrary point $p$, but in that case, the shrinking set $\mathcal{K}$ should be redefined toward $p$ for $(1-\delta)\mathcal{K}\subset\mathcal{K}$, but it is also not done. The proof in the paper is mostly correct when self-contained, but it is very time-consuming to verify the mentioned errors from cited lemmas by chasing around the links, so I am not very sure about the correctness of the theoretical paper.

Second, I am not very sure about the practical aspect of the paper due to its limiting assumptions and lack of experiments. Surely, theoretical papers may not have been experimented with, but the paper is too complex to verify its correctness, so it would be good to show the reader that it works in a sense. I am concerned about the limitation in the assumptions because most of the examples mentioned in the draft's introduction don't fit the assumption, e.g., the spherical constraint in K-means clustering. The manifold must be bounded, containing a non-empty unit ball, and contained within a unit ball of radius $\leq \pi/\sqrt{4\kappa_2}$ (Lemma 19). And I am unsure when the separation oracle and linear optimization oracle are cheaper than the projection in a practical example. Please give the readers more examples to understand when the setting would be suitable.

Finally, the paper is a direct combination between Garber & Kretzu (2022) and Wang et al. (2021). Most of the work here looks like a Riemannian generalization of Garber & Kretzu's (2022)'s framework with Wang et al. (2021,2023)'s tools, replacing the distance with exponential maps and bound them with Wang et al. (2021,2023)'s inequalities. And there are not many new surprising results, but double the complexity. It is not necessary to be novel to publish, but given that I'm not convinced that the results are totally correct, I would recommend a weak rejection. This draft should be more suited to a journal than a conference.

Limitations:
NA

Rating:
5

Confidence:
2

";1
mQPNcBWjGc;"REVIEW 
Summary:
The paper extends previous OWL-ViT detectors with self-training an 10B Web image-text data. The authors design different self-training strategies, such as token dropping, instance selection and Mosaics augmentation. It is an achievement to beat all previous methods on open-vocabulary benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors provide necessary implementation details about scaling up the open-vocabulary detector on massive web data with a self-training method.
- The performance on the open-vocabulary LVIS benchmark is truly impressive.
- The authors promise to open-source their code.

Weaknesses:
- GLIP2 performs better than OWL-ST on the ODinW dataset, can the authors explain the possible reasons?
- The authors may consider adding the ablation studies about w or w/o the mosaic augmentation. The additional gain from the mosaic augmentation is not clear.

Limitations:
I like this paper which includes comprehensive experiments and excellent performance, so I think this paper deserves to be accepted. But to me, this paper is not pretty interesting since this paper is more like a technical report with lots of engineering efforts rather than a research paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper proposes a self-training recipe for open-vocabulary object detection that leverages weak supervision in the form of image-text pairs from the Web. The authors identify three key ingredients for optimizing the use of weak supervision for detection: choice of label space, filtering of pseudo-annotations, and training efficiency. They propose to use all possible N-grams of the image-associated text as detection prompts for that image and apply only weak confidence filtering to the resulting pseudo-labels. The self-training recipe is applied to the OWL-ViT detection architecture and is called OWL-ST. The authors introduce OWLv2, an optimized architecture with improved training efficiency. The proposed method surpasses prior state-of-the-art methods already at moderate amounts of self-training and achieves further large improvements when scaled to billions of examples. The authors also evaluate their models on a suite of ""in the wild"" datasets and study the trade-off between fine-tuned and open-vocabulary performance.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper is well-written, easy to understand, and very rigorous.
This paper thoroughly discusses the methods and challenges of extending OWL-VIT to the billion-level image scale.
This paper achieves surprising performance.

Weaknesses:
As the authors mentioned, the biggest issue with this paper is the significant amount of computation required, which makes it difficult for general research institutions to reproduce. Therefore, my question is whether the authors will make the research results available as a foundational model, similar to SAM, for public use. Additionally, can it create a wave of downstream tasks, like SAM did, by providing basic capabilities?

Limitations:
The main issue is still the enormous amount of computation required. However, if it is open-sourced as a foundational model for public use, it would still make a significant contribution to the community.



Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents OWLv2 model and OWL-ST self-training recipe for open-vocabulary object detection. The detection data is greatly enriched with the aid of self-training. Concretely, the authors use WebLI dataset as the source of weak supervision for self-training. The dataset consists of approximately 10B images and associated alt-text strings (noisy captions). Then OWL-ViT CLIP-L/14 is utilized to annotate all images with pseudo boxes. For self-training at scale, the authors present several techniques including Token dropping, Instance selection and Mosaics to make the training efficient. Experiments on LVIS show the effectiveness of the proposed approach.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. This paper attemps to scale open-vocabulary object detection with self-training on large-scale weakly labeled dataset. This is a good engineering work built upon the existing detectors and datasets. This work verifies that large model + large data is a good option to enable zero-shot open-vocabulary detection.
2. The experimental results look like promising. Result on LVIS dataset is good.


Weaknesses:
1. Authors should carefully polish their paper and fixtypos such as: In L12, L/14->VIT/14; L109, consist->consists.
2. The authors are encouraged to discuss how to handle noise in self-training. What's effect without handling these noise.
3. What's the inference process? The authors are encouraged to describe this in their next version.

Limitations:
The limitations are adequately discussed in Section 5.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work adopts a self-training strategy to generate web scale object annotations for open-vocabulary object detection. It uses an existing  open-vocabulary object detector (OWL-ViT) to annotate 10B image-text pairs, and uses the annotated dataset for self-training. A various architecture is also proposed to improve the training efficiency. Experimental results show a large performance gain on open-vocabulary detection.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. A self-training strategy to annotate the bounding boxes for the large-scale image-text pairs. This provides an open-vocabulary detection dataset for various location-specific tasks, including detection, segmentation, visual grounding, etc. Despite self-training being widely explored in many previous works (discussed in Sec 2.3), this work provided a detailed ablation in the filtering strategy in Sec. 4.4.

2. The performance is promising in rare classes, indicating the ability of zero-shot transfer with the self-training strategy.

Weaknesses:
See questions.

Limitations:
The self-training strategy in the paper requires a large amount of computing resources and web-crawled data, which is not feasible for many colleges and research institutes. 

Rating:
7

Confidence:
4

";1
Dkmpa6wCIx;"REVIEW 
Summary:
This paper sheds light on the relationship between three concepts: (a) generalization (b) flatness and (c) explicitly inducing flatness via SAM. The paper identifies that the relationship is nuanced and is based on certain architectural properties of the model.

 Specifically, depending on the architecture, there are three possible regimes:
1. A regime where _every flat minimizer generalizes well_ and SAM finds these minimizers.
2. A regime where _there exists flat minimizers that generalize poorly_ but SAM does _not_ find these minimizers.
3. A regime where _there exists flat minimizers that generalize poorly_ and SAM _does_ find these minimizers.

The paper shows that 

- Regime (1) happens for 2-layer RELU networks _without_ bias
- Regime (2) happens for 2-layer RELU networks _with_ bias
- Regime (3) happens for 2-layer RELU networks _with_ bias, _with_ a simplified Layer Norm.

Note that in all cases, the existence of a flat solution that generalizes/memorizes is proven theoretically; while the effect of SAM is demonstrated empirically.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. The questions formalized in the paper are novel, and well-formulated. 
2. I gained a much clearer understanding of some generally unclear and confusing concepts: (a) flatness, (b) generalization and (c) explicitly inducing flatness. All these ideas are dealt with care and rigor. 
3. The introduction is well-written and the chain of arguments are explained clearly. Despite being a theoretical and nuanced paper, I found it extremely easy to follow the arguments in each section. I thoroughly enjoyed reading the paper!
4. Rather than unnecessarily complicate the theory, the authors come up with a reasonably minimal settings of a 2-layer _non-linear_ model and support their arguments with either a formal result or with a rightly-designed empirical result. 
5. That there are situations where the flattest solution does not generalize well is surprising from a theoretical point of view.

Weaknesses:
I currently do not have any major concerns about this paper. Most of my comments are follow-up questions, which I enumerate in the next section. My most important question --- which may or may not hint at a weakness --- is below:

### Major question

Q1a: Could you be more explicit about any parameter count dependence in the Rademacher complexity term of Thm 3.1 and Lemma 3.1? I am unsure if this is orthogonal to the main point of the result, so I'd also appreciate some clarity on that.  

Q1b: On that note, I'd also like to know why this bound would break down upon the introduction of bias into the architecture. Would one of the terms within the Rademacher complexity end up scaling with the number of datapoints?

Limitations:
The paper lists reasonable limitations in the conclusion.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper studies the connections between sharpness of the training loss and generalization performance under a simplified MLP architecture. In particular, authors show scenarios of when flat solution do/do not generalize, as well as cases when Sharpness-Aware Minimization (SAM) does/does not generalize.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The theoretical insights of the paper are interesting and hopefully can help better understand the relationship between flatness and generalization.

Weaknesses:
I think the paper has several shortcomings, outlined below.

- Model setup: I think the model setup is too simplistic. The input space is binary, which I think is fine. However, no noise is considered in the model and the output label $y=x[1]x[2]$ satisfies a very specific structure. Therefore, it is not clear to me why results discussed in the paper would generalize to broader model setups, and the observations discussed here are not just an artifact of the simplistic model. As shown in Lemma 3.1, the sharpness under the paper's setup is closely related to the weight norm, which may or may not be the case in a more realistic setup. 

- Experiments: I don't find the experiments on SAM convincing. How is SAM implemented? How are the algorithm hyper-parameters chosen? How is SAM initialized? Maybe changing the SAM parameters and initialization would change the outcome of the experiments.  Can the observation of SAM converging to non-generalizing flat solution be replicated in a more realistic setup? Is weight decay used for SAM, as specially, in this case sharpness seems to be closely related to the weight norm.

- The authors miss [1]. As this paper discusses the connections between generalization and sharpness/SAM, the authors should discuss and compare their results to [1], specially as [1] discusses noisy setups.


[1] Behdin, K., & Mazumder, R. (2023). Sharpness-aware minimization: An implicit regularization perspective. arXiv preprint arXiv:2302.11836.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work aims to examine different cases for which flatness implies generalization. The conclusion heavily depends on the underlying model, while there are three different scenarios that contradict with each other. Additionally, the theoretical guarantees study the generalization error at the solution, however the empirical guarantees consider the generalization error at the last iteration of the SAM algorithm. Despite the convergence of the SAM algorithm, the training horizon may also affect generalization. With alternative sampling schedules of the data-set, it can be possible to minimize the training horizon (for instance see Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning by Okanovic et al.). I think it would be useful to examine if the SAM algorithm generalizes better when considering alternative sampling schedules. In such a case, the batch-selection policy could be more significant than the sharpness minimization. As the author also mention flattest solutions are not directly associated with efficient generalization in general.

Further, there is a deep literature for the edge of stability phenomenon (Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability by Cohen et al.). In this case, the sharpness increases at termination (Understanding Gradient Descent on the Edge of Stability in Deep Learning by Arora et al., Section 2. Related Works), however good generalization performance have been observed in some cases. These cases might be considerd by the authors to provide a more complete picture for the empirical part of the paper, and additional conclusion about sharpness minimization and generalization. 

  

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The results are original and provide a partial evidence between sharpness and generalization, however the conclusion is not clear. Theorem 3.1 is an interesting result, however it is limited to a simple architecture but it is valuable.

Weaknesses:
Some parts of the paper are not clear enough. For instance, there exist statements like ""All flattest models generalize"" while the statement holds only for certain cases. It appears that the theoretical guarantees study generalization at the solution, however in practice the generalization of the algorithm might be different (see also the Summary). The main question of the paper has been considered in prior works as well. 

Limitations:
The authors have addressed the limitations sufficiently.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper explores the relationship between sharpness and generalization in overparameterized neural networks. The authors challenge the popular belief that flatness of the loss function implies better generalization and show that sharpness minimization algorithms do not always lead to better generalization. Through theoretical and empirical investigation, the authors identify three different scenarios for two-layer ReLU networks, showing that the relationship between sharpness and generalization is subtle and depends on the data distributions and the model architectures. The paper's contributions include a theoretical analysis of the relationship between sharpness and generalization.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
The authors identify three scenarios for two-layer ReLU networks and provide theoretical and empirical evidence to support their findings. I believe the subtle details in architecture and distribution located in this paper are remarkable. Overall, this paper makes significant contributions to the field of generalization theory and challenges the popular belief that the flatness of the loss function implies better generalization.  

Weaknesses:
1.	 One potential weakness of the paper is that it only examines the relationship between sharpness and generalization for two-layer ReLU networks (personally, I think this is acceptable since it is very hard to analyze more complex scenarios). Additionally, the authors only examine the CIFAR-10 and CIFAR-100 datasets, and it would be valuable to examine their findings on other datasets to determine if their results generalize to other domains. 
2.	The expression of the article is not very friendly and a little difficult to understand.
3.	There are some grammatical and formatting problems, some of which are shown as follows.
1)	Pictures should be resized to make them look more coherent. In addition, the text in the image should be consistent with the font within the paper.
2)	There are some spelling errors, such as “subtlely”.
3)	References are poorly formatted. Please use a uniform format for references. 
4)	Formulas and variables should be consistent with the font within the paper.
5)	Operators should be carefully defined.
6)	There are some formatting issues. For example, lowercase letters as the first letter of a sentence.


Limitations:
The authors have adequately addressed the limitations.

Rating:
7

Confidence:
3

";1
UvBwXdL95b;"REVIEW 
Summary:
This paper solves an interesting problem — joint pose and NeRF optimization on in-the-wild image collections. Unlike prior works such as BARF, this work aims at handling unconstrained images with varying illumination and transient occluders. To tackle this problem, this work incorporates learnable camera parameters, depth prior, and semantic features from DINO as supervision into the NeRF-W framework. Experiments demonstrate that the proposed method outperforms BARF and its variants on unstructured Internet images.    

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.  This work identifies an interesting and important research problem — joint pose and NeRF optimization on in-the-wild images. Prior works on the joint pose and NeRF optimization are limited to controlled settings. 
2. The proposed method is intuitive and combines the NeRF-W framework and several components (e.g., depth prior, semantic features, etc.) from other works (such as NoPe-NeRF).  

Weaknesses:
1. The main contribution of this paper is the problem setting — pose and NeRF estimation in the wild. The proposed method combines NeRF-W and BARF, which has limited technical novelty. 
2. The presentation may be improved. Figure 1 and Figure 2 have not been referred to and explained in the main text. 
3. Missing references:  Meng et al. GNeRF: GAN-based Neural Radiance Field without Posed Camera. ICCV 2021

Limitations:
I did not find discussions on the limitations of this paper. It would be great to discuss the limitations and analyze the possible reasons and future works.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper tackles NeRF training from in-the-wild Internet photos without pre-computed camera poses. The main idea is to leverage (self-supervised) image features and a carefully designed optimization strategy, together with ideas from existing work, including modeling transient regions and using mono-depth supervision. These components work in combination to avoid local minima and enable joint optimization of both pose and NeRF from scratch.

Experiments show that this joint optimization pipeline successfully recovers camera poses in several in-the-wild scenes and leads to good NVS results comparable to those with COLMAP preprocessing, whereas the previous work of BARF and its variants all fail.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
### S1 - Technically sound approach to a challenging problem
- The task of estimating camera poses on raw in-the-wild Internet photos using a gradient-descent-based optimization is challenging, due to noisy correspondences and local minima.
- The proposed method incorporates many good ideas from existing work and works well on a number challenging scenes.
-- For instance, the use of self-supervised image features for registering semantic correspondences via rendering;
-- The mechanism to model transient objects using learned opacity. This paper further adapts the original volumetric opacity in NeRF-W to per-image opacity, which is claimed to be more effective.
-- The use of depth supervision from pre-trained models.

### S2 - Promising results
- The paper demonstrates good results on a number of challenging scenes using Internet photos, where existing methods clearly fails.

Weaknesses:
### W1 - Complicated pipeline
- The resulting pipeline is complicated, involving many components, eg, 6 MLPs in total, and easily becomes confusing. It took me several passes back and forth to understand the exact implementation.
- It also requires a heavily crafted training schedule, gradually activating and deactivating some of the components.
- A critical concern on such a pipeline is its robustness across various scenes. Would one need to fight against all the hyperparameters and the training schedule when training on other scenes. I strongly suggest the authors also present results on other standard datasets (W3).

### W2 - Unclear motivation and potential redundancy of some technical designs
- It is unclear to me why the per-image ""candidate embeddings"" can help with the pose optimization. It seems the motivation is to allow some of the per-image variations (multi-view inconsistencies) to be factored into these per-image embeddings. The paper presents ablation results which shows numerical benefits of this component, but why is it useful for pose estimation?
- Are they still useful in general in the case of a multi-view static scene without transient entities?
- Isn't this the job of the separate mechanism for handling transient regions?
- Also, it is confusing to me why the transient confidence weights $\mathcal{W}_i^{\text{depth}}$ are calculated from the candidate densities $\sigma^{(c)}$, rather than from the transient opacities $\alpha^{(\tau)}$. Are there some redundancy between the two?

### W3 - Only on one dataset
- The paper only presents results on one dataset, consisting of Internet photo collections of 4 scenes, which the method is specifically tailored to.
- However, the paper claims to solve general ""unposed NeRF"". I strongly recommend the authors to also test it on standard multi-view datasets, eg DTU, CO3D etc, to assess the robustness of this complicated pipeline.

### (minor) W4 - Do intrinsics matter?
- How are the camera intrinsics obtained? From COLMAP? If so, this would undermine the value of avoiding cumbersome COLMAP preprocessing.
- Or are they estimated or assumed to be some value for all images?

### Other comments
- There are a few existing work that leverages DINO for pose estimation of objects, which the authors should consider referencing, eg: Zero-Shot Category-Level Object Pose Estimation [1], LASSIE [2], MagicPony [3].
- Line 125: when the ""candidate embeddings"" are first introduced, it was not immediately obvious to me they are per-image embedding vectors. Also, are they jointly optimized? The term ""candidate head"" is also quite obscure to me.
- Fig 2: the blue arrow in the middle connecting $\hat{\mathbf{F}}_i^{(c)}$ to $\theta_4$ is slightly inaccurate, as the $\hat{\mathbf{F}}_i^{(c)}$ is the result of ray integration, whereas the input to $\theta_4$ should be the raw per-point feature $\hat{\mathbf{f}}_i$, if I understand correctly.
- What is the computational overheads compared to vanilla NeRF?

### References:
- [1] Zero-Shot Category-Level Object Pose Estimation. ECCV 2022.
- [2] LASSIE: Learning Articulated Shape from Sparse Image Ensemble via 3D Part Discovery. NeurIPS 2022.
- [3] MagicPony: Learning Articulated 3D Animals in the Wild. CVPR 2023.

Limitations:
The authors included a brief paragraph on the limitation of the image features. I would expect some discussion on the robustness of the proposed pipeline.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper produces a novel approach for optimisation of pose in NeRF scenarios. The core novelty is the addition of a candidate head that improves network stability when the images poses are not yet converged, along with some other tweaks like a transiency inference head or a feature field. 

Edit: I have read the rebuttal, and given the scores from other reviewers and I would like to keep my score.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The idea is novel, interesting and timely.
* The results show good improvements over the current s-o-t-a.
* The experiments section is thorough enough to demonstrate the qualities of the proposed approach. 

Overall, while my negatives might sound long and would seen to outweigh the positives, I think it brings a potentially valuable contribution to the community, so I very much support the acceptance of the paper. My main worry is about the somewhat confusing and short explanation of the core contribution of the “candidate” heads, which should be expanded and improved.

Weaknesses:
* The paper is somewhat confusing to read at times. For example “unconstrained images” are introduced and used without introducing the term. The meaning does become later in the paper, but it would be good to introduce things earlier on. Similarly, I do not see much point for Figure 1, as features have been used for this type of matching for years.
* Even though the literature review section of the paper is quite good, I think the paper does anchor things a bit too much on BARF, and ignores other works such as NoPe-NERF / GARF / etc in the comparisons. I do understand that some of these works were arxiv at the time of the submission (but are publications now), so this should not penalise the paper too much. That being said, it’d be nice if some of these comparisons could be added.
* I also found some of the notation difficult to follow initially as I found not find any outline of it’s meaning. An example is the (c) in line 126 which is not explained. 
* Looking at the core contribution of the paper (i) the intuition (i.e. the “roughly speaking … “ part at line 125+) could be expanded and (ii) the size of the embedding should have been ablated in the results section.
* The results section could have been expanded e.g. with (i) the ablation noted above, (ii) extra ablations where, e.g., the feature matching part is turned off.

Limitations:
* To some extent, but the paper could benefit from a more clear failure case section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel method for optimizing NeRF without a pose-prior and on in-the-wild image collections containing transient occluders and varying lightings. The main contributions are four fold. Firstly, the authors propose a candidate head for NeRFs that uses image-level representations via a learned embedding for compensating inaccurate poses during the early optimization. Secondly, learning a view-independent feature field based on DINO features as intermediate representation increases the robustness of the joint optimization wrt Varying lightings, weather and time. To reduce the impact of transient occluders the authors suggest using a separate network that predicts occluder in 2D image level based on the feature maps. Additionally, to achieve higher accuracy in the geometry the authors apply monocular depth supervision on regions without occluders. Experiments are conducted on four scenes of the Phototourism dataset with initial camera poses set to the identity.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1) The paper is very nicely presented and easy to read.
2) The paper tackles the challenging and relevant problem of in-the-wild NeRF reconstruction without given posen. The contribution is clear and solves different subproblems that emerge in this domain, e.g. transient occluders.
3) Most parts are well motivated, the methodology is technical sound and experimentally justified e.g.:
 - Candidate head sounds plausible and improves pose optimization and image quality significantly, see Figure 4 and Table 3.
 - Feature field optimization seems to help for in-the-wild images, see Table 3.
 - Depth supervision improves performance, see Table 3 and Figure 5.
4) The experimental section contains a comparison to BARF, adapted variants with additional supervision cues and numbers for NeRF-in-the-wild. It appears to be a plausible choice and supports the contribution of the paper.
5) The authors provide code in the supplementary materials. 


Weaknesses:
1) The candidate head is introduced to output color and density, equation 4, however it is later used to predict features, see Figure 2. This appears to be misleading for the readers in the beginning.
2) One of the main limitations of NeRF based methods is the optimization time and an analysis is missing on that. It would be good to discuss the optimization time for the method and the baselines. To get a sense if the time and computational effort is comparable to COLMAP + NeRF-W and others.


Limitations:
Limitations are discussed in the supplementary.
I'd suggest that the authors discuss the overall optimization time as a general limitation of the method, if applicable. There are state-of-the-art NeRF architecture, such as Instant-NGP that facilitate optimization in a few minutes instead of hour or days, which might be a good follow up.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a joint camera pose and NeRF optimisation method that can handle transient scenes, including moving objects and various light conditions, by integrating NeRF-W, BARF, NoPe-NeRF, and DINO-based feature-metric loss in a sophisticated way. 

The method is evaluated on four scenes in the Phototourism dataset, showing good performance compared with (modified) BARF baselines.

---
**After rebuttal**: I have read authors' rebuttal and it addresses my concerns.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It’s a challenging task to estimate camera poses in scenes with moving objects and different light conditions, especially in a NeRF setup. This method proposes to make the pipeline more robust by considering
* Moving objects and lighting conditions similar to NeRF-W;
* Pose optimisation similar to BARF;
* Monocular depth un-distortion similar to NoPe-NeRF; and 
* DINO-based Feature-metric.

In short, the proposed method is novel and leads to promising results. It’s also a plus that code is also provided in supplementary.

Weaknesses:
I think a primary concern is how robust the pose estimation is in more scenes. The method is only evaluated in 4 scenes. Is it possible to have an experiment, especially for pose estimation successful rate on more scenes?

I understand that it takes a long time for NeRF to reach the best rendering quality, but we should be able to tell if the pose optimisation is successful far before NeRF converges. For example, we can consider pose estimation successful if rotation error is lower than $\theta$ degrees in $x$ epochs. In this way, we can see the pose estimation success rate in many more scenes.

Limitations:
Yes

Rating:
7

Confidence:
4

";1
yloVae273c;"REVIEW 
Summary:
This paper studies offline reinforcement learning (RL) with linear function approximation and partial data coverage. The authors propose a primal-dual optimization method based on the linear programming (LP) formulation of RL. They prove a $O(\epsilon^{-4})$ sample complexity in both discounted setting and average-reward setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The algorithm proposed in this paper only requires near-minimal dataset coverage assumption, which is important in offline RL.
2.	The paper also considers average-reward offline RL, which is often neglected by literature.
3.	I like the table for comparison to previous work, which makes the presentation more clear (although I think there is some missing important literature, which I will mention in the weakness section).
4.	The proposed algorithm is both computationally and sample efficient.

Weaknesses:
1.	The first concern is the ‘linear function approximation’ setting, which is restricted. Actually, the main motivation that this paper studies function approximation beyond tabular settings is large state (or action) spaces in practice. However, in real settings, the linear function approximation assumption hardly ever holds. Even in Table 1, many algorithms in previous work apply to general function approximation, which further makes the setting studied in this paper restricted.
2.	Algorithm 1 in this paper achieves a $O(\epsilon^{-4})$ sample complexity. This is in terms of expectation (as shown in Theorem 3.2) instead of high probability. The previous results that the authors are comparing to are high probability bound (e.g., [1,2]), so it would be more comparable if the authors could also show a $O(\epsilon^{-4})$ sample complexity bound under high probability. Also, since the previous work studies general function approximation while this paper studies only linear function approximation, it is hard to say that a $O(\epsilon^{-4})$ sample complexity bound in linear function approximation setting is better than a $O(\epsilon^{-5})$ bound in general function approximation setting. Moreover, [3] achieves the near-optimal sample complexity $O(\epsilon^{-2})$ with near-identical settings of [1,2]. (So I disagree with the statement that ‘It is very important to notice that no practical algorithm for this setting so far, including ours, can match the minimax optimal sample complexity rate of $O(\epsilon^{-2})$’. Therefore, a $O(\epsilon^{-4})$ in linear function approximation is not that attractive compared to previous work.
3.	The authors use an LP formulation of offline RL. I think it would be better to compare to other work using LP formulation, e.g. [4,5], where [4] is computational and sample efficient under partial data coverage and general function approximation, and [5] achieves near-optimal sample complexity under similar settings.
4.	The authors compare the computational complexity. However, it is not that direct to compare an $O(n)$ complexity in linear settings to a $O(n^{7/5})$ complexity in general settings. If the authors really want to demonstrate that their algorithm has better computational complexity, it would be better to do some simulations in the same environment (even in some toy examples).
5.	Another advantage that the authors claim is that their algorithm could be adapted to average-reward setting. However, neither did the authors emphasize and explain the importance and challenges of average-reward settings, nor discuss why (or whether) previous work could not be adapted to average-reward settings. I suggest the authors discuss this a bit more.

**References**

[1] Xie, T., Cheng, C. A., Jiang, N., Mineiro, P., & Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34, 6683-6694.

[2] Cheng, C. A., Xie, T., Jiang, N., & Agarwal, A. (2022, June). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR.

[3] Zhu, H., Rashidinejad, P., & Jiao, J. (2023). Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

[4] Zhan, W., Huang, B., Huang, A., Jiang, N., & Lee, J. (2022, June). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory (pp. 2730-2775). PMLR.

[5] Rashidinejad, P., Zhu, H., Yang, K., Russell, S., & Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studied offline RL in linear MDP setting, where the transition and reward have low-rank structures and the feature $\phi$ is known. The authors formulated the problem in a primal-dual way and proposed a gradient-based algorithm. They provided convergence guarantees, which only requires coverage over optimal policy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper writing is clear and easy to follow.

The discussion and comparison with previous works is very detailed.

The algorithm is computationally efficient. The algorithm design has some interesting points, especially the reparameterization design to avoid knowlegde of $\Lambda^{-1}$ and updates for variables $v$ and $u$.

The coverage assumption seems weaker than previous literatures.

Weaknesses:
I didn't see too much technical novelty in the method and proof.

The setting is linear MDP, which is kind of restrictive.

Convergance rate is kind of far away from optimal.

Limitations:
N.A.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed an primal-dual framework for offline reinforcement learning in linear MDP Contrary to the more common case of finite horizon, they considered the case of infinite horizon with discounted reward. They reduced the problem of offline reinforcement learning to a problem about solving the saddle-point of a Lagrange form. They designed an algorithm which uses stochastic gradient-based optimization to solve the saddle point. They provide a sample complexity of O(\eps^-4) for both cases of discounted MDP and averaged-reward MDP, and their algorithm is also computational efficient. 

To summarize, the formulation of offline RL into a linear programming problem is very interesting. The proof seems very solid, and I like the comparison for the concentrability constant in the last discussion section. The comparison for the constant C is thorough and very good. 

However, I still have some questions about some details in the main text.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The formulation of offline reinforcement learning to a linear programming problem is very good.
2. The algorithm is clearly motivated by solving the saddle points of a Lagrange form. The algorithm itself is simple and computationally efficient, with a guaranteed sample complexity for both discounted MDP and averaged-reward MDP.
3. They proposed a new concentrability constant C and compare it to other constants appearing in other literatures about offline RL. I think the understanding of the relationship of these concentrability constant is basically correct and very clearly expressed.
4. The proof seems very solid and the result in averaged-reward case is new.

Weaknesses:
1. I have some question about your comparison to previous results. Your main references are Cheng et al and Xie et al. 

1.1 For Xie et al, the Theorem 3.2 in https://proceedings.neurips.cc/paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf implies that their sample complexity is O(1/\eps^2) when applied in linear function approximation. This result is based on assumption3 in their paper. This assumption naturally holds in your paper since you consider linear MDP and they consider the case of 'linear function approximation' (for their difference, see point 2). So it is natural for you to compare your sample complexity to this result, not the O(1/\eps^5) one. [notice that, their algorithm in section 3 is computationally inefficient]

1.2 In Theorem 4.1 in Xie's paper, their sample complexity is O(1/\eps^5) when applied on general function approximation, and O(1/\eps^3) when reduced to linear function approximation case (see paragraph 'Dependence oon T'). Again, their assumption for linear function approximation holds in your case. **This algorithm, however, is computationally efficient.** So you should also compare with this alg with  O(1/\eps^3) sample complexity.

1.3 In Cheng's paper, in theorem 5, their sample complexity seems to be O(1/\eps^3), not O(1/\eps^5). I wonder how you derive their sample complexity in Table one.

1.4 I am not sure how you get the O(n^{7/5}) computational complexity for Xie's paper. Could you derive it in more detail?

2. I think in many places in your paper, you confuse the two terms: linear MDP and linear function approximation. Your case is called linear MDP instead of linear function approximation, so I suggest you changing the wrong terms. For reference, both Xie's paper and Cheng's paper consider the 'linear function approximation' case.

Limitations:
/

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies offline reinforcement learning with linear function approximation. They propose a primal-dual algorithm, formulating linear RL into a minimax problem and solving it with gradient descent-ascent. Sample complexity analysis is provided for infinite-horizon discounted and average-reward MDPs, where the rate is $O(\frac{1}{\epsilon^4})$ for both settings.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is primal-dual and thus easy to implement in practice.
2. The paper provides rigid theoretical analysis.

Weaknesses:
1.  The newly defined coverage ratio $C_{\phi,c}$ is a little strange when $c\neq \frac{1}{2}$. For example, when we choose $c=1$ and thus we don't need the knowledge of $\Lambda$, the coverage ratio $C_{\phi,1}=\sum_{x,a}(\frac{\mu^*(x,a)}{\mu_B(x,a)})^2$. Then when $\mu^*=\mu_B$, $C_{\phi,1}$ will become $|X||A|$. However, in the literature, when the behavior policy is the same as the optimal policy, the coverage is typically 1. The authors claim that we can estimate the $\Lambda$ via the offline dataset so that we can choose $c=\frac{1}{2}$, but do not provide any theoretical analysis about this point. I will be more convinced if the authors can give more rigid proofs for this method.
2. The sample complexity is worse than the typical rate $\frac{1}{\epsilon^2}$.

Limitations:
None.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of offline reinforcement learning (RL) for linear Markov Decision Processes (MDPs) under the infinite-horizon discounted and average-reward settings. The authors propose a primal-dual optimization method based on the linear programming formulation of RL, which allows for efficient learning of near-optimal policies from a fixed dataset of transitions under partial coverage. The proposed algorithms improve the sample complexity compared to previous methods from $O(\epsilon^{-5})$ to $O(\epsilon^{-4})$ under the discounted setting and provide the first line of result in the average-reward setting with realizable linear function approximation and partial coverage.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 The proposed algorithm improves existing algorithms in both statistical efficiency and computational efficiency under the discounted reward setting with the linear function approximation (we note the baseline may handle problems beyond the linear MDPs).

2 The algorithms presented in this paper do not explicitly leverage the principle of pessimism, but focus on the linear programming formulation of MDP, and rely on a new reparametrization trick extended from the tabular case. The technique itself seems to be novel to me.

3 The algorithms present the first line of work for the offline average-reward MDP.  

4 The paper is easy to follow, with a thorough comparison with existing work that clearly positions the results in the literature.

Weaknesses:
1 I am confused about the requirement of $\Lambda$ to be invertible (line 140) as this seems to be very closely related to the uniform coverage condition where we assume that the smallest eigenvalue of $\Lambda$ is lower bounded from zero. I am wondering what is the key difference between them. Can you elaborate on this with some intuitions or examples?

2 The authors discuss the relationship between the coverage condition considered in this paper and that of [1] and show that the coverage condition is a low-variance version of the standard feature coverage ratio if $c=1/2$. However, in this case, the algorithm explicitly uses $\Lambda$, while the PEVI proposed in [1] does not. In contrast, $c=1$ leads to a worse bound but we do not need the knowledge of $\Lambda$. Could you provide a more detailed characterization or example to illustrate the difference between these two cases?


typo: line 328, $\epsilon^2 \to \epsilon^{-2}$

[1] is pessimism provably efficient for offline rl

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors investigate offline RL in linear MDPs and introduce a novel LP-based method. They assert that their proposed approach achieves the lowest sample complexity of $O(1/\epsilon^4)$ among computationally efficient algorithms. In comparison, existing computationally efficient algorithms can achieve $O(1/\epsilon^5)$. Additionally, the author's theory can be extended to the average reward setting.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* To the best of the author’s knowledge, in offline linear MDPs, the result in the average-reward setting is novel. 
* The LP formulation in linear MDPs is worthwhile to investigate 


Weaknesses:
* I am uncertain about whether it is appropriate to claim that existing offline RL algorithms in linear MDPs achieve $O(1/\epsilon^5)$. It appears that [38] may have better sample complexity. In Table 1 of the manuscript, the author mentions that [38] cannot handle the discounted setting. However, extending from the finite-horizon to the discounted infinite-horizon setting is relatively straightforward. Hence, this comparison may not be entirely fair. If [38] indeed has better sample complexity, it significantly impacts the author's contribution. Thus, I currently rate the paper with a score of 4.

* I am not entirely certain about the significance of the extension to the average reward case.

* Presently, I cannot determine whether the reason [9] and [36] cannot handle the average reward case is due to the algorithms or their analysis. If this limitation arises from their analysis, their algorithm has the potential to be superior as it can handle more general MDPs.

Limitations:
They discussed. 

Rating:
4

Confidence:
2

";0
86dXbqT5Ua;"REVIEW 
Summary:
This paper proposes a geometry-informed neural operator for arbitrary geometry, to facilitate the learning on the solution operator for large-scale 3D CFD simulation.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The proposed GINO model applies graph-kernel blocks for the encoder and decoder, for processing features in the latent uniform space, with the Fourier blocks running on the latent space to capture the global interaction. The proposed method provides significant runtime speedup.

Weaknesses:
For the experiment, the authors only perform the evaluation on the car category of ShapeNet dataset. It’ll be more persuasive to have the proposed model evaluate the other categories rather than only a single category.

Limitations:
As pointed out by the authors, this work is constrained to a specific category and limited to CFD with more complex shapes. It'll be with practical significance if the proposed work could tackle these limitations.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper presents geometry-informed neural operator (GINO), solving computational fluid dynamics (CFD)  problems. It combines graph neural operations (GNO) and Fourier neural operators (FNO) to adapt to irregular discretized grids. The authors have tested the model on two large-scale datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:

1. The authors propose combining GNO and FNO to leverage the advantages of both methods, such as analyzing local and global information and efficiently processing irregular grids. They also conduct experiments demonstrating that GINO exhibits discretization invariance over the latent grid and the input-output mesh.

2. The authors have generated two CFD datasets using various vehicle datasets. It would greatly benefit the learning physical simulation community if these datasets were made publicly available upon the paper's acceptance.

Weaknesses:
1. The definition of the $\kappa$ operator in the graph operator block is unclear. It is not specified whether it measures the distance between two points or the similarity of their features. Additionally, it would be helpful to know if the $\kappa$ operator has any learnable parameters. Providing more details in the paper would make it self-contained and enable readers to understand the methodology.

2. The paper mentions the input of SDF features and surface points. It would be beneficial to clarify if these two types of data are fed in a uniform manner. For instance, are the point locations associated with their corresponding SDF values, while the surface points are assigned a value of 0?

3. The authors claim efficient graph construction in the paper. However, it appears that once the graph operator block finishes processing the input, it results in a regular grid. It would be important to address whether the implementation takes this into consideration and ensures the efficiency of the graph construction process.

Limitations:
See weakness.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach for applying Fourier or other Neural operators to complex geometries by prepending a “learnable projection step” via Graph Neural Operator (GNO). Unlike previous methods that morphe complex geometries into regular domains, this approach projects (learnable) sampled nodes onto nearby regular grids, providing advantages such as discretization invariance and reduced computational overhead by sub-sampling.

However, the method is limited to simple geometries and fails to account for variations in geometry and subtle geometry features. **The paper also lacks sufficient datasets and comparisons with relevant methods, such as the GNN family.**

While the ideas are somewhat novel, though not groundbreaking, **the paper requires further evaluation to demonstrate its strengths and limitations.**

**A borderline rejection is assigned, with reconsideration if the issues are addressed during the revision period.**

## After rebuttal
- The authors improved their references, presentation, and empirical evaluation; Hence I increased my score to 5

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Discretization invariant
- Improved efficiency and scalability by sub-sampling
- Improved empirical performance

Weaknesses:
- Insufficient number of datasets and comparisons:
  - More datasets should have been included, such as cylinders and airfoils from [1].
  - Comparisons with representative methods from the GNN family, such as MeshGraphNet[1], MSGNN-Grid[2], and BSMSGNN[3], should have been discussed or ideally conducted.
  - Among the mentioned papers [1] to [3], a crucial comparison would be with [2], which also utilizes a background grid as a helper but differs in the backbone as it did not use FNO. This would have provided a valuable benchmark for evaluation.

- The overly ambitious illustrations: such as the claim in the abstract that ""...(the method) can be applied to any geometry"", which is not the case. A more objective approach in illustrating both the strengths and limitations would be appreciated by readers.

- Lack of clarity in dataset presentation and results:
  - Fig.2 suggests that the shapes are overly simplistic and the pressure distributions appear uniformly smooth, indicating ease of learning. It is essential for the author to provide a more comprehensive and clear explanation, including typical examples of datasets and variations in geometry among examples.

[1] Pfaff, Tobias, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W. Battaglia. ""Learning Mesh-Based Simulation with Graph Networks."" Link: https://openreview.net/forum?id=roNqYL0_XP

[2] Lino, Mario, Chris Cantwell, Anil A Bharath, and Stathi Fotiadis. ""Simulating continuum mechanics with multi-scale graph neural networks."". Link: https://arxiv.org/abs/2106.04900

[3] Cao, Yadi, Menglei Chai, Minchen Li, and Chenfanfu Jiang. ""Efficient learning of mesh-based physical simulation with bi-stride multi-scale graph neural network."". Link: https://openreview.net/forum?id=2Mbo7IEtZW

Limitations:
The method is limited to very simple geometries due to 2 facts:
1. Only relies on the point cloud. A counter-example is porous material where the point cloud can be the same but the tunnel is very different.
   - In other words, although this method is discretization invariant, it is also geometry-ignorant, which is not desired.
2. Sub-sampling, if the key flow feature is determined by some subtle geometries, which is very common, this method fails again. The limitation also is reflected in the dataset, as the pressure seems to look really smooth for all cases, and all geometries are very simple.

I recommend the authors objectively illustrate these facts, even at the beginning (you do not have to emphasize them; mentioning them is enough). More experiments objectively showing the limitation (maybe in the appendix) are also appreciated.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The authors address the task of learning to solve large-scale PDEs based on a geometry-informed neural operator. The combination of graph neural operators (GNO) and Fourier neural operators (FNO) allows the exploration of the benefits of being able to handle irregular grids and locality of operations to allow efficiency (due to GNO) as well as capturing global interactions (due to FNO), thereby overcoming the limitations of the individual approaches. In more detail, the surface (e.g. point cloud) is input to a geometry encoder that encodes the irregular grid information on a regular grid structure based on local kernel integration layers through GNO with graph operations. Then the result is concatenated with signed distance features. Then, a sequence of FNO layers is used (i.e. on latent space) for global kernel integration. The respective intermediate result is projected back to the domain of the input geometry.

To show the potential of their approach, the authors carry out experiments on a novel own dataset as well as a ShapeNet car dataset, where they report quantitative results regarding training and test errors.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Technical soundness and novelty:
The method seems novel and performs beneficial over some potential alternatives in terms of speed and accuracy, since it allows a significant speed-up in comparison to the OpenFOAM solver and offers more accuracy than GNO, GeoFNO and U-Net.

Evaluation:
The authors provide quantitative and qualitative results including comparisons to baselines regarding training/test errors.

Exposition:
The paper is well-structured and mostly readable. Figure/tables and their captions are informative. 


Weaknesses:
Technical soundness and novelty
- The discussion of limitations provided by the authors is rather short. Blending out the benefits of physics-informed approaches, that led to extreme speed-ups over OpenFOAM particularly for fluid simulation as well as offer generalization to novel scenes without being limited to object categories, limits the conclusions drawn from the presented work. The relation to these should be better emphasized to clarify what the presented method adds and whether it can be combined with these.  
- The datasets are described in a very short manner. Especially for the new dataset, it would be relevant to see more details such as a systematic overview on the key aspects. In addition, clarifications on whether the trainings of different approaches converged within the used 100 iterations would be relevant. In Figure 2, the rightmost part is also difficult to interpret given the used color scale.

Evaluation:
- Reporting training and test errors only gives limited insights on where the errors are better/worse in comparison to previous approaches. E.g., it is not demonstrated that physical phenomena (such as the Magnus effect or Karman vortex streets) are accurately represented.
- What are limitations/failure cases that cannot be handled that well with the presented operator?

References:
Discussing other developments such as

Brandstetter et al., CLIFFORD NEURAL LAYERS FOR PDE MODELING
-> usage of multivector representations together with Clifford convolutions. The authors show the benefit of Clifford neural layers by replacing convolution and Fourier operations in common neural PDE surrogates by their Clifford counterparts on 2D Navier-Stokes tasks

or physics-informed upgrades of U-Net such as e.g.

Wandel et al., Teaching the incompressible Navier–Stokes equations to fast neural surrogate models in three dimensions
-> an example of physics-informed fluid simulation based on physics-informed U-Net

or the splitting of the solver into region-wise optimization such as

Balu et al., Distributed Multigrid Neural Solvers on Megavoxel Domain

would improve the discussion of the presented approach and its potential in the context of related work, especially since the presented approach involves quite strong assumptions (dependence on training data/shape category) and not leveraging physics-informed models therefore should be discussed. 


Limitations:
Limitations are shortly discussed, but seem to be severely limiting (category-specific approach, lacking generalization capabilities, unclear relation to speed-ups achieved based on physics-informed approaches).

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a framework to learn a neural operator for large-scale 3D PDEs. The framework uses a well-implemented Graph Neural Operator (GNO) to transform the irregular grid into a regular grid, so that it enables the powerful Fourier Neural Operator (FNO) to work on irregular input data, such as point clouds, in a discretization invariant way. 

The main contribution of the paper is to use GNO to transform large-scale irregular grids (point clouds and SDF) into regular grids, to make the FNO suitable. Based on this, GINO realized 100,000x speed-up compared to GPU-based simulators on large-scale stable CFD problems. The model is proven to be working on datasets with a high level of complexity and realism. The paper also generates two large-scale CDF datasets, which require a large amount of time to simulate and generate, which is valuable.  


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. Clear written about the problem setting and equations and symbols. 

2. The main idea is useful but not too complicated to understand. An efficient combination of existing methods. 

3. The method has a general potential for various kinds of PDEs. 

4. The method has shown great performance in engineering-level experiments, fulfilling its great potential for applications. 

Weaknesses:
Although the experiments have demonstrated the main ability of the model, the experiments are not complete enough to support all claims and novelties. 

1. Since the model is a new combination of two existing methods GNO and FNO, the main contribution is the new usage of GNO. Then the main thing required to be demonstrated should be “GNO is more proper and has great encoding and decoding ability between irregular and regular grids”. Regarding this, some different encoding methods should be compared, such as GNN, kNN.

2. Discretization invariance is said to be important, but no direct experiments could support this. Although Table 5 is relevant to this, but the variables are not kept so it’s not a direct support. In other words, a GNN+FNO baseline should be added. 


Limitations:
1. As the paper stated, the trained model is limited to a specific observed category of shapes. The training for the operator requires a training dataset of high quality. 

2. The proposed framework should be general for various kinds of PDEs, but only stable NS equations are tested. 

3. The framework is for 3D PDEs, which could be modified for time-dependent ones. 

Rating:
5

Confidence:
4

";1
bbL20Oupi4;"REVIEW 
Summary:
The paper studies fractional allocation rules when each agent indicates a ranking over the agents that agrees to represent her, to overcome impossibility results of deterministic rules. The authors consider two different rules which are shown that are equivalent. They also provide a polynomial time algorithm for finding the outcome of one of the two rules.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
-Very well-written paper

-Interesting model 

-Natural extension of the axioms to fractional allocations

-Technically involved and interesting results

-Use of known lemmas in interesting and elegant ways

Further notes:

Line 73: ""casting voter""

Line 134: I think you mean ""|B|=|V(G)|-1""


Weaknesses:
The choice of  the top-rank priority axiom, while it is quite natural, seems a bit arbitrary. It would be nice to show what kind of other axioms would lead to the same impossibility result. 

Limitations:
 The authors adequately addressed the limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper primarily examines two equivalent fractional delegation rules for Liquid Democracy (transitive delegate voting): Mixed Borda Branching and Random Walk Rule. The main contribution of this paper is the equivalence between these two rules in the generalized setting of fractional delegations. This result is complemented by an axiomatic analysis of the rules w.r.t. notions of anonymity, copy-robustness, confluence, and top-rank priority. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The equivalence between the two delegation rules is non-obvious and a significant contribution. Moreover, the proof of equivalence is highly non-trivial. These results in Sections 4-5 are of considerable value. But I am not sure if those alone are enough to carry the paper (see Weaknesses). Although perhaps they are.

Weaknesses:
Unless I have misunderstood something, it doesn’t appear that the axiomatic analysis really solves, or resolves, any of the important issues discussed in the introduction. It circumvents them instead in a way that feels cheap.


First, allowing fractional delegations clearly means confluence is no longer a constraint. The authors state essentially this in an incredibly oblique way in the proof of Theorem 8, “One can verify that every delegation rule that can be formalized via a Markov chain on the delegation graph (G,c) satisfies confluence.” Confluence requires that if v1 delegates to v2 that the remainder of their delegation path much be consistent. Suppose, for example, that v1 is the only voter delegating to v2, and v2’s delegation is split between v3 and v4. No we “send” ½ of v1’s vote and ½ of v2’s vote to v3, and the remaining ½ + ½ to v4. Of course, this is functionally equivalent to sending v1’s entire vote to v3 and v2’s entire vote to v4, violating confluence. In reality, the axiom of confluence wasn’t generalized, it is made irrelevant, because any delegation that violates confluence can be trivially converted to a fractional one that satisfies its relaxation. Whether we talk in terms of dividing vote tokens or probability distributions over delegations doesn’t matter, it’s the same principle. Similarly, simple path vs. walk makes no difference here. 

Second, the authors state that the purpose of giving delegations as orderings is to prevent delegation cycles and isolated voters. In footnote 3 on page 4, the authors admit that they handle isolated voters by ignoring them entirely and removing them from the graph. However it is not discussed how this bears on the axioms. Ignoring such voters is the same as assigning them a voting weight of zero, whereas if they cast their vote directly they get a voting weight of at least one (and possibly more if other isolated voters delegate to them). This violates copy-robustness, but this violation does not appear to be mentioned.


Lastly, while this is a minor note, the authors should not say that they generalized the axioms, but rather that they relaxed the axioms. The authors generalized the class of delegation rules, and relaxed the axioms required.

Edit:
Based on the author(s) rebuttal, I have improved my understanding of the paper and its results.I no longer stand by my original criticism.

Limitations:
No further limitations.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The authors study liquid democracy with fractional (i.e., splittable) delegations. They extend previous work by Brill et al. on delegation rules that satisfy anonymity and copy-robustness, and demonstrate that two delegation rules (mixed Borda branching and the random walk rule) that were previously thought to be different and each satisfy one property are actually the same rule that satisfies (generalized versions of) both properties. Their algorithm for computing the outcome of the combined rule is also the first efficient algorithm for a problem in semi-supervised learning, the directed power watershed.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ The paper is well-written and easy to follow, and the problem they study is well-motivated in the context of liquid democracy.
+ The algorithm is nontrivial and of independent interest to other communities in computer science. 
+ It's also nice to see that two previously-proposed rules are actually the same; it's extra satisfying that this rule happens to satisfy generalizations of two (really, four) axioms that were thought to be hard to simultaneously satisfy.

Weaknesses:
- My main hesitation with this paper is the use of fractional delegations in liquid democracy. One central tenet of LD is the ability to immediately remove a delegation from someone who cast a vote in a way you did not approve of, which becomes much more difficult with splittable delegations. Additionally, if you allow agents to know exactly where portions of their vote ended up, they will potentially be given a lot of information about the rest of the delegation network. 
- Typos: line 73: casting, line 166: copy-robust, not copy-robustness, line 342: lens

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper studies liquid democracy where voters can delegate their votes to others instead of casting them directly. Within this framework, some voters act as casting voters, while others delegate their votes. Delegation rules determine how casting voters are chosen for each set of delegating voters. However, voters have preferences regarding whom they trust more to represent their votes.
When the entire vote must be delegated, it becomes challenging to satisfy all of several desirable axioms simultaneously. Instead, the authors explore fractional/ probabilistic delegations, allowing a vote to be split across casting voters. They demonstrate that by employing the random walk rule, it is possible to recover the possibility of meeting all axioms.
Moreover, the authors note that the distribution over branchings, ensuring that every delegating voter is connected to a casting voter, follows a uniform distribution over all minimum-cost branchings. This connection has also been previously observed in the work of Fita Sanmartin et al.
To compute the delegation distribution efficiently, the authors propose a polynomial time algorithm that builds upon Fulkerson's algorithm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Fractional delegations are a natural extension and furthermore they are transparent in the sense that a voter can know what proportion of her vote was transferred to which casting voter.
- The result on the possibility of confluence, anonymity and copy-robustness if we move to the realm of fractional delegations is nice to have. 
- The authors leverage results from graph theory and combinatorial optimization to obtain their results, hence introducing techniques from this literature to their community.

Weaknesses:
- Equivalence of Mixed Borda Branching and Random Walk rule: Given an understanding of the Markov chain rule, the equivalence between the Random Walk rule and Mixed Borda Branching seems immediate. The authors' claim of surprise at this equivalence is somewhat perplexing, as the connection should have been anticipated with knowledge of the Markov chain rule. It would have been more understandable if other authors had defined Mixed Borda without recognizing this link, but the authors themselves acknowledge that this interpretation has already been observed by Rita Sanmartin et al. Consequently, the discussion on equivalence could be shortened by referring to Rita Sanmartin et al.'s work rather than presenting it as a surprising result.
- Algorithm 2 and Directed Powershed: Unfortunately, I cannot confirm whether the Directed Powershed algorithm is novel to this paper. It appears that Sanmartin et al. discuss an extension of the undirected version to address the directed case (Page 5: ""In section 5, we show how the Power Watershed can be generalized to directed graphs by means of the DProbWS.”, Page 8: ""In the ProbWS paper [12], it was proven that the Power Watershed [6] is equivalent to applying the ProbWS restricted to the minimum cost spanning forests. This restriction corresponds to the case of a Gibbs distribution of minimal entropy over the forests. In this section, we will prove the analogous result for the DProbWS: When the entropy of the Gibbs distribution over the directed in-forests (3.1) is minimal, then DProbWS is restricted to the minimum cost spanning in-forests (mSF). This permits us to define a natural extension of the Power Watershed to directed graphs.
“). So they claim to leverage existing knowledge (Power Watershed: A Unifying Graph-Based Optimization Framework, 2010).  However, if the problem remains unsolved, a discussion of the Power Watershed algorithm and its limitations in extending to the directed case would be necessary. It is worth noting that Algorithm 2 has limitations in practicality, with a runtime of O(n^7) (up to log factors), while Power Watershed may offer better efficiency.
- The anonymity gained through randomizing over mincost branchings seems not surprising,. It is unclear where the challenges lie in proving that the other properties still hold ( as the authors seem to demonstrate more general versions of the axioms).

It would be valuable to have an in-depth discussion of the supervised learning literature, as the problem of liquid democracy appears to be studied under different names with distinct requirements in various fields. Exploring the Power Watershed algorithm and its directed extension, along with providing a comprehensive explanation of the connections between different fields, would be a very useful contribution. This could introduce the supervised learning literature to the computational social choice community, while also allowing the supervised learning community to benefit from the axiomatic analysis conducted in this work. If the extension of power watershed algorithm to the directed from the literature turns out to be faulty or very unclear, I will increase my score.

Limitations:
No concerns, limitations are addressed in the appendix.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies algorithms for assigning delegation weights in liquid democracy in the setting where participants indicate a set of trusted delegates, along with a ranking describing their preferences over these delegates. Notably, the algorithms it considers permit fractional delegations - i.e., voters have voting weight 1, and they can delegate it fractionally across multiple possible delegates.
The paper considers two algorithms for finding a delegation graph (i.e. an assignment of fractional delegations from voters who wish to delegates, to trusted delegates). These two algorithms are: (1) the random walk rule, a Markov-chain based rule proposed in past work;  and (2) the mixed Borda branching rule, which is a uniform average over all min-cost branchings, called “borda branchings”, where a branching occurs on a weighted digraph representing to which casting voters each voter is willing to delegate to. 

The paper makes three main contributions: 
(1) Provides a polynomial-time algorithm for computing mixed Borda branching. This algorithm is an adaptation of Fulkerson’s algorithm, and relies on two of its canonical properties (as proven by Fulkerson). This algorithm is of independent interest to the direct power watershed problem. 
(2) Proves the equivalence of rules (1) and (2) above. 
(3) Shows that rule (1) (and therefore rule (2)) satisfies three axioms: confluence, anonymity, and copy-robustness — a combination of axioms identified in past work that cannot be simultaneously achieved with delegation rules that require voters to delegate their voting weight to a single delegate.


Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The high-level structure of the paper is clearly laid out

- Some aspects of the paper’s analysis (e.g., axiom iii) are practically motivated, and the setting of liquid democracy in general is well-motivated

- The authors made an effort to make the paper understandable, with multiple diagrams to aid explanations

- The paper speaks to and builds on multiple aspects of the literature (e.g., random walk rule, existing axioms)

- The technical results seem to holds, and there is sufficient technical exposition to understand why the results are true.

Weaknesses:
1) The potential impact of this paper is not clear entirely to me. What is the main technical challenge with proving the results and what makes it hard? Why are axioms (i) and (ii) important for a rule to satisfy in practice? 

2) I found it very hard to understand several phrases in the introduction. In many cases, it seems that the paper is assuming too much specific knowledge of the liquid democracy literature. For example:
- Line 47: “(i.e., a digraph with a rank function on the edges)” what is a “rank function on edges”?
- Line 51: In the explanation of confluence, what is a “subpath of v1”?
- Line 64: “However, since none of the axioms connects the meaning of the ranks to the decisions of the delegation rule” I do not understand what this sentence means.
- Line 66: “A single top-rank edge to a casting voter should always be chosen over any other delegation path.” I don’t understand what this sentence means.


Limitations:
Yes

Rating:
6

Confidence:
3

";1
nQ84YY9Iut;"REVIEW 
Summary:
Boosting algorithms is an important fundamental question in SLT starting with celebrated AdaBoost algorithm. While the original results consider binary setting and the condition on the weak learner is straightforward, it is not trivial to formulate the same result for classification with more than one classes.

I did not have the chance to go through the proofs carefully due to time constraint. I do not see where there could be a potential mistake that cannot be fixed. I did not read through section 4 and I am not familiar with list learning.

pros:
- interesting fundamental problem
- the paper is well written
- a new condition for multiclass boosting
- relatively simple proof
- connection to list learning

cons:
- the only disadvantage I can see is that this paper would better fit other venues


minor comments and questions:
- should Theorem 1 be stated only for realizable case, since you relying on the compression scheme generalization bound?
- it is better to refer to Theorem 30.2 (than to Corollary 30.3) in Shalev-Swartz and Ben-David since your ""outer"" risk is equal to zero only with probability close to one
- perhaps this is obvious or I have missed it, but how does your condition compare to conditions of existing results for the multi-class boosting?

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
-

Weaknesses:
-

Limitations:
-

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper tackles the problem of multi-class boosting by proposing a novel definition for weak-learning based on list boosting. The authors define a weak-learning condition for multi-class learning based on the simple principle of slightly better than random guessing. This definition is then used as a stepping stone in order to filter the potential classes for a sample recursively, thus reaching a stage where either the binary weak-learning condition can be applied, or the weak-learning condition BRG leads to boostability. The authors propose several theoretical justifications and definitions for the results presented in the paper.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
# Motivation
Multi-class boosting has received a lot of attention since the introduction of binary boosting, in particular since the extension of the weak-learning condition from binary to multi-class setting is tricky. As such, the novel definition of weak-learning introduced in this paper is quite interesting, especially since its simplicity mirrors the binary one.

# Theoretical results
The strongest point of this paper. The authors propose several theoretical justifications throughout the paper, and the generalization result in Theorem 2 is quite promising, same for the relation between weak PAC learning and list PAC learning in Theorem 3.

Weaknesses:
# Existing frameworks
There are several existing frameworks for multi-class boosting based on specific weak-learning conditions (some of which are cited in the paper). I strongly think that an actual comparison between the proposed framework and the existing ones should have been included. Particularly for methods such as Adaboost.MM and Adaboost.MR and their WL conditions, and the WL framework introduced in ""Multiclass boosting and the cost of weak learning."" [6].

# Experimental results
I'm not entirely sure why no experimental results are proposed in the main paper. There are several multi-class boosting that have been successfully used in practice, despite their WL conditions, as such it is important for new/novel methods to be compared to state-of-the-art approaches. In particular how the proposed method fares against state-of-the-art ones both in accuracy (or other multi-class performance measures) and in runtime. The recursive nature of Algorithm 1 and its dependance on 3 hyper-parameters might lead to prohibitive runtimes, even though several optimizations have been mentioned in the main paper.

# Conclusion and future works
I strongly think that a section on the future works and possibilities would've been more appropriate than the proof of Theorem 2. More globally, the paper could benefit from a better global organization.

Limitations:
The limitation of have not been discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel treatment of traditional (non-gradient) boosting to multiclass prediction problems. In particular it shows that a novel relaxation of the weak learning criterion allows the definition of boosting algorithm with the usual success guarantee of reducing the empirical misclassification rate on the given data sample arbitrarily with high probability. This relaxation requires the base learner to also accept as “hint” a subset of labels and to, for each possible subset size k, to return a hypothesis that achieves an error rate better than 1-1/k with high probability. The proposed boosting algorithm makes use of this property by recursively eliminating candidate labels from the hints of each training example based on the hypotheses produced by previous iterations. Moreover, the paper shows a connection of the proposed theory to list-PAC learning and in particular gives a new characterisation of list-PAC learnability.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The paper develops a beautifully simple extension of boosting to the multiclass case that, in contrast to previous approaches, does actually work to define a successful strong learner
- The connection to list learnability are very interesting
- The paper is well written and accessible

Weaknesses:
- Papers on classical boosting, in contrast to the additive ensembles produced by gradient boosting, feel rather narrow at this point. In particular, generalisations from misclassification rate to proper statistical loss functions are unclear.
- There is no empirical performance investigation of the proposed recursive boosting algorithm

Limitations:
One potential limitation might be the definition of weak learners that satisfy the weak learning assumption. This is because it seems to require the restriction to different subsets of classes for different inputs. It is not immediately obvious to me how one would incorporate this, e.g., with a decision tree (if it depends on the concrete model at all). 


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a generalization of boosting to the multiclass setting. To this end, the authors introduce a new weak learning assumption for multiclass based on a ‘hint’, which takes the form of a list of $k$ labels, named ‘Better-than-Random Guess’ (BRG). the authors present a main boosting method and provide theoretical analysis for PAC guarantees. Finally, the authors demonstrate applications based on the framework of List PAC learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper proposes a new weak learning assumption, which encompasses both the binary case (i.e., $k=2$) as well as the cases with $k>2$.
- The main method is well-written.
- This paper provides abundant theoretical analyses.

Weaknesses:
- Although this paper is based on theories, it lacks experiments, including synthetic ones, and there is no conclusion.
- Due to the lack of experiments, there is insufficient comparison with previous works, with only half a page dedicated to it in section 1.2.

Limitations:
.

Rating:
4

Confidence:
1

";1
j7x9wW3tCf;"REVIEW 
Summary:
This paper addresses the problem of knowledge graph completion by proposing a two-stage framework — learning from structural and textual knowledge (LSTK). This novel framework leverages both structural and textual knowledge to learn rule-based systems, which provides a unique approach in the realm of KGC research. The paper is premised on the idea that the typical reliance on structural knowledge alone in rule-based systems is a limiting factor. This leads to the proposition of a system that utilizes ""soft triples"", or triples with confidence scores derived from textual corpora via distant supervision and a textual entailment model with multi-instance learning. In the second stage, a rule-based system for KGC is learned using both these soft triples and the hard triples of existing KG triples. The paper also introduces a novel formalism for learning rules, referred to as text-enhanced rules or TE-rules, which can mitigate the negative impact of noisy data in soft triples.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
— The two-stage framework introduces an innovative way of leveraging both structural and textual knowledge for KGC, thereby potentially mitigating the limitations of existing methods.

— The introduction of TE-rules provides a robust mechanism for learning rules, to mitigate the negative impact of noises from soft triples.

— The paper introduces three new datasets for empirical evaluation, contributing further to the body of resources available in this field.


Weaknesses:
— The authors utilize a single method to estimate the confidence scores of the soft triples, resulting in a potential lack of generalizability.

— While the introduction of three new datasets is commendable, the paper does not offer a clear comparison of these datasets with established benchmarks. 

— Furthermore, it remains uncertain how well the proposed method would perform on existing benchmarks.

— The paper falls short in discussing the scalability of the proposed method. It is unclear how the proposed framework would handle large-scale knowledge graphs, particularly considering the potential computational costs of generating and handling soft triples.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes an inductive knowledge graph completion method based on both textual and structural information. It introduces a new task of how to leverage information from both modalities when there are both structured and textual data available. This paper proposes a rule mining method called LSTK-TELM, where LSTK represents a rule mining framework. Three datasets are constructed based on the relation extraction dataset for evaluation. Experimental results demonstrate the effectiveness of the proposed framework, with LSTK-TELM outperforming other baseline methods.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
S1. A novel knowledge graph completion scenario that combines both textual and RDF facts.

S2. This paper proposes a framework called LSTK to address this problem and introduces the LSTK-TELM method.

S3. Three datasets are constructed for evaluation. Experimental results demonstrate the effectiveness of the proposed method.

S4. Well written.

Weaknesses:
W1. There is a lack of discussion on related works in the field of inductive link prediction. See references below:
[1] Inductive Relation Prediction by Subgraph Reasoning
[2] Topology-aware correlations between relations for inductive link prediction in knowledge graphs
[3] Communicative Message Passing for Inductive Relation Reasoning
[4] Subgraph Neighboring Relations Infomax for Inductive Link Prediction on Knowledge Graphs
[5] Disconnected Emerging Knowledge Graph Oriented Inductive Link Prediction

W2. The use of the symbol ""t"" for both tail entities and rule lengths may lead to confusion.

Limitations:
Please refer to the ""Weaknesses"" and ""Questions"" sections.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a two-stage framework that imposes both structural and textual knowledge to conduct knowledge graph completion. The first stage aims to extract some soft triples with confidence scores. And then the second stage designs some tailored rules for entity link prediction, including text-enhanced rules and TE-rules. Experiments on three benchmarks show the proposed method significantly outperforms previous work.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The idea of this paper is clear.
2. Experimental results show significant improvements to baseline models.

Weaknesses:
1. The text-enhanced entity representation models have been widely studied, such as [1][2]. Related work should be discussed. 
2. The two-step pipeline will bring unnecessary cascaded errors. The most directed way is enhancing the entity representations using text representations to directly predict the entity links.
3. The first step with confidence score seems to model uncertainty in linking entities, which may be a novelty. However, such a motivation is not clearly described.
4. The relationship between the designed step should also be emphasized. Such as, the first step brings more entity semantics from text and the second step verifies the entity relations via text and structure aspects.
5. LLMs have shown strong ability in building KGs and exacting entity relations. This paper does not conduct experiments on LLMs.

[1] Representation Learning of Knowledge Graphs with Entity Descriptions. AAAI 2016.
[2] Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval. ACL 2018.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, the authors try to solve Knowledge Graph Completion (KGC) by proposing a two-stage framework Learning from Structural and Textual Knowledge (LSTK), that imposes both structural and textual knowledge to learn rule-based systems. In the first stage, the authors compute a set of triples with confidence scores. To mitigate the influence of the noise in the set of triples estimated in the first stage, the authors also propose a threshold-based method text enhanced rules (TE-rules) to filter out low-confident rules. In the second stage, the authors use the filtered triples by TE-rules to train a neural model named TE-rule Learning Model (TELM) for KGC. Furthermore, the authors created new datasets based on the data, HacRED, DocRED, and BioRel. Experimental results show that the proposed method LSTK outperforms the baseline methods regarding MRR, Hits@k (k=1,3,10).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Since the proposed method LSTK relies on rule-based inference, the estimation of this method is interpretable to humans.
- The proposed approach can restrict the automatically extracted rules to high-confidence ones by TE-rules to deal with the noisy rules extracted by distant supervision.
- The threshold of filtering soft triples does not require sensitive tuning and works with 0.5. Thus, LSTK is robust and easy to use.
- This work provides new KG datasets with their corresponding texts.

Weaknesses:
- The paper does not refer to the recent KGC model that can conduct KGC with unseen entities [1].
- Even though recent KGC models utilize pretrained language models considering textual information of triplets in KGs [2], there is no comparison against such models.

[1] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., & Tang, J. (2021). KEPLER: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9, 176-194. (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00360/98089/KEPLER-A-Unified-Model-for-Knowledge-Embedding-and)

[2] Lv, X., Lin, Y., Cao, Y., Hou, L., Li, J., Liu, Z., ... & Zhou, J. (2022, May). Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach. In Findings of the Association for Computational Linguistics: ACL 2022 (pp. 3570-3581). (https://aclanthology.org/2022.findings-acl.282/)

Limitations:
I think you need to add that this approach is difficult to work on knowledge graphs that do not have corresponding text data to Limitations in your paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a rule-based inductive KGC method with two stages. In the first stage, it extracts soft triples from a text corpus using distant supervision. In the second stage, the obtained soft triples are mixed with the original hard triples and used to learn rule-based model for KGC.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
(1) It is interesting and intuitive to enrich the original triples in KG by extract new triples from text corpus.
(2) The idea that uses extracted soft triples from text corpus to enhance logical rules is novel to some extent.

Weaknesses:
(1) This paper uses both structural and textual knowledge for inductive KGC, but there is no discussion about other related works which also use structural and textual knowledge, such as  KEPLER[1] and BERTRL[2].
(2) Since the proposed method extract new triples from text corpus, if the unseen entities in test set may seen in the new soft triples. It is better to provide statistics about the  unseen entities in original triples, soft triples and both triples.
(3) Some notations are confusing, for example the ""t"" is used for a tail entity in a triple (h,r,t), and also used in ""1≤t≤T"". 

[1] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., & Tang, J. (2021). KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. Transactions of the Association for Computational Linguistics, 9, 176–194. 
[2] [2] Zha, H., Chen, Z., & Yan, X. (2022). Inductive Relation Prediction by BERT. Proceedings of the AAAI Conference on Artificial Intelligence, 36(5), 5923-5931.

Limitations:
(1) This paper uses both structural and textual knowledge for inductive KGC, but there is no discussion about other related works which also use structural and textual knowledge, such as  KEPLER[1] and BERTRL[2].
(2) Since the proposed method extract new triples from text corpus, if the unseen entities in test set may seen in the new soft triples. It is better to provide statistics about the  unseen entities in original triples, soft triples and both triples.
(3) Some notations are confusing, for example the ""t"" is used for a tail entity in a triple (h,r,t), and also used in ""1≤t≤T"". 

[1] Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., & Tang, J. (2021). KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. Transactions of the Association for Computational Linguistics, 9, 176–194. 
[2] [2] Zha, H., Chen, Z., & Yan, X. (2022). Inductive Relation Prediction by BERT. Proceedings of the AAAI Conference on Artificial Intelligence, 36(5), 5923-5931.

Rating:
5

Confidence:
4

";1
NWEbeI2HNQ;"REVIEW 
Summary:
The authors propose a model, SCARF, that predicts Tandem-MS spectra in a two-step process: 1) predicting atomic subsets, 2) predicting peaks given the subsets. The model's performance is evaluated on two datasets (NIST20, Natural Product Library) using on spectral similarity, coverage, validity, and retrieval, and compared with relevant baseline models.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ Clear figures and descriptions of the modeling approach. 
+ Clear explanation of background/previous work. The main difference between the SCARF model and other models which use subset prediction is that the SCARF model uses prefix trees to predict subgraphs, so this model considers all relevant subformulae and does not have a limit on the number of valid substructures considered.
+ Compares against relevant modeling baselines. Evaluates models on coverage and validity to judge the interpretability of the spectra.

Weaknesses:
- It would be helpful to provide some more context for the contents of the two datasets, NIST20 and NPLIB. For example, what classes of molecules are present in each dataset (Classyfire might be a helpful tool for this purpose: http://classyfire.wishartlab.com/). What are the molecular weights of compounds of molecules in this dataset? 

- Retrieval task: If I understand correctly, the retrieval task here is set up such that the 'library' or search space is 50 candidates total, 1 target and 49 other decoys, which are selected from the set of chemical isomers with the highest Tanimoto similarity. Each model is used to predict the spectra for all 50 candidates. 

Why not perform the retrieval task on the entire NIST 20 library? Or if NIST20 is too large, why not use the set of all structures with the same chemical formula or precursor formula, rather than limiting the task to 50 isomers? I believe that by limiting the search library to a  set of 50 molecules that are most chemically similar to the target, the retrieval task might become easier than the real world version. In particularly, typically when one wants to identify a compound, you have few assumptions about what your target molecule is, other than the chemical formula, so you would not be able to filter by chemical similarity to the target. Retrieval tasks in previous works such as Wei et al 2019 were performed on the entire NIST17 dataset.


Limitations:
I do not identify any negative broader societal impacts associated with this work.

I agree with the author's statement that the model's performance will be data dependent. If you plan to release your model (which I would highly encourage), it would be helpful to give an overview of the types of molecules that you expect to be in or out of distribution for the model.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a novel method for predicting a tandem mass spectrum from a given molecular graph.  The key idea is to subdivide the problem into predicting fragmentation of the graph and then separately predicting intensities associated with each fragment.  Relative to other recent methods that have adopted this approach, this method uses a prefix tree to make the identification of subgraphs more efficient.  Experimental results show that the proposed method performs better than state-of-the-art methods.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper addresses an important problem.

The paper is extremely well written, with a nice intro to tandem mass spectrometry targeted to a NeurIPS audience.

The treatment of related work is well organized and clear.

The main idea is well supported by empirical results.


Weaknesses:
On lines 37-42, I wasn't super convinced that ""physically-inspired"" (which, incidentally, doesn't need to be hyphenated) belongs in this list of desiderata.  I think the main thing is that the method is fast and accurate.  It seems more like a hypothesis to say that in order to achieve speed and accuracy, a method should be physically inspired.  Perhaps the notion of ""accurate"" should be segregated into various dimensions: e.g., accurate in the sense that it doesn't just predict binned m/z values, or accurate in the sense that it doesn't predict extra peaks that can't be explained by the formula.  I would argue that if someone came along with a method that was very accurate and fast, there is no scenario in which an end user would prefer a slightly slower or slightly less accurate method just because it comes with a story about how it is physically plausible.

Figure 1A is the key idea, and I think it needs to be expanded.  As drawn, it's not clear how, e.g., you decide that the root should have three children (C7, C12 and C14).  Why not C1, C2, etc.?  The answer presumably has to do with the molecular graph, which is the primary input but is not shown here.  I would also like to hear a bit more about this prefix tree step in the text.  E.g., does it matter what order the atom types are introduced?   If so, how is the order chosen?

Related to the first point above, the claim on lines 100-102 that binning methods are less interpretable was hard for me to understand.  The reduced accuracy due to the lack of shared information across fragments is plausible, but that seems like an empirical claim that requires support.  Maybe just say that you hypothesize that this may lead to reduced accuracy. I also found it strange that you didn't explicitly call out the most obvious source of inaccuracy, which is that the binning means that the peak m/z locations are inherently inaccurate.

For the SCARF-Weave predictor, please indicate how large the sets can be come, both in principle and in practice.  I wonder whether this is a potential problem for the method.

When you present the various competing methods on lines 248-255, I think it would be helpful if you could introduce them in terms of the three groups from Section 2.2.

The closest competing methods are references [24] and [45].  In your experimental comparisons, why did you use a method based on MS-GRAFF ([24]) rather than just using MS-GRAFF itself?  And why didn't you use method [45] at all?

On a related note, if FixedVocab falls into the third category of methods, then why aren't its predictions 100% valid in Table 2?


Limitations:
N/A

Rating:
8

Confidence:
4

REVIEW 
Summary:
The manuscript presents a method called SCARF for the prediction of tandem mass spectra from molecular structures. This allows for building up large in-silico libraries that, together with measured spectra, can be used in spectral library search workflows for metabolite identification. From a general machine learning perspective, the method is an interesting approach for the problem of predicting sets of multisets (in this case: sets of molecular formulas). The author note that autoregression is not well suited for this task, as it is not ordering invariant. Instead, they suggest an approach that is quite similar to the way molecular formulas are decomposed from masses: they step-by-step build a prefix-tree where each node represents a subset of the molecular formula space with certain elements are fixed to specific abundancies. The child nodes are then possible abundancies for the next chemical element, restricting the formula space further until the the leafs of the tree describe only a single molecule formula. The resulting set of molecular formulas is then transformed into a spectrum, by utilizing a second transformer based network for intensity prediction.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- it is the first paper that predicts high resolution spectra without involving combinatorial fragmentation

- use of prefix tree for the prediction of molecular formulas (or sets of multisets) is very innovative

- use of random Fourier features for molecular formula embedding is innovative and seem to work much better than the common approaches of encoding formulas as frequency vectors or one-hot vectors/dictionaries

- evaluation on the spectrum prediction part is done very well. In particular, certain methods are retrained (or even re-implemented) and parameter-optimized to ensure fair comparison

- manuscript is clearly written, the method is described in detail with all hyper parameters given in the supplement

- source code is available 

Weaknesses:
- there is already a successor method (ICEBERG) by the same authors for the same task. This is not a problem in general, as both methods have very different strategies to solve the problem. I still think it would have strengthen the manuscript if pros and cons of both methods and their synergistic potential is discussed 

- the manuscripts list three applications for the predicted spectra: molecule identification, learning about fragmentation, and training machine learning methods on spectra. However, in contrast to ICEBERG the method presented here cannot give any insights about the fragmentation process (as it is not a fragmenter). I also doubt that synthetic data (predicted machine learning) is well suited as training data for other machine learning methods. This is some kind of self-training or self-supervised learning which is a difficult and challenging problem on its own. Thus, the only convincing application for the spectrum prediction task is the molecule identification. Beside spectrum prediction, there are other methods that allow for molecule identification (including methods from the same authors such as MIST). Although the evaluation on metabolite identification contains many methods for spectrum prediction, it does not contain any of these alternative approaches.

- the observation that identification rate is decoupled from database retrieval rate is concerning, as this is the main application for spectrum prediction

- although all methods perform worse on the NPLIB1 data, it is strange that NEIMS(FFN) and SCARF perform so very different on the  retrieval task. It would be interesting to find out why the fingerprint based method performs and the dictionary based method both perform so well here but not in NIST.

- besides its application in metabolomics,  the presented approach might be interesting for other areas with sets of multisets as target variable. Unfortunately, the baseline method (autoregression) is not evaluated against the more sophisticated prefix tree decoding method 

Limitations:
The main limitation of the method is in my opinion its limited performance on the metabolite identification task. This and other limitations are discussed properly in the manuscript.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper presents a new method for predicting mass spectra from molecules called SCARF, which stands for Subformulae Classification for Autoregressively Reconstructing Fragmentations. Mass spectra are sets of peaks that represent the masses and intensities of fragments of molecules after they are ionized and broken down in a mass spectrometer. Predicting mass spectra from molecules is useful for identifying unknown molecules from experimental data, as well as for understanding the fragmentation process and generating virtual spectra libraries.

SCARF predicts mass spectra in two steps: first, it generates the set of chemical formulae for the fragments, which define the locations of the peaks on the mass-to-charge axis; second, it assigns intensities to these formulae, which define the heights of the peaks. The key innovation of SCARF is that it uses prefix trees to efficiently generate the set of formulae, overcoming the combinatorial challenge of enumerating all possible subformulae of a given molecule. SCARF also ensures that all predicted peaks are physically plausible, meaning that they correspond to valid subformulae of the original molecule.

The paper evaluates SCARF on two datasets of molecules and their experimentally measured spectra, NIST20 and NPLIB1. The paper shows that SCARF outperforms existing methods based on fragmentation rules or binned prediction in terms of accuracy, physical sensibility, and speed. The paper also demonstrates that SCARF can improve the retrieval of unknown molecules from new spectra by comparing them to predicted spectra from a database of candidate molecules. The paper concludes by discussing the limitations and future directions of SCARF.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
The paper makes several contributions. 

First, the paper introduces a new method for predicting mass spectra from molecules, which is based on a novel combination of subformulae classification and autoregressive reconstruction. The method overcomes the limitations of existing methods based on fragmentation rules or binned prediction, which are either too restrictive or too coarse-grained. It achieves state-of-the-art performance in terms of accuracy, physical sensibility, and speed, as demonstrated on two datasets of molecules and their experimentally measured spectra. The method is based on a simple yet powerful idea of using prefix trees to generate the set of formulae for the fragments efficiently. It has the potential to revolutionize the field of mass spectrometry by enabling more accurate and efficient identification of unknown molecules from experimental data.

Second, the paper provides a thorough evaluation of the proposed method on two datasets of molecules and their experimentally measured spectra, NIST20 and NPLIB1. It uses a new metric called physical sensibility, which measures how well the predicted spectra match the physical constraints of mass spectrometry. SCARF outperforms existing methods based on fragmentation rules or binned prediction in terms of accuracy, physical sensibility, and speed. The paper provides detailed descriptions of the datasets, metrics, baselines, and results. The evaluation is significant because it demonstrates the effectiveness and robustness of SCARF across different datasets and scenarios.

Third, the paper discusses the limitations and future directions of SCARF. It identifies several open problems and challenges in mass spectrometry that SCARF or its variants can address and provides insightful analyses and suggestions for future research. 

Overall, the paper represents a significant advance in mass spectrometry by introducing a new method for predicting mass spectra from molecules.

Weaknesses:
I list some weaknesses below:

1. The paper assumes that the fragmentation process is purely additive, meaning that each fragment is formed by adding one or more atoms to the previous fragment. This assumption may not hold for some molecules that undergo complex fragmentation pathways, such as rearrangement, elimination, or charge transfer. To address this weakness, future work could explore more general models of fragmentation that can capture these pathways, such as machine learning models or expert systems.

2. It assumes that the mass spectra are measured under ideal conditions, meaning there is no interference from other molecules or ions. This assumption may not hold for some real-world scenarios, such as complex mixtures or dirty samples. To address this weakness, future work could investigate how to incorporate prior knowledge or external data sources to improve the accuracy and robustness of mass spectra prediction.

3. The assumption is that the molecules are represented by their molecular formulae, which are discrete and symbolic. This representation may not capture the continuous and structural features of molecules that affect their fragmentation patterns and mass spectra. To address this weakness, future work could explore more expressive and flexible representations of molecules that can capture these features, such as molecular graphs or descriptors.

4. The fragmentation patterns are assumed independent of each other, meaning that each fragment is formed independently of the others. This assumption may not hold for some molecules that undergo correlated fragmentation pathways, such as cleavage of adjacent bonds or the formation of cyclic structures. To address this weakness, future work could investigate how to model these correlations explicitly or implicitly in the prediction process.

5. The paper assumes that the mass spectra are measured with high resolution and accuracy, meaning each peak is resolved and assigned a precise mass-to-charge ratio. This assumption may not hold for some low-quality or noisy spectra that have overlapping or shifted peaks. To address this weakness, future work could develop methods for denoising or deconvolving mass spectra before or after prediction.

Overall, these weaknesses suggest several directions for future research in mass spectrometry and related fields.

Limitations:
The authors have discussed some of the limitations of their work in Section 5, such as the data dependency, the quality of product formula annotation, and the assumptions and simplifications made by their model. However, they could also mention some of the other limitations I pointed out in my previous comments, such as the interference from other molecules or ions, the continuous and structural features of molecules, the correlated fragmentation pathways, and the low-quality or noisy spectra. They could also provide some empirical evidence or analysis to support their claims about the limitations and future directions of their work.

Rating:
8

Confidence:
4

";1
CniUitfEY3;"REVIEW 
Summary:
This paper presents an architecture (Reusable Slotwise Mechanism or RSM)  for modelling the temporal dynamics of objects based on a slot-represent.
The main idea is to extract a global context (Central Contextual Information or CCI)  from the past frames and use this to stochastically select one of a small discrete set of (learned) mechanisms for each slot and time-step. The mechanism computes the update of a slot,  based on its current representation as well as the CCI. The slots are updated sequentially. It is shown anecdotally that they specialize to particular object behaviors/dynamics (such as idle, free-fall and collision) and are reused across slots and time. 
RSM is compared to several strong baseline methods on several tasks including dynamics modelling, question answering, and task planning. It demonstrates significant improvements over the baselines with a comparable amount of parameters and compute.
 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
* The studied problem of modelling temporal dynamics from raw video is an important one, and there still is a lot of headroom.
* The idea behind RSM is simple and well motivated. 
* RSM clearly improves upon the baselines and produces SOTA results on several relevant benchmarks. It thus presents an important step towards unsupervised learning of dynamics models from video.
* The presentation is clear and the paper + supplementary contain all relevant information for reproducing the experiments.

Weaknesses:
To evaluate the different parts of the RSM architecture, the paper presents 4 ablations in section 4.5. Unfortunately the only quantitative results in Fig 5. the present fell out of the main paper into the appendix.

But if I understand correctly there is a more severe problem: it seems from the text that ablations RSM!2, RSM!3 and RSMk  are **inference time** ablations. That means during training the model had access to CCI in steps 2 and 3 and has learned to choose a mechanism. It is thus not at all surprising that changing any part of that during inference will deteriorate the performance. Thus, all that these ablations demonstrate is that the model has not actively learned to ignore the CCI and the mechanisms. 
It would be much more informative to see how the model performs if these modifications are also applied during training. That could for example help answer the much more interesting question ""How much does having access to CCI during step 2 help the model choose the right mechanism"". For this very reason ablation 4 is much more informative. 
In my opinion the such training-time ablations are necessary to properly support the claims about the role and importance of CCI and the mechanisms. 



Other minor weaknesses:
* the differences between RSM and the baselines could be made clearer. There is some information in Sec 3.2 but it doesn't clarify what exactly is different. 
* The citations especially in the introduction and related work seem a bit biased with many citations to Goyal et al while leaving out other relevant and influential work in the area such as:
  - Eslami, S. M., Heess, N. & Weber, T. Attend, infer, repeat: Fast scene understanding with generative models. Adv. Neural Inf. Process. Syst. (2016)
  - Burgess, C. P. et al. MONet: Unsupervised Scene Decomposition and Representation. arXiv [cs.CV] (2019)
  - Greff, K., Kaufman, R. L. & Kabra, R. Multi-object representation learning with iterative variational inference. International (2019)
  - Battaglia, P. W. et al. Relational inductive biases, deep learning, and graph networks. arXiv [cs.LG] (2018)
  - Greff, K., van Steenkiste, S. & Schmidhuber, J. On the Binding Problem in Artificial Neural Networks. arXiv [cs.NE] (2020)
  - Schölkopf, B. et al. Toward Causal Representation Learning. Proc. IEEE 109, 612–634 (2021)

* Sec 2.2 feels a bit repetetive with the enumeration of the three main components of RSM and the list of four steps in RSM mostly explaining the same things. 


Limitations:
The paper discusses some limitations in the supplementary. I see no necessity for a discussion of negative societal impact. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents a study on future prediction using object-centric representations and disentangled and reusable mechanisms (RSM) that govern the interactions between objects. The authors' key contribution lies in the proposal of Central Contextual Information (CCI) representation, which captures the interactions between objects observed in the past. The CCI is encoded using multi-head attention layers with all object-centric representations from previous observations and a bottleneck projection layer. It aids the model in selecting a single mechanism to predict object transitions from the current state to the next state. Experimental results demonstrate that the introduced RSM outperforms several competing baselines in tasks such as future frame prediction, visual question-answering, and action planning in relatively simple environments.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
+) The motivation behind the proposed CCI is clearly explained, with the authors using multi-head attention layers to capture interactions between objects in past observations. The subsequent bottlenecking of the attention layers’ output into the CCI enables the model to learn a concise representation that encapsulates meaningful information about object interactions.

+) The selection process for predicting state transitions is intriguing and surprisingly effective. The use of Gumbel-softmax (assuming it is Gumbel-softmax instead of Gumbel-max as mentioned in the manuscript) allows the model to decompose the potential mechanisms governing state changes.

+) The visualizations in Figures 4 and 5 provide compelling evidence of the model's ability to decompose mechanisms into discrete and reusable components. Figure 4 demonstrates the model's capability in decomposing mechanisms, while Figure 5 showcases how the CCI encodes essential information for determining outcomes following object interactions. 

+) The experimental results support the efficacy of the proposed RSM, as it outperforms several competing baselines, including SlotFormer, SwitchFormer, and NPS, across various tasks such as future frame prediction, visual question-answering (which relies on future visual states), and action planning.

Weaknesses:
-) The assumption that there is only one mechanism responsible for the state change of an object may not always hold true. Factors such as the object's original momentum and collisions with other objects can influence its behavior. Therefore, the use of Gumbel-softmax (again, assuming it is Gumbel-softmax instead of Gumbel-max as mentioned in the manuscript) in Equation (2) might impose overly strong restrictions on modeling interaction mechanisms.

-) There appears to be a misinterpretation regarding the mention of the Gumbel-max layer in Step 2. The Gumbel-max layer, without the Straight-Through (ST) Gumbel Trick, is not differentiable and is primarily used to enable the categorical sampling using unnormalized log probabilities by the reparameterization trick. The references cited at Line 122 also pertain to Gumbel-Softmax, not Gumbel-max. For more details, please refer to the PyTorch implementations of the Gumbel-Softmax layer: https://pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html#torch.nn.functional.gumbel_softmax or the TensorFlow API: https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/RelaxedOneHotCategorical.

-) There is a discrepancy at Line 269 where the authors mention the visualization of disentangled mechanisms in Figure 5. However, Figure 5 is not found in the main submitted manuscript but in the supplementary material (Page 13). The mention of Figure 5 at Line 269 implies it should be present in the main paper, potentially violating the Formatting instructions.

-) While the proposed RSM outperforms competing baselines on various datasets and tasks, it is worth noting that the experiments are conducted in relatively unrealistic environments with toy objects and simple interactions. It would be beneficial to demonstrate the effectiveness of the proposed approach (object-centric representations + RSM) on more realistic datasets such as MOVi-B, C, D, E, and F.

Limitations:
-) As mentioned in the Weaknesses, the experiments conducted in this study are limited to relatively unrealistic environments with toy objects and simple interactions. As a result, it remains uncertain whether the proposed approach would maintain its effectiveness in a more realistic environment or when applied to real-world data.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposed Reusable slot-wise mechanisms wherein the authors introduce ""Central Contextual Information"" -- a bottleneck that captures the global context and helps in choosing which slot to attend to.
Through experiments on several simple and relatively complex datasets are shown with reconstruction, VQA and planning tasks to show the benefit of the reusable mechanisms.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. I find the introduction a bottleneck CCI to summarize the context of past $T$ frames to be intuitive and novel.

2. The method shows significant improvement in OOD action planning tasks which is impressive!


Weaknesses:
3. **CCI**: Have authors tried treating each slot independently as input to the transformer with positional encoding instead of concatenating past $T$ time slots? 

4. In Fig 1(c), is should the $t$ in blue circle (drawn adjacent to the CCI bubble) be $s_t$. If so please clarify in the figure as $t + \delta s_t $ in Step 3 is confusing as one represents time and another represents the predicted change in slot representation.

5. **Memory Constraints**: The CCI (Step 1) has a huge memory constraint of $(\tau+1) \times N \times d_s$ which is $7 \times 6 \times 128 = 5376$ for OBJ3D dataset (which has the smallest memory requirements), and $16\times 6 \times 192 = 18432$ which is huge. This appears to be a huge factor given that object-centric models are already memory expensive when it comes to applications such as RL.

6. **OOD in dynamics**: How does RSM behave when it sees an OOD dynamics for a particular object. For example, in the OBJ3D dataset (or any other simplistic dataset with enough variations in objects and their dynamics) if a green object is static throughout during the training, but during testing scenario if the green object moves would RSM reuse the mechanism corresponding to the motion of object? If time permits, I would like to see a simple experiment on thise on any of the datasets (which ever is faster to train/test). Please reach out to me if you need more clarification on this.

7. **Inference Time**: Can the authors provide comparison between all the baselines and RSM with regards to inference time on the different datasets evaluated?


**[Minor comments which I have not considered for rating of the paper]** -- The authors need not reply to these 2 comments below.

8. I think reconstruction results shown using SSIM or MSE on most simplistic datasets such as OBJ3D or CLEVERER where the values are in the 90s aren't really indicative of how useful the representations are for downstream tasks. It appears that VQA task in CLEVERER is also pretty simplistic and has reached mid-90s -- it would be better the community in general moves to addressing complex tasks of planning, RL and hard-VQA (such as Physion VQA task as shown in the paper).

9. **Readability of methodology section**: In section 2.1 (RSM Overview), it would be helpful to re-iterate what the purpose of having CCI is. The only mention of CCI is in the last section of introduction, so explicitly having it as a part of the RSM overview would be helpful.


----
**Rationale for rating**
I find the contributions of this work to be novel and experimentation to be through and hence I lean towards accepting the paper. However, I would like the authors to address my questions before I finalize my decision. In addition to this, if authors have clarifying questions regarding comment (6), I would request them to reach out to me early on during the rebuttal phase.

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work presents a novel framework to model object dynamics by leveraging communication slots in a modular architecture. The method uses Central Contextual Information (CCI) to allow information exchange among existing slots. The authors demonstrate the efficacy of their method's superior empirical performance on VQA, action planning, and future frame prediction. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using central contextual information is novel and interesting and seems to provide clear empirical benefits over prior work. 
2. The authors have clearly outlined the primary contributions and have carefully designed ablations to support their claim. 

Weaknesses:
One question I have for the authors is how this method would be extended to a real scene where the entities that are interacting cannot be clearly separated. For instance, in CLEVRER or Obj3D, it's known ahead of time that the 3-5 objects would be moving around and interacting with each other. But in a real scene, finding that might get very challenging. I would be interesting in hearing author's perspective about that. 

Limitations:
See weaknesses. 

Rating:
6

Confidence:
2

";1
QatZNssk7T;"REVIEW 
Summary:
The paper considers balanced adversaries that do not have access to the underlying distribution and instead interact with an oracle adversary that chooses the distribution and samples points from it. The authors show that with this more realistic adversarial setting, they can improve the previous known lower bounds for adaptive data analysis.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The authors consider a more realistic balanced adversary that does not have access to the underlying distribution. This adversarial setting is novel and allows for better bounds on the adaptive data analysis.

Weaknesses:
While considering this balanced adversary is more practical, I would suggest the authors to include a summary table comparing their setting, assumptions and their bounds with respect to the prior works. This would better highlight the differences and focus more on the key contribution of the paper.



Limitations:
I don't feel there is any direct or evident negative societal impact of this work.

Rating:
6

Confidence:
2

REVIEW 
Summary:
Adaptive data analysis considers the setting where a mechanism and an analyst play a game against each other, where for $k$ sequential rounds, the analyst issues queries to a mechanism which holds a dataset of size $n$ sampled from a distribution, the mechanism returns a response, and the analyst chooses the next query accordingly. The mechanism wins the game if all responses are close to the expected value of the queries on the dataset.

Using tools developed in the context of differential privacy, it was proven that a mechanism can respond to up to $k = O(n^{2})$ queries, while winning the game with high probability. At the same time, it was proven that if the number of queries exceeds $O(n^{2})$, there exists an analyst that can win the game. This lower bound assumes the analyst can choose the distribution, and is based on the cryptographic assumption of the existence of fingerprinting codes. This imposes an asymmetry between the power and knowledge of the analyst and mechanism which is unnatural, since an analyst that knows the distribution has no reason to query the mechanism.

The current work solves this problem, by proving a similar lower bound for a balanced setting, where the distribution is chosen by a third player. This proof is based on the stronger cryptographic assumption of the existence of public-key cryptography. The authors also prove that the lower bound does not hold, without this assumption, proving a separation between the balanced and unbalanced settings.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The deep asymmetry that was used in the proof the previous lower bounds was undesired, and left an open question whether it is essential, since it is not justified in most natural settings. Solving this open question is an important step in understanding the dynamics of adaptive data analysis. They are based on a novel addition to the existing lower bound framework, and are presented in a clear and formal way.

The distinction between the two cryptographic assumptions required in both cases, might hold implications for cryptography as well.

Weaknesses:
I have only one minor comment to add. It seems like the authors use another implicit assumption throughout the paper, that the mechanism knows the analyst it is playing against. If this was not the case, the separation of the analyst into two analysts (one choosing the distribution and the other issuing the queries) will be meaningless, as we could always consider the case where the first analyst always chooses the same distribution, and so the second one can use that information while the mechanism cannot. 

I do not bring this up as a limitation of the work, but as a recommendation to clarify the setting, so to avoid confusion by the reader.

=======

**Edit after rebuttal discussion:**

I have no further concerns.

Limitations:
N/A

Rating:
9

Confidence:
5

REVIEW 
Summary:
The paper proposes to consider balanced adversaries for ADA. Under the public-key assumptions, the paper proves that 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper assumes a more practical threat model for ADA.

2. The theoretical results in this paper seem to be well-supported. I am not an expert in this research direction, so I am not sure about my judgement.



Weaknesses:
1. There is no discussion about the real-world machine learning applications of the results.

2. There is no conclusion section to summarize the paper. The paper ends after Section 4 and makes me feel that the paper is not complete.

3. The theoretical results are based on the assumption that public-key cryptography exists.

4. I am not an expert in this research direction, and I feel that this paper is not easy to follow because there are many terms. I suggest that the paper could add a table to explain the terms used in the paper to improve the readability.


Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper studies limitations of ""adaptive data analysis"" with a focus on ""computational assumptions"" that are used for proving such limitations (i.e., lower bounds).

A mechanism $M$ answers $m=m(n)$ queries (almost) correctly, if it succeeds in the following game for every distribution $D$ and PPT adversary $A$. The mechanism is given n iid samples from D. Then the adversary $A$ asks $m$ adaptive statistical queries $q_1,\dots,q_m$ from the mechanism $M$. The mechanism succeeds if all of its answers are within $\pm 1/10$ of the true answers with respect to distribution $D$.

Previous work has shown that at most $m=\Theta(n^2)$ queries can be answered adaptively by computationally bounded mechanisms, assuming one-way functions exist. Their proof uses private-key encryption schemes (whose existence is equivalent to one-way functions) and a reduction from a class of mechanisms, called ""natural"". The previous lower bound also implicitly assumes a rather powerful adversary who both chooses the distribution D as well as the queries $q_1,...q_m$.

This paper starts by making the (interesting) observation that if an adversary already knows the distribution $D$, then it won't have a meaningful interest in making the queries. So, in some sense, the previous lower bounds are not as meaningful as one wishes in real world scenarios; this raises the hope for getting positive results that go around the lower bounds by leveraging this observation.

In summary, the main result of the paper is bootstrap the previous lower bound to a model to a weaker adversarial model.
In more detail, the contributions of this paper are as follow:

1. A new formal model of security called the ""balanced model"" is defined, in which the adversary is split into two entities that do not communicate. The first adversary $A_1$ picks the distribution $D$ and the iid samples. The second adversary $A_2$ picks the adaptive queries.
2. It is shows that assuming the stronger primitive of secure public-key encryption (as opposed to just OWFs) one can still extend the previous lower to the balanced model.
3. The assumption of public-key encryption is somehow justified, by showing any lower bound with minimal properties (that hold for current lower bound proofs) would lead to key agreement protocols (which is a close primitive/assumption to public key encryption).

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The main strengths of the paper:

* excellent writing quality. my only criticism is that the definition used in *previous* work is not presented formally, and one cannot determine the exact details from the informal text. this is needed for making a fully detailed comparison between the new definition and the old one, because this is a major part of this paper.

* revisiting a previous lower bound for an important problem from a natural practical perspective.

Weaknesses:
My criticisms of the paper are the following:

1. About the model (Item 1 above): I am not sure if the new split (balanced) model is adding much value actually. suppose we fix a mechanism $M$ and look for a balanced 2-part polynomial-size adversary $(A_1,A_2)$ against $M$. In this setting, it seems meaningless to say that only $A_1$ knows the distribution D, because one can fix the distribution $D$ that $A_1$ picks to its ""best possible"" distribution (against $M$). This way, $A_2$ also would be aware of this distribution implicitly, as it can depend on it. In other words, one can see this argument as giving shared coin to $A_1,A_2$, and then fixing it to its best value (which leads to no communication between $A_1$ and $A_2$). 

I shall add that, from a game theoretic perspective, it would seems a preferred and stronger lower bound if we could use *one* fixed adversary strategy that bounds the power of every computationally limited mechanism. However, for the sake of proving lower bounds, it is fine if we'd pick the adversary $A_M$ based on the mechanism $M$. 

2. About the technical depth of the lower bound (Item 2 above). This proof seems like a rather straightforward adaptation of the previous proof. but using IBE (or a bunch of public key encryption schemes) instead of Secret Key Encryption schemes. This is a mild criticism, as I am in general in favor of simpler proofs. But this proof seems like a simple proof for a new question rather than a simple proof for a question that existed before, so I am a bit torn here.

3. About necessity of Key Agreement (Item 3 above): I am not a fan of investing in weakening assumptions that are used for *lower bounds*. as I believe there is a fundamental difference between weakening assumptions behind positive results vs those of negative results. When we weaken the assumption behind, say, public key encryption from an assumption $P_1$ to a weaker assumption $P_2$, it means that the schemes that would be deployed *right now* would not break at an unknown time (with potential catastrophic consequences) even if $P_1$ turns to be false. However, when we weaken the (still widely believed) computational assumption behind a a *lower bound*, is is a completely different story, because even using an assumption as strong as IO to prove an impossibility has the same practical consequence as using OWFs. The two scenarios would diverge (and a weaker assumption would start to look interesting) after breaking every IO scheme, which is most likely never gonna happen, and the weakening assumption changes nothing right now. So, in summary, when it comes to lower bounds I am totally fine with using something as strong as any assumption that seems strong enough for the foreseeable future.

update: the issues above were discussed and I am happy with author(s)' response.

Limitations:
The limitations are also elaborated in the ""weakness"" section of the review above.

Rating:
6

Confidence:
4

";1
oGxE2Nvlda;"REVIEW 
Summary:
The paper presents UniT, a unified certified robust training against text adversarial perturbation. The paper identifies two frameworks of robust training: type (I) does random data augmentation over the text inputs, and type (II) adds Gaussian noise to the latent feature space. The paper unifies two types of training by doing data augmentation in the inputs and adding Gaussian noise to the latent feature space. The paper also introduces a novel loss function, which modularizes the feature extraction regularization and robust regularization as a loss term to fuse these two types of training besides the original cross-entropy loss.

The paper evaluates UniT and demonstrates that it consistently outperforms SAFER and CISS. The paper also does ablation studies of different loss function designs and different hyper-parameter settings (in the appendix).

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
1. The modular loss function design is interesting and novel. The ablation studies of the loss function are very strong.
2. The paper outperforms existing certified approaches SAFER and CISS.

Weaknesses:
1. The paper only handles the perturbation space of synonym substitutions but not other text adversarial perturbations such as insertion, deletion, and their combinations. This weakness makes the title ""text adversarial perturbation"" overclaimed.
2. The comparison between UniT and CISS needs to be presented more clearly. See questions 2-4.

Limitations:
The authors do not address the limitation. One limitation of the paper is that the paper only handles word substitutions but not general perturbation spaces such as insertion, deletion, and their combinations. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a unified framework called UniT for certified robust training against text adversarial perturbation. UniT can train and certify in both discrete word space and continuous latent space by working in the word embedding space, without extra modules or loose bounds. It also introduces a decoupled regularization loss called DR loss to improve the base model robustness by regularizing the feature extraction and classifier modules separately. The paper shows that UniT outperforms existing state-of-the-art methods in both scenarios, and DR loss enhances the certified robust accuracy by providing modular regularization.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper clearly identifies the limitations of existing approaches and highlights the advantages of combining them with a novel DR loss into a unified framework. The authors have made an effort to address these limitations and have proposed a novel solution that capitalizes on the strengths of both discrete word space and continuous latent space approaches. Its evaluation demonstrates the effectiveness of their proposed approach. 

Weaknesses:
1. The selection of synonyms (Type I) is indeed an important aspect of the paper, and it would be helpful if the authors could provide more clarity on how synonyms are chosen based on embeddings. I'm not sure about the probability of changing labels due to synonym substitution for the task of sentiment analysis. Could the authors provide some concrete examples? Additionally, a discussion on how the base model is obtained (whether it's fine-tuned BERT or not) and any improvements in generalization performance could shed more light on the robustness of the proposed approach.

2. The paper's focus on BERT and sentiment classification may indeed limit its applicability to other tasks or models, especially in the era of large-scale pre-trained models with improved generalization capabilities. The authors could address this concern by discussing the potential of their framework to be extended to other tasks and models, and whether the problem they are investigating remains relevant in the current research context.

3. The paper mainly discusses synonym substitution and noise addition, but there are now more advanced perturbation methods based on large language models (LLMs) that could potentially generate more realistic adversarial examples (e.g., through rephrasing or prompting). A comparison or discussion of these alternative methods and their implications for the proposed approach could provide a more comprehensive understanding this paper's contribution.

Limitations:
See the weakness section

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper focuses on the certified robustness of language models. To improve the certified robustness, the authors propose a better robust training method that enables robust feature extraction and a larger prediction margin. Experiment results show the effectiveness of the proposed DR loss, leading to a better certified accuracy compared to traditionally used CE loss.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-motivated. The authors first identify the existing challenge of certification when using CE loss. Based on that, the DR loss is proposed to alleviate the non-robust feature extraction and improve the prediction margin. Empirical evaluations also demonstrate the effectiveness of the proposed DR loss.

Weaknesses:
- An important baseline is missing: robust training. The overall objective of the proposed DR loss is to improve the robustness of the base classifier so that it is easier to be certified. Robust training such as min-max adversarial training [1], and TRADES [2] is also known to be beneficial for the certification. Since the proposed method manipulates the training objective, which is very similar to min-max adversarial training and TRADES, it would be great to compare the certification performance of the model after training with [1,2] and the proposed loss. For example, one may use greedy search-based attack methods from  [3] to find the perturbation for each batch during training. Then minimize the CE loss on the perturbed batch (when using TRADES it will be slightly different). Please note that the synonym set for each word should be consistent with SAFER's (if compare in the Type-I scenario) rather than using the original candidate set of the attack methods.
- An empirical robustness evaluation would make the evaluation more comprehensive. It would greatly demonstrate the effectiveness of the proposed method if improvements in the empirical robustness can be observed and outperform robust training methods.

Overall, this paper is well-motivated and clearly written. The proposed technique makes sense and is verified to be effective in some settings. If my concerns could be addressed, I would like to raise my rating.



[1] Madry, Aleksander, et al. ""Towards deep learning models resistant to adversarial attacks."" arXiv preprint arXiv:1706.06083 (2017).

[2] Zhang, Hongyang, et al. ""Theoretically principled trade-off between robustness and accuracy."" *International conference on machine learning*. PMLR, 2019.

[3] Morris, John X., et al. ""Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP."" *arXiv preprint arXiv:2005.05909* (2020).

Limitations:
The authors discussed the limitations and potential negative social impact of their work 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This research paper discusses the vulnerability of Deep Neural Networks (DNNs) used in Natural Language Processing (NLP) tasks against adversarial attacks, specifically word-level adversarial perturbation (or synonym substitution). The authors delve into two existing training frameworks (Type I and Type II) for these NLP models, highlighting the shortcomings related to unified training frameworks and the robustness of the base model. To overcome these limitations, the authors suggest a novel framework, UniT, which merges the two types of models to provide stronger certified robustness. They also propose a Decoupled Regularization (DR) loss to optimize robustness regularization for individual modules. Experimental results provided deliver evidence that the UniT with DR loss improves the certified robust accuracy of both types of certification scenarios.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. **Novelty**: The paper presents an original perspective on certified robust training for adversarial attacks in text data bringing unique insights and implementations, which addresses the identified gaps in the field.
2. **Improved Certification Accuracy**: The proposal of a unified framework (UniT) and a novel decoupled regularization (DR) loss show promising results, achieving higher certified robust accuracy. This moves us towards creating models with stronger robustness.
3. **Bypasses IBP Issues**: The UniT framework allows Type II methods to bypass using Interval Bound Propagation (IBP) during training, which has been shown to have problems in certification due to its loose bound problem. This successfully solves a major complexity in the training process.

Weaknesses:
1. **Increased Complexity**: While the UniT framework and DR loss may improve robustness, they potentially increase the complexity of model training because it requires handling the embedding space as an intermediate for unification and decoupling the CE loss, which could be time and resource consuming.
2. **Limited Validation**: Although the paper claims improved results, tests and validations seem limited. It would be beneficial to test the proposed methods on different tasks or datasets to better assess their efficacy and robustness.

Limitations:
yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a unified framework called UniT, aiming to solve the limitations of existing certified robust training pipelines against text adversarial perturbations. The main contribution is that it works in the word embedding space and provides stronger robustness guarantees without extra modules. Additionally, the paper proposes a decoupled regularization (DR) loss to improve the base model's robustness. Experimental results show the effectiveness of the unified framework and DR loss in enhancing certified robust accuracy.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This work presents three advantages:

1. It successfully combines the smoothed model in the discrete word space and the latent space, effectively bridging the structural gap between the two spaces and providing a unified approach for certified robustness. It avoids the loose bound problem caused by IBP.

2. The introduction of a decoupled regularization (DR) loss specifically targets improving the robustness of the base model, separating the robustness regularization terms for feature extraction and classifier modules, leading to better performance.

3. Experiments are conducted on used text classification datasets, demonstrating the effectiveness of the proposed unified framework and the DR loss in improving certified robust accuracy.

Weaknesses:
See the question part.

Limitations:
See above

Rating:
5

Confidence:
4

";1
SycQxJaGIR;"REVIEW 
Summary:
This paper addresses the multi-agent pathfinding problem by proposing an approach that utilizes a combination of a planning algorithm for constructing a long-term plan and reinforcement learning for reaching short-term sub-goals and resolving local conflicts. The results show that the proposed method outperforms decentralized learnable competitors and centralized planner. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The method is straightforward and concise.
2. The writing is clear and easy to understand.

Weaknesses:
1. The proposed method follows a hierarchical reinforcement learning framework, which has been extensively studied in previous works. There are limited contributions to the design of sub-goal selection. 
2. In the heuristic sub-goal decider, A* is used to construct a path, which requires global information. As the sub-goal decider will be used multiple times during the episode, the overall method seems not fully decentralized. 


Limitations:
This work has little negative societal impact. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a decentralized hierarchical approach without agent-to-agent communication for Lifelong Multi-agent Pathfinding (MAPF). The framework adds a congestion-based heuristic to an A*-planner and a low level Reinforcement Learning (RL) - based controller to follow the provided sub-goals. Experimental results show that the proposed method has a higher throughput (or rate of reach of new goals) for a range of maps.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- Easy to understand. Uses the well-studied A* planner with additional heuristics and a low level RL-based controller simply trained to reach goals promoting long term performance.
- Hierarchical framework is simple and could be effective way for decentralized control in an MAPF problem.

Weaknesses:
- The change of the heuristic in the A* planner seems weakly substantiated. While empirical results are promising, the need for hyperparameter tuning for the score and lack of guarantees on behavior may impede the use of this new heuristic.
- Confidence intervals for higher density experiments may be too large to claim better performance. (E.g. Table 1, 16 agents, proposed approach has throughput $ 0.56 \pm 0.34$ vs Primal2 having $0.31 \pm 0.14$ ). This may point to noisier behavior in the presence of more obstacles.

Limitations:
- Access to global map is assumed for use of the A* algorithm.
- Several empirically determined reward function portions may hinder generalizability to different maps.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers a decentralized multi-agent pathfinding (MAPF) problem. The main idea is to combine heuristic-based search and reinforcement learning. This work first determines subgoals and uses this information as intrinsic rewards. Empirically, it outperforms two baselines in the literature,  PRIMAL2 and PICO, in domains with different sizes and different numbers of agents. The method also demonstrates the ability to generalize to domains unseen during training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main observation is that pure heuristic search would not have a good performance in complex domains, where collaborative behaviors like congestion avoidance may not emerge. 
This work shows an inspiring combination of heuristic-based search and reinforcement learning.

The proposed algorithm also outperforms a centralized control algorithm (RHCR) when the number of agents is large or when the computational budget is small (so the centralized algorithm is expensive).


Weaknesses:
Some concerns about the practicality of the algorithm: 1) It requires some hyperparameter selection. 2) It requires pre-train a neural model. When it outperforms RHCR in some settings when RHCR is run for 10s, we need to consider the computational overhead of running the RL algorithm during training.
In terms of performance, the performance between FOLLOWER and PRIMAL2 is very close in some domains.

**Minor points.** Line 203, duplicate “node.”


Limitations:
The authors have mentioned the limitations of this work to be the assumptions of static environment, perfect perception.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel method for decentralized lifelong MAPF. The method consists of two components: a heuristic sub-goal decider, which assigns sub-goals for each agent using a heuristic (e.g., A*), and a learning-based policy network, which outputs actions for achieving the short-term subgoals. The paper compares the proposed method with both learning-based decentralized methods and the search-based centralized method on extensive setups and demonstrates the proposed method's superiority. The paper also provides insightful ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using heuristics to solve long-term planning and utilizing learning-based policies for achieving short-term sub-goal is reasonable and also commonly used in many other tasks.

2. The paper compares the proposed method with both learning-based decentralized methods and a search-based centralized method on extensive setups and demonstrates the proposed method's superiority and generalization capability. 

3. The paper also provides insightful ablation studies and verifies the necessity of each proposed component. 

4. The paper is well-written and easy to follow.

Weaknesses:
1. Since I'm not active in the MAPF field right now, I am unsure if there is literature sharing similar ideas in the MAPF tasks. But at least I know that the idea of using heuristics for long-term sub-goal assignment and learning-based policy for low-level sub-goal achievement is quite common in the RL field.

2. How the RL policy handles the collision and deadlock is unclear to me. What will happen if the agent (or two agents) choose the action(s) that will cause a collision? What will happen if there is a deadlock (e.g., two agents want to pass a narrow corridor)?

3. I am a little confused about what the RL policy can learn if the K is set to 2, which means the sub-goal is just two steps away from the current location. Are there many candidate paths to a sub-goal, which is just two steps away?

4. Lines 211-218: Since the agent doesn't know the future locations of other agents, how does the method count the ""number of times the other agents were seen"" in a future step? Does the method use a static heat map (for only the current step) to count that?

5. Lines 242 and 247: the symbol H was used twice with different meanings.

6. Figure 1 is not mentioned in the text.

7. Lines 175-176: ""as the ratio of the episode length to the number of goals achieved"" -> ""as the number of goals achieved to the ratio of the episode length""?

8. Line 203: ""node node""



Limitations:
Yes

Rating:
5

Confidence:
3

";0
I18BXotQ7j;"REVIEW 
Summary:
This paper tackles the image geolocalization problem. Instead of formulating the task as image retrieval or classification over a set of geospatial bins as is typical, the authors frame it as image-to-GPS retrieval by matching image features with location features. The proposed training strategy leverages Random Fourier Features (RFF) to encode geospatial location across multiple scales and align them with features from a CLIP visual encoder, with several training enhancements. The resulting method achieves state of the art performance on three benchmarks, across all scales, and showcases interesting applications in text geolocalization and classification.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Addresses an important and interesting problem. Image localization is a fundamental problem in vision that continues to receive attention in the literature. 

- The approach has technical novelty and is of the first to formulate the localization task as an image-to-GPS retrieval problem. The key idea is to relate images directly to locations via aligning image features to location embeddings using contrastive learning. Such an approach uses a continuous model of location whereas traditional geolocalization approaches depend on a large reference database of images with known locations to support image-level matching (via image retrieval or classification) and inferring location from the closest match. Instead, the authors propose the concept of a GPS gallery.  Though training still requires geotagged images (image + location), the authors show the proposed approach requires less training data.  Further, at inference, the GPS gallery can theoretically be any set of locations to query against, instead of being limited by the reference image set. Geospatial location is represented using NeRF-style location embeddings (Random Fourier Features with an MLP).  Results show that infusing the location representations with multiple spatial scales improves geolocalization performance. 

- Evaluation is extensive and compelling. State-of-the-art (SOTA) performance on multiple datasets at different threshold scales (street, city, region, country, and continent). Near SOTA performance on Im2GPS3k training using only 20% of the training data.

- Intriguing applications. The results on text geolocalization and geotagged image classification showcase the utility of the location encoding beyond image-based geolocalization. 

Weaknesses:
- A central idea of this paper is the concept of modeling location continuously. I was disappointed to see that during evaluation, the GPS gallery (the set of possible locations) was limited to locations sampled from the training set (L230). Though I understand why this was done from an evaluation perspective, it significantly hurts the ""global localization"" theme. At the very least, I would have liked to see an evaluation of how selecting the GPS gallery at inference (choice of sampling distribution, granularity, etc.) impacts performance. This also raises the issue that the location encoder is not truly global, it is limited by where images have been captured on the globe (as training requires image/location pairs). 

- Some minor details missing. Section 4.2 does not mention which dataset the experiment was performed on. Comparing the numbers from Figure 3 and Table 1a, it can be assumed to have been conducted on Im2GPS3k, but the dataset should still be mentioned to improve clarity. Similarly, section 4.3 does not mention which dataset(s) was/were used for the ablation study.

- The application for text query-based geolocalization is interesting, but the results are lacking. It is only demonstrated qualitatively on a continent scale using a single keyword (desert) in the main paper. While this is interesting, it leaves a lot to be desired in terms of understanding the full or even partial capability of the text encoding for tasks such as geolocalization or digital forensics. 

- Quality of the presentation could be improved. For example, the introduction could provide additional motivation for the underlying problem. In its current form, the introduction mentions that there are a variety of applications for geolocalization, but does not support this claim with any citations or descriptions of how geolocalization benefits them. Instead, the majority of content is related to why the problem is difficult and how existing approaches are inadequate, repeating much of the related work section content (e.g., binning, classification, and scaling methods). The conclusion section, in particular, lacks flow and could benefit from additional editing.  Unclear mathematical notation on L138. What is K?  Minor grammatical errors (L49, L107, L111, L372, etc.).

- Word choice in the conclusion. L353 suggests that this work “solved worldwide geolocalization”. Of course this problem is far from solved! 

Limitations:
Limitations addressed in the manuscript.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper addresses the problem of geo-locating an image, i.e. estimating the latitude and longitude of the where the image was taken. 

Previous works treat the problem as a classification problem (of cells distributed across the globe). In contrast the this work proposes to retrieve the GPS coordinates from a collection of possible locations. In order to do so the authors propose a neural network architecture that encodes the (ground truth) GPS location and the image in a joint feature space and train it via a contrastive learning scheme. During the inference the query image is encoded and the GPS coordinate closest in feature space is taken as the predicted location.

The main contribution of the paper is the positional encoding of GPS coordinates via random fourier features [21], which are demonstrated to help represent higher frequency information and hence improve localization at higher spatial resolution (Tab. 2).

Experimental results demonstrate state-of-the-art retrieval performance on 3 different datasets.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
**S1** The encoding of GPS coordinates via random fourier features (RFF) is novel. While RFF have been demonstrated to be useful to capture higher frequency properties for 2d image or 3d coordinates [21], e.g. in NeRF, to the best of my knowledge they have not been used to encode lat/lng coordinates for geo-localization. The authors also demonstrate that an equal earth projection (EEP) (which preserves the area covered) is beneficial for encoding 2d coordinates on the globe, especially for higher accuracy localization (< 1km in Tab. 2).

**S2** Good ablation studies that demonstrate the usefulness of the chosen solutions, especially Tab. 2 for EEP and RFF, but also the ablations provided in the sup. material in Sec. 3. This helps the reproducibility of the approach.

**S3** The place recognition performance improves considerably over state-of-the-art, even though the taken approach is relatively simple. This can be attributed to the ability of the model to learn an understand of spatial information.

Weaknesses:
**W1** The usage of the CLIP frozen backbone is not well motivated and evaluated. As such it remains unclear why CLIP features are needed for the place recognition task, rather than a network trained from scratch or fine-tuned on the task. Experimental evaluation demonstrates how the learned feature is able to relate textual information to geographic locations, but the influence of this capability on predicting lat/lng locations for a query image is not evaluated. 

**W2** The evaluation of the text-to-location encoding in Sec. 4.5 is very limited. The encoded location is used for image classification, but the paper falls too short in outlining the goal and metric of the chosen classification task. As such the evaluation can not stand by it's own but the reader needs to resort to [4, 31] for context.

**W3** The size of the gallery (the quantization of possible GPS locations) is tuned per dataset. Instead a single size across all dataset would foster broader applicability of the approach. Sec. 3.1 of the sup. material suggests that this would in fact be possible, e.g. with a size of ~250k. (See also Q2)

Limitations:
The ability to accurately locate any photo on the earth represents a security and privacy issue. E.g., even if a user ops out of GPS tagging their photos, the presented approach enables to infer the user location from their images -- and the location trace of the user over time.
This surveillance capability should be stated as negative societal impact or potential harmful consequence of the approach, when deployed in public. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes an image-to-GPS retrieval method, with an image encoder from the pre-trained CLIP and a tailored GPS encoder.

The GPS encoder is implemented by using:  i) the Equal Earth Projection to mitigate the GPS distortion; ii) Random Fourier transform with multiple MLPs to obtain high-frequency `positional encoding'.

Experiments on standard benchmarks demonstrate the effectiveness of the proposed method.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1) This paper is well-written and easy to follow;

2) The idea of this paper is interesting and new;

3) The experimental evaluations are strong, particularly the interesting Section 4.4.

Weaknesses:
I only have some minor comments:

1) Though it is interesting to adopt the CLIP pre-trained encoders, I would like to the experiments substituting the CLIP image encoder with Swin encoder (as GeoDecoder [5]), or substituting the Swin encoder in [5] with the CLIP image encoder.

This would help readers to clearly understand the contribution of the CLIP image encoder.

2) I would like to see an experiment under the following setting:

a) Evenly partition the earth into patches, and use the patch center GPS as the database;

b) perform the proposed image-to-GPS retrieval and report the performance;

c) Repeat a) and b) with coarse-to-fine patch partitions.

3) Please explicitly list the image-encoding time and the image-to-GPS retrieval time;

4) Please tone down the claim of ""However, this approach is not practical as it becomes infeasible to construct a gallery containing images covering the entire world."" The reason is that we have a world satellite image gallery.

Limitations:
Yes.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This work learns an image-to-GPS retrieval approach where the image is a query, and the gallery
is a collection of GPS coordinates. The authors propose GeoCLIP, which consists of a location encoder, and an image encoder. The retrieval then is based given a new query image against a collection of gps embeddings. Experimental results show that this work outperforms previous SOTA at the Im2GPS3k and GWS15k datasets, across all distance thresholds. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* This is a well written paper, with good motivation, writing and experimental section. 
* The extra results on ""Utility of our location encoder beyond geo-localization"" provide some more arguments about the effectiveness of their location encoder
*  The discussion about encoding GPS with an MLP is very interesting and is backed by good experiments. 
 * Hierarchical learning experiments are solid and seem to provide good justification for the design choices. 

Weaknesses:
* For Figure 3, I have no baseline to compare against. How would other methods compare in terms of how fast the curve degrades?
* For the GPS encoding, how come the ""city"" scale is the one with the biggest rise? Could it be something related to the choice of parameters for the Random Fourier Features? It might make sense if some of the discussion from the supplementary material can be transferred to the main paper here since this is a key part of the method. 
* For the hierarchical method, how much does the merging of the outputs from the different encoders affect the overall computational needs?

Limitations:
The authors do address this in their work. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
For global localization task for a query photo, the authors propose a CLIP-inspired image-to-GPS retrieval approach (named ‘GeoCLIP’) where we retrieve the GPS coordinates of an unseen query image by matching it with the gallery of GPS coordinates.  Regarding location encoder, some new ideas such as Random Fourier Features and Hierarchical Representation were introduced. The experimental results showed the effectiveness of the proposed method by outpeforming the existing baselines.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Matching GPS location embeddings and image embeddings (GeoCLIP) are introduced into an image localization problem. This is novel.  

* It is very interesting that the location encoder gets inherent alignment with CLIP’s text features, enabling us to map textual descriptions to geographical coordinates.

* The proposed location encoding method employing Random Fourier Features is much improved compared with the straightforward method which is the direct use of an individual MLP to encode GPS coordinates into feature vectors.

* The experimental results showed that the proposed method outperformed the existing baselines which employs image search methods or grid classification methods. 

Weaknesses:
* Matching an image embedding with many location embedding (100K (Im2GPS3k) and 500K (GWS15k) coordinates) is needed to estimate a GPS location. 


Limitations:
A limitation was mentioned regarding image encoding at times of training. 

Rating:
6

Confidence:
4

";1
